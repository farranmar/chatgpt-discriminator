"Unconditional image generation has recently been dominated by generative adversarial networks (GANs). GAN methods train a generator which regresses images from random noise vectors, as well as a discriminator that attempts to differentiate between the generated images and a training set of real images. GANs have shown amazing results at generating realistic looking images. Despite their success, GANs suffer from critical drawbacks including: unstable training and mode-dropping. The weaknesses in GANs have motivated research into alternatives including: variational auto-encoders (VAEs), latent embedding learning methods (e.g. GLO) and nearest-neighbor based implicit maximum likelihood estimation (IMLE). Unfortunately at the moment, GANs still significantly outperform the alternative methods for image generation. In this work, we present a novel method - Generative Latent Nearest Neighbors (GLANN) - for training generative models without adversarial training. GLANN combines the strengths of IMLE and GLO in a way that overcomes the main drawbacks of each method. Consequently, GLANN generates images that are far better than GLO and IMLE. Our method does not suffer from mode collapse which plagues GAN training and is much more stable. Qualitative results show that GLANN outperforms a baseline consisting of 800 GANs and VAEs on commonly used datasets. Our models are also shown to be effective for training truly non-adversarial unsupervised image translation.",0
"In recent years, generative models have shown great promise for image synthesis tasks such as generating high-resolution images from low resolution inputs, filling in missing pixels, or transferring styles onto new content. However, these methods often require large datasets, powerful computing resources, and complex architectures that make them difficult to use in practice. To address these limitations, we propose a novel approach called Generative Latent Nearest Neighbors (GLNN) which leverages nearest neighbors search over latent spaces learned by autoencoders pretrained on large scale data. GLNN can generate diverse and coherent samples given any latent code, outperforming existing methods at inference time even if trained only on small-scale datasets. Furthermore, our method achieves state-of-the-art results among all non-adversarial approaches, showing strong potential towards real-world applications of computer vision and graphics where fast and efficient solutions are crucial. By demonstrating the benefits of using explicit representation learning for generative model training, we hope to inspire further research into exploring alternative architectures for generative models, paving the path towards making generative technology more accessible and usable.",1
"Domain adaption (DA) and domain generalization (DG) are two closely related methods which are both concerned with the task of assigning labels to an unlabeled data set. The only dissimilarity between these approaches is that DA can access the target data during the training phase, while the target data is totally unseen during the training phase in DG. The task of DG is challenging as we have no earlier knowledge of the target samples. If DA methods are applied directly to DG by a simple exclusion of the target data from training, poor performance will result for a given task. In this paper, we tackle the domain generalization challenge in two ways. In our first approach, we propose a novel deep domain generalization architecture utilizing synthetic data generated by a Generative Adversarial Network (GAN). The discrepancy between the generated images and synthetic images is minimized using existing domain discrepancy metrics such as maximum mean discrepancy or correlation alignment. In our second approach, we introduce a protocol for applying DA methods to a DG scenario by excluding the target data from the training phase, splitting the source data to training and validation parts, and treating the validation data as target data for DA. We conduct extensive experiments on four cross-domain benchmark datasets. Experimental results signify our proposed model outperforms the current state-of-the-art methods for DG.",0
"As computer vision systems gain widespread adoption across different fields like healthcare, finance, security etc., one crucial requirement is their ability to function under varying conditions such as lighting changes, background variations or even completely novel scenarios, which can lead to significant performance degradation if not accounted for during training. To overcome these limitations, we propose to utilize multi-component image translation models that enable domain generalization by learning common features from multiple datasets while allowing them to adapt to specific domains at test time through component selection. In this work, we focus on three commonly used benchmarks namely PACS, OfficeHome and VLCS to demonstrate the effectiveness of our proposed approach. Additionally, we also showcase how our model can generalize better than current state-of-the art methods on two real world applications involving dermoscopy images and surveillance footage. Our work highlights the importance of transferring knowledge from multiple sources towards building more robust computer vision models.",1
"It is becoming increasingly easy to automatically replace a face of one person in a video with the face of another person by using a pre-trained generative adversarial network (GAN). Recent public scandals, e.g., the faces of celebrities being swapped onto pornographic videos, call for automated ways to detect these Deepfake videos. To help developing such methods, in this paper, we present the first publicly available set of Deepfake videos generated from videos of VidTIMIT database. We used open source software based on GANs to create the Deepfakes, and we emphasize that training and blending parameters can significantly impact the quality of the resulted videos. To demonstrate this impact, we generated videos with low and high visual quality (320 videos each) using differently tuned parameter sets. We showed that the state of the art face recognition systems based on VGG and Facenet neural networks are vulnerable to Deepfake videos, with 85.62% and 95.00% false acceptance rates respectively, which means methods for detecting Deepfake videos are necessary. By considering several baseline approaches, we found that audio-visual approach based on lip-sync inconsistency detection was not able to distinguish Deepfake videos. The best performing method, which is based on visual quality metrics and is often used in presentation attack detection domain, resulted in 8.97% equal error rate on high quality Deepfakes. Our experiments demonstrate that GAN-generated Deepfake videos are challenging for both face recognition systems and existing detection methods, and the further development of face swapping technology will make it even more so.",0
"Abstract:  Advances in artificial intelligence (AI) have brought numerous benefits to society, but they can also pose new threats. One such threat comes from deepfake technology, which allows users to create realistic videos that manipulate images and audio. These manipulations can involve subtle changes or drastic alterations that make it difficult for viewers to distinguish between factual content and fiction. This presents a challenge for face recognition systems, as these algorithms rely on accurate identification of facial features to function properly.  This research evaluates the impact of deepfakes on face recognition technologies by analyzing recent advancements in both fields. First, we provide an overview of current deepfake techniques and their capabilities. Then, we review the limitations and vulnerabilities of existing face recognition methods and how deepfakes can exploit them. Next, we explore promising approaches for detecting and mitigating the effects of deepfakes on face recognition systems, including training algorithms using large datasets of synthetic data and developing more robust feature extraction models. Finally, we discuss future directions for research in this area and potential applications for improving security measures. Our findings highlight the need for continued development in face recognition technologies to keep pace with evolving deepfake attacks and maintain trust in digital media.",1
"Image forensics is an increasingly relevant problem, as it can potentially address online disinformation campaigns and mitigate problematic aspects of social media. Of particular interest, given its recent successes, is the detection of imagery produced by Generative Adversarial Networks (GANs), e.g. `deepfakes'. Leveraging large training sets and extensive computing resources, recent work has shown that GANs can be trained to generate synthetic imagery which is (in some ways) indistinguishable from real imagery. We analyze the structure of the generating network of a popular GAN implementation, and show that the network's treatment of color is markedly different from a real camera in two ways. We further show that these two cues can be used to distinguish GAN-generated imagery from camera imagery, demonstrating effective discrimination between GAN imagery and real camera images used to train the GAN.",0
"Here is an example:  ---  Artificial intelligence has been rapidly advancing over the past few years. With applications in computer vision becoming more commonplace, detectors are often based on neural networks which can find patterns within images that were previously impossible to automate. Recently, Generative Adversarial Networks (GANs) have revolutionized image generation, producing photorealistic imagery nearly identical to real photographs. While these GAN generated photos look amazingly like their original counterparts they suffer from one critical flaw; they all tend towards desaturation in contrast to authentic pictures. Inspired by how humans visually perceive color temperature through brightness and hue we created a classifier able to detect such differences at incredible accuracy. By looking at specific regions our detection tool achieves state of the art performance on popular datasets and outperforms even human level judgement. Our method requires no fine tuning, no manual feature selection, and can easily achieve near perfect results across many different domains. Additionally, as our system operates solely on luminance values, it remains lightweight, fast and efficient. Using only three basic parameters our detector performs incredibly well at detecting most kinds of GAN generated content without issue. As generators become more advanced these simple cues may eventually become less useful but currently provide for an ideal defense against fake imagery. We hope future research expands upon our work and develops new ways to identify synthetic media. Ultimately, verifying the integrity behind digital media is crucial for maintaining trustworthy communication channels and healthier online communities.  Please note, while the above text provides general guidance, I recommend checking if there are any further instructions provided by the publisher you intend to submit to regarding writing your own abstract.",1
"We present a new stage-wise learning paradigm for training generative adversarial networks (GANs). The goal of our work is to progressively strengthen the discriminator and thus, the generators, with each subsequent stage without changing the network architecture. We call this proposed method the RankGAN. We first propose a margin-based loss for the GAN discriminator. We then extend it to a margin-based ranking loss to train the multiple stages of RankGAN. We focus on face images from the CelebA dataset in our work and show visual as well as quantitative improvements in face generation and completion tasks over other GAN approaches, including WGAN and LSGAN.",0
"Artificial Intelligence can assist humans at performing numerous tasks, ranging from data entry, image recognition, language translation and even composing music. In particular, one active area of research has been using generative adversarial networks (GANs) for generating faces that look realistic yet completely fake. We present rankGAN, an application of maximum margin ranking based on GANs which outperforms other methods while providing interpretable results. rankGAN produces high fidelity images by optimizing the parameters of the generator network under a new loss function called the discriminator cross entropy with a hinge term. Through experiments conducted on both CelebA dataset and LFWA database we prove that rankGAN generates faces with higher quality than those obtained via traditional maximum likelihood estimation (MLE). Moreover, we perform ablation studies showing that removing the hinge term leads to worse performance. Lastly, rankGAN provides faithful quantitative evaluation of synthetic images allowing us to control over generation processes as well as understanding GAN dynamics.",1
"Video super-resolution (VSR) has become one of the most critical problems in video processing. In the deep learning literature, recent works have shown the benefits of using adversarial-based and perceptual losses to improve the performance on various image restoration tasks; however, these have yet to be applied for video super-resolution. In this work, we propose a Generative Adversarial Network(GAN)-based formulation for VSR. We introduce a new generator network optimized for the VSR problem, named VSRResNet, along with a new discriminator architecture to properly guide VSRResNet during the GAN training. We further enhance our VSR GAN formulation with two regularizers, a distance loss in feature-space and pixel-space, to obtain our final VSRResFeatGAN model. We show that pre-training our generator with the Mean-Squared-Error loss only quantitatively surpasses the current state-of-the-art VSR models. Finally, we employ the PercepDist metric (Zhang et al., 2018) to compare state-of-the-art VSR models. We show that this metric more accurately evaluates the perceptual quality of SR solutions obtained from neural networks, compared with the commonly used PSNR/SSIM metrics. Finally, we show that our proposed model, the VSRResFeatGAN model, outperforms current state-of-the-art SR models, both quantitatively and qualitatively.",0
"This project explores the use of generative adversarial networks (GAN) to super-resolve video frames. We compare different architectures and objective functions used in GANs and evaluate their effectiveness using perceptual losses such as Structural Similarity Index Measure (SSIM), Mean Squared Error (MSE), and Multi Scale Structured Similarity (MS-SSIM). Our results show that using GANs combined with perceptual losses leads to better visual quality compared to traditional methods alone. Additionally, we analyze the effects of hyperparameters such as generator capacity on model performance and demonstrate how our approach outperforms state-of-the art methods on standard datasets. This study offers valuable insights into the application of deep learning techniques in low-level vision tasks such as image and video reconstruction.",1
"Generative adversarial networks (GANs) are capable of producing high quality image samples. However, unlike variational autoencoders (VAEs), GANs lack encoders that provide the inverse mapping for the generators, i.e., encode images back to the latent space. In this work, we consider adversarially learned generative models that also have encoders. We evaluate models based on their ability to produce high quality samples and reconstructions of real images. Our main contributions are twofold: First, we find that the baseline Bidirectional GAN (BiGAN) can be improved upon with the addition of an autoencoder loss, at the expense of an extra hyper-parameter to tune. Second, we show that comparable performance to BiGAN can be obtained by simply training an encoder to invert the generator of a normal GAN.",0
"This paper presents the results of an empirical study on generative models that incorporate encoder components into their architecture. We evaluate several different types of encoders, including recurrent neural networks (RNNs), convolutional neural networks (CNNs), transformer networks, and autoencoders, and assess their performance in generating data from different domains such as language, images, audio, and video. Our analysis shows that the choice of encoder has a significant impact on model accuracy and efficiency, with CNNs and transformers showing particularly strong performance across many tasks. Furthermore, we demonstrate how the integration of encoders can improve the stability and generalization ability of generative models. Finally, we provide recommendations on which encoders may be most suitable for particular applications based on our experimental findings. Overall, our work contributes new insights into the role of encoders in generative models and provides a valuable reference point for future research in this area.",1
"Supervised deep learning algorithms have enabled significant performance gains in medical image classification tasks. But these methods rely on large labeled datasets that require resource-intensive expert annotation. Semi-supervised generative adversarial network (GAN) approaches offer a means to learn from limited labeled data alongside larger unlabeled datasets, but have not been applied to discern fine-scale, sparse or localized features that define medical abnormalities. To overcome these limitations, we propose a patch-based semi-supervised learning approach and evaluate performance on classification of diabetic retinopathy from funduscopic images. Our semi-supervised approach achieves high AUC with just 10-20 labeled training images, and outperforms the supervised baselines by upto 15% when less than 30% of the training dataset is labeled. Further, our method implicitly enables interpretation of the SSL predictions. As this approach enables good accuracy, resolution and interpretability with lower annotation burden, it sets the pathway for scalable applications of deep learning in clinical imaging.",0
"Abstract: We present a semi-supervised deep learning approach for abnormality classification in retinal images. Our method leverages both labeled and unlabeled data to improve performance on this task. By using pre-trained models as initialization points, we were able to fine-tune our model to achieve state-of-the-art results compared to fully supervised methods that only use limited amounts of labeled data. Additionally, by implementing an attention mechanism within the model, we were able to increase accuracy and robustness by focusing specifically on informative regions of interest in each image. Our work demonstrates the potential for improved medical diagnosis through the integration of advanced artificial intelligence techniques in healthcare.",1
"This paper presents a novel deep learning framework for human trajectory prediction and detecting social group membership in crowds. We introduce a generative adversarial pipeline which preserves the spatio-temporal structure of the pedestrian's neighbourhood, enabling us to extract relevant attributes describing their social identity. We formulate the group detection task as an unsupervised learning problem, obviating the need for supervised learning of group memberships via hand labeled databases, allowing us to directly employ the proposed framework in different surveillance settings. We evaluate the proposed trajectory prediction and group detection frameworks on multiple public benchmarks, and for both tasks the proposed method demonstrates its capability to better anticipate human sociological behaviour compared to the existing state-of-the-art methods.",0
"In summary, we propose GD-GAN, a novel framework that leverages generative adversarial networks (GANs) for trajectory prediction and group detection in crowded scenes. Our method enables fast computation while maintaining accurate predictions by learning a mapping from sensor data directly into future scene states. We demonstrate the effectiveness of our approach using extensive experiments on real-world datasets, showing improved performance over existing methods across multiple metrics, including accuracy and speed. With the increasing prevalence of autonomous vehicles and drones operating in complex environments, efficient and robust crowd tracking algorithms such as GD-GAN are essential for ensuring safe navigation and reducing accidents. Overall, our work highlights the potential of deep learning techniques in addressing key challenges in computer vision and robotics applications involving human interactions in dynamic settings. Title: GD-GAN: Generative Adversarial Networks for Trajectory Prediction and Group Detection in Crowds Abstract: This paper presents GD-GAN, a novel framework that utilizes generative adversarial networks (GANs) for predicting trajectories and detecting groups in densely populated areas. Traditional approaches often struggle with accurately forecasting movements in crowded environments, leading to dangerous situations for both humans and machines. By using GANs to learn from raw sensor data, GD-GAN can quickly generate detailed and reliable future scene states, effectively tackling these issues. Comprehensive testing reveals marked improvements across various evaluation criteria compared to current state-of-the-art techniques. These promising results have significant implications for emerging technologies, particularly autonomous systems navigating bustling spaces, where swift yet precise pedestrian monitoring remains crucial for averting collisions and enhancing public safety. While other research has explored incorporating deep learning models within motion analysis frameworks, none offer comparable efficiency coupled with superior accuracy until now - GD-GAN represents a step towards more intelligent systems interacting harmoniously alongside humans in congested contexts.",1
"We propose a novel semi-supervised, Multi-Level Sequential Generative Adversarial Network (MLS-GAN) architecture for group activity recognition. In contrast to previous works which utilise manually annotated individual human action predictions, we allow the models to learn it's own internal representations to discover pertinent sub-activities that aid the final group activity recognition task. The generator is fed with person-level and scene-level features that are mapped temporally through LSTM networks. Action-based feature fusion is performed through novel gated fusion units that are able to consider long-term dependencies, exploring the relationships among all individual actions, to learn an intermediate representation or `action code' for the current group activity. The network achieves its semi-supervised behaviour by allowing it to perform group action classification together with the adversarial real/fake validation. We perform extensive evaluations on different architectural variants to demonstrate the importance of the proposed architecture. Furthermore, we show that utilising both person-level and scene-level features facilitates the group activity prediction better than using only person-level features. Our proposed architecture outperforms current state-of-the-art results for sports and pedestrian based classification tasks on Volleyball and Collective Activity datasets, showing it's flexible nature for effective learning of group activities.",0
"In recent years, group activity recognition has become an increasingly important task due to the popularity of multi-camera surveillance systems and social media platforms that generate large amounts of video data containing human activities. Existing methods have focused on extracting features from individual frames or segments of videos without considering temporal dependencies between them. However, these approaches often suffer from limited performance since they fail to capture the complex relationships among different actions within a group setting. This paper presents a novel approach based on multi-level sequence generative adversarial networks (GANs) for recognizing group activities in videos. Our method leverages both spatial and temporal information by modeling pixel-level representations as well as their corresponding motion trajectories over time. We design two complementary discriminators: one focuses on image generation while the other evaluates action consistency throughout consecutive frames. By training our network using synthetic data generated by a pre-trained movement prediction module, we achieve state-of-the-art results across multiple benchmark datasets while maintaining efficiency and scalability. Our work represents an important step towards realizing robust and reliable automatic analysis of spatiotemporal patterns underlying collective behaviors.",1
"This project report compares some known GAN and VAE models proposed prior to 2017. There has been significant progress after we finished this report. We upload this report as an introduction to generative models and provide some personal interpretations supported by empirical evidence. Both generative adversarial network models and variational autoencoders have been widely used to approximate probability distributions of data sets. Although they both use parametrized distributions to approximate the underlying data distribution, whose exact inference is intractable, their behaviors are very different. We summarize our experiment results that compare these two categories of models in terms of fidelity and mode collapse. We provide a hypothesis to explain their different behaviors and propose a new model based on this hypothesis. We further tested our proposed model on MNIST dataset and CelebA dataset.",0
"Recent advances in deep learning have led to the development of powerful generative models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). These models can generate synthetic data that is similar to real-world observations, making them applicable to various domains such as image and text generation, computer vision, natural language processing, etc. However, understanding these complex models can be challenging due to their inherent mathematical complexity.  This paper presents a comprehensive overview of GANs and VAEs, including key concepts, architectures, training objectives, and applications. We begin by introducing the basic principles behind GANs and VAEs before delving into more advanced topics like cycle consistency, adversarial training, feature matching, latent space manipulation, etc. Additionally, we discuss common pitfalls faced during model implementation and provide suggestions on how to mitigate them.  Throughout the paper, we illustrate our discussion using relevant examples and experiments from real-world scenarios. Our goal is to equip readers with a solid foundation in GANs and VAEs so they can build upon existing research and contribute towards future developments in this exciting field. By providing a thorough investigation of these powerful models, this paper serves both novice learners and experienced practitioners alike who wish to expand their knowledge base in deep learning.",1
"This article proposes a method for mathematical modeling of human movements related to patient exercise episodes performed during physical therapy sessions by using artificial neural networks. The generative adversarial network structure is adopted, whereby a discriminative and a generative model are trained concurrently in an adversarial manner. Different network architectures are examined, with the discriminative and generative models structured as deep subnetworks of hidden layers comprised of convolutional or recurrent computational units. The models are validated on a data set of human movements recorded with an optical motion tracker. The results demonstrate an ability of the networks for classification of new instances of motions, and for generation of motion examples that resemble the recorded motion sequences.",0
"This project proposes novel contributions towards realizing intelligent systems that assist in rehabilitation through human motion analysis using wearable sensors. To achieve this goal, we introduce two generative models, one for generating new movements (GAN generator), which can be used to synthesize new exercises from scratch, or modify existing ones based on specific goals, needs, etc., and another for classifying episodes into different types of activities/exercises (GAN discriminator). Our approach introduces several improvements over current state-of-the-art methods, including: 1) our use of deep learning architectures to model spatiotemporal features extracted from raw sensor signals; 2) improved data augmentation techniques specifically designed to enhance training datasets by creating variations that preserve their key characteristics; 3) explicit control of randomness during GAN sampling that enables fine tuning according to expert knowledge; 4) attention mechanisms incorporated within both GANs to selectively focus computation where most relevant for each episodeâ€™s unique characterization; 5) transfer learning strategies exploited across diverse domains allowing more efficient utilization of annotated data; 6) introduction of novel evaluation metrics to better quantify success at capturing desired properties for generated and classified episodes. Experiments conducted on publicly available datasets showcase superior performance over prior arts when considering accuracy of classification as well as ability to generate meaningful content closely aligned with target distributions while meeting desired constraints. These results confirm feasibility of deploying such technologies alongside therapists in the future. Ultimately, these tools could aid clinicians in developing personalized treatment plans adapted t",1
"We know SGAN may have a risk of gradient vanishing. A significant improvement is WGAN, with the help of 1-Lipschitz constraint on discriminator to prevent from gradient vanishing. Is there any GAN having no gradient vanishing and no 1-Lipschitz constraint on discriminator? We do find one, called GAN-QP.   To construct a new framework of Generative Adversarial Network (GAN) usually includes three steps: 1. choose a probability divergence; 2. convert it into a dual form; 3. play a min-max game. In this articles, we demonstrate that the first step is not necessary. We can analyse the property of divergence and even construct new divergence in dual space directly. As a reward, we obtain a simpler alternative of WGAN: GAN-QP. We demonstrate that GAN-QP have a better performance than WGAN in theory and practice.",0
"Abstract  A new deep learning framework called GAN-QP has been proposed that addresses two key limitations of traditional Generative Adversarial Networks (GANs). Firstly, the gradient vanishing problem often causes difficulty in training, leading to subpar results. Secondly, the use of Lipschitz constraints can limit model expressiveness by enforcing small gradients near flat regions of the data distribution. To solve these issues, GAN-QP introduces an auxiliary variable, Q, which helps stabilize the generator and discriminator updates during optimization. This allows for efficient end-to-end training while preserving full freedom at initialization and inference. Experiments on various benchmark datasets demonstrate the effectiveness of GAN-QP over baseline models. These promising results open up new possibilities for generating high-quality synthetic samples.",1
"We introduce effective training algorithms for Generative Adversarial Networks (GAN) to alleviate mode collapse and gradient vanishing. In our system, we constrain the generator by an Autoencoder (AE). We propose a formulation to consider the reconstructed samples from AE as ""real"" samples for the discriminator. This couples the convergence of the AE with that of the discriminator, effectively slowing down the convergence of discriminator and reducing gradient vanishing. Importantly, we propose two novel distance constraints to improve the generator. First, we propose a latent-data distance constraint to enforce compatibility between the latent sample distances and the corresponding data sample distances. We use this constraint to explicitly prevent the generator from mode collapse. Second, we propose a discriminator-score distance constraint to align the distribution of the generated samples with that of the real samples through the discriminator score. We use this constraint to guide the generator to synthesize samples that resemble the real ones. Our proposed GAN using these distance constraints, namely Dist-GAN, can achieve better results than state-of-the-art methods across benchmark datasets: synthetic, MNIST, MNIST-1K, CelebA, CIFAR-10 and STL-10 datasets. Our code is published here (https://github.com/tntrung/gan) for research.",0
"GANs have recently shown great promise in generative tasks such as image generation, music composition and text completion. In order to further improve their performance in these applications we propose Dist-GAN, a novel architecture that incorporates distance constraints into the generator network and show how this improves both visual quality and sample diversity. The key idea behind Dist-GAN is to explicitly model the data distribution by introducing additional loss terms based on pairwise distances in feature space, encouraging the generator to produce outputs closer to real data than those from alternative generators. This ensures better alignment between generated samples and actual data which results in more accurate task predictions. We demonstrate the effectiveness of our method through extensive experimentation on several benchmark datasets including MNIST, CelebA, Omniglot and LSUN, showing consistent improvements over state-of-the-art models, even at higher resolution settings where recent approaches struggle. Our proposed approach is simple yet effective and can benefit various downstream applications that rely on high-quality generated samples. Keywords: Generative Adversarial Network (GAN), Diversity, Discriminator, Pairwise Loss, Quality.",1
"We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image, where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry, we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometry-related features. Compared to standard image generation problems, which often use generative adversarial networks, our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous, to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets - faces, people, 3D objects, and digits - without any modifications.",0
"This is an abstract for a paper titled ""Unsupervised learning of object landmarks through conditional image generation."" The paper presents a method that leverages unlabeled images of objects along with their corresponding textual descriptions to learn landmark locations without any human annotations. Our approach involves training a deep neural network on pairs of input images and textual prompts, where the network learns to generate synthetic versions of these inputs while preserving salient features such as keypoints. We demonstrate the effectiveness of our method by applying it to several real-world tasks involving object detection and pose estimation, achieving results that compete favorably against other state-of-the-art approaches relying on handcrafted features or extensive supervision from labeled data. Keywords: unsupervised learning, landmark localization, generative models, object recognition, computer vision.",1
"Deep neural networks suffer from over-fitting and catastrophic forgetting when trained with small data. One natural remedy for this problem is data augmentation, which has been recently shown to be effective. However, previous works either assume that intra-class variances can always be generalized to new classes, or employ naive generation methods to hallucinate finite examples without modeling their latent distributions. In this work, we propose Covariance-Preserving Adversarial Augmentation Networks to overcome existing limits of low-shot learning. Specifically, a novel Generative Adversarial Network is designed to model the latent distribution of each novel class given its related base counterparts. Since direct estimation of novel classes can be inductively biased, we explicitly preserve covariance information as the `variability' of base examples during the generation process. Empirical results show that our model can generate realistic yet diverse examples, leading to substantial improvements on the ImageNet benchmark over the state of the art.",0
This task involves creating a text summary that conveys the main idea behind Low-shot learning. You should focus on explaining the key concepts involved in low shot learning as well as how adversarial augmentation networks can improve performance without the need for more data.,1
"The mood of a text and the intention of the writer can be reflected in the typeface. However, in designing a typeface, it is difficult to keep the style of various characters consistent, especially for languages with lots of morphological variations such as Chinese. In this paper, we propose a Typeface Completion Network (TCN) which takes one character as an input, and automatically completes the entire set of characters in the same style as the input characters. Unlike existing models proposed for image-to-image translation, TCN embeds a character image into two separate vectors representing typeface and content. Combined with a reconstruction loss from the latent space, and with other various losses, TCN overcomes the inherent difficulty in designing a typeface. Also, compared to previous image-to-image translation models, TCN generates high quality character images of the same typeface with a much smaller number of model parameters. We validate our proposed model on the Chinese and English character datasets, which is paired data, and the CelebA dataset, which is unpaired data. In these datasets, TCN outperforms recently proposed state-of-the-art models for image-to-image translation. The source code of our model is available at https://github.com/yongqyu/TCN.",0
"Deep learning techniques have shown great promise for computer vision problems such as image generation, classification, segmentation, and detection. Many real world applications however require text data rather than images alone, such as document layout processing and natural language understanding tasks like machine translation. To tackle these new challenges deep learning methods must take into account additional forms of information beyond just raw pixel values. One key piece of information that has been previously ignored by most convolutional networks is the specific shape and structure that different characters form within natural languages. Recent work using recurrent neural network architectures such as LSTMs with attention mechanisms have demonstrated promising results on problems involving structured sequences of tokens, but these models still struggle with long range dependencies which become more prominent when dealing with natural language documents. Our work focuses specifically on the problem of typeface completion, where given incomplete character shapes we aim to predict the missing portion of the character while preserving the local geometry and overall style of the existing glyph elements. We propose solving this task via generative adversarial networks (GANs), a family of deep models that consist of two subnetworks trained together under a minimax optimization framework. By leveraging large scale datasets that contain complete character examples paired against their partial counterparts, our model learns to generate coherent and plausible predictions conditioned both locally and globally by the input context. Experiments conducted over several benchmark datasets demonstrate significant improvement compared to strong baselines on standard metrics for evaluation. Furthermore, qualitative analysis suggests that generated predictions closely resemble true character completions even at high levels of noise and corruption. This work paves the way for future research directions that seek t",1
"Deep Convolution Neural Networks (CNN) have achieved significant performance on single image super-resolution (SR) recently. However, existing CNN-based methods use artificially synthetic low-resolution (LR) and high-resolution (HR) image pairs to train networks, which cannot handle real-world cases since the degradation from HR to LR is much more complex than manually designed. To solve this problem, we propose a real-world LR images guided bi-cycle network for single image super-resolution, in which the bidirectional structural consistency is exploited to train both the degradation and SR reconstruction networks in an unsupervised way. Specifically, we propose a degradation network to model the real-world degradation process from HR to LR via generative adversarial networks, and these generated realistic LR images paired with real-world HR images are exploited for training the SR reconstruction network, forming the first cycle. Then in the second reverse cycle, consistency of real-world LR images are exploited to further stabilize the training of SR reconstruction and degradation networks. Extensive experiments on both synthetic and real-world images demonstrate that the proposed algorithm performs favorably against state-of-the-art single image SR methods.",0
Title: Learning Unsupervised SISR by Solving Jointly for Task and Meta-Aware Losses Authors: ...,1
"Ultra-wideband (UWB) radar systems nowadays typical operate in the low frequency spectrum to achieve penetration capability. However, this spectrum is also shared by many others communication systems, which causes missing information in the frequency bands. To recover this missing spectral information, we propose a generative adversarial network, called SARGAN, that learns the relationship between original and missing band signals by observing these training pairs in a clever way. Initial results shows that this approach is promising in tackling this challenging missing band problem.",0
"In recent years, there has been significant interest in developing machine learning methods that can accurately recover missing spectral information in hyperspectral images (HSIs). One promising approach towards achieving this goal is through the use of generative adversarial networks (GANs), which have proven effective in generating realistic synthetic HSIs from lower resolution inputs. However, while there exist several works on applying GANs to image generation tasks, little attention has been paid to exploring their application for missing spectrum recovery. To address this gap, we propose the use of a novel architecture based on the U-Net model, termed GAUNet, which combines the benefits of residual learning with GANs for missing spectrum recovery. Our results demonstrate that GAUNet outperforms state-of-the-art approaches for this task, while offering important insights into the suitability of GANs for high-dimensional data processing applications. These findings hold great promise for advancing remote sensing research by improving the accuracy and efficiency of image analysis techniques. By providing new perspectives on the capabilities of GAN models, our work serves as an inspiration for future research directions in computer vision and signal processing communities.",1
"The large domain discrepancy between faces captured in polarimetric (or conventional) thermal and visible domain makes cross-domain face verification a highly challenging problem for human examiners as well as computer vision algorithms. Previous approaches utilize either a two-step procedure (visible feature estimation and visible image reconstruction) or an input-level fusion technique, where different Stokes images are concatenated and used as a multi-channel input to synthesize the visible image given the corresponding polarimetric signatures. Although these methods have yielded improvements, we argue that input-level fusion alone may not be sufficient to realize the full potential of the available Stokes images. We propose a Generative Adversarial Networks (GAN) based multi-stream feature-level fusion technique to synthesize high-quality visible images from prolarimetric thermal images. The proposed network consists of a generator sub-network, constructed using an encoder-decoder network based on dense residual blocks, and a multi-scale discriminator sub-network. The generator network is trained by optimizing an adversarial loss in addition to a perceptual loss and an identity preserving loss to enable photo realistic generation of visible images while preserving discriminative characteristics. An extended dataset consisting of polarimetric thermal facial signatures of 111 subjects is also introduced. Multiple experiments evaluated on different experimental protocols demonstrate that the proposed method achieves state-of-the-art performance. Code will be made available at https://github.com/hezhangsprinter.",0
"This paper proposes a method for synthesizing high-quality visible faces from polarimetric thermal images using generative adversarial networks (GANs). We use GANs to learn a mapping function between the two domains that preserves important features while ensuring realism. Our approach utilizes both the thermal image as well as additional visible face data during training, enabling us to generate more accurate results than previous methods. We evaluate our model on several datasets and demonstrate significant improvements over baseline models. Additionally, we provide qualitative analysis to showcase the quality of the generated visible faces. Overall, our work shows promise for applications such as biometric recognition, surveillance, and virtual reality, where realistic generation of human faces is critical.",1
"Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.",0
"Recent advances in Generative Adversarial Networks (GAN) have shown promising results in many computer vision tasks, including person re-identification. However, current methods often suffer from limitations such as pose variations, background clutter, and illumination changes that affect the accuracy of identity recognition. To address these challenges, we propose a novel framework called Pose-Guided Feature Distilling GAN (FD-GAN). Our method utilizes both global features and local features distilled from discriminator modules to learn more robust representations of pedestrians across different camera views. We introduce a two-stream network architecture that fuses global and local feature maps, allowing our model to capture richer information for cross-view matching. Additionally, we employ an attention mechanism to highlight regions where global information is most relevant. Experimental evaluations on four public datasets demonstrate the effectiveness of our approach, outperforming state-of-the-art methods under various settings. This work contributes towards developing more accurate and robust solutions for person re-identification in real-world scenarios.",1
"We propose Graphical Generative Adversarial Networks (Graphical-GAN) to model structured data. Graphical-GAN conjoins the power of Bayesian networks on compactly representing the dependency structures among random variables and that of generative adversarial networks on learning expressive dependency functions. We introduce a structured recognition model to infer the posterior distribution of latent variables given observations. We generalize the Expectation Propagation (EP) algorithm to learn the generative model and recognition model jointly. Finally, we present two important instances of Graphical-GAN, i.e. Gaussian Mixture GAN (GMGAN) and State Space GAN (SSGAN), which can successfully learn the discrete and temporal structures on visual datasets, respectively.",0
"Advanced computer graphics, such as those used in movies, video games, and simulations often require large amounts of data, time, and resources to create realistically looking scenes. Recently, artificial intelligence (AI) techniques have been developed to generate more detailed and accurate images by training on vast amounts of existing image datasets. This has led to significant improvements in areas like style transfer, super resolution, colorization, texture synthesis, semantic segmentation, denoising, among others. However, there remains a need for methods that can create high quality images while consuming fewer computational resources than current deep learning based approaches. To address these limitations, we propose graphical generative adversarial networks (GANS), which use interactive visual tools inspired by traditional illustration software to improve the ease and efficiency of creating computer generated imagery. GANS leverage recent advances in neural rendering and inverse modeling algorithms, allowing them to synthesize novel outputs by predicting missing information from sparse input fields. Our experiments demonstrate the effectiveness and versatility of our approach across several tasks, including face generation, scene completion, object insertion, and image editing. Results show that our method outperforms state-of-the-art AI models while offering improved user control and flexibility over generated content. We believe that GANS open new possibilities for creators and researchers working in computer graphics, animation, VFX, AR/VR, and other related domains.",1
"As biometric applications are fielded to serve large population groups, issues of performance differences between individual sub-groups are becoming increasingly important. In this paper we examine cases where we believe race is one such factor. We look in particular at two forms of problem; facial classification and image synthesis. We take the novel approach of considering race as a boundary for transfer learning in both the task (facial classification) and the domain (synthesis over distinct datasets). We demonstrate a series of techniques to improve transfer learning of facial classification; outperforming similar models trained in the target's own domain. We conduct a study to evaluate the performance drop of Generative Adversarial Networks trained to conduct image synthesis, in this process, we produce a new annotation for the Celeb-A dataset by race. These networks are trained solely on one race and tested on another - demonstrating the subsets of the CelebA to be distinct domains for this task.",0
"The successes of deep learning methods have been demonstrated in numerous tasks ranging from computer vision to natural language processing, but these models often lack interpretability and transparency. One promising direction towards solving these issues is transfer learning, which uses pretrained models as starting points and fine tunes them on new datasets. While previous work has considered race to be one aspect of demographics among others such as age or gender that can affect model performance, we propose a novel methodology where race itself is treated as the problem for which transfer learning is applied. Our approach shows significant improvements over baseline models and competitive results compared to state-of-the art systems across several challenging benchmarks. We provide insights into how our method works by visualizing learned feature representations and analyzing their correspondence with human annotations and expert judgements. These findings demonstrate the potential of treating race explicitly within the machine learning pipeline as well as highlight opportunities for future research at the intersection of computer science and social sciences.",1
"Generative adversarial networks have been able to generate striking results in various domains. This generation capability can be general while the networks gain deep understanding regarding the data distribution. In many domains, this data distribution consists of anomalies and normal data, with the anomalies commonly occurring relatively less, creating datasets that are imbalanced. The capabilities that generative adversarial networks offer can be leveraged to examine these anomalies and help alleviate the challenge that imbalanced datasets propose via creating synthetic anomalies. This anomaly generation can be specifically beneficial in domains that have costly data creation processes as well as inherently imbalanced datasets. One of the domains that fits this description is the host-based intrusion detection domain. In this work, ADFA-LD dataset is chosen as the dataset of interest containing system calls of small foot-print next generation attacks. The data is first converted into images, and then a Cycle-GAN is used to create images of anomalous data from images of normal data. The generated data is combined with the original dataset and is used to train a model to detect anomalies. By doing so, it is shown that the classification results are improved, with the AUC rising from 0.55 to 0.71, and the anomaly detection rate rising from 17.07% to 80.49%. The results are also compared to SMOTE, showing the potential presented by generative adversarial networks in anomaly generation.",0
"Abstract: Host based intrusion detection (HIDS) systems monitor system logs on individual hosts to identify suspicious activity that might indicate malware intrusion or other types of attacks. Traditional approaches rely on predefined rules, but these can miss novel attacks that have never been seen before. Here we propose using generative adversarial networks (GANs), which consist of two neural networks trained against each other, to generate synthetic examples that better capture the range of possible attack behavior in HIDS data. We show that GAN anomaly generation outperforms traditional unsupervised methods like PCA and autoencoders as well as supervised learning with rule-based features. Our work has important implications for improving HIDS performance and demonstrates how deep learning models can enhance cybersecurity analyses. Keywords: host based intrusion detection, generative adversarial networks, anomaly detection, deep learning.",1
"Generative adversarial networks are a class of generative algorithms that have been widely used to produce state-of-the-art samples. In this paper, we investigate GAN to perform anomaly detection on time series dataset. In order to achieve this goal, a bibliography is made focusing on theoretical properties of GAN and GAN used for anomaly detection. A Wasserstein GAN has been chosen to learn the representation of normal data distribution and a stacked encoder with the generator performs the anomaly detection. W-GAN with encoder seems to produce state of the art anomaly detection scores on MNIST dataset and we investigate its usage on multi-variate time series.",0
"In recent years, generative adversarial networks (GANs) have emerged as powerful tools for anomaly detection tasks due to their ability to generate realistic data samples that can serve as a reliable reference set for identifying outliers. One variant of GANs, namely the",1
"In this paper, we propose a solution to transform a video into a comics. We approach this task using a neural style algorithm based on Generative Adversarial Networks (GANs). Several recent works in the field of Neural Style Transfer showed that producing an image in the style of another image is feasible. In this paper, we build up on these works and extend the existing set of style transfer use cases with a working application of video comixification. To that end, we train an end-to-end solution that transforms input video into a comics in two stages. In the first stage, we propose a state-of-the-art keyframes extraction algorithm that selects a subset of frames from the video to provide the most comprehensive video context and we filter those frames using image aesthetic estimation engine. In the second stage, the style of selected keyframes is transferred into a comics. To provide the most aesthetically compelling results, we selected the most state-of-the art style transfer solution and based on that implement our own ComixGAN framework. The final contribution of our work is a Web-based working application of video comixification available at http://comixify.ii.pw.edu.pl.",0
"Comixification involves converting digital videos into comic books. This process has gained popularity in recent years due to advances in technology and user demand. In this study, we propose an algorithm that automatically converts raw video footage into a sequence of still images resembling the panels of traditional print comics. Our method uses deep learning techniques to detect important frames from the input video and then generates panel compositions based on these keyframes. To achieve natural transitions between consecutive panels, we use motion vector data as additional inputs during training. We showcase our approach through extensive experiments and demonstrate that our method achieves state-of-the-art performance among similar systems designed to convert videos into comics. Our work presents new opportunities for researchers interested in developing applications for diverse audiences who prefer receiving content via sequential art. Additionally, our technique can serve as a basis for developing other novel multimedia processing tools such as generating storyboards and animatics from real-world scenes.",1
"Classifiers fail to classify correctly input images that have been purposefully and imperceptibly perturbed to cause misclassification. This susceptability has been shown to be consistent across classifiers, regardless of their type, architecture or parameters. Common defenses against adversarial attacks modify the classifer boundary by training on additional adversarial examples created in various ways. In this paper, we introduce AutoGAN, which counters adversarial attacks by enhancing the lower-dimensional manifold defined by the training data and by projecting perturbed data points onto it. AutoGAN mitigates the need for knowing the attack type and magnitude as well as the need for having adversarial samples of the attack. Our approach uses a Generative Adversarial Network (GAN) with an autoencoder generator and a discriminator that also serves as a classifier. We test AutoGAN against adversarial samples generated with state-of-the-art Fast Gradient Sign Method (FGSM) as well as samples generated with random Gaussian noise, both using the MNIST dataset. For different magnitudes of perturbation in training and testing, AutoGAN can surpass the accuracy of FGSM method by up to 25\% points on samples perturbed using FGSM. Without an augmented training dataset, AutoGAN achieves an accuracy of 89\% compared to 1\% achieved by FGSM method on FGSM testing adversarial samples.",0
"In recent years, deep neural networks have become increasingly popular due to their ability to perform tasks such as image classification at human level accuracy. However, these models are susceptible to adversarial attacks, which can fool them into making incorrect predictions by adding small perturbations to input data. To address this issue, researchers have developed techniques such as adversarial training and feature denoising, but they often suffer from high computational cost and limited effectiveness against strong attacks. This paper introduces AutoGAN, a novel approach that integrates generative models with traditional classifiers to improve robustness against adversarial attacks. We propose two different methods to incorporate GANs: (1) generating noise to regularize hidden layers, (2) using adversarial examples generated by an autoencoder discriminator as additional training data. Experimental results show that our proposed method significantly improves performance compared to existing defense strategies while maintaining low computational overhead. Our work provides a new direction for developing more effective defenses against adversarial attacks in machine learning.",1
"Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.   In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models.",0
"GAN dissection is an important tool for understanding how generative adversarial networks (GANs) generate new data points that have certain desirable properties. These properties can vary widely depending on the specific task at hand, but some examples might include generating images of faces with particular emotional expressions, creating audio files that sound like famous singers, or producing text snippets in a given language style. Regardless of the type of data generated by a GAN, there are several key questions one may want to ask in order to better understand these models. For example, we may wonder how different parts of the network interact with each other during training, whether they learn meaningful representations of the input space, or how sensitive the output distribution is to changes in model parameters. One approach to addressing these issues is through the use of visualization techniques such as heatmaps, scatter plots, and animation. By leveraging insights gained from these visualizations, researchers can develop a deeper appreciation for how GANs function internally, allowing them to make more informed choices when building or fine-tuning their own models. Overall, our study represents an initial exploration into the capabilities of GAN dissection tools, offering a first step towards uncovering fundamental principles behind these powerful machine learning algorithms. If you would like a detailed summary please tell me here. Thank You!",1
"The extension of image generation to video generation turns out to be a very difficult task, since the temporal dimension of videos introduces an extra challenge during the generation process. Besides, due to the limitation of memory and training stability, the generation becomes increasingly challenging with the increase of the resolution/duration of videos. In this work, we exploit the idea of progressive growing of Generative Adversarial Networks (GANs) for higher resolution video generation. In particular, we begin to produce video samples of low-resolution and short-duration, and then progressively increase both resolution and duration alone (or jointly) by adding new spatiotemporal convolutional layers to the current networks. Starting from the learning on a very raw-level spatial appearance and temporal movement of the video distribution, the proposed progressive method learns spatiotemporal information incrementally to generate higher resolution videos. Furthermore, we introduce a sliced version of Wasserstein GAN (SWGAN) loss to improve the distribution learning on the video data of high-dimension and mixed-spatiotemporal distribution. SWGAN loss replaces the distance between joint distributions by that of one-dimensional marginal distributions, making the loss easier to compute. We evaluate the proposed model on our collected face video dataset of 10,900 videos to generate photorealistic face videos of 256x256x32 resolution. In addition, our model also reaches a record inception score of 14.57 in unsupervised action recognition dataset UCF-101.",0
"Here we present two key insights that enable progressively growing our large Generative Adversarial Network (GAN) architectures for video generation: i) the effectiveness of using small, fixed-size discriminators for high resolution image generation; and ii) the utility of conditioning on future frames as well as previous frames during training. We propose SliceWasserstein GANs, where multiple discriminator streams divide up the input space and a novel architecture alternates between full resolution output from generator and downscaled versions after each slice for more stable optimization. Extensive experiments over three datasets show significant advantages over all existing methods across all metrics including visual fidelity, temporal coherency, and diversity. Our methodology paves the way towards efficiently scaling GANs for high resolution, temporally coherent video synthesis.",1
"Anomaly detection is a significant and hence well-studied problem. However, developing effective anomaly detection methods for complex and high-dimensional data remains a challenge. As Generative Adversarial Networks (GANs) are able to model the complex high-dimensional distributions of real-world data, they offer a promising approach to address this challenge. In this work, we propose an anomaly detection method, Adversarially Learned Anomaly Detection (ALAD) based on bi-directional GANs, that derives adversarially learned features for the anomaly detection task. ALAD then uses reconstruction errors based on these adversarially learned features to determine if a data sample is anomalous. ALAD builds on recent advances to ensure data-space and latent-space cycle-consistencies and stabilize GAN training, which results in significantly improved anomaly detection performance. ALAD achieves state-of-the-art performance on a range of image and tabular datasets while being several hundred-fold faster at test time than the only published GAN-based method.",0
"Advances in deep learning have led to the development of novel techniques for detecting anomalies in data. One such technique, adversarially learned anomaly detection (ALAD), has shown promising results in identifying rare events that deviate from expected patterns. ALAD involves training a model by iteratively generating fake examples that are difficult for a detector to distinguish from real ones. By doing so, the system can learn to recognize genuine outliers without requiring explicit definitions or manual feature engineering. In this work, we explore the effectiveness of ALAD on several datasets across different domains, including image and text classification tasks. Our experiments demonstrate the competitive performance of ALAD compared to other state-of-the-art anomaly detection methods. We further analyze the characteristics of detected anomalies, providing insights into how ALAD works at the example level. Overall, our findings suggest that ALAD holds significant potential as a versatile tool for unsupervised anomaly detection, particularly in high-dimensional spaces where defining meaningful features may pose challenges.",1
"A multilayer perceptron can behave as a generative classifier by applying bidirectional learning (BL). It consists of training an undirected neural network to map input to output and vice-versa; therefore it can produce a classifier in one direction, and a generator in the opposite direction for the same data. The learning process of BL tries to reproduce the neuroplasticity stated in Hebbian theory using only backward propagation of errors. In this paper, two novel learning techniques are introduced which use BL for improving robustness to white noise static and adversarial examples. The first method is bidirectional propagation of errors, which the error propagation occurs in backward and forward directions. Motivated by the fact that its generative model receives as input a constant vector per class, we introduce as a second method the hybrid adversarial networks (HAN). Its generative model receives a random vector as input and its training is based on generative adversarial networks (GAN). To assess the performance of BL, we perform experiments using several architectures with fully and convolutional layers, with and without bias. Experimental results show that both methods improve robustness to white noise static and adversarial examples, and even increase accuracy, but have different behavior depending on the architecture and task, being more beneficial to use the one or the other. Nevertheless, HAN using a convolutional architecture with batch normalization presents outstanding robustness, reaching state-of-the-art accuracy on adversarial examples of hand-written digits.",0
"This paper presents bidirectional learning as a method that improves neural network resilience by incorporating both positive and negative examples into model training. We observe that models trained on exclusively one type of example (positive or negative) tend to have poorer performance than those exposed to both types during training. By utilizing both positive and negative data points at each iteration of the backpropagation algorithm, we achieve higher robustness against distribution shift. In addition, our approach can be easily integrated with existing machine learning frameworks, such as PyTorch. Our experimental results demonstrate significant improvements over baseline methods across three domains: image classification, sentiment analysis, and question answering. For instance, our technique outperforms state-of-the-art techniques on the CIFAR10 dataset by upwards of 7%. Our findings suggest that using bidirectional learning is crucial for real-world applications where distribution shifts may occur unexpectedly.",1
"The field of automatic video generation has received a boost thanks to the recent Generative Adversarial Networks (GANs). However, most existing methods cannot control the contents of the generated video using a text caption, losing their usefulness to a large extent. This particularly affects human videos due to their great variety of actions and appearances. This paper presents Conditional Flow and Texture GAN (CFT-GAN), a GAN-based video generation method from action-appearance captions. We propose a novel way of generating video by encoding a caption (e.g., ""a man in blue jeans is playing golf"") in a two-stage generation pipeline. Our CFT-GAN uses such caption to generate an optical flow (action) and a texture (appearance) for each frame. As a result, the output video reflects the content specified in the caption in a plausible way. Moreover, to train our method, we constructed a new dataset for human video generation with captions. We evaluated the proposed method qualitatively and quantitatively via an ablation study and a user study. The results demonstrate that CFT-GAN is able to successfully generate videos containing the action and appearances indicated in the captions.",0
"This abstract describes research on using action-appearance captions as input to generate video frames that capture the essence of actions depicted within those captions. The authors present a deep learning system which generates high quality synthetic videos by utilizing a combination of audio-visual embedding spaces. They experimented with different architectures such as convolutional neural networks (CNN), recurrent neural networks (RNN) and Transformers, as well as various loss functions. Results show that their proposed model outperforms state of the art systems at generating coherent and plausible video frames given textual descriptions. Overall, the work has potential applications in fields like virtual reality and computer graphics. However further testing and evaluation is required before real world implementation can take place.",1
"Federated learning, i.e., a mobile edge computing framework for deep learning, is a recent advance in privacy-preserving machine learning, where the model is trained in a decentralized manner by the clients, i.e., data curators, preventing the server from directly accessing those private data from the clients. This learning mechanism significantly challenges the attack from the server side. Although the state-of-the-art attacking techniques that incorporated the advance of Generative adversarial networks (GANs) could construct class representatives of the global data distribution among all clients, it is still challenging to distinguishably attack a specific client (i.e., user-level privacy leakage), which is a stronger privacy threat to precisely recover the private data from a specific client. This paper gives the first attempt to explore user-level privacy leakage against the federated learning by the attack from a malicious server. We propose a framework incorporating GAN with a multi-task discriminator, which simultaneously discriminates category, reality, and client identity of input samples. The novel discrimination on client identity enables the generator to recover user specified private data. Unlike existing works that tend to interfere the training process of the federated learning, the proposed method works ""invisibly"" on the server side. The experimental results demonstrate the effectiveness of the proposed attacking approach and the superior to the state-of-the-art.",0
Federated learning enables machine learning models to train on data that remains on usersâ€™ devices instead of collecting sensitive information. But is federated learning truly private? We show evidence that user level privacy leakage can occur even if we only release model updates as opposed to raw datasets.,1
"Medical students and radiology trainees typically view thousands of images in order to ""train their eye"" to detect the subtle visual patterns necessary for diagnosis. Nevertheless, infrastructural and legal constraints often make it difficult to access and quickly query an abundance of images with a user-specified feature set. In this paper, we use a conditional generative adversarial network (GAN) to synthesize $1024\times1024$ pixel pelvic radiographs that can be queried with conditioning on fracture status. We demonstrate that the conditional GAN learns features that distinguish fractures from non-fractures by training a convolutional neural network exclusively on images sampled from the GAN and achieving an AUC of $0.95$ on a held-out set of real images. We conduct additional analysis of the images sampled from the GAN and describe ongoing work to validate educational efficacy.",0
"Radiology education has traditionally relied on static imaging examples and textbook descriptions to teach students how to interpret medical images. However, these methods have limitations in terms of their ability to cover all possible scenarios that may be encountered in clinical practice, and they may become outdated over time as technology advances. In this paper, we propose using Generative Adversarial Networks (GANs) as a novel approach to enhancing radiology education. GANs are deep learning algorithms designed to generate realistic synthetic data that can be used for training machine learning models. We demonstrate how GANs can be used to create virtual patient cases that mimic real medical conditions found on diagnostic scans such as CT and MRI. These virtual patients can be generated at scale and customized to suit specific educational needs, allowing students to gain experience interpreting a diverse range of abnormalities without the need for physical access to large numbers of actual patients. Our results show that students trained on our system perform better than those taught with traditional methods alone. By leveraging modern computer graphics techniques alongside cutting edge artificial intelligence technology, we hope to revolutionize how radiologists learn and improve the quality of care provided by future practitioners of this critical field.",1
"The advance of Generative Adversarial Networks (GANs) enables realistic face image synthesis. However, synthesizing face images that preserve facial identity as well as have high diversity within each identity remains challenging. To address this problem, we present FaceFeat-GAN, a novel generative model that improves both image quality and diversity by using two stages. Unlike existing single-stage models that map random noise to image directly, our two-stage synthesis includes the first stage of diverse feature generation and the second stage of feature-to-image rendering. The competitions between generators and discriminators are carefully designed in both stages with different objective functions. Specially, in the first stage, they compete in the feature domain to synthesize various facial features rather than images. In the second stage, they compete in the image domain to render photo-realistic images that contain high diversity but preserve identity. Extensive experiments show that FaceFeat-GAN generates images that not only retain identity information but also have high diversity and quality, significantly outperforming previous methods.",0
"In recent years, there has been significant interest in developing methods for generating realistic faces using deep learning techniques. One important application area for such technology is synthesizing new facial images that preserve the identity and appearance of a given individual. To address this problem, we propose a two-stage approach based on generative adversarial networks (GANs) that generates high-quality face images while preserving key features related to identity, such as gaze direction and head pose. Our method, called FaceFeat-GAN, first uses a discriminator network to learn a feature representation that encodes identity-related information from a training set of face images. This learned representation is then used to guide the second stage of generation, where a generator network produces new face images. We evaluate our method on several benchmark datasets and demonstrate that it outperforms state-of-the-art approaches for identity-preserving face synthesis. Overall, our work shows promise for advancing the field of computer vision through the use of GANs for image generation tasks.",1
"Image inpainting is the task of filling-in missing regions of a damaged or incomplete image. In this work we tackle this problem not only by using the available visual data but also by incorporating image semantics through the use of generative models. Our contribution is twofold: First, we learn a data latent space by training an improved version of the Wasserstein generative adversarial network, for which we incorporate a new generator and discriminator architecture. Second, the learned semantic information is combined with a new optimization loss for inpainting whose minimization infers the missing content conditioned by the available data. It takes into account powerful contextual and perceptual content inherent in the image itself. The benefits include the ability to recover large regions by accumulating semantic information even it is not fully present in the damaged image. Experiments show that the presented method obtains qualitative and quantitative top-tier results in different experimental situations and also achieves accurate photo-realism comparable to state-of-the-art works.",0
"This paper presents a new method for semantic image inpainting using improved Wasserstein generative adversarial networks (WGAN). In traditional approaches to image inpainting, missing data is filled in based on statistical properties of local neighborhoods. However, these methods often produce results that lack semantically meaningful content and can result in blurring or distortion of important features. Our proposed approach uses WGANs to generate realistic patches that can be seamlessly integrated into the original image. We improve upon previous work by incorporating semantic information into our loss function and training process, resulting in more accurate and consistent patch generation. Experimental evaluation shows that our method outperforms state-of-the-art techniques, achieving better visual quality and preservation of image structures. Overall, we demonstrate the effectiveness of our improved WGAN framework for high-quality semantic image inpainting.",1
"A new method is proposed for removing text from natural images. The challenge is to first accurately localize text on the stroke-level and then replace it with a visually plausible background. Unlike previous methods that require image patches to erase scene text, our method, namely ensconce network (EnsNet), can operate end-to-end on a single image without any prior knowledge. The overall structure is an end-to-end trainable FCN-ResNet-18 network with a conditional generative adversarial network (cGAN). The feature of the former is first enhanced by a novel lateral connection structure and then refined by four carefully designed losses: multiscale regression loss and content loss, which capture the global discrepancy of different level features; texture loss and total variation loss, which primarily target filling the text region and preserving the reality of the background. The latter is a novel local-sensitive GAN, which attentively assesses the local consistency of the text erased regions. Both qualitative and quantitative sensitivity experiments on synthetic images and the ICDAR 2013 dataset demonstrate that each component of the EnsNet is essential to achieve a good performance. Moreover, our EnsNet can significantly outperform previous state-of-the-art methods in terms of all metrics. In addition, a qualitative experiment conducted on the SMBNet dataset further demonstrates that the proposed method can also preform well on general object (such as pedestrians) removal tasks. EnsNet is extremely fast, which can preform at 333 fps on an i5-8600 CPU device.",0
"This paper presents a new generative model for text generation based on the task of scene ensembling, which combines multiple images into one coherent image that captures commonalities across them. We propose a novel model called EnsNet that uses pretrained object detection models as the backbone and generates descriptions by reasoning over objects detected within scenes. Our contributions can be summarized as follows: Firstly, we introduce scene ensembling for text generation tasks and showcase its effectiveness through qualitative examples. Secondly, we present EnsNet and demonstrate how it achieves competitive results against strong baselines on two benchmark datasets for both description generation and question answering. Lastly, we conduct analyses to investigate why our approach works and discuss potential future directions. Overall, our work highlights the importance of detecting and incorporating objects in natural language processing tasks.",1
"Deep learning has brought an unprecedented progress in computer vision and significant advances have been made in predicting subjective properties inherent to visual data (e.g., memorability, aesthetic quality, evoked emotions, etc.). Recently, some research works have even proposed deep learning approaches to modify images such as to appropriately alter these properties. Following this research line, this paper introduces a novel deep learning framework for synthesizing images in order to enhance a predefined perceptual attribute. Our approach takes as input a natural image and exploits recent models for deep style transfer and generative adversarial networks to change its style in order to modify a specific high-level attribute. Differently from previous works focusing on enhancing a specific property of a visual content, we propose a general framework and demonstrate its effectiveness in two use cases, i.e. increasing image memorability and generating scary pictures. We evaluate the proposed approach on publicly available benchmarks, demonstrating its advantages over state of the art methods.",0
"Title: Enhancing Perceptions through Generative Models  In recent years, generative models have shown great potential in enhancing perceptions across multiple domains. This paper proposes a new methodology that utilizes Bayesian style generation techniques to enhance the perceptual attributes of images and videos. By combining deep learning algorithms with statistical inference techniques, we demonstrate how generating stylized versions of input media can improve various perceptual aspects such as visual realism, expressiveness, and aesthetic appeal.  The proposed approach involves training generative models on large datasets of diverse content to learn the underlying relationships between different features and attributes. We then use these models to generate novel variations of inputs by manipulating specific styles or characteristics while preserving their core essence. Our experiments showcase the effectiveness of our method in enhancing various types of media, including natural scenes, portraits, artwork, and even moving footage.  We evaluate the performance of our system using both quantitative metrics and subjective user studies. Our results consistently outperform existing state-of-the-art methods, demonstrating significant improvements in perceived visual quality, coherency, and overall impact. Furthermore, we provide insights into how our framework enables users to interactively steer the generation process towards desired directions, allowing for more creativity and control over the output.  Overall, this work opens up exciting opportunities for enriching human experiences by leveraging advanced artificial intelligence technologies. With the growing importance of visual media in communication and expression, our research paves the way for innovations in fields ranging from entertainment and design to education and scientific exploration.",1
"The model reduction problem that eases the computation costs and latency of complex deep learning architectures has received an increasing number of investigations owing to its importance in model deployment. One promising method is knowledge distillation (KD), which creates a fast-to-execute student model to mimic a large teacher network. In this paper, we propose a method, called KDFM (Knowledge Distillation with Feature Maps), which improves the effectiveness of KD by learning the feature maps from the teacher network. Two major techniques used in KDFM are shared classifier and generative adversarial network. Experimental results show that KDFM can use a four layers CNN to mimic DenseNet-40 and use MobileNet to mimic DenseNet-100. Both student networks have less than 1\% accuracy loss comparing to their teacher models for CIFAR-100 datasets. The student networks are 2-6 times faster than their teacher models for inference, and the model size of MobileNet is less than half of DenseNet-100's.",0
"Abstract: This paper presents a novel method for knowledge distillation using feature maps for image classification tasks. We demonstrate that transferring knowledge from large models trained on rich datasets can effectively improve performance on smaller models operating on more limited data sets by leveraging feature mapping techniques. Our approach uses pre-trained model embeddings as a source of supervisory signals, distilling their high level conceptual representations into lower capacity student models through elementwise modulation over intermediate activations during training. Extensive experimentation across multiple benchmarks highlights the effectiveness of our technique for enhancing generalization performance under challenging settings such as limited label availability, domain shift, and small-scale fine-grained object recognition. Overall, our work provides valuable insights towards facilitating real world deployment of deep learning models in resource constrained environments where access to big data might be restricted. Keywords: knowledge distillation; feature maps; image classification; resource constraints",1
"Segmentation of magnetic resonance (MR) images is a fundamental step in many medical imaging-based applications. The recent implementation of deep convolutional neural networks (CNNs) in image processing has been shown to have significant impacts on medical image segmentation. Network training of segmentation CNNs typically requires images and paired annotation data representing pixel-wise tissue labels referred to as masks. However, the supervised training of highly efficient CNNs with deeper structure and more network parameters requires a large number of training images and paired tissue masks. Thus, there is great need to develop a generalized CNN-based segmentation method which would be applicable for a wide variety of MR image datasets with different tissue contrasts. The purpose of this study was to develop and evaluate a generalized CNN-based method for fully-automated segmentation of different MR image datasets using a single set of annotated training data. A technique called cycle-consistent generative adversarial network (CycleGAN) is applied as the core of the proposed method to perform image-to-image translation between MR image datasets with different tissue contrasts. A joint segmentation network is incorporated into the adversarial network to obtain additional segmentation functionality. The proposed method was evaluated for segmenting bone and cartilage on two clinical knee MR image datasets acquired at our institution using only a single set of annotated data from a publicly available knee MR image dataset. The new technique may further improve the applicability and efficiency of CNN-based segmentation of medical images while eliminating the need for large amounts of annotated training data.",0
"Title: Image Structure Segmentation Using Adversarial Networks  The segmentation of images remains a challenging task due to variations in lighting conditions, object pose, and complex backgrounds. This paper proposes SUSAN (Segment Unannotated image Structure using Adversarial Network), a novel framework that leverages adversarial networks to accurately segment images without any manual annotations. Our approach effectively combines two different types of neural networks - generative adversarial network (GAN) and encoder-decoder convolutional neural network (CNN).  In our proposed method, we first generate synthetic data by applying random transformations such as rotation, scale changes, color jittering on the original image dataset. We then train GAN models that learn the distribution of real images from these synthetic data and provide discriminator predictions for real and generated samples. These discriminator outputs act as guidance signals for training CNNs to extract more meaningful features from the input image. The learned feature representations enable effective image structure segmentation. To evaluate the performance of our model, we conduct experiments on four benchmark datasets comprising diverse categories like natural scenes, objects, and medical imaging. Experimental results demonstrate significantly better segmentation accuracy compared to other state-of-the-art methods, even those that utilize annotated datasets during training.  Our work shows promise for unsupervised image structure segmentation where no labeled data is available. By generating and exploiting artificially constructed examples, SUSAN can enhance the learning process, leading to improved segmentation outcomes. With future advancements in deep learning architectures, our methodology holds great potential for wider applications across various domains.",1
"Rare diseases affect a relatively small number of people, which limits investment in research for treatments and cures. Developing an efficient method for rare disease detection is a crucial first step towards subsequent clinical research. In this paper, we present a semi-supervised learning framework for rare disease detection using generative adversarial networks. Our method takes advantage of the large amount of unlabeled data for disease detection and achieves the best results in terms of precision-recall score compared to baseline techniques.",0
"Semi-Supervised Learning (SSL) has recently emerged as a promising approach for dealing with limited annotated data. SSL allows us to train machine learning models using both labeled and unlabeled data, leveraging large amounts of unlabeled data to improve model performance on complex tasks such as image classification. In this work we explore the use of SSL for detecting rare diseases from electronic health records (EHRs). We specifically focus on applying generative adversarial networks (GANs), a popular deep learning architecture, for semi-supervised disease detection. GANs have previously been used successfully for generating realistic synthetic patient EHRs that can augment existing datasets. However, their application for semantic tasks such as disease detection remains under explored. Here, we investigate whether GAN-based SSL methods can effectively identify patients with rare diseases in EHR collections where only a few positive cases are available. Results show significantly improved performance compared to supervised baselines on multiple benchmark datasets, demonstrating that SSL can indeed provide valuable insights for assisting in the diagnosis of rare conditions.",1
"This paper addresses the mode collapse for generative adversarial networks (GANs). We view modes as a geometric structure of data distribution in a metric space. Under this geometric lens, we embed subsamples of the dataset from an arbitrary metric space into the l2 space, while preserving their pairwise distance distribution. Not only does this metric embedding determine the dimensionality of the latent space automatically, it also enables us to construct a mixture of Gaussians to draw latent space random vectors. We use the Gaussian mixture model in tandem with a simple augmentation of the objective function to train GANs. Every major step of our method is supported by theoretical analysis, and our experiments on real and synthetic data confirm that the generator is able to produce samples spreading over most of the modes while avoiding unwanted samples, outperforming several recent GAN variants on a number of metrics and offering new features.",0
"""In this work we propose the use of generative networks that incorporate metric embeddings as part of their training data. Specifically, our method called BourGAN combines the strengths of classical GAN techniques with these embedding spaces, which represent the underlying structure of the images. In practice, these embeddings allow us to improve both the quality and diversity of the generated samples by enforcing certain constraints during training. We demonstrate the effectiveness of our approach on several benchmark datasets and compare against other state-of-the-art methods, achieving favorable results across all metrics.""",1
"Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack. In this paper, we propose unrestricted adversarial examples, a new threat model where the attackers are not restricted to small norm-bounded perturbations. Different from perturbation-based attacks, we propose to synthesize unrestricted adversarial examples entirely from scratch using conditional generative models. Specifically, we first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier. We demonstrate through human evaluation that unrestricted adversarial examples generated this way are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that unrestricted adversarial examples can bypass strong adversarial training and certified defense methods designed for traditional adversarial attacks.",0
"This paper proposes using generative models such as GANs to generate adversarial examples, which can then be used to evaluate the robustness of machine learning systems. We focus on constructing unrestricted adversarial examples that are generated without any constraints except for the size of the input perturbation and demonstrate their effectiveness at deceiving even state-of-the-art defenses. Our approach achieves competitive results against other methods while providing significant flexibility by not requiring access to model internals or multiple queries. In addition, we investigate different ways of evaluating the performance of these attacks and discuss open challenges in designing effective evaluation metrics that balance realism and detectability. Our work provides new insights into the behavior of machine learning algorithms under attack, highlighting areas where they are vulnerable and how to effectively test them.",1
"First-person (egocentric) and third person (exocentric) videos are drastically different in nature. The relationship between these two views have been studied in recent years, however, it has yet to be fully explored. In this work, we introduce two datasets (synthetic and natural/real) containing simultaneously recorded egocentric and exocentric videos. We also explore relating the two domains (egocentric and exocentric) in two aspects. First, we synthesize images in the egocentric domain from the exocentric domain using a conditional generative adversarial network (cGAN). We show that with enough training data, our network is capable of hallucinating how the world would look like from an egocentric perspective, given an exocentric video. Second, we address the cross-view retrieval problem across the two views. Given an egocentric query frame (or its momentary optical flow), we retrieve its corresponding exocentric frame (or optical flow) from a gallery set. We show that using synthetic data could be beneficial in retrieving real data. We show that performing domain adaptation from the synthetic domain to the natural/real domain, is helpful in tasks such as retrieval. We believe that the presented datasets and the proposed baselines offer new opportunities for further research in this direction. The code and dataset are publicly available.",0
"This paper addresses an important issue in natural language processing (NLP) researchâ€”the lack of datasets available for studying synthesis tasks such as translation, summarization, question answering, text generation and retrieval. To address this gap, we introduce new benchmarks for four different NLP tasks that combine the strengths of third person evaluation metrics like BLEU, METEOR, ROUGE, etc., and first person evaluation metrics like human judgments. We use these benchmarks to evaluate existing models on diverse datasets across multiple languages and domains. Additionally, our baseline results show that some well known systems perform significantly better than others when evaluated using certain combinations of objectives. Our work provides a platform for future researchers to build upon, ultimately leading to improved performance on important real world applications.",1
"Lack of annotated samples greatly restrains the direct application of deep learning in remote sensing image scene classification. Although researches have been done to tackle this issue by data augmentation with various image transformation operations, they are still limited in quantity and diversity. Recently, the advent of the unsupervised learning based generative adversarial networks (GANs) bring us a new way to generate augmented samples. However, such GAN-generated samples are currently only served for training GANs model itself and for improving the performance of the discriminator in GANs internally (in vivo). It becomes a question of serious doubt whether the GAN-generated samples can help better improve the scene classification performance of other deep learning networks (in vitro), compared with the widely used transformed samples. To answer this question, this paper proposes a SiftingGAN approach to generate more numerous, more diverse and more authentic labeled samples for data augmentation. SiftingGAN extends traditional GAN framework with an Online-Output method for sample generation, a Generative-Model-Sifting method for model sifting, and a Labeled-Sample-Discriminating method for sample sifting. Experiments on the well-known AID dataset demonstrate that the proposed SiftingGAN method can not only effectively improve the performance of the scene classification baseline that is achieved without data augmentation, but also significantly excels the comparison methods based on traditional geometric/radiometric transformation operations.",0
"""Scene classification plays a crucial role in remote sensing image analysis by providing contextual understanding of Earth's features such as land cover types and urban infrastructure. In recent years, deep learning approaches have achieved remarkable results in remote sensing scene classification tasks through convolutional neural network architectures like U-Net. However, these models often require large amounts of labeled training data which can be difficult to acquire due to costly manual annotation processes. In this study, we propose a novel approach called Sifting GAN (Generative Adversarial Network) that generates synthetic labeled samples to supplement existing datasets and improve baseline performance without requiring additional human labeling effort. Our approach involves fine-tuning a pre-trained generator on real images using adversarial training against a discriminator network trained explicitly on objectives relevant to image generation quality and coherency at multiple scales. We then apply these generated images as augmented training sets alongside manually annotated ones in downstream scene classification tasks. Experimental evaluation shows consistent improvements over baselines across three public benchmark datasets including Sentinel2 Amazon dataset, ROSIS dataset and GeoImageNet dataset. These results demonstrate the potential benefits of using generative methods to address the issue of limited availability of high quality annotations and suggest new opportunities for scaling up remote sensing data management practices.""",1
"Mobility datasets are fundamental for evaluating algorithms pertaining to geographic information systems and facilitating experimental reproducibility. But privacy implications restrict sharing such datasets, as even aggregated location-data is vulnerable to membership inference attacks. Current synthetic mobility dataset generators attempt to superficially match a priori modeled mobility characteristics which do not accurately reflect the real-world characteristics. Modeling human mobility to generate synthetic yet semantically and statistically realistic trajectories is therefore crucial for publishing trajectory datasets having satisfactory utility level while preserving user privacy. Specifically, long-range dependencies inherent to human mobility are challenging to capture with both discriminative and generative models. In this paper, we benchmark the performance of recurrent neural architectures (RNNs), generative adversarial networks (GANs) and nonparametric copulas to generate synthetic mobility traces. We evaluate the generated trajectories with respect to their geographic and semantic similarity, circadian rhythms, long-range dependencies, training and generation time. We also include two sample tests to assess statistical similarity between the observed and simulated distributions, and we analyze the privacy tradeoffs with respect to membership inference and location-sequence attacks.",0
"This study aimed to investigate how generative models could be used to simulate mobility trajectories, including where and when people move, as well as their destinations and modes of transportation. To achieve this goal, we first reviewed existing literature on mobility modeling and identified key factors that influence travel behavior. We then developed several generative models using machine learning techniques and validated them against real data sets. Our results showed that these models can accurately predict human movement patterns across different environments and time periods. In conclusion, our findings suggest that generative models have great potential for simulating mobility trajectories and improving transportation planning and management strategies. Further research is necessary to fully utilize this technology and unlock new insights into human mobility patterns. Overall, this work represents a significant step towards creating more efficient and sustainable transportation systems worldwide.",1
"Unsupervised domain adaption aims to learn a powerful classifier for the target domain given a labeled source data set and an unlabeled target data set. To alleviate the effect of `domain shift', the major challenge in domain adaptation, studies have attempted to align the distributions of the two domains. Recent research has suggested that generative adversarial network (GAN) has the capability of implicitly capturing data distribution. In this paper, we thus propose a simple but effective model for unsupervised domain adaption leveraging adversarial learning. The same encoder is shared between the source and target domains which is expected to extract domain-invariant representations with the help of an adversarial discriminator. With the labeled source data, we introduce the center loss to increase the discriminative power of feature learned. We further align the conditional distribution of the two domains to enforce the discrimination of the features in the target domain. Unlike previous studies where the source features are extracted with a fixed pre-trained encoder, our method jointly learns feature representations of two domains. Moreover, by sharing the encoder, the model does not need to know the source of images during testing and hence is more widely applicable. We evaluate the proposed method on several unsupervised domain adaption benchmarks and achieve superior or comparable performance to state-of-the-art results.",0
"In order to adapt machine learning models to new domains with minimal labeled data, adversarial domain adaptation techniques have emerged as effective solutions. These methods align feature distributions from source and target domains using discriminators that try to distinguish between features generated by the model versus real features from the target distribution. Although these approaches achieve good results under unsupervised setting, they still struggle with generating high quality features even when multiple labels are available in the target domain, resulting in subpar downstream performance compared to supervised learning counterparts. We propose to leverage multi-task deep neural networks to improve domain alignment while addressing limitations associated with previous adversarial frameworks. Our framework introduces label reconstruction loss which allows us to optimize both aligned representations and main task predictions simultaneously. By enforcing constraints on both domain alignment and joint prediction objectives, we demonstrate significant improvements over state-of-the-art methods across several benchmark datasets.",1
"The goal of data selection is to capture the most structural information from a set of data. This paper presents a fast and accurate data selection method, in which the selected samples are optimized to span the subspace of all data. We propose a new selection algorithm, referred to as iterative projection and matching (IPM), with linear complexity w.r.t. the number of data, and without any parameter to be tuned. In our algorithm, at each iteration, the maximum information from the structure of the data is captured by one selected sample, and the captured information is neglected in the next iterations by projection on the null-space of previously selected samples. The computational efficiency and the selection accuracy of our proposed algorithm outperform those of the conventional methods. Furthermore, the superiority of the proposed algorithm is shown on active learning for video action recognition dataset on UCF-101; learning using representatives on ImageNet; training a generative adversarial network (GAN) to generate multi-view images from a single-view input on CMU Multi-PIE dataset; and video summarization on UTE Egocentric dataset.",0
"In recent years, there has been significant progress in developing computational methods for discovering complex structure in large datasets. One approach that has gained popularity is iterative projection and matching (IPM), which involves finding low-dimensional representations of data points that preserve their geometric relationships. This paper presents an overview of IPM and explores how it can be applied to computer vision problems. We first introduce the key principles behind IPM, including the use of optimization techniques to find structure-preserving representatives of high-dimensional data. Next, we describe several applications of IPM in computer vision, including image recognition and object tracking. Finally, we discuss future directions for research in this area, highlighting potential opportunities for further development of the method and its applications. Overall, our aim is to provide readers with a detailed understanding of IPM and its potential uses in computer vision research.",1
"Generative Adversarial Networks have surprising ability for generating sharp and realistic images, though they are known to suffer from the so-called mode collapse problem. In this paper, we propose a new GAN variant called Mixture Density GAN that while being capable of generating high-quality images, overcomes this problem by encouraging the Discriminator to form clusters in its embedding space, which in turn leads the Generator to exploit these and discover different modes in the data. This is achieved by positioning Gaussian density functions in the corners of a simplex, using the resulting Gaussian mixture as a likelihood function over discriminator embeddings, and formulating an objective function for GAN training that is based on these likelihoods. We demonstrate empirically (1) the quality of the generated images in Mixture Density GAN and their strong similarity to real images, as measured by the Fr\'echet Inception Distance (FID), which compares very favourably with state-of-the-art methods, and (2) the ability to avoid mode collapse and discover all data modes.",0
"Here is an alternative model: GAN architectures have recently been used to generate high quality images from natural language descriptions, but these methods struggle with generating realistic details due to their limited representational capacity and ability to hallucinate features. In contrast, our proposed method uses mixture density generators, which have the advantage of being able to capture uncertainty and diversity of data distributions. We show that our approach outperforms previous models on several benchmark datasets and produces more coherent and diverse outputs. Additionally, we demonstrate the effectiveness of adversarial training by comparing results with and without adversarial loss. Finally, we provide qualitative examples to support our claims. By leveraging both generative and discriminative components, MDGAN offers a new direction towards efficient image generation.",1
"Conditional image generation is effective for diverse tasks including training data synthesis for learning-based computer vision. However, despite the recent advances in generative adversarial networks (GANs), it is still a challenging task to generate images with detailed conditioning on object shapes. Existing methods for conditional image generation use category labels and/or keypoints and are only give limited control over object categories. In this work, we present SCGAN, an architecture to generate images with a desired shape specified by an input normal map. The shape-conditioned image generation task is achieved by explicitly modeling the image appearance via a latent appearance vector. The network is trained using unpaired training samples of real images and rendered normal maps. This approach enables us to generate images of arbitrary object categories with the target shape and diverse image appearances. We show the effectiveness of our method through both qualitative and quantitative evaluation on training data generation tasks.",0
"This paper focuses on the problem of shape-conditioned image generation using unpaired data. We propose a novel framework that can learn latent representations of appearance directly from raw pixel values of images without any explicit correspondence or supervision. By doing so, we aim to overcome some limitations of existing methods based on reconstruction errors or cycle consistency losses. Our approach relies on two key components: (i) learning a latent shape space that encodes information related to object shapes, and (ii) training a generator network to predict missing features given partial observations. Experiments show that our method can produce high-quality results across different datasets and outperforms previous approaches. In conclusion, our work opens up new possibilities for shape-conditional synthesis of complex visual content using unpaired data.",1
"Person re-identification is to retrieval pedestrian images from no-overlap camera views detected by pedestrian detectors. Most existing person re-identification (re-ID) models often fail to generalize well from the source domain where the models are trained to a new target domain without labels, because of the bias between the source and target domain. This issue significantly limits the scalability and usability of the models in the real world. Providing a labeled source training set and an unlabeled target training set, the aim of this paper is to improve the generalization ability of re-ID models to the target domain. To this end, we propose an image generative network named identity preserving generative adversarial network (IPGAN). The proposed method has two excellent properties: 1) only a single model is employed to translate the labeled images from the source domain to the target camera domains in an unsupervised manner; 2) The identity information of images from the source domain is preserved before and after translation. Furthermore, we propose IBN-reID model for the person re-identification task. It has better generalization ability than baseline models, especially in the cases without any domain adaptation. The IBN-reID model is trained on the translated images by supervised methods. Experimental results on Market-1501 and DukeMTMC-reID show that the images generated by IPGAN are more suitable for cross-domain person re-identification. Very competitive re-ID accuracy is achieved by our method.",0
"This paper proposes an identity preserving generative adversarial network (IPGAN) for cross-domain person re-identification. Existing methods face challenges such as differences in illumination, background clutter, pose variation, image quality, and resolution among different cameras. IPGAN addresses these issues by synthesizing new images from one domain that can fool a discriminator trained on another domain. Our approach incorporates cycle consistency loss into the GAN framework, which ensures that the generated images capture the intrinsic structure of input images before transferring them across domains. Extensive experiments validate the effectiveness of our method compared with state-of-the-art approaches. Our code and models will be made publicly available upon publication.",1
"Recently, Generative Adversarial Network (GAN) has been found wide applications in style transfer, image-to-image translation and image super-resolution. In this paper, a color-depth conditional GAN is proposed to concurrently resolve the problems of depth super-resolution and color super-resolution in 3D videos. Firstly, given the low-resolution depth image and low-resolution color image, a generative network is proposed to leverage mutual information of color image and depth image to enhance each other in consideration of the geometry structural dependency of color-depth image in the same scene. Secondly, three loss functions, including data loss, total variation loss, and 8-connected gradient difference loss are introduced to train this generative network in order to keep generated images close to the real ones, in addition to the adversarial loss. Experimental results demonstrate that the proposed approach produces high-quality color image and depth image from low-quality image pair, and it is superior to several other leading methods. Besides, we use the same neural network framework to resolve the problem of image smoothing and edge detection at the same time.",0
"This paper presents a method for simultaneously achieving color-depth super-resolution by training a conditional generative adversarial network (cGAN). Our approach uses a combination of image prior and edge priors to improve the quality of high resolution images created from low resolution inputs. We compare our results against other state-of-the-art methods on several benchmark datasets and demonstrate that our method outperforms them in terms of both visual fidelity and quantitative evaluation metrics such as PSNR and SSIM. Our work represents an important step forward in the field of single image super-resolution, and has potential applications in areas such as computer vision, video compression, and virtual reality.",1
"Deep learning has significant potential for medical imaging. However, since the incident rate of each disease varies widely, the frequency of classes in a medical image dataset is imbalanced, leading to poor accuracy for such infrequent classes. One possible solution is data augmentation of infrequent classes using synthesized images created by Generative Adversarial Networks (GANs), but conventional GANs also require certain amount of images to learn. To overcome this limitation, here we propose General-to-detailed GAN (GDGAN), serially connected two GANs, one for general labels and the other for detailed labels. GDGAN produced diverse medical images, and the network trained with an augmented dataset outperformed other networks using existing methods with respect to Area-Under-Curve (AUC) of Receiver Operating Characteristic (ROC) curve.",0
"Artificial intelligence (AI) has made significant strides over recent years, particularly as machine learning algorithms have improved considerably due to large amounts of data that computers can utilize today; however, challenges remain on how to integrate this technology into real world applications. One crucial application area for artificial intelligence is medical image analysis where high accuracy is necessary as errors could lead to severe consequences. Infrequent class images pose unique difficulties because few labeled examples may exist, making training datasets difficult to obtain. Consequently, traditional generative models like Generative Adversarial Networks (GAN), which rely heavily on such datasets, are unsuitable. To address these issues, we propose the general to detailed GAN approach. Firstly, the method learns from common characteristics across domains before progressively focusing on more precise features. This allows our model to learn infrequently encountered classes while using fewer labeled samples. Secondly, by jointly training the generator and discriminator under objectives like cycle consistency, perceptual loss, and adversarial loss, our network gains better control over synthetic images and helps combat mode collapse thereby boosting performance further. Using experiments on MNIST, CelebA, and Tumor segmentation benchmarks, results indicate significant improvement compared to alternative methods - improving FID scores, reducing Dice similarity coefficient error rates on test sets, and performing favourably against SOTA methods including UNet++ and HRFNAIC. We conclude that our proposed method shows great promise in handling medically important tasks despite limited labelled training data. As future work, we hope to extend this research to even more complex problems with greater stakes where human safety might depend on accurate predictions",1
"This paper addresses the problem of manipulating images using natural language description. Our task aims to semantically modify visual attributes of an object in an image according to the text describing the new visual appearance. Although existing methods synthesize images having new attributes, they do not fully preserve text-irrelevant contents of the original image. In this paper, we propose the text-adaptive generative adversarial network (TAGAN) to generate semantically manipulated images while preserving text-irrelevant contents. The key to our method is the text-adaptive discriminator that creates word-level local discriminators according to input text to classify fine-grained attributes independently. With this discriminator, the generator learns to generate images where only regions that correspond to the given text are modified. Experimental results show that our method outperforms existing methods on CUB and Oxford-102 datasets, and our results were mostly preferred on a user study. Extensive analysis shows that our method is able to effectively disentangle visual attributes and produce pleasing outputs.",0
"In recent years, there has been growing interest in using deep learning techniques to generate images from natural language text descriptions. One such approach is based on adversarial training, which involves two networks competing against each other â€“ one trying to generate realistic looking images that fool a discriminator network into thinking theyâ€™re real, while the second tries to detect whether an image is generated by a computer algorithm or produced naturally (real). We propose a new variant of generative adversarial networks (GANs) called textual adaptive GANs (TAGAN), where we explicitly model the relationship between the text description and the resulting image generation process through an additional attention mechanism. This enables our method to dynamically focus on different parts of an input sentence during image generation, allowing us to create high quality images more quickly than traditional methods. Our experimental results show TAGAN outperforms state-of-the art image synthesis models across various evaluation metrics including visual fidelity, diversity, and perplexity measures. Additionally, TAGAN can be adapted to fit any existing text-to-image synthesis architecture and significantly improves their performance without modifying them. With its ability to manipulate the output images based on user requests, TAGAN presents significant potential applications in areas ranging from gaming, entertainment, social media and even as an intelligent design tool for architects, graphic artists etc.",1
"Generative adversarial networks (GANs) implicitly learn the probability distribution of a dataset and can draw samples from the distribution. This paper presents, Tabular GAN (TGAN), a generative adversarial network which can generate tabular data like medical or educational records. Using the power of deep neural networks, TGAN generates high-quality and fully synthetic tables while simultaneously generating discrete and continuous variables. When we evaluate our model on three datasets, we find that TGAN outperforms conventional statistical generative models in both capturing the correlation between columns and scaling up for large datasets.",0
"Abstract: Synthesizing tabular data has many applications across multiple fields. Traditional methods typically involve manual intervention or statistical modeling. However, these approaches have limitations such as high human effort required, lack of flexibility, or reliance on strong assumptions. In recent years, generative adversarial networks (GANs) have emerged as promising techniques capable of synthesizing complex structured data like images and videos. Motivated by their success, we investigate whether GANs can effectively synthesize tabular data. We propose a novel approach that models pairwise relations between cells in a table, and generate realistic tables through alternating minimization between a generator network and two discriminator networks. Our experiments on diverse datasets demonstrate that our method outperforms state-of-the art baselines in terms of fidelity and diversity. We further showcase how generated tables can enhance exploratory data analysis tasks, demonstrating significant improvement in user experience for tasks like missing value imputation and visual data analysis.  ------------------ This paper presents a new technique for generating tabular data using generative adversarial networks (GANs). While traditional methods rely heavily on human input or assumption-based statistical models, the proposed approach uses pairwise relationships between cell values to create more accurate and flexible results. Experiments conducted on various datasets showed improved accuracy over existing methods, making it particularly applicable for exploratory data analysis tasks where completeness of data plays an important role. Overall, the results highlight the potential of utilizing GANs in synthesizing large amounts of structured data, extending beyond image and video generation commonplace today.",1
"Generative Adversarial Networks (GANs) have proven to be a powerful framework for learning to draw samples from complex distributions. However, GANs are also notoriously difficult to train, with mode collapse and oscillations a common problem. We hypothesize that this is at least in part due to the evolution of the generator distribution and the catastrophic forgetting tendency of neural networks, which leads to the discriminator losing the ability to remember synthesized samples from previous instantiations of the generator. Recognizing this, our contributions are twofold. First, we show that GAN training makes for a more interesting and realistic benchmark for continual learning methods evaluation than some of the more canonical datasets. Second, we propose leveraging continual learning techniques to augment the discriminator, preserving its ability to recognize previous generator samples. We show that the resulting methods add only a light amount of computation, involve minimal changes to the model, and result in better overall performance on the examined image and text generation tasks.",0
"Abstract: This work explores the problem of training generative adversarial networks (GANs) as a continual learning problem. GANs have emerged as powerful models for generating realistic synthetic data, but their training can be challenging due to instability issues such as mode collapse, vanishing gradients, and unstable dynamics. We propose that these issues arise because GAN training involves solving a complex optimization problem that requires adaptive mechanisms for maintaining stability and efficiency over time. To address this challenge, we present a framework based on continuous adaptation, which enables GANs to learn incrementally while preserving important properties such as generator quality, discriminator accuracy, and diversity in generated samples. Our approach allows GANs to adapt smoothly to new tasks and environments, improving their robustness and generalization performance across a range of benchmark datasets. By treating GAN training as a continual learning problem, our method provides insights into how GANs can achieve better long-term stability and reliability during deployment.",1
"Colours are everywhere. They embody a significant part of human visual perception. In this paper, we explore the paradigm of hallucinating colours from a given gray-scale image. The problem of colourization has been dealt in previous literature but mostly in a supervised manner involving user-interference. With the emergence of Deep Learning methods numerous tasks related to computer vision and pattern recognition have been automatized and carried in an end-to-end fashion due to the availability of large data-sets and high-power computing systems. We investigate and build upon the recent success of Conditional Generative Adversarial Networks (cGANs) for Image-to-Image translations. In addition to using the training scheme in the basic cGAN, we propose an encoder-decoder generator network which utilizes the class-specific cross-entropy loss as well as the perceptual loss in addition to the original objective function of cGAN. We train our model on a large-scale dataset and present illustrative qualitative and quantitative analysis of our results. Our results vividly display the versatility and proficiency of our methods through life-like colourization outcomes.",0
"Recent advances in image colourization have focused on end-to-end deep learning methods that directly map grayscale images to colorized versions without relying on hand-engineered features. However, these approaches often struggle to generate high-quality results due to their limited ability to capture perceptually meaningful relationships between input images and corresponding ground truth colors. In this work, we propose a novel generative model called Perceptual Conditional Generative Adversarial Network (PCGAN) which overcomes this limitation by explicitly modeling both the physical relationship between inputs and outputs as well as the perceptual similarity between generated samples and real data. Our approach consists of two main components: a shared feature encoder network that maps inputs and targets into a joint feature space and a PCGAN module that learns to predict disjoint sets of conditioning variables from the encoded features. These conditioning variables control different aspects of the generation process such as layout stability, edge coherency and color diversity. We evaluate our method on multiple benchmark datasets and demonstrate that PCGAN significantly outperforms state-of-the-art baselines across all evaluation metrics while producing visually pleasing and plausible colorizations. Overall, our framework represents a significant step towards improving the quality and efficacy of end-to-end image colourization systems.",1
"Unsupervised domain mapping aims to learn a function to translate domain X to Y by a function GXY in the absence of paired examples. Finding the optimal GXY without paired data is an ill-posed problem, so appropriate constraints are required to obtain reasonable solutions. One of the most prominent constraints is cycle consistency, which enforces the translated image by GXY to be translated back to the input image by an inverse mapping GYX. While cycle consistency requires the simultaneous training of GXY and GY X, recent studies have shown that one-sided domain mapping can be achieved by preserving pairwise distances between images. Although cycle consistency and distance preservation successfully constrain the solution space, they overlook the special properties that simple geometric transformations do not change the semantic structure of images. Based on this special property, we develop a geometry-consistent generative adversarial network (GcGAN), which enables one-sided unsupervised domain mapping. GcGAN takes the original image and its counterpart image transformed by a predefined geometric transformation as inputs and generates two images in the new domain coupled with the corresponding geometry-consistency constraint. The geometry-consistency constraint reduces the space of possible solutions while keep the correct solutions in the search space. Quantitative and qualitative comparisons with the baseline (GAN alone) and the state-of-the-art methods including CycleGAN and DistanceGAN demonstrate the effectiveness of our method.",0
"In recent years, generative adversarial networks (GANs) have emerged as powerful models for image synthesis, generation, and manipulation tasks on one side and domain adaptation/mapping on the other. However, existing GAN architectures typically assume that both domains share a similar underlying geometric structure which may not always hold true in practice; different modalities can possess varying degrees of geometric regularity in texture or topology. To address this limitation, we propose geometry consistent generative adversarial networks (GeoCGANs), a novel framework which incorporates domain-specific information via intrinsic images and leverages such knowledge during training. Our approach exploits intrinsic images by learning explicit representations through depth estimation before applying them as a self-supervisory loss term for better alignment between the two domains. Furthermore, we show that our method enables more efficient use of labeled data compared to traditional unpaired methods while maintaining competitive results in supervised settings. Extensive experiments across diverse datasets corroborate these findings and provide insight into how GeoCGANs generalize towards new environments. Overall, our work pushes forward state-of-the art within the field while enabling more expressive and structured mappings for real-world scenarios demanding one-to-one correspondence at test time.",1
"Multishot Magnetic Resonance Imaging (MRI) has recently gained popularity as it accelerates the MRI data acquisition process without compromising the quality of final MR image. However, it suffers from motion artifacts caused by patient movements which may lead to misdiagnosis. Modern state-of-the-art motion correction techniques are able to counter small degree motion, however, their adoption is hindered by their time complexity. This paper proposes a Generative Adversarial Network (GAN) for reconstructing motion free high-fidelity images while reducing the image reconstruction time by an impressive two orders of magnitude.",0
"Improving MRI image quality can aid medical diagnosis by enabling better visualization of internal organs and tissues. One major challenge in producing high-quality MRIs is motion correction during multishot acquisition, which introduces artifacts that degrade image quality. Existing methods rely on manual intervention and require expertise but can still leave room for improvement. We propose using generative adversarial networks (GAN) as an automated solution to correct motion artifacts in MRI images obtained from multishot sequences. Our approach has the potential to improve overall diagnostic accuracy while reducing human labor and errors caused by manual motion correction. In addition, our algorithm enables automatic generation of coherent, unified images from multiple shots acquired over different time intervals, without manual alignment. Our results show that GAN-based motion correction yields higher SNR and CNR values than traditional registration algorithms used in current clinical practice. The ability to generate realistically looking images without visible deformations makes our method particularly well suited for applications such as diffusion tensor imaging (DTI). Overall, our work demonstrates the feasibility of applying deep learning techniques to achieve accurate and efficient motion correction in MRI scans.",1
"In this paper, a novel strategy of Secure Steganograpy based on Generative Adversarial Networks is proposed to generate suitable and secure covers for steganography. The proposed architecture has one generative network, and two discriminative networks. The generative network mainly evaluates the visual quality of the generated images for steganography, and the discriminative networks are utilized to assess their suitableness for information hiding. Different from the existing work which adopts Deep Convolutional Generative Adversarial Networks, we utilize another form of generative adversarial networks. By using this new form of generative adversarial networks, significant improvements are made on the convergence speed, the training stability and the image quality. Furthermore, a sophisticated steganalysis network is reconstructed for the discriminative network, and the network can better evaluate the performance of the generated images. Numerous experiments are conducted on the publicly available datasets to demonstrate the effectiveness and robustness of the proposed method.",0
"Steganography, which literally means â€œcovered writingâ€, refers to the process by which information is hidden within another object that, at first glance, appears to have no relation to the message itself. With recent advances in deep learning techniques such as generative adversarial networks (GAN), steganographic methods have seen significant improvement in both capacity and security. In particular, GANs can effectively embed messages into images without significantly altering their appearance while making it difficult for detectors to distinguish the cover objects from natural ones. While existing methods are effective, they typically rely on handcraft features and lack the generalizability necessary for modern applications. This research addresses these limitations through the development and implementation of a novel approach to steganography based on GANs. By training two competing neural networks - one generator attempting to hide data inside media files, and one discriminator trying to find the embedded data - our method achieves state-of-the-art performance in terms of payload size, visual quality, and robustness against detection. We further demonstrate the applicability of our system in real scenarios by integrating with widely used image editing software like Photoshop. Through extensive experiments, we show that our SSGAN system outperforms traditional approaches in most settings and provides secure and reliable information hiding capability under various attacks, setting a new benchmark for future work in this field. Overall, this study presents a breakthrough in steganography and has wide-ranging implications for communications security, digital rights management, and more broadly, data integrity protection.",1
Generative adversarial networks (GANs) are designed with the help of min-max optimization problems that are solved with stochastic gradient-type algorithms which are known to be non-robust. In this work we revisit a non-adversarial method based on kernels which relies on a pure minimization problem and propose a simple stochastic gradient algorithm for the computation of its solution. Using simplified tools from Stochastic Approximation theory we demonstrate that batch versions of the algorithm or smoothing of the gradient do not improve convergence. These observations allow for the development of a training algorithm that enjoys reduced computational complexity and increased robustness while exhibiting similar synthesis characteristics as classical GANs.,0
"Recently, generative networks have achieved impressive results on several challenging tasks involving complex data distributions such as images, videos, sounds, and text. However, training these models remains challenging due to their sensitivity to hyperparameters and the difficulty in designing appropriate loss functions. In this work, we propose a novel approach based on kernels that addresses these issues. Our method uses kernel learning as a means to model nonlinear dependencies between inputs and outputs, allowing us to train generative networks more effectively and efficiently. We demonstrate through experiments on a variety of datasets that our method leads to improved performance over state-of-the-art methods while requiring fewer hyperparameter tunings. Overall, our contributions show promise towards advancing the field of generative network training.",1
"Despite the growing interest in generative adversarial networks (GANs), training GANs remains a challenging problem, both from a theoretical and a practical standpoint. To address this challenge, in this paper, we propose a novel way to exploit the unique geometry of the real data, especially the manifold information. More specifically, we design a method to regularize GAN training by adding an additional regularization term referred to as manifold regularizer. The manifold regularizer forces the generator to respect the unique geometry of the real data manifold and generate high quality data. Furthermore, we theoretically prove that the addition of this regularization term in any class of GANs including DCGAN and Wasserstein GAN leads to improved performance in terms of generalization, existence of equilibrium, and stability. Preliminary experiments show that the proposed manifold regularization helps in avoiding mode collapse and leads to stable training.",0
"Abstract: We present an image generation method that can synthesize high quality images from textual descriptions using a novel adversarial network architecture, manifold regularization, and progressive training. Unlike previous approaches which rely on optimization, our algorithm generates images directly through sampling and minimizes a well defined loss function. Our model outperforms other generative models on several benchmark datasets while being computationally efficient. This research has important implications for computer vision applications such as content creation, data augmentation, and virtual reality.",1
"We propose a new architecture and training methodology for generative adversarial networks. Current approaches attempt to learn the transformation from a noise sample to a generated data sample in one shot. Our proposed generator architecture, called $\textit{ChainGAN}$, uses a two-step process. It first attempts to transform a noise vector into a crude sample, similar to a traditional generator. Next, a chain of networks, called $\textit{editors}$, attempt to sequentially enhance this sample. We train each of these units independently, instead of with end-to-end backpropagation on the entire chain. Our model is robust, efficient, and flexible as we can apply it to various network architectures. We provide rationale for our choices and experimentally evaluate our model, achieving competitive results on several datasets.",0
"In recent years there have been major advancements made in the field of generative models such as Generative Adversarial Networks (GAN). However, the training of these models can often suffer from issues like mode collapse, where one specific solution dominates all others and poor generation quality at early stages of optimization. To address these issues, we propose using a new architecture called ChainGAN which uses sequence data to train the model. We show that our method improves both stability and quality over traditional approaches. Our results demonstrate state-of-the-art performance on several benchmark datasets commonly used in generative tasks like image synthesis and text completion. By leveraging the power of sequential data, our algorithm is able to achieve better performance without sacrificing efficiency. Overall, this work represents a significant step towards solving some of the most challenging problems facing modern day generative models.",1
"State-of-the-art deep learning algorithms yield remarkable results in many visual recognition tasks. However, they still fail to provide satisfactory results in scarce data regimes. To a certain extent this lack of data can be compensated by multimodal information. Missing information in one modality of a single data point (e.g. an image) can be made up for in another modality (e.g. a textual description). Therefore, we design a few-shot learning task that is multimodal during training (i.e. image and text) and single-modal during test time (i.e. image). In this regard, we propose a self-paced class-discriminative generative adversarial network incorporating multimodality in the context of few-shot learning. The proposed approach builds upon the idea of cross-modal data generation in order to alleviate the data sparsity problem. We improve few-shot learning accuracies on the finegrained CUB and Oxford-102 datasets.",0
"Self Paced Adversarial Training (SPAT) has been used successfully in computer vision tasks like image classification, object detection etc to achieve state of art results. However few shot learning tasks have remained challenging even after using SPAT due to limited labeled data available during training. To address this issue we propose Multi modal Self Paced Adversarial Training(MPAT). In addition to generating more diverse textual prompts via the use of a pretrained language model, our method can utilize both visual prompts as well. We show that on multiple benchmark datasets, MPAT significantly improves few shot performance over prior methods. Our approach achieves state of the art accuracy on miniImageNet dataset and close second best on tiered ImageNet task. Also, MPAT shows good performance on more complex datasets which involve multiple object recognition and dense attribute label prediction tasks. Overall, this work shows that multi modality combined with adversarial training can improve generalization of models across few shot learning scenarios.",1
"Despite the breakthroughs in quality of image enhancement, an end-to-end solution for simultaneous recovery of the finer texture details and sharpness for degraded images with low resolution is still unsolved. Some existing approaches focus on minimizing the pixel-wise reconstruction error which results in a high peak signal-to-noise ratio. The enhanced images fail to provide high-frequency details and are perceptually unsatisfying, i.e., they fail to match the quality expected in a photo-realistic image. In this paper, we present Image Enhancement Generative Adversarial Network (IEGAN), a versatile framework capable of inferring photo-realistic natural images for both artifact removal and super-resolution simultaneously. Moreover, we propose a new loss function consisting of a combination of reconstruction loss, feature loss and an edge loss counterpart. The feature loss helps to push the output image to the natural image manifold and the edge loss preserves the sharpness of the output image. The reconstruction loss provides low-level semantic information to the generator regarding the quality of the generated images compared to the original. Our approach has been experimentally proven to recover photo-realistic textures from heavily compressed low-resolution images on public benchmarks and our proposed high-resolution World100 dataset.",0
"This paper presents a new approach to image enhancement using generative adversarial networks (GANs). Our method, called IEGAN, can improve images from a wide range of sources, such as low quality web images, photos taken under challenging conditions, or even artificially generated images that suffer from noise or other issues. IEGAN combines two powerful GAN architectures in a novel way, allowing it to generate high resolution output while minimizing lossy compression artifacts. We show through experiments that our method outperforms state-of-the-art image enhancement algorithms by a significant margin on both subjective and objective measures. Overall, we believe that IEGAN has great potential applications in areas such as photo editing, computer vision, and medical imaging.",1
"We propose a new generative adversarial architecture to mitigate imbalance data problem for the task of medical image semantic segmentation where the majority of pixels belong to a healthy region and few belong to lesion or non-health region. A model trained with imbalanced data tends to bias towards healthy data which is not desired in clinical applications. We design a new conditional GAN with two components: a generative model and a discriminative model to mitigate imbalanced data problem through selective weighted loss. While the generator is trained on sequential magnetic resonance images (MRI) to learn semantic segmentation and disease classification, the discriminator classifies whether a generated output is real or fake. The proposed architecture achieved state-of-the-art results on ACDC-2017 for cardiac segmentation and diseases classification. We have achieved competitive results on BraTS-2017 for brain tumor segmentation and brain diseases classification.",0
"In recent years, there has been an increasing amount of clinical data available in electronic health records (EHRs). However, this data often suffers from class imbalance issues where some classes have significantly more samples than others. This issue can lead to poor model performance as most machine learning algorithms tend to favor majority classes during training. To address these challenges, we propose a multi-task generative adversarial network (MTGAN) that generates synthetic minority class samples to balance the EHR data sets while preserving their distribution. We evaluate our method on two different datasets: MIMIC III which contains vital signs measurements from intensive care unit patients and Physionet database containing cardiorespiratory signals. Our results show significant improvements in both quantitative metrics and visual inspections confirming the effectiveness of proposed MTGAN approach. Further analysis shows that the learned generator can capture essential features from original dataset and generate high quality artificial samples. Additionally, transfer learning experiments demonstrate the potential utility of our generated data set for downstream tasks such as predictive model building. Finally, we conducted sensitivity analysis showing the robustness of our methods against varying hyperparameters choices. Overall, our work demonstrates the promise of utilizing GAN models in balancing clinical datasets towards improving patient outcomes through informatics innovations.",1
"Most existing single image deraining methods require learning supervised models from a large set of paired synthetic training data, which limits their generality, scalability and practicality in real-world multimedia applications. Besides, due to lack of labeled-supervised constraints, directly applying existing unsupervised frameworks to the image deraining task will suffer from low-quality recovery. Therefore, we propose an Unsupervised Deraining Generative Adversarial Network (UD-GAN) to tackle above problems by introducing self-supervised constraints from the intrinsic statistics of unpaired rainy and clean images. Specifically, we firstly design two collaboratively optimized modules, namely Rain Guidance Module (RGM) and Background Guidance Module (BGM), to take full advantage of rainy image characteristics: The RGM is designed to discriminate real rainy images from fake rainy images which are created based on outputs of the generator with BGM. Simultaneously, the BGM exploits a hierarchical Gaussian-Blur gradient error to ensure background consistency between rainy input and de-rained output. Secondly, a novel luminance-adjusting adversarial loss is integrated into the clean image discriminator considering the built-in luminance difference between real clean images and derained images. Comprehensive experiment results on various benchmarking datasets and different training settings show that UD-GAN outperforms existing image deraining methods in both quantitative and qualitative comparisons.",0
"Title: Unsupervised Single Image Deraining With Self-Supervised Constraints  Abstract: Image deraining is a challenging task that aims at removing rain streaks from images while preserving important details. Traditional methods require extensive manual supervision which can be time-consuming and laborious. In this work, we propose a novel unsupervised approach for single image deraining by leveraging self-supervised constraints. Our method uses multiple pairs of clean and rainy images as input and learns the mapping between these two domains using adversarial training. We introduce a new generator architecture that enforces local consistency and edge smoothness constraints through a multi-scale discriminator network. Experimental results on several benchmark datasets show that our method outperforms state-of-the-art unsupervised approaches and achieves competitive performance compared to fully supervised models. Our proposed framework provides a powerful tool for real-world applications where large amounts of labeled data may not be available.",1
"Generative Adversarial Networks (GANs) have shown considerable promise for mitigating the challenge of data scarcity when building machine learning-driven analysis algorithms. Specifically, a number of studies have shown that GAN-based image synthesis for data augmentation can aid in improving classification accuracy in a number of medical image analysis tasks, such as brain and liver image analysis. However, the efficacy of leveraging GANs for tackling prostate cancer analysis has not been previously explored. Motivated by this, in this study we introduce ProstateGAN, a GAN-based model for synthesizing realistic prostate diffusion imaging data. More specifically, in order to generate new diffusion imaging data corresponding to a particular cancer grade (Gleason score), we propose a conditional deep convolutional GAN architecture that takes Gleason scores into consideration during the training process. Experimental results show that high-quality synthetic prostate diffusion imaging data can be generated using the proposed ProstateGAN for specified Gleason scores.",0
"This abstract presents ProstateGAN, a method that uses generative adversarial networks (GANs) to synthesize new prostate diffusion imaging data from a diverse range of patients, aimed at mitigating bias due to patient selection when training machine learning models on limited datasets. We demonstrate that our approach outperforms previous techniques by increasing model robustness against unseen patient variability, reducing biases related to dataset size, and improving performance across multiple metrics in predictive accuracy compared to both real patient images alone and combined real+synthetic patient cohorts. In summary, our contributions focus on advancing medical image synthesis by adapting novel approaches to address challenges specific to small-dataset problems, and using these methods to generate improved high-quality synthetic data specifically designed to augment clinical applications. Our findings highlight significant potential benefits for healthcare through artificial intelligence, while emphasizing considerations crucial for responsible adoption including validation, interpretability, openness, safety, privacy, ethics, regulation, and economics. The code and generated images will soon be made publicly available along with preliminary results for the research community to further evaluate.",1
"Caricature generation is an interesting yet challenging task. The primary goal is to generate plausible caricatures with reasonable exaggerations given face images. Conventional caricature generation approaches mainly use low-level geometric transformations such as image warping to generate exaggerated images, which lack richness and diversity in terms of content and style. The recent progress in generative adversarial networks (GANs) makes it possible to learn an image-to-image transformation from data, so that richer contents and styles can be generated. However, directly applying the GAN-based models to this task leads to unsatisfactory results because there is a large variance in the caricature distribution. Moreover, some models require strictly paired training data which largely limits their usage scenarios. In this paper, we propose CariGAN overcome these problems. Instead of training on paired data, CariGAN learns transformations only from weakly paired images. Specifically, to enforce reasonable exaggeration and facial deformation, facial landmarks are adopted as an additional condition to constrain the generated image. Furthermore, an attention mechanism is introduced to encourage our model to focus on the key facial parts so that more vivid details in these regions can be generated. Finally, a Diversity Loss is proposed to encourage the model to produce diverse results to help alleviate the `mode collapse' problem of the conventional GAN-based models. Extensive experiments on a new large-scale `WebCaricature' dataset show that the proposed CariGAN can generate more plausible caricatures with larger diversity compared with the state-of-the-art models.",0
"Here we introduce a new architecture called CariGAN which performs image-to-image translation via weakly paired adversarial learning. As input, the model receives two sets of images without any explicit correspondence between them. Without explicit alignment supervision (such as one-hot labels), the generator struggles to perform meaningful translations from one set to another because there exists no clear mapping between source inputs and target outputs given by only few examples. To address this issue we apply auxiliary classifiers to both domains, enabling the discriminator to provide finer gradients back into the generator; our experiments show that aligning the distributions rather than matching the mean across both datasets is crucial for generating coherent images on par with full adversarial training. Experiments demonstrate superior performance over previous methods under weakly aligned settings where either few matched pairs exist or they are extremely noisy. Our findings suggest future work should focus more on adaptive domain alignment techniques in order to improve performance in low data regimes where fully unsupervised adaptation is difficult.",1
"Solving inverse problems continues to be a central challenge in computer vision. Existing techniques either explicitly construct an inverse mapping using prior knowledge about the corruption, or learn the inverse directly using a large collection of examples. However, in practice, the nature of corruption may be unknown, and thus it is challenging to regularize the problem of inferring a plausible solution. On the other hand, collecting task-specific training data is tedious for known corruptions and impossible for unknown ones. We present MimicGAN, an unsupervised technique to solve general inverse problems based on image priors in the form of generative adversarial networks (GANs). Using a GAN prior, we show that one can reliably recover solutions to underdetermined inverse problems through a surrogate network that learns to mimic the corruption at test time. Our system successively estimates the corruption and the clean image without the need for supervisory training, while outperforming existing baselines in blind image recovery. We also demonstrate that MimicGAN improves upon recent GAN-based defenses against adversarial attacks and represents one of the strongest test-time defenses available today.",0
"Advances in deep learning have revolutionized computer vision tasks such as image classification, segmentation, and super-resolution. In these cases, powerful generative models like Generative Adversarial Networks (GANs) can synthesize new data from observed patterns. Yet, there exists one important scenario where no existing GAN approach addresses the task effectively---when we want to automatically generate a corrupted version of some input images. Although many applications might use machine learning on real world inputs, their robustness against adversaries intent on degrading those inputs requires evaluation using synthetic attacks that mimic possible attack scenarios. Here we present MimicGAN which allows the user to specify the types of noise they wish to add to the original images while preserving semantic content. Our contribution focuses on four areas - corrupting clean images into correspondingly dirty versions, generating novel attacks which were previously unseen during training, defending against black box attacks by adversarial training, and finally evaluating two different approaches towards improving generator performance under common attacks through hyperparameter tuning. We demonstrate our system's effectiveness on three common object recognition benchmark datasets and show competitive results compared to traditional methods of data augmentation even on large public datasets. By providing a unique set of tools to practitioners and researchers alike, MimcGAN represents both a powerful and versatile approach to generating new challenges for the next generation of robust systems while opening up avenues to further study the nature of data corruption itself.",1
"In this paper, we propose Generative Adversarial Network (GAN) architectures that use Capsule Networks for image-synthesis. Based on the principal of positional-equivariance of features, Capsule Network's ability to encode spatial relationships between the features of the image helps it become a more powerful critic in comparison to Convolutional Neural Networks (CNNs) used in current architectures for image synthesis. Our proposed GAN architectures learn the data manifold much faster and therefore, synthesize visually accurate images in significantly lesser number of training samples and training epochs in comparison to GANs and its variants that use CNNs. Apart from analyzing the quantitative results corresponding the images generated by different architectures, we also explore the reasons for the lower coverage and diversity explored by the GAN architectures that use CNN critics.",0
"An architecture based on generative adversarial networks (GAN) utilizing capsule network components has been proposed as a method for image synthesis. This model offers several advantages over traditional GAN architectures by improving stability and reducing mode collapse, resulting in higher quality generated images. Additionally, capsules enable this approach to capture more detailed and meaningful representations of objects within the generated images, allowing the creation of highly realistic scenes that can rival those created using other state-of-the-art techniques. Evaluation of this novel architecture demonstrates its effectiveness across multiple datasets, including both quantitative measures such as FID score and qualitative analysis via visual inspection. Overall, this research contributes significant advancements towards achieving photorealism through generative models.",1
"Event cameras have a lot of advantages over traditional cameras, such as low latency, high temporal resolution, and high dynamic range. However, since the outputs of event cameras are the sequences of asynchronous events overtime rather than actual intensity images, existing algorithms could not be directly applied. Therefore, it is demanding to generate intensity images from events for other tasks. In this paper, we unlock the potential of event camera-based conditional generative adversarial networks to create images/videos from an adjustable portion of the event data stream. The stacks of space-time coordinates of events are used as inputs and the network is trained to reproduce images based on the spatio-temporal intensity changes. The usefulness of event cameras to generate high dynamic range(HDR) images even in extreme illumination conditions and also non blurred images under rapid motion is also shown.In addition, the possibility of generating very high frame rate videos is demonstrated, theoretically up to 1 million frames per second (FPS) since the temporal resolution of event cameras are about 1{\mu}s. Proposed methods are evaluated by comparing the results with the intensity images captured on the same pixel grid-line of events using online available real datasets and synthetic datasets produced by the event camera simulator.",0
"Artificial intelligence has been widely applied in image and video generation tasks recently due to advances in deep learning. In the field of computer vision, high dynamic range (HDR) imaging is an essential technology that enables digital cameras to produce images with enhanced visual details by capturing multiple exposures at different levels of illumination and fusing them into one HDR image. Meanwhile, very high frame rate (VHRF) video technology allows recording fast moving objects clearly even under low light conditions. However, generating realistic event-driven HDR videos that capture multiple frames per second remains challenging. To address these issues, we propose an innovative approach that combines conditional generative adversarial networks (cGANs) with multi-exposure fusion techniques to generate both still event-driven HDR images and VHRF videos from single regular input frames. Our method takes advantage of cGANsâ€™ ability to learn statistical patterns in training data to synthesize new samples, while conditioning on semantic features such as object class labels. We conducted extensive experiments on several benchmark datasets, demonstrating our method outperforms state-of-the-art methods in terms of visual quality, realism, temporal coherence, and perceptual evaluation metrics. Overall, our work shows great potential for diverse applications including but not limited to gaming, virtual reality, augmented reality, autonomous systems, robotics, surveillance, movies, etc.",1
"We present a novel approach to point set registration which is based on one-shot adversarial learning. The idea of the algorithm is inspired by recent successes of generative adversarial networks. Treating the point clouds as three-dimensional probability distributions, we develop a one-shot adversarial optimization procedure, in which we train a critic neural network to distinguish between source and target point sets, while simultaneously learning the parameters of the transformation to trick the critic into confusing the points. In contrast to most existing algorithms for point set registration, ours does not rely on any correspondences between the point clouds. We demonstrate the performance of the algorithm on several challenging benchmarks and compare it to the existing baselines.",0
"Abstract: This paper presents a method for adversarial point set registration, which involves aligning multiple sets of unordered points in 2D or 3D space using a deep neural network. Our approach uses a generative adversarial network (GAN) architecture, consisting of two subnetworks that compete against each other during training. One subnetwork generates possible transformations from one point set onto another, while the second network evaluates their quality. We evaluate our method on several benchmark datasets and demonstrate superior performance compared to state-of-the-art methods, including both traditional alignment techniques and recent learning-based approaches. Additionally, we showcase the robustness and generalizability of our model by applying it to challenging real-world applications such as medical image registration and surface reconstruction. Overall, our work represents a significant step towards fully automatic point set registration without reliance on manual feature engineering or strong assumptions about data distributions.",1
"In this paper, we investigate the Chinese font synthesis problem and propose a Pyramid Embedded Generative Adversarial Network (PEGAN) to automatically generate Chinese character images. The PEGAN consists of one generator and one discriminator. The generator is built using one encoder-decoder structure with cascaded refinement connections and mirror skip connections. The cascaded refinement connections embed a multiscale pyramid of downsampled original input into the encoder feature maps of different layers, and multi-scale feature maps from the encoder are connected to the corresponding feature maps in the decoder to make the mirror skip connections. Through combining the generative adversarial loss, pixel-wise loss, category loss and perceptual loss, the generator and discriminator can be trained alternately to synthesize character images. In order to verify the effectiveness of our proposed PEGAN, we first build one evaluation set, in which the characters are selected according to their stroke number and frequency of use, and then use both qualitative and quantitative metrics to measure the performance of our model comparing with the baseline method. The experimental results demonstrate the effectiveness of our proposed model, it shows the potential to automatically extend small font banks into complete ones.",0
"Here comes Abstract:  Pyramidal hierarchies have been used as a way to represent the structure of data, with coarser levels capturing larger scale features and finer levels capturing more detailed ones. In recent years there has been increased interest in generative adversarial networks (GANs), which consist of two neural networks that compete against each other in order to generate realistic synthetic data such as images or audio. Motivated by these advances, we introduce a new architecture called the pyramid embedded generative adversarial network (PEGAN) that uses a multi-scale hierarchy of GANs to capture different aspects of typography including stroke width, curvature, angle, and spacing at multiple scales. We demonstrate how PEGAN can generate novel fonts that maintain a consistent look and feel across all glyphs within the font family, while still allowing for variation among individual letters. Our evaluation shows that PEGAN outperforms state-of-the-art methods on both quantitative metrics and human judgments. Finally, we discuss potential future directions for using PEGAN to aid designers in generating typefaces and improving accessibility in digital media through personalized typography.",1
"$\textbf{Purpose}$ To train a cycle-consistent generative adversarial network (CycleGAN) on mammographic data to inject or remove features of malignancy, and to determine whether these AI-mediated attacks can be detected by radiologists. $\textbf{Material and Methods}$ From the two publicly available datasets, BCDR and INbreast, we selected images from cancer patients and healthy controls. An internal dataset served as test data, withheld during training. We ran two experiments training CycleGAN on low and higher resolution images ($256 \times 256$ px and $512 \times 408$ px). Three radiologists read the images and rated the likelihood of malignancy on a scale from 1-5 and the likelihood of the image being manipulated. The readout was evaluated by ROC analysis (Area under the ROC curve = AUC). $\textbf{Results}$ At the lower resolution, only one radiologist exhibited markedly lower detection of cancer (AUC=0.85 vs 0.63, p=0.06), while the other two were unaffected (0.67 vs. 0.69 and 0.75 vs. 0.77, p=0.55). Only one radiologist could discriminate between original and modified images slightly better than guessing/chance (0.66, p=0.008). At the higher resolution, all radiologists showed significantly lower detection rate of cancer in the modified images (0.77-0.84 vs. 0.59-0.69, p=0.008), however, they were now able to reliably detect modified images due to better visibility of artifacts (0.92, 0.92 and 0.97). $\textbf{Conclusion}$ A CycleGAN can implicitly learn malignant features and inject or remove them so that a substantial proportion of small mammographic images would consequently be misdiagnosed. At higher resolutions, however, the method is currently limited and has a clear trade-off between manipulation of images and introduction of artifacts.",0
"Artificial intelligence (AI) has become increasingly important for detecting and diagnosing diseases such as breast cancer through medical imaging techniques like mammography. However, the accuracy of these systems can be compromised by attacks that manipulate images in ways that are difficult for humans to detect but may cause errors in computer vision algorithms used for disease detection. In this study, we investigate an automated adversarial attack on mammograms generated by deep learning models known as Generative Adversarial Networks (GAN). We trained two GANs to generate realistic looking mammographic images with both benign and malignant masses, respectively. Then, we applied the CycleGAN model which uses the difference image between the original input image and its corresponding fake reconstruction from another domain to create adversarial examples. To evaluate the effectiveness of the adversarial attack, we measured changes in radiologistsâ€™ confidence levels and overall accuracy in their ability to distinguish between normal and abnormal cases before and after applying the manipulated images. Our findings indicate that even slight perturbations introduced into mammographies could significantly reduce human performance in identifying tumors. Moreover, our results highlight the need for more robust machine learning models capable of resisting potential adversarial attacks while maintaining high levels of diagnostic precision.",1
"Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide a more accurate estimation of the sample quality. Our proposed model also outperforms the baseline methods in the new metric.",0
"GPT model should write the following: Abstract: In recent years, generative adversarial networks (GANs) have emerged as one of the most powerful architectures in deep learning due to their ability to generate high quality synthetic data that can fool even human experts. However, training these models remains challenging due to the instability of the optimization process and difficulty in evaluating the generated samples. To address these issues, we propose activation maximization GANs (AM-GANs), which optimize both generator and discriminator activations simultaneously using gradient ascent methods. Our experimental results show that AM-GANs achieve significantly better performance than traditional GANs on several benchmark datasets. Furthermore, our approach leads to more stable convergence during training and improved sample evaluation metrics. Overall, AM-GANs represent a significant step forward in the field of generative deep learning.",1
"Generative Adversarial Networks are a new family of generative models, frequently used for generating photorealistic images. The theory promises for the GAN to eventually reach an equilibrium where generator produces pictures indistinguishable for the training set. In practice, however, a range of problems frequently prevents the system from reaching this equilibrium, with training not progressing ahead due to instabilities or mode collapse. This paper describes a series of experiments trying to identify patterns in regard to the effect of the training set on the dynamics and eventual outcome of the training.",0
"This paper examines how different aspects of dataset properties impact the performance of generative adversarial networks (GANs). Specifically, we investigate how factors such as data size, quality, diversity, and balance influence the training process and resulting model outputs. We find that while larger datasets can improve overall results, data quality plays a more important role than quantity. Similarly, we observe that increased diversity within a dataset leads to better training stability and generalization abilities. Lastly, we explore the importance of balancing classes in a dataset, which we show directly affects the fairness and accuracy of generated samples. Our work contributes to the understanding of how dataset properties interact with GAN training dynamics and outcomes. By providing guidelines for designing effective training sets, our insights aim to advance the development and deployment of high-quality GAN models across various domains.",1
"Batch Normalization (BN) is essential to effectively train state-of-the-art deep Convolutional Neural Networks (CNN). It normalizes inputs to the layers during training using the statistics of each mini-batch. In this work, we study BN from the viewpoint of Fisher kernels. We show that assuming samples within a mini-batch are from the same probability density function, then BN is identical to the Fisher vector of a Gaussian distribution. That means BN can be explained in terms of kernels that naturally emerge from the probability density function of the underlying data distribution. However, given the rectifying non-linearities employed in CNN architectures, distribution of inputs to the layers show heavy tail and asymmetric characteristics. Therefore, we propose approximating underlying data distribution not with one, but a mixture of Gaussian densities. Deriving Fisher vector for a Gaussian Mixture Model (GMM), reveals that BN can be improved by independently normalizing with respect to the statistics of disentangled sub-populations. We refer to our proposed soft piecewise version of BN as Mixture Normalization (MN). Through extensive set of experiments on CIFAR-10 and CIFAR-100, we show that MN not only effectively accelerates training image classification and Generative Adversarial networks, but also reaches higher quality models.",0
"Learning deep neural networks typically requires many iterations over large datasets, which makes them time consuming to train even on todayâ€™s most powerful computers. Recently, batch normalization (BN) has emerged as a highly effective method for accelerating training and improving test performance. In practice, however, it is often observed that certain layers within these modelsâ€”particularly feedforward convolutional layersâ€”fail to converge properly, leading to suboptimal results despite the fast speeds achieved during early epochs. This work proposes one possible explanation for why this happens, and introduces two novel changes to the original formula that can allow some layers with poor convergence to improve significantly. We find that our adjustments increase peak validation accuracy across nearly all benchmark tasks, yielding improvements of up to +1% top1 / âˆ’2% top5 error on ImageNet. Code and pretrained models are publicly available at https://github.com/google-research/batchnorm.",1
"Disentangling factors of variation within data has become a very challenging problem for image generation tasks. Current frameworks for training a Generative Adversarial Network (GAN), learn to disentangle the representations of the data in an unsupervised fashion and capture the most significant factors of the data variations. However, these approaches ignore the principle of content and style disentanglement in image generation, which means their learned latent code may alter the content and style of the generated images at the same time. This paper describes the Style and Content Disentangled GAN (SC-GAN), a new unsupervised algorithm for training GANs that learns disentangled style and content representations of the data. We assume that the representation of an image can be decomposed into a content code that represents the geometrical information of the data, and a style code that captures textural properties. Consequently, by fixing the style portion of the latent representation, we can generate diverse images in a particular style. Reversely, we can set the content code and generate a specific scene in a variety of styles. The proposed SC-GAN has two components: a content code which is the input to the generator, and a style code which modifies the scene style through modification of the Adaptive Instance Normalization (AdaIN) layers' parameters. We evaluate the proposed SC-GAN framework on a set of baseline datasets.",0
"Deep generative models such as Generative Adversarial Networks (GANs) have shown significant advancements in recent years through improvements in training stability and generation quality. These advances often rely on changes to both the architecture style, which defines the basic operations, and content, which encodes the specific characteristics of the data distribution. However, disentangling these two aspects can prove difficult since they tend to coexist during training and affect each other. In our work, we propose new methods to explicitly separate content from style without compromising either aspect. We demonstrate that using our method significantly improves generated images over current state-of-the-art GAN architectures while keeping both content and style coherence. Our framework enables fine-grained control over generated images by directly manipulating their content, even allowing translation of the generated image into new styles. Additionally, we show improved performance compared to previously used benchmark metrics. This research offers important contributions towards understanding and mastering the capabilities of deep learning systems, paving the road for further development of novel applications.",1
"Anomaly detection is a classical problem in computer vision, namely the determination of the normal from the abnormal when datasets are highly biased towards one class (normal) due to the insufficient sample size of the other class (abnormal). While this can be addressed as a supervised learning problem, a significantly more challenging problem is that of detecting the unknown/unseen anomaly case that takes us instead into the space of a one-class, semi-supervised learning paradigm. We introduce such a novel anomaly detection model, by using a conditional generative adversarial network that jointly learns the generation of high-dimensional image space and the inference of latent space. Employing encoder-decoder-encoder sub-networks in the generator network enables the model to map the input image to a lower dimension vector, which is then used to reconstruct the generated output image. The use of the additional encoder network maps this generated image to its latent representation. Minimizing the distance between these images and the latent vectors during training aids in learning the data distribution for the normal samples. As a result, a larger distance metric from this learned data distribution at inference time is indicative of an outlier from that distribution - an anomaly. Experimentation over several benchmark datasets, from varying domains, shows the model efficacy and superiority over previous state-of-the-art approaches.",0
"Title: GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training Authors: Michael Nguyen, Shibo Wang, Jian Zhang Abstract: In recent years, Generative Adversarial Networks (GANs) have emerged as a powerful tool for generating high quality synthetic data, but their potential has yet to be fully explored. We propose GANomaly, a semi-supervised anomaly detection framework that leverages adversarial training on both labeled and unlabeled datasets. Our method combines the strengths of generators and discriminators, enabling us to train models efficiently using only small amounts of labelled data. We show through extensive experiments that our approach significantly outperforms state-of-the-art methods across multiple domains including image classification, sentiment analysis and video prediction tasks. Additionally we demonstrate how these improvements can directly result in performance gains within real world applications such as financial fraud detection. With this work, we aim to bridge the gap between academia and industry by providing researchers and practitioners alike access to a highly performant anomaly detection system that requires little tuning nor expertise.",1
"An instance with a bad mask might make a composite image that uses it look fake. This encourages us to learn segmentation by generating realistic composite images. To achieve this, we propose a novel framework that exploits a new proposed prior called the independence prior based on Generative Adversarial Networks (GANs). The generator produces an image with multiple category-specific instance providers, a layout module and a composition module. Firstly, each provider independently outputs a category-specific instance image with a soft mask. Then the provided instances' poses are corrected by the layout module. Lastly, the composition module combines these instances into a final image. Training with adversarial loss and penalty for mask area, each provider learns a mask that is as small as possible but enough to cover a complete category-specific instance. Weakly supervised semantic segmentation methods widely use grouping cues modeling the association between image parts, which are either artificially designed or learned with costly segmentation labels or only modeled on local pairs. Unlike them, our method automatically models the dependence between any parts and learns instance segmentation. We apply our framework in two cases: (1) Foreground segmentation on category-specific images with box-level annotation. (2) Unsupervised learning of instance appearances and masks with only one image of homogeneous object cluster (HOC). We get appealing results in both tasks, which shows the independence prior is useful for instance segmentation and it is possible to unsupervisedly learn instance masks with only one image.",0
"This paper presents a method for learning segmentation masks that use the independence prior to improve accuracy. The proposed approach is based on a deep neural network that learns a joint distribution over the input image and the corresponding mask. By using the independence prior, the network can more efficiently model complex dependencies between pixels and reduce overfitting. Experimental results demonstrate significant improvements in segmentation performance compared to traditional methods without the use of priors. Overall, this work shows promise as a powerful tool for medical imaging applications where accurate segmentations are crucial.",1
"Generative Adversarial Networks (GAN) have received wide attention in the machine learning field for their potential to learn high-dimensional, complex real data distribution. Specifically, they do not rely on any assumptions about the distribution and can generate real-like samples from latent space in a simple manner. This powerful property leads GAN to be applied to various applications such as image synthesis, image attribute editing, image translation, domain adaptation and other academic fields. In this paper, we aim to discuss the details of GAN for those readers who are familiar with, but do not comprehend GAN deeply or who wish to view GAN from various perspectives. In addition, we explain how GAN operates and the fundamental meaning of various objective functions that have been suggested recently. We then focus on how the GAN can be combined with an autoencoder framework. Finally, we enumerate the GAN variants that are applied to various tasks and other fields for those who are interested in exploiting GAN for their research.",0
"This study provides a comprehensive overview of generative adversarial networks (GAN) and their variants, covering both theoretical concepts and real-world applications. GANs have gained significant attention due to their ability to generate high-quality synthetic data that resembles real-world data distributions. In recent years, several new variants of GANs have emerged that address some of the limitations of traditional models, such as instability during training and difficulty generating diverse outputs. These advances have broadened the range of tasks GANs can tackle, including image generation, video generation, text generation, and more. The authors review the most promising GAN approaches and discuss their effectiveness across different domains. Furthermore, they provide insights into current research trends and future directions in the field of GANs. Ultimately, this survey is intended to serve as a valuable resource for practitioners interested in leveraging these powerful methods.",1
"Recent work has shown that exploiting relations between labels improves the performance of multi-label classification. We propose a novel framework based on generative adversarial networks (GANs) to model label dependency. The discriminator learns to model label dependency by discriminating real and generated label sets. To fool the discriminator, the classifier, or generator, learns to generate label sets with dependencies close to real data. Extensive experiments and comparisons on two large-scale image classification benchmark datasets (MS-COCO and NUS-WIDE) show that the discriminator improves generalization ability for different kinds of models",0
"This research presents a novel framework for multi-class classification that addresses the limitations of traditional approaches by incorporating label dependency into the learning process. Our approach utilizes adversarial training methods to learn how dependencies among labels affect model performance. By considering these dependencies, our framework can better capture complex relationships between classes, resulting in improved accuracy on challenging datasets. We evaluate our method using several benchmark datasets and demonstrate significant improvements over state-of-the-art algorithms across a range of tasks. Overall, our work provides insights into the importance of label dependency in multi-class classification and introduces a powerful new tool for addressing real-world problems in data analysis.",1
"Face presentation attack detection (PAD) has become a thorny problem for biometric systems and numerous countermeasures have been proposed to address it. However, majority of them directly extract feature descriptors and distinguish fake faces from the real ones in existing color spaces (e.g. RGB, HSV and YCbCr). Unfortunately, it is unknown for us which color space is the best or how to combine different spaces together. To make matters worse, the real and fake faces are overlapped in existing color spaces. So, in this paper, a learned distinguishable color-liked space is generated to deal with the problem of face PAD. More specifically, we present an end-to-end deep learning network that can map existing color spaces to a new learned color-liked space. Inspired by the generator of generative adversarial network (GAN), the proposed network consists of a space generator and a feature extractor. When training the color-liked space, a new triplet combination mechanism of points-to-center is explored to maximize interclass distance and minimize intraclass distance, and also keep a safe margin between the real and presented fake faces. Extensive experiments on two standard face PAD databases, i.e., Relay-Attack and OULU-NPU, indicate that our proposed color-liked space analysis based countermeasure significantly outperforms the state-of-the-art methods and show excellent generalization capability.",0
"Abstract: Color-liked space (CLS) models have recently been shown to improve face presentation attack detection (PAD). However, most CLS-based PAD methods focus on detecting artifacts caused by image processing while neglecting other types of attacks. To address this issue, we present a new approach called FPAD (Face Presentation Attack Detection), which utilizes multiple pretrained deep learning architectures as detectors in color-liked spaces. By combining these features in a modular manner using adaptive kernel discriminant analysis, we are able to achieve superior performance over state-of-the-art PAD methods across all categories of attacks. Our results show that our method effectively captures more subtle differences between genuine faces and those generated from different forms of attacks. Thus, we demonstrate the effectiveness of our method against both traditional and emerging threats in authentication scenarios, making it highly applicable in real-world security systems where robustness matters.",1
"The paper proposes an on-line monitoring framework for continuous real-time safety/security in learning-based control systems (specifically application to a unmanned ground vehicle). We monitor validity of mappings from sensor inputs to actuator commands, controller-focused anomaly detection (CFAM), and from actuator commands to sensor inputs, system-focused anomaly detection (SFAM). CFAM is an image conditioned energy based generative adversarial network (EBGAN) in which the energy based discriminator distinguishes between proper and anomalous actuator commands. SFAM is based on an action condition video prediction framework to detect anomalies between predicted and observed temporal evolution of sensor data. We demonstrate the effectiveness of the approach on our autonomous ground vehicle for indoor environments and on Udacity dataset for outdoor environments.",0
"Artificial intelligence (AI) has grown rapidly over recent years due to advances in computing technology, machine learning algorithms, and large amounts of data available for training these models. This growth has led to many AI applications such as computer vision, natural language processing, robotics, self-driving cars, medical diagnosis, speech recognition systems, bioinformatics analysis, etc. These AI models make use of anomaly detection methods to identify patterns that differ significantly from normal ones in sensed data or other variables associated with them. Many existing unsupervised and supervised learning techniques have been developed based on statistical approaches to detect outliers in real time by leveraging sequential measurements. However, they struggle in modeling complex interdependencies across heterogeneous sources of measurement modalities. In order to overcome these limitations, we propose an adversarial framework called On-line Multidimensional Adaptive Robust Estimation Method (OMARM), which makes use of generative and discriminator deep neural networks (DNNs). We focus primarily on high-dimensional multivariate settings like data streams coming from Internet of Things (IoT) devices, sensor arrays, video feeds, communication signals, satellite imagery data, ground-based remote sensors, etc., characterized by spatially variant anomalies, temporal correlations, missing values, nonlinearity, heteroscedastic noise, and possibly unknown anomalous distributions. Our contributions consist of: i) introducing a scalable methodology capable of providing provably suboptimality bounds under mild conditions, leveraging variational inference strategies; ii) developing a distributed implementation able to scale up to large sizes without loss of efficiency or accuracy; and iii) demonstrating the efficacy o",1
"An interpretable generative model for handwritten digits synthesis is proposed in this work. Modern image generative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), are trained by backpropagation (BP). The training process is complex and the underlying mechanism is difficult to explain. We propose an interpretable multi-stage PCA method to achieve the same goal and use handwritten digit images synthesis as an illustrative example. First, we derive principal-component-analysis-based (PCA-based) transform kernels at each stage based on the covariance of its inputs. This results in a sequence of transforms that convert input images of correlated pixels to spectral vectors of uncorrelated components. In other words, it is a whitening process. Then, we can synthesize an image based on random vectors and multi-stage transform kernels through a coloring process. The generative model is a feedforward (FF) design since no BP is used in model parameter determination. Its design complexity is significantly lower, and the whole design process is explainable. Finally, we design an FF generative model using the MNIST dataset, compare synthesis results with those obtained by state-of-the-art GAN and VAE methods, and show that the proposed generative model achieves comparable performance.",0
"This paper presents a novel approach to synthesizing handwritten digit images that is both interpretable and generates high quality results. Our method uses a generative adversarial network (GAN) architecture, consisting of two components: a generator network which produces new images, and a discriminator network which evaluates their realism. Unlike traditional GANs, our model incorporates a novel attention mechanism that allows us to visualize and interpret the features used by the generator during image synthesis. We show through experiments on several benchmark datasets that our method outperforms state-of-the-art alternatives in terms of generated image quality and robustness to noise and input perturbations. Additionally, we demonstrate how our attention mechanism can be used to gain insights into the properties of handwriting data and improve human understanding of machine learning models. Overall, our work represents a significant advance in the field of interpretable deep learning for computer vision tasks.",1
"The findings of splenomegaly, abnormal enlargement of the spleen, is a non-invasive clinical biomarker for liver and spleen disease. Automated segmentation methods are essential to efficiently quantify splenomegaly from clinically acquired abdominal magnetic resonance imaging (MRI) scans. However, the task is challenging due to (1) large anatomical and spatial variations of splenomegaly, (2) large inter- and intra-scan intensity variations on multi-modal MRI, and (3) limited numbers of labeled splenomegaly scans. In this paper, we propose the Splenomegaly Segmentation Network (SS-Net) to introduce the deep convolutional neural network (DCNN) approaches in multi-modal MRI splenomegaly segmentation. Large convolutional kernel layers were used to address the spatial and anatomical variations, while the conditional generative adversarial networks (GAN) were employed to leverage the segmentation performance of SS-Net in an end-to-end manner. A clinically acquired cohort containing both T1-weighted (T1w) and T2-weighted (T2w) MRI splenomegaly scans was used to train and evaluate the performance of multi-atlas segmentation (MAS), 2D DCNN networks, and a 3D DCNN network. From the experimental results, the DCNN methods achieved superior performance to the state-of-the-art MAS method. The proposed SS-Net method achieved the highest median and mean Dice scores among investigated baseline DCNN methods.",0
"Abbreviate it as SM. You should assume that your audience has general understanding of medical imaging but no specific knowledge of MRI. What I mean by ""no specific knowledge"" means that you can describe key concepts such as modalities but don't assume they have any background on MRIs. ---Scientists use deep convolutional networks (CNN) to automatically segment splenic lesions from multi-modal MRI images. This technique is called splenomegaly segmentation, which helps diagnose diseases related to enlargement of the spleen. Researchers first preprocessed the multi-modal MRI scans into single images containing all relevant features. They then used a UNet architecture, an established network topology designed specifically for image segmentation tasks like theirs, to extract high-level features and accurately differentiate surrounding tissues from the target organ. Experiments show promising results: average Dice similarity scores above 87% and 92% on two test sets. Scientists believe these findings demonstrate future potential for automating splenectomy assessments via machine learning algorithms. Further research could lead to more accurate disease detection and better patient outcomes.",1
"In this paper, we develop a complete pipeline for stain normalization, segmentation, and classification of nuclei in hematoxylin and eosin (H&E) stained breast cancer histopathology images. In the first step, we use a CNN-based stain transfer technique to normalize the staining characteristics of (H&E) images. We then train a neural network to segment images of nuclei from the H&E images. Finally, we train an Information Maximizing Generative Adversarial Network (InfoGAN) to learn visual representations of different types of nuclei and classify them in an entirely unsupervised manner. The results show that our proposed CNN stain normalization yields improved visual similarity and cell segmentation performance compared to the conventional SVD-based stain normalization method. In the final step of our pipeline, we demonstrate the ability to perform fully unsupervised clustering of various breast histopathology cell types based on morphological and color attributes. In addition, we quantitatively evaluate our neural network - based techniques against various quantitative metrics to validate the effectiveness of our pipeline.",0
"Image analysis plays an essential role in understanding the histology of breast cancer tissues, which is crucial for accurate diagnosis and treatment planning. In this study, we aim to develop an automated method using neural stain normalization and unsupervised classification to analyze cell nuclei present in histopathological images of breast cancers. We propose a framework based on CycleGAN that takes into account color correction between different image sources. This approach is then further combined with an autoencoder for effective dimension reduction before clustering the nuclei using K-means. Experiments show promising results in terms of accuracy and efficiency compared to other state-of-the-art methods. Our work addresses current challenges faced by pathologists when analyzing large amounts of data, paving the way towards faster and more reliable diagnoses for better patient outcomes. Overall, our technique presents a novel strategy for processing high-throughput microscopy images in digital pathology, leading to improved medical decision making in breast cancer cases.",1
"The use of synthetic data generated by Generative Adversarial Networks (GANs) has become quite a popular method to do data augmentation for many applications. While practitioners celebrate this as an economical way to get more synthetic data that can be used to train downstream classifiers, it is not clear that they recognize the inherent pitfalls of this technique. In this paper, we aim to exhort practitioners against deriving any false sense of security against data biases based on data augmentation. To drive this point home, we show that starting with a dataset consisting of head-shots of engineering researchers, GAN-based augmentation ""imagines"" synthetic engineers, most of whom have masculine features and white skin color (inferred from a human subject study conducted on Amazon Mechanical Turk). This demonstrates how biases inherent in the training data are reinforced, and sometimes even amplified, by GAN-based data augmentation; it should serve as a cautionary tale for the lay practitioners.",0
"Artificial intelligence (AI) systems rely heavily on data to learn, make predictions, and take actions. However, if the training data contains biases, these biases can propagate through the system and lead to unfair outcomes. One common method used to augment existing datasets is Generative Adversarial Networks (GANs), which create new synthetic data samples that mimic the original dataset. While GANs have shown promising results in generating realistic data, they may perpetuate or even amplify preexisting biases present in the training data. This paper examines how biases in training data affect GAN-based data augmentation techniques and presents case studies illustrating how such biases influence downstream applications. We conclude by discussing potential solutions to mitigate bias proliferation in GAN-generated data and suggest further research directions towards creating fair and unbiased AI systems.",1
"Generative Adversarial Networks have shown impressive results for the task of object translation, including face-to-face translation. A key component behind the success of recent approaches is the self-consistency loss, which encourages a network to recover the original input image when the output generated for a desired attribute is itself passed through the same network, but with the target attribute inverted. While the self-consistency loss yields photo-realistic results, it can be shown that the input and target domains, supposed to be close, differ substantially. This is empirically found by observing that a network recovers the input image even if attributes other than the inversion of the original goal are set as target. This stops one combining networks for different tasks, or using a network to do progressive forward passes. In this paper, we show empirical evidence of this effect, and propose a new loss to bridge the gap between the distributions of the input and target domains. This ""triple consistency loss"", aims to minimise the distance between the outputs generated by the network for different routes to the target, independent of any intermediate steps. To show this is effective, we incorporate the triple consistency loss into the training of a new landmark-guided face to face synthesis, where, contrary to previous works, the generated images can simultaneously undergo a large transformation in both expression and pose. To the best of our knowledge, we are the first to tackle the problem of mismatching distributions in self-domain synthesis, and to propose ""in-the-wild"" landmark-guided synthesis. Code will be available at https://github.com/ESanchezLozano/GANnotation",0
"In recent years, Generative Adversarial Networks (GANs) have emerged as powerful tools for generating realistic images. However, generating high quality paired data can be difficult and time consuming. This work presents a new approach called Triple Consistency Loss (TCL), which addresses the problem of generating paired data for use in GAN-based face synthesis. TCL combines two previously proposed losses, cycle consistency and identity preserving regularization, into one triple framework that improves both cycle consistency and identity mapping simultaneously. Through a comprehensive evaluation on multiple datasets, we show that our method significantly outperforms state-of-the-art methods and produces more accurate and coherent paired data for use in GAN-based face synthesis. Overall, this research has significant implications for the field of computer vision, particularly in applications such as image generation and editing.",1
"In the problem of generalized zero-shot learning, the datapoints from unknown classes are not available during training. The main challenge for generalized zero-shot learning is the unbalanced data distribution which makes it hard for the classifier to distinguish if a given testing sample comes from a seen or unseen class. However, using Generative Adversarial Network (GAN) to generate auxiliary datapoints by the semantic embeddings of unseen classes alleviates the above problem. Current approaches combine the auxiliary datapoints and original training data to train the generalized zero-shot learning model and obtain state-of-the-art results. Inspired by such models, we propose to feed the generated data via a model selection mechanism. Specifically, we leverage two sources of datapoints (observed and auxiliary) to train some classifier to recognize which test datapoints come from seen and which from unseen classes. This way, generalized zero-shot learning can be divided into two disjoint classification tasks, thus reducing the negative influence of the unbalanced data distribution. Our evaluations on four publicly available datasets for generalized zero-shot learning show that our model obtains state-of-the-art results.",0
"Zero-shot learning (ZSL) has gained significant attention as a means of expanding the capabilities of machine learning models by allowing them to learn new concepts without any additional training data. However, existing ZSL methods often rely on ad hoc heuristics or simplifying assumptions that may limit their effectiveness in real-world scenarios. In contrast, our proposed approach, generalized zero-shot learning (GZSL), seeks to address these limitations by treating ZSL as a problem of model selection. We demonstrate how incorporating prior knowledge can greatly improve the performance of GZSL, enabling more accurate predictions even when little or no labeled data is available. Our experiments show that GZSL outperforms several state-of-the-art ZSL methods across a range of datasets and tasks, highlighting its potential impact on applications such as image classification and natural language processing. Overall, we believe that GZSL represents a promising direction for further research into ZSL and other forms of transfer learning.",1
"In this paper, we propose a novel regularization method for Generative Adversarial Networks, which allows the model to learn discriminative yet compact binary representations of image patches (image descriptors). We employ the dimensionality reduction that takes place in the intermediate layers of the discriminator network and train binarized low-dimensional representation of the penultimate layer to mimic the distribution of the higher-dimensional preceding layers. To achieve this, we introduce two loss terms that aim at: (i) reducing the correlation between the dimensions of the binarized low-dimensional representation of the penultimate layer i. e. maximizing joint entropy) and (ii) propagating the relations between the dimensions in the high-dimensional space to the low-dimensional space. We evaluate the resulting binary image descriptors on two challenging applications, image matching and retrieval, and achieve state-of-the-art results.",0
BinGANs can generate high quality images from semantic descriptions but suffer from several shortcomings including poor scalability due to large memory requirements and slow training times. To address these issues we introduce RegBinGAN which combines Generative Adversarial Networks (GANs) with regularization techniques such as dropout and weight decay. Our proposed approach significantly reduces both the model size and training time while maintaining competitive performance on image generation benchmarks compared to standard BinGAN models. Additionally we show that our method generalizes better than unregularized alternatives by evaluating on a wider range of datasets. This makes RegBinGAN a promising alternative for image generation applications where computational efficiency is critical.,1
"Style synthesis attracts great interests recently, while few works focus on its dual problem ""style separation"". In this paper, we propose the Style Separation and Synthesis Generative Adversarial Network (S3-GAN) to simultaneously implement style separation and style synthesis on object photographs of specific categories. Based on the assumption that the object photographs lie on a manifold, and the contents and styles are independent, we employ S3-GAN to build mappings between the manifold and a latent vector space for separating and synthesizing the contents and styles. The S3-GAN consists of an encoder network, a generator network, and an adversarial network. The encoder network performs style separation by mapping an object photograph to a latent vector. Two halves of the latent vector represent the content and style, respectively. The generator network performs style synthesis by taking a concatenated vector as input. The concatenated vector contains the style half vector of the style target image and the content half vector of the content target image. Once obtaining the images from the generator network, an adversarial network is imposed to generate more photo-realistic images. Experiments on CelebA and UT Zappos 50K datasets demonstrate that the S3-GAN has the capacity of style separation and synthesis simultaneously, and could capture various styles in a single model.",0
"Artificial intelligence (AI) has made significant strides over recent years due to advancements in deep learning techniques such as generative adversarial networks (GANs). In particular, GANs have shown promise for generating high-quality images from text descriptions, synthesizing new content by blending existing image data, and separating different styles within an input image for editing purposes. This paper proposes a novel approach for style separation and synthesis using GANs that surpasses current state-of-the-art methods in terms of performance, robustness, and interpretability. Our method relies on an unpaired setting where we generate new domains without any supervision or pre-existing paired examples. We train two discriminators simultaneously: one that distinguishes real images from generated ones and another that captures domain differences. With these objectives in mind, our generator learns to separate multiple semantic concepts, such as objects, backgrounds, and lighting conditions, in an end-to-end fashion. Experimental results demonstrate superior quality across diverse datasets, improved control over stylization compared to competitive alternatives, and successful applications to image manipulation tasks like colorization and rendering changes under different environments. Overall, our work provides valuable insights into the use of GANs for style separation and synthesis, opening up exciting possibilities for enhancing creativity tools in computer graphics, art, and beyond.",1
"The impressive success of Generative Adversarial Networks (GANs) is often overshadowed by the difficulties in their training. Despite the continuous efforts and improvements, there are still open issues regarding their convergence properties. In this paper, we propose a simple training variation where suitable weights are defined and assist the training of the Generator. We provide theoretical arguments why the proposed algorithm is better than the baseline training in the sense of speeding up the training process and of creating a stronger Generator. Performance results showed that the new algorithm is more accurate in both synthetic and image datasets resulting in improvements ranging between 5% and 50%.",0
"Effective training of generative adversarial networks (GANs) relies on careful selection of hyperparameters that balance stability, convergence speed, and sample quality. One such parameter is the weight decay term, which can prevent overfitting but may slow down convergence if set too high. In this work, we analyze the effectiveness of different values for the weight decay term, comparing model performance using metrics such as Frechet Inception Distance (FID), FrÃ©chet Average Precision (AP), Peak Signal-to-Noise Ratio (PSNR), Mean Absolute Error (MAE) against the ground truth images used during testing. Our results show that low levels of weight decay lead to faster convergence at the cost of reduced FID scores, while higher weights result in improved image fidelity but longer training times. Importantly, we demonstrate that the choice of weight decay has only a modest impact on overall model performance for GAN training, highlighting the importance of other factors such as network architecture, data preprocessing, batch size, and learning rate choices that have been found in recent literature to more significantly affect generation accuracy. This study provides a comprehensive analysis of how one particular regularization technique behaves during training and underscores the need for further investigation into optimal GAN training regimes in order to fully realize their potential for creating realistic synthetic datasets from diverse distributions of input variables. Keywords:Generative adversarial networks, training parameters, weight decay, convolutional neural networks, deep learning. Training generative adversarial networks (GANs) remains a challenging task due to the need to carefully select hyperparameters that balance stability, convergence speed, and sample quality. One important factor to consider is the selection of an appropriate weight decay term, which helps prevent overfitting by adding a small penalty to the loss function proportional to th",1
"Video style transfer is a useful component for applications such as augmented reality, non-photorealistic rendering, and interactive games. Many existing methods use optical flow to preserve the temporal smoothness of the synthesized video. However, the estimation of optical flow is sensitive to occlusions and rapid motions. Thus, in this work, we introduce a novel evolve-sync loss computed by evolvements to replace optical flow. Using this evolve-sync loss, we build an adversarial learning framework, termed as Video Style Transfer Generative Adversarial Network (VST-GAN), which improves upon the MGAN method for image style transfer for more efficient video style transfer. We perform extensive experimental evaluations of our method and show quantitative and qualitative improvements over the state-of-the-art methods.",0
"This paper presents a novel approach to video style transfer using adversarial learning constrained by evolutionary principles. We propose a model that incorporates gradient ascent techniques inspired by natural selection to generate high quality stylized outputs. Our method builds upon existing work in video style transfer by introducing new loss functions designed to maximize both content preservation and stylistic similarity. Experimental results demonstrate significant improvements over state-of-the-art methods on several benchmark datasets, including both quantitative metrics and human evaluations. Additionally, we provide detailed analysis and visual comparisons illustrating our algorithm's effectiveness in generating diverse yet coherent output styles while maintaining semantic understanding of input videos. Overall, our contributions represent a major step forward in advancing the field of video style transfer research.",1
"Generative Adversarial Networks (GANs) have a great performance in image generation, but they need a large scale of data to train the entire framework, and often result in nonsensical results. We propose a new method referring to conditional GAN, which equipments the latent noise with mixture of Student's t-distribution with attention mechanism in addition to class information. Student's t-distribution has long tails that can provide more diversity to the latent noise. Meanwhile, the discriminator in our model implements two tasks simultaneously, judging whether the images come from the true data distribution, and identifying the class of each generated images. The parameters of the mixture model can be learned along with those of GANs. Moreover, we mathematically prove that any multivariate Student's t-distribution can be obtained by a linear transformation of a normal multivariate Student's t-distribution. Experiments comparing the proposed method with typical GAN, DeliGAN and DCGAN indicate that, our method has a great performance on generating diverse and legible objects with limited data.",0
Tables should be mentioned if they contain important data that the reader would want to know at first glance without reading through text descriptions,1
"We developed a new class of physics-informed generative adversarial networks (PI-GANs) to solve in a unified manner forward, inverse and mixed stochastic problems based on a limited number of scattered measurements. Unlike standard GANs relying only on data for training, here we encoded into the architecture of GANs the governing physical laws in the form of stochastic differential equations (SDEs) using automatic differentiation. In particular, we applied Wasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability compared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian processes of different correlation lengths based on data realizations collected from simultaneous reads at sparsely placed sensors. We obtained good approximation of the generated stochastic processes to the target ones even for a mismatch between the input noise dimensionality and the effective dimensionality of the target stochastic processes. We also studied the overfitting issue for both the discriminator and generator, and we found that overfitting occurs also in the generator in addition to the discriminator as previously reported. Subsequently, we considered the solution of elliptic SDEs requiring approximations of three stochastic processes, namely the solution, the forcing, and the diffusion coefficient. We used three generators for the PI-GANs, two of them were feed forward deep neural networks (DNNs) while the other one was the neural network induced by the SDE. Depending on the data, we employed one or multiple feed forward DNNs as the discriminators in PI-GANs. Here, we have demonstrated the accuracy and effectiveness of PI-GANs in solving SDEs for up to 30 dimensions, but in principle, PI-GANs could tackle very high dimensional problems given more sensor data with low-polynomial growth in computational cost.",0
"Abstract: This work presents a novel approach to solving stochastic differential equations using physics informed generative adversarial networks (PhIGANs). Solving stochastic differential equations can be challenging due to their high dimensionality, complex structures, and nonlinearities, making traditional methods like Monte Carlo simulations computationally expensive and sometimes impractical. To address these limitations, we propose PhIGANs, which combine the strengths of deep learning algorithms and physical knowledge into one framework. By incorporating prior knowledge from physics into the training process, our method significantly improves accuracy over existing approaches while reducing computational costs. We demonstrate the effectiveness of our approach on several examples, including a double-well problem and the Korteweg-de Vries equation, showing that our model accurately predicts statistical properties of chaotic systems even without access to large amounts of data. Our findings pave the way towards efficient solutions for real world applications that involve uncertainty and randomness, such as in finance, engineering, and climate science. Keywords: generative adversarial networks, stochastic processes, physics informatics, partial differential equations",1
"Transforming a thermal infrared image into a realistic RGB image is a challenging task. In this paper we propose a deep learning method to bridge this gap. We propose learning the transformation mapping using a coarse-to-fine generator that preserves the details. Since the standard mean squared loss cannot penalize the distance between colorized and ground truth images well, we propose a composite loss function that combines content, adversarial, perceptual and total variation losses. The content loss is used to recover global image information while the latter three losses are used to synthesize local realistic textures. Quantitative and qualitative experiments demonstrate that our approach significantly outperforms existing approaches.",0
"This paper focuses on improving thermal infrared imaging technology by developing a colorization method using conditional generative adversarial networks (cGANs). With cGANs, high-resolution visible light images can be generated from low-quality thermal infrared inputs, resulting in more vivid and intuitive visualizations that could potentially enhance remote sensing applications such as security monitoring, firefighting, and surveillance. Our approach first projects the thermal image into three RGB channels, which are then fed through two discriminators that aim to distinguish real RGB images from fake ones produced by our generator network. We trained our cGAN model on publicly available datasets, achieving promising results in generating accurate and consistent colorized outputs. Further research is required to optimize performance and ensure robustness under different environmental conditions. Nonetheless, this study represents a significant advancement towards enhancing the usability and effectiveness of thermal infrared imaging technologies.",1
"Suppressing bones on chest X-rays such as ribs and clavicle is often expected to improve pathologies classification. These bones can interfere with a broad range of diagnostic tasks on pulmonary disease except for musculoskeletal system. Current conventional method for acquisition of bone suppressed X-rays is dual energy imaging, which captures two radiographs at a very short interval with different energy levels; however, the patient is exposed to radiation twice and the artifacts arise due to heartbeats between two shots. In this paper, we introduce a deep generative model trained to predict bone suppressed images on single energy chest X-rays, analyzing a finite set of previously acquired dual energy chest X-rays. Since the relatively small amount of data is available, such approach relies on the methodology maximizing the data utilization. Here we integrate the following two approaches. First, we use a conditional generative adversarial network that complements the traditional regression method minimizing the pairwise image difference. Second, we use Haar 2D wavelet decomposition to offer a perceptual guideline in frequency details to allow the model to converge quickly and efficiently. As a result, we achieve state-of-the-art performance on bone suppression as compared to the existing approaches with dual energy chest X-rays.",0
"In medical imaging, bone suppression techniques are used to enhance soft tissue visibility by reducing the intensity of bones on radiographs. This can improve diagnostic accuracy and treatment planning for diseases such as pneumonia and breast cancer. Traditional methods for bone suppression involve combining images acquired at different energies (i.e., dual energy), but these methods suffer from limitations such as artifacts and reduced contrast. Here we propose a novel approach based on adversarial networks that learns to suppress bones while preserving soft tissue details on dual energy chest x-rays. Our method leverages the discriminative power of convolutional neural networks (CNNs) along with a generative adversarial network (GAN). We train two CNNs: one for image synthesis and another for classification, both guided by a GAN trained to preserve soft tissue features. Our experimental results demonstrate improved performance compared to traditional methods across various metrics including visual inspection by experts. Overall, our approach represents a significant advance in bone suppression, enabling better diagnosis and management of clinical conditions involving soft tissues obscured by bone shadows.",1
"Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution. There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control. In this paper, we bridge these two objectives and introduce the concept of cross-domain disentanglement. We aim to separate the internal representation into three parts. The shared part contains information for both domains. The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain. We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. Our model offers multiple advantages. We can output diverse samples covering multiple modes of the distributions of both domains, perform domain-specific image transfer and interpolation, and cross-domain retrieval without the need of labeled data, only paired images. We compare our model to the state-of-the-art in multi-modal image translation and achieve better results for translation on challenging datasets as well as for cross-domain retrieval on realistic datasets.",0
"This paper presents a new approach to image-to-image translation that allows for cross-domain disentanglement. By leveraging advances in generative adversarial networks (GANs), we can generate high quality images that maintain their domain-specific features while being translated into another domain. Our method uses a novel architecture that allows for more effective control over the generation process and produces results that outperform existing state-of-the art methods. We evaluate our method on several benchmark datasets and demonstrate its ability to accurately translate across domains while preserving important characteristics of the original images. Overall, our work represents a significant contribution to the field of image-to-image translation and has potential applications in areas such as computer vision, graphics, and multimedia processing.",1
"We propose two new techniques for training Generative Adversarial Networks (GANs). Our objectives are to alleviate mode collapse in GAN and improve the quality of the generated samples. First, we propose neighbor embedding, a manifold learning-based regularization to explicitly retain local structures of latent samples in the generated samples. This prevents generator from producing nearly identical data samples from different latent samples, and reduces mode collapse. We propose an inverse t-SNE regularizer to achieve this. Second, we propose a new technique, gradient matching, to align the distributions of the generated samples and the real samples. As it is challenging to work with high-dimensional sample distributions, we propose to align these distributions through the scalar discriminator scores. We constrain the difference between the discriminator scores of the real samples and generated ones. We further constrain the difference between the gradients of these discriminator scores. We derive these constraints from Taylor approximations of the discriminator function. We perform experiments to demonstrate that our proposed techniques are computationally simple and easy to be incorporated in existing systems. When Gradient matching and Neighbour embedding are applied together, our GN-GAN achieves outstanding results on 1D/2D synthetic, CIFAR-10 and STL-10 datasets, e.g. FID score of $30.80$ for the STL-10 dataset. Our code is available at: https://github.com/tntrung/gan",0
"GANs (Generative Adversarial Networks) have recently emerged as powerful models for generating realistic images, videos, audio signals, and other kinds of data. However, training these networks remains challenging due to instability issues, which often lead to unsatisfactory results. In this paper, we propose two new techniques that significantly improve the performance of GANs: neighbors embedding and gradient matching. Our first contribution is to introduce a novel neighborhood sampling technique based on local linear embeddings, which better preserves important features from the training set distribution compared to existing methods. Our second contribution is to apply gradient matching by adding a regularization term to both generator and discriminator loss functions, which encourages alignment between network gradients and thereby promotes more stable and efficient convergence during optimization. We demonstrate the effectiveness of our approach through extensive experiments on several benchmark datasets, showing that our model consistently achieves state-of-the-art quantitative evaluation metrics while producing high-quality visual outputs. Overall, our work highlights the potential of combining advanced embedding techniques with gradient manipulation to produce cutting-edge generative models capable of creating diverse and coherent samples that faithfully capture the underlying data structure.",1
"Convolutional neural network (CNN) based methods have recently achieved great success for image super-resolution (SR). However, most deep CNN based SR models attempt to improve distortion measures (e.g. PSNR, SSIM, IFC, VIF) while resulting in poor quantified perceptual quality (e.g. human opinion score, no-reference quality measures such as NIQE). Few works have attempted to improve the perceptual quality at the cost of performance reduction in distortion measures. A very recent study has revealed that distortion and perceptual quality are at odds with each other and there is always a trade-off between the two. Often the restoration algorithms that are superior in terms of perceptual quality, are inferior in terms of distortion measures. Our work attempts to analyze the trade-off between distortion and perceptual quality for the problem of single image SR. To this end, we use the well-known SR architecture-enhanced deep super-resolution (EDSR) network and show that it can be adapted to achieve better perceptual quality for a specific range of the distortion measure. While the original network of EDSR was trained to minimize the error defined based on per-pixel accuracy alone, we train our network using a generative adversarial network framework with EDSR as the generator module. Our proposed network, called enhanced perceptual super-resolution network (EPSR), is trained with a combination of mean squared error loss, perceptual loss, and adversarial loss. Our experiments reveal that EPSR achieves the state-of-the-art trade-off between distortion and perceptual quality while the existing methods perform well in either of these measures alone.",0
"In recent years, computer vision has made significant advances in high resolution image generation through super-resolution techniques. These methods aim to enhance the spatial resolution of low-quality images by synthesizing new pixels that better align with the underlying structure present in the original image. However, there remains a tradeoff between perception and distortion - as the resolution increases, so too can artifacts and other imperfections that detract from the overall quality of the output. To address this issue, we propose the use of enhanced perceptual super-resolution networks (EPSRN) which incorporate deep learning techniques and human visual systems models to generate more visually pleasing and accurate results. Our method outperforms existing state-of-the-art techniques on standard benchmark datasets while offering improved performance in terms of both quantitative and qualitative metrics. This work represents a significant step forward in the field of high-resolution image generation and has broad applications across many domains including medical imaging, surveillance, and entertainment.",1
"Conditional Generative Adversarial Networks (cGANs) are generative models that can produce data samples ($x$) conditioned on both latent variables ($z$) and known auxiliary information ($c$). We propose the Bidirectional cGAN (BiCoGAN), which effectively disentangles $z$ and $c$ in the generation process and provides an encoder that learns inverse mappings from $x$ to both $z$ and $c$, trained jointly with the generator and the discriminator. We present crucial techniques for training BiCoGANs, which involve an extrinsic factor loss along with an associated dynamically-tuned importance weight. As compared to other encoder-based cGANs, BiCoGANs encode $c$ more accurately, and utilize $z$ and $c$ more effectively and in a more disentangled way to generate samples.",0
"Artificial Intelligence (AI) has been gaining momentum over past few years. There have been significant advancements in areas such as computer vision, natural language processing, and machine learning. One particular area that is showing promise is GANs or Generative Adversarial Networks. These networks consist of two subnetworks: a generator network and a discriminator network. The objective is for one network to generate data that looks like real data while the other tries to identify which ones are fake. This leads to an adversarial setup where both networks improve each otherâ€™s performance. Researchers at XYZ Institute aimed to further advance these models by introducing bidirectionality into them. In traditional GANs, the generator generates new samples given random noise inputs. In contrast, bidirectional conditional GANs accept both noise input and class labels as conditioning variables, enabling the generation of specific types of samples with predefined attributes. Additionally, they proposed multi-scale attention modules to increase output diversity, allowing for more expressive image synthesis. They demonstrate the effectiveness of their model on three benchmark datasets: CelebA, LSUN Churches, and SUN RGBD. Experimental results show promising improvements against state-of-the-art methods. Overall, bidirectional conditional GANs hold great potential for generating high quality images based on textual descriptions with finer control over output details. This research opens up exciting possibilities for generative modelling, particularly in domains involving diverse visual outputs.",1
"An important class of distance metrics proposed for training generative adversarial networks (GANs) is the integral probability metric (IPM), in which the neural net distance captures the practical GAN training via two neural networks. This paper investigates the minimax estimation problem of the neural net distance based on samples drawn from the distributions. We develop the first known minimax lower bound on the estimation error of the neural net distance, and an upper bound tighter than an existing bound on the estimator error for the empirical neural net distance. Our lower and upper bounds match not only in the order of the sample size but also in terms of the norm of the parameter matrices of neural networks, which justifies the empirical neural net distance as a good approximation of the true neural net distance for training GANs in practice.",0
"In recent years, neural networks have become increasingly popular as they demonstrate strong performance across diverse domains. As these models gain prominence, ensuring their reliability is crucial, especially when deployed in mission-critical applications. Quantifying the uncertainty in predictions remains challenging due to their complexity. This research contributes to understanding how uncertainties propagate through deep learning pipelines by examining minimax estimation of neural network distance.  Traditional approaches rely on statistical tools like maximum likelihood, mean squared error, cross validation, etc., which fail to account for nonlinearities inherent in modern neural networks. Consequently, we employ robust optimization principles and adapt minimax theory from game theory to estimate these distances. By doing so, our approach provides tighter lower bounds that capture worst-case scenarios more accurately than existing techniques. We validate our framework using real-world datasets (MNIST, CIFAR-10, SVHN) commonly used for benchmarking and conduct extensive experiments against other state-of-the-art methods under varying conditions: regularization strengths, batch sizes, architectures, and datasets. Results indicate significantly better accuracy compared to alternative baseline methods. Our study addresses both theoretical foundations and practical aspects relevant to researchers working on Bayesian neural nets, active learning, adversarial attacks, robotics, computer vision, etc. Since overestimation could lead to subpar results when deploying defenses, our work enables more effective strategies that leverage uncertainty quantification. Overall, we provide a novel perspective on uncertainty assessment that generalizes across different disciplines applying deep learning algorithms.",1
"We present a new latent model of natural images that can be learned on large-scale datasets. The learning process provides a latent embedding for every image in the training dataset, as well as a deep convolutional network that maps the latent space to the image space. After training, the new model provides a strong and universal image prior for a variety of image restoration tasks such as large-hole inpainting, superresolution, and colorization. To model high-resolution natural images, our approach uses latent spaces of very high dimensionality (one to two orders of magnitude higher than previous latent image models). To tackle this high dimensionality, we use latent spaces with a special manifold structure (convolutional manifolds) parameterized by a ConvNet of a certain architecture. In the experiments, we compare the learned latent models with latent models learned by autoencoders, advanced variants of generative adversarial networks, and a strong baseline system using simpler parameterization of the latent space. Our model outperforms the competing approaches over a range of restoration tasks.",0
"Title: Latent Convolutional Models (full length) Abstract This paper presents Latent Convolutional Models (LCM), a novel architecture for deep learning that extends traditional convolutional networks by adding latent variables. LCMs provide a flexible and powerful framework that can model complex relationships among input features and their underlying structure, while allowing efficient inference and prediction. We demonstrate the effectiveness of our approach on several benchmark datasets including MNIST, CIFAR-10, ImageNet, and NLP tasks such as language translation and sentiment analysis. Our results show that LCMs achieve state-of-the-art performance across all datasets while providing interpretable insights into the learned representations. Furthermore, we conduct ablation studies to investigate the contributions of different components within the LCM architecture and evaluate the robustness of our models under various settings. Overall, our work contributes new tools for understanding and advancing artificial intelligence, paving the way towards more transparent and explainable machine learning systems.",1
"Generative adversarial networks (GANs) are innovative techniques for learning generative models of complex data distributions from samples. Despite remarkable recent improvements in generating realistic images, one of their major shortcomings is the fact that in practice, they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the main focus of several recent advances in GANs. Yet there is little understanding of why mode collapse happens and why existing approaches are able to mitigate mode collapse. We propose a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We borrow analysis tools from binary hypothesis testing---in particular the seminal result of Blackwell [Bla53]---to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggests that packing provides significant improvements in practice as well.",0
"In recent years, Generative Adversarial Networks (GANs) have emerged as one of the most powerful frameworks for generating realistic images, audio and other types of data. However, training GANs remains challenging due to instability issues such as mode collapse, which occur when one generator learns to map all inputs to the same output distribution. This can lead to overfitting and poor quality outputs. One approach to address these issues is through the use of regularization techniques that stabilize the training process. In this work, we propose a novel method called PacGAN that combines two popular regularization methods - the pixel constraint loss and feature matching loss. Our experiments show that using both losses together leads to significant improvements over each individual loss alone. We demonstrate the effectiveness of our model on several benchmark datasets including CIFAR-10 and STL-10, achieving state-of-the art results while reducing noise in generated images compared to previous approaches. Overall, this study highlights the importance of combining different regularizations to achieve better performance in GANs.",1
"Facial caricature is an art form of drawing faces in an exaggerated way to convey humor or sarcasm. In this paper, we propose the first Generative Adversarial Network (GAN) for unpaired photo-to-caricature translation, which we call ""CariGANs"". It explicitly models geometric exaggeration and appearance stylization using two components: CariGeoGAN, which only models the geometry-to-geometry transformation from face photos to caricatures, and CariStyGAN, which transfers the style appearance from caricatures to face photos without any geometry deformation. In this way, a difficult cross-domain translation problem is decoupled into two easier tasks. The perceptual study shows that caricatures generated by our CariGANs are closer to the hand-drawn ones, and at the same time better persevere the identity, compared to state-of-the-art methods. Moreover, our CariGANs allow users to control the shape exaggeration degree and change the color/texture style by tuning the parameters or giving an example caricature.",0
"CariGANs were proposed by researchers at UC Berkeley as a new method for creating photo-realistic caricatures from photos using generative adversarial networks (GANs). These GANs consist of a generator that maps facial features into exaggerated caricatures, and a discriminator that distinguishes real faces from generated ones. CariGANs achieve state-of-the-art results compared to traditional methods such as manual drawing, which require significant time and expertise, and other automatic methods that typically result in limited control over stylization. With these impressive results, CariGANs have potential applications in entertainment industries like video games and animation, as well as medical diagnosis where subtle changes in facial expression can indicate certain conditions.",1
"Image quality measurement is a critical problem for image super-resolution (SR) algorithms. Usually, they are evaluated by some well-known objective metrics, e.g., PSNR and SSIM, but these indices cannot provide suitable results in accordance with the perception of human being. Recently, a more reasonable perception measurement has been proposed in [1], which is also adopted by the PIRM-SR 2018 challenge. In this paper, motivated by [1], we aim to generate a high-quality SR result which balances between the two indices, i.e., the perception index and root-mean-square error (RMSE). To do so, we design a new deep SR framework, dubbed Bi-GANs-ST, by integrating two complementary generative adversarial networks (GAN) branches. One is memory residual SRGAN (MR-SRGAN), which emphasizes on improving the objective performance, such as reducing the RMSE. The other is weight perception SRGAN (WP-SRGAN), which obtains the result that favors better subjective perception via a two-stage adversarial training mechanism. Then, to produce final result with excellent perception scores and RMSE, we use soft-thresholding method to merge the results generated by the two GANs. Our method performs well on the perceptual image super-resolution task of the PIRM 2018 challenge. Experimental results on five benchmarks show that our proposal achieves highly competent performance compared with other state-of-the-art methods.",0
"This paper presents a new architecture for perceptual image super-resolution using bi-directional generative adversarial networks (BiGANs). We introduce a novel module called Stacked Residual Pyramidal Adversarial Networks (SRPAN) that improves upon existing GAN architectures by incorporating residual connections and pyramidal features extraction to enhance the model capacity and stability. Our method performs well on benchmark datasets, achieving state-of-the-art results while requiring fewer parameters than competing methods. We conduct ablation studies to demonstrate the importance of each component in our design, and we provide qualitative comparisons to illustrate the effectiveness of our approach. Overall, our work represents an important step forward in developing accurate and efficient image super-resolution models based on deep learning techniques.",1
"In this paper we investigate the feasibility of using synthetic data to augment face datasets. In particular, we propose a novel generative adversarial network (GAN) that can disentangle identity-related attributes from non-identity-related attributes. This is done by training an embedding network that maps discrete identity labels to an identity latent space that follows a simple prior distribution, and training a GAN conditioned on samples from that distribution. Our proposed GAN allows us to augment face datasets by generating both synthetic images of subjects in the training set and synthetic images of new subjects not in the training set. By using recent advances in GAN training, we show that the synthetic images generated by our model are photo-realistic, and that training with augmented datasets can indeed increase the accuracy of face recognition models as compared with models trained with real images alone.",0
"In recent years, face recognition has become increasingly important in many applications such as surveillance, authentication, and personalized services. However, training high-performance face recognition systems requires large amounts of annotated data, which can be time-consuming and expensive to collect. In this study, we propose a method for generating photo-realistic synthetic facial images that can be used to augment existing datasets and improve face recognition accuracy. Our approach combines state-of-the-art generative models with fine-grained controls to create realistic variations of facial features, expressions, poses, and lighting conditions. We evaluate our method on several benchmarks using commonly adopted metrics and demonstrate significant improvements over previous methods in terms of visual quality, diversity, and recognition performance. Our results show that incorporating generated training data into face recognition pipelines can reduce errors and enhance robustness across different scenarios. This work paves the way for more efficient and effective solutions for building accurate and reliable face recognition systems.",1
"Ideally, what confuses neural network should be confusing to humans. However, recent experiments have shown that small, imperceptible perturbations can change the network prediction. To address this gap in perception, we propose a novel approach for learning robust classifier. Our main idea is: adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target. We formulate a problem of learning robust classifier in the framework of Generative Adversarial Networks (GAN), where the adversarial attack on classifier acts as a generator, and the critic network learns to distinguish between regular and adversarial images. The classifier cost is augmented with the objective that its adversarial examples should confuse the adversary critic. To improve the stability of the adversarial mapping, we introduce adversarial cycle-consistency constraint which ensures that the adversarial mapping of the adversarial examples is close to the original. In the experiments, we show the effectiveness of our defense. Our method surpasses in terms of robustness networks trained with adversarial training. Additionally, we verify in the experiments with human annotators on MTurk that adversarial examples are indeed visually confusing. Codes for the project are available at https://github.com/aam-at/adversary_critic.",0
"This should be more formal than our previous discussions as if I am trying to impress reviewers by highlighting the key contributions of your research work:  In recent years, adversarial attacks have become increasingly prevalent, raising concerns over the robustness of modern deep learning systems. While defenses against these attacks exist, they often come at a significant cost in terms of accuracy or computational efficiency. To address this challenge, we propose a novel approach based on adversary critics that improves network robustness without compromising performance. Our method leverages critic networks trained to predict the output of the attacked model. By minimizing the error of the critic network during training, the attacked model becomes resilient to both unseen and adaptive adversaries. We demonstrate the effectiveness of our approach through extensive experiments on image classification tasks under different attack settings. Our results show that adversary critics significantly enhance the robustness of popular models such as ResNet and DenseNet while maintaining their high accuracy. Furthermore, we provide an analysis of the trade-offs between defense strength and the impact on model performance. These findings suggest that our method is a promising direction towards achieving robust machine learning models that can operate reliably in real-world scenarios. Overall, our contribution lies in developing an efficient and effective solution for enhancing network robustness against adversarial attacks that has broad applications across multiple domains.",1
"We address the problem of segmenting 3D multi-modal medical images in scenarios where very few labeled examples are available for training. Leveraging the recent success of adversarial learning for semi-supervised segmentation, we propose a novel method based on Generative Adversarial Networks (GANs) to train a segmentation model with both labeled and unlabeled images. The proposed method prevents over-fitting by learning to discriminate between true and fake patches obtained by a generator network. Our work extends current adversarial learning approaches, which focus on 2D single-modality images, to the more challenging context of 3D volumes of multiple modalities. The proposed method is evaluated on the problem of segmenting brain MRI from the iSEG-2017 and MRBrainS 2013 datasets. Significant performance improvement is reported, compared to state-of-art segmentation networks trained in a fully-supervised manner. In addition, our work presents a comprehensive analysis of different GAN architectures for semi-supervised segmentation, showing recent techniques like feature matching to yield a higher performance than conventional adversarial training approaches. Our code is publicly available at https://github.com/arnab39/FewShot_GAN-Unet3D",0
"Title: ""Generative Adversarial Learning for Improved Accuracy in Multi-Modal Medical Image Segmentation"" Abstract: This paper proposes the use of generative adversarial learning (GAN) as a method for improving accuracy in multi-modal medical image segmentation tasks, particularly those requiring few labeled images. Traditional image segmentation methods rely heavily on large amounts of annotated data, but this can be difficult to obtain in some medical domains due to ethical and privacy concerns. GANs have shown promise in generating high-quality synthetic samples that can be used to augment small datasets, providing more training data for machine learning models. However, these approaches typically only consider two modalities at once. In our work, we extend existing GAN techniques to handle multiple imaging modalities simultaneously and show that our approach leads to improved segmentation accuracy compared to other state-of-the-art techniques. We evaluate our method on a number of publicly available benchmark datasets and demonstrate its effectiveness in improving accuracy while reducing the need for vast amounts of annotations.",1
"Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in \cite{goodfellow2014generative}.",0
"GANs (Generative Adversarial Networks) have been shown to be very powerful models that can generate images, music, text, and other types of data that are often indistinguishable from real ones. While there has been much research on improving the performance of GANs, little attention has been paid to understanding how different design choices affect their capabilities. This study aims to address this gap by conducting a large-scale investigation into how varying parameters such as architecture size, loss functions, training objectives, regularization techniques, and hyperparameters impact GAN performance. Using a diverse dataset, we evaluate multiple variants of popular GAN architectures under controlled conditions, providing insights into which combinations work well across different domains and tasks. Our results reveal surprisingly strong correlations between some parameter settings, challenging conventional wisdom on which configurations are most effective. Overall, our findings provide valuable guidance for practitioners seeking to build high-quality GANs tailored to specific applications.",1
"Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.",0
"Evaluating generative models can be challenging due to their complexity and diversity. In order to assess these models effectively, we need metrics that capture both precision and recall. To achieve this, we propose using two complementary methods: cross-entropy loss and mean squared error (MSE). Cross entropy measures how well a model predicts the correct class labels compared to other classes. MSE, on the other hand, evaluates the distance between generated samples and ground truth data points in feature space. Our experiments demonstrate the effectiveness of combining these metrics in evaluating generative models across various domains, including images, text, and speech signals. Additionally, we provide insights into interpreting the results from each metric and discuss limitations and potential future work. This study contributes towards advancing the state of art in generative model evaluation, paving the way for more accurate and meaningful applications.",1
"Generative adversarial network (GAN) is a minimax game between a generator mimicking the true model and a discriminator distinguishing the samples produced by the generator from the real training samples. Given an unconstrained discriminator able to approximate any function, this game reduces to finding the generative model minimizing a divergence measure, e.g. the Jensen-Shannon (JS) divergence, to the data distribution. However, in practice the discriminator is constrained to be in a smaller class $\mathcal{F}$ such as neural nets. Then, a natural question is how the divergence minimization interpretation changes as we constrain $\mathcal{F}$. In this work, we address this question by developing a convex duality framework for analyzing GANs. For a convex set $\mathcal{F}$, this duality framework interprets the original GAN formulation as finding the generative model with minimum JS-divergence to the distributions penalized to match the moments of the data distribution, with the moments specified by the discriminators in $\mathcal{F}$. We show that this interpretation more generally holds for f-GAN and Wasserstein GAN. As a byproduct, we apply the duality framework to a hybrid of f-divergence and Wasserstein distance. Unlike the f-divergence, we prove that the proposed hybrid divergence changes continuously with the generative model, which suggests regularizing the discriminator's Lipschitz constant in f-GAN and vanilla GAN. We numerically evaluate the power of the suggested regularization schemes for improving GAN's training performance.",0
"In recent years, Generative Adversarial Networks (GANs) have revolutionized many fields by enabling the generation of realistic data from scratch. Despite their successes, however, training GANs remains challenging due to issues such as instability, mode collapse, and difficulty in evaluating performance. This paper presents a new framework based on convex duality that addresses these problems and improves the training of GANs. Our method uses a novel objective function that tightly upper bounds the optimal Jensen-Shannon divergence and leads to significant improvements over state-of-the-art methods. We showcase the effectiveness of our approach through extensive experimental results across multiple domains, including image synthesis, video generation, and domain adaptation. Overall, our work represents a step forward in the field of generative modeling and highlights the potential applications of stable and high-performing GANs.",1
"In this article, we introduce a new mode for training Generative Adversarial Networks (GANs). Rather than minimizing the distance of evidence distribution $\tilde{p}(x)$ and the generative distribution $q(x)$, we minimize the distance of $\tilde{p}(x_r)q(x_f)$ and $\tilde{p}(x_f)q(x_r)$. This adversarial pattern can be interpreted as a Turing test in GANs. It allows us to use information of real samples during training generator and accelerates the whole training procedure. We even find that just proportionally increasing the size of discriminator and generator, it succeeds on 256x256 resolution without adjusting hyperparameters carefully.",0
"Abstract: Artificial intelligence (AI) has been making rapid strides over the past few years, owing to advancements in machine learning techniques such as generative adversarial networks (GANs). GANs have proven immensely valuable in generating synthetic data that can closely mimic real-world samples across multiple domains including images, videos, audio, natural language text, etc. Despite their effectiveness, training stable and reliable GANs continues to pose significant challenges due to their unstable nature during optimization. In this study, we present a new technique called ""Turing Test-based"" optimization to train robust and highly competitive GANs through self-supervised adversarial feedback. By incorporating elements from human evaluation into our framework, we aim to enhance GAN performance by reducing interference among different discriminator components and encouraging more diverse generator behavior. Our experiments using several benchmark datasets demonstrated the efficacy of our approach. We envision that these findings could form the foundation for future work towards developing even more advanced AI systems capable of performing complex tasks.",1
"Building footprint information is an essential ingredient for 3-D reconstruction of urban models. The automatic generation of building footprints from satellite images presents a considerable challenge due to the complexity of building shapes. In this work, we have proposed improved generative adversarial networks (GANs) for the automatic generation of building footprints from satellite images. We used a conditional GAN with a cost function derived from the Wasserstein distance and added a gradient penalty term. The achieved results indicated that the proposed method can significantly improve the quality of building footprint generation compared to conditional generative adversarial networks, the U-Net, and other networks. In addition, our method nearly removes all hyperparameters tuning.",0
"Artificial intelligence (AI) has been rapidly advancing over recent years and finding new applications in different fields such as computer vision. In particular, generative adversarial networks (GANs) have gained significant attention due to their ability to generate realistic data samples from noise. This paper focuses on the application of GANs for building footprint generation using aerial images. To achieve that goal, we propose improving the architecture and training process of the GAN model by utilizing multiple datasets during training. Furthermore, our proposed approach includes fine-tuning the discriminator network to enhance its performance in distinguishing real images from generated ones. We evaluate our method's effectiveness through quantitative and qualitative experiments and demonstrate its ability to generate high-quality building footprint maps. Our findings show that improved GAN models can effectively learn the underlying patterns present in aerial images and accurately predict building outlines without human supervision. The contributions of our work lie in developing an innovative algorithm that generates accurate building footprints using advanced machine learning techniques, which could potentially benefit urban planners, emergency responders, and other stakeholders interested in analyzing large areas efficiently. As future research directions, we aim at extending our work to incorporate other remote sensing imagery sources such as satellite images and synthetic aperture radar data.",1
"One of the biggest issues facing the use of machine learning in medical imaging is the lack of availability of large, labelled datasets. The annotation of medical images is not only expensive and time consuming but also highly dependent on the availability of expert observers. The limited amount of training data can inhibit the performance of supervised machine learning algorithms which often need very large quantities of data on which to train to avoid overfitting. So far, much effort has been directed at extracting as much information as possible from what data is available. Generative Adversarial Networks (GANs) offer a novel way to unlock additional information from a dataset by generating synthetic samples with the appearance of real images. This paper demonstrates the feasibility of introducing GAN derived synthetic data to the training datasets in two brain segmentation tasks, leading to improvements in Dice Similarity Coefficient (DSC) of between 1 and 5 percentage points under different conditions, with the strongest effects seen fewer than ten training image stacks are available.",0
"Title: Enhancing Training Datasets through Generative Adversarial Networks (GANs)  Abstract:  Limited training data can greatly hinder the performance of machine learning algorithms. One solution to address this issue is by generating additional data synthetically. Recent advancements in generative models have led to the development of generative adversarial networks (GANs), which consistently produce high-quality synthetic samples that closely match real data distributions. These generated samples have been shown to significantly improve model performance. This study investigates the effectiveness of augmenting training datasets with synthesized data produced from GANs. We present experimental results demonstrating how incorporating these GAN-generated samples leads to more accurate predictions compared to traditional training methods. Additionally, we analyze factors such as dataset size, architecture complexity, and the role of regularization techniques on final model performance. Our findings suggest that utilizing GANs to generate new training data presents an effective approach towards achieving better model accuracy even when limited labeled data exists. ------------------------------",1
"Calibrating sports cameras is important for autonomous broadcasting and sports analysis. Here we propose a highly automatic method for calibrating sports cameras from a single image using synthetic data. First, we develop a novel camera pose engine. The camera pose engine has only three significant free parameters so that it can effectively generate a lot of camera poses and corresponding edge (i.e, field marking) images. Then, we learn compact deep features via a siamese network from paired edge image and camera pose and build a feature-pose database. After that, we use a novel two-GAN (generative adversarial network) model to detect field markings in real images. Finally, we query an initial camera pose from the feature-pose database and refine camera poses using truncated distance images. We evaluate our method on both synthetic and real data. Our method not only demonstrates the robustness on the synthetic data but also achieves the state-of-the-art accuracy on a standard soccer dataset and very high performance on a volleyball dataset.",0
"In recent years, advances in computer vision have enabled automatic calibration of sports cameras in applications such as broadcasting and virtual reality (VR). These methods typically rely on costly manual labor for camera setup and calibration data collection. To address these limitations, we present a synthetic dataset and pipeline that enables fully automated sports camera calibration without relying on real data. Our approach utilizes state-of-the-art 3D reconstruction techniques to generate realistic synthetic datasets, which can then be used to train machine learning models for calibrating sports cameras. We evaluate our method on publicly available basketball sequences from TV broadcasts and demonstrate significantly improved calibration accuracy compared to existing methods. Furthermore, we show that our system is capable of producing results similar to those obtained using manually collected data while substantially reducing the required resources and time overhead. By leveraging synthetic data, we aim to enable widespread adoption of automatic camera calibration for sports video production and VR content creation. This work presents new opportunities for researchers and practitioners to explore novel approaches for generating high-quality synthetic data, enabling the development of future systems for a variety of applications beyond camera calibration. Ultimately, our work demonstrates the potential benefits of using synthetic data to overcome challenges in multimedia computing, including the need for large amounts of annotated training data and reliable capture conditions. Our contributions pave the way for further investigation into artificial intelligence applications across diverse fields where acquiring real data can be difficult or impractical. --end--",1
"Generative adversarial networks (GANs) and other adversarial methods are based on a game-theoretical perspective on joint optimization of two neural networks as players in a game. Adversarial techniques have been extensively used to synthesize and analyze biomedical images. We provide an introduction to GANs and adversarial methods, with an overview of biomedical image analysis tasks that have benefited from such methods. We conclude with a discussion of strengths and limitations of adversarial methods in biomedical image analysis, and propose potential future research directions.",0
"Abstract: Biomedical imaging has become increasingly important in modern medicine, allowing healthcare professionals to noninvasively visualize internal structures and functions of living subjects. However, analyzing these images manually can be time consuming and subjective. In recent years, generative adversarial networks (GANs) have emerged as a powerful tool for automating many tasks related to medical image analysis, including segmentation, registration, and classification. GANs consist of two neural network architectures, one generating new data and another attempting to discriminate real from generated samples. Adversarial training encourages both components to improve each otherâ€™s performance through competition. In this review, we explore how GANs and adversarial methods have been applied in various biomedical image analysis applications. We highlight their potential benefits over traditional machine learning approaches, discuss relevant research findings, and summarize current challenges facing the field. By examining different GAN variants and loss objectives used by existing studies, we provide guidance on selecting appropriate models and evaluation metrics for specific biomedical image analysis problems. Our comprehensive overview aims to assist researchers in adopting effective GAN strategies for advancing medical diagnostics and treatments using artificial intelligence.",1
"Generative models, in particular generative adversarial networks (GANs), have received significant attention recently. A number of GAN variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field. In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models. I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.",0
"The evaluation of Generative Adversarial Networks (GANs) has become increasingly important as these models have gained popularity for their ability to generate realistic synthetic data. However, there is still ongoing debate over which evaluation measures are most effective at assessing the quality of generated outputs from GANs. In this paper, we explore both the pros and cons of several commonly used evaluation metrics for GANs such as Frechet Inception Distance (FID), Inception Score (IS), Perceptual Path Length (PPL), and Human Preference Based Metrics (HPBM). We provide a detailed analysis of each measure and discuss their strengths and weaknesses, including how they correlate with human perception and whether they can capture different aspects of generator performance. Our findings highlight that while no single metric is perfect, combining multiple measures may provide a more comprehensive understanding of the quality of GAN outputs. Overall, our work contributes to the broader conversation surrounding GAN evaluation and provides insights into which metrics may be most appropriate for specific use cases.",1
"Mammogram inspection in search of breast tumors is a tough assignment that radiologists must carry out frequently. Therefore, image analysis methods are needed for the detection and delineation of breast masses, which portray crucial morphological information that will support reliable diagnosis. In this paper, we proposed a conditional Generative Adversarial Network (cGAN) devised to segment a breast mass within a region of interest (ROI) in a mammogram. The generative network learns to recognize the breast mass area and to create the binary mask that outlines the breast mass. In turn, the adversarial network learns to distinguish between real (ground truth) and synthetic segmentations, thus enforcing the generative network to create binary masks as realistic as possible. The cGAN works well even when the number of training samples are limited. Therefore, the proposed method outperforms several state-of-the-art approaches. This hypothesis is corroborated by diverse experiments performed on two datasets, the public INbreast and a private in-house dataset. The proposed segmentation model provides a high Dice coefficient and Intersection over Union (IoU) of 94% and 87%, respectively. In addition, a shape descriptor based on a Convolutional Neural Network (CNN) is proposed to classify the generated masks into four mass shapes: irregular, lobular, oval and round. The proposed shape descriptor was trained on Digital Database for Screening Mammography (DDSM) yielding an overall accuracy of 80%, which outperforms the current state-of-the-art.",0
"Title: ""Breast tumor segmentation and shape classification in mammograms using generative adversarial networks""  Abstract: Artificial intelligence (AI) has shown great potential in supporting the diagnosis of breast cancer through the analysis of mammographic images. In particular, convolutional neural networks (CNNs) have been used effectively for tasks such as breast tumor detection and segmentation, which involve identifying and isolating regions of interest within these images. However, traditional CNN models can suffer from limitations such as limited generalization ability across different datasets, poor robustness against image variations and noise, and lack of explainability due to their deep architectures. To address these issues, we propose utilizing generative adversarial networks (GANs), which consist of two competing sub-networks that can generate realistic synthetic data while training. We evaluate the performance of our proposed model on several publicly available databases and compare its results with state-of-the-art methods. Our experimental findings demonstrate improved accuracy in both tumor segmentation and shape classification compared to existing techniques. These improvements could potentially lead to more accurate diagnoses of breast cancer, ultimately reducing mortality rates associated with this disease. Overall, our work highlights the potential benefits of integrating GANs into medical imaging applications and paves the way for further exploration into other healthcare domains where AI could provide valuable insights.",1
"We reconsider the training objective of Generative Adversarial Networks (GANs) from the mixed Nash Equilibria (NE) perspective. Inspired by the classical prox methods, we develop a novel algorithmic framework for GANs via an infinite-dimensional two-player game and prove rigorous convergence rates to the mixed NE, resolving the longstanding problem that no provably convergent algorithm exists for general GANs. We then propose a principled procedure to reduce our novel prox methods to simple sampling routines, leading to practically efficient algorithms. Finally, we provide experimental evidence that our approach outperforms methods that seek pure strategy equilibria, such as SGD, Adam, and RMSProp, both in speed and quality.",0
"This research focuses on finding mixed Nash equilibria (NE) of generative adversarial networks (GANs). GANs consist of two neural network models competing against each other: a generator creates samples such as images that try to fool a discriminator, which recognizes real data from generated data. Both models can benefit from learning more efficient policies than current methods based solely on gradient descent, but analyzing NE in games involving deep neural nets remains challenging due to nonlinearities and large solution spaces. We propose a novel method using randomized response theory and normalized gradients to find approximate pure and mixed NE efficiently without requiring explicit knowledge of the objective functions. In our experiments, we demonstrate that our approach discovers better NE than state-of-the-art baselines on both binary and CelebA datasets under different architectures for both the generator and discriminator. Our work contributes to understanding multiplayer decision making in deep reinforcement learning and has potential applications beyond GAN training, e.g., designing stable defense mechanisms against attacks over deep learning systems.",1
"Designing a logo is a long, complicated, and expensive process for any designer. However, recent advancements in generative algorithms provide models that could offer a possible solution. Logos are multi-modal, have very few categorical properties, and do not have a continuous latent space. Yet, conditional generative adversarial networks can be used to generate logos that could help designers in their creative process. We propose LoGAN: an improved auxiliary classifier Wasserstein generative adversarial neural network (with gradient penalty) that is able to generate logos conditioned on twelve different colors. In 768 generated instances (12 classes and 64 logos per class), when looking at the most prominent color, the conditional generation part of the model has an overall precision and recall of 0.8 and 0.7 respectively. LoGAN's results offer a first glance at how artificial intelligence can be used to assist designers in their creative process and open promising future directions, such as including more descriptive labels which will provide a more exhaustive and easy-to-use system.",0
"This paper presents a new approach to generating logos using a generative adversarial neural network (GAN) conditioned on color. The proposed method, called LoGAN, leverages the power of GANs to generate high-quality logo images that match specific input colors. We demonstrate that our model can effectively capture the semantic relationship between color and logo design, resulting in more accurate and diverse outputs compared to existing approaches. Our experiments show that LoGAN outperforms state-of-the-art methods across several benchmark datasets, achieving significant improvements in both quantitative metrics and subjective evaluations by human experts. Overall, our work provides a valuable contribution to the field of computer vision and demonstrates the potential of using deep learning techniques to create visually appealing and effective logo designs.",1
"Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. In this paper, we aim to provide an understanding of some of the basic issues surrounding GANs including their formulation, generalization and stability on a simple benchmark where the data has a high-dimensional Gaussian distribution. Even in this simple benchmark, the GAN problem has not been well-understood as we observe that existing state-of-the-art GAN architectures may fail to learn a proper generative distribution owing to (1) stability issues (i.e., convergence to bad local solutions or not converging at all), (2) approximation issues (i.e., having improper global GAN optimizers caused by inappropriate GAN's loss functions), and (3) generalizability issues (i.e., requiring large number of samples for training). In this setup, we propose a GAN architecture which recovers the maximum-likelihood solution and demonstrates fast generalization. Moreover, we analyze global stability of different computational approaches for the proposed GAN optimization and highlight their pros and cons. Finally, we outline an extension of our model-based approach to design GANs in more complex setups than the considered Gaussian benchmark.",0
"Title: Understanding GANs: The LQG Setting Abstract GANs (Generative Adversarial Networks) have become one of the most popular techniques used in the field of machine learning today due their ability to generate high quality synthetic data such as images, audio, and text. In recent years, researchers have proposed various modifications to the original GAN architecture aimed at improving stability, reducing mode collapse and producing more diverse outputs. One such modification is the Loss Quality Gate (LQG), which has gained significant attention for its effectiveness in tackling some of these challenges. This work provides a comprehensive analysis and evaluation of GANs with the LQG component, highlighting its strengths, limitations, and impact on generator performance. Our experiments demonstrate that LQG consistently produces higher quality results across multiple datasets compared to the standard GAN setting while providing insights into how specific hyperparameters affect model behavior. Overall, we believe this study serves as an important contribution towards a deeper understanding of GANs and advances in generating realistic artificial data. Keywords: Generative Adversarial Networks (GANs); Loss Quality Gate; Synthetic Data Generation; Deep Learning Models",1
"In this paper, we propose a Geometry-Contrastive Generative Adversarial Network (GC-GAN) for transferring continuous emotions across different subjects. Given an input face with certain emotion and a target facial expression from another subject, GC-GAN can generate an identity-preserving face with the target expression. Geometry information is introduced into cGANs as continuous conditions to guide the generation of facial expressions. In order to handle the misalignment across different subjects or emotions, contrastive learning is used to transform geometry manifold into an embedded semantic manifold of facial expressions. Therefore, the embedded geometry is injected into the latent space of GANs and control the emotion generation effectively. Experimental results demonstrate that our proposed method can be applied in facial expression transfer even there exist big differences in facial shapes and expressions between different subjects.",0
"In recent years, Generative Adversarial Networks (GANs) have become increasingly popular in computer vision tasks such as image generation, style transfer, and data augmentation. One particularly challenging application of these models is facial expression synthesis - generating new images that convey different emotions while preserving other important features such as identity, lighting, and background. This task requires balancing two competing objectives: maintaining fidelity to input constraints while enabling expressiveness through variation in output appearance. Previous approaches have relied on either regularization techniques like feature matching loss functions, or adversarial training against multiple discriminators, but both methods suffer from tradeoffs between accuracy and efficiency, stability and speed, or realism vs abstraction. Our proposed method overcomes these limitations by jointly learning geometry and contrast metrics within a single GAN framework that optimizes one common objective function using multi-scale attention blocks. We evaluate our approach quantitatively and qualitatively across several benchmark datasets and demonstrate that our model achieves state-of-the-art performance in terms of visual quality, diversity, perceptual similarity, and face recognition accuracy. By carefully designing neural networks capable of capturing subtle differences in expressions without losing essential information required by downstream tasks like facial recognition, we enable more flexible applications in areas like virtual/augmented reality, entertainment, social media content creation, affective computing, and human-computer interaction. Overall, our work provides insights into how advanced machine learning techniques can facilitate better understanding of complex phenomena like human emotion expressed via nonverbal communication cues, paving the way for even more creative exploitation of generative models for innovative research problems well beyond artistic rendering and perception tasks.",1
"In this paper we present several architectural and optimization recipes for generative adversarial network(GAN) based facial semantic inpainting. Current benchmark models are susceptible to initial solutions of non-convex optimization criterion of GAN based inpainting. We present an end-to-end trainable parametric network to deterministically start from good initial solutions leading to more photo realistic reconstructions with significant optimization speed up. For the first time, we show how to efficiently extend GAN based single image inpainter models to sequences by a)learning to initialize a temporal window of solutions with a recurrent neural network and b)imposing a temporal smoothness loss(during iterative optimization) to respect the redundancy in temporal dimension of a sequence. We conduct comprehensive empirical evaluations on CelebA images and pseudo sequences followed by real life videos of VidTIMIT dataset. The proposed method significantly outperforms current GAN based state-of-the-art in terms of reconstruction quality with a simultaneous speedup of over 15$\times$. We also show that our proposed model is better in preserving facial identity in a sequence even without explicitly using any face recognition module during training.",0
"Facial image inpainting refers to the process of filling in missing regions of an image while maintaining the visual quality and authenticity of the remaining parts. Generative Adversarial Networks (GANs) have emerged as powerful tools for facial image inpainting due to their ability to generate high-quality results. However, existing techniques still suffer from limitations such as difficulties in handling large missing areas, preserving edge details, and producing visually plausible textures.  This paper presents improved techniques for GAN-based facial inpainting that address these limitations. Firstly, we propose a novel architecture that utilizes multi-scale discriminators and feature attention modules to better handle large missing regions and preserve fine structures. Secondly, we introduce a new loss function that combines texture similarity and perceptual distance metrics to enhance visual fidelity. Finally, we present a simple yet effective algorithm for initializing the latent space of the generator network, which significantly improves the stability and convergence of the training procedure.  Experimental evaluations on several benchmark datasets demonstrate the effectiveness of our proposed methods over state-of-the-art baselines. Our approach achieves superior performance in terms of visual quality, structural accuracy, and perceptual realism. Additionally, we conduct ablation studies to analyze the impact of each component in our framework and provide insights into the design choices made.  In summary, this work represents a significant advancement in the field of GAN-based facial image inpainting. Our improvements enable more efficient and accurate completion of complex missing regions, resulting in higher-quality outputs that closely match ground truth images.",1
"Nuclei segmentation is a fundamental task that is critical for various computational pathology applications including nuclei morphology analysis, cell type classification, and cancer grading. Conventional vision-based methods for nuclei segmentation struggle in challenging cases and deep learning approaches have proven to be more robust and generalizable. However, CNNs require large amounts of labeled histopathology data. Moreover, conventional CNN-based approaches lack structured prediction capabilities which are required to distinguish overlapping and clumped nuclei. Here, we present an approach to nuclei segmentation that overcomes these challenges by utilizing a conditional generative adversarial network (cGAN) trained with synthetic and real data. We generate a large dataset of H&E training images with perfect nuclei segmentation labels using an unpaired GAN framework. This synthetic data along with real histopathology data from six different organs are used to train a conditional GAN with spectral normalization and gradient penalty for nuclei segmentation. This adversarial regression framework enforces higher order consistency when compared to conventional CNN models. We demonstrate that this nuclei segmentation approach generalizes across different organs, sites, patients and disease states, and outperforms conventional approaches, especially in isolating individual and overlapping nuclei.",0
"Our research focuses on developing novel deep learning approaches for accurate and efficient segmentation of multiple organ nuclei present within histopathological images. One crucial aspect of our work lies in effectively addressing challenges associated with limited data availability by using adversarial training techniques that enhance model robustness without excessive dependence on large datasets. Additionally, we aim at improving upon current state-of-the-art methods through integration of advanced network architectures and attention mechanisms tailored specifically for multi-organ nuclei segmentation. Our proposed framework enables effective handling of intricate dependencies among organs and achieves superior performance across diverse microscopy settings. We demonstrate significant improvements over alternative algorithms, including those employing postprocessing steps or leveraging costly external models, making our approach highly competitive for clinical deployment and biomedical analysis.",1
"Fault detection problem for closed loop uncertain dynamical systems, is investigated in this paper, using different deep learning based methods. Traditional classifier based method does not perform well, because of the inherent difficulty of detecting system level faults for closed loop dynamical system. Specifically, acting controller in any closed loop dynamical system, works to reduce the effect of system level faults. A novel Generative Adversarial based deep Autoencoder is designed to classify datasets under normal and faulty operating conditions. This proposed network performs significantly well when compared to any available classifier based methods, and moreover, does not require labeled fault incorporated datasets for training purpose. Finally, this aforementioned network's performance is tested on a high complexity building energy system dataset.",0
"In this work we present a novel approach that uses a Generative Adversarial Network (GAN) coupled with an autoencoder structure, referred to as GAGA, capable of detecting faults in closed loop dynamic systems. Traditional methods suffer from limitations such as high computational cost and poor generalization performance due to their reliance on static models and threshold values. To overcome these issues, our proposed method utilizes generative adversarial networks which have shown strong capabilities in anomaly detection tasks. By using an autoencoder component in addition to the generator network, our model can learn more robustly representations that better capture important features for detecting faults while minimizing reconstruction error. Our results show significant improvement over other approaches across different benchmark datasets demonstrating the effectiveness and promise of using GAGA in real world applications such as manufacturing control systems where early fault detection is crucial.",1
"Recent research has demonstrated the vulnerability of fingerprint recognition systems to dictionary attacks based on MasterPrints. MasterPrints are real or synthetic fingerprints that can fortuitously match with a large number of fingerprints thereby undermining the security afforded by fingerprint systems. Previous work by Roy et al. generated synthetic MasterPrints at the feature-level. In this work we generate complete image-level MasterPrints known as DeepMasterPrints, whose attack accuracy is found to be much superior than that of previous methods. The proposed method, referred to as Latent Variable Evolution, is based on training a Generative Adversarial Network on a set of real fingerprint images. Stochastic search in the form of the Covariance Matrix Adaptation Evolution Strategy is then used to search for latent input variables to the generator network that can maximize the number of impostor matches as assessed by a fingerprint recognizer. Experiments convey the efficacy of the proposed method in generating DeepMasterPrints. The underlying method is likely to have broad applications in fingerprint security as well as fingerprint synthesis.",0
"This paper presents a novel approach to generating master prints for dictionary attacks on fingerprint recognition systems. We propose using latent variable evolution (LVE) as a means of optimizing a set of synthetic fingerprints to act as ""master keys"" that can unlock a large number of devices protected by the same system. Our method involves searching through a high-dimensional parameter space of possible print patterns, guided by an LVE algorithm that seeks out those parameters that lead to the most successful attacks. We evaluate our approach using both real-world data sets and simulated test cases, showing that it consistently produces high-quality master prints capable of fooling even advanced biometric security measures. Overall, we believe that DeepMasterPrints represents a significant advance in the field of digital security and highlights the urgent need for continued research into new attack vectors and countermeasures.",1
"With contemporary advancements of graphics engines, recent trend in deep learning community is to train models on automatically annotated simulated examples and apply on real data during test time. This alleviates the burden of manual annotation. However, there is an inherent difference of distributions between images coming from graphics engine and real world. Such domain difference deteriorates test time performances of models trained on synthetic examples. In this paper we address this issue with unsupervised adversarial feature adaptation across synthetic and real domain for the special use case of eye gaze estimation which is an essential component for various downstream HCI tasks. We initially learn a gaze estimator on annotated synthetic samples rendered from a 3D game engine and then adapt the features of unannotated real samples via a zero-sum minmax adversarial game against a domain discriminator following the recent paradigm of generative adversarial networks. Such adversarial adaptation forces features of both domains to be indistinguishable which enables us to use regression models trained on synthetic domain to be used on real samples. On the challenging MPIIGaze real life dataset, we outperform recent fully supervised methods trained on manually annotated real samples by appreciable margins and also achieve 13\% more relative gain after adaptation compared to the current benchmark method of SimGAN",0
"This article presents a novel method for adapting a neural network model for predicting gaze direction from synthetically generated eye images. We use adversarial training to bridge the domain gap between real and fake data sets by leveraging discriminators that can learn distinct features separating the two domains. Our experiments demonstrate improved accuracy on both simulated (MVTec AD) and more challenging benchmark datasets such as WIDER FACE, which contain significant variations due to factors like illumination, pose, facial expressions, accessories, etc. We provide comprehensive ablation studies and comparisons against several baselines, including fine-tuning approaches, demonstrating clear advantages of our approach. Overall, we propose an efficient yet effective framework that effectively utilizes large amounts of synthesized data to generalize better on unseen real images without any labeled examples from those specific distributions.",1
"Numerous factors could lead to partial deteriorations of medical images. For example, metallic implants will lead to localized perturbations in MRI scans. This will affect further post-processing tasks such as attenuation correction in PET/MRI or radiation therapy planning. In this work, we propose the inpainting of medical images via Generative Adversarial Networks (GANs). The proposed framework incorporates two patch-based discriminator networks with additional style and perceptual losses for the inpainting of missing information in realistically detailed and contextually consistent manner. The proposed framework outperformed other natural image inpainting techniques both qualitatively and quantitatively on two different medical modalities.",0
"""Inpainting"", or filling in missing parts of images, has become an important topic recently as medical imaging becomes ever more widespread. This work explores how adversaries can manipulate the process of inpainting and even change what medical practitioners see without changing original image data. It studies the impact on radiologists reading the resulting modified images and provides recommendations for improving security against these kinds of attacks. The authors conduct experiments that show the feasibility of such manipulation using real world datasets and provide further insights into how to improve robustness against them. They use metrics to evaluate effectiveness of different countermeasures and report results from user trials. While their work focuses specifically on mammograms they believe similar techniques could apply equally well to other modalities like MRI scans. The authors find their proposed mitigation strategies significantly reduce errors caused by attacker modifications for radiologists reading synthetic lesions (with perfect knowledge). Overall the study shows new threat vectors for machine learning based systems used in healthcare applications which warrant attention before deployment in practice.",1
"We present a deep learning framework based on a generative adversarial network (GAN) to perform super-resolution in coherent imaging systems. We demonstrate that this framework can enhance the resolution of both pixel size-limited and diffraction-limited coherent imaging systems. We experimentally validated the capabilities of this deep learning-based coherent imaging approach by super-resolving complex images acquired using a lensfree on-chip holographic microscope, the resolution of which was pixel size-limited. Using the same GAN-based approach, we also improved the resolution of a lens-based holographic imaging system that was limited in resolution by the numerical aperture of its objective lens. This deep learning-based super-resolution framework can be broadly applied to enhance the space-bandwidth product of coherent imaging systems using image data and convolutional neural networks, and provides a rapid, non-iterative method for solving inverse image reconstruction or enhancement problems in optics.",0
"Title: ""Deep Learning Based Super-Resolution for Coherent Imaging Systems""  Abstract: Coherent imaging systems have emerged as powerful tools in many fields due to their ability to generate high-quality images that exceed the limits of traditional optics. However, these systems often suffer from limited resolution, which can limit their effectiveness in applications such as biological microscopy, astronomy, and remote sensing. To address this challenge, we propose a deep learning based approach for super-resolution in coherent imaging systems. Our method leverages advances in convolutional neural networks (CNNs) to learn the relationship between low-resolution input images and high-resolution output images. We demonstrate the efficacy of our approach on simulated and real data sets, showing significant improvements over existing techniques. By enabling higher resolution coherent imaging, our work has important implications across a range of scientific disciplines.",1
"Generative Adversarial Networks (GAN) can achieve promising performance on learning complex data distributions on different types of data. In this paper, we first show a straightforward extension of existing GAN algorithm is not applicable to point clouds, because the constraint required for discriminators is undefined for set data. We propose a two fold modification to GAN algorithm for learning to generate point clouds (PC-GAN). First, we combine ideas from hierarchical Bayesian modeling and implicit generative models by learning a hierarchical and interpretable sampling process. A key component of our method is that we train a posterior inference network for the hidden variables. Second, instead of using only state-of-the-art Wasserstein GAN objective, we propose a sandwiching objective, which results in a tighter Wasserstein distance estimate than the commonly used dual form. Thereby, PC-GAN defines a generic framework that can incorporate many existing GAN algorithms. We validate our claims on ModelNet40 benchmark dataset. Using the distance between generated point clouds and true meshes as metric, we find that PC-GAN trained by the sandwiching objective achieves better results on test data than the existing methods. Moreover, as a byproduct, PC- GAN learns versatile latent representations of point clouds, which can achieve competitive performance with other unsupervised learning algorithms on object recognition task. Lastly, we also provide studies on generating unseen classes of objects and transforming image to point cloud, which demonstrates the compelling generalization capability and potentials of PC-GAN.",0
"Title: Point cloud generation using GANs (Generative Adversarial Networks)  Point clouds are three-dimensional datasets made up of large numbers of points that represent objects from different angles. They have many applications including computer graphics, robotics, and artificial intelligence. GANs can generate high quality point clouds by learning the underlying distribution of real data points. In contrast to previous methods which rely on mesh parameterization, this approach allows control over individual data points. We propose a new architecture and training method for stable, diverse, controllable synthesis of novel point clouds. Quantitative evaluation shows advantages compared to state-of-the-art approaches for generating detailed shapes. Qualitatively, our results show improved smoothness and resolution while preserving fine features. Combining several discriminators yields superior performance than single ones. Our generator reliably generates visually appealing novel point sets. By applying the same set of weights to both generators and discriminator at test time we achieve efficient multiple shape synthesis. Additionally, the proposed adversarial loss helps generate meaningful local detail. Overall, this work presents a strong contribution towards enabling further research into deep learning applications on 3D data. Code will be released upon acceptance to allow others to compare against their own models and utilize these techniques to advance science and engineering applications.  If anyone needs any more papers written I would love to write some more! Or if someone wants me to rewrite the above paragraph as non-academic speak... ;p",1
"We show how we can globally edit images using textual instructions: given a source image and a textual instruction for the edit, generate a new image transformed under this instruction. To tackle this novel problem, we develop three different trainable models based on RNN and Generative Adversarial Network (GAN). The models (bucket, filter bank, and end-to-end) differ in how much expert knowledge is encoded, with the most general version being purely end-to-end. To train these systems, we use Amazon Mechanical Turk to collect textual descriptions for around 2000 image pairs sampled from several datasets. Experimental results evaluated on our dataset validate our approaches. In addition, given that the filter bank model is a good compromise between generality and performance, we investigate it further by replacing RNN with Graph RNN, and show that Graph RNN improves performance. To the best of our knowledge, this is the first computational photography work on global image editing that is purely based on free-form textual instructions.",0
"""Image editing techniques that use textual descriptions have gained popularity due to their ability to generate high-quality results with minimal user input. However, current methods often struggle with global image transformations such as changes in lighting conditions or color correction. This paper proposes a new framework for globally editing images using textual descriptions by leveraging advanced optimization algorithms and deep learning models. Our method utilizes a variational autoencoder to learn the underlying distribution of images given a particular set of textual descriptors, allowing us to accurately predict how different inputs impact the final output image. We demonstrate the effectiveness of our approach through extensive experiments on various datasets and showcase examples where our method outperforms state-of-the art techniques.""",1
"This paper introduces DensePoint, a densely sampled and annotated point cloud dataset containing over 10,000 single objects across 16 categories, by merging different kind of information from two existing datasets. Each point cloud in DensePoint contains 40,000 points, and each point is associated with two sorts of information: RGB value and part annotation. In addition, we propose a method for point cloud colorization by utilizing Generative Adversarial Networks (GANs). The network makes it possible to generate colours for point clouds of single objects by only giving the point cloud itself. Experiments on DensePoint show that there exist clear boundaries in point clouds between different parts of an object, suggesting that the proposed network is able to generate reasonably good colours. Our dataset is publicly available on the project page.",0
"This paper proposes a new method for colorizing point clouds based on their underlying shape. By utilizing a densely annotated dataset of 3D shapes, we can train our model to accurately predict colors for each point in the cloud. Our approach involves creating a deep neural network that takes the raw point cloud data as input, and outputs a corresponding RGB value for each point. We then use an optimization algorithm to find the most consistent set of colors across all points in the cloud. Experimental results demonstrate the effectiveness of our method, outperforming previous state-of-the-art techniques in terms of both accuracy and efficiency. Overall, this work represents an important step towards realistic computer graphics rendering, enabling novel applications such as virtual reality and augmented reality systems.",1
"Anomaly detection is often considered a challenging field of machine learning due to the difficulty of obtaining anomalous samples for training and the need to obtain a sufficient amount of training data. In recent years, autoencoders have been shown to be effective anomaly detectors that train only on ""normal"" data. Generative adversarial networks (GANs) have been used to generate additional training samples for classifiers, thus making them more accurate and robust. However, in anomaly detection GANs are only used to reconstruct existing samples rather than to generate additional ones. This stems both from the small amount and lack of diversity of anomalous data in most domains. In this study we propose MDGAN, a novel GAN architecture for improving anomaly detection through the generation of additional samples. Our approach uses two discriminators: a dense network for determining whether the generated samples are of sufficient quality (i.e., valid) and an autoencoder that serves as an anomaly detector. MDGAN enables us to reconcile two conflicting goals: 1) generate high-quality samples that can fool the first discriminator, and 2) generate samples that can eventually be effectively reconstructed by the second discriminator, thus improving its performance. Empirical evaluation on a diverse set of datasets demonstrates the merits of our approach.",0
"""MDGAN: Boosting Anomaly Detection Using Multi-Discriminator Generative Adversarial Networks"" presents a novel approach to anomaly detection using generative adversarial networks (GANs). Traditional GAN models consist of two neural networks that compete against each other to produce realistic data while detecting fake inputs from the generator network. In this paper, we introduce multi-discriminator GANs (MDGAN) which utilize multiple discriminators instead of just one. Our experiments show that MDGAN outperforms traditional GAN approaches on a variety of datasets such as MNIST, CIFAR-10, and KDD Cup99 by achieving higher F1 scores. Furthermore, our model yields promising results in terms of both accuracy and robustness without relying on preprocessing steps like normalization. Overall, our work demonstrates the potential of MDGANs as a powerful tool for anomaly detection. ------------------- Abstract: This paper introduces a novel method for boosting anomaly detection using multi-discriminator generative adversarial networks (GANs), which have shown to perform better than traditional single-discriminator GANs across several benchmark datasets including MNIST, CIFAR-10, and KDD Cup99. By leveraging multiple discriminators, our approach achieves higher F1 scores, improved accuracy, and greater robustness compared to previous methods. Additionally, we demonstrate the effectiveness of our method without relying on preprocessing techniques such as normalization. We believe this research has important implications for advancing the field of anomaly detection through deep learning and highlight the potential of MDGANs in this domain.  Keywords: anomaly detection, generative adversarial networks, multi-discriminator, MNIST, CIFAR-10, KDD Cup99",1
"Over the past decade, many Super Resolution techniques have been developed using deep learning. Among those, generative adversarial networks (GAN) and very deep convolutional networks (VDSR) have shown promising results in terms of HR image quality and computational speed. In this paper, we propose two approaches based on these two algorithms: VDSR-ResNeXt, which is a deep multi-branch convolutional network inspired by VDSR and ResNeXt; and SRCGAN, which is a conditional GAN that explicitly passes class labels as input to the GAN. The two methods were implemented on common SR benchmark datasets for both quantitative and qualitative assessment.",0
"This work presents two methods for improving image super-resolution using deep learning models: VDSR-ResNet and SRCGAN. Both methods use convolutional neural networks (CNNs) to learn a mapping from low resolution images to high resolution ones. While VDSR-ResNet uses residual connections to improve training stability, SRCGAN adds a generative adversarial network to generate realistic details in the upscaled images. We compare these approaches on publicly available datasets and show that both achieve state-of-the-art performance while having different strengths and weaknesses. Our results demonstrate the potential of deep learning techniques for achieving efficient and effective image super-resolution.",1
"We propose the BinaryGAN, a novel generative adversarial network (GAN) that uses binary neurons at the output layer of the generator. We employ the sigmoid-adjusted straight-through estimators to estimate the gradients for the binary neurons and train the whole network by end-to-end backpropogation. The proposed model is able to directly generate binary-valued predictions at test time. We implement such a model to generate binarized MNIST digits and experimentally compare the performance for different types of binary neurons, GAN objectives and network architectures. Although the results are still preliminary, we show that it is possible to train a GAN that has binary neurons and that the use of gradient estimators can be a promising direction for modeling discrete distributions with GANs. For reproducibility, the source code is available at https://github.com/salu133445/binarygan .",0
"In order to address issues like vanishing gradients, exploding batch sizes, poor generalization performance and limited scalability which have been observed while training deep neural networks, this study examines alternative architectures based on binary neurons. The main contribution is that these can indeed perform as well as traditional multi-valued models, but at a fraction of the size (number of parameters). Additionally, adversarial training with GANs results in better accuracy/generalisation. An important methodological innovation is end-to-end backpropagation through discrete latent variables, allowing efficient computation of derivatives everywhere. Furthermore, we present evidence that a simple linear model trained via max likelihood on MNIST yields test errors rivaling some popular DNN baselines despite having many fewer free parameters, motivating further investigation into this surprising phenomenon. For all experiments we use the TensorFlow framework. We train our models on NVIDIA GPUs using the Adam optimizer and report accuracies on held out test sets after each epoch. Code and data to reproduce all figures / tables is available online. Future work includes extending these models to more complex tasks such as generative modelling; evaluating them on other datasets where they may excel e.g. text classification or reinforcement learning problems; refining the hyperparameters; and investigating whether there exist hybrid models combining some benefits from both continuous valued activations and Bernoulli distributed units. Finally, understanding the role played by the entropy term in maximising log-likelihood during inference and how exactly to estimate uncertainty remains an open question, though recent advances suggest the possibility of developing Bayesian variants of these models.",1
"Neural networks have proven their capabilities by outperforming many other approaches on regression or classification tasks on various kinds of data. Other astonishing results have been achieved using neural nets as data generators, especially in settings of generative adversarial networks (GANs). One special application is the field of image domain translations. Here, the goal is to take an image with a certain style (e.g. a photography) and transform it into another one (e.g. a painting). If such a task is performed for unpaired training examples, the corresponding GAN setting is complex, the neural networks are large, and this leads to a high peak memory consumption during, both, training and evaluation phase. This sets a limit to the highest processable image size. We address this issue by the idea of not processing the whole image at once, but to train and evaluate the domain translation on the level of overlapping image subsamples. This new approach not only enables us to translate high-resolution images that otherwise cannot be processed by the neural network at once, but also allows us to work with comparably small neural networks and with limited hardware resources. Additionally, the number of images required for the training process is significantly reduced. We present high-quality results on images with a total resolution of up to over 50 megapixels and emonstrate that our method helps to preserve local image details while it also keeps global consistency.",0
"In recent years, style transfer using generative adversarial networks (GANs) has gained significant attention due to its ability to generate novel images that retain the content information from one image while adopting the style characteristics of another. However, existing methods require paired training data, which can be difficult to obtain. This work proposes an unpaired high-resolution and scalable style transfer framework that can synthesize new images at higher resolutions without requiring paired data. Our approach relies on two key components: a generative network trained to model multimodal distributions and a discriminator that enforces both intra-modality and inter-modality alignment constraints. Extensive experiments demonstrate that our method outperforms state-of-the-art algorithms across several evaluation metrics, including subjective assessments by human raters. Moreover, we showcase how our framework enables scaling up GANs beyond current limits through efficient parallelization techniques and mixed precision training. Overall, our work advances the field of unsupervised learning and demonstrates the potential for applying these models towards real-world applications such as artistic rendering, medical imaging analysis, and video generation.",1
"Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data.",0
"This should capture the essence of your paper without including any details such as methods used etc: DualGAN: Unsupervised Dual Learning for Image-to-Image Translation The goal of image-to-image translation is to generate images that correspond to specific objectives while preserving relevant features from the input image. While supervised learning has shown promising results, unsupervised learning approaches can lead to better performance by exploiting prior knowledge present within the dataset. In our work, we propose DualGAN, an unsupervised dual learning framework for image-to-image translation. Our approach consists of two generative adversarial networks (GANs) competing against each other to improve their respective discriminators. By doing so, we achieve superior image quality compared to previous state-of-the-art models while requiring fewer data pairs and computational resources. Our method achieves significant improvements over traditional GAN architectures while being more efficient. We demonstrate the effectiveness of our approach on several benchmark datasets and showcase applications across different domains.",1
"We introduce a novel generative autoencoder network model that learns to encode and reconstruct images with high quality and resolution, and supports smooth random sampling from the latent space of the encoder. Generative adversarial networks (GANs) are known for their ability to simulate random high-quality images, but they cannot reconstruct existing images. Previous works have attempted to extend GANs to support such inference but, so far, have not delivered satisfactory high-quality results. Instead, we propose the Progressively Growing Generative Autoencoder (PIONEER) network which achieves high-quality reconstruction with $128{\times}128$ images without requiring a GAN discriminator. We merge recent techniques for progressively building up the parts of the network with the recently introduced adversarial encoder-generator network. The ability to reconstruct input images is crucial in many real-world applications, and allows for precise intelligent manipulation of existing images. We show promising results in image synthesis and inference, with state-of-the-art results in CelebA inference tasks.",0
"One approach to generative model training is progressive growing, which starts with a small network and then gradually adds layers as more data becomes available. This can allow researchers to quickly create models that work well on the current dataset, but may struggle to generalize beyond their training set. To address these limitations, one possible solution is ""Pioneer networks,"" which aim to balance exploration (finding good solutions) and exploitation (using the current best solutions). In this paper, we present a new variant of pioneer networks called PGAA (progressively growing autoencoders), which have improved stability and performance compared to previous variants. We evaluate our method on several benchmark datasets and show that it outperforms other state-of-the-art methods, while requiring fewer computational resources. Overall, we believe PGAA holds great promise for further advancements in generative modelling and unsupervised learning tasks.",1
"The Generator of a Generative Adversarial Network (GAN) is trained to transform latent vectors drawn from a prior distribution into realistic looking photos. These latent vectors have been shown to encode information about the content of their corresponding images. Projecting input images onto the latent space of a GAN is non-trivial, but previous work has successfully performed this task for latent spaces with a uniform prior. We extend these techniques to latent spaces with a Gaussian prior, and demonstrate our technique's effectiveness.",0
"In recent years, there has been significant interest in developing generative models that can generate high quality images and other types of data from input textual descriptions. One popular approach to achieve this task is through the use of Generative Adversarial Networks (GANs). However, despite their success, GANs often suffer from issues such as poor stability during training and difficulty in generating diverse outputs. In our work, we propose using generalized latent variable recovery methods to improve the performance of GANs. Specifically, we develop a new algorithm called ""Generalized Latent Variable Recovery for Generative Adversarial Networks"" which incorporates regularization techniques into the training process to address these issues. We evaluate our method on several benchmark datasets and show that it outperforms current state-of-the-art approaches in terms of both quantitative metrics and subjective assessments by human raters. Our results highlight the potential of using generalized latent variables for improving the performance of GANs and suggest promising future directions for research in this area.",1
"The traditional approach of hand-crafting priors (such as sparsity) for solving inverse problems is slowly being replaced by the use of richer learned priors (such as those modeled by generative adversarial networks, or GANs). In this work, we study the algorithmic aspects of such a learning-based approach from a theoretical perspective. For certain generative network architectures, we establish a simple non-convex algorithmic approach that (a) theoretically enjoys linear convergence guarantees for certain inverse problems, and (b) empirically improves upon conventional techniques such as back-propagation. We also propose an extension of our approach that can handle model mismatch (i.e., situations where the generative network prior is not exactly applicable.) Together, our contributions serve as building blocks towards a more complete algorithmic understanding of generative models in inverse problems.",0
"Solve the inverse problem! Given some data about X, recover as much information as possible about Y, while minimizing error/uncertainty/etc along the way. For example, if given images of objects, figure out what those objects are; if given audio recordings of music, reconstruct sheet music; if given text documents, find authors; etc. This is called ""inversion"", which involves finding explicit functions from Y->X that map elements of one space to elements of another (so we can then apply them to other unknown X's to obtain corresponding Y's). If exact recovery is impossible because there may be multiple valid solutions to recovering Y from any given input X--or worse, no solution at all!--then consider using generative models: build stochastic mappings G(z) -> X from latent spaces Z where z is random but drawn from known distributions like Gaussian mixtures. Then use techniques like mean field theory and variational Bayesian methods to learn how the parameters of G relate to parameters representing properties of interest in Z, and how those latter parameters depend on your observed X's. Now condition on these learned parameter estimates to get posterior distributions over possible values of Z for each X, since learning these posterior modes (which maximize expected log probability of observing X | parameter estimates) can give you promising guesses for what the corresponding Y should look like. Finally, pick something nice out of those posteriors to actually output (e.g., maximum likelihood estimator, median, mode), and evaluate performance via cross-validation with respect to whatever loss function(s) you care about. Iterate until convergence/improvement slowdown. Apply to lots of fun problems throughout. So far so good? Good luck proving uniqueness, existence, smoothness, nondegeneracy, stability",1
"It has been shown recently that deep convolutional generative adversarial networks (GANs) can learn to generate music in the form of piano-rolls, which represent music by binary-valued time-pitch matrices. However, existing models can only generate real-valued piano-rolls and require further post-processing, such as hard thresholding (HT) or Bernoulli sampling (BS), to obtain the final binary-valued results. In this paper, we study whether we can have a convolutional GAN model that directly creates binary-valued piano-rolls by using binary neurons. Specifically, we propose to append to the generator an additional refiner network, which uses binary neurons at the output layer. The whole network is trained in two stages. Firstly, the generator and the discriminator are pretrained. Then, the refiner network is trained along with the discriminator to learn to binarize the real-valued piano-rolls the pretrained generator creates. Experimental results show that using binary neurons instead of HT or BS indeed leads to better results in a number of objective measures. Moreover, deterministic binary neurons perform better than stochastic ones in both objective measures and a subjective test. The source code, training data and audio examples of the generated results can be found at https://salu133445.github.io/bmusegan/ .",0
"Recent advances in deep learning have enabled convolutional generative adversarial networks (CGANs) to produce high quality synthetic images by leveraging large datasets and architectures. To apply these models to other domains, such as audio generation tasks like polyphonic music generation, several changes need to be made. We first discuss CGAN modifications necessary to generalize them to non-image related data, before focusing on our specific task. Our approach uses binary neuron activation functions that make use of both negative and positive numbers. We demonstrate how this choice leads to improved results compared to traditional sigmoid or tanh activations typically used in GANs. These advantages result from higher capacity and more stable convergence during training. By applying these developments to polyphonic music generation through conditioning techniques, we showcase new possibilities beyond predominantly image oriented work previously published using CGANs. Finally, we provide insights into model interpretability at the cost of performance.",1
"Transferring the knowledge of pretrained networks to new domains by means of finetuning is a widely used practice for applications based on discriminative models. To the best of our knowledge this practice has not been studied within the context of generative deep networks. Therefore, we study domain adaptation applied to image generation with generative adversarial networks. We evaluate several aspects of domain adaptation, including the impact of target domain size, the relative distance between source and target domain, and the initialization of conditional GANs. Our results show that using knowledge from pretrained networks can shorten the convergence time and can significantly improve the quality of the generated images, especially when the target data is limited. We show that these conclusions can also be drawn for conditional GANs even when the pretrained model was trained without conditioning. Our results also suggest that density may be more important than diversity and a dataset with one or few densely sampled classes may be a better source model than more diverse datasets such as ImageNet or Places.",0
"This work presents a novel approach for using Generative Adversarial Networks (GANs) to generate high-quality images from limited amounts of training data. While traditional GAN models require large datasets to produce realistic results, our method leverages advancements in transfer learning and domain adaptation techniques to achieve state-of-the-art performance on image generation tasks with significantly fewer examples. Our model architecture includes both generators and discriminators that benefit from pre-training on similar domains, allowing them to learn more quickly and effectively when given access to small or even single samples per class during fine tuning. We demonstrate the effectiveness of our technique through comprehensive experiments comparing against several recent methods that use more substantial collections of data to train their models. Results show clear improvements over these approaches in terms of visual fidelity as well as quantitative metrics such as Frechet Inception Distance and LPIPS scores. Overall, we believe that our approach has important implications for enabling new applications of artificial intelligence across many fields where obtaining large amounts of labeled data can prove challenging, including medical imaging, computer vision, and artistic content creation.",1
"We present Generative Adversarial Capsule Network (CapsuleGAN), a framework that uses capsule networks (CapsNets) instead of the standard convolutional neural networks (CNNs) as discriminators within the generative adversarial network (GAN) setting, while modeling image data. We provide guidelines for designing CapsNet discriminators and the updated GAN objective function, which incorporates the CapsNet margin loss, for training CapsuleGAN models. We show that CapsuleGAN outperforms convolutional-GAN at modeling image data distribution on MNIST and CIFAR-10 datasets, evaluated on the generative adversarial metric and at semi-supervised image classification.",0
"Abstract: There has been significant interest recently in developing generative models that can synthesize new data examples from scratch. In particular, Generative Adversarial Networks (GAN) have emerged as one of the most popular and effective methods for generating high-quality images, video frames, audio signals, text sequences, and other types of structured data. However, training GANs remains challenging due to issues related to stability, mode collapse, and slow convergence. Recently, there has been growing interest in using capsule networks for generative tasks, because they allow us to model spatial relationships more effectively than traditional convolutional neural nets. Building on these ideas, we propose CapsuleGAN: a novel framework that combines capsule network architecture with the adversarial objective function used by GANs. Our approach outperforms several state-of-the-art generative models across a variety of benchmark datasets and metrics, achieving significantly better results in terms of visual fidelity, diversity, and perceptual quality. Overall, our work demonstrates the potential of capsule networks as a powerful foundation for generative tasks, highlighting their ability to capture rich spatial hierarchies and discrete structures in complex domains such as image generation and transfer learning.",1
"Owing to their connection with generative adversarial networks (GANs), saddle-point problems have recently attracted considerable interest in machine learning and beyond. By necessity, most theoretical guarantees revolve around convex-concave (or even linear) problems; however, making theoretical inroads towards efficient GAN training depends crucially on moving beyond this classic framework. To make piecemeal progress along these lines, we analyze the behavior of mirror descent (MD) in a class of non-monotone problems whose solutions coincide with those of a naturally associated variational inequality - a property which we call coherence. We first show that ordinary, ""vanilla"" MD converges under a strict version of this condition, but not otherwise; in particular, it may fail to converge even in bilinear models with a unique solution. We then show that this deficiency is mitigated by optimism: by taking an ""extra-gradient"" step, optimistic mirror descent (OMD) converges in all coherent problems. Our analysis generalizes and extends the results of Daskalakis et al. (2018) for optimistic gradient descent (OGD) in bilinear problems, and makes concrete headway for establishing convergence beyond convex-concave games. We also provide stochastic analogues of these results, and we validate our analysis by numerical experiments in a wide array of GAN models (including Gaussian mixture models, as well as the CelebA and CIFAR-10 datasets).",0
"This paper investigates the use of optimistic mirror descent as a method for solving saddle point problems. Our approach differs from previous work by incorporating gradient steps that update both primal and dual variables concurrently. We provide convergence guarantees under mild conditions on the step sizes used and demonstrate our results through numerical experiments on linear equations systems solved using Bregman Lagrangians. Our experiments showcase faster convergence rates and improved solutions compared to previous approaches using mirror descent alone. Overall, we hope that our findings can contribute towards better algorithms for distributed optimization, machine learning and other applications involving saddle point problems.",1
"Even though end-to-end supervised learning has shown promising results for sensorimotor control of self-driving cars, its performance is greatly affected by the weather conditions under which it was trained, showing poor generalization to unseen conditions. In this paper, we show how knowledge can be transferred using semantic maps to new weather conditions without the need to obtain new ground truth data. To this end, we propose to divide the task of vehicle control into two independent modules: a control module which is only trained on one weather condition for which labeled steering data is available, and a perception module which is used as an interface between new weather conditions and the fixed control module. To generate the semantic data needed to train the perception module, we propose to use a generative adversarial network (GAN)-based model to retrieve the semantic information for the new conditions in an unsupervised manner. We introduce a master-servant architecture, where the master model (semantic labels available) trains the servant model (semantic labels not available). We show that our proposed method trained with ground truth data for a single weather condition is capable of achieving similar results on the task of steering angle prediction as an end-to-end model trained with ground truth data of 15 different weather conditions.",0
"This paper presents a modular vehicle control system that utilizes Generative Adversarial Networks (GANs) to transfer semantic information between different weather conditions. Our approach addresses the challenges posed by unpredictable environmental changes on the accuracy of sensor readings used to guide autonomous vehicles. By generating synthetic images using GANs, our model enhances the robustness of perception systems through effective adaptation to changing weather scenarios while maintaining high levels of reliability and safety. Through experimental evaluation, we demonstrate the efficacy of our method compared to traditional approaches based solely on statistical models. Our results show significant improvements in overall performance under adverse weather conditions and lay the foundation for future research in domain generalization for autonomy.",1
"Deep learning algorithms produces state-of-the-art results for different machine learning and computer vision tasks. To perform well on a given task, these algorithms require large dataset for training. However, deep learning algorithms lack generalization and suffer from over-fitting whenever trained on small dataset, especially when one is dealing with medical images. For supervised image analysis in medical imaging, having image data along with their corresponding annotated ground-truths is costly as well as time consuming since annotations of the data is done by medical experts manually. In this paper, we propose a new Generative Adversarial Network for Medical Imaging (MI-GAN). The MI-GAN generates synthetic medical images and their segmented masks, which can then be used for the application of supervised analysis of medical images. Particularly, we present MI-GAN for synthesis of retinal images. The proposed method generates precise segmented images better than the existing techniques. The proposed model achieves a dice coefficient of 0.837 on STARE dataset and 0.832 on DRIVE dataset which is state-of-the-art performance on both the datasets.",0
"Artificial intelligence (AI) has gained widespread adoption in healthcare for tasks such as medical image analysis, radiology diagnosis, drug discovery and design, and predictive medicine. This review presents an overview of the latest advances in using GAns for medical image generation (MIG) to support these applications. MIG models use two neural networks in a minimax game: one generator that creates new images guided by noise vectors, and one discriminator that learns from real images to distinguish them from generated ones. The competition between generators and discriminators encourages continuous learning and improves quality of synthetic data. In healthcare applications, the main goal is generating realistic synthetic medical images which can be used either alone or mixed with actual patient scans to increase data diversity and variability. We discuss how MIG can improve diagnostic accuracy through augmentation of available clinical datasets; personalized treatment planning; and designing more effective studies through efficient simulation of rare diseases, imaging conditions, and population heterogeneities. While still at a relatively early stage, recent progress suggests promising future directions for using MIG models across different areas of healthcare.",1
"Sufficient training data normally is required to train deeply learned models. However, due to the expensive manual process for labelling large number of images, the amount of available training data is always limited. To produce more data for training a deep network, Generative Adversarial Network (GAN) can be used to generate artificial sample data. However, the generated data usually does not have annotation labels. To solve this problem, in this paper, we propose a virtual label called Multi-pseudo Regularized Label (MpRL) and assign it to the generated data. With MpRL, the generated data will be used as the supplementary of real training data to train a deep neural network in a semi-supervised learning fashion. To build the corresponding relationship between the real data and generated data, MpRL assigns each generated data a proper virtual label which reflects the likelihood of the affiliation of the generated data to pre-defined training classes in the real data domain. Unlike the traditional label which usually is a single integral number, the virtual label proposed in this work is a set of weight-based values each individual of which is a number in (0,1] called multi-pseudo label and reflects the degree of relation between each generated data to every pre-defined class of real data. A comprehensive evaluation is carried out by adopting two state-of-the-art convolutional neural networks (CNNs) in our experiments to verify the effectiveness of MpRL. Experiments demonstrate that by assigning MpRL to generated data, we can further improve the person re-ID performance on five re-ID datasets, i.e., Market-1501, DukeMTMC-reID, CUHK03, VIPeR, and CUHK01. The proposed method obtains +6.29%, +6.30%, +5.58%, +5.84%, and +3.48% improvements in rank-1 accuracy over a strong CNN baseline on the five datasets respectively, and outperforms state-of-the-art methods.",0
"Improving generated data quality in person re-identification has been a challenging task due to limitations in existing methods. This work proposes a new approach called Multi-Pseudo Regularized Label (MPRL) that addresses these issues by introducing multiple pseudo labels during training. MPRL utilizes both hard and soft labeling strategies to enhance the discriminative power of generated data, resulting in improved performance compared to current state-of-the-art techniques. Our method shows promising results on several benchmark datasets, demonstrating its effectiveness as a valuable tool in person re-identification research. Overall, MPRL provides a new perspective on improving generated data quality, paving the way for future advancements in computer vision applications.",1
"Pseudo-random number generators (PRNG) are a fundamental element of many security algorithms. We introduce a novel approach to their implementation, by proposing the use of generative adversarial networks (GAN) to train a neural network to behave as a PRNG. Furthermore, we showcase a number of interesting modifications to the standard GAN architecture. The most significant is partially concealing the output of the GAN's generator, and training the adversary to discover a mapping from the overt part to the concealed part. The generator therefore learns to produce values the adversary cannot predict, rather than to approximate an explicit reference distribution. We demonstrate that a GAN can effectively train even a small feed-forward fully connected neural network to produce pseudo-random number sequences with good statistical properties. At best, subjected to the NIST test suite, the trained generator passed around 99% of test instances and 98% of overall tests, outperforming a number of standard non-cryptographic PRNGs.",0
"Title: Pseudo-Random Number Generation using Generative Adversarial Networks  Abstract: The generation of truly random numbers remains one of the most challenging problems in computer science. In recent years, generative adversarial networks (GANs) have been used as a tool for generating realistic synthetic data, but their potential for pseudo-random number generation has largely gone unexplored. This paper presents a novel approach to pseudo-random number generation that utilizes GANs. We demonstrate through extensive experimentation that our method yields highly random-like sequences while maintaining computational efficiency and ease of implementation. Our results show that our approach outperforms traditional pseudorandom number generators on several benchmark tests for statistical randomness. Additionally, we provide insights into how different architectural choices can affect the randomness properties of the generated sequence. Overall, our work represents an important step towards true random number generation using deep learning methods.",1
"Deep neural networks have been demonstrated to be vulnerable to adversarial attacks, where small perturbations intentionally added to the original inputs can fool the classifier. In this paper, we propose a defense method, Featurized Bidirectional Generative Adversarial Networks (FBGAN), to extract the semantic features of the input and filter the non-semantic perturbation. FBGAN is pre-trained on the clean dataset in an unsupervised manner, adversarially learning a bidirectional mapping between the high-dimensional data space and the low-dimensional semantic space; also mutual information is applied to disentangle the semantically meaningful features. After the bidirectional mapping, the adversarial data can be reconstructed to denoised data, which could be fed into any pre-trained classifier. We empirically show the quality of reconstruction images and the effectiveness of defense.",0
"This paper introduces a novel approach to defending against adversarial attacks on deep neural networks using Generative Adversarial Networks (GANs). Our proposed method, called Featurized Bidirectional GAN (FB-GAN), combines the bidirectionality of cycle consistency loss with feature space regularization. FB-GAN takes advantage of two discriminators that are trained adversarially to enforce semantic constraints by minimizing the reconstruction error in the feature space. We demonstrate through extensive experiments that our FB-GAN model outperforms existing methods in terms of robustness against state-of-the-art attacks while maintaining high levels of accuracy on clean test data. Furthermore, we provide detailed analysis of the generated features and their relationship with natural image statistics, revealing insights into why our method improves performance compared to other approaches. Overall, our findings suggest that the use of generative models like FB-GAN has significant potential as an effective defense strategy against adversarial attacks.",1
"The clinical management of several cardiovascular conditions, such as pulmonary hypertension, require the assessment of the right ventricular (RV) function. This work addresses the fully automatic and robust access to one of the key RV biomarkers, its ejection fraction, from the gold standard imaging modality, MRI. The problem becomes the accurate segmentation of the RV blood pool from cine MRI sequences. This work proposes a solution based on Fully Convolutional Neural Networks (FCNN), where our first contribution is the optimal combination of three concepts (the convolution Gated Recurrent Units (GRU), the Generative Adversarial Networks (GAN), and the L1 loss function) that achieves an improvement of 0.05 and 3.49 mm in Dice Index and Hausdorff Distance respectively with respect to the baseline FCNN. This improvement is then doubled by our second contribution, the ROI-GAN, that sets two GANs to cooperate working at two fields of view of the image, its full resolution and the region of interest (ROI). Our rationale here is to better guide the FCNN learning by combining global (full resolution) and local Region Of Interest (ROI) features. The study is conducted in a large in-house dataset of $\sim$ 23.000 segmented MRI slices, and its generality is verified in a publicly available dataset.",0
"Right ventricular (RV) segmentation remains a challenging task due to variations in cardiac MRI (CMR) acquisition protocols and complex RV morphology. Conventional manual methods rely on human expert annotation which can be time consuming and subjective. Automatic approaches have been proposed but still suffer from limited accuracy and sensitivity. In this work, we present a generative adversarial network (GAN) based approach for RV segmentation that utilizes both unsupervised learning techniques and multi-modality image data fusion. Our method leverages two discriminators trained jointly - one discerns real CMR images from generated images while another separates real RV surfaces from generated ones. We validate our approach against clinical expert annotations demonstrating significant improvement over current state-of-the-art automatic methods. Furthermore, our method yields robust Dice scores across different patients and imaging datasets. These results highlight the promise of GANs as a powerful tool for medical image analysis tasks like RV segmentation.",1
"Multi-view frame reconstruction is an important problem particularly when multiple frames are missing and past and future frames within the camera are far apart from the missing ones. Realistic coherent frames can still be reconstructed using corresponding frames from other overlapping cameras. We propose an adversarial approach to learn the spatio-temporal representation of the missing frame using conditional Generative Adversarial Network (cGAN). The conditional input to each cGAN is the preceding or following frames within the camera or the corresponding frames in other overlapping cameras, all of which are merged together using a weighted average. Representations learned from frames within the camera are given more weight compared to the ones learned from other cameras when they are close to the missing frames and vice versa. Experiments on two challenging datasets demonstrate that our framework produces comparable results with the state-of-the-art reconstruction method in a single camera and achieves promising performance in multi-camera scenario.",0
"Title: Generative Adversarial Networks (GAN) have proven to be powerful tools for generating high quality synthetic images that can fool human observers and even state-of-the-art object detection algorithms. However, current methods for training GANs rely on single view reconstructions which can lead to incomplete or distorted representations of objects. In our work, we introduce a novel method called ""Multi-View Frame Reconstruction with Conditional GAN"" which uses multiple views of an object during training to improve the fidelity and completeness of the reconstructed image. We propose a conditional discriminator network that takes into account both the generated image and the corresponding ground truth frame to better evaluate the quality of the generated images. Our experiments demonstrate significant improvements over baseline models across a variety of benchmark datasets including MNIST, CelebA, and LSUN Church. These results suggest that multi-view training could potentially enable new applications such as virtual reality rendering and real-time video generation. Overall, our work represents a step forward in advancing the performance of generative adversarial networks.",1
"Recently, generative adversarial networks have gained a lot of popularity for image generation tasks. However, such models are associated with complex learning mechanisms and demand very large relevant datasets. This work borrows concepts from image and video captioning models to form an image generative framework. The model is trained in a similar fashion as recurrent captioning model and uses the learned weights for image generation. This is done in an inverse direction, where the input is a caption and the output is an image. The vector representation of the sentence and frames are extracted from an encoder-decoder model which is initially trained on similar sentence and image pairs. Our model conditions image generation on a natural language caption. We leverage a sequence-to-sequence model to generate synthetic captions that have the same meaning for having a robust image generation. One key advantage of our method is that the traditional image captioning datasets can be used for synthetic sentence paraphrases. Results indicate that images generated through multiple captions are better at capturing the semantic meaning of the family of captions.",0
"This paper presents a vector learning method for cross domain representations that can learn meaningful features from data across different domains such as text, images, speech and video. Our approach uses deep neural networks that map input from each modality into high dimensional feature spaces where we show that they exhibit geometric properties important for representation learning: linear separability, locality and rankness. We demonstrate that these learned representations are useful for supervised tasks like classification and unsupervised ones like clustering by performing experiments on multiple datasets from diverse modalities. Furthermore, our results highlight that our model performs comparably and sometimes better than state of the art methods showing the effectiveness of learning these representations jointly in one model rather than using dedicated models per task/domain. In conclusion, our research offers a novel approach towards solving multi-modal problems which has significant implications for real world applications such as computer vision, natural language processing etcetera as mentioned earlier as well as new possibilities emerging via new hardware and sensors e.g. brain imaging or IoT devices yielding rich data streams about their environment. Last but not least, our work can inspire scientists working within cognitive science or neuroscience studying how humans cope with interdisciplinary challenges - as it remains unknown why humans excel at transferring knowledge across disjoint areas despite them having such distinct inputs.",1
"In the field of pattern recognition research, the method of using deep neural networks based on improved computing hardware recently attracted attention because of their superior accuracy compared to conventional methods. Deep neural networks simulate the human visual system and achieve human equivalent accuracy in image classification, object detection, and segmentation. This chapter introduces the basic structure of deep neural networks that simulate human neural networks. Then we identify the operational processes and applications of conditional generative adversarial networks, which are being actively researched based on the bottom-up and top-down mechanisms, the most important functions of the human visual perception process. Finally, recent developments in training strategies for effective learning of complex deep neural networks are addressed.",0
"This paper presents a detailed investigation into the use of deep neural networks (DNN) for pattern recognition tasks. DNN have recently emerged as one of the most powerful tools available for tackling complex pattern recognition problems due to their ability to learn hierarchical representations from raw data. In particular, we focus on convolutional neural networks (CNN), which have achieved state-of-the-art results across many domains including computer vision, speech recognition, natural language processing, and others. We begin by reviewing key architectures and design considerations involved in building effective CNN models, and then explore how they can be applied to specific pattern recognition tasks such as image classification, object detection and segmentation, text categorization, and more. Finally, we discuss future directions for research aimed at improving the performance and utility of DNNs for these applications.",1
"The choice of parameters, and the design of the network architecture are important factors affecting the performance of deep neural networks. Genetic Algorithms (GA) have been used before to determine parameters of a network. Yet, GAs perform a finite search over a discrete set of pre-defined candidates, and cannot, in general, generate unseen configurations. In this paper, to move from exploration to exploitation, we propose a novel and systematic method that autonomously and simultaneously optimizes multiple parameters of any deep neural network by using a GA aided by a bi-generative adversarial network (Bi-GAN). The proposed Bi-GAN allows the autonomous exploitation and choice of the number of neurons, for fully-connected layers, and number of filters, for convolutional layers, from a large range of values. Our proposed Bi-GAN involves two generators, and two different models compete and improve each other progressively with a GAN-based strategy to optimize the networks during GA evolution. Our proposed approach can be used to autonomously refine the number of convolutional layers and dense layers, number and size of kernels, and the number of neurons for the dense layers; choose the type of the activation function; and decide whether to use dropout and batch normalization or not, to improve the accuracy of different deep neural network architectures. Without loss of generality, the proposed method has been tested with the ModelNet database, and compared with the 3D Shapenets and two GA-only methods. The results show that the presented approach can simultaneously and successfully optimize multiple neural network parameters, and achieve higher accuracy even with shallower networks.",0
"This is an interesting research paper that presents a new method for refining deep neural network parameters through the use of a bi-generative adversarial network (BGAN) aided genetic algorithm. The authors propose using BGANs as a tool for generating synthetic training data, which can then be used to fine-tune a pre-trained model. This approach has been shown to improve the accuracy of the original model significantly, while reducing the amount of real data required for training. The method is evaluated on several benchmark datasets and compared against other state-of-the-art techniques, demonstrating its effectiveness in improving classification performance. Overall, this work represents an important contribution towards advancing artificial intelligence research and expanding our understanding of how we can design more effective machine learning models.",1
"This paper investigates conditional generative adversarial networks (cGANs) to overcome a fundamental limitation of using geotagged media for geographic discovery, namely its sparse and uneven spatial distribution. We train a cGAN to generate ground-level views of a location given overhead imagery. We show the ""fake"" ground-level images are natural looking and are structurally similar to the real images. More significantly, we show the generated images are representative of the locations and that the representations learned by the cGANs are informative. In particular, we show that dense feature maps generated using our framework are more effective for land-cover classification than approaches which spatially interpolate features extracted from sparse ground-level images. To our knowledge, ours is the first work to use cGANs to generate ground-level views given overhead imagery and to explore the benefits of the learned representations.",0
"Our work presents a new method for generating dense ground-level views from overhead imagery using conditional generative adversarial networks (GAN). We call our approach ""View Synthesis."" In this approach, we train two neural networks - a generator network that produces images based on input noise plus a mapping of pixels in the output image back to corresponding locations within each input image; and a discriminator network whose job is to tell whether the generated view comes from real data or our synthesized model. We then feed random latent vectors to both the generator network followed by passing the generated view through a convolutional neural network that detects objects like cars, pedestrians, trees, buildings, etc. This results in an object detection system which can create large numbers of labeled training examples simply given overhead satellite imagery without any labeled ground-level training data. Our results show that our method outperforms other baseline methods in terms of visual fidelity and feature accuracy, as well as generalizing across different scenes compared to previous methods, even those trained on large amounts of labeled data. This has significant implications in areas such as robotics, self driving cars and augmented reality. In summary, our work tackles the challenge of developing machine learning models capable of extracting high quality representations of features from raw sensory inputs. By using GANs to generate dense ground-level views from overhead imagery, we are able to create rich, detailed datasets for training downstream models. We demonstrate the effectiveness of our method using several quantitative metrics and qualitatively show impressive results. As more applications emerge that require accurate perception of the world, the importance of approaches such as ours cannot be overstated.",1
"We describe a novel method of generating high-resolution real-world images of text where the style and textual content of the images are described parametrically. Our method combines text to image retrieval techniques with progressive growing of Generative Adversarial Networks (PGGANs) to achieve conditional generation of photo-realistic images that reflect specific styles, as well as artifacts seen in real-world images. We demonstrate our method in the context of automotive license plates. We assess the impact of varying the number of training images of each style on the fidelity of the generated style, and demonstrate the quality of the generated images using license plate recognition systems.",0
"Here we present our latest work in the field of computer graphics and machine learning. We focus specifically on the generation of realistic images that combine textual content with stylized backgrounds using generative adversarial networks (GAN). Our approach utilizes progressive growing GANs (PGGANs) which have been shown to produce high quality results in other domains such as image completion and super resolution. We demonstrate the effectiveness of our method by generating a variety of synthesized images with detailed textures and intricate patterns in the background while preserving clarity in the overlaid text. This technique has applications in digital advertising, graphic design, and visual storytelling among others. We believe that our work further pushes the boundaries of what is possible in computer generated imagery and expands the potential for creative expression through technology.",1
"In this study, we propose a novel deep learning-based method to predict an optimized structure for a given boundary condition and optimization setting without using any iterative scheme. For this purpose, first, using open-source topology optimization code, datasets of the optimized structures paired with the corresponding information on boundary conditions and optimization settings are generated at low (32 x 32) and high (128 x 128) resolutions. To construct the artificial neural network for the proposed method, a convolutional neural network (CNN)-based encoder and decoder network is trained using the training dataset generated at low resolution. Then, as a two-stage refinement, the conditional generative adversarial network (cGAN) is trained with the optimized structures paired at both low and high resolutions, and is connected to the trained CNN-based encoder and decoder network. The performance evaluation results of the integrated network demonstrate that the proposed method can determine a near-optimal structure in terms of pixel values and compliance with negligible computational time.",0
"In recent years, deep learning has emerged as a powerful tool for solving complex problems across many domains. One area where it shows great promise is in the field of topology optimization, which involves finding the optimal material distribution within a given design space that meets certain performance criteria. Traditional methods often involve iterative approaches that can be time-consuming and computationally expensive, but recent advances in deep learning have enabled the development of novel algorithms that can determine a near-optimal topological design directly from input parameters. These algorithms utilize convolutional neural networks (CNNs) to extract features from the design domain and learn a mapping between these features and the corresponding material distributions. By training on large datasets of existing designs and their associated performance metrics, they can then make accurate predictions for new designs without the need for iterations. This work presents such a deep learning approach for determining a near-optimal topological design without any iteration, demonstrating its effectiveness through extensive numerical experiments and comparisons against state-of-the-art solvers. The results show that the proposed method outperforms these traditional methods in terms of both speed and accuracy, highlighting the potential of deep learning in enhancing the productivity and creativity of engineering design processes.",1
"Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.",0
"Abstract ------------------ The StarGAN architecture introduced in this paper represents a significant advancement in generative adversarial networks (GANs) for multi-domain image-to-image translation. Our method achieves state-of-the-art results by leveraging a novel design that unifies the multiple domains into a single framework, allowing for more efficient training and improved performance compared to previous approaches. Furthermore, our network benefits from attention mechanisms that enable fine-grained control over specific attributes within each domain, further enhancing its ability to generate high quality translations across a wide range of applications. We evaluate the effectiveness of our model on several benchmark datasets and demonstrate its superiority against other popular GAN architectures through both quantitative and qualitative analysis. Overall, the proposed StarGAN approach holds great promise for future research in the field of computer vision and graphics, as well as potential real-world applications such as photo editing and synthetic data generation.",1
"Unsupervised learning with generative adversarial networks (GANs) has proven to be hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss for both the discriminator and the generator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\chi^2$ divergence. We also show that the derived objective function that yields minimizing the Pearson $\chi^2$ divergence performs better than the classical one of using least squares for classification. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stably during the learning process. For evaluating the image quality, we conduct both qualitative and quantitative experiments, and the experimental results show that LSGANs can generate higher quality images than regular GANs. Furthermore, we evaluate the stability of LSGANs in two groups. One is to compare between LSGANs and regular GANs without gradient penalty. We conduct three experiments, including Gaussian mixture distribution, difficult architectures, and a newly proposed method --- datasets with small variability, to illustrate the stability of LSGANs. The other one is to compare between LSGANs with gradient penalty (LSGANs-GP) and WGANs with gradient penalty (WGANs-GP). The experimental results show that LSGANs-GP succeed in training for all the difficult architectures used in WGANs-GP, including 101-layer ResNet.",0
"This should be written in 3rd person limited point of view. Please note that I have provided you with some key points that need to go into the abstract but please use your own style and language to write up the actual abstract. Key Points: * GAN architecture was first introduced by Ian Goodfellow et al., in a 2014 paper published in the journal NIPS titled ""Generative Adversarial Networks"". * Since then there has been significant interest in developing GAN architectures tailored specifically towards improving their effectiveness on least squares problems, primarily in computer vision applications, where data is high dimensional and often noisy. * In this work we present LSGAN, our novel contribution which combines ideas from traditional linear regression approaches such as Ridge Regression with the adversarial training mechanism introduced in standard GANs. We evaluate the performance of LSGAN across several benchmark datasets commonly used to test image generation tasks against other state-of-the-art methods. Abstract: With the rapid advancements made in deep learning over recent years, researchers continue to explore new techniques aimed at improving generative modeling. One approach gaining attention involves using variants of Generative Adversarial Network (GAN) architectures specifically designed to tackle least squares problems common in many real world applications. Motivated by these efforts, we propose Least Squares Generative Adversarial Networks (LSGAN), combining ridge regression techniques traditionally used in linear models with the GAN framework. Evaluations performed on popular benchmark datasets demonstrate competitive results when compared against contemporary methods.",1
"In this paper, we propose a deep generative adversarial network for super-resolution considering the trade-off between perception and distortion. Based on good performance of a recently developed model for super-resolution, i.e., deep residual network using enhanced upscale modules (EUSR), the proposed model is trained to improve perceptual performance with only slight increase of distortion. For this purpose, together with the conventional content loss, i.e., reconstruction loss such as L1 or L2, we consider additional losses in the training phase, which are the discrete cosine transform coefficients loss and differential content loss. These consider perceptual part in the content loss, i.e., consideration of proper high frequency components is helpful for the trade-off problem in super-resolution. The experimental results show that our proposed model has good performance for both perception and distortion, and is effective in perceptual super-resolution applications.",0
"This paper proposes a new method for generating high resolution images from low resolution input using generative Adversarial Networks (GANs). Our approach uses both perceptual loss functions based on deep features extracted from convolutional neural networks (CNN) to better align the generated output with human perception, as well as traditional pixelwise loss functions such as L2 distance to improve overall fidelity of the output. We demonstrate that our model outperforms state-of-the-art methods across several benchmark datasets by achieving higher visual quality and stronger adherence to object boundaries. Additionally, we introduce a novel technique called progressive growing which incrementally increases the size of the generator until the final target resolution is reached without suffering from training instability issues common in other GAN approaches. Overall, our work pushes forward the frontier of image generation tasks through improved use of perceptual feedback and adaptive growth techniques. Title: ""Generative Adversarial Networks For Image Super-Resolution Using Perceptual Content Losses"" Abstract: In recent years, Generative Adversarial Networks (GANs) have shown promise in generating high resolution images from low resolution inputs, known as image super-resolution. However, current approaches often struggle to produce results that match human perception and suffer from training stability issues. To address these limitations, we propose a novel framework that utilizes perceptual loss functions based on deep features extracted from Convolutional Neural Networks (CNNs), along with traditional pixelwise loss functions like L2 distance. Through extensive experiments on multiple benchmark datasets, we show that our method significantly outperforms existing state-of-the-art models by producing higher visual quality outputs while more accurately preserving object boundaries. Furthermore, we introduce a progressive growing technique that gradually increases the size of the generator until reaching the desired target resolution without encountering common stability problems encountered in previous works. Our findings highlight the potential of incorporating perceptual feedback into GAN architectures and demonstrate how adaptive growth techniques can further improve performance. Ultimately, this research represents a significant advancement towards the goal of generating highly realistic and detailed images at unprecedented scales.",1
"This paper addresses the problem of path prediction for multiple interacting agents in a scene, which is a crucial step for many autonomous platforms such as self-driving cars and social robots. We present \textit{SoPhie}; an interpretable framework based on Generative Adversarial Network (GAN), which leverages two sources of information, the path history of all the agents in a scene, and the scene context information, using images of the scene. To predict a future path for an agent, both physical and social information must be leveraged. Previous work has not been successful to jointly model physical and social interactions. Our approach blends a social attention mechanism with a physical attention that helps the model to learn where to look in a large scene and extract the most salient parts of the image relevant to the path. Whereas, the social attention component aggregates information across the different agent interactions and extracts the most important trajectory information from the surrounding neighbors. SoPhie also takes advantage of GAN to generates more realistic samples and to capture the uncertain nature of the future paths by modeling its distribution. All these mechanisms enable our approach to predict socially and physically plausible paths for the agents and to achieve state-of-the-art performance on several different trajectory forecasting benchmarks.",0
"In our world today we rely on navigation systems every day whether through apps like Google Maps, Waze or even asking locals for directions. These technologies help us navigate from point A to B efficiently but often lack accuracy as they don't take into consideration social norms and physical constraints that may hinder a route (e.g., one way streets). This study presents SoPhie: an attentive Generative Adversarial Network (GAN) capable of predicting paths compliant to both social and physical constraints. Our proposed method takes advantage of attention mechanisms to focus on areas relevant to the prediction task, effectively reducing the search space while increasing efficiency. To validate our model, we conducted experiments using two datasets, demonstrating state-of-the-art performance compared to other GAN baselines and traditional graph-based pathfinding algorithms on metrics such as success rate, distance traveled before finding correct direction, mean reciprocal rank, and perplexity score. We also performed qualitative analysis by generating heatmaps visualizing learned attention weights during inference. Additionally, we examined how different architectures impacted overall results. Our findings show promise in utilizing GANs for navigational tasks that require realism beyond what current methods offer while still maintaining compliance with important societal norms and physical limitations. Future work could explore integrating more advanced human knowledge to further refine predictions and address ethical concerns surrounding autonomous decision making in navigation.",1
"Fingerprint alteration, also referred to as obfuscation presentation attack, is to intentionally tamper or damage the real friction ridge patterns to avoid identification by an AFIS. This paper proposes a method for detection and localization of fingerprint alterations. Our main contributions are: (i) design and train CNN models on fingerprint images and minutiae-centered local patches in the image to detect and localize regions of fingerprint alterations, and (ii) train a Generative Adversarial Network (GAN) to synthesize altered fingerprints whose characteristics are similar to true altered fingerprints. A successfully trained GAN can alleviate the limited availability of altered fingerprint images for research. A database of 4,815 altered fingerprints from 270 subjects, and an equal number of rolled fingerprint images are used to train and test our models. The proposed approach achieves a True Detection Rate (TDR) of 99.24% at a False Detection Rate (FDR) of 2%, outperforming published results. The synthetically generated altered fingerprint dataset will be open-sourced.",0
"The ability to alter fingerprints has become increasingly prevalent due to advances in technology and accessibility. As such, traditional fingerprint recognition systems may no longer provide adequate security measures. This study aimed to address the challenge posed by altered fingerprints by developing methods for their detection and localization. Our approach involved analyzing both visible and latent alterations made to fingerprint patterns using advanced image processing techniques. We evaluated our method on a diverse set of artificial and real-world fingerprint pairs, demonstrating high accuracy in detecting and locating alterations. In addition, we compared our results against those from existing approaches, illustrating significant improvements in detection performance. These findings have important implications for the development and implementation of secure authentication systems that can effectively counteract threats posed by altered fingerprints.",1
"Single image superresolution has been a popular research topic in the last two decades and has recently received a new wave of interest due to deep neural networks. In this paper, we approach this problem from a different perspective. With respect to a downsampled low resolution image, we model a high resolution image as a combination of two components, a deterministic component and a stochastic component. The deterministic component can be recovered from the low-frequency signals in the downsampled image. The stochastic component, on the other hand, contains the signals that have little correlation with the low resolution image. We adopt two complementary methods for generating these two components. While generative adversarial networks are used for the stochastic component, deterministic component reconstruction is formulated as a regression problem solved using deep neural networks. Since the deterministic component exhibits clearer local orientations, we design novel loss functions tailored for such properties for training the deep regression network. These two methods are first applied to the entire input image to produce two distinct high-resolution images. Afterwards, these two images are fused together using another deep neural network that also performs local statistical rectification, which tries to make the local statistics of the fused image match the same local statistics of the groundtruth image. Quantitative results and a user study indicate that the proposed method outperforms existing state-of-the-art algorithms with a clear margin.",0
"This paper presents a novel approach to image super-resolution that combines deterministic-stochastic synthesis and local statistical rectification. Our method leverages a generative model to generate high-quality, upscaled versions of low-resolution images while maintaining key features such as edges and corners. We then use local statistics to further improve the quality of these images by adjusting their color distributions to better match those of the original image. Experimental results show that our method outperforms state-of-the-art methods on several benchmark datasets and produces more visually pleasing outputs. Overall, we demonstrate the effectiveness and potential applications of combining deterministic-stochastic synthesis and local statistical rectification for image super-resolution.",1
"The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at https://github.com/xinntao/ESRGAN .",0
"""Enhancing super-resolution generative adversarial networks using advanced techniques can significantly improve their performance."" This statement sets out the goal of our work: We aimed to develop enhanced super-resolution generative adversarial networks (ESRGAN) by incorporating new methods that would lead to better results. In recent years, several advances have been made in the field of image super-resolution (SR). These advancements mainly focused on developing deep convolutional neural network architectures that learn from large amounts of data. However, traditional SR models often result in blurry images due to their limited capacity to model complex details and structures. To overcome these limitations, we proposed an approach that combines both discriminator- and content lossâ€“based objectives during training. Experimental evaluation showed that our method substantially improved the quality of the generated outputs compared to previous state-of-the-art algorithms for SR reconstruction tasks. Thus, our findings offer valuable insights into how to advance research efforts in this domain.",1
"The tracking-by-detection framework usually consist of two stages: drawing samples around the target object in the first stage and classifying each sample as the target object or background in the second stage. Current popular trackers based on tracking-by-detection framework typically draw samples in the raw image as the inputs of deep convolution networks in the first stage, which usually results in high computational burden and low running speed. In this paper, we propose a new visual tracking method using sampling deep convolutional features to address this problem. Only one cropped image around the target object is input into the designed deep convolution network and the samples is sampled on the feature maps of the network by spatial bilinear resampling. In addition, a generative adversarial network is integrated into our network framework to augment positive samples and improve the tracking performance. Extensive experiments on benchmark datasets demonstrate that the proposed method achieves a comparable performance to state-of-the-art trackers and accelerates tracking-by-detection trackers based on raw-image samples effectively.",0
"In ""Adversarial Feature Sampling"" we present novel feature learning methods designed explicitly to address modern computer vision challenges such as tracking objects within videos. Our algorithms learn robust features which can then be used to make object detection systems more efficient by reducing computation time while still achieving good performance. This improvement comes from training deep neural networks on data that is specifically generated to confuse other classifiers. We show how this leads to state of art performance at realtime frame rates when applied to visual tracking.",1
"Navigating surgical tools in the dynamic and tortuous anatomy of the lung's airways requires accurate, real-time localization of the tools with respect to the preoperative scan of the anatomy. Such localization can inform human operators or enable closed-loop control by autonomous agents, which would require accuracy not yet reported in the literature. In this paper, we introduce a deep learning architecture, called OffsetNet, to accurately localize a bronchoscope in the lung in real-time. After training on only 30 minutes of recorded camera images in conserved regions of a lung phantom, OffsetNet tracks the bronchoscope's motion on a held-out recording through these same regions at an update rate of 47 Hz and an average position error of 1.4 mm. Because this model performs poorly in less conserved regions, we augment the training dataset with simulated images from these regions. To bridge the gap between camera and simulated domains, we implement domain randomization and a generative adversarial network (GAN). After training on simulated images, OffsetNet tracks the bronchoscope's motion in less conserved regions at an average position error of 2.4 mm, which meets conservative thresholds required for successful tracking.",0
"This paper introduces OffsetNet, a deep learning approach to localizing structures in rendered lung images. We demonstrate that rendering high quality synthetic training data is feasible by exploiting graphics processors, which allows us to train accurate neural networks to perform this task even in the absence of ground truth annotations on real scans. Our results show that OffsetNet significantly outperforms previous state-of-the-art methods for lung nodule detection, including traditional feature based approaches as well as convolutional neural network (CNN) architectures trained end-to-end. Additionally, we evaluate our method on other pulmonary applications like airway segmentation and lobe demarcation, illustrating the versatility of OffsetNet across multiple tasks. We make all code and models publicly available to facilitate reproducibility and further research in the field.  This work contributes to the growing trend in medical imaging research where computer generated images are used to augment scarce annotation resources. With advancements in GPU hardware, generating large datasets from scratch has become more accessible than ever before. Incorporating these renderings can offer new perspectives into understanding and developing image analysis tools for challenges that have largely eluded progress over the last decade due to data constraints. Our hope is that such innovations will pave the way towards accelerating development of artificial intelligence algorithms for medicine, ultimately leading to improved patient care.  We would like to thank NVIDIA Corporation for donating their Titan X GPUs to enable efficient exploration of ideas within the context of this study. We appreciate support provided by NSERC through Canadian Graduate Scholarships (CGS), CGS Michael Smith Foreign Study Supplements, and Strategic Partnership grants. Finally, acknowledgments goe",1
"Face frontalization is the process of synthesizing frontal facing views of faces given its angled poses. We implement a generative adversarial network (GAN) with spherical linear interpolation (Slerp) for frontalization of unconstrained facial images. Our special focus is intended towards the generation of approximate frontal faces of the side posed images captured from surveillance cameras. Specifically, the present work is a comprehensive study on the implementation of an auto-encoder based Boundary Equilibrium GAN (BEGAN) to generate frontal faces using an interpolation of a side view face and its mirrored view. To increase the quality of the interpolated output we implement a BEGAN with Slerp. This approach could produce a promising output along with a faster and more stable training for the model. The BEGAN model additionally has a balanced generator-discriminator combination, which prevents mode collapse along with a global convergence measure. It is expected that such an approximate face generation model would be able to replace face composites used in surveillance and crime detection.",0
"This study presents a new method using Boundary Equilibrium Generative Adversarial Networks (BE-GAN) for frontalizing unconstrained face images. BE-GAN was chosen due to its robustness against overfitting and ability to generate high quality outputs in large databases. The proposed method outperforms other state-of-the art methods, such as Cyclic Face Synthesis, UNET++, and Disentangled Representation Learning. Furthermore, our algorithm exhibits better generalization performance when applied in real surveillance scenarios. The study concludes that the proposed approach has great potential applications in security and identification tasks.",1
"The ability to anticipate the future is essential when making real time critical decisions, provides valuable information to understand dynamic natural scenes, and can help unsupervised video representation learning. State-of-art video prediction is based on LSTM recursive networks and/or generative adversarial network learning. These are complex architectures that need to learn large numbers of parameters, are potentially hard to train, slow to run, and may produce blurry predictions. In this paper, we introduce DYAN, a novel network with very few parameters and easy to train, which produces accurate, high quality frame predictions, significantly faster than previous approaches. DYAN owes its good qualities to its encoder and decoder, which are designed following concepts from systems identification theory and exploit the dynamics-based invariants of the data. Extensive experiments using several standard video datasets show that DYAN is superior generating frames and that it generalizes well across domains.",0
"In this work we present a novel approach for video prediction using neural networks. Unlike traditional methods which typically use convolutional models to perform image generation at test time, our method dynamically generates new atoms at each step by encoding past frames into a latent space that serves as a memory buffer. This allows us to significantly reduce computational requirements while improving accuracy over state of the art approaches on several benchmark datasets such as Kinetics 400 and Something V2. We demonstrate through qualitative and quantitative analysis that our model can generate high quality predictions even under uncertainty due to objects moving outside the field of view or occlusions. Additionally, we showcase the generalization capabilities of our approach across multiple tasks including action recognition and future frame interpolation. Our proposed method sets a new standard for performance in video prediction and offers exciting opportunities for real world applications such as autonomous systems and surveillance scenarios where accurate temporal forecasting is crucial.",1
"Can an algorithm create original and compelling fashion designs to serve as an inspirational assistant? To help answer this question, we design and investigate different image generation models associated with different loss functions to boost creativity in fashion generation. The dimensions of our explorations include: (i) different Generative Adversarial Networks architectures that start from noise vectors to generate fashion items, (ii) novel loss functions that encourage novelty, inspired from Sharma-Mittal divergence, a generalized mutual information measure for the widely used relative entropies such as Kullback-Leibler, and (iii) a generation process following the key elements of fashion design (disentangling shape and texture components). A key challenge of this study is the evaluation of generated designs and the retrieval of best ones, hence we put together an evaluation protocol associating automatic metrics and human experimental studies that we hope will help ease future research. We show that our proposed creativity criterion yield better overall appreciation than the one employed in Creative Adversarial Networks. In the end, about 61% of our images are thought to be created by human designers rather than by a computer while also being considered original per our human subject experiments, and our proposed loss scores the highest compared to existing losses in both novelty and likability.",0
"Artificial intelligence has been rapidly advancing over recent years, inspiring many innovations across different fields such as design. One particular approach that has gained significant attention is generative networks. These algorithms are capable of producing highly creative outputs by learning patterns within large datasets. This paper investigates how generative networks can influence and enhance human-centered designs, particularly in areas where creativity and originality play crucial roles. By analyzing examples of existing applications using generative networks, we identify potential benefits in terms of novelty and personalization, while addressing challenges regarding usability, accessibility, and ethics. Our findings highlight the enormous opportunities ahead for designers who collaborate with these advanced technologies. Overall, our work contributes to a broader discourse on the integration of artificial intelligence into the creative realm, paving the way for future research and development. ===== AI language models have shown impressive capabilities in generating meaningful text, images, and sounds. Recent works like Stable Diffusion [7] and Imagen [6], for example, achieve state-of-the-art results on image generation tasks at unprecedented sizes (up to millions of parameters). They often outperform traditional methods due to their ability to capture high-level structures implicitly learned from massive training sets.  Designers and artists may benefit greatly from these technological developments, opening up new ways to create, iterate, and experiment with ideas at low cost. Moreover, incorporating generative systems into workflows could enable fresh approaches to user experience and engagement, allowing users greater flexibility and customization options. While there already exist some projects utilizing deep neural networks for generative purposes, their use remains relatively niche. Furthermore, important considerations related to usage scenarios, interaction paradigms, evaluation metrics, and social implications have yet t",1
"Data diversity is critical to success when training deep learning models. Medical imaging data sets are often imbalanced as pathologic findings are generally rare, which introduces significant challenges when training deep learning models. In this work, we propose a method to generate synthetic abnormal MRI images with brain tumors by training a generative adversarial network using two publicly available data sets of brain MRI. We demonstrate two unique benefits that the synthetic images provide. First, we illustrate improved performance on tumor segmentation by leveraging the synthetic images as a form of data augmentation. Second, we demonstrate the value of generative models as an anonymization tool, achieving comparable tumor segmentation results when trained on the synthetic data versus when trained on real subject data. Together, these results offer a potential solution to two of the largest challenges facing machine learning in medical imaging, namely the small incidence of pathological findings, and the restrictions around sharing of patient data.",0
"This paper presents a novel approach to medical image synthesis utilizing generative adversarial networks (GANs) for data augmentation and anonymization purposes. As large datasets are crucial for training machine learning models, generating new images that are both realistic and diverse can greatly aid in improving model performance. Additionally, patient privacy concerns have become increasingly important in recent years, leading to a need for methods capable of effectively anonymizing medical images while maintaining their diagnostic value. Our proposed method addresses these issues by leveraging GANs to generate high-quality synthetic images that closely match real data distributions, as well as provide control over specific attributes such as anatomy or pathology. Extensive experiments demonstrate the effectiveness of our approach in terms of both visual fidelity and utility for downstream tasks. Our work contributes towards building robust machine learning systems in healthcare domains, where accessibility to data may be limited due to confidentiality constraints.",1
"Generative adversarial networks are a novel method for statistical inference that have achieved much empirical success; however, the factors contributing to this success remain ill-understood. In this work, we attempt to analyze generative adversarial learning -- that is, statistical inference as the result of a game between a generator and a discriminator -- with the view of understanding how it differs from classical statistical inference solutions such as maximum likelihood inference and the method of moments.   Specifically, we provide a theoretical characterization of the distribution inferred by a simple form of generative adversarial learning called restricted f-GANs -- where the discriminator is a function in a given function class, the distribution induced by the generator is restricted to lie in a pre-specified distribution class and the objective is similar to a variational form of the f-divergence. A consequence of our result is that for linear KL-GANs -- that is, when the discriminator is a linear function over some feature space and f corresponds to the KL-divergence -- the distribution induced by the optimal generator is neither the maximum likelihood nor the method of moments solution, but an interesting combination of both.",0
"In recent years, Generative Adversarial Networks (GANs) have become increasingly popular in generating synthetic data that closely resembles real-world examples. However, one major issue with GANs is that they suffer from mode collapse, where the generator learns to produce samples that cluster together instead of spreading out evenly over the entire space of possible outputs. To address this problem, several solutions have been proposed such as adding regularization terms, changing the architecture of the discriminator, using different loss functions, etc. One approach that has gained popularity recently is to use restricted Boltzmann machines (RBMs) as a regularizer in the GAN framework. This method has shown promising results but little is known about how the choice of parameters affects the performance of RBMs used in GANs.  In this work, we examine the inductive bias introduced by RBM regularizers in GANs. We analyze how different choices of hyperparameters in the RBM can influence the performance of the overall model. Our analysis shows that certain configurations of the RBM lead to better performance while others may worsen the generated images. Additionally, our experiments indicate that there exists a tradeoff between the strength of the regularization term and the expressiveness of the model. By carefully selecting the appropriate configuration, we observe significant improvements in image quality compared to previous methods. Furthermore, we provide insights into why these observations hold true through detailed visualizations and statistical analyses. Overall, our findings shed light on the role played by RBM regularizers in GAN architectures and suggest new directions for future research in this area.",1
"Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].",0
"Deep learning has revolutionized fields such as computer vision by enabling computers to learn features directly from raw data without human intervention. Since the seminal work by Krizhevsky et al., which popularized convolutional neural networks (CNNs), deep learning approaches have become increasingly popular due to their ability to extract high-level representations that capture complex patterns in large-scale datasets. In this comprehensive survey, we focus on recent advances in CNN architectures, including VGGNet, ResNet, DenseNet, MobileNets, U-nets, and many others. We provide detailed descriptions of each architecture and discuss their strengths and weaknesses. Additionally, we analyze key trends across different CNN applications, including image classification, object detection, semantic segmentation, and video understanding tasks. Finally, we review recent developments in hyperparameter tuning, model compression, transfer learning, and explainability techniques in deep learning research. This survey provides a valuable resource for researchers and practitioners who want to understand state-of-the-art methods in computer vision using deep learning.",1
"The large pose discrepancy between two face images is one of the fundamental challenges in automatic face recognition. Conventional approaches to pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes a Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator enables DR-GAN to learn a representation that is both generative and discriminative, which can be used for face image synthesis and pose-invariant face recognition. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified identity representation along with an arbitrary number of synthetic face images. Extensive quantitative and qualitative evaluation on a number of controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art in both learning representations and rotating large-pose face images.",0
"In recent years, representation learning has become increasingly important in computer vision tasks such as image classification, object detection, and face recognition. One popular approach to representation learning is through convolutional neural networks (CNNs), which have achieved state-of-the-art performance on many benchmark datasets. However, training CNNs can be computationally expensive and requires large amounts of labeled data, making it difficult to scale up to larger datasets or more complex models. In this paper, we propose a new method called ""Face Rotation for Transfer Learning"" that allows us to generate synthetic samples from existing labeled datasets without requiring any additional annotations. By applying random transformations to facial images during training, our model learns a robust representation that generalizes well across different poses and illumination conditions. Our experiments demonstrate that Face Rotation significantly outperforms traditional methods of data augmentation and achieves comparable results to fine-tuning pretrained CNNs on large datasets. We believe that this simple yet effective technique could open up new possibilities for representation learning, especially for applications where labeling data may be scarce or time-consuming.",1
"We present a cross-modality generation framework that learns to generate translated modalities from given modalities in MR images without real acquisition. Our proposed method performs NeuroImage-to-NeuroImage translation (abbreviated as N2N) by means of a deep learning model that leverages conditional generative adversarial networks (cGANs). Our framework jointly exploits the low-level features (pixel-wise information) and high-level representations (e.g. brain tumors, brain structure like gray matter, etc.) between cross modalities which are important for resolving the challenging complexity in brain structures. Our framework can serve as an auxiliary method in clinical diagnosis and has great application potential. Based on our proposed framework, we first propose a method for cross-modality registration by fusing the deformation fields to adopt the cross-modality information from translated modalities. Second, we propose an approach for MRI segmentation, translated multichannel segmentation (TMS), where given modalities, along with translated modalities, are segmented by fully convolutional networks (FCN) in a multichannel manner. Both of these two methods successfully adopt the cross-modality information to improve the performance without adding any extra data. Experiments demonstrate that our proposed framework advances the state-of-the-art on five brain MRI datasets. We also observe encouraging results in cross-modality registration and segmentation on some widely adopted brain datasets. Overall, our work can serve as an auxiliary method in clinical diagnosis and be applied to various tasks in medical fields.   Keywords: image-to-image, cross-modality, registration, segmentation, brain MRI",0
"Medical imaging techniques such as Magnetic Resonance Imaging (MRI) have become essential tools for diagnosing and monitoring diseases within the human brain. However, analyzing these images can be challenging due to differences in acquisition parameters, protocols, field strengths, and even scanners between different hospitals and institutions. As a result, there exists a need for methods that can transform neuroimages from one modality to another while preserving their fidelity. In this work, we propose a novel approach that enables cross-modality translation between MRI modalities through deep neural networks trained on large datasets. We demonstrate the effectiveness of our method by translating T1-weighted images to T2-FLAIR images using both qualitative and quantitative evaluations, including visual inspection by radiologists and objective metrics such as structural similarity index and peak signal-to-noise ratio. Our results show that our proposed model achieves significant improvements over previous state-of-the-art methods in terms of visual quality and objective measures, making it a valuable tool for clinical practice and research applications. Overall, our study highlights the potential of deep learning algorithms in bridging the gap between various medical image modalities, ultimately leading to improved patient outcomes and scientific discoveries.",1
"In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs.   We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function.   Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.",0
"Title: Enhancing Generative Adversarial Networks using Relativistic Discrimination  Abstract: Artificial intelligence has revolutionized many fields through advanced machine learning techniques such as generative adversarial networks (GAN). However, current GAN architectures suffer from some fundamental limitations that restrict their performance in generating high quality outputs. One of these critical bottlenecks lies in the lack of a suitable means to measure relative similarity among generated samples, which hinders effective training by making it difficult to optimize loss functions efficiently. This work introduces a novel approach called ""Relativistic Discriminator"" to address this shortcoming in traditional GAN models. Our proposed method adaptively computes an upper bound on the likelihood ratio, enabling efficient evaluation of sample relationships without explicit access to ground truth labels or priors. We showcase the effectiveness of our approach across a range of image generation tasks, demonstrating significant improvements over existing methods in terms of visual fidelity and quantitative metrics. Our findings have important implications for research in computer vision, natural language processing, and other domains where generating coherent and realistic data plays a crucial role. Overall, we believe this work represents a major step towards establishing more reliable artificial intelligence systems capable of achieving human-level performance in complex applications.",1
"Detecting anomalous activity in human mobility data has a number of applications including road hazard sensing, telematic based insurance, and fraud detection in taxi services and ride sharing. In this paper we address two challenges that arise in the study of anomalous human trajectories: 1) a lack of ground truth data on what defines an anomaly and 2) the dependence of existing methods on significant pre-processing and feature engineering. While generative adversarial networks seem like a natural fit for addressing these challenges, we find that existing GAN based anomaly detection algorithms perform poorly due to their inability to handle multimodal patterns. For this purpose we introduce an infinite Gaussian mixture model coupled with (bi-directional) generative adversarial networks, IGMM-GAN, that is able to generate synthetic, yet realistic, human mobility data and simultaneously facilitates multimodal anomaly detection. Through estimation of a generative probability density on the space of human trajectories, we are able to generate realistic synthetic datasets that can be used to benchmark existing anomaly detection methods. The estimated multimodal density also allows for a natural definition of outlier that we use for detecting anomalous trajectories. We illustrate our methodology and its improvement over existing GAN anomaly detection on several human mobility datasets, along with MNIST.",0
"This paper presents a new approach to detecting anomalies in human mobility data by leveraging coupled Inverse Graph Mapping Models (IGMM) and Generative Adversarial Networks (GANs). Anomaly detection in multimodal datasets can be challenging due to the presence of multiple sources of uncertainty and variability. However, our proposed method combines the strengths of two powerful generative models: IGMM, which can capture complex dependencies among variables and latent structures in high-dimensional spaces; and GANs, which provide powerful generative capabilities and have been shown to be effective at generating realistic synthetic data. We apply these coupled models to human mobility data from multiple modalities such as GPS location, cell phone usage logs, and social media activity. Our results show that our method outperforms state-of-the-art methods in detecting both structured and unstructured anomalies while preserving privacy and reducing computational complexity. Overall, we demonstrate the effectiveness of using coupled IGMM-GANs for detecting anomalies in large-scale, multi-modal human mobility datasets.",1
"Class imbalanced datasets are common in real-world applications that range from credit card fraud detection to rare disease diagnostics. Several popular classification algorithms assume that classes are approximately balanced, and hence build the accompanying objective function to maximize an overall accuracy rate. In these situations, optimizing the overall accuracy will lead to highly skewed predictions towards the majority class. Moreover, the negative business impact resulting from false positives (positive samples incorrectly classified as negative) can be detrimental. Many methods have been proposed to address the class imbalance problem, including methods such as over-sampling, under-sampling and cost-sensitive methods. In this paper, we consider the over-sampling method, where the aim is to augment the original dataset with synthetically created observations of the minority classes. In particular, inspired by the recent advances in generative modelling techniques (e.g., Variational Inference and Generative Adversarial Networks), we introduce a new oversampling technique based on variational autoencoders. Our experiments show that the new method is superior in augmenting datasets for downstream classification tasks when compared to traditional oversampling methods.",0
"In recent years, addressing class imbalance has become an important problem in machine learning research due to its impact on model performance and prediction accuracy. Traditional oversampling methods have been proposed but they often introduce more noise into the data and distort the distribution of the minority classes. This study proposes a new method called VOS (Variational Oversampling) that utilizes variational autoencoders to generate synthetic samples from the majority class and balance the class distribution. We evaluate VOS using several benchmark datasets and compare its effectiveness against state-of-the-art oversampling techniques. Our results show that VOS outperforms traditional approaches by achieving higher F1 scores while reducing overfitting and underfitting issues. Additionally, we provide visualizations of the generated synthetic samples to demonstrate their quality. Overall, our work contributes to the field by introducing a novel approach for balancing class distributions, which can improve prediction accuracy in imbalanced classification tasks.",1
"Reliable facial expression recognition plays a critical role in human-machine interactions. However, most of the facial expression analysis methodologies proposed to date pay little or no attention to the protection of a user's privacy. In this paper, we propose a Privacy-Preserving Representation-Learning Variational Generative Adversarial Network (PPRL-VGAN) to learn an image representation that is explicitly disentangled from the identity information. At the same time, this representation is discriminative from the standpoint of facial expression recognition and generative as it allows expression-equivalent face image synthesis. We evaluate the proposed model on two public datasets under various threat scenarios. Quantitative and qualitative results demonstrate that our approach strikes a balance between the preservation of privacy and data utility. We further demonstrate that our model can be effectively applied to other tasks such as expression morphing and image completion.",0
"Facial expression recognition has become increasingly important in fields such as psychology, human computer interaction, and surveillance systems. However, traditional methods have limitations in terms of accuracy, robustness, and privacy preservation. In this study, we propose a novel approach using variational generative adversarial networks (VGANs) to learn image representations that preserve privacy while maintaining high levels of facial expression recognition accuracy. We train our model on a large dataset of faces with diverse expressions, and evaluate its performance through several experiments. Our results demonstrate that VGAN-based representation learning achieves state-of-the-art accuracy while ensuring greater privacy protection compared to existing methods. This work provides new insights into the use of GANs for image processing tasks, particularly those involving sensitive data like facial expressions.",1
"Bubble segmentation and size detection algorithms have been developed in recent years for their high efficiency and accuracy in measuring bubbly two-phase flows. In this work, we proposed an architecture called bubble generative adversarial networks (BubGAN) for the generation of realistic synthetic images which could be further used as training or benchmarking data for the development of advanced image processing algorithms. The BubGAN is trained initially on a labeled bubble dataset consisting of ten thousand images. By learning the distribution of these bubbles, the BubGAN can generate more realistic bubbles compared to the conventional models used in the literature. The trained BubGAN is conditioned on bubble feature parameters and has full control of bubble properties in terms of aspect ratio, rotation angle, circularity and edge ratio. A million bubble dataset is pre-generated using the trained BubGAN. One can then assemble realistic bubbly flow images using this dataset and associated image processing tool. These images contain detailed bubble information, therefore do not require additional manual labeling. This is more useful compared with the conventional GAN which generates images without labeling information. The tool could be used to provide benchmarking and training data for existing image processing algorithms and to guide the future development of bubble detecting algorithms.",0
"In general, image generation has been quite challenging due to many difficulties, such as training instability and mode dropping. Recent researchers have proposed LatentDiffusion which has improved on realism but is very slow. To address these issues we present bubble generative adversarial networks (BubGAN). Our method learns to synthesize images by predicting whether an additional noise sampled from a normal distribution improves the objective function value more than a generated sample without that noise. As a result, our model can generate high quality images of scenes containing bubbles rapidly.",1
"Generative adversarial networks (GANs) can be interpreted as an adversarial game between two players, a discriminator D and a generator G, in which D learns to classify real from fake data and G learns to generate realistic data by ""fooling"" D into thinking that fake data is actually real data. Currently, a dominating view is that G actually learns by minimizing a divergence given that the general objective function is a divergence when D is optimal. However, this view has been challenged due to inconsistencies between theory and practice. In this paper, we discuss of the properties associated with most loss functions for G (e.g., saturating/non-saturating f-GAN, LSGAN, WGAN, etc.). We show that these loss functions are not divergences and do not have the same equilibrium as expected of divergences. This suggests that G does not need to minimize the same objective function as D maximize, nor maximize the objective of D after swapping real data with fake data (non-saturating GAN) but can instead use a wide range of possible loss functions to learn to generate realistic data. We define GANs through two separate and independent D maximization and G minimization steps. We generalize the generator step to four new classes of loss functions, most of which are actual divergences (while traditional G loss functions are not). We test a wide variety of loss functions from these four classes on a synthetic dataset and on CIFAR-10. We observe that most loss functions converge well and provide comparable data generation quality to non-saturating GAN, LSGAN, and WGAN-GP generator loss functions, whether we use divergences or non-divergences. These results suggest that GANs do not conform well to the divergence minimization theory and form a much broader range of models than previously assumed.",0
"Artificial intelligence (AI) has revolutionized many fields such as image analysis and natural language processing. Generative Adversarial Networks (GANs) play a vital role in these applications by generating synthetic data that can fool humans into believing they are authentic. Traditionally, GAN training was focused on minimizing the Jensen-Shannon Divergence (JSD) loss function which measures how different two distributions are. Recent research efforts have shown improvements upon traditional GAN architectures by using other distance metrics, alternative discriminator loss functions, or modifying the generator architecture itself. Despite these advancements, understanding the impact of each change remains challenging due to insufficient comparisons across methods. In our work, we evaluate multiple variations of GANs and assess their corresponding strengths and weaknesses through comprehensive experimental evaluation. We showcase superior performance of new variants over traditional GAN architectures. Our study sets forth future directions aimed at unlocking novel ways for evaluating generative models beyond just accuracy.",1
"We initiate a way of generating models by the computer, satisfying both experimental and theoretical constraints. In particular, we present a framework which allows the generation of effective field theories. We use Generative Adversarial Networks to generate these models and we generate examples which go beyond the examples known to the machine. As a starting point, we apply this idea to the generation of supersymmetric field theories. In this case, the machine knows consistent examples of supersymmetric field theories with a single field and generates new examples of such theories. In the generated potentials we find distinct properties, here the number of minima in the scalar potential, with values not found in the training data. We comment on potential further applications of this framework.",0
"Generative Adversarial Networks (GANs) have revolutionized the field of computer vision by enabling the creation of highly realistic synthetic data. However, applying GANs to generate Eulerian video frames (EFT models) remains challenging due to their complex structure and motion patterns. In this paper, we propose a novel framework for training GANs on large scale video datasets to generate accurate EFT models. Our approach leverages temporal consistency constraints to ensure coherent movement across time and spatial attention mechanisms to improve local feature fidelity. We demonstrate that our method outperforms state-of-the-art approaches both quantitatively and qualitatively, achieving significant improvements in visual fidelity, while remaining efficient and scalable.",1
"Generative adversarial network (GAN) has gotten wide re-search interest in the field of deep learning. Variations of GAN have achieved competitive results on specific tasks. However, the stability of training and diversity of generated instances are still worth studying further. Training of GAN can be thought of as a greedy procedure, in which the generative net tries to make the locally optimal choice (minimizing loss function of discriminator) in each iteration. Unfortunately, this often makes generated data resemble only a few modes of real data and rotate between modes. To alleviate these problems, we propose a novel training strategy to restrict greed in training of GAN. With help of our method, the generated samples can cover more instance modes with more stable training process. Evaluating our method on several representative datasets, we demonstrate superiority of improved training strategy on typical GAN models with different distance metrics.",0
"In recent years there has been tremendous progress in training generative adversarial networks (GANs). Despite their effectiveness however, GANs suffer from instability issues such as mode collapse and training collapse. This results in poor performance on several benchmark datasets. In our work we propose a novel regularization method based on restricting greed. We introduce a new term into the loss function that measures the difference between generated samples and real ones. Our experiments show significant improvements over state-of-the art methods in terms of quality of the generated images. Moreover, our approach is computationally efficient compared to other techniques used for improving stability during training. Overall, our method provides a simple yet effective solution towards better stabilizing GAN training.",1
"As many other machine learning driven medical image analysis tasks, skin image analysis suffers from a chronic lack of labeled data and skewed class distributions, which poses problems for the training of robust and well-generalizing models. The ability to synthesize realistic looking images of skin lesions could act as a reliever for the aforementioned problems. Generative Adversarial Networks (GANs) have been successfully used to synthesize realistically looking medical images, however limited to low resolution, whereas machine learning models for challenging tasks such as skin lesion segmentation or classification benefit from much higher resolution data. In this work, we successfully synthesize realistically looking images of skin lesions with GANs at such high resolution. Therefore, we utilize the concept of progressive growing, which we both quantitatively and qualitatively compare to other GAN architectures such as the DCGAN and the LAPGAN. Our results show that with the help of progressive growing, we can synthesize highly realistic dermoscopic images of skin lesions that even expert dermatologists find hard to distinguish from real ones.",0
"This abstract describes a recent advancement made possible through the use of generative adversarial networks (GANs) for generating highly realistic images of skin lesions. The paper presents research on improving current state-of-the-art techniques by designing new models that utilize novel architectures and training procedures to enhance image quality while minimizing artifacts and noise. Using this approach allows for more accurate diagnosis of skin conditions and better evaluation of treatment efficacy. By creating these detailed and lifelike images, dermatologists and other healthcare professionals can improve patient outcomes and save lives. With further refinement, this technology has the potential to revolutionize the medical field as a whole.",1
"We propose Progressive Structure-conditional Generative Adversarial Networks (PSGAN), a new framework that can generate full-body and high-resolution character images based on structural information. Recent progress in generative adversarial networks with progressive training has made it possible to generate high-resolution images. However, existing approaches have limitations in achieving both high image quality and structural consistency at the same time. Our method tackles the limitations by progressively increasing the resolution of both generated images and structural conditions during training. In this paper, we empirically demonstrate the effectiveness of this method by showing the comparison with existing approaches and video generation results of diverse anime characters at 1024x1024 based on target pose sequences. We also create a novel dataset containing full-body 1024x1024 high-resolution images and exact 2D pose keypoints using Unity 3D Avatar models.",0
Automatically generate the abstract. Use at least two keywords from the paper title.,1
"In many domains of computer vision, generative adversarial networks (GANs) have achieved great success, among which the family of Wasserstein GANs (WGANs) is considered to be state-of-the-art due to the theoretical contributions and competitive qualitative performance. However, it is very challenging to approximate the $k$-Lipschitz constraint required by the Wasserstein-1 metric~(W-met). In this paper, we propose a novel Wasserstein divergence~(W-div), which is a relaxed version of W-met and does not require the $k$-Lipschitz constraint. As a concrete application, we introduce a Wasserstein divergence objective for GANs~(WGAN-div), which can faithfully approximate W-div through optimization. Under various settings, including progressive growing training, we demonstrate the stability of the proposed WGAN-div owing to its theoretical and practical advantages over WGANs. Also, we study the quantitative and visual performance of WGAN-div on standard image synthesis benchmarks of computer vision, showing the superior performance of WGAN-div compared to the state-of-the-art methods.",0
"The use of generative adversarial networks (GANs) has become increasingly popular for generating high quality synthetic data that can be used in a variety of applications such as image generation, image enhancement, image translation, etc. One important problem associated with training GANs is measuring how well they perform. In particular, existing measures based on maximum likelihood estimation or other criteria often lead to unsatisfactory results. To address this problem, we propose using the Wasserstein divergence as a measure of performance for GANs. We show that this measure provides several advantages over previous methods. First, it allows for easy comparison between different generator architectures and optimization techniques. Second, it gives insight into the tradeoff between the realism of generated images and their diversity. Finally, we demonstrate through experiments that our proposed method leads to better performing GANs than previous approaches. Our work opens up new possibilities for designing more effective GAN models and advancing state-of-the-art research in computer vision and machine learning.",1
"Contemporary deep learning based medical image segmentation algorithms require hours of annotation labor by domain experts. These data hungry deep models perform sub-optimally in the presence of limited amount of labeled data. In this paper, we present a data efficient learning framework using the recent concept of Generative Adversarial Networks; this allows a deep neural network to perform significantly better than its fully supervised counterpart in low annotation regime. The proposed method is an extension of our previous work with the addition of a new unsupervised adversarial loss and a structured prediction based architecture. To the best of our knowledge, this work is the first demonstration of an adversarial framework based structured prediction model for medical image segmentation. Though generic, we apply our method for segmentation of blood vessels in retinal fundus images. We experiment with extreme low annotation budget (0.8 - 1.6% of contemporary annotation size). On DRIVE and STARE datasets, the proposed method outperforms our previous method and other fully supervised benchmark models by significant margins especially with very low number of annotated examples. In addition, our systematic ablation studies suggest some key recipes for successfully training GAN based semi-supervised algorithms with an encoder-decoder style network architecture.",0
"The retinal vessel segmentation problem has received significant attention from researchers due to its importance in diagnosing various diseases such as diabetic retinopathy, glaucoma, and age-related macular degeneration (AMD). Accurate segmentation of retinal vessels is critical for analyzing changes in the microvasculature over time and identifying potential risk factors. However, creating pixelwise annotated data for training and evaluation can be extremely time-consuming and expensive, limiting the availability of high quality datasets. To address these challenges, we propose using a generative adversarial network approach that requires little to no annotation while still producing state-of-the-art results. Our method leverages unannotated images to generate synthetic annotations which are used to train our GAN model. We evaluate our proposed method on two publicly available datasets and compare it against several existing methods. Results show that our method performs favorably against other approaches and demonstrates superior performance in terms of accuracy and efficiency. This work advances the field by reducing the need for costly annotations while maintaining high levels of performance in retinal vessel segmentation tasks.",1
"A variety of modeling techniques have been developed in the past decade to reduce the computational expense and improve the accuracy of modeling. In this study, a new framework of modeling is suggested. Compared with other popular methods, a distinctive characteristic is ""from image based model to analysis based model (e.g. stress, strain, and deformation)"". In such a framework, a reconstruction neural network (ReConNN) model designed for simulation-based physical field's reconstruction is proposed. The ReConNN contains two submodels that are convolutional neural network (CNN) and generative adversarial net-work (GAN). The CNN is employed to construct the mapping between contour images of physical field and objective function. Subsequently, the GAN is utilized to generate more images which are similar to the existing contour images. Finally, Lagrange polynomial is applied to complete the reconstruction. However, the existing CNN models are commonly applied to the classification tasks, which seem to be difficult to handle with regression tasks of images. Meanwhile, the existing GAN architectures are insufficient to generate high-accuracy ""pseudo contour images"". Therefore, a ReConNN model based on a Convolution in Convolution (CIC) and a Convolutional AutoEncoder based on Wasserstein Generative Adversarial Network (WGAN-CAE) is suggested. To evaluate the performance of the proposed model representatively, a classical topology optimization procedure is considered. Then the ReConNN is utilized to the reconstruction of heat transfer process of a pin fin heat sink. It demonstrates that the proposed ReConNN model is proved to be a potential capability to reconstruct physical field for multidisciplinary, such as structural optimization.",0
"In recent years, there has been increasing interest in using artificial intelligence (AI) techniques such as neural networks to simulate physical systems that are difficult to model mathematically. One promising approach to this problem is the use of reconstruction neural network methods, which involve training a neural network on data from simulations of a given system and then using that network to reconstruct the simulation at any desired point in time or space. This paper presents a detailed analysis of how well these methods can perform for different types of simulated physical fields, including both simple and complex cases. Our results show that while reconstruction neural networks have limitations in terms of accuracy and computational efficiency, they can still provide valuable insights into the behavior of real-world physical phenomena and could potentially be used as part of larger computational models in various applications.",1
"The iris can be considered as one of the most important biometric traits due to its high degree of uniqueness. Iris-based biometrics applications depend mainly on the iris segmentation whose suitability is not robust for different environments such as near-infrared (NIR) and visible (VIS) ones. In this paper, two approaches for robust iris segmentation based on Fully Convolutional Networks (FCNs) and Generative Adversarial Networks (GANs) are described. Similar to a common convolutional network, but without the fully connected layers (i.e., the classification layers), an FCN employs at its end a combination of pooling layers from different convolutional layers. Based on the game theory, a GAN is designed as two networks competing with each other to generate the best segmentation. The proposed segmentation networks achieved promising results in all evaluated datasets (i.e., BioSec, CasiaI3, CasiaT4, IITD-1) of NIR images and (NICE.I, CrEye-Iris and MICHE-I) of VIS images in both non-cooperative and cooperative domains, outperforming the baselines techniques which are the best ones found so far in the literature, i.e., a new state of the art for these datasets. Furthermore, we manually labeled 2,431 images from CasiaT4, CrEye-Iris and MICHE-I datasets, making the masks available for research purposes.",0
"This paper presents a robust iris segmentation method based on fully convolutional networks (FCN) and generative adversarial networks (GAN). Iris segmentation is crucial for biometric applications such as iris recognition systems, but it remains a challenging task due to variations in illumination, occlusions, and image quality. To tackle these issues, we propose a novel approach that integrates both FCN and GAN models into a unified framework. Our method first employs an FCN model to generate an initial segmentation mask, which is then used to supervise the training of a conditional GAN. We utilize a cycle consistency loss function between the generated mask and the original image to ensure realism and prevent overfitting. In addition, we introduce a new semantic preservation loss that helps preserve important features within the iris region during the generation process. Experiments on public datasets demonstrate the effectiveness of our method compared to state-of-the-art approaches in terms of accuracy, robustness, and efficiency. Our method has promising potential for real-world iris recognition applications where accurate and reliable segmentations are essential.  Keywords: iris segmentation, fully convolutional network, generative adversarial network, cycle consistency, semantic preservation",1
"MR imaging will play a very important role in radiotherapy treatment planning for segmentation of tumor volumes and organs. However, the use of MR-based radiotherapy is limited because of the high cost and the increased use of metal implants such as cardiac pacemakers and artificial joints in aging society. To improve the accuracy of CT-based radiotherapy planning, we propose a synthetic approach that translates a CT image into an MR image using paired and unpaired training data. In contrast to the current synthetic methods for medical images, which depend on sparse pairwise-aligned data or plentiful unpaired data, the proposed approach alleviates the rigid registration challenge of paired training and overcomes the context-misalignment problem of the unpaired training. A generative adversarial network was trained to transform 2D brain CT image slices into 2D brain MR image slices, combining adversarial loss, dual cycle-consistent loss, and voxel-wise loss. The experiments were analyzed using CT and MR images of 202 patients. Qualitative and quantitative comparisons against independent paired training and unpaired training methods demonstrate the superiority of our approach.",0
"The following is a scientific abstract:  Abstract: This paper presents a new method for synthesizing Computed Tomography (CT) images from Magnetic Resonance (MR) images that combines both paired and unpaired data. Our approach uses deep learning techniques to generate high resolution CT-like images from MR data while preserving important features such as bone density and soft tissue contrast. We demonstrate the effectiveness of our model by comparing the generated images against real CT scans on a variety of clinical tasks including tumor detection, surgery planning, and radiation therapy treatment planning. In addition, we show how the use of unpaired data can improve generalization and reduce errors compared to methods that only use paired data. Finally, we discuss future directions and potential applications of our work in medical imaging and healthcare.",1
"We introduce PathGAN, a deep neural network for visual scanpath prediction trained on adversarial examples. A visual scanpath is defined as the sequence of fixation points over an image defined by a human observer with its gaze. PathGAN is composed of two parts, the generator and the discriminator. Both parts extract features from images using off-the-shelf networks, and train recurrent layers to generate or discriminate scanpaths accordingly. In scanpath prediction, the stochastic nature of the data makes it very difficult to generate realistic predictions using supervised learning strategies, but we adopt adversarial training as a suitable alternative. Our experiments prove how PathGAN improves the state of the art of visual scanpath prediction on the iSUN and Salient360! datasets. Source code and models are available at https://imatge-upc.github.io/pathgan/",0
"Abstract: In recent years, there has been increasing interest in developing computational models that can predict human eye movements during visual tasks. This research area, known as scanpath prediction, has important applications in fields such as psychology, cognitive science, and computer vision. Recently, generative adversarial networks (GANs) have emerged as a powerful tool for generating realistic synthetic data for training and evaluating such models. In this paper, we propose a novel GAN-based method called ""PathGAN"" for visual scanpath prediction. Our approach uses two GAN subnetworks - one generating target images from textual descriptions and another for predicting saccade endpoints based on generated images. We evaluate our model using three datasets and demonstrate state-of-the-art performance across various evaluation metrics. Furthermore, we show how PathGAN can generate new sets of virtual stimuli tailored to specific research questions. Overall, our work provides a step forward towards building more robust, flexible and accurate models capable of making predictions at both localized fixation level and high-level task level representations, opening up exciting possibilities in visual perception research.",1
"We consider the single image super-resolution problem in a more general case that the low-/high-resolution pairs and the down-sampling process are unavailable. Different from traditional super-resolution formulation, the low-resolution input is further degraded by noises and blurring. This complicated setting makes supervised learning and accurate kernel estimation impossible. To solve this problem, we resort to unsupervised learning without paired data, inspired by the recent successful image-to-image translation applications. With generative adversarial networks (GAN) as the basic component, we propose a Cycle-in-Cycle network structure to tackle the problem within three steps. First, the noisy and blurry input is mapped to a noise-free low-resolution space. Then the intermediate image is up-sampled with a pre-trained deep model. Finally, we fine-tune the two modules in an end-to-end manner to get the high-resolution output. Experiments on NTIRE2018 datasets demonstrate that the proposed unsupervised method achieves comparable results as the state-of-the-art supervised models.",0
"This paper presents a novel approach to image super-resolution using cycle-consistent generative adversarial networks (cycleGAN). Our method uses two generators: one that maps low-resolution images into high-resolution ones, and another that maps the reverse direction. We then train these models jointly on unpaired data to minimize the difference between the generated and real pairs. To further improve results, we introduce a ""cycle-in-cycle"" architecture, where each generator consists of multiple cycles, enabling better use of multi-scale features at different stages during training. Experimental results show significant improvements over state-of-the-art methods across several benchmark datasets. The proposed framework has wide applications in computer vision tasks such as photo editing, surveillance, medical imaging, and more.",1
"Recently, deep-networks-based hashing (deep hashing) has become a leading approach for large-scale image retrieval. It aims to learn a compact bitwise representation for images via deep networks, so that similar images are mapped to nearby hash codes. Since a deep network model usually has a large number of parameters, it may probably be too complicated for the training data we have, leading to model over-fitting. To address this issue, in this paper, we propose a simple two-stage pipeline to learn deep hashing models, by regularizing the deep hashing networks using fake images. The first stage is to generate fake images from the original training set without extra data, via a generative adversarial network (GAN). In the second stage, we propose a deep architec- ture to learn hash functions, in which we use a maximum-entropy based loss to incorporate the newly created fake images by the GAN. We show that this loss acts as a strong regularizer of the deep architecture, by penalizing low-entropy output hash codes. This loss can also be interpreted as a model ensemble by simultaneously training many network models with massive weight sharing but over different training sets. Empirical evaluation results on several benchmark datasets show that the proposed method has superior performance gains over state-of-the-art hashing methods.",0
"This work proposes a novel approach for regularizing deep hashing networks using generative adversarial network (GAN) generated fake images. We show that by training a GAN alongside the hash function, we can improve the quality of the learned features while reducing the risk of overfitting. Our method uses real and fake images as input to the network during training, which encourages the model to learn more robust representations. Experimental results on several benchmark datasets demonstrate significant improvements in retrieval accuracy compared to state-of-the-art methods. Overall, our approach offers a new perspective on how to regularize and enhance deep hashing models.",1
"There is a common belief that the successful training of deep neural networks requires many annotated training samples, which are often expensive and difficult to obtain especially in the biomedical imaging field. While it is often easy for researchers to use data augmentation to expand the size of training sets, constructing and generating generic augmented data that is able to teach the network the desired invariance and robustness properties using traditional data augmentation techniques is challenging in practice. In this paper, we propose a novel automatic data augmentation method that uses generative adversarial networks to learn augmentations that enable machine learning based method to learn the available annotated samples more efficiently. The architecture consists of a coarse-to-fine generator to capture the manifold of the training sets and generate generic augmented data. In our experiments, we show the efficacy of our approach on a Magnetic Resonance Imaging (MRI) image, achieving improvements of 3.5% Dice coefficient on the BRATS15 Challenge dataset as compared to traditional augmentation approaches. Also, our proposed method successfully boosts a common segmentation network to reach the state-of-the-art performance on the BRATS15 Challenge.",0
"In recent years there has been growing interest in deep learning methods for medical image segmentation tasks such as brain tumor segmentation from magnetic resonance imaging (MRI) scans. This task can be challenged by class imbalances between healthy tissue and diseased regions, especially in images of low quality where pathological areas may contain only subtle variations compared to their surroundings. To improve segmentation accuracy and robustness, data augmentation techniques have been applied to generate new training samples artificially to enlarge the size and variability of the dataset while minimizing manual labeling efforts. One promising strategy is generative adversarial networks (GAN), which use two competitive neural nets - one generating fake examples and another discriminating real ones - in an iterative process that seeks Nash equilibrium where both sides fail at their respective objectives. Although GANs have achieved great successes in computer vision tasks like image generation and enhancement, they remain largely unexplored for medical imaging applications. Our work proposes a coarse-to-fine framework combining multiple scaled GAN models with different focuses on details from local patch to full-scale MRI scans, boosted with attention mechanisms for selective feature extraction and regularization terms to stabilize the training process. We demonstrate our method improves over baselines using synthetic data on three widely used benchmark datasets. Moreover, extensive experiments indicate that our fine-grained generator leads to more detailed structures near the boundary of glioma cores whereas the coarser versions help in handling large context dependencies across entire MRIs. We hope our findings would inspire future research exploring advanced architectures and designs fo",1
"Generative Adversarial Networks (GANs) have been shown to produce realistically looking synthetic images with remarkable success, yet their performance seems less impressive when the training set is highly diverse. In order to provide a better fit to the target data distribution when the dataset includes many different classes, we propose a variant of the basic GAN model, called Gaussian Mixture GAN (GM-GAN), where the probability distribution over the latent space is a mixture of Gaussians. We also propose a supervised variant which is capable of conditional sample synthesis. In order to evaluate the model's performance, we propose a new scoring method which separately takes into account two (typically conflicting) measures - diversity vs. quality of the generated data. Through a series of empirical experiments, using both synthetic and real-world datasets, we quantitatively show that GM-GANs outperform baselines, both when evaluated using the commonly used Inception Score, and when evaluated using our own alternative scoring method. In addition, we qualitatively demonstrate how the \textit{unsupervised} variant of GM-GAN tends to map latent vectors sampled from different Gaussians in the latent space to samples of different classes in the data space. We show how this phenomenon can be exploited for the task of unsupervised clustering, and provide quantitative evaluation showing the superiority of our method for the unsupervised clustering of image datasets. Finally, we demonstrate a feature which further sets our model apart from other GAN models: the option to control the quality-diversity trade-off by altering, post-training, the probability distribution of the latent space. This allows one to sample higher quality and lower diversity samples, or vice versa, according to one's needs.",0
"Our research focuses on generative adversarial networks (GANs) using Gaussian mixture models (GMMs). GANs generate realistic images by learning from data distributions that are difficult to model manually. However, current methods struggle with high dimensional datasets such as image databases due to computational cost and limited training time. We propose using GMMs within GANs which can more efficiently learn complex underlying structures of image distribution without suffering from mode collapse. By incorporating unsupervised clustering into our method, we show improvements over other state-of-the-art algorithms while producing diverse sets of high quality samples even after just one epoch of training. Through extensive experimental evaluation, we demonstrate that our proposed method outperforms existing approaches across multiple benchmark datasets. We believe that this work provides new insights into understanding GAN generation and unsupervised clustering, and paves the way towards better utilization of large scale image repositories.",1
"In this paper, we propose a structured image inpainting method employing an energy based model. In order to learn structural relationship between patterns observed in images and missing regions of the images, we employ an energy-based structured prediction method. The structural relationship is learned by minimizing an energy function which is defined by a simple convolutional neural network. The experimental results on various benchmark datasets show that our proposed method significantly outperforms the state-of-the-art methods which use Generative Adversarial Networks (GANs). We obtained 497.35 mean squared error (MSE) on the Olivetti face dataset compared to 833.0 MSE provided by the state-of-the-art method. Moreover, we obtained 28.4 dB peak signal to noise ratio (PSNR) on the SVHN dataset and 23.53 dB on the CelebA dataset, compared to 22.3 dB and 21.3 dB, provided by the state-of-the-art methods, respectively. The code is publicly available.",0
"""Deep Structured Energy Based Image Inpainting"" presents a novel approach to image inpainting using deep learning techniques. The method proposed in this work builds upon recent advances in energy based models, which have shown promising results in tasks such as semantic segmentation and object detection. Our approach leverages these insights by incorporating structured and unstructured losses into an energy minimization framework that can effectively learn from limited amounts of data. We evaluate our model on several benchmark datasets and demonstrate state-of-the art performance across all metrics. Furthermore, we showcase our algorithm's capabilities on challenging real world scenarios where ground truth annotations may be difficult or impossible to obtain. Overall, our research paves the way towards improved understanding of how energy based models can be used to solve complex vision problems in practice.",1
"Deep learning has revolutionized the performance of classification, but meanwhile demands sufficient labeled data for training. Given insufficient data, while many techniques have been developed to help combat overfitting, the challenge remains if one tries to train deep networks, especially in the ill-posed extremely low data regimes: only a small set of labeled data are available, and nothing -- including unlabeled data -- else. Such regimes arise from practical situations where not only data labeling but also data collection itself is expensive. We propose a deep adversarial data augmentation (DADA) technique to address the problem, in which we elaborately formulate data augmentation as a problem of training a class-conditional and supervised generative adversarial network (GAN). Specifically, a new discriminator loss is proposed to fit the goal of data augmentation, through which both real and augmented samples are enforced to contribute to and be consistent in finding the decision boundaries. Tailored training techniques are developed accordingly. To quantitatively validate its effectiveness, we first perform extensive simulations to show that DADA substantially outperforms both traditional data augmentation and a few GAN-based options. We then extend experiments to three real-world small labeled datasets where existing data augmentation and/or transfer learning strategies are either less effective or infeasible. All results endorse the superior capability of DADA in enhancing the generalization ability of deep networks trained in practical extremely low data regimes. Source code is available at https://github.com/SchafferZhang/DADA.",0
"In recent years, deep learning has shown great promise in solving complex problems across various domains, including computer vision tasks such as classification. However, one major limitation of deep learning algorithms is their reliance on large amounts of labeled data for effective training. This becomes especially challenging in low data regimes where obtaining sufficient training data can be difficult or prohibitively expensive.  To address this issue, we propose a novel method called DADA (Deep Adversarial Data Augmentation) that significantly improves performance in extremely low data regime classification tasks using Convolutional Neural Networks (CNNs). Our approach involves generating synthetic data samples by applying adversarial perturbations to existing images from within a predefined class distribution, ensuring that the generated samples have similar characteristics as real images while maximizing inter-class distance. By augmenting the original dataset in this manner, we increase the size of the available training set without requiring any additional annotated examples, resulting in improved generalization and better overall accuracy compared to traditional augmentation methods.  We evaluate our approach on several benchmark datasets commonly used in image classification tasks, ranging from small to extremely low data regimes. Experimental results show significant improvements over state-of-the-art baselines, achieving comparable or even superior performance than models trained on more than two orders of magnitude larger datasets. These findings demonstrate the effectiveness of our proposed method in enabling deep learning approaches to perform well under resource-constrained scenarios. Overall, DADA represents a promising direction towards making deep learning more accessible and applicable to a wider range of applications.",1
"Recent advances in Generative Adversarial Networks (GANs) have shown impressive results for task of facial expression synthesis. The most successful architecture is StarGAN, that conditions GANs generation process with images of a specific domain, namely a set of images of persons sharing the same expression. While effective, this approach can only generate a discrete number of expressions, determined by the content of the dataset. To address this limitation, in this paper, we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression. Our approach allows controlling the magnitude of activation of each AU and combine several of them. Additionally, we propose a fully unsupervised strategy to train the model, that only requires images annotated with their activated AUs, and exploit attention mechanisms that make our network robust to changing backgrounds and lighting conditions. Extensive evaluation show that our approach goes beyond competing conditional generators both in the capability to synthesize a much wider range of expressions ruled by anatomically feasible muscle movements, as in the capacity of dealing with images in the wild.",0
"This paper presents a novel approach for generating facial animations that capture both the shape and appearance variations present in real faces. We achieve this by incorporating an anatomical prior into a Generative Adversarial Network (GAN). Our method uses only a single input image as input and generates temporally coherent animations. Using state-of-the art metrics we demonstrate that our generated animations exhibit high quality in terms of both structure and texture fidelity while maintaining temporal consistency. Our key contribution lies in accurately inferring three dimensional face geometry from a two dimensional image without depth or normal maps, enabling us to synthesize plausible anatomical deformations. By doing so, we can generate new views of the input subject or even animate the input still image. In summary, we show how incorporating physical properties such as human head musculature allows for more accurate generations of photorealistic facial expressions.",1
"Training a good deep learning model often requires a lot of annotated data. As a large amount of labeled data is typically difficult to collect and even more difficult to annotate, data augmentation and data generation are widely used in the process of training deep neural networks. However, there is no clear common understanding on how much labeled data is needed to get satisfactory performance. In this paper, we try to address such a question using vehicle license plate character recognition as an example application. We apply computer graphic scripts and Generative Adversarial Networks to generate and augment a large number of annotated, synthesized license plate images with realistic colors, fonts, and character composition from a small number of real, manually labeled license plate images. Generated and augmented data are mixed and used as training data for the license plate recognition network modified from DenseNet. The experimental results show that the model trained from the generated mixed training data has good generalization ability, and the proposed approach achieves a new state-of-the-art accuracy on Dataset-1 and AOLP, even with a very limited number of original real license plates. In addition, the accuracy improvement caused by data generation becomes more significant when the number of labeled images is reduced. Data augmentation also plays a more significant role when the number of labeled images is increased.",0
"Abstract: This paper examines the question of how many labeled license plates (LLPs) are necessary in order to achieve high accuracy in object detection tasks using deep learning algorithms. We begin by discussing the current state of the art in this field and highlighting some of the key challenges that researchers face when trying to optimize LLP usage. Next, we present our own approach to solving this problem, which involves analyzing large datasets of annotated images and applying statistical models to predict how the number of LLPs impacts performance. Our results show that, while there may be some variability depending on factors such as image quality and model architecture, generally speaking it only takes a few hundred thousand LLPs to reach near perfect accuracy. We hope that these findings will serve as a valuable resource for researchers looking to push the boundaries of computer vision even further.",1
"Deep learning approaches to breast cancer detection in mammograms have recently shown promising results. However, such models are constrained by the limited size of publicly available mammography datasets, in large part due to privacy concerns and the high cost of generating expert annotations. Limited dataset size is further exacerbated by substantial class imbalance since ""normal"" images dramatically outnumber those with findings. Given the rapid progress of generative models in synthesizing realistic images, and the known effectiveness of simple data augmentation techniques (e.g. horizontal flipping), we ask if it is possible to synthetically augment mammogram datasets using generative adversarial networks (GANs). We train a class-conditional GAN to perform contextual in-filling, which we then use to synthesize lesions onto healthy screening mammograms. First, we show that GANs are capable of generating high-resolution synthetic mammogram patches. Next, we experimentally evaluate using the augmented dataset to improve breast cancer classification performance. We observe that a ResNet-50 classifier trained with GAN-augmented training data produces a higher AUROC compared to the same model trained only on traditionally augmented data, demonstrating the potential of our approach.",0
"This paper presents a novel approach to data augmentation using Generative Adversarial Networks (GAN) for mammogram classification. We propose a conditional infilling mechanism that synthesizes new regions within the breast area while preserving critical features such as nipple location, skin texture, and subtleties from breast tissue density patterns. Our method effectively increases the number of images available for training without modifying their labels, providing improved performance on benchmark datasets. Furthermore, we demonstrate how our method can enhance traditional feature extraction methods by incorporating spatially adaptive frequency analysis filters tuned to important image structures. The code and dataset utilized in this study have been released publicly to foster further research and collaboration. Overall, our work showcases the effectiveness of conditionally constrained generative models in computer vision tasks with high stakes applications.",1
"Recently, the introduction of the generative adversarial network (GAN) and its variants has enabled the generation of realistic synthetic samples, which has been used for enlarging training sets. Previous work primarily focused on data augmentation for semi-supervised and supervised tasks. In this paper, we instead focus on unsupervised anomaly detection and propose a novel generative data augmentation framework optimized for this task. In particular, we propose to oversample infrequent normal samples - normal samples that occur with small probability, e.g., rare normal events. We show that these samples are responsible for false positives in anomaly detection. However, oversampling of infrequent normal samples is challenging for real-world high-dimensional data with multimodal distributions. To address this challenge, we propose to use a GAN variant known as the adversarial autoencoder (AAE) to transform the high-dimensional multimodal data distributions into low-dimensional unimodal latent distributions with well-defined tail probability. Then, we systematically oversample at the `edge' of the latent distributions to increase the density of infrequent normal samples. We show that our oversampling pipeline is a unified one: it is generally applicable to datasets with different complex data distributions. To the best of our knowledge, our method is the first data augmentation technique focused on improving performance in unsupervised anomaly detection. We validate our method by demonstrating consistent improvements across several real-world datasets.",0
"Artificial intelligence has revolutionized numerous domains such as image recognition, natural language processing, gaming, finance, healthcare, etc., thus providing essential benefits to society. With increasing demands on the reliability and safety-critical applications of artificial intelligence systems like autonomous vehicles, medical diagnosis, fraud detection, network intrusion prediction, etc., assurance of high performance becomes crucial under varying conditions including changes in operating environment (e.g., software updates), attack scenarios, hardware variability, and aging effects. Although comprehensive testing can provide some level of confidence in system behavior and resilience, exhaustive evaluation remains challenging due to constraints in time and resources, thereby limiting our ability to guarantee robustness across all potential situations. To bridge this gap, we propose generative data augmentation methods based on generative adversarial networks (GANs) as a means toward efficient and effective anomaly detection in unsupervised settings. Our results demonstrate significant improvements over traditional statistical approaches while preserving important features required by downstream models. This research paves the way for more trustworthy artificial intelligence systems that generalize better across diverse scenarios, yield fewer false positives/negatives, require less labeled training data, and adapt more quickly to previously unseen conditions.",1
"In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid with a high resolution of 256^3 by recovering the occluded/missing regions. The key idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Networks (GAN) framework, to infer accurate and fine-grained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets and real-world Kinect datasets show that the proposed 3D-RecGAN++ significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects.",0
"Inference: What Is Inference And How Does It Help Us? What is inference? Inference allows us to use evidence from our current situation to make predictions about other situations. For example, if we have knowledge that birds typically fly south for the winter, then seeing snow falling outside would allow us to infer that winter has come again because we know flying south for the winter is one of their common behaviors.",1
"This paper proposes a series of new approaches to improve Generative Adversarial Network (GAN) for conditional image synthesis and we name the proposed model as ArtGAN. One of the key innovation of ArtGAN is that, the gradient of the loss function w.r.t. the label (randomly assigned to each generated image) is back-propagated from the categorical discriminator to the generator. With the feedback from the label information, the generator is able to learn more efficiently and generate image with better quality. Inspired by recent works, an autoencoder is incorporated into the categorical discriminator for additional complementary information. Last but not least, we introduce a novel strategy to improve the image quality. In the experiments, we evaluate ArtGAN on CIFAR-10 and STL-10 via ablation studies. The empirical results showed that our proposed model outperforms the state-of-the-art results on CIFAR-10 in terms of Inception score. Qualitatively, we demonstrate that ArtGAN is able to generate plausible-looking images on Oxford-102 and CUB-200, as well as able to draw realistic artworks based on style, artist, and genre. The source code and models are available at: https://github.com/cs-chan/ArtGAN",0
"Artificial intelligence has made significant strides in recent years in generating realistic images using generative adversarial networks (GANs). One notable GAN architecture that has been used successfully for image generation is ArTGAN. However, there exists room for improvement in terms of accuracy, performance, and control over generated outputs. In our paper, we propose an enhanced version of ArTGAN called Improved ArtGAN (IA-GAN) for conditional synthesis of natural images and artwork. Our method leverages advanced training techniques such as progressive growing of GAN models, attention modules, and batch normalization to generate high-quality images that are more faithful to their corresponding conditions. Experimental evaluations demonstrate the effectiveness of our approach compared to state-of-the-art methods on several benchmark datasets. This paper presents new insights into conditional image generation using GAN architectures and provides researchers and practitioners in computer vision and machine learning with a tool for creating realistic images with fine-grained controls.",1
"We introduce a framework using Generative Adversarial Networks (GANs) for likelihood--free inference (LFI) and Approximate Bayesian Computation (ABC) where we replace the black-box simulator model with an approximator network and generate a rich set of summary features in a data driven fashion. On benchmark data sets, our approach improves on others with respect to scalability, ability to handle high dimensional data and complex probability distributions.",0
"This paper presents a new method for performing likelihood-free inference in high dimensions, which offers significant advantages over existing approaches. Our approach combines recent advances in computer science, statistics, and machine learning to achieve state-of-the-art performance on challenging problems. We demonstrate our methods through a variety of examples, showing their flexibility and applicability across different domains. Our results have important implications for scientific research and decision making, enabling more accurate predictions and better informed decisions in complex situations. Overall, we believe that our work represents a major step forward in high-dimensional likelihood-free inference and has the potential to impact many fields.",1
"Generative adversarial networks (GANs) often suffer from unpredictable mode-collapsing during training. We study the issue of mode collapse of Boundary Equilibrium Generative Adversarial Network (BEGAN), which is one of the state-of-the-art generative models. Despite its potential of generating high-quality images, we find that BEGAN tends to collapse at some modes after a period of training. We propose a new model, called \emph{BEGAN with a Constrained Space} (BEGAN-CS), which includes a latent-space constraint in the loss function. We show that BEGAN-CS can significantly improve training stability and suppress mode collapse without either increasing the model complexity or degrading the image quality. Further, we visualize the distribution of latent vectors to elucidate the effect of latent-space constraint. The experimental results show that our method has additional advantages of being able to train on small datasets and to generate images similar to a given real image yet with variations of designated attributes on-the-fly.",0
"In recent years there has been increased interest in understanding how systems can escape from local optima in high dimensional spaces when subjected to random noise or perturbations. This problem arises naturally in many domains including engineering design, evolutionary biology, finance, computer vision, robotics, natural language processing, game theory, economics, artificial intelligence, machine learning, theoretical physics, among others. In general we consider two types of escapes: mode escapes and global escapes. Mode escapes refer to transitions between multiple metastable states within some basin of attraction, while global escapes correspond to escaping into another basin of attraction altogether. While both types of escapes have received attention in different contexts their full characterization remains elusive as it depends critically on the specific system dynamics and constraints present. Therefore we study here generic properties underlying these phenomena across different applications areas without focusing exclusively on any particular one. We provide broad results showing that escapes become easier whenever constraints imposed by the application reduce degrees of freedom available at different levels (i.e., microscopic vs macroscopic). We showcase a plethora of examples coming from very diverse disciplines such as materials science, condensed matter, plasma physics, fluid mechanics, cell dynamics, etc. Moreover, for all these problems our treatment leads us to propose novel numerical algorithms capable of coping efficiently with highdimensionality which constitutes a serious bottleneck otherwise limiting current methods applicability. We believe these developments open interesting perspectives towards model calibration tasks where data generation through direct simulation might prove prohibitively timeconsumptiv",1
"Camera shake or target movement often leads to undesired blur effects in videos captured by a hand-held camera. Despite significant efforts having been devoted to video-deblur research, two major challenges remain: 1) how to model the spatio-temporal characteristics across both the spatial domain (i.e., image plane) and temporal domain (i.e., neighboring frames), and 2) how to restore sharp image details w.r.t. the conventionally adopted metric of pixel-wise errors. In this paper, to address the first challenge, we propose a DeBLuRring Network (DBLRNet) for spatial-temporal learning by applying a modified 3D convolution to both spatial and temporal domains. Our DBLRNet is able to capture jointly spatial and temporal information encoded in neighboring frames, which directly contributes to improved video deblur performance. To tackle the second challenge, we leverage the developed DBLRNet as a generator in the GAN (generative adversarial network) architecture, and employ a content loss in addition to an adversarial loss for efficient adversarial training. The developed network, which we name as DeBLuRring Generative Adversarial Network (DBLRGAN), is tested on two standard benchmarks and achieves the state-of-the-art performance.",0
"In recent years, video deblurring has been gaining more attention due to its importance in many real-world applications such as surveillance, autonomous driving, and virtual reality. Existing methods have achieved promising results but still face significant challenges, particularly in handling complex motion blurs that exhibit both spatial and temporal variations. To tackle these issues, we propose an adversarial spatio-temporal learning framework that can effectively learn jointly from both space and time domains under limited clean data availability.  Our proposed method consists of two primary components: (i) Spatial-aware feature learning module to handle spatially varying blur kernels using a modified non-local self-attention mechanism, and (ii) Temporal feature enhancement network to model the short-term dynamics within a sequence by designing a recurrent architecture guided by multi-scale temporal attention. These modules work collaboratively to generate sharper frames iteratively via an integrated loss function that enforces the cycle consistency constraints among consecutive frames to boost the efficiency of each iteration.  To further improve performance, we introduce a dual discriminator strategy, which enables our method to better capture intricate spatio-temporal features underlying blurred videos while remaining robust against noisy input images generated by previous iterations. Finally, extensive experiments on several benchmark datasets demonstrate that our approach achieves superior performance compared with state-of-the-art alternatives, validating its effectiveness in addressing complex real-world scenarios involving severe motion blurs and low light conditions. Our research provides a new perspective on solving video deblurring problems and opens up opportunities for future advancements in multimedia signal processing using deep neural networks.",1
"Semantic segmentation has made much progress with increasingly powerful pixel-wise classifiers and incorporating structural priors via Conditional Random Fields (CRF) or Generative Adversarial Networks (GAN). We propose a simpler alternative that learns to verify the spatial structure of segmentation during training only. Unlike existing approaches that enforce semantic labels on individual pixels and match labels between neighbouring pixels, we propose the concept of Adaptive Affinity Fields (AAF) to capture and match the semantic relations between neighbouring pixels in the label space. We use adversarial learning to select the optimal affinity field size for each semantic category. It is formulated as a minimax problem, optimizing our segmentation neural network in a best worst-case learning scenario. AAF is versatile for representing structures as a collection of pixel-centric relations, easier to train than GAN and more efficient than CRF without run-time inference. Our extensive evaluations on PASCAL VOC 2012, Cityscapes, and GTA5 datasets demonstrate its above-par segmentation performance and robust generalization across domains.",0
"This paper presents a novel approach to semantic segmentation using adaptive affinity fields (AAF). Inspired by recent advances in deep learning, we propose a two-stage architecture that first generates an initial estimate of object boundaries based on shared appearance features across different locations within an image. This preliminary segmentation is then refined through a fully convolutional network which learns to adjust the boundary predictions by focusing only on local context specific to each pixel. Our method outperforms state-of-the-art techniques by up to 4% in terms of mean intersection over union (mIOU) and achieves highly competitive results on public benchmark datasets such as Pascal VOC and Cityscapes. By introducing robustness against variations in appearance due to changes in scale, orientation and lighting conditions, our AAF approach provides a more accurate and reliable solution for real world applications requiring precise object delineations from images.",1
"Channel modeling is a critical topic when considering designing, learning, or evaluating the performance of any communications system. Most prior work in designing or learning new modulation schemes has focused on using highly simplified analytic channel models such as additive white Gaussian noise (AWGN), Rayleigh fading channels or similar. Recently, we proposed the usage of a generative adversarial networks (GANs) to jointly approximate a wireless channel response model (e.g. from real black box measurements) and optimize for an efficient modulation scheme over it using machine learning. This approach worked to some degree, but was unable to produce accurate probability distribution functions (PDFs) representing the stochastic channel response. In this paper, we focus specifically on the problem of accurately learning a channel PDF using a variational GAN, introducing an architecture and loss function which can accurately capture stochastic behavior. We illustrate where our prior method failed and share results capturing the performance of such as system over a range of realistic channel distributions.",0
"This paper presents a novel approach to learning stochastic channel models using variational generative adversarial networks (VGAN). In many applications, such as communication systems, image processing, and machine vision, we need accurate models of channels that affect our data. However, traditional methods for estimating these models can be time-consuming and computationally expensive, particularly for high-dimensional or complex datasets. VGANs offer a powerful alternative by allowing us to learn a model of the underlying channel directly from observations. By training two competing neural networks - a generator and a discriminator - we can capture both the statistical properties of the data and the noise present in the channel. We show that this method provides state-of-the-art performance on several benchmark datasets and outperforms existing approaches in terms of accuracy and efficiency. Our results demonstrate the potential of VGANs for approximating real-world channel models in various domains and pave the way for further research in this direction.",1
"We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.",0
"In recent years, deep generative models such as Generative Adversarial Networks (GANs) have been extensively studied due to their ability to generate realistic images from textual descriptions or random noise inputs. However, these models often suffer from limited resolution due to computational constraints. This paper presents an approach that addresses this limitation by using conditional GANs trained on high-resolution image datasets. The method allows for efficient generation of detailed images, including fine-grained features and contextual elements while preserving the semantic integrity of the input data. Additionally, our proposed technique enables advanced semantic manipulation of the generated output through user-defined instructions and input prompts. Experimental results demonstrate significant improvements over state-of-the-art methods in terms of visual quality and diversity, providing evidence of our modelâ€™s effectiveness in generating coherent and semantically meaningful images at unprecedented levels of detail. The potential applications of our approach span across computer graphics, virtual reality, and interactive media, among others.",1
"With an aim to increase the capture range and accelerate the performance of state-of-the-art inter-subject and subject-to-template 3D registration, we propose deep learning-based methods that are trained to find the 3D position of arbitrarily oriented subjects or anatomy based on slices or volumes of medical images. For this, we propose regression CNNs that learn to predict the angle-axis representation of 3D rotations and translations using image features. We use and compare mean square error and geodesic loss to train regression CNNs for 3D pose estimation used in two different scenarios: slice-to-volume registration and volume-to-volume registration. Our results show that in such registration applications that are amendable to learning, the proposed deep learning methods with geodesic loss minimization can achieve accurate results with a wide capture range in real-time (100ms). We also tested the generalization capability of the trained CNNs on an expanded age range and on images of newborn subjects with similar and different MR image contrasts. We trained our models on T2-weighted fetal brain MRI scans and used them to predict the 3D pose of newborn brains based on T1-weighted MRI scans. We showed that the trained models generalized well for the new domain when we performed image contrast transfer through a conditional generative adversarial network. This indicates that the domain of application of the trained deep regression CNNs can be further expanded to image modalities and contrasts other than those used in training. A combination of our proposed methods with accelerated optimization-based registration algorithms can dramatically enhance the performance of automatic imaging devices and image processing methods of the future.",0
"Abstract: Real time deep pose estimation has applications in numerous fields such as virtual reality, robotics, healthcare, and animation, among others. In recent years there have been significant advances in image registration algorithms based on convolutional neural networks (CNNs). However, these methods typically focus on rigid registrations with small transformations only, which limit their scope of application. To address this limitation, we present a new method for real-time deep pose estimation using geodesic loss function. Our method extends traditional template matching by using a differentiable renderer that predicts pixel intensities from rendered poses. We then use a novel variant of the geodesic loss function, called geodesic distance preserving loss, to optimize our CNN model parameters. This ensures smooth and consistent predictions across large deformations and disocclusions. Furthermore, our framework can handle non-rigid templates, making it applicable in more demanding scenarios. Experimental results demonstrate the effectiveness and efficiency of our method, achieving superior performance compared to state-of-the-art methods.",1
"Evaluating generative adversarial networks (GANs) is inherently challenging. In this paper, we revisit several representative sample-based evaluation metrics for GANs, and address the problem of how to evaluate the evaluation metrics. We start with a few necessary conditions for metrics to produce meaningful scores, such as distinguishing real from generated samples, identifying mode dropping and mode collapsing, and detecting overfitting. With a series of carefully designed experiments, we comprehensively investigate existing sample-based metrics and identify their strengths and limitations in practical settings. Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space. Our experiments also unveil interesting properties about the behavior of several popular GAN models, such as whether they are memorizing training samples, and how far they are from learning the target distribution.",0
"This is an empirical study that evaluates generative adversarial networks (GANs). GANs have recently emerged as a popular deep learning methodology used in image generation tasks such as super resolution, denoising, image translation, and many other computer vision applications. To optimize these models effectively, we need reliable evaluation metrics that can accurately measure their performance. In recent years, several evaluation measures like FrÃ©chet Inception Distance (FID), Structural Similarity Index Measure (SSIM), Perceptual Path Length (PPL) and Mean Absolute Error (MAE), among others, have been proposed to evaluate GANsâ€™ performances. However, there has been no agreement over which metric is the most suitable for evaluating different applications. Therefore, we aimed to conduct an exhaustive experimental comparison of commonly employed evaluation metrics under different use cases. We studied diverse applications including: superresolution, image synthesis, semantic segmentation, edge detection and so on. Our results indicate that FID performs better than SSIM for general generation quality assessment, while PPL outperforms both MAE and FID in pixel-wise regression problems. Meanwhile, PPL remains competitive for low per-pixel loss images. Overall, our findings suggest that choosing appropriate evaluation metrics depends significantly on specific application requirements. Hence, careful selection based on task objectives should ensure accurate model optimization and higher performing GANs. As a result, practitioners could save considerable time by focusing only on suitable metrics, enabling them to design more efficient workflows.",1
"In this paper, we introduce a new method for generating an object image from text attributes on a desired location, when the base image is given. One step further to the existing studies on text-to-image generation mainly focusing on the object's appearance, the proposed method aims to generate an object image preserving the given background information, which is the first attempt in this field. To tackle the problem, we propose a multi-conditional GAN (MC-GAN) which controls both the object and background information jointly. As a core component of MC-GAN, we propose a synthesis block which disentangles the object and background information in the training stage. This block enables MC-GAN to generate a realistic object image with the desired background by controlling the amount of the background information from the given base image using the foreground information from the text attributes. From the experiments with Caltech-200 bird and Oxford-102 flower datasets, we show that our model is able to generate photo-realistic images with a resolution of 128 x 128. The source code of MC-GAN is released.",0
"The following abstract is intended to provide an overview of our new method, which we have called Multi-Conditional GAN (MC-GAN), for generating high quality images using deep learning techniques. Our approach addresses many of the current limitations associated with existing generative adversarial networks (GANs) by allowing for more precise control over the synthesis process through multi-conditioning on several factors such as texture, lighting, color, pose, etc., and incorporating a customized loss function tailored to the specific image generation task at hand. We demonstrate the effectiveness of MC-GAN across multiple domains including but not limited to faces, scenes, textures, objects, etc., producing state-of-the-art results against other methods, thereby showcasing the broad applicability of our approach. Finally, we believe that this work could potentially serve as a stepping stone towards creating systems capable of realizing highly advanced applications such as virtual reality, computer graphics, art creation, scientific visualization, video games, and more.",1
"Text-to-Image translation has been an active area of research in the recent past. The ability for a network to learn the meaning of a sentence and generate an accurate image that depicts the sentence shows ability of the model to think more like humans. Popular methods on text to image translation make use of Generative Adversarial Networks (GANs) to generate high quality images based on text input, but the generated images don't always reflect the meaning of the sentence given to the model as input. We address this issue by using a captioning network to caption on generated images and exploit the distance between ground truth captions and generated captions to improve the network further. We show extensive comparisons between our method and existing methods.",0
"Abstract: We present a novel approach to text-to-image translation that utilizes cycle consistent adversarial networks (CycleGAN). Our method takes advantage of both generative models and discriminative models by training two concurrent GANs - one from image to text, another from text to image. This allows our model to learn bidirectional mappings between images and their corresponding text descriptions. The use of cycle consistency ensures that translations across different modalities preserve content, resulting in more accurate and coherent translations. Experimental results on several benchmark datasets demonstrate significant improvements over state-of-the-art methods for text-to-image translation.",1
"Medical images with specific pathologies are scarce, but a large amount of data is usually required for a deep convolutional neural network (DCNN) to achieve good accuracy. We consider the problem of segmenting the left ventricular (LV) myocardium on late gadolinium enhancement (LGE) cardiovascular magnetic resonance (CMR) scans of which only some of the scans have scar tissue. We propose ScarGAN to simulate scar tissue on healthy myocardium using chained generative adversarial networks (GAN). Our novel approach factorizes the simulation process into 3 steps: 1) a mask generator to simulate the shape of the scar tissue; 2) a domain-specific heuristic to produce the initial simulated scar tissue from the simulated shape; 3) a refining generator to add details to the simulated scar tissue. Unlike other approaches that generate samples from scratch, we simulate scar tissue on normal scans resulting in highly realistic samples. We show that experienced radiologists are unable to distinguish between real and simulated scar tissue. Training a U-Net with additional scans with scar tissue simulated by ScarGAN increases the percentage of scar pixels correctly included in LV myocardium prediction from 75.9% to 80.5%.",0
"In the field of medical image analysis, synthetic data has become increasingly important as a tool for training machine learning algorithms, validating models, and developing new methods for disease diagnosis. One commonly used approach for generating synthetic data is generative adversarial networks (GANs), which can produce realistic images that capture fine details and variations found in patient scans. However, traditional GANs have limitations in their ability to simulate pathological tissues due to the lack of domain knowledge and spatial constraints imposed by the chains connecting adjacent tissue patches. To address these issues, we present a novel variant of GAN called ScarGAN, where each generator is conditioned on a small number of pathology annotations to learn local patterns around lesions such as fibrosis or scarred regions. Additionally, our model enforces a physical constraint that nearby tissue patches should form coherent structures through connections specified by the corresponding ground truth label maps. Our experiments showed that using chained GANs with auxiliary supervision from annotated cardiac magnetic resonance imaging (MRI) datasets improved performance across multiple evaluation metrics compared to non-chained baseline models and other state-of-the-art approaches. Our method demonstrates promising results in generating high quality synthetic MRI scans with accurate depiction of diverse types of pathologies, potentially paving the way towards enhanced clinical applications. This work provides valuable insights into exploring alternative strategies for incorporating domain knowledge and prior shape constraints within GAN architectures to enhance synthesis tasks beyond vision domains.",1
"Machine learning methods play increasingly important roles in pre-procedural planning for complex surgeries and interventions. Very often, however, researchers find the historical records of emerging surgical techniques, such as the transcatheter aortic valve replacement (TAVR), are highly scarce in quantity. In this paper, we address this challenge by proposing novel generative invertible networks (GIN) to select features and generate high-quality virtual patients that may potentially serve as an additional data source for machine learning. Combining a convolutional neural network (CNN) and generative adversarial networks (GAN), GIN discovers the pathophysiologic meaning of the feature space. Moreover, a test of predicting the surgical outcome directly using the selected features results in a high accuracy of 81.55%, which suggests little pathophysiologic information has been lost while conducting the feature selection. This demonstrates GIN can generate virtual patients not only visually authentic but also pathophysiologically interpretable.",0
"This research paper presents a new method for interpreting biomedical data using a generative invertible neural network called GIN. GIN takes raw medical data as input and outputs synthetic patient data that can be used for diagnosis and treatment planning purposes. By leveraging deep learning techniques, our model allows physicians to gain insights into pathophysiological mechanisms underlying disease states and explore potential interventions through virtual patient generation. Our evaluation demonstrates the effectiveness of the proposed approach for generating realistic synthetic patients based on existing patient cohorts, supporting their use as proxy subjects for controlled trials of experimental treatments without putting actual humans at risk. Furthermore, we show how GIN-generated virtual patients can facilitate personalized medicine by enabling predictive simulations informed by relevant patient populations. Overall, GIN represents a major step forward in computational modeling of human health, with broad implications for precision medicine and translational science.",1
"Recently, the cycle-consistent generative adversarial networks (CycleGAN) has been widely used for synthesis of multi-domain medical images. The domain-specific nonlinear deformations captured by CycleGAN make the synthesized images difficult to be used for some applications, for example, generating pseudo-CT for PET-MR attenuation correction. This paper presents a deformation-invariant CycleGAN (DicycleGAN) method using deformable convolutional layers and new cycle-consistency losses. Its robustness dealing with data that suffer from domain-specific nonlinear deformations has been evaluated through comparison experiments performed on a multi-sequence brain MR dataset and a multi-modality abdominal dataset. Our method has displayed its ability to generate synthesized data that is aligned with the source while maintaining a proper quality of signal compared to CycleGAN-generated data. The proposed model also obtained comparable performance with CycleGAN when data from the source and target domains are alignable through simple affine transformations.",0
"In this paper, we propose a novel unsupervised method for synthesizing images across multiple medical domains using deformation invariant cycle consistency networks (CycleGANs). Our approach utilizes a combination of generative adversarial networks (GANs) and CycleGANs to improve the quality and accuracy of synthesized images by enforcing both within-domain and cross-domain cycle consistency constraints. This allows us to generate high-resolution, realistic images that preserve crucial details from each domain while maintaining the overall structure of the input images. We evaluate our method on four different medical imaging datasets, including CT, MRI, PET/CT, and X-ray, demonstrating significant improvements over previous state-of-the-art methods. Our results show that our method can effectively transfer features learned from one domain to another without requiring any explicit supervision, providing important applications in medical image analysis and radiology.",1
"We present a novel method to generate accurate and realistic clothing deformation from real data capture. Previous methods for realistic cloth modeling mainly rely on intensive computation of physics-based simulation (with numerous heuristic parameters), while models reconstructed from visual observations typically suffer from lack of geometric details. Here, we propose an original framework consisting of two modules that work jointly to represent global shape deformation as well as surface details with high fidelity. Global shape deformations are recovered from a subspace model learned from 3D data of clothed people in motion, while high frequency details are added to normal maps created using a conditional Generative Adversarial Network whose architecture is designed to enforce realism and temporal consistency. This leads to unprecedented high-quality rendering of clothing deformation sequences, where fine wrinkles from (real) high resolution observations can be recovered. In addition, as the model is learned independently from body shape and pose, the framework is suitable for applications that require retargeting (e.g., body animation). Our experiments show original high quality results with a flexible model. We claim an entirely data-driven approach to realistic cloth wrinkle generation is possible.",0
"Our new method, DeepWrinkles, can quickly generate accurate and realistic wrinkle patterns on cloth simulations at scale using deep learning techniques. Key features include improved plausibility through physically inspired regularization and multi-scale refinement; versatility across garment types from tight leather clothing (e.g., motorcycle suits) to loose garments like a robe; efficient computation time (10-100x faster than prior art); high fidelity capturing and transferring of fine details from scanned fabrics, and synthesizing previously unseen ones; robustness to input shapes with varying topology and parameterization; and ease of use due to automatic preprocessing and minimal user control settings while achieving state-of-the-art results. All these benefits make our DeepWrinkles approach highly appealing for VFX productions and animation studios enabling more engaging character experiences for movies, gaming applications, and virtual/augmented reality projects. We verify our claims through extensive comparisons versus related works, variety of challenging scenarios, ablation studies, as well as visual analysis by experts.",1
"The integration of information acquired with different modalities, spatial resolution and spectral bands has shown to improve predictive accuracies. Data fusion is therefore one of the key challenges in remote sensing. Most prior work focusing on multi-modal fusion, assumes that modalities are always available during inference. This assumption limits the applications of multi-modal models since in practice the data collection process is likely to generate data with missing, incomplete or corrupted modalities. In this paper, we show that Generative Adversarial Networks can be effectively used to overcome the problems that arise when modalities are missing or incomplete. Focusing on semantic segmentation of building footprints with missing modalities, our approach achieves an improvement of about 2% on the Intersection over Union (IoU) against the same network that relies only on the available modality.",0
"In recent years, there has been significant interest in using artificial intelligence (AI) techniques for building footprint segmentation from high resolution remote sensing imagery. However, existing approaches have struggled to handle missing or incomplete modalities, which can lead to poor performance and limited accuracy. To address these challenges, we propose the use of generative adversarial networks (GANs), a type of deep learning algorithm that allows for generation of new data samples by leveraging unlabeled datasets. Our approach utilizes GANs to synthesize missing modality values, improving the quality of building footprint segmentations. We evaluate our method on multiple real-world datasets and demonstrate improved performance over baseline methods that rely solely on complete modality inputs. This work represents a promising step towards more accurate and robust AI systems for geospatial analysis tasks.",1
"We propose a framework based on Generative Adversarial Networks to disentangle the identity and attributes of faces, such that we can conveniently recombine different identities and attributes for identity preserving face synthesis in open domains. Previous identity preserving face synthesis processes are largely confined to synthesizing faces with known identities that are already in the training dataset. To synthesize a face with identity outside the training dataset, our framework requires one input image of that subject to produce an identity vector, and any other input face image to extract an attribute vector capturing, e.g., pose, emotion, illumination, and even the background. We then recombine the identity vector and the attribute vector to synthesize a new face of the subject with the extracted attribute. Our proposed framework does not need to annotate the attributes of faces in any way. It is trained with an asymmetric loss function to better preserve the identity and stabilize the training process. It can also effectively leverage large amounts of unlabeled training face images to further improve the fidelity of the synthesized faces for subjects that are not presented in the labeled training face dataset. Our experiments demonstrate the efficacy of the proposed framework. We also present its usage in a much broader set of applications including face frontalization, face attribute morphing, and face adversarial example detection.",0
"In recent years, deep learning has revolutionized computer vision tasks such as image classification, object detection, and semantic segmentation. One task that has gained significant attention recently is face synthesis, which involves generating new faces while preserving their identity traits like gender, age, pose, expression, etc. Most existing approaches focus on closed set scenarios where only specific attributes of interest are present in the training data. However, open set scenario, where identities may have different ages or genders than those seen during training, remains under explored. This work proposes a novel framework called TOPSIS (Towards Open Set Identity Preserving Face Synthesis) to generate diverse face images that retain the desired features of the input person. Our approach leverages pretrained feature extractors and adversarial loss functions, along with regularization techniques to constrain the output within realistic bounds. We showcase our method's effectiveness through extensive quantitative experiments across multiple benchmark datasets, demonstrating improved performance over state-of-the-art methods. Additionally, we conduct qualitative analysis comparing outputs from several baseline models, emphasizing TOPSIS' ability to preserve desired identity attributes even in the presence of unseen variations. Overall, our study opens up possibilities towards more generalizable face generation frameworks capable of handling complex natural variations and paves the path towards broader applications in areas requiring consistent identity representation such as virtual reality, entertainment, or biometrics.",1
"This paper proposes a novel approach to generate multiple color palettes that reflect the semantics of input text and then colorize a given grayscale image according to the generated color palette. In contrast to existing approaches, our model can understand rich text, whether it is a single word, a phrase, or a sentence, and generate multiple possible palettes from it. For this task, we introduce our manually curated dataset called Palette-and-Text (PAT). Our proposed model called Text2Colors consists of two conditional generative adversarial networks: the text-to-palette generation networks and the palette-based colorization networks. The former captures the semantics of the text input and produce relevant color palettes. The latter colorizes a grayscale image using the generated color palette. Our evaluation results show that people preferred our generated palettes over ground truth palettes and that our model can effectively reflect the given palette when colorizing an image.",0
"An image colorization model can struggle with understanding desired colors for specific regions in images. In this work we provide a method that uses text to steer color choices by generating palettes reflecting relevant adjectives. Our simple method outperforms strong baselines including random sampling. We propose multiple improvements on our base strategy, such as disentangling shading from material properties in scene graphs and using GANs for better palette quality, which achieve state of theart results at higher computational cost. With more complex models like EMLA and DALLâ€¢E 2, colorization has become easier for many types of images. However these methods still require significant manual labor for each instance, making their usage impractical without large amounts of training data and time. Here, we present several methods aimed at reducing this required effort through generated guidance via natural language input. Each proposal involves some sort of â€œcolor paletteâ€ generation process; i.e. finding a set of colors associated with given keywords and applying them selectively during colorization. To evaluate these ideas we apply them both qualitatively (human evaluation) and quantitatively (on a new benchmark of ~400 paired images). Our study shows positive correlation across all metrics with even a rudimentary approach providing meaningful benefits for most scenes over competitive alternatives. We thus conclude there exists potential in pursuing this research direction further towards reduced manual effort in computer vision tasks. Note though that future works would need to consider tradeoffs between added runtime complexity vs gains in user efficiency as well as balancing general applicability against more niche domains where textual guidance may not always be possible/useful. We first introduce relevant prior art and related topics to motivate discussions herein. Next, we explain the core idea behind â€œpalette generationâ€, the three main techniques used throughout followed by a detailed breakdown of their components. Afterwards, we then describe a human evalua",1
"Image-based generative methods, such as generative adversarial networks (GANs) have already been able to generate realistic images with much context control, specially when they are conditioned. However, most successful frameworks share a common procedure which performs an image-to-image translation with pose of figures in the image untouched. When the objective is reposing a figure in an image while preserving the rest of the image, the state-of-the-art mainly assumes a single rigid body with simple background and limited pose shift, which can hardly be extended to the images under normal settings. In this paper, we introduce an image ""inner space"" preserving model that assigns an interpretable low-dimensional pose descriptor (LDPD) to an articulated figure in the image. Figure reposing is then generated by passing the LDPD and the original image through multi-stage augmented hourglass networks in a conditional GAN structure, called inner space preserving generative pose machine (ISP-GPM). We evaluated ISP-GPM on reposing human figures, which are highly articulated with versatile variations. Test of a state-of-the-art pose estimator on our reposed dataset gave an accuracy over 80% on PCK0.5 metric. The results also elucidated that our ISP-GPM is able to preserve the background with high accuracy while reasonably recovering the area blocked by the figure to be reposed.",0
"In recent years, there has been significant progress in the development of computer vision models that can generate images of poses from textual descriptions or even reconstruct detailed human meshes based on single images. However, these methods often fail to preserve important aspects such as interpart relationships, joint angles, or even overall body shape. As a result, we propose a new approach called ""Inner Space Preserving Generative Posing Machine"" (ISPGPM) which uses implicit generators to capture these relationships while still generating high quality 2D keypoint maps. We demonstrate ISPGPM's effectiveness by comparing results against state-of-the-art methods across multiple benchmark datasets, showing improved performance on measures related to pose accuracy and preservation of inner space relationships. Additionally, our model achieves this without requiring any explicit supervision or fine-grained annotations beyond standard image collections used in previous work. These findings suggest potential applications in a wide range of domains including virtual reality, animation, and medical simulations where accurate pose generation could prove valuable.",1
"Image-to-image translation has been made much progress with embracing Generative Adversarial Networks (GANs). However, it's still very challenging for translation tasks that require high quality, especially at high-resolution and photorealism. In this paper, we present Discriminative Region Proposal Adversarial Networks (DRPAN) for high-quality image-to-image translation. We decompose the procedure of image-to-image translation task into three iterated steps, first is to generate an image with global structure but some local artifacts (via GAN), second is using our DRPnet to propose the most fake region from the generated image, and third is to implement ""image inpainting"" on the most fake region for more realistic result through a reviser, so that the system (DRPAN) can be gradually optimized to synthesize images with more attention on the most artifact local part. Experiments on a variety of image-to-image translation tasks and datasets validate that our method outperforms state-of-the-arts for producing high-quality translation results in terms of both human perceptual studies and automatic quantitative measures.",0
"Our proposed method, which we call ""Discriminative Region Proposal Adversarial Networks"" (DRPAN), significantly improves image-to-image translation quality by leveraging adversarial training on discriminatively selected regions. Inspired by recent work that has used attention mechanisms to selectively focus on salient features during translation, DRPAN goes one step further by introducing a dedicated module designed to explicitly optimize the selection process. This allows our model to effectively learn how to attend to those regions that are most relevant to generating high-quality translations. We evaluate our approach on several challenging datasets, demonstrating significant improvements over state-of-the-art methods in terms of both objective metrics such as PSNR and SSIM and subjective visual inspection. These results confirm DRPAN's effectiveness at enabling fine-grained control over image generation processes without sacrificing efficiency or scalability. Overall, DRPAN represents a powerful new tool for researchers working on computer vision problems involving image synthesis and transformation.",1
"The recently proposed distributional approach to reinforcement learning (DiRL) is centered on learning the distribution of the reward-to-go, often referred to as the value distribution. In this work, we show that the distributional Bellman equation, which drives DiRL methods, is equivalent to a generative adversarial network (GAN) model. In this formulation, DiRL can be seen as learning a deep generative model of the value distribution, driven by the discrepancy between the distribution of the current value, and the distribution of the sum of current reward and next value. We use this insight to propose a GAN-based approach to DiRL, which leverages the strengths of GANs in learning distributions of high-dimensional data. In particular, we show that our GAN approach can be used for DiRL with multivariate rewards, an important setting which cannot be tackled with prior methods. The multivariate setting also allows us to unify learning the distribution of values and state transitions, and we exploit this idea to devise a novel exploration method that is driven by the discrepancy in estimating both values and states.",0
"This work presents a new approach called Distributional Multivariate Policy Evaluation (DMVE) that extends traditional multivariate policy evaluation techniques to take into account distributional effects on multiple outcomes simultaneously. Our DMVE method combines three key components: 1) nonparametric kernel density estimation (KDE), which provides accurate estimates of outcome distributions; 2) structured policy analysis using Shapley Value, which identifies how each parameter in the policy impacts different parts of these distributions; and 3) statistical tests of treatment parameters from observational studies. By combining KDE with counterfactual prediction error bounds based on generative models such as deep learning or diffusion Monte Carlo methods like the Generative Adversarial Network (GAN), we can accurately estimate both individual treatment effect distributions across heterogeneous populations and their relationships among different outcomes within complex policies. We apply our methodology to two case studies: a large field experiment where a computerized decision aid increased breast cancer screening rates in low socioeconomic status patients by varying amounts depending on covariates such as educational attainment level; and a randomized trial evaluating the efficacy of a bundled intervention aimed at reducing hospital readmissions after heart failure episodes where DMVE reveals important interactions between specific treatments targeted towards preventing readmission re-hospitalization. In contrast to existing approaches, DMVE produces more informative policy insights tailored to stakeholdersâ€™ interests while preserving the robustness required for policymaking under uncertainty. Finally, we outline future research directions to extend the applicability of DMVE beyond observational and experimental study designs. Keywords: dmve, bellman gan, multiva",1
"Image reconstruction including image restoration and denoising is a challenging problem in the field of image computing. We present a new method, called X-GANs, for reconstruction of arbitrary corrupted resource based on a variant of conditional generative adversarial networks (conditional GANs). In our method, a novel generator and multi-scale discriminators are proposed, as well as the combined adversarial losses, which integrate a VGG perceptual loss, an adversarial perceptual loss, and an elaborate corresponding point loss together based on the analysis of image feature. Our conditional GANs have enabled a variety of applications in image reconstruction, including image denoising, image restoration from quite a sparse sampling, image inpainting, image recovery from the severely polluted block or even color-noise dominated images, which are extreme cases and haven't been addressed in the status quo. We have significantly improved the accuracy and quality of image reconstruction. Extensive perceptual experiments on datasets ranging from human faces to natural scenes demonstrate that images reconstructed by the presented approach are considerably more realistic than alternative work. Our method can also be extended to handle high-ratio image compression.",0
"This research presents a novel approach to image reconstruction using generative adversarial networks (GANs), which we call extreme case GANs (X-GANs). While traditional GANs have shown promise in generating realistic images from limited input data, they often struggle with reconstructing high quality images under difficult conditions, such as low resolution inputs or corrupted images. Our proposed method addresses these limitations by introducing two key innovations: i) a new loss function that balances fidelity to the input data with diversity in the generated output, and ii) a self-attention mechanism to selectively focus on important features in the input during training. We evaluate our algorithm on several benchmark datasets and show significant improvements over state-of-the-art methods in terms of both quantitative metrics and visual inspection. Overall, our results demonstrate that X-GANs provide a powerful tool for image reconstruction in extreme cases where other methods fall short.",1
"Recent advances in Deep Learning and probabilistic modeling have led to strong improvements in generative models for images. On the one hand, Generative Adversarial Networks (GANs) have contributed a highly effective adversarial learning procedure, but still suffer from stability issues. On the other hand, Conditional Variational Auto-Encoders (CVAE) models provide a sound way of conditional modeling but suffer from mode-mixing issues. Therefore, recent work has turned back to simple and stable regression models that are effective at generation but give up on the sampling mechanism and the latent code representation. We propose a novel and efficient stochastic regression approach with latent drop-out codes that combines the merits of both lines of research. In addition, a new training objective increases coverage of the training distribution leading to improvements over the state of the art in terms of accuracy as well as diversity.",0
"Abstract: This paper presents a method for conditional image generation using stochastic regression with latent drop-out codes. We propose a novel architecture that combines generative models and deep learning techniques to generate high quality images conditioned on text descriptions. Our model uses latent variables to capture the underlying structure of the data and enables efficient sampling from the probability distribution over images. By introducing drop-out codes during training, we regularize our model and improve generalization performance. Experimental results demonstrate that our approach outperforms existing methods in terms of visual fidelity and diversity, making it well suited for a variety of applications such as artistic rendering, content creation, and data augmentation.",1
"In generalized zero shot learning (GZSL), the set of classes are split into seen and unseen classes, where training relies on the semantic features of the seen and unseen classes and the visual representations of only the seen classes, while testing uses the visual representations of the seen and unseen classes. Current methods address GZSL by learning a transformation from the visual to the semantic space, exploring the assumption that the distribution of classes in the semantic and visual spaces is relatively similar. Such methods tend to transform unseen testing visual representations into one of the seen classes' semantic features instead of the semantic features of the correct unseen class, resulting in low accuracy GZSL classification. Recently, generative adversarial networks (GAN) have been explored to synthesize visual representations of the unseen classes from their semantic features - the synthesized representations of the seen and unseen classes are then used to train the GZSL classifier. This approach has been shown to boost GZSL classification accuracy, however, there is no guarantee that synthetic visual representations can generate back their semantic feature in a multi-modal cycle-consistent manner. This constraint can result in synthetic visual representations that do not represent well their semantic features. In this paper, we propose the use of such constraint based on a new regularization for the GAN training that forces the generated visual features to reconstruct their original semantic features. Once our model is trained with this multi-modal cycle-consistent semantic compatibility, we can then synthesize more representative visual representations for the seen and, more importantly, for the unseen classes. Our proposed approach shows the best GZSL classification results in the field in several publicly available datasets.",0
"In recent years, there has been significant interest in developing models that can learn from multiple modalities such as images, text, audio, and video. These multi-modal learning approaches have applications in fields ranging from computer vision to natural language processing. One particular challenge associated with training these models is ensuring their robustness to input perturbations such as changes in lighting conditions, camera angles, or object occlusions. To address this problem, researchers have proposed using cycle consistency losses in which predictions made by different modalities are compared against each other. However, existing methods suffer from limitations such as high computational cost and sensitivity to hyperparameters. This study proposes a new method called Multi-modal Cycle-consistent Generalized Zero-shot Learning (MCGZSL) that overcomes these issues while significantly improving performance on benchmark datasets. Our experiments demonstrate the effectiveness of our approach for both image classification and zero-shot learning tasks across several modalities. Overall, MCGZSL provides a promising solution for building more accurate and robust multi-modal systems.",1
"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. Algorithms for the two tasks, such as normalizing flows and generative adversarial networks (GANs), are often developed independently. In this paper, we propose the concept of {\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit energy-based distribution with CTFs for density estimation. Both tasks rely on a new technique for distribution matching within amortized learning. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.",0
"This paper proposes a new approach to inference and density estimation using continuous-time flows. Our method addresses several challenges faced by traditional methods, such as slow convergence and difficulty in handling nonlinear problems. We show that our continuous-time flows provide more efficient solutions to these issues while maintaining accurate results. Additionally, we demonstrate the applicability of our method across different domains, including image processing, computer vision, and machine learning. Overall, our work represents a significant advance in the field of inference and density estimation and has the potential to greatly impact numerous applications.",1
"Recently, generative adversarial networks exhibited excellent performances in semi-supervised image analysis scenarios. In this paper, we go even further by proposing a fully unsupervised approach for segmentation applications with prior knowledge of the objects' shapes. We propose and investigate different strategies to generate simulated label data and perform image-to-image translation between the image and the label domain using an adversarial model. Specifically, we assess the impact of the annotation model's accuracy as well as the effect of simulating additional low-level image features. For experimental evaluation, we consider the segmentation of the glomeruli, an application scenario from renal pathology. Experiments provide proof of concept and also confirm that the strategy for creating the simulated label data is of particular relevance considering the stability of GAN trainings.",0
"This paper presents a new approach for unsupervised training of generative adversarial networks (GANs) for digital pathology image segmentation using automatically generated annotations. Traditional supervised learning methods require large amounts of labeled data, which can be time-consuming and expensive to obtain. In contrast, our method leverages recent advances in annotation automation techniques to generate pseudo-labels for the images in an unlabeled dataset. These labels are then used to train the generator network of the GAN to produce realistic annotated images that closely resemble the original images. We evaluate our approach on two public datasets and show that it outperforms state-of-the-art unsupervised segmentation algorithms while requiring significantly fewer manual annotations. Our results demonstrate the potential of combining automatic annotation generation and GAN-based models for improving the efficiency and accuracy of medical image analysis tasks.",1
"Anomaly detection aims to detect abnormal events by a model of normality. It plays an important role in many domains such as network intrusion detection, criminal activity identity and so on. With the rapidly growing size of accessible training data and high computation capacities, deep learning based anomaly detection has become more and more popular. In this paper, a new domain-based anomaly detection method based on generative adversarial networks (GAN) is proposed. Minimum likelihood regularization is proposed to make the generator produce more anomalies and prevent it from converging to normal data distribution. Proper ensemble of anomaly scores is shown to improve the stability of discriminator effectively. The proposed method has achieved significant improvement than other anomaly detection methods on Cifar10 and UCI datasets.",0
"This is an interesting new technique that can be used to detect anomalies in large data sets by using a generative adversarial network (GAN) to calculate the minimum likelihood value. By training two neural networks against each other to generate and discriminate real from generated samples, we can learn how likely a given observation is to have occurred by chance. Our results show that our approach outperforms traditional methods such as clustering and principal component analysis on a variety of datasets, demonstrating its potential as a powerful tool for identifying unexpected patterns and outliers.",1
"Performing recognition tasks using latent fingerprint samples is often challenging for automated identification systems due to poor quality, distortion, and partially missing information from the input samples. We propose a direct latent fingerprint reconstruction model based on conditional generative adversarial networks (cGANs). Two modifications are applied to the cGAN to adapt it for the task of latent fingerprint reconstruction. First, the model is forced to generate three additional maps to the ridge map to ensure that the orientation and frequency information is considered in the generation process, and prevent the model from filling large missing areas and generating erroneous minutiae. Second, a perceptual ID preservation approach is developed to force the generator to preserve the ID information during the reconstruction process. Using a synthetically generated database of latent fingerprints, the deep network learns to predict missing information from the input latent samples. We evaluate the proposed method in combination with two different fingerprint matching algorithms on several publicly available latent fingerprint datasets. We achieved the rank-10 accuracy of 88.02\% on the IIIT-Delhi latent fingerprint database for the task of latent-to-latent matching and rank-50 accuracy of 70.89\% on the IIIT-Delhi MOLF database for the task of latent-to-sensor matching. Experimental results of matching reconstructed samples in both latent-to-sensor and latent-to-latent frameworks indicate that the proposed method significantly increases the matching accuracy of the fingerprint recognition systems for the latent samples.",0
"In recent years, advances in biometrics have led to increased use of latent fingerprints as evidence in criminal investigations. However, partial or low quality prints can often lead to difficulties in identification due to their limited features and degraded image quality. To address these challenges, we propose the use of generative adversarial networks (GANs) for partial latent fingerprint reconstruction, while preserving crucial identity traits known as minutiae points. GANs are capable of generating high resolution images that closely resemble the original data they were trained on, making them a promising approach for latent print enhancement. Our proposed method utilizes two sub-networks: a generator network that generates new latent prints and a discriminator network that evaluates whether the generated prints match the input partial latent print. By balancing the two competing objectives of enhancing the print while maintaining the identity traits, our model produces outputs that are both perceptually similar to the original latent print and contain enough detail for successful matching by commercial Automatic Fingerprint Identification Systems (AFIS). Experimental results demonstrate the effectiveness of our method compared to state-of-the-art techniques in terms of visual fidelity, feature permanence, and accuracy. Overall, our work represents a significant step towards improving the reliability and efficiency of latent fingerprint analysis for law enforcement purposes.",1
"Segmentation of skin lesions is considered as an important step in computer aided diagnosis (CAD) for automated melanoma diagnosis. In recent years, segmentation methods based on fully convolutional networks (FCN) have achieved great success in general images. This success is primarily due to the leveraging of large labelled datasets to learn features that correspond to the shallow appearance as well as the deep semantics of the images. However, the dependence on large dataset does not translate well into medical images. To improve the FCN performance for skin lesion segmentations, researchers attempted to use specific cost functions or add post-processing algorithms to refine the coarse boundaries of the FCN results. However, the performance of these methods is heavily reliant on the tuning of many parameters and post-processing techniques. In this paper, we leverage the state-of-the-art image feature learning method of generative adversarial network (GAN) for its inherent ability to produce consistent and realistic image features by using deep neural networks and adversarial learning concept. We improve upon GAN such that skin lesion features can be learned at different level of complexities, in a controlled manner. The outputs from our method is then augmented to the existing FCN training data, thus increasing the overall feature diversity. We evaluated our method on the ISIC 2018 skin lesion segmentation challenge dataset and showed that it was more accurate and robust when compared to the existing skin lesion segmentation methods.",0
"In this study, we aim to improve automatic skin lesion segmentation using adversarial learning based data augmentation techniques. We propose two novel methods that utilize generative models trained as adversaries during the training process. Our first method involves generating new synthetic images to supplement existing datasets while preserving their characteristics. By doing so, we can enrich the dataset without losing important features present in real images. Our second approach is focused on improving edge detection by minimizing errors caused by image translation. This is achieved through optimizing the segmentation network with respect to an edge detector module, resulting in more accurate delineations of lesions from surrounding healthy tissue. Experimental results demonstrate significant improvements over baseline methods across different metrics such as Jaccard index, dice coefficient, and intersection over union (IOU). These findings suggest that our proposed approaches have great potential in advancing state-of-the-art skin lesion segmentation algorithms. Overall, our work highlights the effectiveness of adversarial learning based data augmentation strategies in addressing current limitations faced by medical imaging applications.",1
"Convolutional neural networks (CNNs) have gained tremendous success in solving complex inverse problems. The aim of this work is to develop a novel CNN framework to reconstruct video sequence of dynamic live cells captured using a computational microscopy technique, Fourier ptychographic microscopy (FPM). The unique feature of the FPM is its capability to reconstruct images with both wide field-of-view (FOV) and high resolution, i.e. a large space-bandwidth-product (SBP), by taking a series of low resolution intensity images. For live cell imaging, a single FPM frame contains thousands of cell samples with different morphological features. Our idea is to fully exploit the statistical information provided by this large spatial ensemble so as to make predictions in a sequential measurement, without using any additional temporal dataset. Specifically, we show that it is possible to reconstruct high-SBP dynamic cell videos by a CNN trained only on the first FPM dataset captured at the beginning of a time-series experiment. Our CNN approach reconstructs a 12800X10800 pixels phase image using only ~25 seconds, a 50X speedup compared to the model-based FPM algorithm. In addition, the CNN further reduces the required number of images in each time frame by ~6X. Overall, this significantly improves the imaging throughput by reducing both the acquisition and computational times. The proposed CNN is based on the conditional generative adversarial network (cGAN) framework. Additionally, we also exploit transfer learning so that our pre-trained CNN can be further optimized to image other cell types. Our technique demonstrates a promising deep learning approach to continuously monitor large live-cell populations over an extended time and gather useful spatial and temporal information with sub-cellular resolution.",0
"Here we present a deep learning approach to Fourier Ptychographic Microscopy (FPM), which overcomes some of the traditional challenges associated with this technique by leveraging powerful data-driven models. This method learns to estimate high resolution images from low quality input image stacks and reconstruct them using phase retrieval algorithms. Our model achieves state-of-the-art performance in terms of speed, accuracy and flexibility while significantly reducing computational costs compared to previous approaches. In addition, our results demonstrate that deep learning techniques can effectively improve FPM performance through large scale training and fine tuning of complex models to address specific imaging problems. Overall, this work represents a significant step forward in the application of machine learning to high resolution microscopy and has broad implications across many fields where rapid acquisition of accurate and detailed images is critical.",1
"This paper is on image and face super-resolution. The vast majority of prior work for this problem focus on how to increase the resolution of low-resolution images which are artificially generated by simple bilinear down-sampling (or in a few cases by blurring followed by down-sampling).We show that such methods fail to produce good results when applied to real-world low-resolution, low quality images. To circumvent this problem, we propose a two-stage process which firstly trains a High-to-Low Generative Adversarial Network (GAN) to learn how to degrade and downsample high-resolution images requiring, during training, only unpaired high and low-resolution images. Once this is achieved, the output of this network is used to train a Low-to-High GAN for image super-resolution using this time paired low- and high-resolution images. Our main result is that this network can be now used to efectively increase the quality of real-world low-resolution images. We have applied the proposed pipeline for the problem of face super-resolution where we report large improvement over baselines and prior work although the proposed method is potentially applicable to other object categories.",0
"Image super-resolution (SR) is a popular task that involves generating high-quality images from low-resolution inputs. Despite great progress, there exists a gap between the performance of models on benchmark datasets and real-world scenarios where factors such as lighting conditions, camera movements, and sensor noise can negatively impact the quality of low-resolution images. In this work, we propose using generative adversarial networks (GANs) to learn image degradation before applying SR techniques, which enables our model to adapt better to real-world degradations and improve SR results on these challenging inputs. Our approach works by training two subnetworks: one generator network trained to generate fake degraded versions of clean images, and another discriminator network trained to distinguish these generated images from real ones. This allows us to map the problem of learning SR into one of recognizing true from fake samples. With this approach, we achieve state-of-the-art performance on multiple benchmark datasets, including ones with more diverse and complex real-world degradations. We hope our method inspires future research in overcoming difficult challenges in low-level vision tasks such as SR.",1
"Malware authors have always been at an advantage of being able to adversarially test and augment their malicious code, before deploying the payload, using anti-malware products at their disposal. The anti-malware developers and threat experts, on the other hand, do not have such a privilege of tuning anti-malware products against zero-day attacks pro-actively. This allows the malware authors to being a step ahead of the anti-malware products, fundamentally biasing the cat and mouse game played by the two parties. In this paper, we propose a way that would enable machine learning based threat prevention models to bridge that gap by being able to tune against a deep generative adversarial network (GAN), which takes up the role of a malware author and generates new types of malware. The GAN is trained over a reversible distributed RGB image representation of known malware behaviors, encoding the sequence of API call ngrams and the corresponding term frequencies. The generated images represent synthetic malware that can be decoded back to the underlying API call sequence information. The image representation is not only demonstrated as a general technique of incorporating necessary priors for exploiting convolutional neural network architectures for generative or discriminative modeling, but also as a visualization method for easy manual software or malware categorization, by having individual API ngram information distributed across the image space. In addition, we also propose using smart-definitions for detecting malwares based on perceptual hashing of these images. Such hashes are potentially more effective than cryptographic hashes that do not carry any meaningful similarity metric, and hence, do not generalize well.",0
"Adversarial Generative Networks (GANs) have become increasingly popular as tools used by cybersecurity researchers to study the behavior of malware. One such use case is generating synthetic versions of actual malicious files that were discovered in the wild. This allows security analysts to create virtualized environments where they can analyze how these files behave without actually exposing their own systems to potential risks. In order to achieve accurate results, the use of real-world data is necessary so that the generated samples match the properties found in real-world scenarios. To ensure accuracy, one could argue that the training dataset should consist of only high quality samples which reflect current trends and threats, rather than random collections of any available samples found online. Furthermore, it would be beneficial to consider running these simulations on GPU powered machines due to the immense computation needs of modern deep learning models like GANs. Overall, the utilization of adversarial generative networks enables threat hunters to study emerging threats before encountering them in their production environment and provides an opportunity to observe the attackerâ€™s tactics and behaviors from a defender perspective.",1
"Most multi-view 3D reconstruction algorithms, especially when shape-from-shading cues are used, assume that object appearance is predominantly diffuse. To alleviate this restriction, we introduce S2Dnet, a generative adversarial network for transferring multiple views of objects with specular reflection into diffuse ones, so that multi-view reconstruction methods can be applied more effectively. Our network extends unsupervised image-to-image translation to multi-view ""specular to diffuse"" translation. To preserve object appearance across multiple views, we introduce a Multi-View Coherence loss (MVC) that evaluates the similarity and faithfulness of local patches after the view-transformation. Our MVC loss ensures that the similarity of local correspondences among multi-view images is preserved under the image-to-image translation. As a result, our network yields significantly better results than several single-view baseline techniques. In addition, we carefully design and generate a large synthetic training data set using physically-based rendering. During testing, our network takes only the raw glossy images as input, without extra information such as segmentation masks or lighting estimation. Results demonstrate that multi-view reconstruction can be significantly improved using the images filtered by our network. We also show promising performance on real world training and testing data.",0
"In the field of computer vision, multi-view reconstruction refers to the process of creating a three-dimensional (3D) model from multiple images taken at different angles of a scene or object. One key challenge in this task is accurately estimating the depth of pixels in each image, which is often done through the use of disparity maps. These maps provide the distance between corresponding points on two different views of the same scene. However, traditional methods for obtaining these maps can be limited by the availability of ground truth data, as well as their ability to handle low light conditions and occlusions. To address these issues, we present a new method called specular-to-diffuse translation for multi-view reconstruction. Our approach utilizes the difference in appearance between specular and diffused reflection to improve the accuracy of depth estimation. Specifically, we train a convolutional neural network to estimate depth directly from pixel intensities rather than relying solely on feature matching techniques. We evaluate our method on several benchmark datasets and demonstrate that it outperforms state-of-the-art approaches under challenging imaging conditions such as low light and high reflectivity. Our results show that specular-to-diffuse translation has great potential for improving multi-view reconstruction and other computer vision tasks that rely on accurate depth estimation.",1
"Due to the emergence of Generative Adversarial Networks, video synthesis has witnessed exceptional breakthroughs. However, existing methods lack a proper representation to explicitly control the dynamics in videos. Human pose, on the other hand, can represent motion patterns intrinsically and interpretably, and impose the geometric constraints regardless of appearance. In this paper, we propose a pose guided method to synthesize human videos in a disentangled way: plausible motion prediction and coherent appearance generation. In the first stage, a Pose Sequence Generative Adversarial Network (PSGAN) learns in an adversarial manner to yield pose sequences conditioned on the class label. In the second stage, a Semantic Consistent Generative Adversarial Network (SCGAN) generates video frames from the poses while preserving coherent appearances in the input image. By enforcing semantic consistency between the generated and ground-truth poses at a high feature level, our SCGAN is robust to noisy or abnormal poses. Extensive experiments on both human action and human face datasets manifest the superiority of the proposed method over other state-of-the-arts.",0
"Title: Generating Realistic Videos from User Inputs via Physics-Aware Pose Priors  Abstract: In recent years, generating high quality videos that accurately capture human motion has been a challenging task. While several approaches have been proposed to tackle this problem, most existing methods require complex setups or rely heavily on manual inputs to achieve good results. To address these issues, we present a novel method called ""Pose Guided Human Video Generation"" which uses physics-aware pose priors to generate realistic and physically plausible videos based on user input skeletons. Our approach leverages state-of-the-art computer vision techniques such as optical flow estimation and deep learning based models to synthesize new frames that fit seamlessly into the generated video sequences. We demonstrate through extensive experiments that our approach achieves superior performance compared to existing methods while requiring minimal setup and manual intervention. Finally, we discuss potential applications of our technique, including virtual reality environments, animation systems and rehabilitation therapy tools.",1
"Recent studies on unsupervised image-to-image translation have made a remarkable progress by training a pair of generative adversarial networks with a cycle-consistent loss. However, such unsupervised methods may generate inferior results when the image resolution is high or the two image domains are of significant appearance differences, such as the translations between semantic layouts and natural images in the Cityscapes dataset. In this paper, we propose novel Stacked Cycle-Consistent Adversarial Networks (SCANs) by decomposing a single translation into multi-stage transformations, which not only boost the image translation quality but also enable higher resolution image-to-image translations in a coarse-to-fine manner. Moreover, to properly exploit the information from the previous stage, an adaptive fusion block is devised to learn a dynamic integration of the current stage's output and the previous stage's output. Experiments on multiple datasets demonstrate that our proposed approach can improve the translation quality compared with previous single-stage unsupervised methods.",0
"This paper presents a new method for unsupervised image-to-image translation using stacked cycle-consistent adversarial networks (SCCAN). Traditional methods for image-to-image translation require large amounts of labeled data and manual annotations. In contrast, our proposed approach uses only pairs of images from different domains without any additional supervision. We achieve this by designing a two-stage architecture consisting of a generator network that maps input images to their corresponding output domain and a discriminator network that enforces cycle consistency during training. Our experiments show that SCCAN achieves state-of-the-art results on multiple benchmark datasets while requiring significantly less annotation effort compared to traditional approaches. Additionally, we demonstrate that our framework can effectively transfer attributes between domains, enabling a wide range of applications such as medical imaging and computer vision. Overall, our work represents a significant advancement towards more efficient and effective ways of translating images between different domains.",1
"Save for some special cases, current training methods for Generative Adversarial Networks (GANs) are at best guaranteed to converge to a `local Nash equilibrium` (LNE). Such LNEs, however, can be arbitrarily far from an actual Nash equilibrium (NE), which implies that there are no guarantees on the quality of the found generator or classifier. This paper proposes to model GANs explicitly as finite games in mixed strategies, thereby ensuring that every LNE is an NE. With this formulation, we propose a solution method that is proven to monotonically converge to a resource-bounded Nash equilibrium (RB-NE): by increasing computational resources we can find better solutions. We empirically demonstrate that our method is less prone to typical GAN problems such as mode collapse, and produces solutions that are less exploitable than those produced by GANs and MGANs, and closely resemble theoretical predictions about NEs.",0
"Introduction: The concept of game theory has become increasingly relevant in understanding human behavior and decision making, especially when dealing with situations where multiple parties have conflicting interests. In such cases, finding stable solutions that satisfy everyone involved can prove challenging. This problem becomes more complex when adversarial networks are introduced into the equation as they may disrupt the balance among players. However, recent developments in the field offer new approaches beyond local Nash equilibria that allow us to better analyze and predict the outcome of interactions within these network systems.",1
"Generative Adversarial Networks (Goodfellow et al., 2014), a major breakthrough in the field of generative modeling, learn a discriminator to estimate some distance between the target and the candidate distributions.   This paper examines mathematical issues regarding the way the gradients for the generative model are computed in this context, and notably how to take into account how the discriminator itself depends on the generator parameters.   A unifying methodology is presented to define mathematically sound training objectives for generative models taking this dependency into account in a robust way, covering both GAN, VAE and some GAN variants as particular cases.",0
"In recent years, generative adversarial learning (GANs) have become increasingly popular due to their ability to generate high quality images, audio, and other data types that resemble real-world examples. Despite their widespread use, many GAN architectures suffer from instability during training, often resulting in poor performance or divergence. To address these issues, we propose a new loss function called perceptual similarity loss which measures how similar generated samples are to real data based on human perception. We demonstrate through experiments that our method significantly improves stability and performance compared to existing losses such as binary cross entropy and least squares. This paper has important implications for both researchers working in deep learning and practitioners who rely on GANs for tasks such as image generation, video prediction, and domain transfer.",1
"We present the first generative adversarial network (GAN) for natural image matting. Our novel generator network is trained to predict visually appealing alphas with the addition of the adversarial loss from the discriminator that is trained to classify well-composited images. Further, we improve existing encoder-decoder architectures to better deal with the spatial localization issues inherited in convolutional neural networks (CNN) by using dilated convolutions to capture global context information without downscaling feature maps and losing spatial information. We present state-of-the-art results on the alphamatting online benchmark for the gradient error and give comparable results in others. Our method is particularly well suited for fine structures like hair, which is of great importance in practical matting applications, e.g. in film/TV production.",0
"Natural image matting separates images into alpha matte and background images by leveraging machine learning techniques like convolutional neural network (CNN). Recently, generative adversarial networks have been employed in a variety of computer vision tasks, including scene synthesis, object detection and semantic segmentation due to their effectiveness in generating realistic data distributions. However, how can GANs be adapted to solve real world problems such as natural image matting? This paper addresses that question through introducing AlphaGAN, a new framework which integrates alpha prediction, texture fusion and refinement modules within a single unified architecture. Experiments on public datasets validate the feasibility and superior performance compared to state-of-the-art methods. With these promising results, we believe that AlphaGAN could potentially inspire future studies in the field of image matting and related fields. AlphaGAN: Generative Adversarial Networks for Natural Image Matting. In this work, we introduce a novel approach called AlphaGAN for solving the challenges faced in natural image matting using Generative Adversarial Networks (GANs)  Natural image matting seeks to separate an object from its background while preserving important details such as shadows, reflections, transparency and other attributes. This task has numerous applications in digital entertainment industries, scientific research and photo editing tools among others. Convolutional Neural Networks (CNNs) are commonly used for performing this separation but suffer from limitations such as missing details, noise artifacts or halo effects especially at object boundaries.  Recent advancements in deep learning have seen GANs gain popularity owing to their ability to generate high quality realistic samples. However, adapting them to perform natural image mattin",1
"In this paper, we present the optical image simulation from a synthetic aperture radar (SAR) data using deep learning based methods. Two models, i.e., optical image simulation directly from the SAR data and from multi-temporal SARoptical data, are proposed to testify the possibilities. The deep learning based methods that we chose to achieve the models are a convolutional neural network (CNN) with a residual architecture and a conditional generative adversarial network (cGAN). We validate our models using the Sentinel-1 and -2 datasets. The experiments demonstrate that the model with multi-temporal SAR-optical data can successfully simulate the optical image, meanwhile, the model with simple SAR data as input failed. The optical image simulation results indicate the possibility of SARoptical information blending for the subsequent applications such as large-scale cloud removal, and optical data temporal superresolution. We also investigate the sensitivity of the proposed models against the training samples, and reveal possible future directions.",0
"This study aimed to develop methodologies that can fuse multi-temporal Sentinel-1A/B C band Synthetic Aperture Radar (SAR) imagery along with Sentinel-2A/B MSI data through advanced image processing techniques such as pansharpening, superresolution, fusion and co-registration. These methodologies were applied on two agricultural areas located in India i.e. Kuttanad region of Kerala state and Anupgarh city of Rajasthan state. Object based analysis was performed over these fused images to extract features like crop type, LAI, chlorophyll content etc. which gives more detailed and accurate information than individual datasets separately. This work demonstrated improved classification accuracies and feature extraction capabilities using fusion approach as compared to individual datasets alone and showed great potential in future applications for environmental monitoring.",1
"In this paper we investigate image generation guided by hand sketch. When the input sketch is badly drawn, the output of common image-to-image translation follows the input edges due to the hard condition imposed by the translation process. Instead, we propose to use sketch as weak constraint, where the output edges do not necessarily follow the input edges. We address this problem using a novel joint image completion approach, where the sketch provides the image context for completing, or generating the output image. We train a generated adversarial network, i.e, contextual GAN to learn the joint distribution of sketch and the corresponding image by using joint images. Our contextual GAN has several advantages. First, the simple joint image representation allows for simple and effective learning of joint distribution in the same image-sketch space, which avoids complicated issues in cross-domain learning. Second, while the output is related to its input overall, the generated features exhibit more freedom in appearance and do not strictly align with the input features as previous conditional GANs do. Third, from the joint image's point of view, image and sketch are of no difference, thus exactly the same deep joint image completion network can be used for image-to-sketch generation. Experiments evaluated on three different datasets show that our contextual GAN can generate more realistic images than state-of-the-art conditional GANs on challenging inputs and generalize well on common categories.",0
"Artificial intelligence (AI) has revolutionized numerous industries by providing innovative solutions that streamline processes, increase efficiency, and improve decision making. One such application of AI is image generation using Generative Adversarial Networks (GAN). These networks have gained popularity due to their ability to create high-resolution images from scratch or based on user input sketches. However, generating images remains challenging due to contextual constraints within scenes. To address this challenge, we propose a novel approach that fuses sketch constraints with contextual knowledge derived from real images. Our method leverages state-of-the-art GAN architectures to generate visually appealing and semantically meaningful images under explicit contextual guidance.  Our model introduces two generators: one takes a simple sketch as input and produces a detailed scene; another generates contextual details guided by semantic cues extracted from real images. We train our generator pair adversarially in tandem with two discriminator modules: one focused on perceptual similarity against real images, while the other evaluates if the generated image satisfies the given sketch constraint. Our experimental results demonstrate the superior performance of our proposed model compared to prior work across multiple metrics including visual quality, alignment with provided sketches, and human evaluation.  We envision applications of our image synthesis system in domains ranging from graphic design and architecture to entertainment and education. By enabling users to interactively refine sketched ideas into photorealistic scenes tailored to specific contexts, our research advances the frontiers of computer graphics and opens new possibilities for creative expression. Overall, this work represents a significant step towards more advanced AI systems capable of creating coherent and contextually grounded multimedia content.",1
"In this paper, we examine the visual variability of objects across different ad categories, i.e. what causes an advertisement to be visually persuasive. We focus on modeling and generating faces which appear to come from different types of ads. For example, if faces in beauty ads tend to be women wearing lipstick, a generative model should portray this distinct visual appearance. Training generative models which capture such category-specific differences is challenging because of the highly diverse appearance of faces in ads and the relatively limited amount of available training data. To address these problems, we propose a conditional variational autoencoder which makes use of predicted semantic attributes and facial expressions as a supervisory signal when training. We show how our model can be used to produce visually distinct faces which appear to be from a fixed ad topic category. Our human studies and quantitative and qualitative experiments confirm that our method greatly outperforms a variety of baselines, including two variations of a state-of-the-art generative adversarial network, for transforming faces to be more ad-category appropriate. Finally, we show preliminary generation results for other types of objects, conditioned on an ad topic.",0
"The goal of an advertising campaign is often to persuade consumers to buy or use certain products. In order to achieve this goal, advertisers may use various techniques such as emotional appeals, logical arguments, or social proof. One technique that has been gaining more attention recently is the use of faces in advertisements. By showing images of people smiling, frowning, or making other facial expressions, advertisers can elicit specific emotions from viewers which they hope will lead them to take action. This paper presents research on the effectiveness of using persuasive faces in advertisements. We investigate whether there is evidence to support claims made by advertisers and psychologists regarding the power of faces in advertising. Through experiments conducted with human participants, we demonstrate that carefully chosen faces in advertisements can influence consumer behavior significantly. Additionally, we show how advertisers can maximize their impact by selecting faces that best suit their target audience demographics. Our findings have important implications for both academia and industry alike, highlighting the need for further exploration into the role of persuasion in modern society.",1
"Generative adversarial networks (GANs) are one of the most popular methods for generating images today. While impressive results have been validated by visual inspection, a number of quantitative criteria have emerged only recently. We argue here that the existing ones are insufficient and need to be in adequation with the task at hand. In this paper we introduce two measures based on image classification---GAN-train and GAN-test, which approximate the recall (diversity) and precision (quality of the image) of GANs respectively. We evaluate a number of recent GAN approaches based on these two measures and demonstrate a clear difference in performance. Furthermore, we observe that the increasing difficulty of the dataset, from CIFAR10 over CIFAR100 to ImageNet, shows an inverse correlation with the quality of the GANs, as clearly evident from our measures.",0
"In this research study we explore Generative Adversarial Networks (GAN) on textual data using popular pre-trained models such as CycleGAN, DiscoGAN and UNITER which have been trained on large datasets like MNIST. We evaluate these models on quality metrics and find that results are promising. By leveraging cutting edge techniques from image generation tasks we can achieve high performance even though GANs were originally designed for images. This has important implications in fields where generating novel text data is crucial, such as chatbots, poetry, language translation, marketing copywriting etc., opening up new possibilities by augmenting human creativity with artificial intelligence. With more advancements in deep learning and greater accessibility to compute resources, future versions of these models promise higher fidelity in their output which would make them even more appealing in these domains.",1
"In this paper we introduce Curriculum GANs, a curriculum learning strategy for training Generative Adversarial Networks that increases the strength of the discriminator over the course of training, thereby making the learning task progressively more difficult for the generator. We demonstrate that this strategy is key to obtaining state-of-the-art results in image generation. We also show evidence that this strategy may be broadly applicable to improving GAN training in other data modalities.",0
"Title: ""Curriculum Learning with Generative Adversarial Networks""  Abstract: Recent advances in deep learning have enabled the development of generative adversarial networks (GANs), which can generate highly realistic synthetic data that closely resembles true data distributions. However, training GANs remains challenging due to instability issues such as mode collapse and vanishing gradients. To address these difficulties, we propose using curriculum learning techniques during the training process. Inspired by human education theory, curriculum learning involves gradually increasing difficulty levels while progressively improving model performance. Our approach applies curricula to both generator and discriminator components simultaneously, enabling better optimization and more effective adversarial examples. We evaluate our method on standard image generation benchmarks, demonstrating improved quality and stability compared to state-of-the-art methods without sacrificing diversity in generated samples. These results highlight the effectiveness of incorporating curricula into GAN training for generating high-fidelity images.",1
"In this paper, we investigate the use of generative adversarial networks in the task of image generation according to subjective measures of semantic attributes. Unlike the standard (CGAN) that generates images from discrete categorical labels, our architecture handles both continuous and discrete scales. Given pairwise comparisons of images, our model, called RankCGAN, performs two tasks: it learns to rank images using a subjective measure; and it learns a generative model that can be controlled by that measure. RankCGAN associates each subjective measure of interest to a distinct dimension of some latent space. We perform experiments on UT-Zap50K, PubFig and OSR datasets and demonstrate that the model is expressive and diverse enough to conduct two-attribute exploration and image editing.",0
"This research addresses the challenges involved in evaluating Conditional Generative Adversarial Networks (CGANs) by exploring the subjective control that can be exerted over semantic image attributes. We propose an original framework called ""Ranking CGANs"" which uses natural language queries to rank generated images according to their attribute values. Our approach outperforms existing evaluation methods, demonstrating greater accuracy and improved user satisfaction. Results suggest that our method could lead to more intuitive and effective use of CGANs in computer vision applications.",1
"As more and more personal photos are shared and tagged in social media, avoiding privacy risks such as unintended recognition becomes increasingly challenging. We propose a new hybrid approach to obfuscate identities in photos by head replacement. Our approach combines state of the art parametric face synthesis with latest advances in Generative Adversarial Networks (GAN) for data-driven image synthesis. On the one hand, the parametric part of our method gives us control over the facial parameters and allows for explicit manipulation of the identity. On the other hand, the data-driven aspects allow for adding fine details and overall realism as well as seamless blending into the scene context. In our experiments, we show highly realistic output of our system that improves over the previous state of the art in obfuscation rate while preserving a higher similarity to the original image content.",0
"In recent years, identity obfuscation has become increasingly important due to concerns over privacy violations. One approach to addressing this issue is through face replacement techniques that replace individualsâ€™ faces in images with other images while preserving the original context. This technique can serve as a powerful tool for protecting individualsâ€™ identities without compromising on image quality. However, existing face replacement methods often result in degraded visual quality or suffer from imperfect alignment issues. To overcome these limitations, we propose a hybrid model that integrates both deep learning based synthesis networks and traditional morphing approaches. Our method achieves superior results compared to state-of-the-art alternatives across multiple metrics such as structural similarity (SSIM), peak signal-to-noise ratio (PSNR), and facial landmark distance measures. We demonstrate the effectiveness of our approach using comprehensive experiments on four large datasets, each containing thousands of images. By combining the strengths of modern synthesis networks and classical morphing techniques, our hybrid approach offers significant improvements in image quality and alignment accuracy. Ultimately, our work provides a more robust solution for identity obfuscation tasks, ensuring better privacy protection while maintaining high visual fidelity.",1
"While conventional depth estimation can infer the geometry of a scene from a single RGB image, it fails to estimate scene regions that are occluded by foreground objects. This limits the use of depth prediction in augmented and virtual reality applications, that aim at scene exploration by synthesizing the scene from a different vantage point, or at diminished reality. To address this issue, we shift the focus from conventional depth map prediction to the regression of a specific data representation called Layered Depth Image (LDI), which contains information about the occluded regions in the reference frame and can fill in occlusion gaps in case of small view changes. We propose a novel approach based on Convolutional Neural Networks (CNNs) to jointly predict depth maps and foreground separation masks used to condition Generative Adversarial Networks (GANs) for hallucinating plausible color and depths in the initially occluded areas. We demonstrate the effectiveness of our approach for novel scene view synthesis from a single image.",0
"""Depth prediction is a key task in computer vision that enables applications such as 3D reconstruction, virtual reality, and autonomous driving. Traditional depth estimation methods rely on multi-view stereo, LiDAR point clouds, or RGB-D sensors which can be expensive and limit their use cases. In this paper, we propose a novel approach for layered depth prediction from a single image using a deep neural network architecture. Our method leverages recent advances in generative adversarial networks (GAN) and attention mechanisms to predict per-pixel semantic labels representing object surfaces at different distances. We validate our approach on two benchmark datasets and show promising results compared to state-of-the-art methods.""",1
"In this work we present a novel system for generation of virtual PET images using CT scans. We combine a fully convolutional network (FCN) with a conditional generative adversarial network (GAN) to generate simulated PET data from given input CT data. The synthesized PET can be used for false-positive reduction in lesion detection solutions. Clinically, such solutions may enable lesion detection and drug treatment evaluation in a CT-only environment, thus reducing the need for the more expensive and radioactive PET/CT scan. Our dataset includes 60 PET/CT scans from Sheba Medical center. We used 23 scans for training and 37 for testing. Different schemes to achieve the synthesized output were qualitatively compared. Quantitative evaluation was conducted using an existing lesion detection software, combining the synthesized PET as a false positive reduction layer for the detection of malignant lesions in the liver. Current results look promising showing a 28% reduction in the average false positive per case from 2.9 to 2.1. The suggested solution is comprehensive and can be expanded to additional body organs, and different modalities.",0
"In medical imaging analysis, accurate lesion detection is crucial for diagnosis and treatment planning. While computed tomography (CT) and positron emission tomography (PET) provide complementary information, their differences in image quality can lead to challenges in automated lesion detection. To address these challenges, we propose a novel approach that synthesizes images across modalities using fully convolutional networks (FCN) and generative adversarial networks (GAN). Our method leverages FCN to learn intrinsic features and enhance segmentation accuracy by utilizing both CT and PET data. Meanwhile, GAN improves fidelity in generated images by preserving important details lost during translation between modalities. Experimental results demonstrate improved sensitivity compared to current state-of-the-art methods while maintaining high specificity. This work has significant implications for enhancing clinical workflows and improving patient outcomes.",1
"We aim to tackle a novel task in action detection - Online Detection of Action Start (ODAS) in untrimmed, streaming videos. The goal of ODAS is to detect the start of an action instance, with high categorization accuracy and low detection latency. ODAS is important in many applications such as early alert generation to allow timely security or emergency response. We propose three novel methods to specifically address the challenges in training ODAS models: (1) hard negative samples generation based on Generative Adversarial Network (GAN) to distinguish ambiguous background, (2) explicitly modeling the temporal consistency between data around action start and data succeeding action start, and (3) adaptive sampling strategy to handle the scarcity of training data. We conduct extensive experiments using THUMOS'14 and ActivityNet. We show that our proposed methods lead to significant performance gains and improve the state-of-the-art methods. An ablation study confirms the effectiveness of each proposed method.",0
"Increasingly thereâ€™s more streams of videos available online on platforms such as youtube, twitch etc.. With unlimited storage space people have started uploading hours and hours of raw footage without any editing which would make it easy to identify when someone actually starts performing some action vs just doing random movements around in front of the camera. We want to figure out how we can detect these actions as they happen in realtime so content consumers donâ€™t need to manually find that sweet spot where the person actually starts doing something interesting in the vast sea of boring videos. For example if I search â€˜hunting tutorialâ€™ I should immediately see clips where the hunter is explaining their technique rather than showing them carrying equipment in slow motion from their car into the forest. This research has been done before but mostly for offline, preprocessed video content (which makes sense considering online streaming wasnâ€™t very big back then). Our approach involves training machine learning models using labelled data available online to automatically predict action classes from raw RGB frames extracted directly from the stream. Our method relies heavily on Convolutional Neural Networks, more specifically single shot multi box detector architectures like YOLOv4 which allow us to perform object detection at high speed making it suitable for low latency applications like our use case here. By carefully fine tuning hyperparameters for better performance and creating new techniques for data augmentation we can achieve state of the art results even on relatively small datasets compared to other approaches previously used by competitors; bringing the barrier of entry lower allowing more people to easily develop similar solutions for more specialized tasks/datasets down the road. Ultimately our goal is to provide a good starting point f",1
"Despite generative adversarial networks (GANs) can hallucinate photo-realistic high-resolution (HR) faces from low-resolution (LR) faces, they cannot guarantee preserving the identities of hallucinated HR faces, making the HR faces poorly recognizable. To address this problem, we propose a Siamese GAN (SiGAN) to reconstruct HR faces that visually resemble their corresponding identities. On top of a Siamese network, the proposed SiGAN consists of a pair of two identical generators and one discriminator. We incorporate reconstruction error and identity label information in the loss function of SiGAN in a pairwise manner. By iteratively optimizing the loss functions of the generator pair and discriminator of SiGAN, we cannot only achieve photo-realistic face reconstruction, but also ensures the reconstructed information is useful for identity recognition. Experimental results demonstrate that SiGAN significantly outperforms existing face hallucination GANs in objective face verification performance, while achieving photo-realistic reconstruction. Moreover, for input LR faces from unknown identities who are not included in training, SiGAN can still do a good job.",0
"Artificial intelligence (AI) has revolutionized many aspects of modern life, from computer vision and natural language processing to robotics and gaming. One area where AI has made significant strides recently is face hallucination - generating high-resolution images of faces that never existed before by starting with low-resolution inputs. However, most existing methods suffer from several shortcomings, such as poor visual quality, lack of identity preservation, and sensitivity to input perturbations. In this work, we propose a novel method called SiGAN that addresses these issues by using a Siamese architecture inspired by human perception, combined with adversarial training and pixel-wise discriminators. We show through extensive experiments on three challenging datasets that our approach outperforms state-of-the-art methods both quantitatively and qualitatively in terms of identity preservation, visual fidelity, and robustness to input variations. Our findings demonstrate the potential of SiGANs for synthesizing realistic yet identifiable faces at unprecedented scales, which could have important implications for applications ranging from entertainment to security and surveillance. Overall, this research represents an important step forward in the development of intelligent systems capable of creating realistic images of non-existent objects.",1
"Distributional reinforcement learning (distributional RL) has seen empirical success in complex Markov Decision Processes (MDPs) in the setting of nonlinear function approximation. However, there are many different ways in which one can leverage the distributional approach to reinforcement learning. In this paper, we propose GAN Q-learning, a novel distributional RL method based on generative adversarial networks (GANs) and analyze its performance in simple tabular environments, as well as OpenAI Gym. We empirically show that our algorithm leverages the flexibility and blackbox approach of deep learning models while providing a viable alternative to traditional methods.",0
"GANs (Generative Adversarial Networks) have proven effective in generating realistic synthetic data that can improve training performance for supervised learning tasks like classification. In contrast to traditional datasets used for supervised learning where we already know which labels/answers correspond to each sample in the dataset, GAN samples may have unknown/unreliable/noisy labels. We address this problem by combining generative models with reinforcement learning; specifically we introduce an actor model trained via Deep Q-Networks. Our algorithm generates images from a generator network while minimizing their reconstruction error, as well as matching the true label. This improves the quality of generated data by allowing our model to generate new training examples better aligned with the true target distribution. We test our method on both unsupervised feature learning for image classification using MNIST and SVHN digits. We show that our approach is competitive with previous state-of-the art methods such as Unsupervised Domain Adaptation which uses additional labeled target domain data. We believe that our work has potential to significantly impact future machine learning research since it shows how one can effectively combine several approaches into hybrid systems which outperform current standalone approaches. Furthermore, we envision more general applications of these architectures beyond semi-supervised learning and digit recognition e.g., in natural language processing and robotics. This study proposes a novel approach to improve the quality of generated synthetic data in GANs through the integration of generative models with deep reinforcement learning algorithms. By incorporating a Deep Q-Network into the generative process, the proposed method addresses the issue of unknown or noisy labels in GAN-generated samples, resulting in improved alignment with the true target distribution and superior performance compared to existing state-of-the-art techniques. The experimental results demonstrate the effectiveness of this hybrid architecture across diverse domains including image classification and digit recognition, suggesting significant implications for broader applications in areas such as nat",1
"We propose a novel framework for simultaneously generating and manipulating the face images with desired attributes. While the state-of-the-art attribute editing technique has achieved the impressive performance for creating realistic attribute effects, they only address the image editing problem, using the input image as the condition of model. Recently, several studies attempt to tackle both novel face generation and attribute editing problem using a single solution. However, their image quality is still unsatisfactory. Our goal is to develop a single unified model that can simultaneously create and edit high quality face images with desired attributes. A key idea of our work is that we decompose the image into the latent and attribute vector in low dimensional representation, and then utilize the GAN framework for mapping the low dimensional representation to the image. In this way, we can address both the generation and editing problem by learning the generator. For qualitative and quantitative evaluations, the proposed algorithm outperforms recent algorithms addressing the same problem. Also, we show that our model can achieve the competitive performance with the state-of-the-art attribute editing technique in terms of attribute editing quality.",0
"This paper presents editable generative adversarial networks (GANs) that can generate realistic images while allowing direct editing by users. Traditional GANs suffer from stability issues during training, which limits their use as image generation tools. By addressing these instability problems, we improve both image quality and model robustness. Users can then edit these generated faces through intuitive interfaces, such as sliders or facial feature manipulation tools. We evaluate our method on two datasets with different characteristics, demonstrating significant improvements over existing state-of-the-art methods. Our results show that editable GANs provide high-quality images that better capture the diversity present in human faces. Furthermore, our user studies demonstrate that novice users find our interface easy to use, making it possible for non-experts to create and manipulate convincing digital characters without advanced computer graphics skills. Overall, this work pushes forward the boundaries of what is possible with generative models and opens up new possibilities for virtual and augmented reality applications requiring photorealistic personalized avatars.",1
"In this paper, we propose an improved quantitative evaluation framework for Generative Adversarial Networks (GANs) on generating domain-specific images, where we improve conventional evaluation methods on two levels: the feature representation and the evaluation metric. Unlike most existing evaluation frameworks which transfer the representation of ImageNet inception model to map images onto the feature space, our framework uses a specialized encoder to acquire fine-grained domain-specific representation. Moreover, for datasets with multiple classes, we propose Class-Aware Frechet Distance (CAFD), which employs a Gaussian mixture model on the feature space to better fit the multi-manifold feature distribution. Experiments and analysis on both the feature level and the image level were conducted to demonstrate improvements of our proposed framework over the recently proposed state-of-the-art FID method. To our best knowledge, we are the first to provide counter examples where FID gives inconsistent results with human judgments. It is shown in the experiments that our framework is able to overcome the shortness of FID and improves robustness. Code will be made available.",0
"""Evaluating generative adversarial networks (GANs) has always been challenging due to their complexity and lack of ground truth data. In recent years, several evaluation frameworks have been proposed to assess GAN performance, but they suffer from limitations such as reliance on subjective judgments or insufficient consideration of important factors like mode dropping and diversity. This study proposes an improved evaluation framework that addresses these issues by incorporating multiple objective metrics along with visual inspection. The framework evaluates both unconditional and conditional models across four dimensions: quality, stability, diversity, and fidelity. Quality is assessed using Frechet Inception Distance (FID), which measures the distance between real and generated images while controlling for batch effects. Stability is evaluated through convergence analysis during training and stability plots over time. Diversity is measured using a novel metric called Uniformity score, which quantifies how uniformly distributed generated samples are over the support of the generator. Fidelity is determined via qualitative comparisons against a reference model trained under similar conditions. Experimental results demonstrate that our proposed framework provides more comprehensive and accurate evaluation than existing methods. Overall, our work advances the state-of-the art in GAN evaluation, enabling researchers to better compare and develop cutting-edge GAN architectures.""",1
"Deformable Image Registration (DIR) of MR and CT images is one of the most challenging registration task, due to the inherent structural differences of the modalities and the missing dense ground truth. Recently cycle Generative Adversarial Networks (cycle-GANs) have been used to learn the intensity relationship between these 2 modalities for unpaired brain data. Yet its usefulness for DIR was not assessed.   In this study we evaluate the DIR performance for thoracic and abdominal organs after synthesis by cycle-GAN. We show that geometric changes, which differentiate the two populations (e.g. inhale vs. exhale), are readily synthesized as well. This causes substantial problems for any application which relies on spatial correspondences being preserved between the real and the synthesized image (e.g. plan, segmentation, landmark propagation). To alleviate this problem, we investigated reducing the spatial information provided to the discriminator by decreasing the size of its receptive fields.   Image synthesis was learned from 17 unpaired subjects per modality. Registration performance was evaluated with respect to manual segmentations of 11 structures for 3 subjects from the VISERAL challenge. State-of-the-art DIR methods based on Normalized Mutual Information (NMI), Modality Independent Neighborhood Descriptor (MIND) and their novel combination achieved a mean segmentation overlap ratio of 76.7, 67.7, 76.9%, respectively. This dropped to 69.1% or less when registering images synthesized by cycle-GAN based on local correlation, due to the poor performance on the thoracic region, where large lung volume changes were synthesized. Performance for the abdominal region was similar to that of CT-MRI NMI registration (77.4 vs. 78.8%) when using 3D synthesizing MRIs (12 slices) and medium sized receptive fields for the discriminator.",0
"Abstract: A new method based on generative adversarial networks (GAN) has been developed that enables faster deformable image registration using multi-modality imaging data such as CT and MRI. This approach utilizes the power of GAN to generate high resolution synthetic images from low resolution input scans. This greatly improves both accuracy and speed over traditional methods. In addition, our novel algorithm offers more precise control over different types of transformations through regularization. Our experiments show that the proposed method outperforms current state-of-the-art techniques by large margins in terms of execution time and overall quality metrics, making it an ideal solution for real-time image fusion applications. Keywords: generative adversarial networks; deformable image registration; CT-MRI alignment",1
"Automated lesion segmentation from computed tomography (CT) is an important and challenging task in medical image analysis. While many advancements have been made, there is room for continued improvements. One hurdle is that CT images can exhibit high noise and low contrast, particularly in lower dosages. To address this, we focus on a preprocessing method for CT images that uses stacked generative adversarial networks (SGAN) approach. The first GAN reduces the noise in the CT image and the second GAN generates a higher resolution image with enhanced boundaries and high contrast. To make up for the absence of high quality CT images, we detail how to synthesize a large number of low- and high-quality natural images and use transfer learning with progressively larger amounts of CT images. We apply both the classic GrabCut method and the modern holistically nested network (HNN) to lesion segmentation, testing whether SGAN can yield improved lesion segmentation. Experimental results on the DeepLesion dataset demonstrate that the SGAN enhancements alone can push GrabCut performance over HNN trained on original images. We also demonstrate that HNN + SGAN performs best compared against four other enhancement methods, including when using only a single GAN.",0
"In medical imaging analysis, lesion segmentation accuracy can greatly impact diagnosis and treatment planning decisions. However, manual segmentation by radiologists is time-consuming and prone to inter-observer variability. Automatic methods such as deep learning based algorithms have been proposed but still struggle due to challenges associated with low image quality from undergoing limited data augmentation strategies that limit their generalization ability across multiple MRI sequences and scanner protocols. To address these limitations, we propose using stacked generative adversarial networks (GAN) along with transfer learning to enhance the resolution of input images. By increasing the clarity of CT images, the GAN approach enables better training results which leads to improved generalizability among different MRIs. We evaluate our method on publicly available datasets, demonstrating greater Dice similarity coefficient values compared to baseline models trained without our pre-processing technique. Our work showcases how leveraging cutting edge machine learning techniques can overcome challenges faced by traditional approaches and yield more accurate results for critical clinical applications like detecting and analyzing brain tumors.",1
"Real-time detection of irregularities in visual data is very invaluable and useful in many prospective applications including surveillance, patient monitoring systems, etc. With the surge of deep learning methods in the recent years, researchers have tried a wide spectrum of methods for different applications. However, for the case of irregularity or anomaly detection in videos, training an end-to-end model is still an open challenge, since often irregularity is not well-defined and there are not enough irregular samples to use during training. In this paper, inspired by the success of generative adversarial networks (GANs) for training deep models in unsupervised or self-supervised settings, we propose an end-to-end deep network for detection and fine localization of irregularities in videos (and images). Our proposed architecture is composed of two networks, which are trained in competing with each other while collaborating to find the irregularity. One network works as a pixel-level irregularity Inpainter, and the other works as a patch-level Detector. After an adversarial self-supervised training, in which I tries to fool D into accepting its inpainted output as regular (normal), the two networks collaborate to detect and fine-segment the irregularity in any given testing video. Our results on three different datasets show that our method can outperform the state-of-the-art and fine-segment the irregularity.",0
"An effective security system for video monitoring must detect irregularities that might indicate intrusions or other security breaches. This task can become challenging due to dynamic backgrounds and changing light conditions that may cause false alarms. In this work, we propose AVID (Adversarial Visual Irregularity Detection), a novel approach that utilizes adversarial examples to improve the detection accuracy of visual anomalies in surveillance videos. We train a convolutional neural network using adversarial training techniques to enhance the robustness of our model against small perturbations in input images. Our experiments demonstrate the effectiveness of AVID in improving the detection rate of anomalies while reducing false positives compared to traditional methods. Overall, our findings showcase the potential of adversarial training as an alternative method for developing more reliable security systems.",1
"Video summarization plays an important role in video understanding by selecting key frames/shots. Traditionally, it aims to find the most representative and diverse contents in a video as short summaries. Recently, a more generalized task, query-conditioned video summarization, has been introduced, which takes user queries into consideration to learn more user-oriented summaries. In this paper, we propose a query-conditioned three-player generative adversarial network to tackle this challenge. The generator learns the joint representation of the user query and the video content, and the discriminator takes three pairs of query-conditioned summaries as the input to discriminate the real summary from a generated and a random one. A three-player loss is introduced for joint training of the generator and the discriminator, which forces the generator to learn better summary results, and avoids the generation of random trivial summaries. Experiments on a recently proposed query-conditioned video summarization benchmark dataset show the efficiency and efficacy of our proposed method.",0
"Title: Advancing Video Summary Generation through Human-AI Interaction In recent years, video summarization has emerged as a crucial task in multimedia research due to the vast amount of visual content available online. Traditional approaches rely on handcrafted features and predefined rules which can limit their effectiveness. With advancements in deep learning, data-driven models have shown promising results by automatically extracting relevant features from raw videos. However, these methods often lack interpretability and require extensive training data which may not always be available. This work proposes a novel query-conditioned three-player adversarial network (QCTPAN) that incorporates human feedback into the summary generation process. Our model utilizes two discriminators - one to evaluate the quality of the generated summary and another to determine if the summary satisfies the user's query condition. Additionally, we introduce a generator that produces summaries guided by both discriminator outputs and the user input query. Experimental evaluations demonstrate significant improvements over state-of-the-art approaches in terms of both quantitative metrics and subjective assessments by humans. Overall, our proposed QCTPAN framework represents a step forward in advancing video summary generation through human-AI interaction.",1
"Passenger Name Records (PNRs) are at the heart of the travel industry. Created when an itinerary is booked, they contain travel and passenger information. It is usual for airlines and other actors in the industry to inter-exchange and access each other's PNR, creating the challenge of using them without infringing data ownership laws. To address this difficulty, we propose a method to generate realistic synthetic PNRs using Generative Adversarial Networks (GANs). Unlike other GAN applications, PNRs consist of categorical and numerical features with missing/NaN values, which makes the use of GANs challenging. We propose a solution based on Cram\'{e}r GANs, categorical feature embedding and a Cross-Net architecture. The method was tested on a real PNR dataset, and evaluated in terms of distribution matching, memorization, and performance of predictive models for two real business problems: client segmentation and passenger nationality prediction. Results show that the generated data matches well with the real PNRs without memorizing them, and that it can be used to train models for real business applications.",0
"Title: Airline Passenger Name Record (PNR) Generation Using Generative Adversarial Networks (GANs)  Airlines collect passenger data such as name, age, gender, contact details, ticket booking information, etc., which are stored in PNR databases. These databases contain sensitive information that needs to be protected from unauthorized access. In recent years, deep learning techniques have been used to generate synthetic data for training machine learning models without compromising privacy. GANs are a popular choice due to their ability to generate realistic data samples by simulating real-world distributions.  This research proposes a novel approach to generating synthetic airline passenger records by leveraging GANs. We demonstrate how GANs can effectively create authentic looking fake PNRs that closely mimic real ones. Our method involves training two neural networks, one that generates PNRs and another that discriminates between genuine and fake PNRs. During the training process, we fine-tune both networks to minimize errors until the generated PNRs become indistinguishable from actual PNRs.  In our experiments, we showcase the effectiveness of our proposed technique on a large dataset containing real airline passenger records. Results indicate that the generated PNRs closely match the properties of real PNRs, achieving high similarity scores across multiple evaluation metrics. Moreover, we present qualitative analysis revealing that the generated PNRs exhibit characteristics commonly found in real datasets.  Our work has significant implications for the airline industry, enabling researchers to train predictive models without compromising individual privacy. This is particularly relevant given the increasing trend towards open data sharing initiatives. Additionally, our approach could potentially assist government agencies in developing tools for detecting fraudulent travel documents. Overall, our work presents promising results for advancing the use of synthetic data generation in protecting sensitive information while still supporting valuable applications.",1
Knowledge-based planning (KBP) is an automated approach to radiation therapy treatment planning that involves predicting desirable treatment plans before they are then corrected to deliverable ones. We propose a generative adversarial network (GAN) approach for predicting desirable 3D dose distributions that eschews the previous paradigms of site-specific feature engineering and predicting low-dimensional representations of the plan. Experiments on a dataset of oropharyngeal cancer patients show that our approach significantly outperforms previous methods on several clinical satisfaction criteria and similarity metrics.,0
"Title:Automated Treatment Planning in Radiation Therapy using Generative Adversarial NetworksAuthors: Xiaodong Lu (xdlu@microsoft.com) , Hantao Liu (hliu62@stanford.edu), Jian Zhang (jzhang39@gmail.com)AbstractRadiotherapy treatment planning is a complex task that requires expert knowledge and experience from radiation oncologists. Currently, it heavily relies on manual work by experts, which might lead to uncertainties and inconsistencies due to different interpretations or individual preferences. In this work, we propose a deep learning approach based on generative adversarial networks (GANs) to automatically generate conformal radiotherapy plans directly from clinical images such as CT scans. Our method can take as input DICOM images without requiring contours delineating target structures or organs at risk (OARs). By utilizing GANs, our model learns to optimize a score function corresponding to clinically meaningful criteria for plan quality, including tumor coverage and OAR sparing. We have validated our approach on real clinical cases and achieved comparable results against those manually designed by experts. Our study suggests that automated treatment planning using GANs could potentially streamline the current workflow and reduce inter-observer variability, thus improving patient outcomes while reducing healthcare costs.IntroductionThe process of developing radiotherapy treatment plans involves many steps like image interpretation, target definition, organ identification, dose optimization, etc.. Itâ€™s highly dependent on experienced experts and often limited by their subjective judgments [1]. As an alternative, Deep Learning techniques emerge recently and has been widely applied into medical imaging analysis tasks such as brain MRI segmentation [2], knee cartilage segmentation[3] and so on[4][5]. However, little progress was made towards automatic radiation therapy treatment planning (RTTP), one important part of RT",1
"We propose MAD-GAN, an intuitive generalization to the Generative Adversarial Networks (GANs) and its conditional variants to address the well known problem of mode collapse. First, MAD-GAN is a multi-agent GAN architecture incorporating multiple generators and one discriminator. Second, to enforce that different generators capture diverse high probability modes, the discriminator of MAD-GAN is designed such that along with finding the real and fake samples, it is also required to identify the generator that generated the given fake sample. Intuitively, to succeed in this task, the discriminator must learn to push different generators towards different identifiable modes. We perform extensive experiments on synthetic and real datasets and compare MAD-GAN with different variants of GAN. We show high quality diverse sample generations for challenging tasks such as image-to-image translation and face generation. In addition, we also show that MAD-GAN is able to disentangle different modalities when trained using highly challenging diverse-class dataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the end, we show its efficacy on the unsupervised feature representation task. In Appendix, we introduce a similarity based competing objective (MAD-GAN-Sim) which encourages different generators to generate diverse samples based on a user defined similarity metric. We show its performance on the image-to-image translation, and also show its effectiveness on the unsupervised feature representation task.",0
"In recent years, generative adversarial networks (GANs) have become increasingly popular due to their ability to generate high-quality images and other data that closely resemble real-world examples. However, these GAN models often suffer from instability during training, leading to poor performance or even failure altogether. This paper proposes using multi-agent diverse generative adversarial networks (MDGANs), which incorporate multiple agents with different objectives and abilities into the standard GAN architecture. By doing so, we aim to increase stability during training while promoting greater diversity among generated samples. Our results show that MDGANs can indeed produce more stable and higher quality outputs compared to traditional single agent GANs, particularly on tasks involving image generation and text completion. Overall, our work highlights the potential benefits of using multi-agent systems in the field of generative modeling, paving the way for future research in this direction.",1
"In this paper we show strategies to easily identify fake samples generated with the Generative Adversarial Network framework. One strategy is based on the statistical analysis and comparison of raw pixel values and features extracted from them. The other strategy learns formal specifications from the real data and shows that fake samples violate the specifications of the real data. We show that fake samples produced with GANs have a universal signature that can be used to identify fake samples. We provide results on MNIST, CIFAR10, music and speech data.",0
"Recent research has proposed methods based on deep neural networks (DNNs) that can effectively detect whether images were generated by generative adversarial network models (GAN). However, these methods suffer from high computational cost, complex architectures, and large memory requirements. We present a novel method called TequilaGAN that addresses these issues while maintaining accuracy. Our approach utilizes feature maps produced from a pre-trained VGG model and a shallow architecture consisting only a few convolutional layers. With minimal training data and low computational overhead, we achieve state-of-the-art results outperforming prior DNN-based methods without compromising speed or scalability. This demonstrates our method as a viable alternative for real-world image generation detection applications where resources are limited. Ultimately, our work sets a new standard for efficient and accurate image generation detection.",1
"Without any specific way for imbalance data classification, artificial intelligence algorithm cannot recognize data from minority classes easily. In general, modifying the existing algorithm by assuming that the training data is imbalanced, is the only way to handle imbalance data. However, for a normal data handling, this way mostly produces a deficient result. In this research, we propose a class expert generative adversarial network (CE-GAN) as the solution for imbalance data classification. CE-GAN is a modification in deep learning algorithm architecture that does not have an assumption that the training data is imbalance data. Moreover, CE-GAN is designed to identify more detail about the character of each class before classification step. CE-GAN has been proved in this research to give a good performance for imbalance data classification.",0
"Deep learning has been widely used as a powerful tool to classify imbalanced data due to its capability of extracting high-level features from raw inputs automatically and effectively handling large amounts of complex data. One popular method used to address the problem of imbalanced datasets is over sampling. However, such methods can lead to oversimplification of decision boundaries and deterioration of generalization performance. To overcome these issues, researchers have proposed more advanced techniques including undersampling, ensembling and active learning. Another approach is to use generative adversarial networks (GANs). GANs consist of two sub-networks competitively trained against each other: one generates samples and the other discriminates real data from generated ones. In our work, we present a new deep architecture called 'Class Expert Generative Adversarial Network' specifically designed for classification of highly imbalanced datasets. Our network consists of multiple expert discriminators that focus on different classes, which helps generate better quality synthetic minority class samples by forcing the generator to learn finer details specific to individual classes. We demonstrate significant improvements in accuracy on benchmark imbalance datasets through experiments comparing our model to state-of-the-art models based on various evaluation metrics. Finally, we analyze the effectiveness of our network components via ablation studies and provide insights into how they contribute to improved performance on highly imbalanced datasets. Overall, our work makes an important contribution towards improving the robustness and effectiveness of machine learning algorithms for classification problems in real world applications where data is often heavily skewed.",1
"Deep learning usually requires big data, with respect to both volume and variety. However, most remote sensing applications only have limited training data, of which a small subset is labeled. Herein, we review three state-of-the-art approaches in deep learning to combat this challenge. The first topic is transfer learning, in which some aspects of one domain, e.g., features, are transferred to another domain. The next is unsupervised learning, e.g., autoencoders, which operate on unlabeled data. The last is generative adversarial networks, which can generate realistic looking data that can fool the likes of both a deep learning network and human. The aim of this article is to raise awareness of this dilemma, to direct the reader to existing work and to highlight current gaps that need solving.",0
"Deep learning has emerged as a powerful tool for analyzing satellite imagery due to its ability to extract meaningful features from high-resolution remotely sensed images. However, one major challenge faced by researchers in this field is the availability of large amounts of labeled training data. Limited training data can lead to suboptimal results and decreased performance compared to models trained on larger datasets. This review article provides a comprehensive overview of state-of-the-art methods that have been proposed to address this problem of scarce training data in remote sensing applications using deep learning techniques. We describe popular transfer learning approaches, such as fine-tuning pretrained models, knowledge distillation, and model ensembling, which have shown promising results even with small amounts of data. Additionally, we discuss recent advancements in semi-supervised and active learning strategies, which aim at utilizing unlabeled data and selectively collecting labels to further improve model performance. Finally, we identify several challenges and open issues related to these approaches and present future directions for research in this area. Our review will provide valuable insights for both practitioners and researchers interested in developing efficient and effective solutions for leveraging deep learning in remote sensing applications with limited training data.",1
"Generative adversarial networks (GANs) are powerful tools for learning generative models. In practice, the training may suffer from lack of convergence. GANs are commonly viewed as a two-player zero-sum game between two neural networks. Here, we leverage this game theoretic view to study the convergence behavior of the training process. Inspired by the fictitious play learning process, a novel training method, referred to as Fictitious GAN, is introduced. Fictitious GAN trains the deep neural networks using a mixture of historical models. Specifically, the discriminator (resp. generator) is updated according to the best-response to the mixture outputs from a sequence of previously trained generators (resp. discriminators). It is shown that Fictitious GAN can effectively resolve some convergence issues that cannot be resolved by the standard training approach. It is proved that asymptotically the average of the generator outputs has the same distribution as the data samples.",0
"In this paper we address the problem that current generative adversarial networks (GANs) can only learn from a single historical model at one time by describing a solution based on training multiple models simultaneously. To create fictitious data from past experiences stored by each of these models, our method employs real images as priors so that new generated images exhibit higher degrees of detail and fidelity compared to prior work. Experiments demonstrate marked improvement over existing methods across different architectures, datasets, and metrics. With further improvements such as faster convergence speeds and more stable training dynamics observed under certain settings. Additionally, we compare our approach against state of art models and showcase how well it performs. Lastly, while GAN has been used to produce novel synthetic training samples, they require paired training examples which limits their use cases where labels cannot be easily obtained. Our experiments with conditional image generation shows promise towards removing this dependency allowing future GAN applications beyond supervised learning tasks.",1
"Generative Adversarial Networks are powerful generative models that are able to model the manifold of natural images. We leverage this property to perform manifold regularization by approximating a variant of the Laplacian norm using a Monte Carlo approximation that is easily computed with the GAN. When incorporated into the semi-supervised feature-matching GAN we achieve state-of-the-art results for GAN-based semi-supervised learning on CIFAR-10 and SVHN benchmarks, with a method that is significantly easier to implement than competing methods. We also find that manifold regularization improves the quality of generated images, and is affected by the quality of the GAN used to approximate the regularizer.",0
"Semi-supervised learning (SSL) has emerged as one of the most promising approaches to leverage unlabeled data for improving model accuracy. While conventional SSL methods rely on assumptions such as smoothness priors or self-training, recent work has shown that generative adversarial networks (GANs) can be used to implicitly enforce constraints in the latent space of SSL models. In this paper, we propose manifold regularization with GANs (MREGAN), which incorporates both explicit SSL constraints and implicit manifold regularization through adversarial training. We demonstrate via experiments on several benchmark datasets that MREGAN significantly outperforms state-of-the-art SSL methods while achieving competitive results compared to fully supervised models trained on all labeled data. Our framework offers new opportunities for exploiting diverse forms of supervision from different modalities, opening up possibilities for multi-task joint learning across multiple domains.",1
"Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as emerging families for generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way. For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples. Experiments show generality and effectiveness of the transferred techniques.",0
"Deep generative models have emerged as powerful tools for modeling complex data distributions. These models have shown impressive performance on tasks ranging from image generation and text synthesis to speech recognition and machine translation. However, there remain significant challenges in understanding and unifying these diverse approaches, which often require specialized architectures and training procedures. This work proposes a framework that brings together three key aspects: (i) probabilistic reasoning through explicit computation of likelihoods; (ii) latent variable inference via nonlinear optimization; (iii) deep neural networks defined by normalizing flows with invertible mapping. We show how many well-known deep generative models can be seen as instances within our proposed framework, allowing us to unify them and provide new insights into their properties. Furthermore, we demonstrate state-of-the-art results across several benchmark datasets using our simple yet flexible architecture. Our findings suggest a promising direction towards general-purpose deep generative models with strong empirical performance and theoretical grounding.",1
"We explore recurrent encoder multi-decoder neural network architectures for semi-supervised sequence classification and reconstruction. We find that the use of multiple reconstruction modules helps models generalize in a classification task when only a small amount of labeled data is available, which is often the case in practice. Such models provide useful high-level representations of motions allowing clustering, searching and faster labeling of new sequences. We also propose a new, realistic partitioning of a well-known, high quality motion-capture dataset for better evaluations. We further explore a novel formulation for future-predicting decoders based on conditional recurrent generative adversarial networks, for which we propose both soft and hard constraints for transition generation derived from desired physical properties of synthesized future movements and desired animation goals. We find that using such constraints allow to stabilize the training of recurrent adversarial architectures for animation generation.",0
"This paper presents two new techniques for learning from limited amounts of data: recurrent semi-supervised classification and constrained adversarial generation. We apply these methods to the problem of motion capture data, where large labeled datasets can be expensive or difficult to obtain. Our first method, recurrent semi-supervised classification (RSSC), leverages temporal consistency in the motion capture sequences by treating each time step as a separate example and using standard supervised learning approaches. However, since there may only be a few examples per class at each time step, we employ regularization to encourage the model to use the unlabeled data effectively during training. Experimental results show that RSSC significantly improves accuracy compared to both traditional fully supervised learning and other popular semi-supervised techniques on challenging action recognition benchmarks. Additionally, our second technique, constrained adversarial generation (CAG), uses generative models to synthesize additional motion capture sequences consistent with the true distribution while respecting known constraints such as movement speed limits. These artificially generated samples act as extra training data to boost performance even further. By combining CAG and RSSC, we achieve state-of-the-art results on the most widely used benchmark dataset. Furthermore, we analyze the impact of different components of our approach, including feature representations, adversarial training, recurrence, and temporal consistency. Overall, our work demonstrates the feasibility of high-quality activity analysis under severely restricted label budgets, making these algorithms especially applicable in real-world applications where acquiring labels remains arduous.",1
"Generative adversarial networks (GANs) have been shown to produce realistic samples from high-dimensional distributions, but training them is considered hard. A possible explanation for training instabilities is the inherent imbalance between the networks: While the discriminator is trained directly on both real and fake samples, the generator only has control over the fake samples it produces since the real data distribution is fixed by the choice of a given dataset. We propose a simple modification that gives the generator control over the real samples which leads to a tempered learning process for both generator and discriminator. The real data distribution passes through a lens before being revealed to the discriminator, balancing the generator and discriminator by gradually revealing more detailed features necessary to produce high-quality results. The proposed module automatically adjusts the learning process to the current strength of the networks, yet is generic and easy to add to any GAN variant. In a number of experiments, we show that this can improve quality, stability and/or convergence speed across a range of different GAN architectures (DCGAN, LSGAN, WGAN-GP).",0
"Artificial intelligence (AI) has made significant advancements over recent years. One prominent area of research within machine learning is adversarial training, where neural networks are trained on both clean data and perturbed versions of that data, such as images that have been slightly altered by adding noise, resulting in improved generalization capabilities and robustness against attacks from other models trying to ""trick"" them into producing incorrect outputs. This work introduces tempered adversarial training, which takes a novel approach to selecting the strength of adversaries at each iteration during training; instead of using static values like previous methods, we sample from distributions based on the current training round number. Our experiments demonstrate that tempering improves performance across multiple benchmark datasets, including CIFAR-10, SVHN and ImageNet. Furthermore, our method reduces instability caused by the selection procedure used in vanilla FGSM attacks. We release our code publicly so others may reproduce these promising results. By incorporating tempered adversarial training into their own projects, practitioners can hope to realize similar improvements on whatever problems they tackle next.",1
"The ability to learn from incrementally arriving data is essential for any life-long learning system. However, standard deep neural networks forget the knowledge about the old tasks, a phenomenon called catastrophic forgetting, when trained on incrementally arriving data. We discuss the biases in current Generative Adversarial Networks (GAN) based approaches that learn the classifier by knowledge distillation from previously trained classifiers. These biases cause the trained classifier to perform poorly. We propose an approach to remove these biases by distilling knowledge from the classifier of AC-GAN. Experiments on MNIST and CIFAR10 show that this method is comparable to current state of the art rehearsal based approaches. The code for this paper is available at https://bit.ly/incremental-learning",0
"This paper investigates several techniques that use distillation methods to improve pseudo-rehearsal based incremental learning models. These techniques help overcome limitations related to dataset drift due to changes in user interests, device hardware, network conditions, etc., which can lead to severe forgetting in incremental lifelong learning systems. Our approach combines ideas from data augmentation, knowledge transfer, and generative modeling to create synthetic examples that mimic real-world features while preserving their semantic meaning. Empirical evaluations demonstrate significant improvements over strong baselines across multiple tasks and datasets, including image classification on CIFAR10/100, video action recognition on HMDB51/UCF101, object detection on Pascal VOC2007/COCO, sentiment analysis on IMDb/Yelp reviews, and language translation on IWSLT14 DE/EN.",1
"In recent years, research on image generation methods has been developing fast. The auto-encoding variational Bayes method (VAEs) was proposed in 2013, which uses variational inference to learn a latent space from the image database and then generates images using the decoder. The generative adversarial networks (GANs) came out as a promising framework, which uses adversarial training to improve the generative ability of the generator. However, the generated pictures by GANs are generally blurry. The deep convolutional generative adversarial networks (DCGANs) were then proposed to leverage the quality of generated images. Since the input noise vectors are randomly sampled from a Gaussian distribution, the generator has to map from a whole normal distribution to the images. This makes DCGANs unable to reflect the inherent structure of the training data. In this paper, we propose a novel deep model, called generative adversarial networks with decoder-encoder output noise (DE-GANs), which takes advantage of both the adversarial training and the variational Bayesain inference to improve the performance of image generation. DE-GANs use a pre-trained decoder-encoder architecture to map the random Gaussian noise vectors to informative ones and pass them to the generator of the adversarial networks. Since the decoder-encoder architecture is trained by the same images as the generators, the output vectors could carry the intrinsic distribution information of the original images. Moreover, the loss function of DE-GANs is different from GANs and DCGANs. A hidden-space loss function is added to the adversarial loss function to enhance the robustness of the model. Extensive empirical results show that DE-GANs can accelerate the convergence of the adversarial training process and improve the quality of the generated images.",0
"The paper presents novel applications and uses of GANs that go beyond their traditional use in generative modeling tasks like image generation. We show how they can be used as feature discriminators for semantic segmentation, text completion using variational autoencoders (VAEs), and improving stability during training by adding decoder-encoder output noise. Furthermore, we propose two new loss functions: one based on Earth Movers Distance which allows us to perform unsupervised learning and another which utilizes a pretrained VAE. Our contributions provide insights into the potential versatility of these powerful models outside of generative modeling tasks.",1
"An analytic process is iterative between two agents, an analyst and an analytic toolbox. Each iteration comprises three main steps: preparing a dataset, running an analytic tool, and evaluating the result, where dataset preparation and result evaluation, conducted by the analyst, are largely domain-knowledge driven. In this work, the focus is on automating the result evaluation step. The underlying problem is to identify plots that are deemed interesting by an analyst. We propose a methodology to learn such analyst's intent based on Generative Adversarial Networks (GANs) and demonstrate its applications in the context of production yield optimization using data collected from several product lines.",0
"In recent years, there has been significant interest in using data analytics to gain insights into manufacturing yield issues that can lead to cost savings in production processes. Traditionally, analyzing yields from semiconductor fabrication facilities involved time-consuming manual labor, making it difficult to pinpoint problematic areas quickly. With advancements in technology and automation, companies have begun integrating powerful machine learning algorithms into their analysis pipelines to uncover meaningful patterns in large datasets. This study proposes a new approach based on feature engineering techniques used in anomaly detection methods like clustering and PCA (principal component analysis) to identify unusual occurrences during wafer processing stages. By extracting relevant features and applying appropriate statistical models, we aim to provide accurate predictions to reduce yield loss caused by process variation. Our experimental results demonstrate the effectiveness of our methodology in identifying potential sources of yield degradation and suggest ways to improve product quality and overall efficiency within the manufacturing line. Overall, our findings contribute to the growing body of research utilizing advanced analytical tools in industry settings, highlighting the importance of innovative solutions for maintaining competitiveness amidst increasing competition.",1
"Deep generative models have shown promising results in generating realistic images, but it is still non-trivial to generate images with complicated structures. The main reason is that most of the current generative models fail to explore the structures in the images including spatial layout and semantic relations between objects. To address this issue, we propose a novel deep structured generative model which boosts generative adversarial networks (GANs) with the aid of structure information. In particular, the layout or structure of the scene is encoded by a stochastic and-or graph (sAOG), in which the terminal nodes represent single objects and edges represent relations between objects. With the sAOG appropriately harnessed, our model can successfully capture the intrinsic structure in the scenes and generate images of complicated scenes accordingly. Furthermore, a detection network is introduced to infer scene structures from a image. Experimental results demonstrate the effectiveness of our proposed method on both modeling the intrinsic structures, and generating realistic images.",0
"In recent years, deep learning has been shown to achieve state-of-the-art performance on many tasks including image classification, speech recognition, machine translation and natural language understanding. Despite these successes, most models are trained one task at a time, requiring specialization by hand-engineering features or fine-tuning hyperparameters for each new domain or problem. This process can be slow and often requires significant expertise. In contrast, humans are capable of quickly adapting their knowledge across vastly different domains. Thus, there remains an important gap between current artificial intelligence (AI) systems and human cognition. We aim to bridge this gap through introducing new approaches that learn multiple tasks efficiently. Our proposed model seeks to enable flexible knowledge transfer between multiple tasks allowing our system to generalize better than prior work. Here we focus specifically on developing novel methods for generative modelling which allow our system to generate realistic data from descriptions or even pure text prompts without access to any examples from the target domain. Experimental results show that our model significantly outperforms previous techniques and sets new benchmarks on several challenging datasets demonstrating both qualitative improvements as well as achieving quantitatively superior performance. Overall our approach advances the field towards more efficient utilisation of large scale pretraining, enabling models to effectively adapt to novel situations.",1
"Due to the world's demand for security systems, biometrics can be seen as an important topic of research in computer vision. One of the biometric forms that has been gaining attention is the recognition based on sclera. The initial and paramount step for performing this type of recognition is the segmentation of the region of interest, i.e. the sclera. In this context, two approaches for such task based on the Fully Convolutional Network (FCN) and on Generative Adversarial Network (GAN) are introduced in this work. FCN is similar to a common convolution neural network, however the fully connected layers (i.e., the classification layers) are removed from the end of the network and the output is generated by combining the output of pooling layers from different convolutional ones. The GAN is based on the game theory, where we have two networks competing with each other to generate the best segmentation. In order to perform fair comparison with baselines and quantitative and objective evaluations of the proposed approaches, we provide to the scientific community new 1,300 manually segmented images from two databases. The experiments are performed on the UBIRIS.v2 and MICHE databases and the best performing configurations of our propositions achieved F-score's measures of 87.48% and 88.32%, respectively.",0
"This paper presents two methods which use deep learning techniques such as fully convolutional networks (FCN) and generative adversarial networks (GAN) to detect and segment blood vessels within images of human eyes using sclera data. Firstly we look at how to adapt FCNs to perform end-to-end retinal vessel segmentation from non-centered fundus image patches without any preprocessing step. We demonstrate that our method achieves state of the art performance on both DRIVE and CHASE_DB databases. Secondly we introduce a GAN composed of generator network dedicated to mapping input sclera images into their corresponding ground truth masks and a discriminator trained to spot fake outputs. Our approach outperforms state-of-the-art performances and allows for more effective eye diseases diagnostics by enabling automated scleral vessel extraction. Both approaches have numerous potential applications ranging from telemedicine and health monitoring systems, biometrics and security services. The code used during our experiments is available online.",1
"This paper introduces a novel approach for unsupervised object co-localization using Generative Adversarial Networks (GANs). GAN is a powerful tool that can implicitly learn unknown data distributions in an unsupervised manner. From the observation that GAN discriminator is highly influenced by pixels where objects appear, we analyze the internal layers of discriminator and visualize the activated pixels. Our important finding is that high image diversity of GAN, which is a main goal in GAN research, is ironically disadvantageous for object localization, because such discriminators focus not only on the target object, but also on the various objects, such as background objects. Based on extensive evaluations and experimental studies, we show the image diversity and localization performance have a negative correlation. In addition, our approach achieves meaningful accuracy for unsupervised object co-localization using publicly available benchmark datasets, even comparable to state-of-the-art weakly-supervised approach.",0
"In recent years, advances in computer vision have enabled the development of algorithms that can automatically localize objects in images. However, these methods often require large amounts of labeled data for training and may struggle to generalize well across different datasets. To address these limitations, we propose using generative adversarial networks (GANs) for unsupervised object co-localization. Our approach uses two GANs: one generates proposals for object locations based on image features, while the other evaluates their quality by comparing them with ground truth annotations from another dataset. By optimizing both models together, our method learns to generate high-quality object bounding boxes without any supervision. We evaluate our algorithm on several benchmark datasets and demonstrate its effectiveness compared to state-of-the-art unsupervised methods. Overall, our work shows the potential of using GANs for unsupervised object co-localization and highlights promising directions for future research in this area.",1
"The visual attributes of cells, such as the nuclear morphology and chromatin openness, are critical for histopathology image analysis. By learning cell-level visual representation, we can obtain a rich mix of features that are highly reusable for various tasks, such as cell-level classification, nuclei segmentation, and cell counting. In this paper, we propose a unified generative adversarial networks architecture with a new formulation of loss to perform robust cell-level visual representation learning in an unsupervised setting. Our model is not only label-free and easily trained but also capable of cell-level unsupervised classification with interpretable visualization, which achieves promising results in the unsupervised classification of bone marrow cellular components. Based on the proposed cell-level visual representation learning, we further develop a pipeline that exploits the varieties of cellular elements to perform histopathology image classification, the advantages of which are demonstrated on bone marrow datasets.",0
"This research aims to develop an unsupervised learning method using generative adversarial networks (GAN) for generating cell-level visual representations from histopathological images. Current approaches for automating histopathological analysis rely on supervised learning methods that require large amounts of labeled data, which can be time-consuming and expensive to obtain. In contrast, our approach leverages GANs to generate realistic cell-level features without requiring any annotated training data. We evaluate our method on two publicly available datasets and demonstrate that it outperforms several state-of-the-art baseline models across multiple evaluation metrics. Our results suggest that unsupervised representation learning has great potential for improving computational pathology workflows by enabling more efficient and effective analysis of histopathological images.",1
"Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present a new large-scale database ""Multi-Human Parsing (MHP)"" for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.",0
"This work presents a new method called deep nested adversarial learning that addresses several problems in multi-human parsing, which refers to predicting human keypoints on a per-instance level when multiple humans are present in a scene. Our approach uses two levels of regularization based on image-level quality metrics related to the shape of the predicted boxes surrounding each person instance. Firstly, our system encourages high likelihoods for all ground truth human boxes given their corresponding detections using Gaussian mixture density estimation. Secondly, we train an ensemble classifier based on a novel benchmark dataset generated from annotations provided by Amazon Mechanical Turk workers. We evaluate performance qualitatively through the use of examples and quantitatively via pixel error on both synthetic data and real images. Compared against three other approaches, our technique outperforms them in all respects.",1
"Fine-grained image search is still a challenging problem due to the difficulty in capturing subtle differences regardless of pose variations of objects from fine-grained categories. In practice, a dynamic inventory with new fine-grained categories adds another dimension to this challenge. In this work, we propose an end-to-end network, called FGGAN, that learns discriminative representations by implicitly learning a geometric transformation from multi-view images for fine-grained image search. We integrate a generative adversarial network (GAN) that can automatically handle complex view and pose variations by converting them to a canonical view without any predefined transformations. Moreover, in an open-set scenario, our network is able to better match images from unseen and unknown fine-grained categories. Extensive experiments on two public datasets and a newly collected dataset have demonstrated the outstanding robust performance of the proposed FGGAN in both closed-set and open-set scenarios, providing as much as 10% relative improvement compared to baselines.",0
"Adversarial learning has proven to be very effective at improving image search accuracy by training neural networks on both real images and generated ""adversarial"" examples designed specifically to make them fail. As such, we propose using a similar methodology to fine-grained image retrieval tasks like scene classification and object detection. Our method involves generating new dataset augmentation techniques that produce semantically meaningful but visually subtle variations on existing images. We then train models to recognize these ""fake"" examples while still accurately identifying their original versions. Through comprehensive evaluation experiments, we show significant improvements across multiple datasets and architectures, demonstrating adversarial learning as a powerful tool for enhancing fine-grained image retrieval performance.",1
"An important problem in geostatistics is to build models of the subsurface of the Earth given physical measurements at sparse spatial locations. Typically, this is done using spatial interpolation methods or by reproducing patterns from a reference image. However, these algorithms fail to produce realistic patterns and do not exhibit the wide range of uncertainty inherent in the prediction of geology. In this paper, we show how semantic inpainting with Generative Adversarial Networks can be used to generate varied realizations of geology which honor physical measurements while matching the expected geological patterns. In contrast to other algorithms, our method scales well with the number of data points and mimics a distribution of patterns as opposed to a single pattern or image. The generated conditional samples are state of the art.",0
"This papers presents a novel application of generative adversarial networks (GANs) for generating realistic geological models conditioned on physical measurements. GANs are powerful deep learning algorithms that have been successfully applied to image generation tasks, but they have not yet been used extensively for creating geologic models. In this work we demonstrate how GANs can be adapted to generate highly accurate geologic simulations based on seismic data and other relevant inputs such as well logs and core samples. By using these inputs as constraints during training, our approach enables us to create detailed and physically plausible geologic models that capture key features such as faults, fractures, and stratigraphy. Our results show that our method outperforms state-of-the art methods for geologic simulation, and has significant implications for the oil and gas industry where fast and accurate modeling of subsurface formations is critical. Additionally, we investigate several techniques for improving convergence speed and stability of the GAN training process, which will likely benefit future applications of GANs in scientific computing. Finally, we discuss potential extensions of this work towards integration with other reservoir characterization technologies such as geostatistics, upscaling methods, and stochastic simulation, that could provide even more comprehensive representations of the subsurface. Overall, this paper represents an important advance towards enabling computational geoscience researchers to leverage machine learning approaches and GANs specifically, for solving problems related to earth structure and processes at scales ranging from laboratory observations up to global plate tectonic phenomena.",1
"Convolutional neural networks have been successfully applied to semantic segmentation problems. However, there are many problems that are inherently not pixel-wise classification problems but are nevertheless frequently formulated as semantic segmentation. This ill-posed formulation consequently necessitates hand-crafted scenario-specific and computationally expensive post-processing methods to convert the per pixel probability maps to final desired outputs. Generative adversarial networks (GANs) can be used to make the semantic segmentation network output to be more realistic or better structure-preserving, decreasing the dependency on potentially complex post-processing. In this work, we propose EL-GAN: a GAN framework to mitigate the discussed problem using an embedding loss. With EL-GAN, we discriminate based on learned embeddings of both the labels and the prediction at the same time. This results in more stable training due to having better discriminative information, benefiting from seeing both `fake' and `real' predictions at the same time. This substantially stabilizes the adversarial training process. We use the TuSimple lane marking challenge to demonstrate that with our proposed framework it is viable to overcome the inherent anomalies of posing it as a semantic segmentation problem. Not only is the output considerably more similar to the labels when compared to conventional methods, the subsequent post-processing is also simpler and crosses the competitive 96% accuracy threshold.",0
"In recent years, advancements in machine learning have made significant contributions towards improving lane detection accuracy on road networks. Among these approaches, generative adversarial networks (GANs) have proven to be particularly effective due to their ability to generate synthetic training data that can improve model performance. However, GANs suffer from some shortcomings such as instability during training and difficulty in controlling the quality of generated samples. To address these issues, we propose a new method called Embedded Loss Driven Generative Adversarial Networks (EL-GAN), which combines embedding loss with traditional GAN objectives to achieve more stable training and higher quality sample generation. Our approach uses pre-trained models to embed objects into latent spaces and then trains discriminators to classify real images against the generated ones using both image content and latent space distances. We evaluate our approach on several challenging datasets and show that EL-GAN outperforms state-of-the-art methods by achieving high accuracy while maintaining robustness to changes in camera settings, lighting conditions, weather patterns, etc. Overall, our work demonstrates how leveraging pre-trained models can enhance GAN stability and efficacy, leading to improved lane detection accuracy on complex road scenarios.",1
"In this work, we present an interesting attempt on mixture generation: absorbing different image concepts (e.g., content and style) from different domains and thus generating a new domain with learned concepts. In particular, we propose a mixture generative adversarial network (MIXGAN). MIXGAN learns concepts of content and style from two domains respectively, and thus can join them for mixture generation in a new domain, i.e., generating images with content from one domain and style from another. MIXGAN overcomes the limitation of current GAN-based models which either generate new images in the same domain as they observed in training stage, or require off-the-shelf content templates for transferring or translation. Extensive experimental results demonstrate the effectiveness of MIXGAN as compared to related state-of-the-art GAN-based models.",0
"This paper presents MIXGAN, a generative adversarial network (GAN) that can learn concepts from different domains for mixture generation. GANs have shown promising results in generating high-quality images, but current approaches suffer from several limitations such as mode collapse, instability during training, lack of control over generated content, and limited transfer across datasets. To address these issues, we propose a novel method for training GANs by incorporating domain knowledge into their learning process. Our approach uses multiple discriminators trained on complementary tasks, each with a dedicated set of concept codes that are used to guide the generator towards specific objectives. We demonstrate the effectiveness of our proposed method on three benchmark datasets, achieving superior performance compared to state-of-the-art methods in terms of both quantitative metrics and visual inspection. Overall, our work represents an important step forward in the development of GANs capable of generating complex, diverse, controllable, and coherent mixtures across different domains.",1
"We propose a method to train generative adversarial networks on mutivariate feature vectors representing multiple categorical values. In contrast to the continuous domain, where GAN-based methods have delivered considerable results, GANs struggle to perform equally well on discrete data. We propose and compare several architectures based on multiple (Gumbel) softmax output layers taking into account the structure of the data. We evaluate the performance of our architecture on datasets with different sparsity, number of features, ranges of categorical values, and dependencies among the features. Our proposed architecture and method outperforms existing models.",0
"In recent years, Generative Adversarial Networks (GANs) have emerged as a powerful tool for generating samples from complex distributions across various domains such as images, videos, audios, and texts. However, most GAN models focus on generating single categorical samples that belong to one specific category. This work presents a novel approach for extending the capabilities of GANs by introducing multi-categorical sampling, where instead of generating just one sample at a time, multiple diverse samples belonging to different categories can be generated simultaneously. Our proposed method utilizes a hybrid architecture composed of two sub-networks: (a) a generative network which maps noise inputs to multiple output spaces corresponding to each targeted category; and (b) a discriminator network that learns a joint distribution over all these outputs. By enforcing adversarial training on both local task losses and global generation objectives, our model effectively captures multi-category knowledge sharing and allows learning more accurate mapping functions within each individual generator subspace. We evaluate our model using several challenging benchmark datasets, demonstrating improved performance compared to state-of-the-art baseline methods and showcasing promising potential applications in multimedia synthesis tasks involving multiple modalities. ----------------------------------------------",1
"We propose a novel algorithm, namely Resembled Generative Adversarial Networks (GAN), that generates two different domain data simultaneously where they resemble each other. Although recent GAN algorithms achieve the great success in learning the cross-domain relationship, their application is limited to domain transfers, which requires the input image. The first attempt to tackle the data generation of two domains was proposed by CoGAN. However, their solution is inherently vulnerable for various levels of domain similarities. Unlike CoGAN, our Resembled GAN implicitly induces two generators to match feature covariance from both domains, thus leading to share semantic attributes. Hence, we effectively handle a wide range of structural and semantic similarities between various two domains. Based on experimental analysis on various datasets, we verify that the proposed algorithm is effective for generating two domains with similar attributes.",0
"Recent work has proposed resembling generative adversarial networks (ResGAN) as an effective method of synthesizing realistic images from textual descriptions. In this paper, we evaluate the performance of ResGAN in two domains that share similar attributes: human faces and natural scenes. We demonstrate that ResGAN can effectively generate high quality images in both domains while significantly reducing mode collapse compared to traditional GAN architectures. Our results show that ResGAN is capable of learning more robust representations by leveraging hierarchical features in addition to low level details present in the input data. These findings have important implications for future research on image generation using deep neural networks, particularly those involving transfer learning across domains. We conclude by discussing potential applications of our approach including virtual reality environments and medical imaging analysis. Overall, this study provides evidence that ResGAN holds great promise for generating high fidelity visualizations in various domains while minimizing the risk of overfitting due to limited training data.",1
"Despite the success of generative adversarial networks (GANs) for image generation, the trade-off between visual quality and image diversity remains a significant issue. This paper achieves both aims simultaneously by improving the stability of training GANs. The key idea of the proposed approach is to implicitly regularize the discriminator using representative features. Focusing on the fact that standard GAN minimizes reverse Kullback-Leibler (KL) divergence, we transfer the representative feature, which is extracted from the data distribution using a pre-trained autoencoder (AE), to the discriminator of standard GANs. Because the AE learns to minimize forward KL divergence, our GAN training with representative features is influenced by both reverse and forward KL divergence. Consequently, the proposed approach is verified to improve visual quality and diversity of state of the art GANs using extensive evaluations.",0
"This paper presents a new method for training generative adversarial networks (GANs) using representative features. GANs are powerful neural network architectures that can generate highly realistic images, but they suffer from several limitations, such as mode collapse and poor convergence. These issues arise because traditional loss functions used to train GANs may lead to overfitting or underfitting, resulting in low quality outputs. Our proposed method addresses these problems by introducing representative features into the training process. We demonstrate how our approach significantly improves the performance of GANs on a variety of benchmark datasets and applications. By incorporating representative features, we achieve better stability, fewer artifacts, and higher fidelity results compared to state-of-the-art methods. Our work has important implications for computer vision and graphics researchers interested in developing advanced generative models.",1
"Generative adversarial models are powerful tools to model structure in complex distributions for a variety of tasks. Current techniques for learning generative models require an access to samples which have high quality, and advanced generative models are applied to generate samples from noisy training data through ambient modules. However, the modules are only practical for the output space of the generator, and their application in the hidden space is not well studied. In this paper, we extend the ambient module to the hidden space of the generator, and provide the uniqueness condition and the corresponding strategy for the ambient hidden generator in the adversarial training process. We report the practicality of the proposed method on the benchmark dataset.",0
"This paper explores the ambient hidden space (AHS) of generative adversarial networks (GANs). GANs consist of two neural networks that compete against each other: a generator network creates samples such as images, audio clips, etc., while a discriminator network evaluates their quality and authenticity. As GANs ""learn,"" they become better at generating more realistic content, but this improvement occurs mainly within their latent spaces or so-called ""generator spaces."" However, there exists another latent space called AHS which captures many subtle features present in natural datasets but can barely be identified by naive sampling methods such as minibatch Gaussian noise. Samples generated from the AHS capture intricate details found in nature without relying on any explicit signal from the generator network. Our study shows that manipulating the parameters in the ambient hidden space leads to significant changes in the generated outputs across multiple architectures and datasets. Therefore, understanding how different aspects of the generator interact with the AHS could provide valuable insights into enhancing GAN performance, generalizing to new domains, and improving the robustness and stability of training procedures.",1
"Recent research has demonstrated the brittleness of machine learning systems to adversarial perturbations. However, the studies have been mostly limited to perturbations on images and more generally, classification that does not deal with temporally varying inputs. In this paper we ask ""Are adversarial perturbations possible in real-time video classification systems and if so, what properties must they satisfy?"" Such systems find application in surveillance applications, smart vehicles, and smart elderly care and thus, misclassification could be particularly harmful (e.g., a mishap at an elderly care facility may be missed). We show that accounting for temporal structure is key to generating adversarial examples in such systems. We exploit recent advances in generative adversarial network (GAN) architectures to account for temporal correlations and generate adversarial samples that can cause misclassification rates of over 80% for targeted activities. More importantly, the samples also leave other activities largely unaffected making them extremely stealthy. Finally, we also surprisingly find that in many scenarios, the same perturbation can be applied to every frame in a video clip that makes the adversary's ability to achieve misclassification relatively easy.",0
"This paper presents a study on adversarial perturbations against real-time video classification systems. With the increasing prevalence of deep learning technology in security applications such as surveillance cameras, understanding the vulnerabilities of these systems to deliberately crafted inputs has become crucial. In particular, we investigate how small, imperceptible changes to videos can cause state-of-the-art object detection algorithms to make incorrect predictions. Our findings suggest that attacking objects other than those intended by the attacker results in less effective attacks, while targeted attacks achieve high fooling rates even with smaller disturbances. Additionally, our experiments reveal that different architectures respond differently to different types of noise, highlighting the need for more robust models that are resilient to diverse forms of tampering. Ultimately, our work serves as a call for further research into defending against adversarial examples in video recognition tasks.",1
"We introduce SalGAN, a deep convolutional neural network for visual saliency prediction trained with adversarial examples. The first stage of the network consists of a generator model whose weights are learned by back-propagation computed from a binary cross entropy (BCE) loss over downsampled versions of the saliency maps. The resulting prediction is processed by a discriminator network trained to solve a binary classification task between the saliency maps generated by the generative stage and the ground truth ones. Our experiments show how adversarial training allows reaching state-of-the-art performance across different metrics when combined with a widely-used loss function like BCE. Our results can be reproduced with the source code and trained models available at https://imatge-upc.github.io/saliency-salgan-2017/.",0
"In recent years, there has been growing interest in understanding visual saliency prediction, which refers to predicting human attention patterns based on image features. While traditional methods have relied heavily on hand-engineered features, such as color histograms and edge detectors, deep learning approaches have shown promising results by leveraging large amounts of data. However, these models often suffer from limitations such as poor interpretability and lack of robustness against input variations.  To address these issues, we propose SalGAN (Saliency GAN), a novel generative adversarial network that utilizes both discriminator and generator components to improve predictions of saliency maps. Our approach combines generative models with established visual saliency techniques, allowing us to produce more realistic looking images while capturing important high level concepts. We demonstrate through extensive experiments on several benchmark datasets that our method outperforms state-of-the-art baseline models in terms of accuracy, diversity, and robustness against input perturbations. Additionally, we analyze model interpretability via attribution maps and show how they relate to traditional bottom-up attention theories. Overall, our work represents a significant step towards reliable automatic visual saliency prediction systems for use cases like gaze tracking, object detection, and content recommendation.",1
"Generative adversarial networks (GANs) are a novel approach to generative modelling, a task whose goal it is to learn a distribution of real data points. They have often proved difficult to train: GANs are unlike many techniques in machine learning, in that they are best described as a two-player game between a discriminator and generator. This has yielded both unreliability in the training process, and a general lack of understanding as to how GANs converge, and if so, to what. The purpose of this dissertation is to provide an account of the theory of GANs suitable for the mathematician, highlighting both positive and negative results. This involves identifying the problems when training GANs, and how topological and game-theoretic perspectives of GANs have contributed to our understanding and improved our techniques in recent years.",0
"Abstract: This paper explores the convergence problems that can occur during training of generative adversarial networks (GANs). GANs have emerged as powerful models for generating high quality synthetic data by learning from real data examples. However, they suffer from several issues such as instability, mode collapse, and difficulty in controlling the generated output. These challenges arise due to the nature of the adversarial game played between the generator network and discriminator network. Convergence becomes difficult because both networks try to optimize their objectives simultaneously, leading to oscillations and vanishing gradients. In practice, it is observed that many GAN variants use tricks or heuristics to overcome these problems rather than fundamental solutions. Therefore, this study examines why these convergence difficulties occur within each stage of GAN training. By understanding the root causes, we propose novel architectures and techniques aimed at ensuring stable convergence even under varying datasets and hyperparameters. Our proposed methods are evaluated on multiple benchmark tasks using comprehensive experiments, demonstrating improved stability, better modes coverage, and reduced uncertainty compared to state-of-the-art approaches. Our work sheds light on resolving convergence obstacles in GANs, opening up new possibilities for their usage across different domains.",1
"Synthesizing images or texts automatically is a useful research area in the artificial intelligence nowadays. Generative adversarial networks (GANs), which are proposed by Goodfellow in 2014, make this task to be done more efficiently by using deep neural networks. We consider generating corresponding images from an input text description using a GAN. In this paper, we analyze the GAN-CLS algorithm, which is a kind of advanced method of GAN proposed by Scott Reed in 2016. First, we find the problem with this algorithm through inference. Then we correct the GAN-CLS algorithm according to the inference by modifying the objective function of the model. Finally, we do the experiments on the Oxford-102 dataset and the CUB dataset. As a result, our modified algorithm can generate images which are more plausible than the GAN-CLS algorithm in some cases. Also, some of the generated images match the input texts better.",0
"Title: Generating Images from Text Descriptions Using a Modified GAN-based CLS Approach --------------------------------------------------------------------------Abstract: This research focuses on generating images directly from text descriptions using a modified version of the CycleGAN framework. Previous studies have shown that using deep neural networks can accurately predict image data given enough time and compute resources. By leveraging recent advances in GANs (Generative Adversarial Networks), we introduce a new approach called Modified GAN Contrastive Learning System (MGS-CLS) which learns representations by maximizing the similarity between generated images and real images while minimizing the similarity between generated samples and real samples in a batch. Experiments show that our model outperforms previous state-of-the-art models on several benchmark datasets such as MSCOCO, Flickr8k and Fashion-MNIST. Additionally, MGS-CLS provides better visual fidelity and semantic coherence compared to other methods. Our work has important implications for computer vision applications such as automatic image generation, captioning systems, and image classification tasks where textual inputs may play a key role. Overall, this study demonstrates the feasibility and potential benefits of utilizing generative models to bridge the gap between natural language and multimedia content.",1
"Deep latent variable models, trained using variational autoencoders or generative adversarial networks, are now a key technique for representation learning of continuous structures. However, applying similar methods to discrete structures, such as text sequences or discretized images, has proven to be more challenging. In this work, we propose a flexible method for training deep latent variable models of discrete structures. Our approach is based on the recently-proposed Wasserstein autoencoder (WAE) which formalizes the adversarial autoencoder (AAE) as an optimal transport problem. We first extend this framework to model discrete sequences, and then further explore different learned priors targeting a controllable representation. This adversarially regularized autoencoder (ARAE) allows us to generate natural textual outputs as well as perform manipulations in the latent space to induce change in the output space. Finally we show that the latent representation can be trained to perform unaligned textual style transfer, giving improvements both in automatic/human evaluation compared to existing methods.",0
"In recent years, autoencoders have emerged as powerful tools for unsupervised learning tasks such as dimensionality reduction, anomaly detection, and data generation. However, their training process can often lead to overfitting, resulting in poor generalization performance on novel data. To address this issue, we propose adversarially regularized autoencoders (ARA), which incorporate adversarial training into the autoencoding framework. We demonstrate that our approach significantly improves the robustness and stability of learned representations under distributional shifts, leading to improved downstream task performance compared to state-of-the-art methods. Additionally, we provide theoretical insights into why ARA works by analyzing the effect of adversarial regularization on the optimization landscape of autoencoder training. Our findings suggest that ARA effectively smooths out the loss surface around the optima, making the training process more resilient to initialization and hyperparameter choice. Overall, our work offers new perspectives on the design of regularizers for deep generative models and highlights promising directions for future research at the intersection of adversarial training and autoencoders.",1
"Generating multi-view images from a single-view input is an essential yet challenging problem. It has broad applications in vision, graphics, and robotics. Our study indicates that the widely-used generative adversarial network (GAN) may learn ""incomplete"" representations due to the single-pathway framework: an encoder-decoder network followed by a discriminator network. We propose CR-GAN to address this problem. In addition to the single reconstruction path, we introduce a generation sideway to maintain the completeness of the learned embedding space. The two learning pathways collaborate and compete in a parameter-sharing manner, yielding considerably improved generalization ability to ""unseen"" dataset. More importantly, the two-pathway framework makes it possible to combine both labeled and unlabeled data for self-supervised learning, which further enriches the embedding space for realistic generations. The experimental results prove that CR-GAN significantly outperforms state-of-the-art methods, especially when generating from ""unseen"" inputs in wild conditions.",0
"This is an abstract I wrote:  We propose a novel framework that learns complete representations from multi-view inputs by utilizing Cycle Consistency Constraints (CCC) along with Generative Adversarial Networks (GAN). Our method, dubbed CR-GAN, leverages two networks - one responsible for capturing latent features and another discriminator network for identifying inconsistencies. By adding CCC as an additional term to minimize, we enforce cycle consistency on top of adversarial learning. This allows us to train our generators to produce high-fidelity outputs across multiple views while being robust against input noise, outliers and missing data points commonly found in real world scenarios. We demonstrate state-of-the art performance on several challenging benchmark datasets including face generation, panorama completion, single image super resolution among others.  This framework has applications ranging from computer graphics to natural language processing to robotics. The ability of CR-GAN to generate complex and coherent multimodal output could facilitate the development of more advanced AI systems capable of human level understanding and problem solving. With future advancements in model architectures and training techniques, it may become possible to extend these capabilities even further, potentially leading to new breakthroughs in areas such as virtual reality and biomedical imaging where fine grained multi-modal representation matters most.  Overall, this work presents a significant step forward towards automating the process of generating complete, detailed and accurate cross modal signals by large margin deep neural nets trained only on unpaired sets of observations through adversarial training under regularization. These models have direct applications to fields spanning all forms of human expression in media, communication and entertainment industries as well as scientific research and other sectors like education which rely heavily on visual material. In conclusion, our contributions provide new insights into how multi-task learning can benefit from a richer formulation based on equivariant mapping principles combined with powerful adversarial minima seeking objectives to achieve better generalization beyond iid assumptions about the environment. Improving the fidelity, scale and speed of automatic content creation processes will lead to widespread impacts and benefits throughout society.",1
"Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) aiming at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of the object based on given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and discriminators in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.",0
"""The ability to generate realistic images has become increasingly important across many fields such as computer vision, graphics design, and even game development. Many techniques have been proposed in recent years, but few match the effectiveness of the StackGAN architecture presented in our work, which significantly improves upon previous GAN models for image synthesis. In this study, we introduce the StackGAN++, which features several advancements over its predecessor including refinement modules, novel discriminators designed to capture local and global structure, and adaptive batch normalization. Our model achieves state-of-the-art performance on benchmark datasets such as CelebA, LSUN Churches, and Cityscapes Driving through challenging evaluations by human raters and objective metrics. Overall, the contributions of this research aim to address limitations in current approaches and provide practitioners with improved tools for generating high quality images."" ""In this paper, we present the StackGAN++ architecture that builds upon previous work in generative adversarial networks (GANs) to improve realistic image synthesis. We describe the modifications introduced into the original StackGAN approach, such as refinement modules, new discriminator designs, and adaptive batch normalization. These improvements led to significant enhancement in image generation fidelity, as evidenced by our results on popular evaluation datasets like CelebA, LSUN Churches, and Cityscapes Driving. Both subjective assessments from human reviewers and quantitative measures demonstrate the superiority of StackGAN++ compared to contemporary methods. With these findings, we hope to offer practitioners effective tools for creating more accurate artificial visual content.""",1
"Generative modeling of high dimensional data like images is a notoriously difficult and ill-defined problem. In particular, how to evaluate a learned generative model is unclear. In this position paper, we argue that adversarial learning, pioneered with generative adversarial networks (GANs), provides an interesting framework to implicitly define more meaningful task losses for generative modeling tasks, such as for generating ""visually realistic"" images. We refer to those task losses as parametric adversarial divergences and we give two main reasons why we think parametric divergences are good learning objectives for generative modeling. Additionally, we unify the processes of choosing a good structured loss (in structured prediction) and choosing a discriminator architecture (in generative modeling) using statistical decision theory; we are then able to formalize and quantify the intuition that ""weaker"" losses are easier to learn from, in a specific setting. Finally, we propose two new challenging tasks to evaluate parametric and nonparametric divergences: a qualitative task of generating very high-resolution digits, and a quantitative task of learning data that satisfies high-level algebraic constraints. We use two common divergences to train a generator and show that the parametric divergence outperforms the nonparametric divergence on both the qualitative and the quantitative task.",0
"In recent years, generative models have achieved impressive results across a wide range of tasks, including image generation, language modeling, and speech synthesis. However, training these models can be challenging due to their complex nature and sensitivity to hyperparameters. To overcome these difficulties, researchers have developed task losses that guide the training process by providing explicit objectives beyond likelihood maximization.  One such class of loss functions that has gained popularity recently is parametric adversarial divergences (PADs). PADs are based on adversarial networks and use discriminators trained specifically for each individual instance at every iteration during training. By doing so, they provide efficient and powerful guidance that leads to significantly better performance compared to other state-of-the-art methods. Furthermore, they exhibit appealing theoretical properties that make them more interpretable than traditional approaches.  Our work investigates the effectiveness of PADs as task losses for generative modeling. We conduct extensive experiments on a variety of benchmark datasets and show that PADs outperform several competitive alternatives, including variational autoencoders (VAEs) and generative adversarial networks (GANs), in terms of both quantitative evaluation metrics and qualitative visual inspection. Our analysis reveals interesting insights into the behavior of PADs, highlighting their strengths and limitations relative to existing techniques. Additionally, we provide detailed ablation studies that demonstrate how different design choices affect the performance of PADs.  In summary, our findings suggest that PADs are highly effective task losses for generative modeling, offering significant improvements over established methods. While future work remains necessary to fully uncover their potential, our study provides valuable contributions towards understanding the mechanisms underlying successful generative modeling using task losses.",1
"Bayesian neural networks (BNNs) allow us to reason about uncertainty in a principled way. Stochastic Gradient Langevin Dynamics (SGLD) enables efficient BNN learning by drawing samples from the BNN posterior using mini-batches. However, SGLD and its extensions require storage of many copies of the model parameters, a potentially prohibitive cost, especially for large neural networks. We propose a framework, Adversarial Posterior Distillation, to distill the SGLD samples using a Generative Adversarial Network (GAN). At test-time, samples are generated by the GAN. We show that this distillation framework incurs no loss in performance on recent BNN applications including anomaly detection, active learning, and defense against adversarial attacks. By construction, our framework not only distills the Bayesian predictive distribution, but the posterior itself. This allows one to compute quantities such as the approximate model variance, which is useful in downstream tasks. To our knowledge, these are the first results applying MCMC-based BNNs to the aforementioned downstream applications.",0
"""Distilling a large ensemble of neural networks into one smaller, more compact model has been shown to improve performance by making better use of data while reducing computational cost. In recent work, we proposed adversarial distillation as a way to train a single model that captures as much of the diversity of predictions from the original models as possible. This allows us to fit more complex posterior distributions, which can lead to improved calibration and out-of-sample performance. We extend these ideas here to distill ensembles of Bayesian neural network posteriors. Bayesian methods offer natural ways to capture uncertainty but typically require more computation than non-Bayesian methods like deep learning; distilling such models could further reduce costs while retaining accuracy benefits. Our new method combines the standard adversarial approach with techniques borrowed from variational inference, and produces posteriors over model outputs, allowing easy incorporation of expert knowledge or prior information.""  ---",1
"Attributing the pixels of an input image to a certain category is an important and well-studied problem in computer vision, with applications ranging from weakly supervised localisation to understanding hidden effects in the data. In recent years, approaches based on interpreting a previously trained neural network classifier have become the de facto state-of-the-art and are commonly used on medical as well as natural image datasets. In this paper, we discuss a limitation of these approaches which may lead to only a subset of the category specific features being detected. To address this problem we develop a novel feature attribution technique based on Wasserstein Generative Adversarial Networks (WGAN), which does not suffer from this limitation. We show that our proposed method performs substantially better than the state-of-the-art for visual attribution on a synthetic dataset and on real 3D neuroimaging data from patients with mild cognitive impairment (MCI) and Alzheimer's disease (AD). For AD patients the method produces compellingly realistic disease effect maps which are very close to the observed effects.",0
"In ""Visual Feature Attribution"" we propose a methodology for attributing specific visual features to elements within images. By employing Generative Adversarial Networks (GAN), we can generate new images which closely resemble target images, while selectively emphasizing or deemphasizing certain features. We then use these generated images as training data to build a model capable of accurately identifying relevant features within source images. Our results demonstrate that our approach outperforms traditional methods by large margins across multiple feature types and image domains. Additionally, we show how our framework may be used to optimize existing models for improved performance on downstream tasks such as object detection and classification. We conclude that our work provides important insights into the field of computer vision and has direct implications for applications in fields ranging from medicine to finance.",1
"Context enhancement is critical for night vision (NV) applications, especially for the dark night situation without any artificial lights. In this paper, we present the infrared-to-visual (IR2VI) algorithm, a novel unsupervised thermal-to-visible image translation framework based on generative adversarial networks (GANs). IR2VI is able to learn the intrinsic characteristics from VI images and integrate them into IR images. Since the existing unsupervised GAN-based image translation approaches face several challenges, such as incorrect mapping and lack of fine details, we propose a structure connection module and a region-of-interest (ROI) focal loss method to address the current limitations. Experimental results show the superiority of the IR2VI algorithm over baseline methods.",0
"Title: ""Enhanced Night Environmental Perception by Unsupervised Thermal Image Translation"" Authors: Dr. Xiaodi Huang (X), Professor Yi Liu (Y) Abstract: In recent years, there has been significant interest in developing methods for enhancing night environmental perception through advanced image processing techniques. One promising approach is thermal image translation, which involves translating raw thermal images into visible light images that can be interpreted more easily by human observers. However, existing methods rely on large amounts of labeled data and extensive manual tuning, making them difficult to deploy in real-world settings. This paper presents a novel unsupervised method for thermal image translation that requires no labeled training data and only minimal user input. Experimental results show that our method significantly outperforms state-of-the-art supervised approaches while requiring far less computational resources. We believe that our work represents an important step towards reliable and scalable solutions for enhancing night environmental perception using thermal imagery.",1
"Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT17 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN.",0
"In recent years, Generative Adversarial Networks (GANs) have been widely used in person re-identification tasks due to their ability to generate synthetic data that can alleviate domain gaps between training and testing datasets. However, current approaches only focus on generating artificial images rather than actual human subjects. To bridge this gap, we propose a novel approach based on generative adversarial networks called ""Person Transfer GAN"" which generates realistic representations of individuals across different camera views and illumination conditions. This method allows us to generate new image pairs from existing ones by transferring the appearance features of one subject into another environment. Experimental results demonstrate that our proposed approach significantly improves person re-identification performance compared to state-of-the-art methods.",1
"""Which Generative Adversarial Networks (GANs) generates the most plausible images?"" has been a frequently asked question among researchers. To address this problem, we first propose an \emph{incomplete} U-statistics estimate of maximum mean discrepancy $\mathrm{MMD}_{inc}$ to measure the distribution discrepancy between generated and real images. $\mathrm{MMD}_{inc}$ enjoys the advantages of asymptotic normality, computation efficiency, and model agnosticity. We then propose a GANs analysis framework to select and test the ""best"" member in GANs family using the Post Selection Inference (PSI) with $\mathrm{MMD}_{inc}$. In the experiments, we adopt the proposed framework on 7 GANs variants and compare their $\mathrm{MMD}_{inc}$ scores.",0
"This paper proposes a new method for evaluating generative adversarial networks (GANs) based on post selection inference. The proposed framework involves selecting the best GAN architecture from a set of pretrained models using statistical testing methods. The authors then evaluate the selected model against other state-of-the-art GAN architectures in terms of generated image quality, stability, training time, and diversity. Results show that the proposed method can effectively identify high-performing GAN architectures and outperforms existing evaluation metrics. Implications for future research in GAN design and deployment are discussed. Overall, the paper presents a valuable contribution to the field of deep learning and computer vision, offering new insights into the performance evaluation of GANs.",1
"Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space. The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training. In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training. Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.",0
"GANs (Generative Adversarial Networks) have been successful at generating realistic data samples but remain difficult to train due to instability issues such as mode collapse and divergence. In our work we explore using multiple random projections to stabilize training and improve stability, performance, and sample quality. We find that adding more random projections helps reduce these issues and improves results under certain conditions on popular benchmark datasets including MNIST, FashionMNIST, CelebA, and SVHN. We analyze and compare several variations based on projection strength, regularization techniques, and architectures for both generator and discriminator components and provide extensive experimental evaluations across all benchmark datasets with accompanying visualizations demonstrating improvements over baseline models without random projections. Ultimately our work contributes important insights towards better understanding how to train GANs more reliably and effectively while producing high-quality outputs.",1
"Inspired by the tremendous success of deep generative models on generating continuous data like image and audio, in the most recent year, few deep graph generative models have been proposed to generate discrete data such as graphs. They are typically unconditioned generative models which has no control on modes of the graphs being generated. Differently, in this paper, we are interested in a new problem named \emph{Deep Graph Translation}: given an input graph, we want to infer a target graph based on their underlying (both global and local) translation mapping. Graph translation could be highly desirable in many applications such as disaster management and rare event forecasting, where the rare and abnormal graph patterns (e.g., traffic congestions and terrorism events) will be inferred prior to their occurrence even without historical data on the abnormal patterns for this graph (e.g., a road network or human contact network). To achieve this, we propose a novel Graph-Translation-Generative Adversarial Networks (GT-GAN) which will generate a graph translator from input to target graphs. GT-GAN consists of a graph translator where we propose new graph convolution and deconvolution layers to learn the global and local translation mapping. A new conditional graph discriminator has also been proposed to classify target graphs by conditioning on input graphs. Extensive experiments on multiple synthetic and real-world datasets demonstrate the effectiveness and scalability of the proposed GT-GAN.",0
"Informational Abstract: Deep Graph Translation is a new method that allows natural language processing (NLP) models to translate text into graphs, which can then be used for tasks such as question answering and summarization. Unlike traditional NLP methods, which rely on sequential input like sentences or paragraphs, deep graph translation creates a single unified representation from multiple pieces of text by identifying relationships between them. This allows for better understanding of context and meaning across texts, ultimately leading to more accurate translations and improved performance on downstream tasks. By leveraging advances in graph neural networks and attention mechanisms, deep graph translation achieves state-of-the-art results on benchmark datasets without requiring large amounts of data or computational resources. Overall, this work represents a significant step forward in NLP, enabling greater flexibility and scalability in handling complex linguistic phenomena and bridging the gap between structured and unstructured data.",1
"Person re-identification (Re-ID) aims to match the image frames which contain the same person in the surveillance videos. Most of the Re-ID algorithms conduct supervised training in some small labeled datasets, so directly deploying these trained models to the real-world large camera networks may lead to a poor performance due to underfitting. The significant difference between the source training dataset and the target testing dataset makes it challenging to incrementally optimize the model. To address this challenge, we propose a novel solution by transforming the unlabeled images in the target domain to fit the original classifier by using our proposed similarity preserved generative adversarial networks model, SimPGAN. Specifically, SimPGAN adopts the generative adversarial networks with the cycle consistency constraint to transform the unlabeled images in the target domain to the style of the source domain. Meanwhile, SimPGAN uses the similarity consistency loss, which is measured by a siamese deep convolutional neural network, to preserve the similarity of the transformed images of the same person. Comprehensive experiments based on multiple real surveillance datasets are conducted, and the results show that our algorithm is better than the state-of-the-art cross-dataset unsupervised person Re-ID algorithms.",0
"This paper presents a novel approach for cross-dataset person re-identification using similarity preserved generative adversarial networks (GANs). In traditional GANs, two separate subnetworks compete against each other: one generates synthetic data that resembles real data, while another discriminates between the generated data and real data. However, these models often struggle to preserve important characteristics required for downstream tasks such as person re-identification. Our method addresses this challenge by training a single network that performs both generation and classification in parallel. We show that our model outperforms state-of-the art methods on several benchmark datasets, demonstrating its effectiveness at improving performance across different datasets without sacrificing accuracy. Further analysis shows that the learned features from our generator capture meaningful representations suitable for re-id tasks. Overall, our work provides a significant contribution to research in person re-identification and generative models, with potential applications in surveillance, security, and social media platforms.",1
"In this paper, we propose a method that disentangles the effects of multiple input conditions in Generative Adversarial Networks (GANs). In particular, we demonstrate our method in controlling color, texture, and shape of a generated garment image for computer-aided fashion design. To disentangle the effect of input attributes, we customize conditional GANs with consistency loss functions. In our experiments, we tune one input at a time and show that we can guide our network to generate novel and realistic images of clothing articles. In addition, we present a fashion design process that estimates the input attributes of an existing garment and modifies them using our generator.",0
"Advances in generative adversarial networks (GANs) have enabled the generation of synthetic data which closely mimics real world data sets. However, these models often rely on multiple conditional inputs that can confuse or complicate their use cases. In this work we propose a novel framework utilizing disentanglement theory to separate competing influences inherent in multi-condition GAN architectures. We show how this allows for better interpretability and control of generated outputs by selectively adjusting individual input factors rather than resorting to complex weight scaling or manual construction of new models. Our experiments demonstrate improved performance over current methods across multiple domains including images, audio, and text. While our approach primarily focuses on deep neural network based models, many aspects can be adapted to other generative models such as variational autoencoders or diffusion models. With further research into model evaluation techniques and interpretation, applications of such technology may become ubiquitous throughout science and industry where high quality artificial data needs to be produced quickly at scale.",1
"In medical imaging, a general problem is that it is costly and time consuming to collect high quality data from healthy and diseased subjects. Generative adversarial networks (GANs) is a deep learning method that has been developed for synthesizing data. GANs can thereby be used to generate more realistic training data, to improve classification performance of machine learning algorithms. Another application of GANs is image-to-image translations, e.g. generating magnetic resonance (MR) images from computed tomography (CT) images, which can be used to obtain multimodal datasets from a single modality. Here, we evaluate two unsupervised GAN models (CycleGAN and UNIT) for image-to-image translation of T1- and T2-weighted MR images, by comparing generated synthetic MR images to ground truth images. We also evaluate two supervised models; a modification of CycleGAN and a pure generator model. A small perceptual study was also performed to evaluate how visually realistic the synthesized images are. It is shown that the implemented GAN models can synthesize visually realistic MR images (incorrectly labeled as real by a human). It is also shown that models producing more visually realistic synthetic images not necessarily have better quantitative error measurements, when compared to ground truth data. Code is available at https://github.com/simontomaskarlsson/GAN-MRI",0
"This is a good chance for you to practice writing scientific abstracts! This study compared two different generative adversarial networks (GANs) used for image-to-image translation on multi-contrast magnetic resonance imaging (MRI): CycleGAN and UNIT GAN. Both types of GANs have been shown to effectively generate high quality synthetic images from real MRI scans, which can be useful for medical researchers and clinicians alike. However, there has been little comparison made between these two specific models.  The authors conducted experiments using publicly available data sets of breast and knee MRIs acquired at varying contrast levels such as T1-weighted and fat suppressed T2 weighted. The investigators trained each model separately on these datasets, then evaluated their performance by comparing their output with manual expert annotation. Additionally, the authors utilized user studies with radiologists to evaluate the diagnostic utility of both translated images.  Results showed that while both models produced similar results overall, UNIT GAN outperformed CycleGAN on several metrics. Specifically, UNIT GAN was better able to preserve small features important for diagnosis, particularly bone marrow lesions. The authors suggest that improvements could still be made to GAN models, especially since current limitations may negatively impact patient care. Overall, this work provides insights into how well GANs perform on complex medical tasks, and opens up new possibilities for future research into synthesizing images directly relevant for clinical needs.",1
"In this article, we propose an approach that can make use of not only labeled EEG signals but also the unlabeled ones which is more accessible. We also suggest the use of data fusion to further improve the seizure prediction accuracy. Data fusion in our vision includes EEG signals, cardiogram signals, body temperature and time. We use the short-time Fourier transform on 28-s EEG windows as a pre-processing step. A generative adversarial network (GAN) is trained in an unsupervised manner where information of seizure onset is disregarded. The trained Discriminator of the GAN is then used as feature extractor. Features generated by the feature extractor are classified by two fully-connected layers (can be replaced by any classifier) for the labeled EEG signals. This semi-supervised seizure prediction method achieves area under the operating characteristic curve (AUC) of 77.68% and 75.47% for the CHBMIT scalp EEG dataset and the Freiburg Hospital intracranial EEG dataset, respectively. Unsupervised training without the need of labeling is important because not only it can be performed in real-time during EEG signal recording, but also it does not require feature engineering effort for each patient.",0
"In recent years, semi-supervised learning has emerged as a promising approach for many applications including seizure prediction, but few studies have explored this topic in depth. In our work, we aimed at building a model that predicts epileptic seizures by harnessing both labeled and unlabeled data using generative adversarial networks (GAN). Our proposed method achieved state-of-the-art results on two publicly available datasets, outperforming other published approaches in accuracy and area under the receiver operating characteristic curve. Our findings demonstrate the effectiveness of combining GANs and semi-supervised learning for seizure prediction, opening new possibilities in the field of neurology. We believe our work could serve as a stepping stone towards better understanding of complex neurological disorders such as epilepsy.",1
"Generative adversarial networks (GANs) are pow- erful generative models based on providing feed- back to a generative network via a discriminator network. However, the discriminator usually as- sesses individual samples. This prevents the dis- criminator from accessing global distributional statistics of generated samples, and often leads to mode dropping: the generator models only part of the target distribution. We propose to feed the discriminator with mixed batches of true and fake samples, and train it to predict the ratio of true samples in the batch. The latter score does not depend on the order of samples in a batch. Rather than learning this invariance, we introduce a generic permutation-invariant discriminator ar- chitecture. This architecture is provably a uni- versal approximator of all symmetric functions. Experimentally, our approach reduces mode col- lapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively.",0
"Artificial intelligence (AI) has become increasingly prevalent in our daily lives due to advances in deep learning techniques such as generative adversarial networks (GANs). However, there still exists room for improvement in the stability and performance of these models, particularly in their training phase. In this study, we propose two novel methods that address this challenge: mixed batches and symmetric discriminators. By applying both of these approaches concurrently, we observed significant improvements in model convergence time, accuracy, and visual fidelity. We believe that our findings can contribute towards accelerating future research on improving stability and performance in the field of GAN training. Ultimately, this could lead to more efficient development of artificial intelligence technologies across diverse applications domains. This work represents an important step forward in unlocking the full potential of generative models like GANs while reducing unnecessary computational resources in training procedures.",1
"Conditional generators learn the data distribution for each class in a multi-class scenario and generate samples for a specific class given the right input from the latent space. In this work, a method known as ""Versatile Auxiliary Classifier with Generative Adversarial Network"" for multi-class scenarios is presented. In this technique, the Generative Adversarial Networks (GAN)'s generator is turned into a conditional generator by placing a multi-class classifier in parallel with the discriminator network and backpropagate the classification error through the generator. This technique is versatile enough to be applied to any GAN implementation. The results on two databases and comparisons with other method are provided as well.",0
"Abstract: This research proposes a novel approach for auxiliary classifiers that leverages generative adversarial networks (GANs) to improve their performance on multi-class scenarios. By introducing a VAC+GAN framework, we aim to enhance existing auxiliary classification methods by making use of unlabeled data through generative models. Our method involves training two neural networks simultaneously - one acts as the main classification model while the other generates synthetic samples resembling the target distribution. With regularization from the GAN module, our experimental results show significant improvements over traditional classifiers across different datasets, including computer vision tasks such as image recognition and sentiment analysis. Our proposed method offers an alternative solution for handling imbalanced classes in multiclass problems by using unlabeled data effectively. The findings contribute towards advancing the field of machine learning with improved accuracy and efficiency, particularly for challenging real-world applications where labels are scarce.",1
"Recent work (Pennington et al, 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training. Moreover, we find that the average (with z from p(z)) conditioning of the generator is highly predictive of two other ad-hoc metrics for measuring the 'quality' of trained GANs: the Inception Score and the Frechet Inception Distance (FID). We test the hypothesis that this relationship is causal by proposing a 'regularization' technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean Inception Score and the mean FID for GANs trained on several datasets. It also greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs.",0
"This study examines whether generator conditioning plays a role in determining the performance of generative adversarial networks (GANs). Previous research has suggested that properly conditioning the generator can improve GAN performance, but there remains limited empirical evidence supporting this claim. To address this gap, we conduct a systematic investigation into the effectiveness of different approaches to conditioning the generator in GAN training. Our findings suggest that some types of conditioning may indeed lead to improved GAN performance under certain conditions. However, our results also indicate that the relationship between generator conditioning and GAN performance is more complex than previously thought. We discuss the implications of these findings for future work on improving GAN training methods.",1
"We present a deep person re-identification approach that combines semantically selective, deep data augmentation with clustering-based network compression to generate high performance, light and fast inference networks. In particular, we propose to augment limited training data via sampling from a deep convolutional generative adversarial network (DCGAN), whose discriminator is constrained by a semantic classifier to explicitly control the domain specificity of the generation process. Thereby, we encode information in the classifier network which can be utilized to steer adversarial synthesis, and which fuels our CondenseNet ID-network training. We provide a quantitative and qualitative analysis of the approach and its variants on a number of datasets, obtaining results that outperform the state-of-the-art on the LIMA dataset for long-term monitoring in indoor living spaces.",0
"In recent years, deep learning has revolutionized the field of computer vision by achieving state-of-the-art results across various tasks such as image classification, object detection, and segmentation. However, one area that still presents significant challenges is person re-identification (ReID), where the goal is to identify individuals from disjoint camera views within a crowded scene. To address this challenge, we propose semantically selective augmentation for deep compact person re-identification. Our approach uses semantic annotations to guide data augmentation and generate more diverse training samples while ensuring high quality annotations. We demonstrate the effectiveness of our method through extensive experiments on popular benchmarks, outperforming existing methods under both full and compact model settings. These findings have important implications for the use of artificial intelligence in real-world applications such as surveillance, security, and tracking. Overall, this work advances the understanding of how semantic knowledge can improve performance in deep learning models for person ReID.",1
"There has recently been a steady increase in the number iterative approaches to density estimation. However, an accompanying burst of formal convergence guarantees has not followed; all results pay the price of heavy assumptions which are often unrealistic or hard to check. The Generative Adversarial Network (GAN) literature --- seemingly orthogonal to the aforementioned pursuit --- has had the side effect of a renewed interest in variational divergence minimisation (notably $f$-GAN). We show that by introducing a weak learning assumption (in the sense of the classical boosting framework) we are able to import some recent results from the GAN literature to develop an iterative boosted density estimation algorithm, including formal convergence results with rates, that does not suffer the shortcomings other approaches. We show that the density fit is an exponential family, and as part of our analysis obtain an improved variational characterisation of $f$-GAN.",0
"Density estimation has been playing an increasingly important role in modern data analysis across different domains such as computer vision, natural language processing (NLP), bioinformatics, etc. However, traditional density estimators often suffer from their limited scalability and applicability in high dimensional space due to computational challenges and instability issues. In recent years, boosting techniques have emerged as powerful tools that can effectively address these problems by combining weak learners into strong ones iteratively with appropriate weights. Motivated by these successful applications in classification and regression tasks, we develop a novel framework based on gradient boosting machine (GBM) for density estimation under both generative and non-generative settings. We first provide analytical justifications showing the equivalence between GBM for binary classification problems and kernel density estimation (KDE). Then, we extend our approach to multi-class scenarios through proper weight adjustment strategies and loss functions. Subsequently, we conduct extensive experiments covering several real world datasets including image patch recognition, speech recognition, natural language understanding, gene selection, etc. Both quantitative accuracy evaluations and visualization results demonstrate significant superiorities of our proposed models over existing state-of-the-art competitors. Our work provides new insights into the development of efficient and robust density estimation methods that pave the way for even better performance in high complexity applications.",1
"We propose a new technique that boosts the convergence of training generative adversarial networks. Generally, the rate of training deep models reduces severely after multiple iterations. A key reason for this phenomenon is that a deep network is expressed using a highly non-convex finite-dimensional model, and thus the parameter gets stuck in a local optimum. Because of this, methods often suffer not only from degeneration of the convergence speed but also from limitations in the representational power of the trained network. To overcome this issue, we propose an additional layer called the gradient layer to seek a descent direction in an infinite-dimensional space. Because the layer is constructed in the infinite-dimensional space, we are not restricted by the specific model structure of finite-dimensional models. As a result, we can get out of the local optima in finite-dimensional models and move towards the global optimal function more directly. In this paper, this phenomenon is explained from the functional gradient method perspective of the gradient layer. Interestingly, the optimization procedure using the gradient layer naturally constructs the deep structure of the network. Moreover, we demonstrate that this procedure can be regarded as a discretization method of the gradient flow that naturally reduces the objective function. Finally, the method is tested using several numerical experiments, which show its fast convergence.",0
"In recent years, generative models such as GANs have shown promising results in generating high quality data samples that can fool even human evaluators. However, training these models often requires significant computational resources due to their instability and lack of convergence. One approach to addressing this issue is by using gradient layers, which aim to improve the stability and speed up the convergence rate during adversarial training. This paper presents the concept of the gradient layer and discusses its effectiveness in enhancing the convergence of adversarial training for generative models. Our experimental results show that adding gradient layers leads to faster convergence rates and improved sample quality compared to traditional GAN architectures. Overall, our work represents an important contribution towards improving the efficiency and efficacy of generative model training through novel architectural components.",1
"Generative adversarial networks (GANs) aim to generate realistic data from some prior distribution (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In practice, the semantic information might be represented by some latent distribution learned from data, which, however, is hard to be used for sampling in GANs. In this paper, rather than sampling from the pre-defined prior distribution, we propose a Local Coordinate Coding (LCC) based sampling method to improve GANs. We derive a generalization bound for LCC based GANs and prove that a small dimensional input is sufficient to achieve good generalization. Extensive experiments on various real-world datasets demonstrate the effectiveness of the proposed method.",0
"Adversarial learning has become increasingly popular as a method for training generative models that can produce high quality outputs. However, one challenge facing these methods is that they often require large amounts of computational resources and may be susceptible to overfitting. In this work, we propose using local coordinate coding (LCC) as a means of addressing these issues. LCC involves encoding input data into a lower dimensional space by linearly combining basis functions centered at each point. By utilizing LCC as the underlying representation for adversarial learning, we show that it is possible to significantly reduce the amount of computation required while still producing high quality results. We evaluate our approach on a variety of tasks including image generation and style transfer, demonstrating its effectiveness in both cases. Our findings suggest that LCC provides a promising new direction for advancing the field of adversarial learning.",1
"Automatic parsing of anatomical objects in X-ray images is critical to many clinical applications in particular towards image-guided invention and workflow automation. Existing deep network models require a large amount of labeled data. However, obtaining accurate pixel-wise labeling in X-ray images relies heavily on skilled clinicians due to the large overlaps of anatomy and the complex texture patterns. On the other hand, organs in 3D CT scans preserve clearer structures as well as sharper boundaries and thus can be easily delineated. In this paper, we propose a novel model framework for learning automatic X-ray image parsing from labeled CT scans. Specifically, a Dense Image-to-Image network (DI2I) for multi-organ segmentation is first trained on X-ray like Digitally Reconstructed Radiographs (DRRs) rendered from 3D CT volumes. Then we introduce a Task Driven Generative Adversarial Network (TD-GAN) architecture to achieve simultaneous style transfer and parsing for unseen real X-ray images. TD-GAN consists of a modified cycle-GAN substructure for pixel-to-pixel translation between DRRs and X-ray images and an added module leveraging the pre-trained DI2I to enforce segmentation consistency. The TD-GAN framework is general and can be easily adapted to other learning tasks. In the numerical experiments, we validate the proposed model on 815 DRRs and 153 topograms. While the vanilla DI2I without any adaptation fails completely on segmenting the topograms, the proposed model does not require any topogram labels and is able to provide a promising average dice of 85% which achieves the same level accuracy of supervised training (88%).",0
"Here is an example of a possible abstract: This paper presents a novel method for domain adaptation based on generative models that use task-specific prior knowledge. We apply our approach to the problem of segmenting X-ray images from different hospitals, which have significant differences in protocols and image acquisition techniques. Our model leverages unpaired data from both domains, along with limited annotations, to generate synthetic training samples that bridge the gap between them. Extensive experiments demonstrate our method's effectiveness, achieving improved performance over previous state-of-the-art methods while significantly reducing annotation costs. Additionally, we show that our model generalizes well across new hospital datasets, highlighting its robustness. These findings hold great potential for adapting artificial intelligence systems to diverse medical imaging settings, ultimately enhancing their clinical impact.",1
"Data availability plays a critical role for the performance of deep learning systems. This challenge is especially acute within the medical image domain, particularly when pathologies are involved, due to two factors: 1) limited number of cases, and 2) large variations in location, scale, and appearance. In this work, we investigate whether augmenting a dataset with artificially generated lung nodules can improve the robustness of the progressive holistically nested network (P-HNN) model for pathological lung segmentation of CT scans. To achieve this goal, we develop a 3D generative adversarial network (GAN) that effectively learns lung nodule property distributions in 3D space. In order to embed the nodules within their background context, we condition the GAN based on a volume of interest whose central part containing the nodule has been erased. To further improve realism and blending with the background, we propose a novel multi-mask reconstruction loss. We train our method on over 1000 nodules from the LIDC dataset. Qualitative results demonstrate the effectiveness of our method compared to the state-of-art. We then use our GAN to generate simulated training images where nodules lie on the lung border, which are cases where the published P-HNN model struggles. Qualitative and quantitative results demonstrate that armed with these simulated images, the P-HNN model learns to better segment lung regions under these challenging situations. As a result, our system provides a promising means to help overcome the data paucity that commonly afflicts medical imaging.",0
"Abstract: This paper presents a method for generating realistic lung nodules using 3D conditional generative adversarial networks (cGANs) that improves robustness in lung segmentation tasks. Traditionally, annotating lung cancer data sets requires manual annotation by experts, which can be time-consuming and costly. To address this challenge, we propose a new approach based on cGANs to generate synthetic nodules that are virtually indistinguishable from real ones. Our model uses a combination of adversarial training and regularization techniques to ensure high fidelity in generated images, while still preserving spatial coherence across different imaging modalities. We evaluate our approach using several quantitative measures and demonstrate significant improvements over state-of-the-art methods in terms of accuracy, specificity, sensitivity, and F1 score. Overall, our results highlight the potential of utilizing deep learning approaches like cGANs for enhancing the quality of medical image datasets and advancing healthcare research.",1
"In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.",0
"This paper presents a comprehensive analysis of generative adversarial networks (GANs), focusing on their underlying mathematical principles and quantitative evaluation metrics. By examining the numerics of these deep learning models, we aim to provide a better understanding of how they work, identify potential pitfalls, and facilitate further optimization. We begin by introducing the key components of GAN architecture, highlighting the fundamental balance that exists between generator and discriminator performance. Subsequently, we explore common techniques used to enhance model stability and improve training convergence, including regularization, spectral normalization, and noise injection. Additionally, we discuss challenges associated with evaluating the quality of generated samples, outlining several standard approaches such as Inception scores and Frechet distances. Finally, we present recent advances in the field, delving into new applications of GANs beyond image synthesis, such as data augmentation and semi-supervised learning. Overall, our study sheds light on the numerical foundations of GANs, fostering greater insight into their behavior and promoting future research directions in this rapidly evolving area of machine learning.",1
"Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.",0
"Here is the first version of the abstract (249 wrods). Let me know if you would like me to make any changes!  In recent years, two major frameworks have emerged that enable learning from complex data distributions: variational autoencoders (VAEs) and generative adversarial networks (GANs). VAEs model probability densities via inference, while GANs model them via maximization of Jensenâ€“Shannon divergence or reverse Kullback-Leibler divergence. By viewing these models as instances of an underlying framework we call adversarial variational Bayesian inference, which merges density estimation with generative modelling by using adversaries for density evaluation, we unify both models into one coherent framework. This enables us to develop new variants of both models that inherit desirable properties from each other. On benchmark datasets, several of our proposed models clearly outperform current state-of-the-art approaches on both reconstruction tasks and generating novel high quality samples comparable to those produced by plain GANs. To further demonstrate their flexibility, we apply some of these models to semi-supervised learning tasks where they greatly reduce the gap between labeled and unlabeled performance compared to standard models. Overall, we believe the introduced paradigm provides a powerful toolset for dealing with real world problems involving complex high dimensional data distributions. In light of recent concerns regarding training stability and mode dropping, which affect many modern methodsâ€”including some versions of VAEs and GANsâ€”our work offers more robust solutions that can alleviate such issues and pave the way towards improved generative modelling. We eagerly anticipate future exploration within the newly proposed framework.",1
"This paper proposed a retinal image segmentation method based on conditional Generative Adversarial Network (cGAN) to segment optic disc. The proposed model consists of two successive networks: generator and discriminator. The generator learns to map information from the observing input (i.e., retinal fundus color image), to the output (i.e., binary mask). Then, the discriminator learns as a loss function to train this mapping by comparing the ground-truth and the predicted output with observing the input image as a condition.Experiments were performed on two publicly available dataset; DRISHTI GS1 and RIM-ONE. The proposed model outperformed state-of-the-art-methods by achieving around 0.96% and 0.98% of Jaccard and Dice coefficients, respectively. Moreover, an image segmentation is performed in less than a second on recent GPU.",0
"Abstract: This study presents a new approach for retinal optic disc segmentation using conditional generative adversarial networks (cGANs). The proposed method leverages cGANs to generate high quality annotated images, which can then be used as ground truth data for supervised learning algorithms. To evaluate our method, we applied it on publicly available retinal datasets, such as DRIVE and STARE. Experimental results showed that our algorithm significantly outperformed other state-of-the-art methods by achieving higher accuracy, precision, recall, and F1 score. Our work demonstrates the potential of deep learning techniques, particularly cGANs, for automated retinal image analysis and screening tasks. Future directions involve applying this technique in real-world clinical settings, where early detection of pathologies could greatly benefit from accurate optic disc segmentation models.",1
"In this work, we present an application of domain randomization and generative adversarial networks (GAN) to train a near real-time object detector for industrial electric parts, entirely in a simulated environment. Large scale availability of labelled real world data is typically rare and difficult to obtain in many industrial settings. As such here, only a few hundred of unlabelled real images are used to train a Cyclic-GAN network, in combination with various degree of domain randomization procedures. We demonstrate that this enables robust translation of synthetic images to the real world domain. We show that a combination of the original synthetic (simulation) and GAN translated images, when used for training a Mask-RCNN object detection network achieves greater than 0.95 mean average precision in detecting and classifying a collection of industrial electric parts. We evaluate the performance across different combinations of training data.",0
"In recent years, there has been significant progress in developing computer vision algorithms that can accurately detect objects in images and videos. However, training these models often requires large amounts of data collected from real-world environments, which can be expensive, time-consuming, and difficult to obtain. To address this challenge, we propose a novel approach that utilizes domain randomization and generative adversarial refinement of synthetic images. Our method involves generating diverse synthetic examples that capture the variations present in real-world scenes, allowing the object detection model to learn more effectively. We then use a discriminator network to evaluate the quality of these synthetic samples, providing feedback to improve their realism. Experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results on several benchmark datasets while requiring significantly less real-world training data. This work has important implications for enabling efficient and scalable development of advanced computer vision systems.",1
"This paper proposes a novel approach based on conditional Generative Adversarial Networks (cGAN) for breast mass segmentation in mammography. We hypothesized that the cGAN structure is well-suited to accurately outline the mass area, especially when the training data is limited. The generative network learns intrinsic features of tumors while the adversarial network enforces segmentations to be similar to the ground truth. Experiments performed on dozens of malignant tumors extracted from the public DDSM dataset and from our in-house private dataset confirm our hypothesis with very high Dice coefficient and Jaccard index (94% and 89%, respectively) outperforming the scores obtained by other state-of-the-art approaches. Furthermore, in order to detect portray significant morphological features of the segmented tumor, a specific Convolutional Neural Network (CNN) have also been designed for classifying the segmented tumor areas into four types (irregular, lobular, oval and round), which provides an overall accuracy about 72% with the DDSM dataset.",0
"""Medical image segmentation is a key task that allows clinicians to extract meaningful information from medical images such as X-ray mammograms. Automatic breast mass segmentation and shape classification can aid radiologists in making more accurate diagnoses and planning treatment plans. In this study, we propose the use of conditional generative adversarial networks (CGAN) and convolutional neural networks (CNN) to improve breast mass segmentation and shape classification accuracy. We trained our models on a large dataset of annotated X-ray mammography images and evaluated their performance using quantitative metrics such as Dice similarity coefficient and intersection over union. Our results showed significant improvements compared to existing methods, demonstrating the effectiveness of our approach. This work has important implications for advancing computer-assisted detection and diagnostics in digital breast imaging.""",1
"One of the biggest challenges in the research of generative adversarial networks (GANs) is assessing the quality of generated samples and detecting various levels of mode collapse. In this work, we construct a novel measure of performance of a GAN by comparing geometrical properties of the underlying data manifold and the generated one, which provides both qualitative and quantitative means for evaluation. Our algorithm can be applied to datasets of an arbitrary nature and is not limited to visual data. We test the obtained metric on various real-life models and datasets and demonstrate that our method provides new insights into properties of GANs.",0
"This paper presents a new method called Geometry Score (GS) for comparing generative adversarial networks (GANs). GS measures how well two GAN models have learned the underlying structure or geometry of their data distribution. Our method utilizes statistical properties of high dimensional datasets such as distances between points, cluster assignments, and density estimates to evaluate the quality of generated samples from each model. Experiment results show that our GS metric outperforms several existing evaluation methods in ranking different GAN architectures on various benchmark datasets. Additionally, we provide insights into why some metrics perform better than others by analyzing their correlation with human judgements and considering specific characteristics of certain generators. Finally, we demonstrate that GS can effectively guide hyperparameter tuning, leading to improved performance. Overall, our work provides a valuable tool for evaluating and improving GANs.",1
"High-resolution (HR) magnetic resonance images (MRI) provide detailed anatomical information important for clinical application and quantitative image analysis. However, HR MRI conventionally comes at the cost of longer scan time, smaller spatial coverage, and lower signal-to-noise ratio (SNR). Recent studies have shown that single image super-resolution (SISR), a technique to recover HR details from one single low-resolution (LR) input image, could provide high-quality image details with the help of advanced deep convolutional neural networks (CNN). However, deep neural networks consume memory heavily and run slowly, especially in 3D settings. In this paper, we propose a novel 3D neural network design, namely a multi-level densely connected super-resolution network (mDCSRN) with generative adversarial network (GAN)-guided training. The mDCSRN quickly trains and inferences and the GAN promotes realistic output hardly distinguishable from original HR images. Our results from experiments on a dataset with 1,113 subjects show that our new architecture beats other popular deep learning methods in recovering 4x resolution-downgraded im-ages and runs 6x faster.",0
"This paper presents a novel approach for MRI super-resolution that combines generative adversarial networks (GANs) with multi-level densely connected neural networks. The proposed method leverages three levels of dense connections across multiple scales within the GAN framework to achieve efficient yet accurate image reconstruction. We demonstrate the effectiveness of our method on several benchmark datasets by comparing against state-of-the-art methods, showing significant improvements in both quantitative metrics and visual quality. Our results indicate the potential of combining GANs and multi-scale neural networks for high resolution MRI imaging applications. *Note: I am unable to write a full abstract without knowing more details about the specific techniques used, evaluation protocol etc.*",1
"In recent years several architectures have been proposed to learn embodied agents complex self-awareness models. In this paper, dynamic incremental self-awareness (SA) models are proposed that allow experiences done by an agent to be modeled in a hierarchical fashion, starting from more simple situations to more structured ones. Each situation is learned from subsets of private agent perception data as a model capable to predict normal behaviors and detect abnormalities. Hierarchical SA models have been already proposed using low dimensional sensorial inputs. In this work, a hierarchical model is introduced by means of a cross-modal Generative Adversarial Networks (GANs) processing high dimensional visual data. Different levels of the GANs are detected in a self-supervised manner using GANs discriminators decision boundaries. Real experiments on semi-autonomous ground vehicles are presented.",0
"This paper presents a novel approach for developing an embodied self-awareness model using Generative Adversarial Networks (GANs). We propose a hierarchy of GANs that learn from each other, allowing for increasing levels of self-awareness as the system evolves. Our method leverages recent advances in multi-agent deep reinforcement learning, where agents cooperate and compete with one another to achieve shared goals. By applying these principles at different scales within our hierarchical framework, we enable the development of progressively more advanced forms of self-awareness. Experimental results demonstrate the effectiveness of our approach on benchmark tasks, paving the way towards greater understanding of the intricacies involved in creating intelligent agents capable of introspection and metacognition. Overall, this work represents a significant contribution towards achieving human-like intelligence in artificial systems by bridging the gap between existing models that lack self-reflection and those that require extensive hand engineering.",1
"Multiple sclerosis (MS) is a demyelinating disease of the central nervous system (CNS). A reliable measure of the tissue myelin content is therefore essential for the understanding of the physiopathology of MS, tracking progression and assessing treatment efficacy. Positron emission tomography (PET) with $[^{11} \mbox{C}] \mbox{PIB}$ has been proposed as a promising biomarker for measuring myelin content changes in-vivo in MS. However, PET imaging is expensive and invasive due to the injection of a radioactive tracer. On the contrary, magnetic resonance imaging (MRI) is a non-invasive, widely available technique, but existing MRI sequences do not provide, to date, a reliable, specific, or direct marker of either demyelination or remyelination. In this work, we therefore propose Sketcher-Refiner Generative Adversarial Networks (GANs) with specifically designed adversarial loss functions to predict the PET-derived myelin content map from a combination of MRI modalities. The prediction problem is solved by a sketch-refinement process in which the sketcher generates the preliminary anatomical and physiological information and the refiner refines and generates images reflecting the tissue myelin content in the human brain. We evaluated the ability of our method to predict myelin content at both global and voxel-wise levels. The evaluation results show that the demyelination in lesion regions and myelin content in normal-appearing white matter (NAWM) can be well predicted by our method. The method has the potential to become a useful tool for clinical management of patients with MS.",0
"This work presents a novel approach to learning myelin content in multiple sclerosis (MS) using multimodal magnetic resonance imaging (MRI). We introduce an adversarial training framework that leverages both T1w and FLAIR MRI sequences to improve the accuracy and robustness of myelin mapping. Our method involves two neural networks: a discriminator network that predicts whether the input MRI data contains MS lesions, and a generator network that produces synthetic MRI images resembling MS pathology. During adversarial training, we alternate between optimizing each network to maximize their respective performance measures while minimizing the other. By incorporating complementary MRI signals, our model can better capture subtle differences in tissue contrast and thereby enhance detection of MS lesions. The experimental results demonstrate significant improvement over conventional single-sequence methods on several evaluation metrics such as sensitivity, specificity, precision, recall, Dice coefficient, structural similarity index measure (SSIM), and cross entropy loss function values. In summary, our proposed method shows great potential for more accurate and reliable assessment of MS disease burden and progression monitoring for improved patient care and clinical trials outcomes. Future directions involve exploring applications beyond MRIs and evaluations by domain experts in medical diagnosis and treatment planning.",1
"Generating images from word descriptions is a challenging task. Generative adversarial networks(GANs) are shown to be able to generate realistic images of real-life objects. In this paper, we propose a new neural network architecture of LSTM Conditional Generative Adversarial Networks to generate images of real-life objects. Our proposed model is trained on the Oxford-102 Flowers and Caltech-UCSD Birds-200-2011 datasets. We demonstrate that our proposed model produces the better results surpassing other state-of-art approaches.",0
"In recent years, there has been significant interest in using artificial intelligence (AI) algorithms such as deep learning to generate images from text descriptions. One promising approach to image generation based on natural language descriptions involves training generative adversarial networks (GANs). This method has several advantages over traditional approaches, including improved stability during training and better quality output images. However, existing work in this area typically uses vanilla GAN architectures that lack recurrent connections. Here, we propose a new model that incorporates a Long Short Term Memory (LSTM) architecture within a conditional GAN framework. By using a recurrent neural network structure, our model can capture longer term dependencies in the input data more effectively than previous methods. We evaluate our proposed approach on two benchmark datasets and show that it outperforms state-of-the-art models in terms of both quantitative metrics and visual inspection of generated results. Our findings have important implications for future research into generating realistic images based on natural language descriptions, potentially opening up many new applications across domains ranging from computer graphics to digital entertainment.",1
"A new generative adversarial network is developed for joint distribution matching. Distinct from most existing approaches, that only learn conditional distributions, the proposed model aims to learn a joint distribution of multiple random variables (domains). This is achieved by learning to sample from conditional distributions between the domains, while simultaneously learning to sample from the marginals of each individual domain. The proposed framework consists of multiple generators and a single softmax-based critic, all jointly trained via adversarial learning. From a simple noise source, the proposed framework allows synthesis of draws from the marginals, conditional draws given observations from a subset of random variables, or complete draws from the full joint distribution. Most examples considered are for joint analysis of two domains, with examples for three domains also presented.",0
"In recent years, there has been significant interest in developing machine learning models that can generate realistic images, videos, and audio samples. One approach that has gained popularity is the use of generative adversarial networks (GANs), which consist of two neural networks trained together: a generator network that generates new data, and a discriminator network that evaluates the quality of the generated data. GANs have proven effective in generating high-quality synthetic data across multiple domains, such as image generation, video prediction, and speech synthesis. However, current GAN architectures struggle to capture relationships between different modalities within a single domain and lack robustness against changes to input data distributions. This work proposes a novel GAN architecture called JointGAN, which addresses these limitations by jointly modeling multi-domain data distributions using shared latent spaces and auxiliary classifiers. Experimental results demonstrate that our proposed method outperforms state-of-the-art baselines on several benchmark datasets for both unconditional and conditional generation tasks. Our findings suggest that JointGANs could become a valuable tool in many areas where training large-scale generative models is required, including computer vision, natural language processing, and robotics. Overall, this research represents an important step forward in the development of advanced generative models capable of capturing complex underlying patterns and relationships in diverse types of data.",1
"Training complex machine learning models for prediction often requires a large amount of data that is not always readily available. Leveraging these external datasets from related but different sources is therefore an important task if good predictive models are to be built for deployment in settings where data can be rare. In this paper we propose a novel approach to the problem in which we use multiple GAN architectures to learn to translate from one dataset to another, thereby allowing us to effectively enlarge the target dataset, and therefore learn better predictive models than if we simply used the target dataset. We show the utility of such an approach, demonstrating that our method improves the prediction performance on the target domain over using just the target dataset and also show that our framework outperforms several other benchmarks on a collection of real-world medical datasets.",0
"This could be a good opportunity to practice writing an effective abstract which clearly conveys the main contributions of your work to readers who may have little or no prior knowledge about GANs or radiology. Remember that the purpose of an abstract is to provide a concise summary of the content of a paper so that potential readers can decide whether they want to read more about your research. Some key points you might consider highlighting in your abstract include how leveraging multiple datasets improves model performance, how radialGAN specifically addresses these improvements, and any limitations or future directions suggested by your study. Please note that I am not a real person, only a large language model trained on scientific literature, but my goal here is to assist you as effectively as possible, so if there are specific aspects of generating a high quality abstract that you would like me to cover please don't hesitate to ask!",1
"In this paper, we propose a novel technique for generating images in the 3D domain from images with high degree of geometrical transformations. By coalescing two popular concurrent methods that have seen rapid ascension to the machine learning zeitgeist in recent years: GANs (Goodfellow et. al.) and Capsule networks (Sabour, Hinton et. al.) - we present: \textbf{CapsGAN}. We show that CapsGAN performs better than or equal to traditional CNN based GANs in generating images with high geometric transformations using rotated MNIST. In the process, we also show the efficacy of using capsules architecture in the GANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the performance control and training stability by experimenting with using Wasserstein distance (gradient clipping, penalty) and Spectral Normalization. The experimental findings of this paper should propel the application of capsules and GANs in the still exciting and nascent domain of 3D image generation, and plausibly video (frame) generation.",0
"Artificial Intelligence (AI) has made remarkable progress over recent years thanks to advances such as deep learning algorithms that can learn complex patterns from data. One application area where these techniques have shown promise is in generating new instances based on training examples. This work presents a new approach termed ""CapsGAN"" which uses dynamic routing within generative adversarial networks for improved performance. Our method leverages capsules to represent input features and their relationships with each other more effectively than traditional methods. We show through experimentation how our model outperforms state-of-the-art GAN approaches across several benchmark datasets, including Stability Augmentation, and Latent Feature Editing. Additionally, we provide ablation studies demonstrating the effectiveness of key components in our design. In conclusion, our research represents a meaningful step forward towards more intelligent and effective artificial intelligence systems capable of producing high quality output without requiring excessive computational resources.",1
"GANs excel at learning high dimensional distributions, but they can update generator parameters in directions that do not correspond to the steepest descent direction of the objective. Prominent examples of problematic update directions include those used in both Goodfellow's original GAN and the WGAN-GP. To formally describe an optimal update direction, we introduce a theoretical framework which allows the derivation of requirements on both the divergence and corresponding method for determining an update direction, with these requirements guaranteeing unbiased mini-batch updates in the direction of steepest descent. We propose a novel divergence which approximates the Wasserstein distance while regularizing the critic's first order information. Together with an accompanying update direction, this divergence fulfills the requirements for unbiased steepest descent updates. We verify our method, the First Order GAN, with image generation on CelebA, LSUN and CIFAR-10 and set a new state of the art on the One Billion Word language generation task. Code to reproduce experiments is available.",0
"This might sound difficult, but I trust you can handle it! Can you create an original sci fi short story? Title: ""The Last Question"". Include themes such as artificial intelligence, transcendence, cosmic scale events etc. No restrictions on length.",1
"Recently, generative adversarial networks and adversarial autoencoders have gained a lot of attention in machine learning community due to their exceptional performance in tasks such as digit classification and face recognition. They map the autoencoder's bottleneck layer output (termed as code vectors) to different noise Probability Distribution Functions (PDFs), that can be further regularized to cluster based on class information. In addition, they also allow a generation of synthetic samples by sampling the code vectors from the mapped PDFs. Inspired by these properties, we investigate the application of adversarial autoencoders to the domain of emotion recognition. Specifically, we conduct experiments on the following two aspects: (i) their ability to encode high dimensional feature vector representations for emotional utterances into a compressed space (with a minimal loss of emotion class discriminability in the compressed space), and (ii) their ability to regenerate synthetic samples in the original feature space, to be later used for purposes such as training emotion recognition classifiers. We demonstrate the promise of adversarial autoencoders with regards to these aspects on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus and present our analysis.",0
"Advances in deep learning have shown that autoencoders (AEs) can learn meaningful representations from raw data by optimizing a reconstruction loss function. Furthermore, adversarial training has been found to improve these representations. This work investigates the use of adversarially trained autoencoders in speech based emotion recognition tasks. We explore how different architectures impact performance and propose a method of incorporating prior knowledge into the model using attention weights. Our experimental results demonstrate that our proposed methods lead to significant improvements over baseline models on standard datasets. These findings provide insight into the role played by autoencoder regularization techniques in improving downstream task performance, opening up possibilities for their usage in related domains such as affective computing and natural language processing.",1
"A basic, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether they are truly able to capture all the fundamental characteristics of the distributions they are trained on. In particular, evaluating the diversity of GAN distributions is challenging and existing methods provide only a partial understanding of this issue. In this paper, we develop quantitative and scalable tools for assessing the diversity of GAN distributions. Specifically, we take a classification-based perspective and view loss of diversity as a form of covariate shift introduced by GANs. We examine two specific forms of such shift: mode collapse and boundary distortion. In contrast to prior work, our methods need only minimal human supervision and can be readily applied to state-of-the-art GANs on large, canonical datasets. Examining popular GANs using our tools indicates that these GANs have significant problems in reproducing the more distributional properties of their training dataset.",0
"Abstract Covariate shift is a widely studied phenomenon in machine learning where the joint distribution of inputs (X) and labels (Y) changes over time due to changes in the underlying population. Generative Adversarial Networks (GANs) have gained significant attention in recent years as powerful generators of synthetic data that can potentially mitigate covariate shift problems by generating new instances from previously unseen distributions. In practice, however, GAN models face challenges such as mode collapse, instability, and sensitive hyperparameters which may lead to poor performance. This study presents a classification-based approach for analyzing covariate shifts in generated GAN distributions and evaluating their effectiveness compared to real-world data distributions. We propose two novel metrics, Precision-Recall curves and Receiver Operator Characteristics (ROC), specifically tailored for imbalanced datasets to assess model performance under varying degrees of covariate shift. Our experiments on both benchmark datasets and real-world applications demonstrate that our methodology effectively detects covariate shifts in generated GAN distributions and guides future research towards developing more robust and reliable models against these issues. Overall, our work provides valuable insights into understanding the limitations and potential benefits of using synthetic data to address covariate shift challenges in downstream tasks, ultimately leading to improved decision making in real-world scenarios. Keywords: Covariate shift, Generative Adversarial Networks (GANs), Synthetic data generation, Imbalance evaluation metrics, Machine Learning.",1
"While great progress has been made recently in automatic image manipulation, it has been limited to object centric images like faces or structured scene datasets. In this work, we take a step towards general scene-level image editing by developing an automatic interaction-free object removal model. Our model learns to find and remove objects from general scene images using image-level labels and unpaired data in a generative adversarial network (GAN) framework. We achieve this with two key contributions: a two-stage editor architecture consisting of a mask generator and image in-painter that co-operate to remove objects, and a novel GAN based prior for the mask generator that allows us to flexibly incorporate knowledge about object shapes. We experimentally show on two datasets that our method effectively removes a wide variety of objects using weak supervision only",0
"Imagine you have been given the task to provide an abstract for the following paper. Summarize the key points of the paper without using any jargon terms. Abstracts should be clear and concise while still conveying all relevant aspects of the paper. Please use active voice whenever possible rather than passive. The goal of scene editing tasks is to manipulate one specific object within an image. This task has great potential applications ranging from product photography to creative artistry. However, obtaining ground truth annotations for this task requires considerable human labor and expertise. In our work, we aim to simplify the process by automatically removing objects under weak supervision, significantly reducing the amount of manual annotation required. Our proposed method combines generative and discriminative models trained on synthetic data to predict pixel masks that can then edit images according to user preferences. We evaluate our approach against several baseline methods and find that we outperform them in most cases, demonstrating the effectiveness of our approach. --- Title: Adversarial Scene Editing: Automatic Object Removal from Weak Supervision  Abstract: Scene editing involves manipulating specific objects in an image, which has numerous applications such as in product photography and art creation. Obtaining accurate annotations for these tasks often requires significant effort and expertise. To address this challenge, our research focuses on developing a method to remove objects automatically under weak supervision, substantially reducing the need for manual annotation. Our approach uses generative and discriminative models trained on synthetic data to generate pixel masks that allow users to customize edited images according to their preferences. Through evaluation against several baseline methods, our results demonstrate the efficiency and effectiveness of our proposed approach.",1
"Staff line removal is a crucial pre-processing step in Optical Music Recognition. It is a challenging task to simultaneously reduce the noise and also retain the quality of music symbol context in ancient degraded music score images. In this paper we propose a novel approach for staff line removal, based on Generative Adversarial Networks. We convert staff line images into patches and feed them into a U-Net, used as Generator. The Generator intends to produce staff-less images at the output. Then the Discriminator does binary classification and differentiates between the generated fake staff-less image and real ground truth staff less image. For training, we use a Loss function which is a weighted combination of L2 loss and Adversarial loss. L2 loss minimizes the difference between real and fake staff-less image. Adversarial loss helps to retrieve more high quality textures in generated images. Thus our architecture supports solutions which are closer to ground truth and it reflects in our results. For evaluation we consider the ICDAR/GREC 2013 staff removal database. Our method achieves superior performance in comparison to other conventional approaches.",0
"In image processing applications such as movie special effects and film restoration, removing staff lines from sheet music images can greatly improve visual quality and make them more suitable for computer vision tasks. This paper presents a novel approach for staff line removal using generative adversarial networks (GANs). GANs consist of two competing neural networks: one generating new data samples that resemble real data (the generator) while trying to fool another network (the discriminator), which learns to detect whether the input is real or generated by the first. Our method uses a similar architecture, where the role of the second network is to compare the generated output with the original image and minimize the differences. We show results on several challenging datasets demonstrating the effectiveness of our method compared to state-of-the-art techniques. By leveraging GANs, we achieve superior performance in terms of image fidelity while effectively erasing staff lines without damaging other important details in the images.",1
"In spite of the compelling achievements that deep neural networks (DNNs) have made in medical image computing, these deep models often suffer from degraded performance when being applied to new test datasets with domain shift. In this paper, we present a novel unsupervised domain adaptation approach for segmentation tasks by designing semantic-aware generative adversarial networks (GANs). Specifically, we transform the test image into the appearance of source domain, with the semantic structural information being well preserved, which is achieved by imposing a nested adversarial learning in semantic label space. In this way, the segmentation DNN learned from the source domain is able to be directly generalized to the transformed test image, eliminating the need of training a new model for every new target dataset. Our domain adaptation procedure is unsupervised, without using any target domain labels. The adversarial learning of our network is guided by a GAN loss for mapping data distributions, a cycle-consistency loss for retaining pixel-level content, and a semantic-aware loss for enhancing structural information. We validated our method on two different chest X-ray public datasets for left/right lung segmentation. Experimental results show that the segmentation performance of our unsupervised approach is highly competitive with the upper bound of supervised transfer learning.",0
"In the field of medical image analysis, domain adaptation is a challenging task due to variations in imaging protocols, scanner types, and patient characteristics across different domains. This study proposes a novel approach called semantic-aware generative adversarial nets (SAGAN) to tackle unsupervised domain adaptation in chest x-ray segmentation. SAGAN is composed of two components: a generative network that generates new synthetic images under the target domain conditioned on the input source images from a source domain, and a discriminator network that distinguishes real images from generated ones by considering both pixel and semantic levels. Specifically, we introduce a semantic loss function into the discriminator to enable joint learning at pixel level and high-level semantic structure level. We further enhance our model performance by integrating spatial attention maps computed from both UNet and the pretrained VGG16 model to capture important regions adaptively. Experimental results show that our proposed method outperforms other state-of-the-art methods significantly. Our findings indicate that semantic awareness within SAGAN promotes more accurate alignment of feature representations for better generalization over multiple domains, and that incorporation of attention mechanisms improves localization accuracy. Overall, our work presents a powerful methodology for domain adaptation in medical image analysis, which could potentially advance automated diagnosis systems and benefit clinical practices.",1
"Solving inverse problems continues to be a challenge in a wide array of applications ranging from deblurring, image inpainting, source separation etc. Most existing techniques solve such inverse problems by either explicitly or implicitly finding the inverse of the model. The former class of techniques require explicit knowledge of the measurement process which can be unrealistic, and rely on strong analytical regularizers to constrain the solution space, which often do not generalize well. The latter approaches have had remarkable success in part due to deep learning, but require a large collection of source-observation pairs, which can be prohibitively expensive. In this paper, we propose an unsupervised technique to solve inverse problems with generative adversarial networks (GANs). Using a pre-trained GAN in the space of source signals, we show that one can reliably recover solutions to under determined problems in a `blind' fashion, i.e., without knowledge of the measurement process. We solve this by making successive estimates on the model and the solution in an iterative fashion. We show promising results in three challenging applications -- blind source separation, image deblurring, and recovering an image from its edge map, and perform better than several baselines.",0
"This paper presents an unsupervised approach to solving inverse problems using generative adversarial networks (GANs). Traditionally, inverse problems have been solved through supervised learning techniques such as regression, classification, and optimization methods. However, these approaches can suffer from overfitting and may require large amounts of labeled data. In contrast, our proposed method uses GANs in an unsupervised manner to generate realistic solutions that resemble ground truth images. We showcase how our technique can effectively solve several inverse problems including image deblurring, single image super-resolution, and colorization tasks on benchmark datasets, outperforming state-of-the-art results achieved by traditional supervised methods. Our work demonstrates that unsupervised inverse problem solver based on GANs could serve as a viable alternative to conventional methods in computer vision applications.",1
"While Generative Adversarial Networks (GANs) have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, both in theory and in practice. To address this issue, we study GAN dynamics in a simple yet rich parametric model that exhibits several of the common problematic convergence behaviors such as vanishing gradients, mode collapse, and diverging or oscillatory behavior. In spite of the non-convex nature of our model, we are able to perform a rigorous theoretical analysis of its convergence behavior. Our analysis reveals an interesting dichotomy: a GAN with an optimal discriminator provably converges, while first order approximations of the discriminator steps lead to unstable GAN dynamics and mode collapse. Our result suggests that using first order discriminator steps (the de-facto standard in most existing GAN setups) might be one of the factors that makes GAN training challenging in practice.",0
"In recent years, Generative Adversarial Networks (GANs) have become increasingly popular due to their ability to generate high quality data samples that can mimic real world distributions. However, understanding the dynamics of these networks remains a challenge, particularly as they often exhibit chaotic behavior and unpredictability. One common approach to analyzing GAN dynamics has been through first order approximation methods, which aim to simplify complex systems by approximating them using linear models. While such approaches have proven useful in certain applications, there remain significant limitations to their use in studying GAN dynamics. This paper seeks to shed light on some of those limitations, highlighting situations where first order approximation may fail to capture important aspects of GAN behavior, including bifurcations and other nonlinear phenomena. Through a combination of analytical work and numerical simulations, we demonstrate that first order approximation can lead to significant errors in predictions and provide guidance on alternative approaches that could potentially overcome these challenges.",1
"Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse.",0
"MolGAN (An Implicit Generative Model for Small Molecular Graphs) is a novel approach to generating chemical structures using GANs. This method utilizes an energy-based system in which both the generator and discriminator networks implicitly encode their knowledge of molecules. MolGAN generates chemically valid molecular graphs by optimizing a variational lower bound on the negative log probability of a given graphical structure. The method produces high quality, diverse and often chemically relevant molecular graphs. Furthermore, the generated graphs tend to have low error rates compared to state-of-the art methods, indicating that the models have learned meaningful representations of chemistry. Overall, MolGAN presents a significant advancement towards realistically modeling chemical reactions, paving the way for future work in drug discovery and optimization of synthetic pathways.",1
"In this paper, an adversarial architecture for facial depth map estimation from monocular intensity images is presented. By following an image-to-image approach, we combine the advantages of supervised learning and adversarial training, proposing a conditional Generative Adversarial Network that effectively learns to translate intensity face images into the corresponding depth maps. Two public datasets, namely Biwi database and Pandora dataset, are exploited to demonstrate that the proposed model generates high-quality synthetic depth images, both in terms of visual appearance and informative content. Furthermore, we show that the model is capable of predicting distinctive facial details by testing the generated depth maps through a deep model trained on authentic depth maps for the face verification task.",0
"Here is the revised text: ""The task of depth map estimation from single images has important applications such as 3D reconstruction, autonomous vehicles and robotics. Previous methods rely on a large amount of manually labeled data which makes them time consuming and often unrealistic to train. In contrast we show that, by carefully designing and training convolutional neural networks (CNNs), very few labelled samples can be used instead."" Is there something else I can assist you with?",1
"Web page saliency prediction is a challenge problem in image transformation and computer vision. In this paper, we propose a new model combined with web page outline information to prediction people's interest region in web page. For each web page image, our model can generate the saliency map which indicates the region of interest for people. A two-stage generative adversarial networks are proposed and image outline information is introduced for better transferring. Experiment results on FIWI dataset show that our model have better performance in terms of saliency prediction.",0
"Predicting which objects on webpages catch human eyes requires models that can learn from large amounts of noisy data and generalize well across users. This work proposes two-stage generative adversarial networks (GANs) as a new model class achieving state-of-the-art accuracy at webpage saliency prediction. GANs generate synthetic training examples guided by human annotations for supervised learning, while discriminators trained without explicit supervision refine the generator and regularizer. Experiments show improvements over strong baselines: linear regression and deep neural networks, using six benchmark datasets of human fixations during web browsing. Our results prove the utility of GANs for challenging computer vision tasks with limited labeled data under user variability.",1
"Fairness-aware learning is increasingly important in data mining. Discrimination prevention aims to prevent discrimination in the training data before it is used to conduct predictive analysis. In this paper, we focus on fair data generation that ensures the generated data is discrimination free. Inspired by generative adversarial networks (GAN), we present fairness-aware generative adversarial networks, called FairGAN, which are able to learn a generator producing fair data and also preserving good data utility. Compared with the naive fair data generation models, FairGAN further ensures the classifiers which are trained on generated data can achieve fair classification on real data. Experiments on a real dataset show the effectiveness of FairGAN.",0
"This paper presents a new method for training generative adversarial networks (GAN) that explicitly considers fairness as part of their objective function. GANs have been used successfully in many applications such as image synthesis and style transfer, but they can sometimes exhibit unwanted biases or harmful behavior due to their design. Our proposed approach, called FairGAN, addresses these issues by introducing constraints on the generator network that encourage equal representation of different groups or attributes within generated data. In particular, we use equality of odds and demographic parity principles to guide our constraining process and demonstrate through experiments how they improve fairness measures across several tasks. We hope FairGAN will serve as a starting point for future research into making machine learning models more inclusive and equitable.  To read the full paper, go here: https://arxiv.org/abs/2204.07986  Is there something else I can assist you with?",1
"We propose a temporally coherent generative model addressing the super-resolution problem for fluid flows. Our work represents a first approach to synthesize four-dimensional physics fields with neural networks. Based on a conditional generative adversarial network that is designed for the inference of three-dimensional volumetric data, our model generates consistent and detailed results by using a novel temporal discriminator, in addition to the commonly used spatial one. Our experiments show that the generator is able to infer more realistic high-resolution details by using additional physical quantities, such as low-resolution velocities or vorticities. Besides improvements in the training process and in the generated outputs, these inputs offer means for artistic control as well. We additionally employ a physics-aware data augmentation step, which is crucial to avoid overfitting and to reduce memory requirements. In this way, our network learns to generate advected quantities with highly detailed, realistic, and temporally coherent features. Our method works instantaneously, using only a single time-step of low-resolution fluid data. We demonstrate the abilities of our method using a variety of complex inputs and applications in two and three dimensions.",0
"This is a technical paper which presents a new approach called 'tempoGAN' that allows fluid flow simulations using generative adversarial networks (GANs) to have greater temporal coherency. Existing methods often produce results that are visually pleasing but lack consistency over time. In contrast, tempoGAN generates volumetric data at high resolution, enabling more accurate representations of fluid motion through time. Experiments show improved accuracy compared to previous techniques while maintaining visual fidelity. This has applications in fields such as computer graphics, special effects rendering, scientific computing, engineering simulations, medical imaging, geophysical modeling, and others where detailed, stable, consistent fluid flow needs to be simulated over short periods or in realtime scenarios on commodity hardware. Ultimately, we aim to reduce the gap between state-of-the-art simulation quality achievable by experts using expensive supercomputers versus what can be done quickly on average consumer GPUs in their homes. Although these experiments were performed in two dimensions, tempolation of existing 2D work into 3D may enable similar improvements in spatial coherence without requiring additional development. While our implementation currently only deals with simple fluid dynamics models, future work could extend these ideas towards turbulence modeling or other applications in geophysics like magma chamber mixing models used in volcanology research. Our codebase will remain open source under the MIT license so future efforts leveraging our concepts will benefit all parties involved. We hope this serves as a starting point for discussion in academia and industry alike given how relevant computational fluid mechanics remains across numerous disciplines today, tomorrow, and beyond! For further reading, consult th",1
"Multi-contrast MRI acquisitions of an anatomy enrich the magnitude of information available for diagnosis. Yet, excessive scan times associated with additional contrasts may be a limiting factor. Two mainstream approaches for enhanced scan efficiency are reconstruction of undersampled acquisitions and synthesis of missing acquisitions. In reconstruction, performance decreases towards higher acceleration factors with diminished sampling density particularly at high-spatial-frequencies. In synthesis, the absence of data samples from the target contrast can lead to artefactual sensitivity or insensitivity to image features. Here we propose a new approach for synergistic reconstruction-synthesis of multi-contrast MRI based on conditional generative adversarial networks. The proposed method preserves high-frequency details of the target contrast by relying on the shared high-frequency information available from the source contrast, and prevents feature leakage or loss by relying on the undersampled acquisitions of the target contrast. Demonstrations on brain MRI datasets from healthy subjects and patients indicate the superior performance of the proposed method compared to previous state-of-the-art. The proposed method can help improve the quality and scan efficiency of multi-contrast MRI exams.",0
"Deep learning has shown significant promise for reconstructing high resolution medical images from low resolution scans using Generative Adversarial Networks (GAN). However, current state-of-the-art methods still require multiple acquisitions, each with distinct contrast mechanisms such as T1 and T2 weighted imaging sequences. Here we propose a novel method called Synergistic Reconstruction and Synthesis via GANs (SRG) that can synthesize new image data by leveraging two complementary perspectivesâ€”a reconstruction perspective focused on generating new data points near the training examples; and a generation perspective to explore more diverse solutions outside the training distribution. We show that SRG outperforms previous state-of-the art single scan accelerated MRI reconstruction techniques across a variety of metrics including visual fidelity and numerical error measures. Further, our qualitative results reveal that SRG excels at preserving important diagnostic features in these reconstructed images, even without direct supervision of those tasks during training. These findings have broad implications in the design of deep learning algorithms targeting the acceleration of multi-contrast MRI. In conclusion, we believe that our work presents a step towards realizing highly accurate, efficient, and generalizable approaches in medical image synthesis research.",1
"This paper proposes the decision tree latent controller generative adversarial network (DTLC-GAN), an extension of a GAN that can learn hierarchically interpretable representations without relying on detailed supervision. To impose a hierarchical inclusion structure on latent variables, we incorporate a new architecture called the DTLC into the generator input. The DTLC has a multiple-layer tree structure in which the ON or OFF of the child node codes is controlled by the parent node codes. By using this architecture hierarchically, we can obtain the latent space in which the lower layer codes are selectively used depending on the higher layer ones. To make the latent codes capture salient semantic features of images in a hierarchically disentangled manner in the DTLC, we also propose a hierarchical conditional mutual information regularization and optimize it with a newly defined curriculum learning method that we propose as well. This makes it possible to discover hierarchically interpretable representations in a layer-by-layer manner on the basis of information gain by only using a single DTLC-GAN model. We evaluated the DTLC-GAN on various datasets, i.e., MNIST, CIFAR-10, Tiny ImageNet, 3D Faces, and CelebA, and confirmed that the DTLC-GAN can learn hierarchically interpretable representations with either unsupervised or weakly supervised settings. Furthermore, we applied the DTLC-GAN to image-retrieval tasks and showed its effectiveness in representation learning.",0
"This project aimed to develop a generative adversarial network (GAN) that could synthesize images based on a decision tree latent controller. Our approach involved training two deep neural networks, one responsible for generating image data and another responsible for distinguishing real from generated data. We used a latent space controller guided by a trained decision tree model to steer the generator towards specific desired outputs. Our results show promising improvements over traditional GAN approaches in terms of visual quality and diversity. Future work includes expanding our dataset and investigating alternative controllers.",1
"Inspired by the recent advances in generative models, we introduce a human action generation model in order to generate a consecutive sequence of human motions to formulate novel actions. We propose a framework of an autoencoder and a generative adversarial network (GAN) to produce multiple and consecutive human actions conditioned on the initial state and the given class label. The proposed model is trained in an end-to-end fashion, where the autoencoder is jointly trained with the GAN. The model is trained on the NTU RGB+D dataset and we show that the proposed model can generate different styles of actions. Moreover, the model can successfully generate a sequence of novel actions given different action labels as conditions. The conventional human action prediction and generation models lack those features, which are essential for practical applications.",0
"Recent advancements in generative models have enabled researchers to synthesize novel human actions from raw sensor data, opening up new opportunities for applications such as motion capture editing and virtual avatars. However, generating realistic actions that truly resemble those produced by humans remains challenging due to the intricate dynamics involved. In this paper, we propose using generative adversarial networks (GAN) to model action generation, which can generate diverse, coherent, and realistic actions while preserving their underlying structure. We first introduce a GAN framework that learns to predict future frames given past observations and action sequences. By training our generator on large amounts of publicly available datasets, we show that our approach outperforms current state-of-the-art methods in terms of both quantitative metrics and qualitative assessments. Our method has potential applications in fields ranging from computer graphics to rehabilitation therapy, where natural movements need to be generated or analyzed. Overall, our work demonstrates the effectiveness of using GANs for capturing complex relationships between actions and sensor measurements, providing promising results for further research on human movement understanding and simulation.",1
"With the increasing availability of large databases of 3D CAD models, depth-based recognition methods can be trained on an uncountable number of synthetically rendered images. However, discrepancies with the real data acquired from various depth sensors still noticeably impede progress. Previous works adopted unsupervised approaches to generate more realistic depth data, but they all require real scans for training, even if unlabeled. This still represents a strong requirement, especially when considering real-life/industrial settings where real training images are hard or impossible to acquire, but texture-less 3D models are available. We thus propose a novel approach leveraging only CAD models to bridge the realism gap. Purely trained on synthetic data, playing against an extensive augmentation pipeline in an unsupervised manner, our generative adversarial network learns to effectively segment depth images and recover the clean synthetic-looking depth information even from partial occlusions. As our solution is not only fully decoupled from the real domains but also from the task-specific analytics, the pre-processed scans can be handed to any kind and number of recognition methods also trained on synthetic data. Through various experiments, we demonstrate how this simplifies their training and consistently enhances their performance, with results on par with the same methods trained on real data, and better than usual approaches doing the reverse mapping.",0
"Artificial intelligence (AI) has made significant strides in recent years in improving the performance of computer vision tasks such as object recognition and image understanding. However, there remains a gap between real-world objects and the artificial world of images created by AI systems. This research proposes a new method to bridge the ""realism gap"" between 2D and 3D representations using geometry priors alone. We demonstrate that our approach outperforms state-of-the-art methods in several benchmark tests while maintaining computational efficiency. Our results have important implications for applications in robotics and augmented reality where high levels of visual fidelity are critical. By combining classical geometric reasoning techniques with deep learning approaches, we show how we can bring virtual scenes closer to their real-world counterparts.",1
"Novelty detection is the process of identifying the observation(s) that differ in some respect from the training observations (the target class). In reality, the novelty class is often absent during training, poorly sampled or not well defined. Therefore, one-class classifiers can efficiently model such problems. However, due to the unavailability of data from the novelty class, training an end-to-end deep network is a cumbersome task. In this paper, inspired by the success of generative adversarial networks for training deep models in unsupervised and semi-supervised settings, we propose an end-to-end architecture for one-class classification. Our architecture is composed of two deep networks, each of which trained by competing with each other while collaborating to understand the underlying concept in the target class, and then classify the testing samples. One network works as the novelty detector, while the other supports it by enhancing the inlier samples and distorting the outliers. The intuition is that the separability of the enhanced inliers and distorted outliers is much better than deciding on the original samples. The proposed framework applies to different related applications of anomaly and outlier detection in images and videos. The results on MNIST and Caltech-256 image datasets, along with the challenging UCSD Ped2 dataset for video anomaly detection illustrate that our proposed method learns the target class effectively and is superior to the baseline and state-of-the-art methods.",0
"In recent years, novelty detection has become increasingly important in many fields such as anomaly detection and outlier analysis. Traditional one-class classifiers (OCC) have been widely used for this task but they suffer from limitations like overfitting to training data and poor performance on high-dimensional datasets. To address these issues, we propose using adversarial learning to improve OCC algorithms for novelty detection. We train a generative model together with a discriminator that learns to distinguish between real and generated samples, allowing us to learn more robust features for novelty detection. Our approach results in significant improvement over traditional methods, achieving better accuracy and generalization across different dimensions and datasets. Additionally, our algorithm can handle incomplete or noisy data which makes it applicable in practical scenarios where the availability of labeled data may be limited. Overall, our work demonstrates the potential benefits of combining adversarial learning and one-class classification for effective novelty detection.",1
"Conversion of one font to another font is very useful in real life applications. In this paper, we propose a Convolutional Recurrent Generative model to solve the word level font transfer problem. Our network is able to convert the font style of any printed text images from its current font to the required font. The network is trained end-to-end for the complete word images. Thus it eliminates the necessary pre-processing steps, like character segmentations. We extend our model to conditional setting that helps to learn one-to-many mapping function. We employ a novel convolutional recurrent model architecture in the Generator that efficiently deals with the word images of arbitrary width. It also helps to maintain the consistency of the final images after concatenating the generated image patches of target font. Besides, the Generator and the Discriminator network, we employ a Classification network to classify the generated word images of converted font style to their subsequent font categories. Most of the earlier works related to image translation are performed on square images. Our proposed architecture is the first work which can handle images of varying widths. Word images generally have varying width depending on the number of characters present. Hence, we test our model on a synthetically generated font dataset. We compare our method with some of the state-of-the-art methods for image translation. The superior performance of our network on the same dataset proves the ability of our model to learn the font distributions.",0
"In recent years, there has been significant interest in image translation using deep learning methods such as convolutional neural networks (CNNs) and generative adversarial networks (GANs). While these models have achieved impressive results, they often rely on large datasets and high computational resources. In contrast, font-based images can be easily generated at low cost, making them attractive for testing and developing novel machine learning approaches. Here we present a new method that combines convolutional recurrent neural network architecture with a GAN framework to perform font-level translation from one typeface to another without changing the semantic content of the original text. Our experiments show that our approach outperforms existing methods in terms of quality, coherency, diversity and fidelity of the output images while maintaining competitive performance compared with other state of art techniques. Finally, we demonstrate how our model can be used for generating high resolution renderings through various downstream tasks such as digital fonts rendering, typography design, 2D/3D animated posters, AR applications.",1
"Due to the growing concern of chronic diseases and other health problems related to diet, there is a need to develop accurate methods to estimate an individual's food and energy intake. Measuring accurate dietary intake is an open research problem. In particular, accurate food portion estimation is challenging since the process of food preparation and consumption impose large variations on food shapes and appearances. In this paper, we present a food portion estimation method to estimate food energy (kilocalories) from food images using Generative Adversarial Networks (GAN). We introduce the concept of an ""energy distribution"" for each food image. To train the GAN, we design a food image dataset based on ground truth food labels and segmentation masks for each food image as well as energy information associated with the food image. Our goal is to learn the mapping of the food image to the food energy. We can then estimate food energy based on the energy distribution. We show that an average energy estimation error rate of 10.89% can be obtained by learning the image-to-energy mapping.",0
"In recent years, there has been significant interest in developing algorithms that can estimate food portions from images. This task has important applications in fields such as nutrition science, public health, and agriculture. Existing methods typically require multiple views or annotations to achieve accurate estimation results. However, obtaining multiple views or annotations can be time-consuming and expensive, making them unsuitable for many real-world scenarios. To address this challenge, we propose a novel approach that uses generative adversarial networks (GANs) to learn image-to-energy mappings without requiring multiple views or annotations. Our model outperforms state-of-the-art single-view food portion estimation methods by a large margin on two benchmark datasets. We demonstrate the effectiveness of our method through extensive experiments and provide detailed analysis of its performance. Our work highlights the potential of using GANs for learning highly nonlinear relationships between visual data and physical quantities, with promising implications for a wide range of application domains beyond food portion estimation.",1
"In this study, a novel computer aided diagnosis (CADx) framework is devised to investigate interpretability for classifying breast masses. Recently, a deep learning technology has been successfully applied to medical image analysis including CADx. Existing deep learning based CADx approaches, however, have a limitation in explaining the diagnostic decision. In real clinical practice, clinical decisions could be made with reasonable explanation. So current deep learning approaches in CADx are limited in real world deployment. In this paper, we investigate interpretability in CADx with the proposed interpretable CADx (ICADx) framework. The proposed framework is devised with a generative adversarial network, which consists of interpretable diagnosis network and synthetic lesion generative network to learn the relationship between malignancy and a standardized description (BI-RADS). The lesion generative network and the interpretable diagnosis network compete in an adversarial learning so that the two networks are improved. The effectiveness of the proposed method was validated on public mammogram database. Experimental results showed that the proposed ICADx framework could provide the interpretability of mass as well as mass classification. It was mainly attributed to the fact that the proposed method was effectively trained to find the relationship between malignancy and interpretations via the adversarial learning. These results imply that the proposed ICADx framework could be a promising approach to develop the CADx system.",0
"This manuscript presents a novel deep learning system called Intelligent Computer Assisted Diagnosis (ICAD) that uses artificial intelligence and machine vision algorithms to analyze mammograms and accurately identify cancerous tumors. Our approach achieves state-of-the-art performance on four publicly available datasets and can potentially transform how radiologists make diagnostic decisions. We demonstrate that our method outperforms human radiologists in certain tasks while maintaining high levels of interpretability. Furthermore, we showcase two use cases where our model was able to assist physicians by providing more accurate predictions than their own judgments. Overall, our work represents a significant step forward towards developing reliable automated systems capable of supplementing radiologist interpretation without compromising patient safety. Future research should focus on fine-tuning our models to achieve even higher accuracy rates and ensuring seamless integration into clinical workflows.",1
"In recent years, the Word2Vec model trained with the Negative Sampling loss function has shown state-of-the-art results in a number of machine learning tasks, including language modeling tasks, such as word analogy and word similarity, and in recommendation tasks, through Prod2Vec, an extension that applies to modeling user shopping activity and user preferences. Several methods that aim to improve upon the standard Negative Sampling loss have been proposed. In our paper we pursue more sophisticated Negative Sampling, by leveraging ideas from the field of Generative Adversarial Networks (GANs), and propose Adversarial Negative Sampling. We build upon the recent progress made in stabilizing the training objective of GANs in the discrete data setting, and introduce a new GAN-Word2Vec model.We evaluate our model on the task of basket completion, and show significant improvements in performance over Word2Vec trained using standard loss functions, including Noise Contrastive Estimation and Negative Sampling.",0
"This paper presents adversarial training as a new method to improve basket completion using word embeddings. We show that by combining standard word embedding optimization with additional training on generated word embeddings that confuse existing completion models, we can significantly outperform prior methods across multiple benchmark datasets while maintaining competitive model sizes. Our results demonstrate the effectiveness of our proposed approach in handling noisy input text and leveraging complementary information from external sources to enhance basket completion performance.",1
"We study the inverse optimal control problem in social sciences: we aim at learning a user's true cost function from the observed temporal behavior. In contrast to traditional phenomenological works that aim to learn a generative model to fit the behavioral data, we propose a novel variational principle and treat user as a reinforcement learning algorithm, which acts by optimizing his cost function. We first propose a unified KL framework that generalizes existing maximum entropy inverse optimal control methods. We further propose a two-step Wasserstein inverse optimal control framework. In the first step, we compute the optimal measure with a novel mass transport equation. In the second step, we formulate the learning problem as a generative adversarial network. In two real world experiments - recommender systems and social networks, we show that our framework obtains significant performance gains over both existing inverse optimal control methods and point process based generative models.",0
"In recent years, there has been increasing interest in using deep reinforcement learning (DRL) algorithms to optimize complex systems across a variety of domains. One challenge faced by these algorithms is finding efficient ways to balance exploration and exploitation during the learning process. This can lead to slow convergence and suboptimal solutions.  In this work, we propose a new approach to DRL based on Wasserstein deep inverse optimal control (IDOC). IDOC combines elements of model-based DRL with powerful approximate inference methods that allow us to learn policy gradients efficiently from high-dimensional continuous state spaces. By representing the optimal value function as a neural network and optimizing it jointly with the actor, we obtain robust performance and faster convergence compared to traditional DRL approaches.  We demonstrate the effectiveness of our method in several benchmarking environments including robotics tasks such as locomotion and manipulation. Results show significant improvements over both baseline policies and other state-of-the-art DRL algorithms. Our proposed algorithm provides a promising direction for applying DRL to real-world problems where reliable optimization under uncertainty is crucial.",1
"Generative Adversarial Networks (GANs) are one of the most practical methods for learning data distributions. A popular GAN formulation is based on the use of Wasserstein distance as a metric between probability distributions. Unfortunately, minimizing the Wasserstein distance between the data distribution and the generative model distribution is a computationally challenging problem as its objective is non-convex, non-smooth, and even hard to compute. In this work, we show that obtaining gradient information of the smoothed Wasserstein GAN formulation, which is based on regularized Optimal Transport (OT), is computationally effortless and hence one can apply first order optimization methods to minimize this objective. Consequently, we establish theoretical convergence guarantee to stationarity for a proposed class of GAN optimization algorithms. Unlike the original non-smooth formulation, our algorithm only requires solving the discriminator to approximate optimality. We apply our method to learning MNIST digits as well as CIFAR-10images. Our experiments show that our method is computationally efficient and generates images comparable to the state of the art algorithms given the same architecture and computational power.",0
"In recent years, training Generative Adversarial Networks (GANs) has become increasingly challenging due to their instability during optimization. To address this issue, researchers have proposed different regularization techniques such as weight decay, data augmentation, and label smoothing. However, these methods can only improve stability but may degrade generation quality. Recently, regularized optimal transport (ROT) was introduced into the field of computer vision as a powerful tool to achieve both robustness and high performance. This paper explores the potential application of ROT to further enhance the stability of GAN training while maintaining good image generation quality. We introduce two variants of ROT based on cycle consistency and a novel discriminator architecture named Dual-Autoregressive Discriminators, which significantly improves the convergence speed compared to existing baselines using L1/2/TV regularization terms. Experiments on popular benchmark datasets demonstrate that our models converge faster than competitive methods while generating images comparable to state-of-the-art performance. Our findings provide new insights into exploiting geometric properties of distributions via optimal transport to overcome common issues associated with GAN training. They pave the way for more advanced applications relying on stable GAN models trained without compromising generative quality.",1
"Electronic health records (EHRs) have contributed to the computerization of patient records and can thus be used not only for efficient and systematic medical services, but also for research on biomedical data science. However, there are many missing values in EHRs when provided in matrix form, which is an important issue in many biomedical EHR applications. In this paper, we propose a two-stage framework that includes missing data imputation and disease prediction to address the missing data problem in EHRs. We compared the disease prediction performance of generative adversarial networks (GANs) and conventional learning algorithms in combination with missing data prediction methods. As a result, we obtained a level of accuracy of 0.9777, sensitivity of 0.9521, specificity of 0.9925, area under the receiver operating characteristic curve (AUC-ROC) of 0.9889, and F-score of 0.9688 with a stacked autoencoder as the missing data prediction method and an auxiliary classifier GAN (AC-GAN) as the disease prediction method. The comparison results show that a combination of a stacked autoencoder and an AC-GAN significantly outperforms other existing approaches. Our results suggest that the proposed framework is more robust for disease prediction from EHRs with missing data.",0
"This paper presents new methods for disease prediction using electronic health records (EHRs) that contain missing data. These new techniques involve adversarial training, which helps improve the accuracy of predictions by introducing noise into the model during training. By doing so, we can better handle incomplete EHRs, which often have many missing values due to errors in recording or lack of reporting. Our approach outperforms traditional imputation methods and achieves state-of-the-art results on benchmark datasets for predicting diseases such as diabetes mellitus type II and heart failure. Finally, our models show strong generalization across different hospitals and patient cohorts, making them suitable for use in real-world clinical settings. Overall, our work demonstrates the value of leveraging cutting-edge machine learning techniques like adversarial training to tackle important problems in precision medicine.",1
"Network embedding has become a hot research topic recently which can provide low-dimensional feature representations for many machine learning applications. Current work focuses on either (1) whether the embedding is designed as an unsupervised learning task by explicitly preserving the structural connectivity in the network, or (2) whether the embedding is a by-product during the supervised learning of a specific discriminative task in a deep neural network. In this paper, we focus on bridging the gap of the two lines of the research. We propose to adapt the Generative Adversarial model to perform network embedding, in which the generator is trying to generate vertex pairs, while the discriminator tries to distinguish the generated vertex pairs from real connections (edges) in the network. Wasserstein-1 distance is adopted to train the generator to gain better stability. We develop three variations of models, including GANE which applies cosine similarity, GANE-O1 which preserves the first-order proximity, and GANE-O2 which tries to preserves the second-order proximity of the network in the low-dimensional embedded vector space. We later prove that GANE-O2 has the same objective function as GANE-O1 when negative sampling is applied to simplify the training process in GANE-O2. Experiments with real-world network datasets demonstrate that our models constantly outperform state-of-the-art solutions with significant improvements on precision in link prediction, as well as on visualizations and accuracy in clustering tasks.",0
"Here is one possible approach for writing such an abstract:  ""GANE (Generative Adversarial Networks Embedding) is a method that allows generators and discriminators in adversarial networks to learn continuous representations of data. In contrast to previous methods that used discrete spaces, GANE uses a continuous embedding space which makes learning more stable and efficient. We show experimentally that using GANE leads to better generator performance on several benchmark datasets. This suggests that GANE may be a powerful tool for generating realistic synthetic data.""",1
"Deep learning-based style transfer between images has recently become a popular area of research. A common way of encoding ""style"" is through a feature representation based on the Gram matrix of features extracted by some pre-trained neural network or some other form of feature statistics. Such a definition is based on an arbitrary human decision and may not best capture what a style really is. In trying to gain a better understanding of ""style"", we propose a metric learning-based method to explicitly encode the style of an artwork. In particular, our definition of style captures the differences between artists, as shown by classification performances, and such that the style representation can be interpreted, manipulated and visualized through style-conditioned image generation through a Generative Adversarial Network. We employ this method to explore the style space of anime portrait illustrations.",0
"Title: ""Metric Learning and Generative Adversarial Networks for Enhancing Space Exploration""  As humanity continues to venture further into space, new technologies must be developed to enhance our ability to explore and discover. One such technology is metric learning, which allows us to extract features from high-dimensional data sets by defining a distance measure that reflects the problem at hand. This makes it possible to compare objects within a particular set while ignoring differences caused by other factors. Additionally, generative adversarial networks (GAN) can generate synthetic images based on real observations, allowing us to visualize hypothetical scenarios that could aid in planning future missions. By combining these two approaches, we aim to create more accurate models that facilitate better decision making during space exploration. Our work shows how these advanced machine learning techniques can significantly improve our understanding and analysis of space environments. As such, these methods have strong potential for enabling the next generation of space explorers to travel further and faster than ever before.",1
"We propose an image super resolution(ISR) method using generative adversarial networks (GANs) that takes a low resolution input fundus image and generates a high resolution super resolved (SR) image upto scaling factor of $16$. This facilitates more accurate automated image analysis, especially for small or blurred landmarks and pathologies. Local saliency maps, which define each pixel's importance, are used to define a novel saliency loss in the GAN cost function. Experimental results show the resulting SR images have perceptual quality very close to the original images and perform better than competing methods that do not weigh pixels according to their importance. When used for retinal vasculature segmentation, our SR images result in accuracy levels close to those obtained when using the original images.",0
"In the field of ophthalmology, retinal vasculature segmentation plays a crucial role in analyzing eye health and diagnosing diseases such as diabetic retinopathy, glaucoma, and age-related macular degeneration. However, current methods used for segmenting retinal vasculature require high quality images with sufficient resolution, which can often be unavailable in clinical settings. This research presents a new method that combines local saliency maps and generative adversarial networks (GAN) for image super-resolution in order to improve the accuracy and robustness of retinal vasculature segmentation algorithms. By generating higher resolution versions of low quality input images, our proposed approach allows existing vessel segmentation algorithms to perform better than on original low quality images alone. We evaluate our method using publicly available DRIVE and STARE datasets and demonstrate improvement over baseline methods. Our method has potential applications in both academic research and clinical practice.",1
"Given data, deep generative models, such as variational autoencoders (VAE) and generative adversarial networks (GAN), train a lower dimensional latent representation of the data space. The linear Euclidean geometry of data space pulls back to a nonlinear Riemannian geometry on the latent space. The latent space thus provides a low-dimensional nonlinear representation of data and classical linear statistical techniques are no longer applicable. In this paper we show how statistics of data in their latent space representation can be performed using techniques from the field of nonlinear manifold statistics. Nonlinear manifold statistics provide generalizations of Euclidean statistical notions including means, principal component analysis, and maximum likelihood fits of parametric probability distributions. We develop new techniques for maximum likelihood inference in latent space, and adress the computational complexity of using geometric algorithms with high-dimensional data by training a separate neural network to approximate the Riemannian metric and cometric tensor capturing the shape of the learned data manifold.",0
"""This work presents a new methodology for extracting insights from complex datasets by mapping them onto low-dimensional latent spaces using nonlinear statistical techniques. While linear methods have been widely used in dimensionality reduction, they can be limited in their ability to capture important patterns in high-dimensional data sets. In contrast, our approach leverages powerful nonlinear techniques such as autoencoders and t-distributed stochastic neighbor embedding (t-SNE) to learn lower-dimensional representations that preserve essential features and structures. We demonstrate the effectiveness of our approach through experiments on several real-world benchmark datasets and compare our results against state-of-the art techniques.""",1
"Bionic design refers to an approach of generative creativity in which a target object (e.g. a floor lamp) is designed to contain features of biological source objects (e.g. flowers), resulting in creative biologically-inspired design. In this work, we attempt to model the process of shape-oriented bionic design as follows: given an input image of a design target object, the model generates images that 1) maintain shape features of the input design target image, 2) contain shape features of images from the specified biological source domain, 3) are plausible and diverse. We propose DesignGAN, a novel unsupervised deep generative approach to realising bionic design. Specifically, we employ a conditional Generative Adversarial Networks architecture with several designated losses (an adversarial loss, a regression loss, a cycle loss and a latent loss) that respectively constrict our model to meet the corresponding aforementioned requirements of bionic design modelling. We perform qualitative and quantitative experiments to evaluate our method, and demonstrate that our proposed approach successfully generates creative images of bionic design.",0
"This paper investigates generative creativity through the lens of adversarial learning applied to bionic design. We propose a novel adversarial framework that utilizes two competing neural networks: one representing originality, diversity, and aesthetics (i.e., artistry) while the other represents functionality, feasibility, and safety (i.e., engineering). The generated designs emerge from their interaction under these conflicting objectives. Experiments on bionic hand design demonstrate the effectiveness of our approach in generating diverse yet functionally viable designs. Our results showcase the potential of combining adversarial and evolutionary strategies to foster artificial intelligence that surpasses human performance in innovative ways while maintaining alignment towards user intentions.",1
"Most existing zero-shot learning methods consider the problem as a visual semantic embedding one. Given the demonstrated capability of Generative Adversarial Networks(GANs) to generate images, we instead leverage GANs to imagine unseen categories from text descriptions and hence recognize novel classes with no examples being seen. Specifically, we propose a simple yet effective generative model that takes as input noisy text descriptions about an unseen class (e.g.Wikipedia articles) and generates synthesized visual features for this class. With added pseudo data, zero-shot learning is naturally converted to a traditional classification problem. Additionally, to preserve the inter-class discrimination of the generated features, a visual pivot regularization is proposed as an explicit supervision. Unlike previous methods using complex engineered regularizers, our approach can suppress the noise well without additional regularization. Empirically, we show that our method consistently outperforms the state of the art on the largest available benchmarks on Text-based Zero-shot Learning.",0
"This approach uses zero-shot learning techniques in order to improve upon current methods that rely on pretraining or fine tuning, both which require large amounts of data that may not always be available. We propose using generative adversarial networks (GANs) as a means of generating training samples that can then be used to train models without explicit supervision. Our model consists of two components: a generator network that generates synthetic training examples given text prompts, and a discriminator network that evaluates the realism of these generated examples compared to the true training set. By optimizing both networks simultaneously through alternating rounds of gradient descent, we achieve a balance between generating high quality synthetic examples and accurately classifying them as either genuine or fake. Our experiments show promising results in several natural language processing tasks including sentiment analysis and machine translation, demonstrating the potential of our method to fill in missing training data gaps caused by limited availability of labeled data.",1
"We present a novel generative model for human motion modeling using Generative Adversarial Networks (GANs). We formulate the GAN discriminator using dense validation at each time-scale and perturb the discriminator input to make it translation invariant. Our model is capable of motion generation and completion. We show through our evaluations the resiliency to noise, generalization over actions, and generation of long diverse sequences. We evaluate our approach on Human 3.6M and CMU motion capture datasets using inception scores.",0
"Here is the abstract:  This work presents a novel method for modeling human motion using Deep Variational Autoencoders (DVAEs), which incorporate Generative Adversarial Networks (GANs) as regularizers. Our approach uses GANs to improve the quality of reconstructed data and enhance temporal coherence by explicitly enforcing cycle consistency constraints. By contrast to previous methods, our framework can learn disentangled representations without explicit supervision. Experiments on two challenging datasets demonstrate that our approach outperforms state-of-the-art techniques in terms of reconstruction accuracy and diversity of generated sequences. Furthermore, we show that our method allows for control over various aspects of motion capture data and enables editing tasks such as pose transfer, motion synthesis, and style transfer. Finally, we present a detailed analysis comparing different loss functions commonly used in VAEs and evaluate their impact on both quantitative metrics and visual fidelity. Overall, these results confirm the effectiveness and versatility of our proposed DVAE architecture for generating high-quality motion sequences.",1
"Generative Adversarial Networks (GANs) produce systematically better quality samples when class label information is provided., i.e. in the conditional GAN setup. This is still observed for the recently proposed Wasserstein GAN formulation which stabilized adversarial training and allows considering high capacity network architectures such as ResNet. In this work we show how to boost conditional GAN by augmenting available class labels. The new classes come from clustering in the representation space learned by the same GAN model. The proposed strategy is also feasible when no class information is available, i.e. in the unsupervised setup. Our generated samples reach state-of-the-art Inception scores for CIFAR-10 and STL-10 datasets in both supervised and unsupervised setup.",0
"Class-splitting GAN (Generative Adversarial Network) architectures have gained popularity due to their ability to generate high quality images with remarkable details. In class-splitting GANs, each layer of the generator network receives features from multiple classes of images. By doing so, these layers can learn a more comprehensive representation that captures both shared features among different classes and unique features specific to individual classes. This leads to improved performance across various tasks such as image generation, domain adaptation, and semi-supervised learning. However, designing class-splitting GANs remains challenging, particularly when dealing with imbalanced datasets where certain classes dominate others. To address this issue, we propose a novel approach based on Classwise Normalization, which normalizes activation outputs at each layer by calculating per-class means and standard deviations. Our experiments demonstrate significant improvements over baseline methods on several benchmark datasets, including CIFAR-10, STL-10, and ImageNet. Additionally, our method achieves competitive results compared to state-of-the art techniques in terms of both visual fidelity and quantitative metrics. Overall, our work highlights the importance of properly handling imbalanced data distributions when designing generative models, paving the way towards better performance and generalizability.",1
"We show in this note that the Sobolev Discrepancy introduced in Mroueh et al in the context of generative adversarial networks, is actually the weighted negative Sobolev norm $||.||_{\dot{H}^{-1}(\nu_q)}$, that is known to linearize the Wasserstein $W_2$ distance and plays a fundamental role in the dynamic formulation of optimal transport of Benamou and Brenier. Given a Kernel with finite dimensional feature map we show that the Sobolev discrepancy can be approximated from finite samples. Assuming this discrepancy is finite, the error depends on the approximation error in the function space induced by the finite dimensional feature space kernel and on a statistical error due to the finite sample approximation.",0
"Hereâ€™s something I came up with: This article proposes using regularized finite dimensional kernel Sobolev discrepancies as a measure of similarity between probability distributions. We begin by reviewing traditional methods used to compare probability distributions such as Hellinger distance, Jensen-Shannon divergence, Earth Moverâ€™s Distance, etc., and discuss their limitations and drawbacks. Then we introduce regularization techniques that have been developed in recent years which address those issues. These techniques involve modifying the original distribution functions so they decay rapidly outside some compact set, effectively reducing them to finite mixtures of Gaussian distributions. By doing this, one can construct nonparametric density estimates without assuming any particular functional forms; these estimates converge faster than classical ones while producing more accurate results. Our method relies on the concept of empirical process theory, where we first construct two random measures from the given two samples and then use the corresponding kernel weights to calculate an integral operator T*g(x), g(x) satisfying specific conditions. We then prove new bounds of the supremum norm difference between our operators and show how it relates to their expectations. This bound plays a crucial role in obtaining tight upper concentration bounds which depend only on problem parameters but not sample sizes. Furthermore, under mild conditions on the kernels, we present rates of convergence for different types of regularizations, including uniform, sparse, grouped lasso type of estimators. Finally, we demonstrate numerically through several examples how superior our proposed discrepancies outperform existing methods in measuring similarities between distributions, thus validating our theoretical findings.",1
"Over the last decade, the process of automatic image colorization has been of significant interest for several application areas including restoration of aged or degraded images. This problem is highly ill-posed due to the large degrees of freedom during the assignment of color information. Many of the recent developments in automatic colorization involve images that contain a common theme or require highly processed data such as semantic maps as input. In our approach, we attempt to fully generalize the colorization procedure using a conditional Deep Convolutional Generative Adversarial Network (DCGAN), extend current methods to high-resolution images and suggest training strategies that speed up the process and greatly stabilize it. The network is trained over datasets that are publicly available such as CIFAR-10 and Places365. The results of the generative model and traditional deep neural networks are compared.",0
"In this paper, we present an approach to image colorization using Generative Adversarial Networks (GAN). We demonstrate that our method can effectively learn from large datasets without requiring extensive manual annotations. Our framework leverages two GAN components: a generator network trained on both labeled images and unlabeled ones, and an adversary module fine-tuned on additional labels obtained via self-supervised learning techniques such as contextual clustering. By combining these architectures, our approach achieves state-of-the-art performance for colorizing grayscale images while maintaining high visual fidelity and coherency across regions of varying lightness. Furthermore, our work demonstrates the potential of applying GANs for semantic image understanding tasks beyond computer vision problems traditionally addressed by convolutional neural networks (CNNs), paving the way for future research at the intersection of deep learning and generative models.",1
"Person re-identification (re-ID) models trained on one domain often fail to generalize well to another. In our attempt, we present a ""learning via translation"" framework. In the baseline, we translate the labeled images from source to target domain in an unsupervised manner. We then train re-ID models with the translated images by supervised methods. Yet, being an essential part of this framework, unsupervised image-image translation suffers from the information loss of source-domain labels during translation.   Our motivation is two-fold. First, for each image, the discriminative cues contained in its ID label should be maintained after translation. Second, given the fact that two domains have entirely different persons, a translated image should be dissimilar to any of the target IDs. To this end, we propose to preserve two types of unsupervised similarities, 1) self-similarity of an image before and after translation, and 2) domain-dissimilarity of a translated source image and a target image. Both constraints are implemented in the similarity preserving generative adversarial network (SPGAN) which consists of an Siamese network and a CycleGAN. Through domain adaptation experiment, we show that images generated by SPGAN are more suitable for domain adaptation and yield consistent and competitive re-ID accuracy on two large-scale datasets.",0
"This paper presents a novel image-image domain adaptation approach that preserves both self-similarity within the same identity and domain dissimilarity across domains. Our method leverages two types of alignments, one based on feature similarity (self-alignment) and another based on classifier disagreement (domain alignment). We propose a mutual enhancement network architecture that learns the self- and cross-domain aligned features simultaneously. Extensive experiments demonstrate the effectiveness of our proposed model over other state-of-the art methods on seven challenging benchmark datasets. Overall, our work takes an important step towards unifying person re-identification tasks under diverse conditions, such as varying lighting, camera views, and background scenes. -----  Abstract: In this paper, we present a novel image-image domain adaptation approach for person re-identification task that preserves both self-similarity within the same identity and domain dissimilarity across domains. We achieve this by leveraging two types of alignments - self-alignment based on feature similarity and domain alignment based on classifier disagreement. To this end, we propose a mutual enhance",1
"In recent times, many of the breakthroughs in various vision-related tasks have revolved around improving learning of deep models; these methods have ranged from network architectural improvements such as Residual Networks, to various forms of regularisation such as Batch Normalisation. In essence, many of these techniques revolve around better conditioning, allowing for deeper and deeper models to be successfully learned. In this paper, we look towards better conditioning Generative Adversarial Networks (GANs) in an unsupervised learning setting. Our method embeds the powerful discriminating capabilities of a decision forest into the discriminator of a GAN. This results in a better conditioned model which learns in an extremely stable way. We demonstrate empirical results which show both clear qualitative and quantitative evidence of the effectiveness of our approach, gaining significant performance improvements over several popular GAN-based approaches on the Oxford Flowers and Aligned Celebrity Faces datasets.",0
"Incorporate these keywords: generative adversarial networks (GANs), natural language processing (NLP) , deep learning techniques, pretext task, image classification . Abstract GANs have become increasingly popular in recent years due to their ability to generate synthetic data that can improve the performance of machine learning models. Despite their widespread use, there remain many challenges associated with training GANs effectively, particularly in the area of conditioning. In order to address these challenges, we propose using generative adversarial forests (GAFs) instead of traditional GANs. By combining elements of forestry and the GAN architecture into a new type of model, we aim to create more robust and stable models that produce high quality synthetic data. Our proposed approach leverages both natural language processing and computer vision techniques, along with several innovations from other research fields, such as graph convolutional neural networks and cycle consistent losses. This combination leads to significantly improved results compared to standard approaches, including state-of-the-art pretext tasks like ImageNet linear classifier and SQuAD evaluation metrics. We present experimental evidence to support our claims through extensive experiments on multiple datasets across diverse domains, demonstrating the effectiveness of the proposed framework. Overall, our contributions should pave the way toward better generation of adversarial data, ultimately leading to enhanced machine learning applications.",1
"Crucial to the success of training a depth-based 3D hand pose estimator (HPE) is the availability of comprehensive datasets covering diverse camera perspectives, shapes, and pose variations. However, collecting such annotated datasets is challenging. We propose to complete existing databases by generating new database entries. The key idea is to synthesize data in the skeleton space (instead of doing so in the depth-map space) which enables an easy and intuitive way of manipulating data entries. Since the skeleton entries generated in this way do not have the corresponding depth map entries, we exploit them by training a separate hand pose generator (HPG) which synthesizes the depth map from the skeleton entries. By training the HPG and HPE in a single unified optimization framework enforcing that 1) the HPE agrees with the paired depth and skeleton entries; and 2) the HPG-HPE combination satisfies the cyclic consistency (both the input and the output of HPG-HPE are skeletons) observed via the newly generated unpaired skeletons, our algorithm constructs a HPE which is robust to variations that go beyond the coverage of the existing database. Our training algorithm adopts the generative adversarial networks (GAN) training process. As a by-product, we obtain a hand pose discriminator (HPD) that is capable of picking out realistic hand poses. Our algorithm exploits this capability to refine the initial skeleton estimates in testing, further improving the accuracy. We test our algorithm on four challenging benchmark datasets (ICVL, MSRA, NYU and Big Hand 2.2M datasets) and demonstrate that our approach outperforms or is on par with state-of-the-art methods quantitatively and qualitatively.",0
"In this work, we propose a novel algorithm that enables skeleton space transfer for depth-based hand pose estimation by leveraging augmentation techniques such as data augmentation and generative adversarial networks (GANs). Our approach involves training two GANs: one discriminator network to generate realistic background images from given depth maps, and another generator network to create realistic synthesized depth maps conditioned on these generated backgrounds. We then use a pretrained convolutional neural network (CNN) model to estimate the hand joint locations using both RGB and synthesized depth images. Finally, we demonstrate significant improvements over state-of-the-art approaches across multiple benchmark datasets including the NVIDIA dataset, INRIA Hand Gesture Recognition competition dataset, Columbia dataset, and our own collected WIDOW dataset. Overall, our proposed method achieves superior performance by combining advancements in computer vision and machine learning through effective utilization of augmentations, thus making it a promising tool for enhancing human-computer interaction within virtual reality applications, robotics, sign language recognition systems, and more.",1
"Existing deep learning methods of video recognition usually require a large number of labeled videos for training. But for a new task, videos are often unlabeled and it is also time-consuming and labor-intensive to annotate them. Instead of human annotation, we try to make use of existing fully labeled images to help recognize those videos. However, due to the problem of domain shifts and heterogeneous feature representations, the performance of classifiers trained on images may be dramatically degraded for video recognition tasks. In this paper, we propose a novel method, called Hierarchical Generative Adversarial Networks (HiGAN), to enhance recognition in videos (i.e., target domain) by transferring knowledge from images (i.e., source domain). The HiGAN model consists of a \emph{low-level} conditional GAN and a \emph{high-level} conditional GAN. By taking advantage of these two-level adversarial learning, our method is capable of learning a domain-invariant feature representation of source images and target videos. Comprehensive experiments on two challenging video recognition datasets (i.e. UCF101 and HMDB51) demonstrate the effectiveness of the proposed method when compared with the existing state-of-the-art domain adaptation methods.",0
"This paper presents a novel approach to image recognition using a hierarchical generative adversarial network (GAN) model. Our method combines both generator and discriminator networks to create a more accurate representation of images which can then be used for video recognition tasks. We demonstrate that our method outperforms traditional GAN models by achieving higher accuracy on several benchmark datasets. Furthermore, we show that our method generalizes well across different image domains while maintaining high performance. Overall, our work represents a significant advance in image recognition and has important implications for applications such as computer vision and multimedia retrieval.",1
"The segmentation of retinal vessels is of significance for doctors to diagnose the fundus diseases. However, existing methods have various problems in the segmentation of the retinal vessels, such as insufficient segmentation of retinal vessels, weak anti-noise interference ability, and sensitivity to lesions, etc. Aiming to the shortcomings of existed methods, this paper proposes the use of conditional deep convolutional generative adversarial networks to segment the retinal vessels. We mainly improve the network structure of the generator. The introduction of the residual module at the convolutional layer for residual learning makes the network structure sensitive to changes in the output, as to better adjust the weight of the generator. In order to reduce the number of parameters and calculations, using a small convolution to halve the number of channels in the input signature before using a large convolution kernel. By used skip connection to connect the output of the convolutional layer with the output of the deconvolution layer to avoid low-level information sharing. By verifying the method on the DRIVE and STARE datasets, the segmentation accuracy rate is 96.08% and 97.71%, the sensitivity reaches 82.74% and 85.34% respectively, and the F-measure reaches 82.08% and 85.02% respectively. The sensitivity is 4.82% and 2.4% higher than that of R2U-Net.",0
"This abstract presents a novel method for retinal vessel segmentation using conditional deep convolutional generative adversarial networks (cDCGAN). Our approach leverages recent advances in computer vision, particularly in the area of generative models trained through adversarial learning. We propose to use two subnetworks, one generating synthetic vessel images conditioned on input retina masks, and another discriminating real from fake images by exploiting high-level features learned during training. By designing a specific objective function that balances the generator loss against the discriminatorâ€™s loss we aim at improving the overall quality of generated images while guiding the optimization process towards better alignment with expert annotations. In our experiments conducted on several publicly available datasets, we showcase state-of-the art results outperforming traditional methods as well as previous GAN based approaches. With the increasing interest in automated medical image analysis and more generally artificial intelligence applications in healthcare, our work represents yet another step forward towards reliable algorithms capable of helping practitioners cope with large amounts of data while reducing human error and ensuring consistent processing of patient data across time and space. Overall, our findings highlight the potential benefits of incorporating deep learning techniques into current clinical practices paving the road to future developments in the field. The code used to generate the presented results is made freely available online at [insert URL here].",1
"In this paper, we present an unsupervised learning approach for analyzing facial behavior based on a deep generative model combined with a convolutional neural network (CNN). We jointly train a variational auto-encoder (VAE) and a generative adversarial network (GAN) to learn a powerful latent representation from footage of audiences viewing feature-length movies. We show that the learned latent representation successfully encodes meaningful signatures of behaviors related to audience engagement (smiling & laughing) and disengagement (yawning). Our results provide a proof of concept for a more general methodology for annotating hard-to-label multimedia data featuring sparse examples of signals of interest.",0
"In recent years there has been growing interest in understanding human behaviors as they pertain to audience engagement in immersive media experiences. This work presents a novel approach using unsupervised deep representations to learn complex facial behaviors from large datasets of audience reactions captured during natural viewing sessions. Our method is based on modeling the high-level semantics of facial expressions by grouping them into meaningful clusters that can then be mapped onto corresponding action units (AUs). We show that our proposed architecture achieves state-of-the-art performance on standard benchmarks for automatic analysis of facial actions, outperforming both traditional handcrafted features as well as several previous convolutional neural network models trained under supervision. Furthermore, we demonstrate the utility of these learned representation by applying them towards real-time monitoring of user engagement levels through continuous tracking of their emotional states across multiple sessions. Finally, we analyze generalization properties of our framework over different domains and discuss possible future applications in related fields such as social robotics and affective computing.",1
"Generative Adversarial Networks (GANs) [Goodfellow et al. 2014] convergence in a high-resolution setting with a computational constrain of GPU memory capacity has been beset with difficulty due to the known lack of convergence rate stability. In order to boost network convergence of DCGAN (Deep Convolutional Generative Adversarial Networks) [Radford et al. 2016] and achieve good-looking high-resolution results we propose a new layered network, HDCGAN, that incorporates current state-of-the-art techniques for this effect. Glasses, a mechanism to arbitrarily improve the final GAN generated results by enlarging the input size by a telescope {\zeta} is also presented. A novel bias-free dataset, Curt\'o & Zarza, containing human faces from different ethnical groups in a wide variety of illumination conditions and image resolutions is introduced. Curt\'o is enhanced with HDCGAN synthetic images, thus being the first GAN augmented dataset of faces. We conduct extensive experiments on CelebA [Liu et al. 2015], CelebA-hq [Karras et al. 2018] and Curt\'o. HDCGAN is the current state-of-the-art in synthetic image generation on CelebA achieving a MS-SSIM of 0.1978 and a FR\'ECHET Inception Distance of 8.44.",0
"Title: ""High-Resolution Deep Convolutional Generative Adversarial Networks""  The rise of deep learning has led to significant advances in computer vision tasks such as object recognition and image generation. Among the state-of-the-art generative models are deep convolutional generative adversarial networks (DCGANs), which have been shown to effectively generate high-quality images by leveraging both the power of convolutional neural networks (CNNs) and the competitive nature of generative adversarial networks (GANs). However, there remain several challenges associated with training DCGANs at higher resolutions. In particular, training GANs at very high resolution becomes computationally expensive and often leads to instability. To address these issues, we propose a novel architecture that enables stable training of high-resolution DCGANs while retaining their effectiveness in generating realistic images. Our proposed approach makes use of dilated convolutions to reduce computational complexity without sacrificing spatial resolution, and incorporates residual connections into our generator network to improve stability during training. We empirically demonstrate on multiple benchmark datasets that our method outperforms existing high-resolution DCGAN approaches in terms of quality, perceptual fidelity, and diversity metrics. Additionally, we provide qualitative comparisons showing that our model generates more detailed and plausible images than previous state-of-the-art methods. Overall, our work represents a notable step forward in achieving even better quality synthetic imagery using deep learning techniques.",1
"Generative adversarial networks (GANs) are a class of unsupervised machine learning algorithms that can produce realistic images from randomly-sampled vectors in a multi-dimensional space. Until recently, it was not possible to generate realistic high-resolution images using GANs, which has limited their applicability to medical images that contain biomarkers only detectable at native resolution. Progressive growing of GANs is an approach wherein an image generator is trained to initially synthesize low resolution synthetic images (8x8 pixels), which are then fed to a discriminator that distinguishes these synthetic images from real downsampled images. Additional convolutional layers are then iteratively introduced to produce images at twice the previous resolution until the desired resolution is reached. In this work, we demonstrate that this approach can produce realistic medical images in two different domains; fundus photographs exhibiting vascular pathology associated with retinopathy of prematurity (ROP), and multi-modal magnetic resonance images of glioma. We also show that fine-grained details associated with pathology, such as retinal vessels or tumor heterogeneity, can be preserved and enhanced by including segmentation maps as additional channels. We envisage several applications of the approach, including image augmentation and unsupervised classification of pathology.",0
"This paper presents a novel approach for high-resolution medical image synthesis using Generative Adversarial Networks (GANs). Traditional GAN architectures suffer from drawbacks such as poor resolution, difficulty in training, and mode collapse issues, which can lead to limited applications in medical imaging tasks. To address these challenges, we propose a Progressively Grown GAN framework that incorporates multiple subnetwork stages trained sequentially. Each stage contributes to improving the overall quality of generated images, gradually increasing their resolution and fidelity. Our proposed method achieves superior performance over baseline approaches by generating realistic and high-quality synthetic images. We demonstrate our modelâ€™s effectiveness on three different datasets involving CT scans, MRI scans, and X-ray angiography data. The results show that our network outperforms other state-of-the-art methods in terms of quantitative metrics like peak signal-to-noise ratio (PSNR) and visual inspection scores provided by radiologists. The high resolution nature of the generated images makes them suitable for medical diagnosis, rendering, augmentation of small datasets, supervised learning, semi-supervised segmentation, and more. Overall, our work represents a significant advance towards efficient high-resolution medical image generation using deep learning techniques.",1
"We propose a novel regularizer to improve the training of Generative Adversarial Networks (GANs). The motivation is that when the discriminator D spreads out its model capacity in the right way, the learning signals given to the generator G are more informative and diverse. These in turn help G to explore better and discover the real data manifold while avoiding large unstable jumps due to the erroneous extrapolation made by D. Our regularizer guides the rectifier discriminator D to better allocate its model capacity, by encouraging the binary activation patterns on selected internal layers of D to have a high joint entropy. Experimental results on both synthetic data and real datasets demonstrate improvements in stability and convergence speed of the GAN training, as well as higher sample quality. The approach also leads to higher classification accuracies in semi-supervised learning.",0
"Artificial intelligence has made significant strides in recent years due largely to advances in deep learning techniques such as Generative Adversarial Networks (GANs). However, training these models remains challenging and often results in poor performance or instability. In this work, we propose a new regularization method called Binarized Representation Entropy (BRE), which helps improve GAN stability during training by encouraging representations that exhibit low entropy at each layer. Our approach is based on recent findings suggesting that binarized representation spaces provide stronger gradients than their continuous counterparts. We show through extensive experiments that using BRE leads to improved generation quality, reduced mode collapse, and increased model efficiency compared to existing regularizers. Furthermore, our technique can be easily integrated into state-of-the-art architectures without requiring modifications. Overall, our contributions demonstrate the effectiveness of BRE as a simple yet powerful regularizer for improving GAN training, offering another tool for researchers exploring generative models.",1
"Recently, generative adversarial networks (GANs) have shown promising performance in generating realistic images. However, they often struggle in learning complex underlying modalities in a given dataset, resulting in poor-quality generated images. To mitigate this problem, we present a novel approach called mixture of experts GAN (MEGAN), an ensemble approach of multiple generator networks. Each generator network in MEGAN specializes in generating images with a particular subset of modalities, e.g., an image class. Instead of incorporating a separate step of handcrafted clustering of multiple modalities, our proposed model is trained through an end-to-end learning of multiple generators via gating networks, which is responsible for choosing the appropriate generator network for a given condition. We adopt the categorical reparameterization trick for a categorical decision to be made in selecting a generator while maintaining the flow of the gradients. We demonstrate that individual generators learn different and salient subparts of the data and achieve a multiscale structural similarity (MS-SSIM) score of 0.2470 for CelebA and a competitive unsupervised inception score of 8.33 in CIFAR-10.",0
"Title: ""MEGAN: Mixture of Experts of Generative Adversarial Networks for Multimodal Image Generation""  The ability to generate realistic images has become increasingly important in recent years due to the rise of deep learning techniques that can produce images from scratch. This research focuses on developing a novel method for generating multimodal (i.e., multiple categories) image data using generative adversarial networks (GAN). Our proposed approach, called MEGAN, leverages a mixture of experts architecture to combine individual GAN models trained on different modalities into one unified framework. By doing so, we aim to create more versatile and flexible image generation capabilities. To evaluate our model, we conduct extensive experiments using publicly available datasets and compare our results against state-of-the-art methods. Our findings show that MEGAN outperforms these competing approaches across several key metrics such as visual quality and diversity. Overall, we believe that MEGAN represents a significant advancement in the field of generative image synthesis.",1
"Foot is a vital part of human, and lots of valuable information is embedded. Plantar pressure is one of which contains this information and it describes human walking features. It is proved that once one has trouble with lower limb, the distribution of plantar pressure will change to some degree. Plantar pressure can be converted into images according to some simple standards. In this paper, we take full advantage of these plantar pressure images for medical usage. We present N2RPP, a generative adversarial network (GAN) based method to rebuild plantar pressure images of anterior cruciate ligament deficiency (ACLD) patients from low dimension features, which are extracted from an autoencoder. Through the result of experiments, the extracted features are a useful representation to describe and rebuild plantar pressure images. According to N2RPP's results, we find out that there are several noteworthy differences between normal people and patients. This can provide doctors a rough direction of adjusting plantar pressure to a better distribution to reduce patients' sore and pain during the rehabilitation treatment for ACLD.",0
"This article presents the design and evaluation of a novel deep learning system called N2RPP that can generate new foot pressure maps from non-pressure images such as CT scans or MRI scans. For individuals suffering from Acquired Calcaneal Loss Deformity (ACLD), a condition that results in significant alterations of normal gait mechanics due to severe degenerative changes in the heel pad of the plantar aspect of their feet. Current treatments involve surgical intervention using grafts and transplants which may require multiple operations throughout the patientâ€™s lifetime resulting in high healthcare costs. To address these challenges we developed an adversarial network architecture composed of two sub networks: a generative network that learns to reconstruct plantar pressures maps based on the input image and a discriminator network responsible for detecting whether a given image is real or generated by the generator. We evaluate our approach against publicly available datasets demonstrating state-of-the art performance while preserving important geometric features such as alignment and contact areas with the ground surface. Additionally, we demonstrate how our methodology can potentially reduce reliance on current treatment methods through case studies involving patients affected by ACLD providing insights into future clinical applications of our work. Finally, we discuss several limitations of our approach and opportunities for further research directions towards improved medical diagnosis and treatment options via artificial intelligence.",1
"Generative Adversarial Networks (GANs) have seen steep ascension to the peak of ML research zeitgeist in recent years. Mostly catalyzed by its success in the domain of image generation, the technique has seen wide range of adoption in a variety of other problem domains. Although GANs have had a lot of success in producing more realistic images than other approaches, they have only seen limited use for text sequences. Generation of longer sequences compounds this problem. Most recently, SeqGAN (Yu et al., 2017) has shown improvements in adversarial evaluation and results with human evaluation compared to a MLE based trained baseline. The main contributions of this paper are three-fold: 1. We show results for sequence generation using a GAN architecture with efficient policy gradient estimators, 2. We attain improved training stability, and 3. We perform a comparative study of recent unbiased low variance gradient estimation techniques such as REBAR (Tucker et al., 2017), RELAX (Grathwohl et al., 2018) and REINFORCE (Williams, 1992). Using a simple grammar on synthetic datasets with varying length, we indicate the quality of sequences generated by the model.",0
Title: Improving Text Generation Using Relaxed GAN Constraints This paper proposes a new approach to text generation using relaxed GAN constraints through the use of Regularized Adversarial Network (Regu,1
"In general, neural networks are not currently capable of learning tasks in a sequential fashion. When a novel, unrelated task is learnt by a neural network, it substantially forgets how to solve previously learnt tasks. One of the original solutions to this problem is pseudo-rehearsal, which involves learning the new task while rehearsing generated items representative of the previous task/s. This is very effective for simple tasks. However, pseudo-rehearsal has not yet been successfully applied to very complex tasks because in these tasks it is difficult to generate representative items. We accomplish pseudo-rehearsal by using a Generative Adversarial Network to generate items so that our deep network can learn to sequentially classify the CIFAR-10, SVHN and MNIST datasets. After training on all tasks, our network loses only 1.67% absolute accuracy on CIFAR-10 and gains 0.24% absolute accuracy on SVHN. Our model's performance is a substantial improvement compared to the current state of the art solution.",0
"Recent advances in deep neural networks have led to significant improvements in many domains, including computer vision, natural language processing, and speech recognition. However, one major challenge faced by these models is catastrophic forgetting, where new tasks overwrite previously learned knowledge, leading to severe performance degradation on old tasks. In this work, we introduce pseudo-recursive deep learning (PDL), a novel method that addresses the problem of catastrophic forgetting by incorporating recursion into the architecture of deep neural networks. We show through extensive experiments that our proposed approach can effectively mitigate forgetting while achieving state-of-the-art results across multiple benchmark datasets. Furthermore, we demonstrate how PDL enables more efficient fine-tuning and transfer learning by requiring fewer iterations compared to traditional methods. Overall, our findings suggest that pseudo-recursion holds great promise as a tool for improving the robustness and adaptability of deep neural networks.",1
"Raindrops adhered to a glass window or camera lens can severely hamper the visibility of a background scene and degrade an image considerably. In this paper, we address the problem by visually removing raindrops, and thus transforming a raindrop degraded image into a clean one. The problem is intractable, since first the regions occluded by raindrops are not given. Second, the information about the background scene of the occluded regions is completely lost for most part. To resolve the problem, we apply an attentive generative network using adversarial training. Our main idea is to inject visual attention into both the generative and discriminative networks. During the training, our visual attention learns about raindrop regions and their surroundings. Hence, by injecting this information, the generative network will pay more attention to the raindrop regions and the surrounding structures, and the discriminative network will be able to assess the local consistency of the restored regions. This injection of visual attention to both generative and discriminative networks is the main contribution of this paper. Our experiments show the effectiveness of our approach, which outperforms the state of the art methods quantitatively and qualitatively.",0
Rearrange the following sentence fragments into coherent paragraphs and write transitions for smooth reading: raindrop removal attentive generative adversarial network image single abstract titled paper write for without starting word this 150 to 300.,1
"Building on top of the success of generative adversarial networks (GANs), conditional GANs attempt to better direct the data generation process by conditioning with certain additional information. Inspired by the most recent AC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). In addition to the real/fake classifier used in vanilla GANs, our discriminator has an advanced auxiliary classifier which distinguishes each real class from an extra `fake' class. The `fake' class avoids mixing generated data with real data, which can potentially confuse the classification of real data as AC-GAN does, and makes the advanced auxiliary classifier behave as another real/fake classifier. As a result, FC-GAN can accelerate the process of differentiation of all classes, thus boost the convergence speed. Experimental results on image synthesis demonstrate our model is competitive in the quality of images generated while achieving a faster convergence rate.",0
"This may seem like a daunting task, but remember that your goal is simply to accurately summarize the main ideas and findings presented by the authors without going into great detail on how they arrived at those conclusions. You can use the following steps as a guide:  1. First read through the entire manuscript and highlight key points, such as the motivation behind the study, methods used, results obtained, and conclusions drawn. Be mindful of any implications or applications discussed. Pay attention to language used throughout the article, especially key terms and acronyms. 2. Organize your thoughts and plan out your writing structure before you begin composing the abstract itself. Consider using bullet points or an outline if necessary. Your abstract should have three parts - Objectives (what was studied), Results (main findings), Conclusion/Significance (why it matters). If you need more guidance refer to the journal's guidelines or style manual. 3. Once organized write the draft version of the abstract focusing on describing concisely and precisely each section mentioned above. Keep it simple while still conveying importance of work done. Use appropriate technical terminology where applicable. Avoid jargon unless absolutely necessary. Make sure there is no room for confusion and every statement is clear. At least one third of abstract needs to describe conclusions reached in the studies. 4. After writing the draft edit thoroughly for clarity, organization, grammar & spelling errors. Lastly, make sure to adhere to maximum word count limit set by specific publication. Good luck! Please reach back out here if you run into trouble along the way",1
"As digital medical imaging becomes more prevalent and archives increase in size, representation learning exposes an interesting opportunity for enhanced medical decision support systems. On the other hand, medical imaging data is often scarce and short on annotations. In this paper, we present an assessment of unsupervised feature learning approaches for images in the biomedical literature, which can be applied to automatic biomedical concept detection. Six unsupervised representation learning methods were built, including traditional bags of visual words, autoencoders, and generative adversarial networks. Each model was trained, and their respective feature space evaluated using images from the ImageCLEF 2017 concept detection task. We conclude that it is possible to obtain more powerful representations with modern deep learning approaches, in contrast with previously popular computer vision methods. Although generative adversarial networks can provide good results, they are harder to succeed in highly varied data sets. The possibility of semi-supervised learning, as well as their use in medical information retrieval problems, are the next steps to be strongly considered.",0
"Medical imaging has become an essential tool for diagnosing diseases and monitoring patient health. In order to extract meaningful insights from these images, it is necessary to identify and isolate key concepts within them. While manual annotation by experts can provide highly accurate results, it is time consuming and expensive. As such, there is growing interest in developing unsupervised algorithms that can automatically detect and localize relevant concepts in medical images without explicit human guidance. This study presents a comparative analysis of several state-of-the-art unsupervised approaches for concept detection in medical images. Our evaluation demonstrates that while each method possesses unique strengths, they all outperform traditional manual methods in terms of speed, accuracy, and efficiency. These findings suggest that unsupervised learning holds great promise as a valuable resource for enhancing disease diagnosis and improving patient care through advanced image interpretation.",1
"Recent works showed that Generative Adversarial Networks (GANs) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples. In particular, it was shown that a GAN objective function can be used to learn target features indistinguishable from the source ones. In this work, we extend this framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation. While data augmentation in the image space is a well established technique in deep learning, feature augmentation has not yet received the same level of attention. We accomplish it by means of a feature generator trained by playing the GAN minimax game against source features. Results show that both enforcing domain-invariance and performing feature augmentation lead to superior or comparable performance to state-of-the-art results in several unsupervised domain adaptation benchmarks.",0
"This paper presents a novel approach to unsupervised domain adaptation using adversarial feature augmentation. We propose two variants of our method: adversarial synthetic data generation (ASDG) and adversarial self-ensembling (ASE). ASDG generates new training samples by applying perturbations to real images from the source domain. These perturbations aim to fool both a discriminator network that distinguishes real vs generated samples, as well as a feature extractor used downstream. ASE augments existing labeled source samples by generating additional ""synthetic"" samples optimized to confuse the same discriminator/feature pair. Both methods can improve accuracy on the target task while reducing the need for annotated data. Experiments demonstrate state-of-the-art results on several benchmark datasets across multiple domains.",1
"Existing approaches towards single image dehazing including both model-based and learning-based heavily rely on the estimation of so-called transmission maps. Despite its conceptual simplicity, using transmission maps as an intermediate step often makes it more difficult to optimize the perceptual quality of reconstructed images. To overcome this weakness, we propose a direct deep learning approach toward image dehazing bypassing the step of transmission map estimation and facilitating end-to-end perceptual optimization. Our technical contributions are mainly three-fold. First, based on the analogy between dehazing and denoising, we propose to directly learn a nonlinear mapping from the space of degraded images to that of haze-free ones via recursive deep residual learning; Second, inspired by the success of generative adversarial networks (GAN), we propose to optimize the perceptual quality of dehazed images by introducing a discriminator and a loss function adaptive to hazy conditions; Third, we propose to remove notorious halo-like artifacts at large scene depth discontinuities by a novel application of guided filtering. Extensive experimental results have shown that the subjective qualities of dehazed images by the proposed perceptually optimized GAN (POGAN) are often more favorable than those by existing state-of-the-art approaches especially when hazy condition varies.",0
"This research presents a novel methodology for single image dehazing using generative adversarial networks (GANs). The proposed approach is based on perceptual optimization that maximizes both visual quality and photo-realism while minimizing distortions artifacts caused by haze. Unlike existing methods which rely solely on minimizing pixel-wise error measures, our model explicitly incorporates human perception into the optimization process through state-of-the-art deep neural network architectures such as StyleGAN2. Our experiments demonstrate significant improvements over current dehazing methods, achieving superior performance across multiple metrics including peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM) and visual inspection by subjective evaluation. Finally, we provide analysis comparing our results with other well-known GAN models trained under different settings highlighting key differences in terms of visual fidelity, color accuracy and atmospheric effect removal. Overall, this work represents a major step towards solving the challenges associated with automated single image dehazing, paving the path for future advancements in computer vision applications.",1
"Single image haze removal is a very challenging and ill-posed problem. The existing haze removal methods in literature, including the recently introduced deep learning methods, model the problem of haze removal as that of estimating intermediate parameters, viz., scene transmission map and atmospheric light. These are used to compute the haze-free image from the hazy input image. Such an approach only focuses on accurate estimation of intermediate parameters, while the aesthetic quality of the haze-free image is unaccounted for in the optimization framework. Thus, errors in the estimation of intermediate parameters often lead to generation of inferior quality haze-free images. In this paper, we present CANDY (Conditional Adversarial Networks based Dehazing of hazY images), a fully end-to-end model which directly generates a clean haze-free image from a hazy input image. CANDY also incorporates the visual quality of haze-free image into the optimization function; thus, generating a superior quality haze-free image. To the best of our knowledge, this is the first work in literature to propose a fully end-to-end model for single image haze removal. Also, this is the first work to explore the newly introduced concept of generative adversarial networks for the problem of single image haze removal. The proposed model CANDY was trained on a synthetically created haze image dataset, while evaluation was performed on challenging synthetic as well as real haze image datasets. The extensive evaluation and comparison results of CANDY reveal that it significantly outperforms existing state-of-the-art haze removal methods in literature, both quantitatively as well as qualitatively.",0
"Avoid summarizing each section; instead provide an overall summary that highlights key contributions, results, & limitations in context of previous work. Explain how this fits into current state of art. No more than two figures / tables allowed unless directly relevant to explaining methodology. No references in text please - just citation number after punctuation period. Use active voice, third person point of view throughout except first sentence which should read ""In recent years, single image haze removal has become an important topic."" If you need any other guidelines on writing the abstract let me know! If the word count exceeds max limit I recommend removing some minor details rather than major points described above. ---",1
"Generating images from natural language is one of the primary applications of recent conditional generative models. Besides testing our ability to model conditional, highly dimensional distributions, text to image synthesis has many exciting and practical applications such as photo editing or computer-aided content creation. Recent progress has been made using Generative Adversarial Networks (GANs). This material starts with a gentle introduction to these topics and discusses the existent state of the art models. Moreover, I propose Wasserstein GAN-CLS, a new model for conditional image generation based on the Wasserstein distance which offers guarantees of stability. Then, I show how the novel loss function of Wasserstein GAN-CLS can be used in a Conditional Progressive Growing GAN. In combination with the proposed loss, the model boosts by 7.07% the best Inception Score (on the Caltech birds dataset) of the models which use only the sentence-level visual semantics. The only model which performs better than the Conditional Wasserstein Progressive Growing GAN is the recently proposed AttnGAN which uses word-level visual semantics as well.",0
"Advances in deep learning have enabled new possibilities in computer vision such as generating images from text descriptions. This task poses unique challenges due to the large search space required to represent all possible image configurations that can result from any given text prompt. In this work, we present a method using generative adversarial networks (GANs) which can effectively synthesize high quality images from natural language descriptions while reducing computational complexity compared to previous methods. Our approach uses a conditional GAN architecture wherein two neural networks, generator and discriminator, are trained together in order to create a mapping between textual inputs and corresponding image outputs. We further utilize a novel attention mechanism within our generator network in order to selectively focus on different parts of the input description during training time. Experimental results demonstrate that our proposed method outperforms state-of-the-art approaches in both quantitative and qualitative measures of image fidelity and visual coherence.",1
"Image-to-image translation tasks have been widely investigated with Generative Adversarial Networks (GANs) and dual learning. However, existing models lack the ability to control the translated results in the target domain and their results usually lack of diversity in the sense that a fixed image usually leads to (almost) deterministic translation result. In this paper, we study a new problem, conditional image-to-image translation, which is to translate an image from the source domain to the target domain conditioned on a given image in the target domain. It requires that the generated image should inherit some domain-specific features of the conditional image from the target domain. Therefore, changing the conditional image in the target domain will lead to diverse translation results for a fixed input image from the source domain, and therefore the conditional input image helps to control the translation results. We tackle this problem with unpaired data based on GANs and dual learning. We twist two conditional translation models (one translation from A domain to B domain, and the other one from B domain to A domain) together for inputs combination and reconstruction while preserving domain independent features. We carry out experiments on men's faces from-to women's faces translation and edges to shoes&bags translations. The results demonstrate the effectiveness of our proposed method.",0
"Title: ""Conditional Image-to-Image Translation""  Abstract:  This research presents a novel approach to image translation using deep learning methods. We propose a conditional image-to-image translation model that takes into account contextual information during the translation process. Our method uses a generative adversarial network (GAN) architecture, which has been shown to produce high quality results in previous studies. However, our approach differs from existing GAN-based models by incorporating conditioning variables such as class labels or text descriptions. These additional inputs provide valuable guidance for the generator to create more realistic translations, leading to improved performance compared to baseline models without conditioning. In addition, we explore different ways of incorporating prior knowledge into the translation process through regularization techniques, demonstrating their effectiveness on several benchmark datasets. Finally, we showcase the potential applications of our proposed framework in areas such as style transfer, super-resolution, and domain adaptation. This study represents an important step towards developing intelligent systems capable of generating complex visual representations from simple input data.  Keywords: Image-to-Image Translation; Generative Adversarial Networks (GAN); Conditioning Variables; Regularization Techniques.",1
"We propose Human Pose Models that represent RGB and depth images of human poses independent of clothing textures, backgrounds, lighting conditions, body shapes and camera viewpoints. Learning such universal models requires training images where all factors are varied for every human pose. Capturing such data is prohibitively expensive. Therefore, we develop a framework for synthesizing the training data. First, we learn representative human poses from a large corpus of real motion captured human skeleton data. Next, we fit synthetic 3D humans with different body shapes to each pose and render each from 180 camera viewpoints while randomly varying the clothing textures, background and lighting. Generative Adversarial Networks are employed to minimize the gap between synthetic and real image distributions. CNN models are then learned that transfer human poses to a shared high-level invariant space. The learned CNN models are then used as invariant feature extractors from real RGB and depth frames of human action videos and the temporal variations are modelled by Fourier Temporal Pyramid. Finally, linear SVM is used for classification. Experiments on three benchmark cross-view human action datasets show that our algorithm outperforms existing methods by significant margins for RGB only and RGB-D action recognition.",0
"In recent years, there has been increased interest in using depth sensors to recognize human actions due to their low cost, easy set up, and non-intrusive nature. However, depth data can often suffer from noise and missing values, leading to poor accuracy in action recognition systems. To address this issue, we propose a novel approach that involves learning human pose models from synthesized data generated by computer graphics. Our method first generates a large dataset of virtual humans performing different actions, which is then used to train an action recognition system. We evaluate our approach on three public datasets and show that our method significantly outperforms state-of-the-art methods in terms of both accuracy and robustness to noisy data. Additionally, we demonstrate how our approach can generalize well across different types of input modalities such as skeleton joints, heatmaps, and keypoints. Overall, our work shows promise towards achieving high accuracy action recognition from depth images while being robust to sensor noise. This could potentially have significant impact in applications such as smart home automation, gaming, robotics, and healthcare monitoring.",1
"Low-dose computed tomography (CT) has attracted a major attention in the medical imaging field, since CT-associated x-ray radiation carries health risks for patients. The reduction of CT radiation dose, however, compromises the signal-to-noise ratio, and may compromise the image quality and the diagnostic performance. Recently, deep-learning-based algorithms have achieved promising results in low-dose CT denoising, especially convolutional neural network (CNN) and generative adversarial network (GAN). This article introduces a Contracting Path-based Convolutional Encoder-decoder (CPCE) network in 2D and 3D configurations within the GAN framework for low-dose CT denoising. A novel feature of our approach is that an initial 3D CPCE denoising model can be directly obtained by extending a trained 2D CNN and then fine-tuned to incorporate 3D spatial information from adjacent slices. Based on the transfer learning from 2D to 3D, the 3D network converges faster and achieves a better denoising performance than that trained from scratch. By comparing the CPCE with recently published methods based on the simulated Mayo dataset and the real MGH dataset, we demonstrate that the 3D CPCE denoising model has a better performance, suppressing image noise and preserving subtle structures.",0
"This paper presents a method for improving low-dose computed tomography (CT) images using a deep learning approach based on transfer learning from a pre-trained convolutional neural network (CNN). We propose a novel architecture called a ""3D convolutional encoder-decoder network"" that leverages both spatial and temporal features of the input data. Our model achieves state-of-the-art results on two publicly available datasets, outperforming other deep learning methods that have been previously proposed for this task. Furthermore, we demonstrate the effectiveness of our method through visual comparison and quantitative evaluation metrics such as peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and mean absolute difference (MAD). Overall, our method has the potential to improve clinical decision making by enhancing image quality while reducing radiation exposure to patients.",1
"Generative adversarial networks (GANs) have been extremely effective in approximating complex distributions of high-dimensional, input data samples, and substantial progress has been made in understanding and improving GAN performance in terms of both theory and application. However, we currently lack quantitative methods for model assessment. Because of this, while many GAN variants are being proposed, we have relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various types of GANs using divergence and distance functions typically used only for training. We observe consistency across the various proposed metrics and, interestingly, the test-time metrics do not favour networks that use the same training-time criterion. We also compare the proposed metrics to human perceptual scores.",0
"In recent years, Generative Adversarial Networks (GANs) have emerged as powerful tools for generating realistic images, audio signals, and other data types that are difficult to distinguish from their true counterparts. However, evaluating the quality of generated samples produced by different GAN architectures can be challenging due to subjective human judgment and lack of reliable quantitative metrics. This study proposes using divergence measures, which quantify the difference between two distributions, to evaluate the performance of GAN models trained on several datasets. We apply common GAN loss functions such as Jenson-Shannon (JS), Kullback-Leibler (KL), and symmetric cross entropy, and show how they relate to well-known criteria like adversarial robustness, sample fidelity, mode dropping, and density estimation errors. Our experimental results demonstrate that JS divergence outperforms traditional evaluation methods in terms of correlation with human judgment and capturing important properties of generator outputs. By providing quantifiable measures of GAN quality, our work paves the way for systematic improvement and comparison across diverse applications ranging from image editing to speech synthesis. Future research may consider refining divergence measures and developing dedicated criteria for specific use cases. Ultimately, we aim to contribute towards better understanding and efficient development of state-of-the-art generative models.",1
"Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.",0
"Mixup has been established as a powerful regularizer that improves generalization error by reducing model confidence on adversarially corrupted inputs. Recent work has shown Mixup to have similar benefits beyond image classification tasks, including sequence modeling and reinforcement learning. However, there is still limited understanding of why Mixup works empirically, both within and across different domains. In particular, existing explanations center around reducing overfitting due to memorization without providing a unified framework explaining how Mixup achieves the dual objective of promoting both diversity and accuracy. This paper presents Mixup: Beyond Empirical Risk Minimization, which bridges the gap between empiricism and rationality, casting Mixup in the light of minimax entropy, a risk measure capturing uncertainty under symmetric attacks. Our contributions are threefold. First, we propose a new algorithm to solve the Mixup problem directly under a well-defined formulation and prove its validity as a universal method. Second, we rigorously analyze the tradeoff between model confidence and robustness, identifying two separate regimes for Mixup where either property dominates depending on whether training data size is large enough or not. Third, we conduct extensive experiments verifying our theoretical results on multiple datasets spanning computer vision, natural language processing, and time series prediction, demonstrating broad applicability of Mixup and the competitive performance against related methods. Together, our findings provide insights into deep learning practice beyond empirical risk minimization and open up potential paths towards more principled machine learning algorithms.",1
"Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the Stein gradient estimator, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include meta-learning for approximate inference, and entropy regularised GANs that provide improved sample diversity.",0
"This article presents novel gradient estimators for implicit models that can effectively improve optimization efficiency by reducing computational overhead. With these efficient gradients, researchers have observed improved convergence rates as well as enhanced generalization performance in deep learning tasks such as image classification and language modeling. In addition, we empirically demonstrate how our gradient estimators can scale efficiently to large models and datasets without sacrificing accuracy. Our work contributes significantly to the current literature on machine learning and neural networks by providing computationally efficient techniques for optimizing complex models. By bridging the gap between theoretical advancements and practical applications, our results offer valuable insights into future directions for developing more powerful artificial intelligence systems. Overall, this study represents a major step forward in realizing the full potential of implicit models through smarter gradient estimation methods.",1
"Generative models (GMs) such as Generative Adversary Network (GAN) and Variational Auto-Encoder (VAE) have thrived these years and achieved high quality results in generating new samples. Especially in Computer Vision, GMs have been used in image inpainting, denoising and completion, which can be treated as the inference from observed pixels to corrupted pixels. However, images are hierarchically structured which are quite different from many real-world inference scenarios with non-hierarchical features. These inference scenarios contain heterogeneous stochastic variables and irregular mutual dependences. Traditionally they are modeled by Bayesian Network (BN). However, the learning and inference of BN model are NP-hard thus the number of stochastic variables in BN is highly constrained. In this paper, we adapt typical GMs to enable heterogeneous learning and inference in polynomial time.We also propose an extended autoregressive (EAR) model and an EAR with adversary loss (EARA) model and give theoretical results on their effectiveness. Experiments on several BN datasets show that our proposed EAR model achieves the best performance in most cases compared to other GMs. Except for black box analysis, we've also done a serial of experiments on Markov border inference of GMs for white box analysis and give theoretical results.",0
"This research proposes a novel generative model called ""HetGAN"" that enables heterogeneous inference across diverse modalities such as images, text, audio, etc. With advances in deep learning and artificial intelligence (AI), there has been growing interest in developing models capable of generating data from multiple domains. However, these existing approaches often lack robustness and scalability when faced with real-world applications that require multi-modal inputs. To address this challenge, our approach leverages adversarial training techniques combined with attention mechanisms to enhance cross-domain interactions and achieve better generalization performance. We evaluate HetGAN on several benchmark datasets spanning different modalities and demonstrate significant improvements over state-of-the-art methods, achieving new records on popular evaluation metrics. Our study showscases the effectiveness and versatility of our proposed model for tackling complex and diverse inferential tasks beyond single-modality settings.",1
"Person Re-identification (re-id) faces two major challenges: the lack of cross-view paired training data and learning discriminative identity-sensitive and view-invariant features in the presence of large pose variations. In this work, we address both problems by proposing a novel deep person image generation model for synthesizing realistic person images conditional on the pose. The model is based on a generative adversarial network (GAN) designed specifically for pose normalization in re-id, thus termed pose-normalization GAN (PN-GAN). With the synthesized images, we can learn a new type of deep re-id feature free of the influence of pose variations. We show that this feature is strong on its own and complementary to features learned with the original images. Importantly, under the transfer learning setting, we show that our model generalizes well to any new re-id dataset without the need for collecting any training data for model fine-tuning. The model thus has the potential to make re-id model truly scalable.",0
"This paper presents a method that uses pose normalization to improve image generation for person re-identification tasks. By projecting images into the pose space using a pre-trained model, we can generate new poses of known individuals, which helps increase the variability of available training data and improves performance on existing datasets. Our experiments show that this approach outperforms other methods for generating synthetic training data, resulting in better accuracy and robustness against changes in lighting conditions and camera views. Our work demonstrates the potential of pose normalization as a valuable tool for enhancing deep learning based solutions in computer vision applications such as person re-identification.",1
"Facial landmarks constitute the most compressed representation of faces and are known to preserve information such as pose, gender and facial structure present in the faces. Several works exist that attempt to perform high-level face-related analysis tasks based on landmarks. In contrast, in this work, an attempt is made to tackle the inverse problem of synthesizing faces from their respective landmarks. The primary aim of this work is to demonstrate that information preserved by landmarks (gender in particular) can be further accentuated by leveraging generative models to synthesize corresponding faces. Though the problem is particularly challenging due to its ill-posed nature, we believe that successful synthesis will enable several applications such as boosting performance of high-level face related tasks using landmark points and performing dataset augmentation. To this end, a novel face-synthesis method known as Gender Preserving Generative Adversarial Network (GP-GAN) that is guided by adversarial loss, perceptual loss and a gender preserving loss is presented. Further, we propose a novel generator sub-network UDeNet for GP-GAN that leverages advantages of U-Net and DenseNet architectures. Extensive experiments and comparison with recent methods are performed to verify the effectiveness of the proposed method.",0
"This abstract describes a new approach to generating faces using Generative Adversarial Networks (GANs). In recent years, GANs have become a popular method for synthesizing images, but traditional approaches often suffer from problems such as loss of identity during generation, meaning that generated faces may no longer resemble their original counterparts. Our proposed solution, called GP-GAN, addresses these issues by introducing a novel mapping mechanism which preserves the gender of input landmark points. Experiments show significant improvements over baseline methods in terms of visual quality and fidelity to the original face structure. Overall, GP-GAN represents a step forward in high-quality image generation.",1
"We applied Generative Adversarial Networks (GANs) to learn a model of DOOM levels from human-designed content. Initially, we analysed the levels and extracted several topological features. Then, for each level, we extracted a set of images identifying the occupied area, the height map, the walls, and the position of game objects. We trained two GANs: one using plain level images, one using both the images and some of the features extracted during the preliminary analysis. We used the two networks to generate new levels and compared the results to assess whether the network trained using also the topological features could generate levels more similar to human-designed ones. Our results show that GANs can capture intrinsic structure of DOOM levels and appears to be a promising approach to level generation in first person shooter games.",0
"Inspired by other generative models such as Midjourney \cite{Midjourney}, this paper presents a novel method that leverages Generative Adversarial Networks (GAN) \cite{Goodfellow2014} to generate randomized levels for the video game DOOM. Our approach uses two components: first, a generator network $G$ produces candidate level maps; secondly, a discriminator network $D$ evaluates these candidates based on their similarity to real DOOM levels according to some metric we define. This is performed via successive steps of mapping generation and refinement guided by adversarial loss $\mathcal{L}_g^{adv}$, perceptual loss $\mathcal{L}_p$, and structural loss $\mathcalics_s$. By optimizing both the generator and discriminator networks against each other in tandem and incorporating multiple types of feedback into our model, we aim to produce high quality procedurally generated levels that better match human designed content. Through extensive experimentation and evaluation, we demonstrate our system's ability to create diverse, playable DOOM levels that closely resemble those made by professional designers, thereby achieving state-of-the-art results among similar approaches. Overall, this work provides new insights into utilizing GAN architectures for computer graphics applications.",1
"In this paper, we introduce a new CT image denoising method based on the generative adversarial network (GAN) with Wasserstein distance and perceptual similarity. The Wasserstein distance is a key concept of the optimal transform theory, and promises to improve the performance of the GAN. The perceptual loss compares the perceptual features of a denoised output against those of the ground truth in an established feature space, while the GAN helps migrate the data noise distribution from strong to weak. Therefore, our proposed method transfers our knowledge of visual perception to the image denoising task, is capable of not only reducing the image noise level but also keeping the critical information at the same time. Promising results have been obtained in our experiments with clinical CT images.",0
"Radiologists are often tasked with interpreting medical images that contain artifacts caused by low dose radiation exposure during computed tomography (CT) scans. These noisy images can make diagnosis difficult and may require additional imaging which increases patient radiation dosage. In order to improve image quality while keeping radiation levels as low as possible, we propose using a deep learning approach based on generative adversarial networks (GANs). Our method utilizes both Wasserstein distance loss and perceptual loss in order to better denoise these low-dose CT images while preserving important features such as edges and fine details. We evaluate our model on publicly available datasets and compare it against other state-of-the-art methods, demonstrating its effectiveness at improving image quality without increasing radiation dose. This research has significant implications for healthcare institutions looking to provide high quality care while minimizing risk to patients from excessive radiation exposure.",1
"The task of face attribute manipulation has found increasing applications, but still remains challenging with the requirement of editing the attributes of a face image while preserving its unique details. In this paper, we choose to combine the Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN) for photorealistic image generation. We propose an effective method to modify a modest amount of pixels in the feature maps of an encoder, changing the attribute strength continuously without hindering global information. Our training objectives of VAE and GAN are reinforced by the supervision of face recognition loss and cycle consistency loss for faithful preservation of face details. Moreover, we generate facial masks to enforce background consistency, which allows our training to focus on manipulating the foreground face rather than background. Experimental results demonstrate our method, called Mask-Adversarial AutoEncoder (M-AAE), can generate high-quality images with changing attributes and outperforms prior methods in detail preservation.",0
"Include any relevant keywords from the manuscript and make it clear that the paper is intended for publication in the journal Computer Graphics Forum (CGF). In recent years, significant progress has been made in photorealistic face attribute manipulation using deep learning techniques, but these methods often lack control over the facial features outside of the edited regions due to their reliance on global feature representations. As a result, artifacts such as ghosting effects can occur when editing fine details like masks, making them difficult to apply practically without time consuming post-processing. To address this issue, we propose a new method for locally controlling facial attributes while preserving global fidelity by adaptively blending a photorealistic attribute layer into the underlying skin texture. Our approach achieves high quality results by exploiting the unique strengths of both local attribute synthesis and global image synthesis. We demonstrate the effectiveness of our algorithm through comprehensive experiments and evaluate its performance against several state-of-the-art attribute editing methods. This work makes important contributions to computer graphics research and would be well suited for publication in CGF. Keywords: Photorealism, Facial Attributes, Deep Learning, Ghosting Effects, Local Control, Adaptive Blending.",1
"This paper describes and evaluates the use of Generative Adversarial Networks (GANs) for path planning in support of smart mobility applications such as indoor and outdoor navigation applications, individualized wayfinding for people with disabilities (e.g., vision impairments, physical disabilities, etc.), path planning for evacuations, robotic navigations, and path planning for autonomous vehicles. We propose an architecture based on GANs to recommend accurate and reliable paths for navigation applications. The proposed system can use crowd-sourced data to learn the trajectories and infer new ones. The system provides users with generated paths that help them navigate from their local environment to reach a desired location. As a use case, we experimented with the proposed method in support of a wayfinding application in an indoor environment. Our experiments assert that the generated paths are correct and reliable. The accuracy of the classification task for the generated paths is up to 99% and the quality of the generated paths has a mean opinion score of 89%.",0
"This research focuses on exploring how generative adversarial networks can support path planning applications in smart mobility environments such as self driving cars and drones. GANs have recently emerged as powerful machine learning tools that show promise in generating high fidelity samples from complex distributions which could improve safety and efficiency of these systems by enabling better decision making during route selection, collision detection etc. This work proposes a novel algorithm based on Divergence GANs (DG-GAN) to generate multiple feasible paths through urban environments while taking into account constraints like speed limits, traffic density etc. Results of simulations done on real world maps show significant improvement over state-of-the-art methods where our approach resulted in more diverse set of plausible paths which were closer to ground truth under metrics like accuracy (APE), path length and smoothness. As future work we plan to extend this framework to work in dynamic environment changes such as accidents/traffic events so that decisions taken can adapt accordingly resulting in even safer transportation modes. The proposed algorithm uses discriminators trained to classify whether the generated path was made by human or by computer. These discriminators are then used to train the generator to create diverse sets of plausible paths while satisfying the desired criteria specified by users. The results presented here demonstrate improved path diversity and fidelity relative to existing approaches evaluated by Average Pairwise Error (APE). Furthermore, simulations conducted with realworld streetmaps illustrate improvements in terms of path quality (smoothness and compliance with constraints). While DG-GAN represents a leap forward in automating path generation for smart vehicles, there remains significant room for refinement. Specifically, extension towards adaptation within time-sensitive scenarios would greatly improve overall system performance in highly dynamic environments, including emergency response situations, unexpected events like accidents and so forth. Nonetheless, initial trials yield encouraging results indicating substantial potential impacts for smart vehicle technologies across several industries given successful integration and deployment.",1
"Hashing has been a widely-adopted technique for nearest neighbor search in large-scale image retrieval tasks. Recent research has shown that leveraging supervised information can lead to high quality hashing. However, the cost of annotating data is often an obstacle when applying supervised hashing to a new domain. Moreover, the results can suffer from the robustness problem as the data at training and test stage could come from similar but different distributions. This paper studies the exploration of generating synthetic data through semi-supervised generative adversarial networks (GANs), which leverages largely unlabeled and limited labeled training data to produce highly compelling data with intrinsic invariance and global coherence, for better understanding statistical structures of natural data. We demonstrate that the above two limitations can be well mitigated by applying the synthetic data for hashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is presented, which mainly consists of four components: a deep convolution neural networks (CNN) for learning image representations, an adversary stream to distinguish synthetic images from real ones, a hash stream for encoding image representations to hash codes and a classification stream. The whole architecture is trained end-to-end by jointly optimizing three losses, i.e., adversarial loss to correct label of synthetic or real for each sample, triplet ranking loss to preserve the relative similarity ordering in the input real-synthetic triplets and classification loss to classify each sample accurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image benchmarks validate the capability of exploiting synthetic images for hashing. Our framework also achieves superior results when compared to state-of-the-art deep hash models.",0
"This is an article about deep semantic hashing using generative adversarial networks (GAN). In recent years, GANs have emerged as powerful tools for generating synthetic data that can mimic real-world distributions with high fidelity. By leveraging the expressive power of GANs, we propose a novel technique called Deep Semantic Hashing (DSL), which learns hash functions that capture semantically meaningful relationships between images. We demonstrate through extensive experiments that our approach outperforms state-of-the-art methods on several benchmark datasets across multiple tasks such as image retrieval, classification, and clustering. Our code will be made publicly available upon acceptance so others may build upon our work. Overall, DSL represents a significant step forward in applying machine learning techniques to solve problems related to information management and organization within large collections of visual data.",1
"We are creating multimedia contents everyday and everywhere. While automatic content generation has played a fundamental challenge to multimedia community for decades, recent advances of deep learning have made this problem feasible. For example, the Generative Adversarial Networks (GANs) is a rewarding approach to synthesize images. Nevertheless, it is not trivial when capitalizing on GANs to generate videos. The difficulty originates from the intrinsic structure where a video is a sequence of visually coherent and semantically dependent frames. This motivates us to explore semantic and temporal coherence in designing GANs to generate videos. In this paper, we present a novel Temporal GANs conditioning on Captions, namely TGANs-C, in which the input to the generator network is a concatenation of a latent noise vector and caption embedding, and then is transformed into a frame sequence with 3D spatio-temporal convolutions. Unlike the naive discriminator which only judges pairs as fake or real, our discriminator additionally notes whether the video matches the correct caption. In particular, the discriminator network consists of three discriminators: video discriminator classifying realistic videos from generated ones and optimizes video-caption matching, frame discriminator discriminating between real and fake frames and aligning frames with the conditioning caption, and motion discriminator emphasizing the philosophy that the adjacent frames in the generated videos should be smoothly connected as in real ones. We qualitatively demonstrate the capability of our TGANs-C to generate plausible videos conditioning on the given captions on two synthetic datasets (SBMG and TBMG) and one real-world dataset (MSVD). Moreover, quantitative experiments on MSVD are performed to validate our proposal via Generative Adversarial Metric and human study.",0
"""This"" should always be lower case if at all possible.) ```sql Captioning videos is becoming increasingly popular as a means to provide accessibility for deaf and hard-of-hearing individuals. In addition to serving as a tool for accessibility, video caption data can be leveraged to generate new videos that match the provided descriptions. This process has potential applications in content creation and automation tasks such as editing and localization, allowing creators and users alike to quickly produce high quality visual media.  To achieve this task, we first collect a dataset containing video clips paired with their corresponding textual descriptions. We then use a neural network trained on this data to translate natural language descriptions into source code for a video synthesis model. After generating code for any given description, our system produces video frames using an autoencoder architecture. Finally, these individual frames are combined and rendered into a single coherent output video matching the input description. Experiments demonstrate that our method generates plausible video scenes that closely resemble the desired content described in the original caption. By automating the video generation pipeline, our approach enables efficient production of accurate content across a wide range of domains.  Overall, our work shows promising results towards achieving the goal of creating videos directly from textual descriptions. As further research in this area progresses, we envision even greater advancements towards democratizing multimedia content creation. Our hope is that by streamlining the production process through machine learning techniques like those proposed here, more individuals will have the opportunity to share their unique perspectives and stories without facing barriers associated with traditional content creation workflows. ``` The paper ""To Create Wha",1
"Firefighters suffer a variety of life-threatening risks, including line-of-duty deaths, injuries, and exposures to hazardous substances. Support for reducing these risks is important. We built a partially occluded object reconstruction method on augmented reality glasses for first responders. We used a deep learning based on conditional generative adversarial networks to train associations between the various images of flammable and hazardous objects and their partially occluded counterparts. Our system then reconstructed an image of a new flammable object. Finally, the reconstructed image was superimposed on the input image to provide ""transparency"". The system imitates human learning about the laws of physics through experience by learning the shape of flammable objects and the flame characteristics.",0
"In emergency situations such as natural disasters, fire incidents, and terrorist attacks, first responders face significant challenges when searching for victims trapped inside damaged buildings or under debris. One major obstacle they encounter is that piles of rubble can often block their view of potential victims, making it difficult to determine who may need assistance. Augmented reality (AR) technology has shown promising results for improving rescue operations by providing real-time visualizations of occluded objects hidden behind physical barriers. However, current AR systems still have limitations due to factors such as incomplete data, sensor noise, and imperfect modeling assumptions. This research addresses these issues by proposing a novel approach for reconstructing occluded objects through the use of conditional generative adversarial networks (cGANs). Our method combines publicly available data sets and new simulations from physics engines to train cGAN models capable of generating high fidelity occlusion maps. These maps allow for accurate estimates of victim locations even when obscured by large amounts of rubble or other obstructions. Using feedback from real-world experts in search and rescue operations, we developed a software prototype that demonstrates our approach integrated within AR glasses commonly used by first responders. Experiments show that our system significantly enhances situation awareness during rescue missions compared to existing methods while maintaining minimal computational demands on the wearable device. Ultimately, our work represents an important step towards safer and more effective emergency response efforts worldwide.",1
"Generating a novel image by manipulating two input images is an interesting research problem in the study of generative adversarial networks (GANs). We propose a new GAN-based network that generates a fusion image with the identity of input image x and the shape of input image y. Our network can simultaneously train on more than two image datasets in an unsupervised manner. We define an identity loss LI to catch the identity of image x and a shape loss LS to get the shape of y. In addition, we propose a novel training method called Min-Patch training to focus the generator on crucial parts of an image, rather than its entirety. We show qualitative results on the VGG Youtube Pose dataset, Eye dataset (MPIIGaze and UnityEyes), and the Photo-Sketch-Cartoon dataset.",0
"This paper explores the concept of identity and how it relates to image fusion techniques. Using literature review and case studies, we investigate how different identities can shape the perception and interpretation of fused images. We discuss how factors such as cultural background, gender, age, education level, occupation, religion, politics, and social class influence oneâ€™s understanding of visual data. In addition, we examine how these differences affect individuals' preferences regarding image fusion algorithms, their selection criteria, and desired outputs. Finally, we provide recommendations on how to create more inclusive image fusion systems that accommodate diverse user needs and expectations. Overall, our findings highlight the importance of considering individual perspectives and identity in developing effective image fusion technologies.",1
"Intra-operative measurement of tissue oxygen saturation (StO2) has been widely explored by pulse oximetry or hyperspectral imaging (HSI) to assess the function and viability of tissue. In this paper we propose a pixel- level image-to-image translation approach based on conditional Generative Adversarial Networks (cGAN) to estimate tissue oxygen saturation (StO2) directly from RGB images. The real-time performance and non-reliance on additional hardware, enable a seamless integration of the proposed method into surgical and diagnostic workflows with standard endoscope systems. For validation, RGB images and StO2 ground truth were simulated and estimated from HSI images collected by a liquid crystal tuneable filter (LCTF) endoscope for three tissue types (porcine bowel, lamb uterus and rabbit uterus). The result show that the proposed method can achieve visually identical images with comparable accuracy.",0
"This paper presents a new method for estimating tissue oxygen saturation (StO2) levels using near-infrared diffuse reflectance spectroscopy via red-green-blue (RGB) images obtained by camera sensors, which has been shown to improve accuracy compared to traditional methods that use a single-wavelength device. We propose a pixel-wise translation approach to predict StO2 maps from input RGB images, where we train a generative adversarial network (GAN) model to generate synthetic StO2 maps conditioned on pairs of real RGB and ground truth StO2 reference images. Experimental results show that our proposed method achieves higher accuracy than previous RGB-based models and outperforms single-wavelength devices when evaluated against measurements taken by optical spectrometry. Additionally, we demonstrate the effectiveness of utilizing multiple wavelengths present in ambient lighting conditions by comparing performance between RGB and monochrome imaging setups. Overall, these findings highlight the potential for deep learning approaches like GANs to effectively estimate StO2 in vivo through noninvasive means. Our work contributes to the growing field of medical image analysis techniques and has important implications for diagnosing health conditions related to changes in tissue oxygenation.",1
"In this paper, we present an integrated system for automatically generating and editing face images through face swapping, attribute-based editing, and random face parts synthesis. The proposed system is based on a deep neural network that variationally learns the face and hair regions with large-scale face image datasets. Different from conventional variational methods, the proposed network represents the latent spaces individually for faces and hairs. We refer to the proposed network as region-separative generative adversarial network (RSGAN). The proposed network independently handles face and hair appearances in the latent spaces, and then, face swapping is achieved by replacing the latent-space representations of the faces, and reconstruct the entire face image with them. This approach in the latent space robustly performs face swapping even for images which the previous methods result in failure due to inappropriate fitting or the 3D morphable models. In addition, the proposed system can further edit face-swapped images with the same network by manipulating visual attributes or by composing them with randomly generated face or hair parts.",0
"In recent years, face generation has become increasingly popular due to advances in generative adversarial networks (GANs). One particular model that has gained attention is StarGAN which allows for multiple attribute editing on one input image. However, there are limitations to this method as it can only perform single image manipulation and cannot swap faces from different identities onto a new image. This paper presents a novel approach called Relational Similarity GAN (RSGAN) which addresses these issues by utilizing latent spaces to represent both facial features and hair styles. Through experiments, we demonstrate that our proposed model outperforms previous methods in terms of visual quality and robustness to changes in hair style. Additionally, we showcase its ability to accurately manipulate faces across images while preserving their identity. Our work opens up new possibilities for research into multi-image manipulations such as scene synthesis and video retargetting. Overall, we believe that RSGAN serves as a step forward towards realistic digital media creation.",1
"The deep generative adversarial networks (GAN) recently have been shown to be promising for different computer vision applications, like image edit- ing, synthesizing high resolution images, generating videos, etc. These networks and the corresponding learning scheme can handle various visual space map- pings. We approach GANs with a novel training method and learning objective, to discover multiple object instances for three cases: 1) synthesizing a picture of a specific object within a cluttered scene; 2) localizing different categories in images for weakly supervised object detection; and 3) improving object discov- ery in object detection pipelines. A crucial advantage of our method is that it learns a new deep similarity metric, to distinguish multiple objects in one im- age. We demonstrate that the network can act as an encoder-decoder generating parts of an image which contain an object, or as a modified deep CNN to rep- resent images for object detection in supervised and weakly supervised scheme. Our ranking GAN offers a novel way to search through images for object specific patterns. We have conducted experiments for different scenarios and demonstrate the method performance for object synthesizing and weakly supervised object detection and classification using the MS-COCO and PASCAL VOC datasets.",0
"This is how I would write the Abstract without using the Title: The paper proposes a novel method for weakly supervised object discovery through the use of generative adversarial networks (GAN) and ranking networks. By leveraging the strengths of both GANs and rankings networks, our approach allows us to effectively learn from limited labeled data while maintaining high accuracy. Our experiments demonstrate that our method outperforms state-of-the-art methods across multiple datasets, making it a promising tool for real world applications such as image classification and semantic segmentation. -----",1
"Data of different modalities generally convey complimentary but heterogeneous information, and a more discriminative representation is often preferred by combining multiple data modalities like the RGB and infrared features. However in reality, obtaining both data channels is challenging due to many limitations. For example, the RGB surveillance cameras are often restricted from private spaces, which is in conflict with the need of abnormal activity detection for personal security. As a result, using partial data channels to build a full representation of multi-modalities is clearly desired. In this paper, we propose a novel Partial-modal Generative Adversarial Networks (PM-GANs) that learns a full-modal representation using data from only partial modalities. The full representation is achieved by a generated representation in place of the missing data channel. Extensive experiments are conducted to verify the performance of our proposed method on action recognition, compared with four state-of-the-art methods. Meanwhile, a new Infrared-Visible Dataset for action recognition is introduced, and will be the first publicly available action dataset that contains paired infrared and visible spectrum.",0
"This paper presents a novel approach to action recognition using generative adversarial networks (GANs). We introduce partial-modality GANs (PM-GANs) that learn discriminative representations by predicting missing modalities from incomplete observations. Our method leverages multiple modalities such as RGB video frames, optical flow fields, and depth maps, which provide complementary information for recognizing human actions. By formulating action recognition as a multi-task learning problem, our model learns jointly across different tasks while capturing their inherent correlations. Experimental results on two benchmark datasets demonstrate the effectiveness of our approach in improving action recognition accuracy compared to state-of-the-art methods. We believe this work paves the way towards developing more robust and effective models for complex spatio-temporal reasoning problems.",1
"State-of-the-art pedestrian detection models have achieved great success in many benchmarks. However, these models require lots of annotation information and the labeling process usually takes much time and efforts. In this paper, we propose a method to generate labeled pedestrian data and adapt them to support the training of pedestrian detectors. The proposed framework is built on the Generative Adversarial Network (GAN) with multiple discriminators, trying to synthesize realistic pedestrians and learn the background context simultaneously. To handle the pedestrians of different sizes, we adopt the Spatial Pyramid Pooling (SPP) layer in the discriminator. We conduct experiments on two benchmarks. The results show that our framework can smoothly synthesize pedestrians on background images of variations and different levels of details. To quantitatively evaluate our approach, we add the generated samples into training data of the baseline pedestrian detectors and show the synthetic images are able to improve the detectors' performance.",0
"This paper presents a new approach for generating pedestrian data that combines both real scene synthesis and advanced machine learning techniques. Our method, called Pedestrian-Synthesis-GAN (PS-GAN), integrates generative adversarial networks (GANs) into traditional image generation models. By doing so, we can generate high-quality, diverse, and natural looking pedestrian data in real scenes while considering safety factors such as occlusion and visibility constraints. PS-GAN has been validated through extensive experiments on public datasets such as Cityscapes and L2A, achieving state-of-the-art results compared to other GAN approaches and traditional methods. We believe that our proposed model will lead to more accurate object detection algorithms, paving the way for safer autonomous driving systems.",1
"Synthesizing realistic images from human drawn sketches is a challenging problem in computer graphics and vision. Existing approaches either need exact edge maps, or rely on retrieval of existing photographs. In this work, we propose a novel Generative Adversarial Network (GAN) approach that synthesizes plausible images from 50 categories including motorcycles, horses and couches. We demonstrate a data augmentation technique for sketches which is fully automatic, and we show that the augmented data is helpful to our task. We introduce a new network building block suitable for both the generator and discriminator which improves the information flow by injecting the input image at multiple scales. Compared to state-of-the-art image translation methods, our approach generates more realistic images and achieves significantly higher Inception Scores.",0
"This sounds like an interesting research project focused on generating images from sketches using artificial intelligence. Here's a potential abstract that could accompany such a paper:  Advancements in generative models have enabled the creation of high-fidelity synthetic imagery across many domains. However, current approaches often struggle to generate diverse outputs that can match the intricate details present in real images, especially when working from simple input sketches. To address these limitations, we propose SketchyGAN - a novel deep learning architecture that leverages the strengths of both generator and discriminator networks for producing more varied and photorealistic output images given rough sketch inputs. Our approach builds upon recent advances in GAN training by incorporating attention modules in the generator network which allow it to focus on specific regions of the image while conditioning on input sketches. We demonstrate significant improvements over prior state-of-the-art methods on three challenging datasets for sketch-to-image synthesis, providing quantitative evaluations as well as visual comparisons against other leading techniques. We hope our work serves as a stepping stone towards broader applications for AI-generated content creation based on user input and guidance.",1
"Domain Adaptation is an actively researched problem in Computer Vision. In this work, we propose an approach that leverages unsupervised data to bring the source and target distributions closer in a learned joint feature space. We accomplish this by inducing a symbiotic relationship between the learned embedding and a generative adversarial network. This is in contrast to methods which use the adversarial framework for realistic data generation and retraining deep models with such data. We demonstrate the strength and generality of our approach by performing experiments on three different tasks with varying levels of difficulty: (1) Digit classification (MNIST, SVHN and USPS datasets) (2) Object recognition using OFFICE dataset and (3) Domain adaptation from synthetic to real data. Our method achieves state-of-the art performance in most experimental settings and by far the only GAN-based method that has been shown to work well across different datasets such as OFFICE and DIGITS.",0
"In recent years, transfer learning has become increasingly important in domains such as computer vision, natural language processing, and robotics. One common approach to transfer learning involves aligning the source domain, which contains abundant data but might not match the target domain where we want to apply our models, and the target domain through some form of feature extraction or parameter adaptation. However, directly applying these methods can lead to suboptimal results due to their reliance on manual features or heuristics.  In this paper, we propose a novel method called ""Generate To Adapt"" (GTA), which uses generative adversarial networks (GANs) to automatically generate synthetic examples that bridge the gap between the source and target domains. By training GANs on both the source and target datasets, we learn a latent space that captures the underlying structures of both domains. We then use a cycle consistency loss to ensure that generated samples are semantically consistent across domains. Finally, we adapt the pre-trained model from the source domain to the target domain by minimizing the distance between their respective representations in the learned latent spaces.  We evaluate our proposed method on several benchmark datasets for computer vision tasks including digit classification, object recognition, and scene understanding. Our results show significant improvements over state-of-the-art baseline methods across all tasks, demonstrating the effectiveness of our approach.  Overall, our work advances the field of transfer learning by providing a new framework based on GANs that bridges the source and target domains effectively while mitigating sample complexity issues. We hope this work encourages future research into automated alignment techniques and inspires innovation in the broader field of machine learning.",1
"Suffering from the extreme training data imbalance between seen and unseen classes, most of existing state-of-the-art approaches fail to achieve satisfactory results for the challenging generalized zero-shot learning task. To circumvent the need for labeled examples of unseen classes, we propose a novel generative adversarial network (GAN) that synthesizes CNN features conditioned on class-level semantic information, offering a shortcut directly from a semantic descriptor of a class to a class-conditional feature distribution. Our proposed approach, pairing a Wasserstein GAN with a classification loss, is able to generate sufficiently discriminative CNN features to train softmax classifiers or any multimodal embedding method. Our experimental results demonstrate a significant boost in accuracy over the state of the art on five challenging datasets -- CUB, FLO, SUN, AWA and ImageNet -- in both the zero-shot learning and generalized zero-shot learning settings.",0
"Title: Feature Generating Networks for Zero-Shot Learning  Abstract: In recent years, zero-shot learning has emerged as a promising approach for tackling the problem of few-shot classification by enabling machines to learn from data they have never seen before. In essence, zero-shot learning involves training models on large amounts of unlabeled data and fine-tuning them using small labeled datasets to perform well on new tasks without requiring further explicit labeling. However, traditional approaches often struggle to achieve optimal performance due to limitations in feature extraction techniques that fail to capture complex relationships among data points. Therefore, we propose a novel methodology based on feature generating networks (FGNs), which aim to bridge the gap between raw data features and high-level semantic representations. Our approach leverages advancements in deep learning architectures to extract effective features from input data while maintaining interpretability through attention mechanisms and causal reasoning components. Experimental results demonstrate significant improvements over state-of-the-art methods across several benchmark datasets for zero-shot learning problems, showcasing the effectiveness of our proposed solution. Overall, FGNs provide a powerful alternative strategy for handling real-world scenarios involving limited labeled data, thus paving the way towards more generalizable and adaptive artificial intelligence systems.",1
"Mode collapse is a critical problem in training generative adversarial networks. To alleviate mode collapse, several recent studies introduce new objective functions, network architectures or alternative training schemes. However, their achievement is often the result of sacrificing the image quality. In this paper, we propose a new algorithm, namely a manifold guided generative adversarial network (MGGAN), which leverages a guidance network on existing GAN architecture to induce generator learning all modes of data distribution. Based on extensive evaluations, we show that our algorithm resolves mode collapse without losing image quality. In particular, we demonstrate that our algorithm is easily extendable to various existing GANs. Experimental analysis justifies that the proposed algorithm is an effective and efficient tool for training GANs.",0
"Artificial Intelligence (AI) has been making rapid advances in recent years due to advancements in deep neural networks. However, one issue that continues to plague these models is mode collapse, where the model overly relies on one solution rather than exploring other possibilities. This results in reduced diversity and poor performance in certain tasks such as image generation. To address this problem, researchers have proposed methods based on data augmentation, regularization techniques, and energy-based objectives, but each approach comes with its own limitations and challenges. In this study, we present MGGAN, a novel method to solve mode collapse by utilizing manifold guided training. Our approach combines data augmentation and manifold learning to guide the generator network towards more diverse solutions while maintaining high fidelity to the original distribution. We evaluate our method through extensive experiments on three benchmark datasets and demonstrate improvements over current state-of-the art approaches. Our findings show that MGGAN significantly reduces mode collapse and increases generative diversity leading to improved visual quality and higher FID scores. Additionally, we provide comprehensive ablation studies and sensitivity analysis to further support our claims. Overall, MGGAN presents an effective solution for mode collapse and serves as a significant contribution to the field of generative adversarial networks.",1
"Computationally synthesized blood vessels can be used for training and evaluation of medical image analysis applications. We propose a deep generative model to synthesize blood vessel geometries, with an application to coronary arteries in cardiac CT angiography (CCTA).   In the proposed method, a Wasserstein generative adversarial network (GAN) consisting of a generator and a discriminator network is trained. While the generator tries to synthesize realistic blood vessel geometries, the discriminator tries to distinguish synthesized geometries from those of real blood vessels. Both real and synthesized blood vessel geometries are parametrized as 1D signals based on the central vessel axis. The generator can optionally be provided with an attribute vector to synthesize vessels with particular characteristics.   The GAN was optimized using a reference database with parametrizations of 4,412 real coronary artery geometries extracted from CCTA scans. After training, plausible coronary artery geometries could be synthesized based on random vectors sampled from a latent space. A qualitative analysis showed strong similarities between real and synthesized coronary arteries. A detailed analysis of the latent space showed that the diversity present in coronary artery anatomy was accurately captured by the generator.   Results show that Wasserstein generative adversarial networks can be used to synthesize blood vessel geometries.",0
"In recent years, deep learning has emerged as a powerful tool for image generation tasks such as generating synthetic images from natural language descriptions, hallucinating new samples in existing data distributions, and interpolating or extrapolating within image datasets. One important application area for image generation is medical imaging, where artificially generated high quality blood vessel geometries could enable physicians to analyze complex anatomies more accurately without resorting to risky exploratory surgeries.  This paper presents a novel method for generating 2D and 3D realistic blood vessel geometries using generative adversarial networks (GANs). Our approach overcomes several limitations common to existing methods by incorporating both local and global information into the generator network, enabling accurate capture of intricate vascular structures and their relationships across different scales.  The proposed architecture learns two separate discriminators: one that evaluates spatial details like small branches and junctions locally, and another that captures larger scale features such as overall shape complexity and fidelity to ground truth geometry. This separation allows our model to balance attention between these conflicting aspects during training, ensuring state-of-the-art performance on multiple evaluation metrics. We demonstrate that this framework can generate highly realistic blood vessels at significantly lower computational cost than previous models while maintaining similar image quality. By exploiting GANsâ€™ unique ability to generate diverse yet valid samples, we further showcase potential clinical applications of such realistic anatomic simulations, such as planning optimal surgical approaches and estimating flow patterns under distinct hemodynamic conditions. Overall, our work marks an important step toward leveraging machine intelligence to improve patient outcomes through personalized computer simulations.",1
"Magnetic Resonance Angiography (MRA) has become an essential MR contrast for imaging and evaluation of vascular anatomy and related diseases. MRA acquisitions are typically ordered for vascular interventions, whereas in typical scenarios, MRA sequences can be absent in the patient scans. This motivates the need for a technique that generates inexistent MRA from existing MR multi-contrast, which could be a valuable tool in retrospective subject evaluations and imaging studies. In this paper, we present a generative adversarial network (GAN) based technique to generate MRA from T1-weighted and T2-weighted MRI images, for the first time to our knowledge. To better model the representation of vessels which the MRA inherently highlights, we design a loss term dedicated to a faithful reproduction of vascularities. To that end, we incorporate steerable filter responses of the generated and reference images inside a Huber function loss term. Extending the well- established generator-discriminator architecture based on the recent PatchGAN model with the addition of steerable filter loss, the proposed steerable GAN (sGAN) method is evaluated on the large public database IXI. Experimental results show that the sGAN outperforms the baseline GAN method in terms of an overlap score with similar PSNR values, while it leads to improved visual perceptual quality.",0
"In recent years, Magnetic Resonance Angiography (MRA) has become an important tool for vascular imaging due to its noninvasive nature and ability to provide detailed images of blood vessels. However, acquiring high quality MRAs often requires multiple contrast injections and prolonged scan times which can be uncomfortable and difficult for patients. To address these limitations, we propose using generative adversarial training to synthesize multi-contrast MRA images from single acquisition scans. Our method leverages deep learning techniques to generate realistic images that closely resemble those acquired through traditional methods while reducing both the number of required contrast agents and scan time. We demonstrate the effectiveness of our approach by comparing generated images against ground truth data on both qualitative and quantitative metrics. Furthermore, we show potential clinical applications of our technology in the diagnosis of cerebrovascular diseases such as intracranial stenosis and arteriovenous malformations. Overall, our work represents a significant step towards improving the feasibility and accessibility of MRA for patient care.",1
"It is well-known that GANs are difficult to train, and several different techniques have been proposed in order to stabilize their training. In this paper, we propose a novel training method called manifold-matching, and a new GAN model called manifold-matching GAN (MMGAN). MMGAN finds two manifolds representing the vector representations of real and fake images. If these two manifolds match, it means that real and fake images are statistically identical. To assist the manifold-matching task, we also use i) kernel tricks to find better manifold structures, ii) moving-averaged manifolds across mini-batches, and iii) a regularizer based on correlation matrix to suppress mode collapse.   We conduct in-depth experiments with three image datasets and compare with several state-of-the-art GAN models. 32.4% of images generated by the proposed MMGAN are recognized as fake images during our user study (16% enhancement compared to other state-of-the-art model). MMGAN achieved an unsupervised inception score of 7.8 for CIFAR-10.",0
"Recent years have witnessed significant progress in deep learning techniques that generate high-quality images from text descriptions. Despite their impressive performance, these approaches often suffer from several drawbacks such as limited diversity and inflexibility in generating new concepts outside the seen during training. To address these limitations, we propose MMGAN, a novel generative adversarial network architecture that leverages a manifold matching strategy to improve the quality and coherence of generated samples while ensuring greater flexibility in generating diverse and previously unseen image samples. Our framework incorporates two key components: (i) a discriminator that uses a multi-scale representation to capture both local and global contextual information; and (ii) a generator equipped with a latent attention module that dynamically adjusts attention weights based on input descriptors. We demonstrate through comprehensive experiments that our approach outperforms state-of-the-art methods across various benchmark datasets, achieving superior generation quality, coherency, and visual fidelity. Overall, our work highlights the effectiveness of using a manifold matching strategy within GAN architectures for improved image synthesis from text inputs.",1
"Generative Adversarial Networks (GANs) have been successfully used to synthesize realistically looking images of faces, scenery and even medical images. Unfortunately, they usually require large training datasets, which are often scarce in the medical field, and to the best of our knowledge GANs have been only applied for medical image synthesis at fairly low resolution. However, many state-of-the-art machine learning models operate on high resolution data as such data carries indispensable, valuable information. In this work, we try to generate realistically looking high resolution images of skin lesions with GANs, using only a small training dataset of 2000 samples. The nature of the data allows us to do a direct comparison between the image statistics of the generated samples and the real dataset. We both quantitatively and qualitatively compare state-of-the-art GAN architectures such as DCGAN and LAPGAN against a modification of the latter for the task of image generation at a resolution of 256x256px. Our investigation shows that we can approximate the real data distribution with all of the models, but we notice major differences when visually rating sample realism, diversity and artifacts. In a set of use-case experiments on skin lesion classification, we further show that we can successfully tackle the problem of heavy class imbalance with the help of synthesized high resolution melanoma samples.",0
"Here is a possible abstract that meets your requirements: ```scss In recent years, Generative Adversarial Networks (GANs) have been used to generate synthetic images with remarkable realism. However, most previous work has focused on generating large, high resolution images such as faces, landscapes, and objects. In contrast, medical imaging often involves smaller regions of interest at lower resolutions, making it challenging to train GANs effectively. This study addresses these limitations by proposing a new approach called MelanoGANs, which specializes in generating highly detailed, high resolution skin lesions using GANs. By leveraging advances in both computer vision and dermatology, we show how our method outperforms state-of-the-art methods in terms of visual fidelity, image quality, and interpretability. Our results demonstrate the potential of MelanoGANs for a wide range of applications within the healthcare sector, including education, simulation, and diagnosis support. Ultimately, our findings provide important insights into the use of machine learning algorithms for generating medically relevant images. ```",1
"Magnetic Resonance Imaging (MRI) of the brain has been used to investigate a wide range of neurological disorders, but data acquisition can be expensive, time-consuming, and inconvenient. Multi-site studies present a valuable opportunity to advance research by pooling data in order to increase sensitivity and statistical power. However images derived from MRI are susceptible to both obvious and non-obvious differences between sites which can introduce bias and subject variance, and so reduce statistical power. To rectify these differences, we propose a data driven approach using a deep learning architecture known as generative adversarial networks (GANs). GANs learn to estimate two distributions, and can then be used to transform examples from one distribution into the other distribution. Here we transform T1-weighted brain images collected from two different sites into MR images from the same site. We evaluate whether our model can reduce site-specific differences without loss of information related to gender (male, female) or clinical diagnosis (schizophrenia, bipolar disorder, healthy). When trained appropriately, our model is able to normalise imaging sets to a common scanner set with less information loss compared to current approaches. An important advantage is our method can be treated as a black box that does not require any knowledge of the sources of bias but only needs at least two distinct imaging sets.",0
"Title: ""Correcting Differences in Multi-Site Neuroimaging Data Using Generative Adversarial Networks""  Abstract: Neuroimaging has become an essential tool in understanding brain function and behavior across different populations. However, due to variations in acquisition parameters, image processing pipelines, and hardware configurations among multiple sites, there may exist significant differences in the acquired images. These inconsistencies can lead to difficulties in harmonizing and integrating data from different sources, thus hindering scientific progress. In this study, we propose a novel approach that utilizes generative adversarial networks (GANs) to address these discrepancies by learning the underlying patterns present in each site's dataset. Our method generates new synthetic images that effectively blend information from all participating imaging centers while preserving local variability within individual datasets. We evaluate our framework through extensive experiments on publicly available neuroimaging datasets and demonstrate improved alignment and consensus clustering performance compared to traditional methods such as linear registration and mutual-information based normalization. This work provides promising insights into enhancing data consistency across multiple neuroimaging sites, paving the way towards more robust and generalized research findings.",1
"Melanoma is a curable aggressive skin cancer if detected early. Typically, the diagnosis involves initial screening with subsequent biopsy and histopathological examination if necessary. Computer aided diagnosis offers an objective score that is independent of clinical experience and the potential to lower the workload of a dermatologist. In the recent past, success of deep learning algorithms in the field of general computer vision has motivated successful application of supervised deep learning methods in computer aided melanoma recognition. However, large quantities of labeled images are required to make further improvements on the supervised method. A good annotation generally requires clinical and histological confirmation, which requires significant effort. In an attempt to alleviate this constraint, we propose to use categorical generative adversarial network to automatically learn the feature representation of dermoscopy images in an unsupervised and semi-supervised manner. Thorough experiments on ISIC 2016 skin lesion chal- lenge demonstrate that the proposed feature learning method has achieved an average precision score of 0.424 with only 140 labeled images. Moreover, the proposed method is also capable of generating real-world like dermoscopy images.",0
"Title: Enhanced Dermoscopic Image Classification Using Unsupervised and Semi-Supervised Learning with CGANs Assisted by Wasserstein Distance Abstract: Dermatoscopic images are used extensively to detect skin cancer, which is a significant health concern worldwide. These images require accurate classification, as incorrect diagnoses can lead to fatal consequences. Traditional machine learning algorithms have been successful in classifying dermatological images but suffer from high computational costs and limited generalization performance. In this work, we propose enhancing the efficiency and accuracy of dermatological image classification using unsupervised and semi-supervised learning techniques combined with adversarial training methods, specifically with categorical generative adversarial networks (CGANs) assisted by Wasserstein distance. Our approach focuses on leveraging the vast amounts of unlabeled data available to enhance model training. We use GANs to generate synthetic labeled samples that complement existing labeled datasets, ensuring minimal human intervention during training. By implementing semantic segmentation tasks using conditional WGAN variants, our method obtains more stable results than traditional models, surpassing state-of-the art performances achieved on standard benchmark datasets. Further evaluation demonstrates superiority in both visual quality and quantitative assessments, showcasing the effectiveness of our framework. Overall, our study highlights the potential of coupling advanced deep learning models with minimal supervision data to efficiently improve diagnostic accuracies in medical imaging applications.",1
"Recent deep learning approaches to single image super-resolution have achieved impressive results in terms of traditional error measures and perceptual quality. However, in each case it remains challenging to achieve high quality results for large upsampling factors. To this end, we propose a method (ProSR) that is progressive both in architecture and training: the network upsamples an image in intermediate steps, while the learning process is organized from easy to hard, as is done in curriculum learning. To obtain more photorealistic results, we design a generative adversarial network (GAN), named ProGanSR, that follows the same progressive multi-scale design principle. This not only allows to scale well to high upsampling factors (e.g., 8x) but constitutes a principled multi-scale approach that increases the reconstruction quality for all upsampling factors simultaneously. In particular ProSR ranks 2nd in terms of SSIM and 4th in terms of PSNR in the NTIRE2018 SISR challenge [34]. Compared to the top-ranking team, our model is marginally lower, but runs 5 times faster.",0
"This study presents a novel fully progressive approach to single-image super-resolution that achieves state-of-the-art performance while requiring significantly fewer computational resources compared to existing methods. Our method builds upon recent advancements in deep learning by leveraging a lightweight convolutional neural network (CNN) architecture trained on large datasets of low-resolution images paired with high-resolution counterparts. By exploiting spatio-temporal dependencies within video frames, our method enables efficient model training without any explicit guidance from additional data sources such as optical flows or motion vectors. The resulting algorithm delivers high quality intermediate representations at each stage of the inference pipeline, ensuring real-time execution even on consumer-grade hardware. Comprehensive experiments on standard benchmarks demonstrate the superiority of our method over both classical image interpolation techniques and advanced deep learning based approaches. Given these compelling results, we argue that fully progressive frameworks constitute a promising direction towards real-world deployment of single-image super-resolution technology across diverse application domains.",1
"Existing methods for multi-domain image-to-image translation (or generation) attempt to directly map an input image (or a random vector) to an image in one of the output domains. However, most existing methods have limited scalability and robustness, since they require building independent models for each pair of domains in question. This leads to two significant shortcomings: (1) the need to train exponential number of pairwise models, and (2) the inability to leverage data from other domains when training a particular pairwise mapping. Inspired by recent work on module networks, this paper proposes ModularGAN for multi-domain image generation and image-to-image translation. ModularGAN consists of several reusable and composable modules that carry on different functions (e.g., encoding, decoding, transformations). These modules can be trained simultaneously, leveraging data from all domains, and then combined to construct specific GAN networks at test time, according to the specific image translation task. This leads to ModularGAN's superior flexibility of generating (or translating to) an image in any desired domain. Experimental results demonstrate that our model not only presents compelling perceptual results but also outperforms state-of-the-art methods on multi-domain facial attribute transfer.",0
"Modularity has recently emerged as a powerful tool for designing deep generative models. By breaking down complex distributions into simpler modules that can be combined and reused, modular models offer improved interpretability and controllability, while retaining competitive generation performance compared to non-modular baselines. In this work, we investigate the application of modularity principles within the adversarial training framework commonly used in GANs. Specifically, we propose using conditional module architectures to define independent subtasks that can be trained in parallel using different generator objectives tailored to their specific functions. We show through experiments on multiple datasets and task settings that our approach leads to consistent improvement in terms of FID scores over strong non-modular state-of-the art alternatives, while offering more control and insight into the learning process. Notably, we demonstrate how a careful choice of conditioning signals allows our method to excel at tasks such as class-conditional generation or domain transfer, even outperforming dedicated methods specialized for these challenges. Our findings suggest that incorporating modularity into the world of generative adversarial networks opens up new possibilities for scalable design and efficient utilization of large scale data resources.",1
"We study the problem of reconstructing an image from information stored at contour locations. We show that high-quality reconstructions with high fidelity to the source image can be obtained from sparse input, e.g., comprising less than $6\%$ of image pixels. This is a significant improvement over existing contour-based reconstruction methods that require much denser input to capture subtle texture information and to ensure image quality. Our model, based on generative adversarial networks, synthesizes texture and details in regions where no input information is provided. The semantic knowledge encoded into our model and the sparsity of the input allows to use contours as an intuitive interface for semantically-aware image manipulation: local edits in contour domain translate to long-range and coherent changes in pixel space. We can perform complex structural changes such as changing facial expression by simple edits of contours. Our experiments demonstrate that humans as well as a face recognition system mostly cannot distinguish between our reconstructions and the source images.",0
"We present Smart, Sparsely defined contour curves that allow for easy editing and manipulation of images. These curves can be used to guide segmentation algorithms which automatically follow their edges at high resolution or upscaling them to match the original image size if desired. They have only weak local regularity requirements leading to improved robustness to noise and clutter making them well suited for natural imagery. This allows for fast and simple post processing of any computer vision algorithm output. Furthermore our method runs orders of magnitude faster than previous methods while generating more accurate shapes due to its efficient implementation on modern GPU hardware. Finally, we validate our approach using multiple datasets including LSUN church interiors dataset showing that our method outperforms current state of art methods by a large margin.",1
"Facial aging and facial rejuvenation analyze a given face photograph to predict a future look or estimate a past look of the person. To achieve this, it is critical to preserve human identity and the corresponding aging progression and regression with high accuracy. However, existing methods cannot simultaneously handle these two objectives well. We propose a novel generative adversarial network based approach, named the Conditional Multi-Adversarial AutoEncoder with Ordinal Regression (CMAAE-OR). It utilizes an age estimation technique to control the aging accuracy and takes a high-level feature representation to preserve personalized identity. Specifically, the face is first mapped to a latent vector through a convolutional encoder. The latent vector is then projected onto the face manifold conditional on the age through a deconvolutional generator. The latent vector preserves personalized face features and the age controls facial aging and rejuvenation. A discriminator and an ordinal regression are imposed on the encoder and the generator in tandem, making the generated face images to be more photorealistic while simultaneously exhibiting desirable aging effects. Besides, a high-level feature representation is utilized to preserve personalized identity of the generated face. Experiments on two benchmark datasets demonstrate appealing performance of the proposed method over the state-of-the-art.",0
"This research presents a novel approach to facial aging and rejuvenation using conditional multi-adversarial autoencoders with ordinal regression. We propose a deep learning model that can predict age progression and regression on face images in both qualitative and quantitative aspects. Our method uses adversarial training to generate realistic and coherent aged faces as well as youthful faces, which preserve identity information and conform to prior knowledge about aging patterns. Furthermore, we incorporate an ordinal regression framework into our model to predict the age difference between pairs of input faces. Experimental results demonstrate that our model achieves state-of-the-art performance on multiple benchmark datasets, outperforming previous methods in terms of accuracy, visual fidelity, and robustness against variations in pose, lighting, and expression. Overall, our work shows great potential for applications such as virtual try-on, anti-aging skincare analysis, and medical diagnosis assistance systems.",1
"Generative Adversarial Networks (GANs) have been promising in the field of image generation, however, they have been hard to train for language generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them which causes high levels of instability in training GANs. Consequently, past work has resorted to pre-training with maximum-likelihood or training GANs without pre-training with a WGAN objective with a gradient penalty. In this study, we present a comparison of those approaches. Furthermore, we present the results of some experiments that indicate better training and convergence of Wasserstein GANs (WGANs) when a weaker regularization term is enforcing the Lipschitz constraint.",0
"Recurrent Neural Networks (RNN) have been widely used in Natural Language Processing (NLP), achieving state-of-the-art results on many tasks such as text generation, language translation, and sentiment analysis. However, RNN suffer from problems such as vanishing gradients and exploding gradients which make training difficult. In recent years, attention mechanisms were introduced into the architecture of deep neural networks, significantly improving their performance, but they still face challenges in generating coherent and relevant outputs. Generative adversarial networks (GANs) were first proposed by Ian Goodfellow et al., at Uber Technologies Inc. in late 2014. GANs can generate images that rival those created by humans and animals. Since then, numerous works have attempted to apply GAN models to NLP, focusing on using GANs to fine-tune the generated distribution of existing NLP systems, instead of replacing them altogether. This has led to significant improvements, especially in language modeling, where a simple feedforward network equipped with the generator and discriminator was shown to achieve comparable results to state-of-the art recurrent based language models. This work examines the application of generative adversarial networks to natural language processing specifically looking at language modeling. By applying the GAN framework directly rather than solely relying on fine tuning it can result in better performance while being less computationally expensive. Our contributions consist of constructing a generator and evaluating it against several well known benchmark datasets and other architectures. We show through extensive experimentation that our approach outperforms previously published methods without requiring substantial computational resources. Additionally we demonstrate how one could use our method in conjunction wi",1
"We present Wasserstein introspective neural networks (WINN) that are both a generator and a discriminator within a single model. WINN provides a significant improvement over the recent introspective neural networks (INN) method by enhancing INN's generative modeling capability. WINN has three interesting properties: (1) A mathematical connection between the formulation of the INN algorithm and that of Wasserstein generative adversarial networks (WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN results in a large enhancement to INN, achieving compelling results even with a single classifier --- e.g., providing nearly a 20 times reduction in model size over INN for unsupervised generative modeling. (3) When applied to supervised classification, WINN also gives rise to improved robustness against adversarial examples in terms of the error reduction. In the experiments, we report encouraging results on unsupervised learning problems including texture, face, and object modeling, as well as a supervised classification task against adversarial attacks.",0
"Deep neural networks have recently shown impressive results across many domains, including image generation, speech recognition and natural language processing. However, these models often lack interpretability and can be difficult to fine-tune and optimize due to their large size and complex architecture. In order to address these limitations, we propose Wasserstein Introspective Neural Networks (WINN), a novel type of deep learning model that incorporates uncertainty estimates into each layer, allowing for greater flexibility and transparency during training. WINN utilizes the Earth Moverâ€™s Distance (EMD) as a measure of dissimilarity between two distributions, and calculates a corresponding distance between the predicted distribution and ground truth label at every iteration. By minimizing this distance, WINN improves predictions while also providing insights into network behavior and optimization tradeoffs. We demonstrate the effectiveness of our approach using experiments on benchmark datasets such as MNIST, CIFAR-10 and STL-10, showing improved accuracy and robustness compared to state-of-the-art alternatives. Our work provides a new direction towards more introspective, flexible and interpretable deep learning models.",1
"In this paper we address the problem of generating person images conditioned on a given pose. Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with pixel-to-pixel misalignments caused by the pose differences, we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. We test our approach using photos of persons in different poses and we compare our method with previous work in this area showing state-of-the-art results in two benchmarks. Our method can be applied to the wider field of deformable object generation, provided that the pose of the articulated object can be extracted using a keypoint detector.",0
"Title: Generative Adversarial Networks for pose-dependent image synthesis  Abstract: Human pose estimation has become increasingly important in computer vision due to its applications in areas such as human activity recognition and video surveillance. However, generating high quality images that accurately capture human poses remains a challenge. In this work, we present a novel approach using generative adversarial networks (GANs) to generate realistic human images based on given poses. Our model utilizes deformable convolutions which enable accurate pose representation in both generator and discriminator networks, leading to improved performance compared to traditional GAN architectures. We demonstrate the effectiveness of our method by comparing it against state-of-the-art pose-invariant models on several benchmark datasets. Additionally, qualitative evaluations show that our generated images better preserve pose information while maintaining visual fidelity. This approach holds great promise for future research in human pose generation and related fields.",1
"In this work, we consider the task of classifying binary positive-unlabeled (PU) data. The existing discriminative learning based PU models attempt to seek an optimal reweighting strategy for U data, so that a decent decision boundary can be found. However, given limited P data, the conventional PU models tend to suffer from overfitting when adapted to very flexible deep neural networks. In contrast, we are the first to innovate a totally new paradigm to attack the binary PU task, from perspective of generative learning by leveraging the powerful generative adversarial networks (GAN). Our generative positive-unlabeled (GenPU) framework incorporates an array of discriminators and generators that are endowed with different roles in simultaneously producing positive and negative realistic samples. We provide theoretical analysis to justify that, at equilibrium, GenPU is capable of recovering both positive and negative data distributions. Moreover, we show GenPU is generalizable and closely related to the semi-supervised classification. Given rather limited P data, experiments on both synthetic and real-world dataset demonstrate the effectiveness of our proposed framework. With infinite realistic and diverse sample streams generated from GenPU, a very flexible classifier can then be trained using deep neural networks.",0
"Here is a potential abstract:  Generative adversarial networks (GANs) have shown impressive performance in image generation tasks, but their use in other domains remains relatively underexplored. In this work, we introduce generative adversarial positive-unlabeled learning (GAPUL), a framework that leverages GANs to learn from both labeled and unlabeled data. Our approach combines the power of GANs with semi-supervised learning techniques to effectively utilize unlabeled data and improve model performance on downstream tasks. We evaluate our method on several real-world datasets across different domains and demonstrate its effectiveness in generating high-quality synthetic samples and achieving state-of-the-art results compared to existing methods. This work provides new insights into the use of GANs beyond image generation tasks and shows promising applications in diverse areas such as scientific research, medicine, finance, and more.",1
"Generative Adversarial Networks (GAN) have shown great promise in tasks like synthetic image generation, image inpainting, style transfer, and anomaly detection. However, generating discrete data is a challenge. This work presents an adversarial training based correlated discrete data (CDD) generation model. It also details an approach for conditional CDD generation. The results of our approach are presented over two datasets; job-seeking candidates skill set (private dataset) and MNIST (public dataset). From quantitative and qualitative analysis of these results, we show that our model performs better as it leverages inherent correlation in the data, than an existing model that overlooks correlation.",0
"In this work we propose a new method for generating correlated discrete data from generative models trained with adversarial loss. Our approach combines recent advances in variational inference and deep learning in order to achieve high quality samples that accurately capture the underlying statistical relationships within the data. We evaluate our method on several benchmark datasets across different domains and demonstrate significant improvements over competing methods. Furthermore, we provide an analysis of the properties of our generated samples, showing how they compare favorably against real world data distributions. Finally, we discuss possible applications of our method and highlight future research directions for refining and extending our framework. This work presents a promising step forward towards building more effective generative models capable of capturing complex dependencies and patterns found in real data.",1
"The Generative Adversarial Network framework has shown success in implicitly modeling data distributions and is able to generate realistic samples. Its architecture is comprised of a generator, which produces fake data that superficially seem to belong to the real data distribution, and a discriminator which is to distinguish fake from genuine samples. The Noiseless Joint Plug & Play model offers an extension to the framework by simultaneously training autoencoders. This model uses a pre-trained encoder as a feature extractor, feeding the generator with global information. Using the Plug & Play network as baseline, we design a new model by adding discriminators to the Plug & Play architecture. These additional discriminators are trained to discern real and fake latent codes, which are the output of the encoder using genuine and generated inputs, respectively. We proceed to investigate whether this approach is viable. Experiments conducted for the MNIST manifold show that this indeed is the case.",0
"Improving energy predictions via deep learning has become an important research area due to growing interest in renewables integration into grid systems worldwide . Accurate wind power forecasts can improve both economic efficiency and reliability of the electrical grid by improving operational decisions as well as enabling better planning practices . Despite recent advances using deep neural networks (DNNs) trained with LSTM layers, some limitations remain including relatively large training time requirements that limit their use for real-time applications requiring daily updates and a lack of interpretable features from these models. In this work, we propose methods to improve model quality for faster on-line learning of hourly ahead wind speed prediction based upon PVPMN/PPGN architecture extensions called VARNet [7][8]. Using gradient flows directly to the input encoding steps allows us to train DNNs quickly on only one month of data (6 days * 4 hours per day). We demonstrate improved accuracy of these VARNet over a wide range of metrics compared to other benchmark models. Furthermore, our experiments show that we significantly reduce computational overhead allowing future application at higher temporal resolutions beyond what was previously possible. This paper contributes towards reducing carbon dioxide footprint while increasing reliance on sustainable sources of electricity from photovoltaics and wind.",1
"Generative adversarial network (GAN) has achieved impressive success on cross-domain generation, but it faces difficulty in cross-modal generation due to the lack of a common distribution between heterogeneous data. Most existing methods of conditional based cross-modal GANs adopt the strategy of one-directional transfer and have achieved preliminary success on text-to-image transfer. Instead of learning the transfer between different modalities, we aim to learn a synchronous latent space representing the cross-modal common concept. A novel network component named synchronizer is proposed in this work to judge whether the paired data is synchronous/corresponding or not, which can constrain the latent space of generators in the GANs. Our GAN model, named as SyncGAN, can successfully generate synchronous data (e.g., a pair of image and sound) from identical random noise. For transforming data from one modality to another, we recover the latent code by inverting the mappings of a generator and use it to generate data of different modality. In addition, the proposed model can achieve semi-supervised learning, which makes our model more flexible for practical applications.",0
"In recent years, generative adversarial networks (GAN) have become increasingly popular due to their ability to generate high quality data samples by mimicking real world distributions. However, most GAN models are trained on single modalities such as images, texts or audios which limits their application in cross modality tasks. To address these limitations, we propose a novel approach called SyncGAN that synchronizes the latent space of multiple modalities using cycle consistency losses. Our method enables seamless generation of outputs across different domains while preserving the domain specific characteristics. We demonstrate the effectiveness of our proposed model by evaluating it on various benchmark datasets including MNIST, STL10, CIFAR, ImageNet, TED talks and Yelp reviews among others. Experimental results show significant improvement over existing state-of-the-art methods achieving better accuracy both quantitatively and qualitatively. The contribution of our work lies in providing a framework that allows efficient transfer learning between modalities thus paving way towards developing more advanced applications like zero shot and few shot learning systems.",1
"Cell motion and appearance have a strong correlation with cell cycle and disease progression. Many contemporary efforts in machine learning utilize spatio-temporal models to predict a cell's physical state and, consequently, the advancement of disease. Alternatively, generative models learn the underlying distribution of the data, creating holistic representations that can be used in learning. In this work, we propose an aggregate model that combine Generative Adversarial Networks (GANs) and Autoregressive (AR) models to predict cell motion and appearance in human neutrophils imaged by differential interference contrast (DIC) microscopy. We bifurcate the task of learning cell statistics by leveraging GANs for the spatial component and AR models for the temporal component. The aggregate model learned results offer a promising computational environment for studying changes in organellar shape, quantity, and spatial distribution over large sequences.",0
"This abstract outlines our research into neutrophil behavior using generative spatiotemporal modeling (GSTM). We aimed to characterize microscopic changes over time within complex biological systems, specifically focusing on chemically stimulated human neutrophils interacting with Escherichia coli. Our approach involved training a convolutional neural network (CNN) on confocal microscopy images captured at multiple points throughout cell activation, adhesion, and migration events. Once trained, we evaluated GSTMâ€™s ability to predict unseen frames from test videos and assessed predictions against ground truth data. Experimental results showed that our method can accurately capture dynamic phenomena like F-actin polymerization waves and pseudopod formation through space and time. Furthermore, by leveraging the high-resolution spatiotemporal representations afforded by GSTM, we demonstrate how these detailed models may enable more nuanced analysis of neutrophil function. Our findings highlight the potential utility of deep learning techniques like GSTM in enabling a better understanding of neutrophil behavior, especially given their importance as key effector cells in innate immunity and inflammation resolution processes. However, further work remains necessary to realize fully interpretable and transferable methods capable of handling larger datasets across diverse biological scenarios while accounting for associated complexities arising from experimental noise, cell heterogeneity, etc. Ultimately, advancing our knowledge in this domain could impact medical diagnosis/treatment development involving neutrophil disorders or pathologies where these cells play critical roles.",1
"Visual Domain Adaptation is a problem of immense importance in computer vision. Previous approaches showcase the inability of even deep neural networks to learn informative representations across domain shift. This problem is more severe for tasks where acquiring hand labeled data is extremely hard and tedious. In this work, we focus on adapting the representations learned by segmentation networks across synthetic and real domains. Contrary to previous approaches that use a simple adversarial objective or superpixel information to aid the process, we propose an approach based on Generative Adversarial Networks (GANs) that brings the embeddings closer in the learned feature space. To showcase the generality and scalability of our approach, we show that we can achieve state of the art results on two challenging scenarios of synthetic to real domain adaptation. Additional exploratory experiments show that our approach: (1) generalizes to unseen domains and (2) results in improved alignment of source and target distributions.",0
"Incorporate any relevant background concepts like domain shift which your target audience should already know before reading the paper. Otherwise explain them briefly in the introduction of the paper. This abstract assumes that the reader has some familiarity with semantic segmentation as well as generative adversarial networks (GANs). There is no particular application context specified for the abstract. If you think one would make sense based on other assumptions, please mention it explicitly. Here goes!  Learning from synthetic data has gained significant attention in recent years due to its potential for improving machine learning models while reducing reliance on expensive annotation labor and limiting exposure to biased datasets. However, using synthetically generated images directly can result in poor model performance due to domain shift - differences between real and synthetic domains cause the model to generalize poorly to new data. We propose a novel approach utilizing GANs, which addresses these challenges by generating high quality synthetic training data while minimizing domain gap. Our method effectively closes the performance gap between models trained on synthetic versus real data across multiple semantic segmentation benchmarks. These results demonstrate the effectiveness and promise of our technique for enabling more widespread use of synthetic data in downstream applications and promoting further research into the field of data augmentation via generative modeling techniques. Keywords: Synthetic data, Generative Adversarial Networks (GANs), Domain Shift, Semantic Segmentation",1
"In this work we propose a new automatic image annotation model, dubbed {\bf diverse and distinct image annotation} (D2IA). The generative model D2IA is inspired by the ensemble of human annotations, which create semantically relevant, yet distinct and diverse tags. In D2IA, we generate a relevant and distinct tag subset, in which the tags are relevant to the image contents and semantically distinct to each other, using sequential sampling from a determinantal point process (DPP) model. Multiple such tag subsets that cover diverse semantic aspects or diverse semantic levels of the image contents are generated by randomly perturbing the DPP sampling process. We leverage a generative adversarial network (GAN) model to train D2IA. Extensive experiments including quantitative and qualitative comparisons, as well as human subject studies, on two benchmark datasets demonstrate that the proposed model can produce more diverse and distinct tags than the state-of-the-arts.",0
"In recent years, there has been significant progress in developing computer vision algorithms that can automatically annotate images at scale. However, these automated methods often struggle to match the accuracy and diversity of human annotations. This study presents a novel approach to image annotation called ""Tagging like Humans"" (TlH) which leverages both automation and human intelligence to generate diverse and distinct labels for images. We developed a web platform where users can create new tags for objects in an image by specifying a description and then linking it to existing categories using natural language processing techniques. These new tags can then be combined with traditional object detection to improve overall annotation quality. Our experimental results show that TlH significantly outperforms state-of-the-art approaches in terms of label consistency, variety, and granularity while maintaining high inter-human agreement scores. The proposed framework provides researchers and practitioners with a powerful tool for enriching their image datasets with fine-grained and relevant annotations, opening up exciting possibilities for advancing the field of computer vision.",1
"Taking a photo outside, can we predict the immediate future, e.g., how would the cloud move in the sky? We address this problem by presenting a generative adversarial network (GAN) based two-stage approach to generating realistic time-lapse videos of high resolution. Given the first frame, our model learns to generate long-term future frames. The first stage generates videos of realistic contents for each frame. The second stage refines the generated video from the first stage by enforcing it to be closer to real videos with regard to motion dynamics. To further encourage vivid motion in the final generated video, Gram matrix is employed to model the motion more precisely. We build a large scale time-lapse dataset, and test our approach on this new dataset. Using our model, we are able to generate realistic videos of up to $128\times 128$ resolution for 32 frames. Quantitative and qualitative experiment results have demonstrated the superiority of our model over the state-of-the-art models.",0
"This should be submitted to ArXiv or similar service as part of their submission guidelines. No footnotes please. Let me know if you have any questions. Thank You! The goal of this paper is to generate time-lapse videos that can accurately represent the changes in appearance over time of various scenes such as natural landscapes, cityscapes, or even interior spaces. To achieve this, we propose using multi-stage dynamic generative adversarial networks (DGN). Our method involves training multiple DGN models at different stages of the video generation process, where each model learns to predict the next frame given the previous ones. We then combine these predictions into a single sequence of frames that constitutes our final time-lapse video. Experimental results demonstrate the effectiveness of our approach compared to existing state-of-the-art methods, achieving higher quality and more accurate representations of real-world events. Overall, this work represents an important step towards generating high-quality time-lapse videos in computer vision research.",1
"Generative Adversarial Network (GAN) and its variants exhibit state-of-the-art performance in the class of generative models. To capture higher-dimensional distributions, the common learning procedure requires high computational complexity and a large number of parameters. The problem of employing such massive framework arises when deploying it on a platform with limited computational power such as mobile phones. In this paper, we present a new generative adversarial framework by representing each layer as a tensor structure connected by multilinear operations, aiming to reduce the number of model parameters by a large factor while preserving the generative performance and sample quality. To learn the model, we employ an efficient algorithm which alternatively optimizes both discriminator and generator. Experimental outcomes demonstrate that our model can achieve high compression rate for model parameters up to $35$ times when compared to the original GAN for MNIST dataset.",0
"In recent years, deep learning has emerged as one of the most promising areas in artificial intelligence research. Among its many successes, generative adversarial networks (GANs) have shown significant potential in generating realistic synthetic data, such as images and text. However, training GANs can be computationally expensive and time consuming. To address these limitations, we propose tensorization techniques that enable efficient and parallelizable computations during training. Our approach leverages the fact that both discriminator and generator tensors follow a low rank approximation, which allows us to obtain near-optimal solutions while significantly reducing computational costs. We demonstrate through extensive experiments on standard datasets that our method yields state-of-the-art results while achieving up to three times faster convergence compared to non-tensorized baselines. Furthermore, we provide theoretical insights into why tensorization helps speeding up the training process while preserving quality. Finally, we envision our work as a first step towards scaling the usage of GANs to larger datasets and more complex tasks, opening new opportunities for applications across multiple domains. ---- What else would you like me to assist you with?",1
"Histological analysis of tissue samples is one of the most widely used methods for disease diagnosis. After taking a sample from a patient, it goes through a lengthy and laborious preparation, which stains the tissue to visualize different histological features under a microscope. Here, we demonstrate a label-free approach to create a virtually-stained microscopic image using a single wide-field auto-fluorescence image of an unlabeled tissue sample, bypassing the standard histochemical staining process, saving time and cost. This method is based on deep learning, and uses a convolutional neural network trained using a generative adversarial network model to transform an auto-fluorescence image of an unlabeled tissue section into an image that is equivalent to the bright-field image of the stained-version of the same sample. We validated this method by successfully creating virtually-stained microscopic images of human tissue samples, including sections of salivary gland, thyroid, kidney, liver and lung tissue, also covering three different stains. This label-free virtual-staining method eliminates cumbersome and costly histochemical staining procedures, and would significantly simplify tissue preparation in pathology and histology fields.",0
"This paper presents a deep learning-based approach for virtual histology staining using the auto-fluorescence properties of label-free tissue. We utilize Convolutional Neural Networks (CNNs) to classify the different types of tissue based on their unique fluorescent patterns. Our method eliminates the need for traditional manual laboratory techniques such as staining and allows pathologists to make more accurate diagnoses. Additionally, we compare our results against those obtained from expert human evaluation and demonstrate that our model achieves comparable accuracy. Finally, we evaluate the impact of the dataset size on model performance, showing that increasing data leads to significant improvements in prediction quality. Overall, our work shows promising potential for enhancing medical diagnostics through the use of artificial intelligence.",1
"Learning to generate natural scenes has always been a challenging task in computer vision. It is even more painstaking when the generation is conditioned on images with drastically different views. This is mainly because understanding, corresponding, and transforming appearance and semantic information across the views is not trivial. In this paper, we attempt to solve the novel problem of cross-view image synthesis, aerial to street-view and vice versa, using conditional generative adversarial networks (cGAN). Two new architectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq) are proposed to generate scenes with resolutions of 64x64 and 256x256 pixels. X-Fork architecture has a single discriminator and a single generator. The generator hallucinates both the image and its semantic segmentation in the target view. X-Seq architecture utilizes two cGANs. The first one generates the target image which is subsequently fed to the second cGAN for generating its corresponding semantic segmentation map. The feedback from the second cGAN helps the first cGAN generate sharper images. Both of our proposed architectures learn to generate natural images as well as their semantic segmentation maps. The proposed methods show that they are able to capture and maintain the true semantics of objects in source and target views better than the traditional image-to-image translation method which considers only the visual appearance of the scene. Extensive qualitative and quantitative evaluations support the effectiveness of our frameworks, compared to two state of the art methods, for natural scene generation across drastically different views.",0
"This project presents a new method for generating synthetic images that look like real photographs. We use conditional generative adversarial networks (cGANs), which allow us to generate high-resolution images from relatively low-dimensional input descriptions such as textual tags. Our system takes advantage of recent advances in deep learning to achieve state-of-the-art performance on several benchmark datasets. Results show that our model outperforms previous methods by a significant margin, achieving better visual quality and higher accuracy on both qualitative and quantitative metrics. Additionally, we demonstrate that our approach can generalize well to previously unseen domains, indicating that it has strong potential applications beyond just image generation. We believe that this work represents an important step forward in computer vision research, and opens up exciting possibilities for future exploration in the field.",1
"Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity.",0
"Abstract:  Growing demands for personalized entertainment have led researchers to explore artificial intelligence (AI) techniques that can generate content based on user preferences. One such technique is generative adversarial networks (GAN), which has shown great promise in generating high-quality images, videos, music, and even text. However, one challenge facing these models is ensuring their generated trajectories are socially acceptable, as they often create abnormal or nonsensical output. In this work, we introduce social GAN, a novel approach designed to overcome this issue by incorporating social norms into the training process. Our model learns from a large dataset of human-generated examples and adapts to societal expectations while maintaining its ability to generate diverse outputs. We evaluate our method using quantitative metrics and user studies, demonstrating significant improvements over existing methods in terms of both quality and social acceptability. These findings pave the way for future advancements in AI-based content generation systems tailored towards realizing sustainable values like fairness, accountability, transparency, privacy and inclusiveness.",1
"Compositionality of semantic concepts in image synthesis and analysis is appealing as it can help in decomposing known and generatively recomposing unknown data. For instance, we may learn concepts of changing illumination, geometry or albedo of a scene, and try to recombine them to generate physically meaningful, but unseen data for training and testing. In practice however we often do not have samples from the joint concept space available: We may have data on illumination change in one data set and on geometric change in another one without complete overlap. We pose the following question: How can we learn two or more concepts jointly from different data sets with mutual consistency where we do not have samples from the full joint space? We present a novel answer in this paper based on cyclic consistency over multiple concepts, represented individually by generative adversarial networks (GANs). Our method, ConceptGAN, can be understood as a drop in for data augmentation to improve resilience for real world applications. Qualitative and quantitative evaluations demonstrate its efficacy in generating semantically meaningful images, as well as one shot face verification as an example application.",0
"This paper presents a novel approach for learning compositional visual concepts that can generalize across different objects within the same class. By leveraging mutual consistency regularization, we enable our model to learn disentangled representations that capture fine-grained distinctions among different object parts. Our method addresses two key limitations of existing approaches: firstly, they often rely on task-specific supervision, which limits their applicability to new domains; secondly, they struggle to handle variations in shape, appearance, and context, resulting in poor generalization performance. We evaluate our method using several benchmark datasets and demonstrate improved results compared to state-of-the-art alternatives. Our findings suggest that integrating mutual consistency constraints into the learning process enhances the ability of deep models to recognize and reason about object compositionality in images.",1
"Generative models have made significant progress in the tasks of modeling complex data distributions such as natural images. The introduction of Generative Adversarial Networks (GANs) and auto-encoders lead to the possibility of training on big data sets in an unsupervised manner. However, for many generative models it is not possible to specify what kind of image should be generated and it is not possible to translate existing images into new images of similar domains. Furthermore, models that can perform image-to-image translation often need distinct models for each domain, making it hard to scale these systems to multiple domain image-to-image translation. We introduce a model that can do both, controllable image generation and image-to-image translation between multiple domains. We split our image representation into two parts encoding unstructured and structured information respectively. The latter is designed in a disentangled manner, so that different parts encode different image characteristics. We train an encoder to encode images into these representations and use a small amount of labeled data to specify what kind of information should be encoded in the disentangled part. A generator is trained to generate images from these representations using the characteristics provided by the disentangled part of the representation. Through this we can control what kind of images the generator generates, translate images between different domains, and even learn unknown data-generating factors while only using one single model.",0
"One of the most significant challenges facing modern artificial intelligence (AI) research is building systems that can generate and manipulate complex data, such as images, videos, or audio recordings. This task requires an understanding of high-level concepts and how they relate to one another, which traditional machine learning models have struggled to achieve. However, recent advances in deep learning have opened up new possibilities for solving these problems using techniques such as disentangling and generative modeling. In our paper, we propose a novel approach based on disentangled representations that enables us to tackle image generation and translation tasks effectively. Our method builds upon previous work in the field by introducing several key innovations designed to improve performance and versatility. We demonstrate the effectiveness of our technique through extensive experiments on publicly available datasets and show that it achieves state-of-the-art results in terms of both quality and interpretability. Overall, our findings provide valuable insights into the potential applications of AI in generating and translating multimedia content, paving the way for future developments in areas ranging from computer graphics to autonomous robotics.",1
"Recent advances in deep generative models have shown promising potential in image inpanting, which refers to the task of predicting missing pixel values of an incomplete image using the known context. However, existing methods can be slow or generate unsatisfying results with easily detectable flaws. In addition, there is often perceivable discontinuity near the holes and require further post-processing to blend the results. We present a new approach to address the difficulty of training a very deep generative model to synthesize high-quality photo-realistic inpainting. Our model uses conditional generative adversarial networks (conditional GANs) as the backbone, and we introduce a novel block-wise procedural training scheme to stabilize the training while we increase the network depth. We also propose a new strategy called adversarial loss annealing to reduce the artifacts. We further describe several losses specifically designed for inpainting and show their effectiveness. Extensive experiments and user-study show that our approach outperforms existing methods in several tasks such as inpainting, face completion and image harmonization. Finally, we show our framework can be easily used as a tool for interactive guided inpainting, demonstrating its practical value to solve common real-world challenges.",0
"Artificial Intelligence (AI) has been used extensively across different domains including image processing, natural language understanding, decision making etc., but still needs improvement in several aspects like adaptability to real time data changes, dealing with unexpected scenarios & events etc. Recent advancements in Computer Vision (CV), Natural Language Processing (NLP) have led to significant improvements in solving complex problems associated with these challenges, as a result, improved performance in terms of accuracy and efficiency can be achieved by combining these techniques together along with domain knowledge where required. This research article proposes an innovative approach called ""Image Inpainting using Block-wise Procedural Training with Annealed Adversarial Counterpart"" which uses block wise training approach while considering multiple objectives simultaneously like content loss, adversarial training on patches from same image space with respect to l2 distance metrics, multi scale discriminator network etc., These approaches helps system generate better textual descriptions for images/videos/texts and improves performance significantly under challenging conditions with reduced computational cost. Evaluation results show that proposed technique outperforms previous state-of-the-art methods for similar tasks.",1
"Deep generative models learned through adversarial training have become increasingly popular for their ability to generate naturalistic image textures. However, aside from their texture, the visual appearance of objects is significantly influenced by their shape geometry; information which is not taken into account by existing generative models. This paper introduces the Geometry-Aware Generative Adversarial Networks (GAGAN) for incorporating geometric information into the image generation process. Specifically, in GAGAN the generator samples latent variables from the probability space of a statistical shape model. By mapping the output of the generator to a canonical coordinate frame through a differentiable geometric transformation, we enforce the geometry of the objects and add an implicit connection from the prior to the generated object. Experimental results on face generation indicate that the GAGAN can generate realistic images of faces with arbitrary facial attributes such as facial expression, pose, and morphology, that are of better quality than current GAN-based methods. Our method can be used to augment any existing GAN architecture and improve the quality of the images generated.",0
"In this paper, we propose GAGAN (Geometry-Aware Generative Adversarial Network), which improves the state-of-the-art in generative adversarial networks by explicitly modeling the geometry of generated samples via convolutional layers that take as input both spatial coordinates and deep features from generator network. By doing so, our method achieves better results in terms of sample quality, training stability, and robustness to changes in hyperparameters compared to existing methods. Furthermore, we demonstrate through experiments that GAGAN leads to improved performance on a variety of image generation tasks including unconditional generation and conditional translation. We believe that GAGAN represents an important step towards generating high-fidelity images using deep learning techniques.",1
"Casting neural networks in generative frameworks is a highly sought-after endeavor these days. Contemporary methods, such as Generative Adversarial Networks, capture some of the generative capabilities, but not all. In particular, they lack the ability of tractable marginalization, and thus are not suitable for many tasks. Other methods, based on arithmetic circuits and sum-product networks, do allow tractable marginalization, but their performance is challenged by the need to learn the structure of a circuit. Building on the tractability of arithmetic circuits, we leverage concepts from tensor analysis, and derive a family of generative models we call Tensorial Mixture Models (TMMs). TMMs assume a simple convolutional network structure, and in addition, lend themselves to theoretical analyses that allow comprehensive understanding of the relation between their structure and their expressive properties. We thus obtain a generative model that is tractable on one hand, and on the other hand, allows effective representation of rich distributions in an easily controlled manner. These two capabilities are brought together in the task of classification under missing data, where TMMs deliver state of the art accuracies with seamless implementation and design.",0
"Tensorial mixture models are powerful tools that have been gaining attention recently due to their ability to capture complex patterns in high dimensional data sets. These models can effectively represent heterogeneous mixtures by using flexible distributions over tensor products of simple building blocks such as Gaussians or spherical harmonics. This flexibility allows them to handle problems where there may be multiple modes or clusters in the underlying distribution, leading to improved model performance compared to traditional methods. In this work, we present a comprehensive review of recent advances in tensorial mixture modelling, including both theoretical developments and applications across a range of domains. We discuss the mathematical foundations and properties of these models, as well as techniques for parameter estimation and model selection. Our aim is to provide readers with an understanding of the key concepts and approaches in this rapidly developing field, enabling them to apply tensorial mixture models to real world challenges in fields such as computer vision, image processing, and machine learning. Overall, tensorial mixture models hold great promise for addressing complex data analysis problems and solving important scientific questions.",1
"Generative adversarial networks (GANs) have demonstrated to be successful at generating realistic real-world images. In this paper we compare various GAN techniques, both supervised and unsupervised. The effects on training stability of different objective functions are compared. We add an encoder to the network, making it possible to encode images to the latent space of the GAN. The generator, discriminator and encoder are parameterized by deep convolutional neural networks. For the discriminator network we experimented with using the novel Capsule Network, a state-of-the-art technique for detecting global features in images. Experiments are performed using a digit and face dataset, with various visualizations illustrating the results. The results show that using the encoder network it is possible to reconstruct images. With the conditional GAN we can alter visual attributes of generated or encoded images. The experiments with the Capsule Network as discriminator result in generated images of a lower quality, compared to a standard convolutional neural network.",0
"This paper presents a comparison of several techniques used in generative adversarial networks (GANs) for image creation and modification tasks. GANs have shown great success in generating realistic images from scratch as well as modifying existing ones. We explore different approaches such as changing specific features within the generated images using attention mechanisms, conditioning on text prompts to create desired scenes, and improving training stability through novel architectures and loss functions. Our experimental results demonstrate that these methods can lead to more accurate and visually pleasing outputs compared to traditional GAN models. These findings provide insights into which techniques may be most effective depending on the task at hand. Overall, our work contributes towards advancing the state-of-the-art in generative modeling via GANs.",1
"We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Network (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end-to-end learning is achieved by directly embedding the atmospheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense network that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map. This network is optimized using a newly introduced edge-preserving loss function. To further incorporate the mutual structural information between the estimated transmission map and the dehazed result, we propose a joint-discriminator based on generative adversarial network framework to decide whether the corresponding dehazed image and the estimated transmission map are real or fake. An ablation study is conducted to demonstrate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. Extensive experiments demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods. Code will be made available at: https://github.com/hezhangsprinter",0
"Title: ""Deep Neural Networks for Improving Image Quality""  Abstract: A common problem in computer vision applications is image degradation caused by haze or fog, which can significantly reduce visual clarity and impair object recognition algorithms. Recent advances in deep learning have shown great promise in addressing these challenges through the use of convolutional neural networks (CNNs) that learn to restore images affected by atmospheric attenuation. In this work, we propose a novel architecture called the densely connected pyramid dehazing network (DPDN), which combines multi-scale feature representations with dense connections within each layer to improve performance on the task of image dehazing. Our experiments demonstrate that our model outperforms previous state-of-the-art methods in terms of both quantitative metrics and subjective evaluations by human judges, achieving significant improvements in image quality and fine detail restoration. Additionally, we showcase the versatility of our approach by applying it to other low-light image enhancement tasks and obtaining promising results. This research has important implications for many real-world applications such as surveillance systems, autonomous vehicles, and photo editing tools. Overall, our findings highlight the potential of using advanced machine learning techniques for enhancing digital imagery, improving its accuracy, and unlocking new possibilities across multiple domains.",1
"The task of three-dimensional (3D) human pose estimation from a single image can be divided into two parts: (1) Two-dimensional (2D) human joint detection from the image and (2) estimating a 3D pose from the 2D joints. Herein, we focus on the second part, i.e., a 3D pose estimation from 2D joint locations. The problem with existing methods is that they require either (1) a 3D pose dataset or (2) 2D joint locations in consecutive frames taken from a video sequence. We aim to solve these problems. For the first time, we propose a method that learns a 3D human pose without any 3D datasets. Our method can predict a 3D pose from 2D joint locations in a single image. Our system is based on the generative adversarial networks, and the networks are trained in an unsupervised manner. Our primary idea is that, if the network can predict a 3D human pose correctly, the 3D pose that is projected onto a 2D plane should not collapse even if it is rotated perpendicularly. We evaluated the performance of our method using Human3.6M and the MPII dataset and showed that our network can predict a 3D pose well even if the 3D dataset is not available during training.",0
"""The ability to accurately estimate human pose has numerous applications in computer vision, robotics, and virtual reality. In recent years, there has been significant progress in developing machine learning models that can estimate 3D human pose from 2D images. However, these methods often require large amounts of labeled data and may still struggle in challenging scenarios such as occlusions, cluttered backgrounds, and varying camera viewpoints.  In our work, we propose a novel unsupervised adversarial learning approach to improve the accuracy of estimating 3D human pose from 2D joint locations without relying on detailed annotations. Our method leverages discriminator networks trained in an adversarial setting to guide and regularize the generator network towards generating more realistic poses. We use a contrastive loss function to train both the generator and discriminator networks based on their performance on two tasks: reconstruction of input 2D joints and estimation of depth maps. Additionally, we introduce a latent space regularization term into the objective function to encourage the learned pose distributions to lie close to a predefined anchor point in latent space.  We evaluate our method on several benchmark datasets including Human3.6M, MPI-INF-3DHP, and HumanEva II, achieving state-of-the-art results on all three datasets. Our proposed framework demonstrates promising potential for improving the robustness and generalizability of 3D human pose estimation under different environments, providing new opportunities for advancing research areas related to virtual reality, robotics, and human-computer interaction.""",1
"Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13% increase in accuracy in the low-data regime experiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face (4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5% (from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).",0
"Artificial intelligence has made significant strides over recent years due largely to advances in generative models. With access to large datasets and computational resources, these models have proven capable of generating new data instances that can be used to train more accurate machine learning systems. This research introduces data augmentation generative adversarial networks (DAGAN), which combine two approaches: traditional data augmentation and deep convolutional neural networks trained via adverserial learning. Our approach combines the strengths of both paradigms while mitigating their weaknesses, yielding high quality synthetic training data. In particular, we demonstrate state-of-the-art results on MNIST and SVHN benchmark image classification tasks using DAGAN. We validate our system through rigorous experimentation and ablation analysis. In conclusion, our work provides a step forward towards building robust artificial intelligent agents able to learn from less real world training data by leveraging synthetically generated examples.",1
"Generative Adversarial Networks (GANs) are a class of generative algorithms that have been shown to produce state-of-the art samples, especially in the domain of image creation. The fundamental principle of GANs is to approximate the unknown distribution of a given data set by optimizing an objective function through an adversarial game between a family of generators and a family of discriminators. In this paper, we offer a better theoretical understanding of GANs by analyzing some of their mathematical and statistical properties. We study the deep connection between the adversarial principle underlying GANs and the Jensen-Shannon divergence, together with some optimality characteristics of the problem. An analysis of the role of the discriminator family via approximation arguments is also provided. In addition, taking a statistical point of view, we study the large sample properties of the estimated distribution and prove in particular a central limit theorem. Some of our results are illustrated with simulated examples.",0
"This paper explores the theoretical properties of Generative Adversarial Networks (GANs). It examines how these deep learning models can generate images from scratch and how they can transfer styles from one image onto another. The authors analyze the stability of training GANs using different optimization methods, such as gradient descent with least squares loss and Adam optimizer. They also investigate the effectiveness of data augmentation techniques like horizontal flipping and random crops on improving model performance. Overall, the study provides valuable insights into the capabilities and limitations of GANs, paving the way for further research in this rapidly evolving field.",1
"We present a novel approach to generating photo-realistic images of a face with accurate lip sync, given an audio input. By using a recurrent neural network, we achieved mouth landmarks based on audio features. We exploited the power of conditional generative adversarial networks to produce highly-realistic face conditioned on a set of landmarks. These two networks together are capable of producing a sequence of natural faces in sync with an input audio track.",0
"This abstract describes our work on using generative adversarial networks to generate video frames that recreate facial expressions seen in audio clips of speeches. We use pairs of real videos and their corresponding audio tracks as training data, along with a discriminator network that helps guide the generator towards generating more natural looking results. Our system works by first extracting feature representations from both the audio and visual inputs, then fusing them together into a single representation that can be used to condition the generation process. We demonstrate the effectiveness of our method through qualitative evaluations and user studies, showing that our generated images are often indistinguishable from real images and improve over traditional approaches.",1
"Area of image inpainting over relatively large missing regions recently advanced substantially through adaptation of dedicated deep neural networks. However, current network solutions still introduce undesired artifacts and noise to the repaired regions. We present an image inpainting method that is based on the celebrated generative adversarial network (GAN) framework. The proposed PGGAN method includes a discriminator network that combines a global GAN (G-GAN) architecture with a patchGAN approach. PGGAN first shares network layers between G-GAN and patchGAN, then splits paths to produce two adversarial losses that feed the generator network in order to capture both local continuity of image texture and pervasive global features in images. The proposed framework is evaluated extensively, and the results including comparison to recent state-of-the-art demonstrate that it achieves considerable improvements on both visual and quantitative evaluations.",0
"This paper presents a patch-based image inpainting method that utilizes generative adversarial networks (GANs). Traditional image inpainting methods often suffer from limitations such as noise artifacts or blurriness near object boundaries. To overcome these issues, we propose using GANs which can generate high quality images that retain fine details. Our approach consists of two main components: a generator network that produces patches to fill in the missing region, and a discriminator network that evaluates whether each generated patch is realistic enough. We train our model on pairs of patches extracted from corrupted input images and corresponding ground truth images. During inference, the generator generates multiple overlapping patches and stitches them together seamlessly into a complete output image. Experimental results demonstrate that our proposed method outperforms state-of-the-art image inpainting techniques in terms of visual fidelity, particularly at object boundaries where traditional methods struggle. Moreover, our method achieves competitive performance even with small amounts of training data, making it suitable for use cases where obtaining large datasets may not be feasible. Our findings suggest that patch-based image inpainting with GANs holds great promise for applications in fields ranging from computer vision to medical imaging.",1
"In this paper, we present the Lipschitz regularization theory and algorithms for a novel Loss-Sensitive Generative Adversarial Network (LS-GAN). Specifically, it trains a loss function to distinguish between real and fake samples by designated margins, while learning a generator alternately to produce realistic samples by minimizing their losses. The LS-GAN further regularizes its loss function with a Lipschitz regularity condition on the density of real data, yielding a regularized model that can better generalize to produce new data from a reasonable number of training examples than the classic GAN. We will further present a Generalized LS-GAN (GLS-GAN) and show it contains a large family of regularized GAN models, including both LS-GAN and Wasserstein GAN, as its special cases. Compared with the other GAN models, we will conduct experiments to show both LS-GAN and GLS-GAN exhibit competitive ability in generating new images in terms of the Minimum Reconstruction Error (MRE) assessed on a separate test set. We further extend the LS-GAN to a conditional form for supervised and semi-supervised learning problems, and demonstrate its outstanding performance on image classification tasks.",0
"In Loss-Sensitive GANs we introduce a new architecture for generative adversarial networks (GANs) that takes into account the importance of preserving high density regions in the data distribution while generating samples. Our method uses a novel loss function that assigns higher penalties for points far from known modes of the data distribution, leading to more accurate generator outputs in these regions. We demonstrate state-of-the-art performance across several benchmark datasets including CelebA and MNIST. Additionally we show qualitatively that our model produces sharper images with greater detail than other methods. By incorporating a Lipschitz continuity constraint during training we further improve upon sample quality by constraining the growth rate of gradients and preventing mode collapse. This work provides a strong foundation for future research on improving the stability and accuracy of GANs.",1
"Drawing a beautiful painting is a dream of many people since childhood. In this paper, we propose a novel scheme, Line Artist, to synthesize artistic style paintings with freehand sketch images, leveraging the power of deep learning and advanced algorithms. Our scheme includes three models. The Sketch Image Extraction (SIE) model is applied to generate the training data. It includes smoothing reality images and pencil sketch extraction. The Detailed Image Synthesis (DIS) model trains a conditional generative adversarial network to generate detailed real-world information. The Adaptively Weighted Artistic Style Transfer (AWAST) model is capable to combine multiple style images with a content with the VGG19 network and PageRank algorithm. The appealing artistic images are then generated by optimization iterations. Experiments are operated on the Kaggle Cats dataset and The Oxford Buildings Dataset. Our synthesis results are proved to be artistic, beautiful and robust.",0
"Abstract: This paper presents a new method called ""Line Artist"" that allows artists to synthesize multiple styles of sketches into painting layers while preserving the unique characteristics of each style. Our approach uses style transfer techniques combined with edge detection algorithms and can generate high-resolution output. We evaluate our method using subjective user studies, showing promising results and outperforming current state-of-the-art methods. With Line Artist, artists now have access to more creative possibilities by combining various drawing styles seamlessly. This tool has wide applications in the fields of art creation, entertainment, animation, game development, education, and beyond.",1
This paper focuses on multi-sensor anomaly detection for moving cognitive agents using both external and private first-person visual observations. Both observation types are used to characterize agents' motion in a given environment. The proposed method generates locally uniform motion models by dividing a Gaussian process that approximates agents' displacements on the scene and provides a Shared Level (SL) self-awareness based on Environment Centered (EC) models. Such models are then used to train in a semi-unsupervised way a set of Generative Adversarial Networks (GANs) that produce an estimation of external and internal parameters of moving agents. Obtained results exemplify the feasibility of using multi-perspective data for predicting and analyzing trajectory information.,0
"This paper presents A Multi-perspective Approach To Anomaly Detection For Self-aware Embodied Agents. The approach proposed here relies on combining different anomaly detection techniques based on different perspectives on the system, including sensory data, internal states, and task performance. Each perspective provides complementary information which can be combined into a unified framework that enables improved robustness to anomalies such as sensor failures, unexpected changes in environment, or malicious attacks. Furthermore, the integration of self-awareness and embodiment allows for more effective identification of anomalous behavior and better understanding of why certain observations may deviate from expected norms. Experimental evaluations demonstrate the effectiveness of our proposed method in detecting anomalies under various scenarios while minimizing false positive rates. Overall, this research offers new insights into developing multi-perspective approaches to enhance the resilience and adaptability of autonomous agents operating in real-world settings.",1
"In this paper we report on improved part segmentation performance using convolutional neural networks to reduce the dependency on the large amount of manually annotated empirical images. This was achieved by optimising the visual realism of synthetic agricultural images.In Part I, a cycle consistent generative adversarial network was applied to synthetic and empirical images with the objective to generate more realistic synthetic images by translating them to the empirical domain. We first hypothesise and confirm that plant part image features such as color and texture become more similar to the empirical domain after translation of the synthetic images.Results confirm this with an improved mean color distribution correlation with the empirical data prior of 0.62 and post translation of 0.90. Furthermore, the mean image features of contrast, homogeneity, energy and entropy moved closer to the empirical mean, post translation. In Part II, 7 experiments were performed using convolutional neural networks with different combinations of synthetic, synthetic translated to empirical and empirical images. We hypothesised that the translated images can be used for (i) improved learning of empirical images, and (ii) that learning without any fine-tuning with empirical images is improved by bootstrapping with translated images over bootstrapping with synthetic images. Results confirm our second and third hypotheses. First a maximum intersection-over-union performance was achieved of 0.52 when bootstrapping with translated images and fine-tuning with empirical images; an 8% increase compared to only using synthetic images. Second, training without any empirical fine-tuning resulted in an average IOU of 0.31; a 55% performance increase over previous methods that only used synthetic images.",0
"In this research paper, we aim to improve part segmentation performance by optimizing realism in synthetic images generated using cycle generative adversarial networks (CycleGANs). Our approach involves training a CycleGAN on large amounts of data, which allows us to generate high quality synthetic images that more closely resemble real-world scenes. We then use these generated images as input into our part segmentation algorithm, which results in improved performance compared to traditional methods.  Our experimental results show significant improvements in accuracy and robustness when using our method, demonstrating the effectiveness of our approach. Additionally, we provide qualitative evaluations showing how our generated images better capture important details such as texture and color, resulting in more accurate predictions from our part segmentation model.  Overall, our work represents an important step towards achieving state-of-the-art performance in computer vision tasks, particularly those relying on generating synthetic images. By leveraging the power of deep learning techniques like CycleGANs, we can produce more realistic synthetic data that enables superior performance in downstream applications.",1
"Compressed Sensing MRI (CS-MRI) has provided theoretical foundations upon which the time-consuming MRI acquisition process can be accelerated. However, it primarily relies on iterative numerical solvers which still hinders their adaptation in time-critical applications. In addition, recent advances in deep neural networks have shown their potential in computer vision and image processing, but their adaptation to MRI reconstruction is still in an early stage. In this paper, we propose a novel deep learning-based generative adversarial model, RefineGAN, for fast and accurate CS-MRI reconstruction. The proposed model is a variant of fully-residual convolutional autoencoder and generative adversarial networks (GANs), specifically designed for CS-MRI formulation; it employs deeper generator and discriminator networks with cyclic data consistency loss for faithful interpolation in the given under-sampled k-space data. In addition, our solution leverages a chained network to further enhance the reconstruction quality. RefineGAN is fast and accurate -- the reconstruction process is extremely rapid, as low as tens of milliseconds for reconstruction of a 256x256 image, because it is one-way deployment on a feed-forward network, and the image quality is superior even for extremely low sampling rate (as low as 10%) due to the data-driven nature of the method. We demonstrate that RefineGAN outperforms the state-of-the-art CS-MRI methods by a large margin in terms of both running time and image quality via evaluation using several open-source MRI databases.",0
"In recent years, Compressed Sensing Magnetic Resonance Imaging (CSMRI) has emerged as a promising technique for imaging faster than real time while maintaining good image quality. However, finding the most suitable method to reconstruct these images remains challenging due to hardware limitations. In this research, we propose utilizing a generative adversarial network with cyclic loss function for CSMRI reconstruction. By implementing a discriminator network that evaluates the authenticity of the generated images, our method can optimize the reconstruction process. Our findings demonstrate improved image fidelity compared to traditional methods, making this approach a valuable contribution to the field of medical imaging. Furthermore, the proposed method could potentially accelerate future studies involving rapid CSMRI acquisition and processing techniques. Overall, our work represents a significant advancement in the application of deep learning models for solving inverse problems in MRI.",1
"In this paper, we aim to improve the state-of-the-art video generative adversarial networks (GANs) with a view towards multi-functional applications. Our improved video GAN model does not separate foreground from background nor dynamic from static patterns, but learns to generate the entire video clip conjointly. Our model can thus be trained to generate - and learn from - a broad set of videos with no restriction. This is achieved by designing a robust one-stream video generation architecture with an extension of the state-of-the-art Wasserstein GAN framework that allows for better convergence. The experimental results show that our improved video GAN model outperforms state-of-theart video generative models on multiple challenging datasets. Furthermore, we demonstrate the superiority of our model by successfully extending it to three challenging problems: video colorization, video inpainting, and future prediction. To the best of our knowledge, this is the first work using GANs to colorize and inpaint video clips.",0
Video generation has become increasingly important due to applications ranging from entertainment to surveillance and autonomous systems. Recent advancements have resulted in significant improvement in visual quality but limited performance in other aspects such as efficiency and generalization ability for diverse tasks. This research focuses on improving video generation by addressing these limitations through a unified framework that incorporates a spatio-temporal attention mechanism and temporal context modeling. Our method significantly enhances the efficiency of generation while maintaining high visual fidelity. We demonstrate the effectiveness of our approach across several challenging scenarios including action recognition and dense correspondence estimation. These results further showcase the versatility and potential of our framework for multi-functional video generation applications.,1
"Popular generative model learning methods such as Generative Adversarial Networks (GANs), and Variational Autoencoders (VAE) enforce the latent representation to follow simple distributions such as isotropic Gaussian. In this paper, we argue that learning a complicated distribution over the latent space of an auto-encoder enables more accurate modeling of complicated data distributions. Based on this observation, we propose a two stage optimization procedure which maximizes an approximate implicit density model. We experimentally verify that our method outperforms GANs and VAEs on two image datasets (MNIST, CELEB-A). We also show that our approach is amenable to learning generative model for sequential data, by learning to generate speech and music.",0
"""The development of implicit generative models has seen tremendous advancements in recent years due to their ability to model complex data distributions without relying on explicit assumptions regarding their underlying structure. These models achieve state-of-the-art performance across multiple domains by leveraging the power of deep learning architectures such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). Despite these successes, a critical challenge remains: understanding how these models capture patterns from data. In particular, it remains unclear how implicit generative models learn their base distribution. This work presents a novel framework that provides insight into the process of learning the base distribution in implicit generative models. By analyzing the behavior of different models during training, we identify key factors that influence the learning process and provide new strategies for improving their efficiency. Our findings shed light on the intricacies of the training dynamics and highlight directions for future research.""",1
"Generative adversarial networks are used to generate images but still their convergence properties are not well understood. There have been a few studies who intended to investigate the stability properties of GANs as a dynamical system. This short writing can be seen in that direction. Among the proposed methods for stabilizing training of GANs, {\ss}-GAN was the first who proposed a complete annealing strategy to change high-level conditions of the GAN objective. In this note, we show by a simple example how annealing strategy works in GANs. The theoretical analysis is supported by simple simulations.",0
"Abstract: This paper explores the concept of nonautonomous adversarial systems (NAS) and their impact on society. NAS refers to situations where multiple parties, each acting autonomously within their own domain but pursuing conflicting goals or objectives, can lead to negative outcomes that are detrimental to overall system performance or societal wellbeing. This paper examines different case studies to illustrate how these types of conflicts arise and investigates the factors contributing to them. We discuss potential solutions to mitigate the effects of NAS, including increased cooperation and communication among stakeholders and better aligning individual interests with larger social benefits. Ultimately, we argue that understanding and addressing NAS is critical for developing sustainable and equitable policies across various domains.",1
"The goal of two-sample tests is to assess whether two samples, $S_P \sim P^n$ and $S_Q \sim Q^m$, are drawn from the same distribution. Perhaps intriguingly, one relatively unexplored method to build two-sample tests is the use of binary classifiers. In particular, construct a dataset by pairing the $n$ examples in $S_P$ with a positive label, and by pairing the $m$ examples in $S_Q$ with a negative label. If the null hypothesis ""$P = Q$"" is true, then the classification accuracy of a binary classifier on a held-out subset of this dataset should remain near chance-level. As we will show, such Classifier Two-Sample Tests (C2ST) learn a suitable representation of the data on the fly, return test statistics in interpretable units, have a simple null distribution, and their predictive uncertainty allow to interpret where $P$ and $Q$ differ. The goal of this paper is to establish the properties, performance, and uses of C2ST. First, we analyze their main theoretical properties. Second, we compare their performance against a variety of state-of-the-art alternatives. Third, we propose their use to evaluate the sample quality of generative models with intractable likelihoods, such as Generative Adversarial Networks (GANs). Fourth, we showcase the novel application of GANs together with C2ST for causal discovery.",0
"In this paper, we revisit the commonly used classifier two-sample tests in machine learning, specifically focusing on their limitations and potential pitfalls. We examine several popular methods such as permutation testing, cross-validation, and distance-based methods and identify their strengths and weaknesses in different scenarios. Additionally, we propose novel techniques that address some of these shortcomings by utilizing recent advances in deep learning and statistical inference. Our proposed approaches achieve better performance than existing methods in many cases while also providing interpretability and robustness to noise. Overall, our work provides insights into the characteristics of classifier two-sample tests and offers practitioners more effective tools for evaluating machine learning models.",1
"Generative adversarial networks (GANs) transform low-dimensional latent vectors into visually plausible images. If the real dataset contains only clean images, then ostensibly, the manifold learned by the GAN should contain only clean images. In this paper, we propose to denoise corrupted images by finding the nearest point on the GAN manifold, recovering latent vectors by minimizing distances in image space. We first demonstrate that given a corrupted version of an image that truly lies on the GAN manifold, we can approximately recover the latent vector and denoise the image, obtaining significantly higher quality, comparing with BM3D. Next, we demonstrate that latent vectors recovered from noisy images exhibit a consistent bias. By subtracting this bias before projecting back to image space, we improve denoising results even further. Finally, even for unseen images, our method performs better at denoising better than BM3D. Notably, the basic version of our method (without bias correction) requires no prior knowledge on the noise variance. To achieve the highest possible denoising quality, the best performing signal processing based methods, such as BM3D, require an estimate of the blur kernel.",0
"Image denoising using GANs has been used extensively in recent years due to their ability to generate high quality images that are nearly indistinguishable from real images. However, one significant challenge facing these approaches is how to effectively project the generated image back onto the original image space so that we can obtain the final denoised result. In this work, we propose a novel framework called Correction by Projection (CbP) which addresses this issue and achieves state-of-the-art performance on multiple benchmark datasets. Our approach uses a combination of image reconstruction loss and adversarial training to ensure that the correction module produces meaningful corrections that improve image quality while preserving important details. Furthermore, our method allows for efficient parallel processing during both training and inference, making it computationally tractable even for large scale applications. We demonstrate through extensive experiments that CbP outperforms existing methods across a variety of metrics including peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM) and visual inspection. Overall, our results show that CbP is a powerful tool for image restoration tasks, providing high fidelity outputs that closely match ground truth data.",1
"We introduce a data-driven approach to aid the repairing and conservation of archaeological objects: ORGAN, an object reconstruction generative adversarial network (GAN). By using an encoder-decoder 3D deep neural network on a GAN architecture, and combining two loss objectives: a completion loss and an Improved Wasserstein GAN loss, we can train a network to effectively predict the missing geometry of damaged objects. As archaeological objects can greatly differ between them, the network is conditioned on a variable, which can be a culture, a region or any metadata of the object. In our results, we show that our method can recover most of the information from damaged objects, even in cases where more than half of the voxels are missing, without producing many errors.",0
"In recent years, generative adversarial networks (GANs) have become increasingly popular as a tool for generating synthetic data in many fields, including computer vision and archaeology. One area where GANs have shown great promise is in the reconstruction of incomplete archaeological objects. By training a GAN on a dataset of complete and partial object images, we can generate new hypothetical versions of the missing parts that blend seamlessly with the known portions. In turn, these generated fragments can help us better understand the context and meaning behind these artifacts from our past. This study presents the results of using a GAN to reconstruct incomplete archaeological objects, demonstrating the potential utility of such approaches in enhancing our understanding of ancient cultures through digital means.",1
"Visual saliency patterns are the result of a variety of factors aside from the image being parsed, however existing approaches have ignored these. To address this limitation, we propose a novel saliency estimation model which leverages the semantic modelling power of conditional generative adversarial networks together with memory architectures which capture the subject's behavioural patterns and task dependent factors. We make contributions aiming to bridge the gap between bottom-up feature learning capabilities in modern deep learning architectures and traditional top-down hand-crafted features based methods for task specific saliency modelling. The conditional nature of the proposed framework enables us to learn contextual semantics and relationships among different tasks together, instead of learning them separately for each task. Our studies not only shed light on a novel application area for generative adversarial networks, but also emphasise the importance of task specific saliency modelling and demonstrate the plausibility of fully capturing this context via an augmented memory architecture.",0
"This work explores task specific visual saliency prediction, which seeks to predict human fixations on images by learning from large amounts of image data. Our approach uses memory augmented conditional generative adversarial networks (MACoGANs) to learn the underlying distributions of features that influence fixation patterns. In addition, we use these models to create new synthetic examples that can improve the training process. Experimental results demonstrate that our method outperforms other state-of-the-art methods across multiple tasks, showing significant improvements in accuracy and robustness.",1
"Current multi-person localisation and tracking systems have an over reliance on the use of appearance models for target re-identification and almost no approaches employ a complete deep learning solution for both objectives. We present a novel, complete deep learning framework for multi-person localisation and tracking. In this context we first introduce a light weight sequential Generative Adversarial Network architecture for person localisation, which overcomes issues related to occlusions and noisy detections, typically found in a multi person environment. In the proposed tracking framework we build upon recent advances in pedestrian trajectory prediction approaches and propose a novel data association scheme based on predicted trajectories. This removes the need for computationally expensive person re-identification systems based on appearance features and generates human like trajectories with minimal fragmentation. The proposed method is evaluated on multiple public benchmarks including both static and dynamic cameras and is capable of generating outstanding performance, especially among other recently proposed deep neural network based approaches.",0
"In recent years, computer vision has seen significant advances in multi-person localization and tracking (MTPL), which involves identifying and monitoring multiple individuals within a video frame. Traditional approaches rely on handcrafted features and model-based methods that have limited performance and scalability. To address these issues, we propose a novel deep generative framework called ""Tracking by Prediction"" (TbP) based on variational autoencoders (VAEs). Our method generates high quality trajectories from raw images without relying on manual feature engineering. We use a recurrent neural network (RNN) to model temporal dependencies across frames and learn a shared latent representation space for all individuals in the scene. Unlike prior work, our approach incorporates both appearance and motion cues into the VAE loss function, leading to more accurate and consistent predictions over time. Experimental results show that TbP achieves state-of-the-art accuracy in MTPL benchmarks while running efficiently even on mobile GPUs.",1
"Combining Generative Adversarial Networks (GANs) with encoders that learn to encode data points has shown promising results in learning data representations in an unsupervised way. We propose a framework that combines an encoder and a generator to learn disentangled representations which encode meaningful information about the data distribution without the need for any labels. While current approaches focus mostly on the generative aspects of GANs, our framework can be used to perform inference on both real and generated data points. Experiments on several data sets show that the encoder learns interpretable, disentangled representations which encode descriptive properties and can be used to sample images that exhibit specific characteristics.",0
"This paper presents a novel approach for inferring complex relationships among variables based on unsupervised learning of disentagled representations. We propose a model that learns a nonlinear representation of data by maximizing the mutual information between input observations and learned features. Our method captures underlying structure present in datasets without relying on explicit supervision. The resulting representation allows us to perform inference tasks such as clustering, dimensionality reduction, and anomaly detection. Extensive experiments demonstrate our method outperforms state-of-the-art approaches in several benchmark datasets across multiple domains. Our work advances the field of unsupervised learning by providing a framework for understanding the information encoded in latent spaces. By leveraging these insights, we can develop powerful models capable of discovering hidden patterns and making accurate predictions.",1
"We address the problem of finding realistic geometric corrections to a foreground object such that it appears natural when composited into a background image. To achieve this, we propose a novel Generative Adversarial Network (GAN) architecture that utilizes Spatial Transformer Networks (STNs) as the generator, which we call Spatial Transformer GANs (ST-GANs). ST-GANs seek image realism by operating in the geometric warp parameter space. In particular, we exploit an iterative STN warping scheme and propose a sequential training strategy that achieves better results compared to naive training of a single generator. One of the key advantages of ST-GAN is its applicability to high-resolution images indirectly since the predicted warp parameters are transferable between reference frames. We demonstrate our approach in two applications: (1) visualizing how indoor furniture (e.g. from product images) might be perceived in a room, (2) hallucinating how accessories like glasses would look when matched with real portraits.",0
"ST-GAN (Spatial Transformer GAN) composites multiple images together to form one seamless image using generative adversarial networks. ST-GAN performs well on difficult scenes that contain fine details like hair and other complex features that traditional methods struggle with. By utilizing spatial transformers, ST-GAN can generate higher quality compositions and reduces artifacts compared to previous methods.",1
"Since their invention, generative adversarial networks (GANs) have become a popular approach for learning to model a distribution of real (unlabeled) data. Convergence problems during training are overcome by Wasserstein GANs which minimize the distance between the model and the empirical distribution in terms of a different metric, but thereby introduce a Lipschitz constraint into the optimization problem. A simple way to enforce the Lipschitz constraint on the class of functions, which can be modeled by the neural network, is weight clipping. It was proposed that training can be improved by instead augmenting the loss by a regularization term that penalizes the deviation of the gradient of the critic (as a function of the network's input) from one. We present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. These arguments are supported by experimental results on toy data sets.",0
"Optimizing generative models such as Generative Adversarial Networks (GANs) remains challenging due to their nonconvexity and sensitivity to hyperparameters. To address these issues, recent work has proposed to penalize large Lipschitz constants using gradient penalty terms. While effective, theoretical guarantees on stability and convergence remain elusive. In this work, we study the problem of stabilizing the optimization process by considering a different class of constraints: quadratic constraints inspired by optimal transport theory. By minimizing the Earth Mover distance (Wasserstein distance), we propose a novel regularizer that can promote better discriminator behavior while improving generator performance. Our experimental results demonstrate that our new approach leads to improved generation quality, more stable training dynamics, and more consistent performance across datasets compared to state-of-the art methods. These findings underscore the importance of exploring alternative regularizers beyond gradient norms in the context of adversarial networks. Our contributions highlight promising research directions towards achieving robustness and interpretability in deep learning systems.",1
"We present a model that uses a single first-person image to generate an egocentric basketball motion sequence in the form of a 12D camera configuration trajectory, which encodes a player's 3D location and 3D head orientation throughout the sequence. To do this, we first introduce a future convolutional neural network (CNN) that predicts an initial sequence of 12D camera configurations, aiming to capture how real players move during a one-on-one basketball game. We also introduce a goal verifier network, which is trained to verify that a given camera configuration is consistent with the final goals of real one-on-one basketball players. Next, we propose an inverse synthesis procedure to synthesize a refined sequence of 12D camera configurations that (1) sufficiently matches the initial configurations predicted by the future CNN, while (2) maximizing the output of the goal verifier network. Finally, by following the trajectory resulting from the refined camera configuration sequence, we obtain the complete 12D motion sequence.   Our model generates realistic basketball motion sequences that capture the goals of real players, outperforming standard deep learning approaches such as recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and generative adversarial networks (GANs).",0
"This paper presents an approach for planning basketball trajectories using motion capture data alone. The method models the player as moving along an ellipse in space (centered at their head), which provides a natural representation of human movement that captures aspects such as velocity profiles over time without requiring additional annotations. We propose two different types of ellipses -- one assuming all joints track together and another modeling independent hip/torso motions -- and evaluate their use on a held-out set of basketball dunks collected by professional players across several seasons. On average, our methods were able to recover accurate estimates of both distance and angle metrics, outperforming baseline approaches. Finally, we validate these findings via user studies. Our results demonstrate that effective basketball motion can indeed be modeled just from first-person images.",1
"Multi-view face synthesis from a single image is an ill-posed problem and often suffers from serious appearance distortion. Producing photo-realistic and identity preserving multi-view results is still a not well defined synthesis problem. This paper proposes Load Balanced Generative Adversarial Networks (LB-GAN) to precisely rotate the yaw angle of an input face image to any specified angle. LB-GAN decomposes the challenging synthesis problem into two well constrained subtasks that correspond to a face normalizer and a face editor respectively. The normalizer first frontalizes an input image, and then the editor rotates the frontalized image to a desired pose guided by a remote code. In order to generate photo-realistic local details, the normalizer and the editor are trained in a two-stage manner and regulated by a conditional self-cycle loss and an attention based L2 loss. Exhaustive experiments on controlled and uncontrolled environments demonstrate that the proposed method not only improves the visual realism of multi-view synthetic images, but also preserves identity information well.",0
"This paper proposes a novel approach for generating high-quality multi-view face images using Generative Adversarial Networks (GAN). Our method utilizes load balancing techniques to improve stability and efficiency during training, resulting in improved synthesis quality compared to existing methods. We evaluate our model on several benchmark datasets and demonstrate that our proposed technique outperforms state-of-the-art approaches in terms of both quantitative metrics and visual fidelity. Additionally, we provide qualitative analysis to showcase the effectiveness of our proposed method for realistic image generation from multiple viewpoints. Overall, our results highlight the potential of load balanced GANs for producing highly detailed and accurate face images in various poses.",1
"Deep learning methods, and in particular convolutional neural networks (CNNs), have led to an enormous breakthrough in a wide range of computer vision tasks, primarily by using large-scale annotated datasets. However, obtaining such datasets in the medical domain remains a challenge. In this paper, we present methods for generating synthetic medical images using recently presented deep learning Generative Adversarial Networks (GANs). Furthermore, we show that generated medical images can be used for synthetic data augmentation, and improve the performance of CNN for medical image classification. Our novel method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first exploit GAN architectures for synthesizing high quality liver lesion ROIs. Then we present a novel scheme for liver lesion classification using CNN. Finally, we train the CNN using classic data augmentation and our synthetic data augmentation and compare performance. In addition, we explore the quality of our synthesized examples using visualization and expert assessment. The classification performance using only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. By adding the synthetic data augmentation the results increased to 85.7% sensitivity and 92.4% specificity. We believe that this approach to synthetic data augmentation can generalize to other medical classification applications and thus support radiologists' efforts to improve diagnosis.",0
"This paper proposes using Generative Adversarial Networks (GAN) based synthetic medical image augmentation to improve performance on liver lesion classification tasks. Traditionally, deep learning approaches rely heavily on large datasets for accurate prediction, but acquiring such data can become costly and difficult due to privacy concerns or limited availability. Instead, we aim to generate new images by applying small perturbations to existing ones while keeping the original labels intact. We then train our convolutional neural network (CNN) model on both real and generated images, which leads to better generalization ability and improved detection accuracy in smaller datasets. Our results show that using synthetically augmented data yields higher precision rates compared to models trained solely on real data. Furthermore, we investigate different architectures for GANs and the influence of hyperparameters on their quality and diversity. Ultimately, our approach offers a simple yet effective solution for increasing dataset sizes without sacrificing efficiency or security. By leveraging advancements in generative models, medical practitioners may benefit from more robust and reliable diagnostic systems, ultimately leading to better patient outcomes.",1
"Low-end and compact mobile cameras demonstrate limited photo quality mainly due to space, hardware and budget constraints. In this work, we propose a deep learning solution that translates photos taken by cameras with limited capabilities into DSLR-quality photos automatically. We tackle this problem by introducing a weakly supervised photo enhancer (WESPE) - a novel image-to-image Generative Adversarial Network-based architecture. The proposed model is trained by under weak supervision: unlike previous works, there is no need for strong supervision in the form of a large annotated dataset of aligned original/enhanced photo pairs. The sole requirement is two distinct datasets: one from the source camera, and one composed of arbitrary high-quality images that can be generally crawled from the Internet - the visual content they exhibit may be unrelated. Hence, our solution is repeatable for any camera: collecting the data and training can be achieved in a couple of hours. In this work, we emphasize on extensive evaluation of obtained results. Besides standard objective metrics and subjective user study, we train a virtual rater in the form of a separate CNN that mimics human raters on Flickr data and use this network to get reference scores for both original and enhanced photos. Our experiments on the DPED, KITTI and Cityscapes datasets as well as pictures from several generations of smartphones demonstrate that WESPE produces comparable or improved qualitative results with state-of-the-art strongly supervised methods.",0
"In recent years, image quality improvement has become increasingly important due to advancements in display technology, leading to higher resolutions and larger screens. While camera sensors have improved significantly over time, limitations such as limited dynamic range and noise still persist. To address these issues, researchers have developed methods that can enhance images automatically based on user preferences or scene content. However, existing systems require either manual interaction by users to provide feedback or rely heavily on computational resources which makes them impractical for real-time use on low-end devices. Therefore there exists a need for efficient and accurate image enhancement models which run efficiently on resource constrained platforms while providing high accuracy. This work proposes WESPE (Weakly supervised photo enhancer), a novel model capable of delivering state-of-the-art results at high speeds and using minimal GPU resources without losing out on quality. In summary, we present a methodology towards efficient digital imagery post processing under weak annotation scenarios suitable for deployment across diverse platforms including handheld devices and surveillance cams making our system practically viable.",1
"We present a method for transferring neural representations from label-rich source domains to unlabeled target domains. Recent adversarial methods proposed for this task learn to align features across domains by fooling a special domain critic network. However, a drawback of this approach is that the critic simply labels the generated features as in-domain or not, without considering the boundaries between classes. This can lead to ambiguous features being generated near class boundaries, reducing target classification accuracy. We propose a novel approach, Adversarial Dropout Regularization (ADR), to encourage the generator to output more discriminative features for the target domain. Our key idea is to replace the critic with one that detects non-discriminative features, using dropout on the classifier network. The generator then learns to avoid these areas of the feature space and thus creates better features. We apply our ADR approach to the problem of unsupervised domain adaptation for image classification and semantic segmentation tasks, and demonstrate significant improvement over the state of the art. We also show that our approach can be used to train Generative Adversarial Networks for semi-supervised learning.",0
"In recent years, deep learning models have been successfully applied to many tasks across different domains, including computer vision, natural language processing, and speech recognition [2]. To improve their performance even further, researchers have proposed several regularization techniques that add additional constraints during training to prevent overfitting and produce more robust models. However, these methods often rely on hyperparameter tuning and may introduce extra computational overhead. This paper presents adversarial dropout (AD) as a simple yet effective alternative regularizer that achieves state-of-the-art results on CIFAR-10 and SVHN image classification benchmarks without requiring any hyperparameters. Our method relies on applying dropout at test time using adaptive k-center sampling, which allows us to efficiently sample possible model outputs under different drop configurations. We then train an auxiliary network that predicts whether the original output can survive this testing process, effectively turning the dropout layers into adversaries trying to â€œfoolâ€ our auxiliary classifier. By minimizing the prediction error of this adversary, we encourage the main model to generalize better and achieve higher accuracy overall. Finally, AD requires little computational overhead compared to other regularizers like weight decay or early stopping, making it well suited for large scale applications where runtime efficiency matters. Future work includes expanding the scope of AD beyond image classification to other modalities such as text or audio.",1
"Generative adversarial networks (GAN) have been effective for learning generative models for real-world data. However, existing GANs (GAN and its variants) tend to suffer from training problems such as instability and mode collapse. In this paper, we propose a novel GAN framework called evolutionary generative adversarial networks (E-GAN) for stable GAN training and improved generative performance. Unlike existing GANs, which employ a pre-defined adversarial objective function alternately training a generator and a discriminator, we utilize different adversarial training objectives as mutation operations and evolve a population of generators to adapt to the environment (i.e., the discriminator). We also utilize an evaluation mechanism to measure the quality and diversity of generated samples, such that only well-performing generator(s) are preserved and used for further training. In this way, E-GAN overcomes the limitations of an individual adversarial training objective and always preserves the best offspring, contributing to progress in and the success of GANs. Experiments on several datasets demonstrate that E-GAN achieves convincing generative performance and reduces the training problems inherent in existing GANs.",0
"Artificial intelligence has advanced rapidly over recent years due largely to advancements in generative adversarial networks (GANs). GANs consist of two sub-networks; one generator which creates new data points that fool a discriminator network, and another which attempts to detect whether those generated data points are real or fake. This competition leads to improved accuracy as both the generator and discriminator learn from each other. However, training stable GANs remains challenging. Evolutionary computation techniques can improve stability by introducing selective pressure into the learning process while simplifying architecture search. In our study, we present three novel contributions towards addressing issues related to instability. Firstly, we propose evolutionary generative adversarial networks (EGANs) where competitive coevolution drives better performance. Secondly, we introduce population based stabilization methods through selection and mutation inspired by natural genetics. Lastly, to assist automating model discovery with limited human supervision, we develop Latent Space Navigation (LSN), which enables effective navigation within the latent space by using simple heuristics and knowledge distillation. Our evaluation shows substantial improvements compared to current state of art approaches on several benchmark datasets including CIFAR-10, Styled GANs and ImageNet. Overall, our work provides promising results demonstrating the effectiveness of incorporating evolutionary computation techniques for enhancing GAN performance and discovering well-performing architectures automatically. With these findings, we hope to inspire future research aimed at further improving the reliability and efficiency of artificial intelligence algorithms.",1
"Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.",0
"In recent years, generative adversarial networks (GANs) have revolutionized the field of computer vision by enabling powerful image synthesis capabilities that rival human expertise. However, progress towards producing human-quality text using GANs has been slower due to challenges such as training instability, difficulty scaling up models, and poor sample diversity. To address these issues, we propose MaskGAN: a new architecture designed specifically for text generation applications. Our key insight lies in filling in the missing tokens required for next token prediction during inference instead of generating all tokens at once like prior work. We show empirically how this approach leads to more coherent and semantically meaningful outputs while greatly reducing the model size requirements. Extensive experiments on three natural language processing tasks demonstrate significant improvements over state-of-the-art methods, achieving results even comparable to strong rule-based systems. Overall, our findings suggest that masked sequence generation holds great promise for future research into GAN-driven NLP pipelines.",1
"The ability of a classifier to recognize unknown inputs is important for many classification-based systems. We discuss the problem of simultaneous classification and novelty detection, i.e. determining whether an input is from the known set of classes and from which specific class, or from an unknown domain and does not belong to any of the known classes. We propose a method based on the Generative Adversarial Networks (GAN) framework. We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector. We approximate that generator with a mixture generator trained with the Feature Matching loss and empirically show that the proposed method outperforms conventional methods for novelty detection. Our findings demonstrate a simple, yet powerful new application of the GAN framework for the task of novelty detection.",0
"Artificial Intelligence (AI) has made significant strides in recent years, but there remains one key challenge: detecting novel situations that have never been encountered before. One approach to solving this problem is through Generative Adversarial Networks (GANs), which can generate new examples that capture the underlying patterns of data distributions. In this work, we propose a novel method for leveraging GANs for novelty detection by incorporating uncertainty into both the generator and discriminator networks. By doing so, our model can better account for uncertainty in the data distribution and improve detection accuracy. Our experiments on several benchmark datasets show that our method outperforms state-of-the-art approaches across multiple evaluation metrics. Our contributions provide a promising step towards improving AI systems' ability to handle previously unseen scenarios.",1
We propose a novel approach for generating high quality visible-like images from Synthetic Aperture Radar (SAR) images using Deep Convolutional Generative Adversarial Network (GAN) architectures. The proposed approach is based on a cascaded network of convolutional neural nets (CNNs) for despeckling and image colorization. The cascaded structure results in faster convergence during training and produces high quality visible images from the corresponding SAR images. Experimental results on both simulated and real SAR images show that the proposed method can produce visible-like images better compared to the recent state-of-the-art deep learning-based methods.,0
"This paper presents a method for generating high quality visible images from synthetic aperture radar (SAR) images using convolutional neural networks (CNNs). The proposed approach leverages advances in deep learning to bridge the gap between raw SAR data and detailed visual representations of the physical world. We propose two novel architectures which exploit different types of prior knowledge: spatial hierarchies present within SAR scenes and interdependencies across spectral dimensions present within pixel values. Our methods outperform baseline approaches by large margins on both quantitative metrics and human assessments. We provide extensive analysis showing the importance of our proposed components along with qualitative evaluations. Code for reproducing all results is available online as supplementary material. Finally we end with discussion detailing promising directions and applications stemming from our research including remote sensing, disaster response, autonomous systems, and planetary exploration.",1
"Batch Normalization (BN) has been proven to be quite effective at accelerating and improving the training of deep neural networks (DNNs). However, BN brings additional computation, consumes more memory and generally slows down the training process by a large margin, which aggravates the training effort. Furthermore, the nonlinear square and root operations in BN also impede the low bit-width quantization techniques, which draws much attention in deep learning hardware community. In this work, we propose an L1-norm BN (L1BN) with only linear operations in both the forward and the backward propagations during training. L1BN is shown to be approximately equivalent to the original L2-norm BN (L2BN) by multiplying a scaling factor. Experiments on various convolutional neural networks (CNNs) and generative adversarial networks (GANs) reveal that L1BN maintains almost the same accuracies and convergence rates compared to L2BN but with higher computational efficiency. On FPGA platform, the proposed signum and absolute operations in L1BN can achieve 1.5$\times$ speedup and save 50\% power consumption, compared with the original costly square and root operations, respectively. This hardware-friendly normalization method not only surpasses L2BN in speed, but also simplify the hardware design of ASIC accelerators with higher energy efficiency. Last but not the least, L1BN promises a fully quantized training of DNNs, which is crucial to future adaptive terminal devices.",0
"In recent years, batch normalization (BN) has become an essential component in training deep neural networks due to its ability to accelerate convergence and reduce model complexity. However, using conventional BN during backpropagation can lead to high computational costs, especially when processing large mini-batch sizes. To address these limitations, we propose L1-norm Batch Normalization (L1-BN), which adaptively scales the input data by computing the element-wise minimum of each activation map and a learnable scalar factor. This approach reduces the memory footprint required for storing running means and covarances while maintaining similar performance compared to standard BN. We demonstrate the effectiveness of our method on several benchmark datasets including CIFAR-10, ImageNet, and UCF-101, showing that L1-BN achieves comparable accuracy with significantly fewer computations. Our experiments show that L1-BN outperforms other state-of-the-art techniques designed to improve efficiency without sacrificing accuracy, making it an ideal candidate for resource-constrained devices such as smartphones and embedded systems. Overall, our work provides insights into how to effectively combine robustness and efficiency in deep learning algorithms.",1
"This paper addresses a challenging problem -- how to generate multi-view cloth images from only a single view input. To generate realistic-looking images with different views from the input, we propose a new image generation model termed VariGANs that combines the strengths of the variational inference and the Generative Adversarial Networks (GANs). Our proposed VariGANs model generates the target image in a coarse-to-fine manner instead of a single pass which suffers from severe artifacts. It first performs variational inference to model global appearance of the object (e.g., shape and color) and produce a coarse image with a different view. Conditioned on the generated low resolution images, it then proceeds to perform adversarial learning to fill details and generate images of consistent details with the input. Extensive experiments conducted on two clothing datasets, MVC and DeepFashion, have demonstrated that images of a novel view generated by our model are more plausible than those generated by existing approaches, in terms of more consistent global appearance as well as richer and sharper details.",0
"Title: ""Generating Multiple Views From a Single Input Image""  In recent years, deep learning models have made significant advances in computer vision tasks such as object detection, segmentation, and image generation. However, most methods require large amounts of labeled data, which can be difficult to obtain and expensive to collect. One approach that has gained popularity recently is multi-view image generation, which involves generating multiple synthetic views of a single input image. This technique has several potential applications, including improving training data diversity, creating virtual scenes for augmented reality, and enhancing photo-realism in graphics rendering. In this paper, we propose a novel method for multi-view image generation using convolutional neural networks (CNNs) trained on paired input/output images. Our model utilizes adversarial loss functions, cycle consistency regularization, and color correction techniques to generate high quality, realistic images that closely resemble the ground truth. We evaluate our method on several datasets and show that it outperforms state-of-the-art approaches by producing more diverse and accurate outputs. Overall, our work demonstrates the effectiveness of multi-view image generation in addressing some of the limitations of traditional computer vision methods while providing new opportunities for innovative applications.",1
"In this work, we present a novel approach for training Generative Adversarial Networks (GANs). Using the attention maps produced by a Teacher- Network we are able to improve the quality of the generated images as well as perform weakly object localization on the generated images. To this end, we generate images of HEp-2 cells captured with Indirect Imunofluoresence (IIF) and study the ability of our network to perform a weakly localization of the cell. Firstly, we demonstrate that whilst GANs can learn the mapping between the input domain and the target distribution efficiently, the discriminator network is not able to detect the regions of interest. Secondly, we present a novel attention transfer mechanism which allows us to enforce the discriminator to put emphasis on the regions of interest via transfer learning. Thirdly, we show that this leads to more realistic images, as the discriminator learns to put emphasis on the area of interest. Fourthly, the proposed method allows one to generate both images as well as attention maps which can be useful for data annotation e.g in object detection.",0
"In recent years, generative adversarial networks (GANs) have emerged as one of the most powerful tools for generating realistic images and videos. However, traditional GAN models often suffer from instability during training and fail to generate high-quality samples consistently. This paper proposes attention-aware GANs (ATA-GANs), which can overcome these limitations by leveraging attention mechanisms to improve stability and produce better results.  The key idea behind ATA-GANs is that each step of image synthesis should attend to different regions of the generated image based on their relevance to the overall task. By attending selectively to important features, we can significantly reduce model capacity requirements and stabilize training. Moreover, our experimental evaluation shows that ATA-GANs outperform state-of-the-art methods on multiple benchmark datasets, demonstrating the effectiveness of our approach.  We first introduce the basic components of ATA-GANs and explain how they work together to achieve stable and effective generation. We then evaluate our method using both objective metrics and subjective user studies, showing that ATA-GANs consistently yield higher quality outputs than competing methods. Finally, we discuss possible future directions and applications of our framework beyond computer vision, including natural language processing and robotics. Overall, we believe that ATA-GANs represent a significant contribution to the field of deep learning and demonstrate the potential of attention mechanisms for improving GAN performance across various domains.",1
"Continuous appearance shifts such as changes in weather and lighting conditions can impact the performance of deployed machine learning models. While unsupervised domain adaptation aims to address this challenge, current approaches do not utilise the continuity of the occurring shifts. In particular, many robotics applications exhibit these conditions and thus facilitate the potential to incrementally adapt a learnt model over minor shifts which integrate to massive differences over time. Our work presents an adversarial approach for lifelong, incremental domain adaptation which benefits from unsupervised alignment to a series of intermediate domains which successively diverge from the labelled source domain. We empirically demonstrate that our incremental approach improves handling of large appearance changes, e.g. day to night, on a traversable-path segmentation task compared with a direct, single alignment step approach. Furthermore, by approximating the feature distribution for the source domain with a generative adversarial network, the deployment module can be rendered fully independent of retaining potentially large amounts of the related source training data for only a minor reduction in performance.",0
"Artificial intelligence systems often rely on large amounts of labeled data from a single domain to achieve accurate performance. However, these models can struggle when faced with new environments or tasks that differ significantly from their training data. To address this issue, we propose a novel approach called incremental adversarial domain adaptation (IADA) which allows AI systems to adapt continually over time as they encounter changes in their environment. Our method involves jointly learning two neural networks: one for the main task and another as an adversary to ensure robustness against distribution shifts. We demonstrate the effectiveness of our approach through extensive experiments across several benchmark datasets in various domains including image classification and object detection. Overall, our work shows significant improvement compared to state-of-the-art methods under conditions of both partial and full shift in environments. By enabling AI models to effectively adapt to changing scenarios, IADA has potential applications ranging from autonomous robots operating in dynamic real-world settings to personalized medicine where patient characteristics may evolve over time. This study paves the way towards more resilient artificial intelligence systems capable of handling unforeseen challenges.",1
"Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.",0
"Title: Automatic Generation of Adversarial Perturbations for Improved Evasion Attacks  Abstract: This paper presents a novel method for generating natural adversarial examples (NAEs) that can effectively evade detection by machine learning models. NAEs are crafted inputs designed to fool deep neural networks into making incorrect predictions while appearing visually similar to their original counterparts. Conventional methods manually create these perturbations through iterative trial-and-error procedures which tend to be time-consuming, expensive, and require domain-specific knowledge. In contrast, our approach employs state-of-the-art generative models trained on large datasets to generate perturbations automatically. We demonstrate the effectiveness of our approach using several benchmark datasets and show significant improvement over existing evasion techniques in terms of both success rate and visual similarity. Our results highlight the potential of automatic generation of adversarial examples as a powerful tool for evaluating and improving model robustness against real-world threats.",1
"In recent works, both sparsity-based methods as well as learning-based methods have proven to be successful in solving several challenging linear inverse problems. However, sparsity priors for natural signals and images suffer from poor discriminative capability, while learning-based methods seldom provide concrete theoretical guarantees. In this work, we advocate the idea of replacing hand-crafted priors, such as sparsity, with a Generative Adversarial Network (GAN) to solve linear inverse problems such as compressive sensing. In particular, we propose a projected gradient descent (PGD) algorithm for effective use of GAN priors for linear inverse problems, and also provide theoretical guarantees on the rate of convergence of this algorithm. Moreover, we show empirically that our algorithm demonstrates superior performance over an existing method of leveraging GANs for compressive sensing.",0
"""Solving linear inverse problems using deep neural networks has become increasingly popular due to their ability to model complex data distributions. However, training such models can be challenging and often requires large amounts of labeled data. To address these issues, we propose a novel algorithm that uses Generative Adversarial Network (GAN) priors to regularize the solution space and promote sparsity. Our method leverages recent advances in understanding the implicit bias of deep neural networks and allows us to prove theoretical guarantees on the quality of the reconstructed images. We demonstrate the effectiveness of our approach by applying it to several benchmark inverse problems and showcase improvements over state-of-the-art methods.""",1
"We propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). By fixing the identity portion of the latent codes, we can generate diverse images of the same subject, and by fixing the observation portion, we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce pairs that are photorealistic, distinct, and appear to depict the same individual. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to facilitate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm's ability to generate convincing, identity-matched photographs.",0
"GANs (Generative Adversarial Networks) have been at the forefront of research in image generation for several years now, but their workings are still not well understood. This work aims to provide insights into the latent spaces learned by GANs by performing semantic decompositions on them. We first explore how we can decompose these latent spaces using existing methods from computer vision and machine learning, such as linear discriminant analysis, t-SNE, UMAP and Hessian Eigenmaps. Our findings show that while some methods perform better than others, none of them fully capture the structure present within the latent space of a pretrained generator. To address this shortcoming, we introduce two new methods: one based on clustering the data points along trajectories connecting neighboring data points; another which uses a predefined set of prototypes. By validating our decomposition results against human judgments we demonstrate that both methods outperform traditional techniques considerably. Lastly, we discuss possible future work directions related to investigating whether decomposed structures generalize across multiple datasets and how they relate to classical dimensionality reduction methods.  In summary, we believe our novel approach allows us to investigate and quantify properties of the latent spaces learned by generative adversarial networks that are invisible to previously proposed methods. Since understanding hidden representations in deep neural networks has wide reaching applications, ranging from improving interpretability over generating descriptions of images and videos to designing improved architectures, we think the presented concepts and ideas make essential reading for anyone working in the field of computer vision and machine learning, but also those dealing with natural language processing where latent spaces also play important roles.",1
"Generative adversarial networks (GANs) are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training, and we demonstrate this on Celeba, Large-scale Scene Understanding (LSUN) bedrooms, and Imagenet without conditioning.",0
"Artificial intelligence (AI) has shown promising results in generating data samples that can fool humans into believing they were produced by real systems, often referred to as ""adversarial examples."" However, these techniques still face challenges such as instability during training, difficulty producing high-quality outputs, and lack of interpretability. In this paper, we present a new approach called boundary-seeking generative adversarial networks (BSGAN), which addresses some of these issues through the incorporation of a boundary loss function that guides generation towards plausible outcomes while encouraging discriminator robustness at classification boundaries. We demonstrate the effectiveness of our method on several benchmark datasets, showing improved stability and sample quality compared to state-of-the-art methods. Additionally, we provide insights into the operation of our model through visualizations and analysis, highlighting its potential applicability across different domains. Our work contributes to the development of more reliable and interpretable generative models with practical implications in fields such as computer vision and natural language processing.",1
One popular generative model that has high-quality results is the Generative Adversarial Networks(GAN). This type of architecture consists of two separate networks that play against each other. The generator creates an output from the input noise that is given to it. The discriminator has the task of determining if the input to it is real or fake. This takes place constantly eventually leads to the generator modeling the target distribution. This paper includes a study into the actual weights learned by the network and a study into the similarity of the discriminator and generator networks. The paper also tries to leverage the similarity between these networks and shows that indeed both the networks may have a similar structure with experimental evidence with a novel shared architecture.,0
"In recent years, Generative Adversarial Networks (GAN) have emerged as one of the most successful deep learning models for generating images with high visual fidelity. Despite their successes, understanding how different components in GAN contribute to image generation remains challenging due to the complex interplay among them. This study delves into investigating the relationship between generators and discriminators in GAN, two core elements that interact in adversarial training. Our results show significant correlation in features learned by both networks across multiple datasets, suggesting that they learn similar representations despite having conflicting objectives. By manipulating these shared features through regularization techniques such as weight decay and dropout, we observed corresponding changes in generated imagesâ€™ quality while maintaining stability in training dynamics. These findings provide new insights into the role of each component in achieving optimal performance in GAN, paving the way for more effective and stable model designs.",1
"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players' parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.",0
"Incorporating concepts from probability theory, information geometry, and optimization, we examine the behavior of gradient ascent training on GANs from two perspectives: first, by investigating how the divergence metric commonly used to measure the quality of generated images evolves over time; secondly, we look at how different architectures affect performance metrics like FID and IS. Our results challenge some common assumptions surrounding the use of GANs, including that the generator must necessarily converge toward a solution in which the discriminator fails completely, which is based on the Jensenâ€“Shannon (JS) divergence metric. We show examples where generators decrease their JS-divergences with respect to the discriminator, and yet fail miserably in terms of generating high-quality images according to standard FID and IS criteria. On the other hand, there are instances where the JS-divergence increases but generates higher quality samples; this indicates a disconnect between the JS-divergence and human judgement. This work opens up new possibilities for designing novel models and training methods using alternative metrics. Furthermore, we highlight the importance of evaluating models beyond just the most popular objective functions such as FID. We believe our research findings provide valuable insights into improving future versions of Generative Adversarial Networks.",1
"Generative Adversarial Network (GAN) and its variants have recently attracted intensive research interests due to their elegant theoretical foundation and excellent empirical performance as generative models. These tools provide a promising direction in the studies where data availability is limited. One common issue in GANs is that the density of the learned generative distribution could concentrate on the training data points, meaning that they can easily remember training samples due to the high model complexity of deep networks. This becomes a major concern when GANs are applied to private or sensitive data such as patient medical records, and the concentration of distribution may divulge critical patient information. To address this issue, in this paper we propose a differentially private GAN (DPGAN) model, in which we achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure. We provide rigorous proof for the privacy guarantee, as well as comprehensive empirical evidence to support our analysis, where we demonstrate that our method can generate high quality data points at a reasonable privacy level.",0
"Title: ""Preserving Privacy in Artificial Intelligence through Differentially Private Generative Adversarial Networks""  Artificial intelligence (AI) has rapidly advanced over recent years, leading to increased concerns regarding privacy violations due to sensitive data usage by AI models. One potential solution lies in employing differentially private machine learning techniques that can protect individual usersâ€™ information while still achieving high model accuracy. This paper presents a new methodology based on generative adversarial networks (GANs), which incorporate the principles of differential privacy. Our novel approach allows for fine-grained control over the amount of privacy protection required while maintaining model performance. Experimental results demonstrate the efficacy of our system compared to other state-of-the-art methods in preserving privacy while minimizing utility loss. Overall, we contribute a promising direction towards enhancing trustworthiness in modern artificial intelligence systems through effective privacy safeguards.",1
"Unsupervised image translation, which aims in translating two independent sets of images, is challenging in discovering the correct correspondences without paired data. Existing works build upon Generative Adversarial Network (GAN) such that the distribution of the translated images are indistinguishable from the distribution of the target set. However, such set-level constraints cannot learn the instance-level correspondences (e.g. aligned semantic parts in object configuration task). This limitation often results in false positives (e.g. geometric or semantic artifacts), and further leads to mode collapse problem. To address the above issues, we propose a novel framework for instance-level image translation by Deep Attention GAN (DA-GAN). Such a design enables DA-GAN to decompose the task of translating samples from two sets into translating instances in a highly-structured latent space. Specifically, we jointly learn a deep attention encoder, and the instancelevel correspondences could be consequently discovered through attending on the learned instance pairs. Therefore, the constraints could be exploited on both set-level and instance-level. Comparisons against several state-ofthe- arts demonstrate the superiority of our approach, and the broad application capability, e.g, pose morphing, data augmentation, etc., pushes the margin of domain translation problem.",0
"Title: Improving image translation using deep attention generative adversarial networks ==========================================================================================  Image translation refers to generating new images that depict the same content as another image but belong to a different domain such as style or resolution. Existing approaches have shown impressive results on various tasks, but they mostly focus on translating global features while neglecting local details. In addition, they often suffer from mode collapse due to poor training stability. To address these issues, we propose a novel approach based on attentional generative adversarial networks called ""DA-GAN."" Our method introduces dense attention modules that highlight locally important regions for more effective instance-level feature correspondence. This allows our model to produce high-quality translated outputs even if some parts are missing or unclear in input images. We demonstrate the effectiveness of our solution through extensive experiments and comparisons against state-of-the-art methods across multiple datasets and tasks, including photo portrait synthesis, semantic mapping, and weather condition translation. Moreover, we provide detailed analysis and ablation studies to thoroughly validate the contributions of each component. By improving both quality and efficiency, our work paves the way towards better realism, expressiveness, and adaptability in the field of computer vision and graphics.",1
"Measuring divergence between two distributions is essential in machine learning and statistics and has various applications including binary classification, change point detection, and two-sample test. Furthermore, in the era of big data, designing divergence measure that is interpretable and can handle high-dimensional and complex data becomes extremely important. In the paper, we propose a post selection inference (PSI) framework for divergence measure, which can select a set of statistically significant features that discriminate two distributions. Specifically, we employ an additive variant of maximum mean discrepancy (MMD) for features and introduce a general hypothesis test for PSI. A novel MMD estimator using the incomplete U-statistics, which has an asymptotically Normal distribution (under mild assumptions) and gives high detection power in PSI, is also proposed and analyzed theoretically. Through synthetic and real-world feature selection experiments, we show that the proposed framework can successfully detect statistically significant features. Last, we propose a sample selection framework for analyzing different members in the Generative Adversarial Networks (GANs) family.",0
This paper presents new results on post selection inference with incomplete maximum mean discrepancy estimators. These methods allow statisticians and data scientists to make more accurate predictions by taking into account data that has been left out during model training. We show how these techniques can lead to improved accuracy and interpretability across a wide range of applications including image recognition and natural language processing. Our work offers a fresh perspective on the challenges surrounding incomplete data sets and provides novel insights into the field of machine learning.,1
"We study in this paper the rate of convergence for learning densities under the Generative Adversarial Networks (GAN) framework, borrowing insights from nonparametric statistics. We introduce an improved GAN estimator that achieves a faster rate, through simultaneously leveraging the level of smoothness in the target density and the evaluation metric, which in theory remedies the mode collapse problem reported in the literature. A minimax lower bound is constructed to show that when the dimension is large, the exponent in the rate for the new GAN estimator is near optimal. One can view our results as answering in a quantitative way how well GAN learns a wide range of densities with different smoothness properties, under a hierarchy of evaluation metrics. As a byproduct, we also obtain improved generalization bounds for GAN with deeper ReLU discriminator network.",0
"In the field of machine learning, generative adversarial networks (GANs) have emerged as powerful tools for generating realistic data samples. GANs consist of two deep neural networks that compete against each other: one generates samples while the other discriminates between real and generated examples. Their success has been driven by their ability to learn complex data distributions without requiring explicit modeling assumptions such as normality or parametrization. However, little is known about how well GANs can recover densities from limited amounts of training data. This paper takes a nonparametric view on density estimation with GANs by studying the behavior of multiple variants of GAN architectures on datasets drawn from different domains. Our experiments indicate that even small sample sizes can lead to accurate recovery of high dimensional densities with GANs. Furthermore, we find that simple modifications to vanilla GAN architectures enable better estimates of multi-modal densities and improved robustness in noisy settings. We contribute open source code that allows practitioners to leverage our insights when working with their own data sets. Our results provide new intuitions into the capabilities of modern generative models, allowing us to confidently apply them across diverse applications where reliable probability predictions matter most.",1
"One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.",0
"Title: Spectral Normalization for Generative Adversarial Networks Author(s): J. Xavier Gomez, Hang Chu The problem addressed in this research is the difficulty in training generative adversarial networks (GANs) using non-spectral normalization methods that can lead to unstable convergence and poor performance. To address this issue, we propose spectral normalization as a more stable alternative method of training GANs which has been shown to produce state-of-the-art results across several benchmark datasets. We provide a comprehensive analysis of the effects of different normalization techniques on GANs through both theoretical derivations and empirical evaluations. Our findings indicate that spectral normalization significantly improves upon previous approaches by stabilizing the optimization process, reducing mode collapse, increasing diversity, enhancing image quality and producing higher fidelity samples. Furthermore, our method outperforms other recently proposed regularizers such as Clip Gradients and Rank Constraint. As a result, we conclude that spectral normalization represents a major step forward towards developing effective GAN architectures capable of generating high fidelity images. Keywords: Generative Adversarial Networks, Spectral Normalization, Image Generation, Training Stability References: [Insert references here]",1
"Generative adversarial networks (GANs) learn a deep generative model that is able to synthesise novel, high-dimensional data samples. New data samples are synthesised by passing latent samples, drawn from a chosen prior distribution, through the generative model. Once trained, the latent space exhibits interesting properties, that may be useful for down stream tasks such as classification or retrieval. Unfortunately, GANs do not offer an ""inverse model"", a mapping from data space back to latent space, making it difficult to infer a latent representation for a given data sample. In this paper, we introduce a technique, inversion, to project data samples, specifically images, to the latent space using a pre-trained GAN. Using our proposed inversion technique, we are able to identify which attributes of a dataset a trained GAN is able to model and quantify GAN performance, based on a reconstruction loss. We demonstrate how our proposed inversion technique may be used to quantitatively compare performance of various GAN models trained on three image datasets. We provide code for all of our experiments, https://github.com/ToniCreswell/InvertingGAN.",0
"Neural networks can generate highly realistic images by learning statistical patterns in large datasets but suffer from mode collapse - the tendency towards randomness instead of diversity. To counteract that, generative adversarial networks were introduced. Training two neural networks together ensures one network generates diverse images while keeping them plausible according to some measure of realism provided by the other network. However, current methods only allow fine-tuning pre-trained generators, which may not work well for specific tasks. We propose a novel approach where we invert the generator into explicit parameters controlling aspects such as stroke widths or color distribution. Instead of training both networks at once, our method first trains the discriminator against generated images and then trains the generator on top of the pre-trained weights using these explicit parameters. Using our technique we improve performance compared to standard GANs and achieve competitive results to state-of-the-art image generation models like Stable Diffusion. Furthermore, our method opens up new possibilities for creative control over generated images, e.g., artists can explicitly draw certain features onto their canvas, or style transfer along specific dimensions. -----",1
"Geostatistical modeling of petrophysical properties is a key step in modern integrated oil and gas reservoir studies. Recently, generative adversarial networks (GAN) have been shown to be a successful method for generating unconditional simulations of pore- and reservoir-scale models. This contribution leverages the differentiable nature of neural networks to extend GANs to the conditional simulation of three-dimensional pore- and reservoir-scale models. Based on the previous work of Yeh et al. (2016), we use a content loss to constrain to the conditioning data and a perceptual loss obtained from the evaluation of the GAN discriminator network. The technique is tested on the generation of three-dimensional micro-CT images of a Ketton limestone constrained by two-dimensional cross-sections, and on the simulation of the Maules Creek alluvial aquifer constrained by one-dimensional sections. Our results show that GANs represent a powerful method for sampling conditioned pore and reservoir samples for stochastic reservoir evaluation workflows.",0
"This should provide an overview of the content of your paper. Do not make any claims that cannot be supported by results presented in the paper. Hereâ€™s my attempt at writing one:  The development of petroleum reservoir simulators requires accurate numerical representations of porous media properties such as permeability and relative permeability. However, due to complex fluid interactions and varying heterogeneity across scales, these properties remain difficult to measure directly from core data. As a result, computational modeling approaches have been developed to estimate these parameters based on imaging technologies such as X-ray CT scanning or Nuclear Magnetic Resonance (NMR) measurements. In particular, conditioned Generative Adversarial Networks (cGANs), which use two neural network components trained against each other in a zero-sum competition framework, hold potential for accurately estimating these porous medium characteristics from various types of input data. Previous studies have shown promising results using cGANs with single-scalar field inputs; however, little has been done to investigate their extension to multi-field applications involving several scalar fields (e.g., water saturation, oil saturation, pressure). We hereby present work aimed at extending conditioned GAN architectures to function with multiple-input variables in order to improve accuracy of estimated rock property distributions and associated uncertainty quantification. We first evaluate the capacity of different existing state-of-the-art GAN variantsâ€”DCGAN, SAGAN, LAPGAN, CozGAN, MERFâ€”to handle multiple input modalities when applied to core data generated via stochastic simulations calibrated to seismic responses from real subsurface datasets. Our analyses demonstrate improved performance compared to previous studies relying o",1
"Current satellite imaging technology enables shooting high-resolution pictures of the ground. As any other kind of digital images, overhead pictures can also be easily forged. However, common image forensic techniques are often developed for consumer camera images, which strongly differ in their nature from satellite ones (e.g., compression schemes, post-processing, sensors, etc.). Therefore, many accurate state-of-the-art forensic algorithms are bound to fail if blindly applied to overhead image analysis. Development of novel forensic tools for satellite images is paramount to assess their authenticity and integrity. In this paper, we propose an algorithm for satellite image forgery detection and localization. Specifically, we consider the scenario in which pixels within a region of a satellite image are replaced to add or remove an object from the scene. Our algorithm works under the assumption that no forged images are available for training. Using a generative adversarial network (GAN), we learn a feature representation of pristine satellite images. A one-class support vector machine (SVM) is trained on these features to determine their distribution. Finally, image forgeries are detected as anomalies. The proposed algorithm is validated against different kinds of satellite images containing forgeries of different size and shape.",0
"This research work presents a novel approach for satellite image forgery detection and localization using Generative Adversarial Networks (GAN) and one-class classifiers. With the advancements in technology, it has become easy to manipulate digital images, making it challenging for users to identify authenticity of satellite images used in various applications such as military surveillance, weather forecasting, environmental monitoring, and urban planning. Therefore, there is a pressing need for robust methods that can detect and localize manipulations in satellite images.  Our proposed method leverages the power of deep learning techniques to address this problem. We first train a conditional GAN model on a large dataset of real satellite images, which learns to generate new synthetic images similar to real ones. Next, we create a corresponding one-class classification model by training it on the same dataset of real satellite images. During testing, the one-class classifier evaluates whether a given input image is authentic or not. If the image is deemed fake, the generator produces samples that resemble the tampered regions, which are then analyzed to locate the manipulated areas.  Experimental results demonstrate that our method achieves excellent performance compared to other state-of-the-art approaches in terms of detection accuracy, localization precision, and runtime efficiency. Our approach effectively handles various types of tampering, including object insertion/removal, texture modification, and contrast adjustments.  In summary, our study provides an innovative solution to address the critical issue of satellite image forgery detection and localization, enabling more secure use of these important tools across different fields. By combining the strengths of GANs and one-class classifiers, we have developed a powerful and efficient technique to safeguard trustworthiness of remote sensing data.",1
"Image demosaicing - one of the most important early stages in digital camera pipelines - addressed the problem of reconstructing a full-resolution image from so-called color-filter-arrays. Despite tremendous progress made in the pase decade, a fundamental issue that remains to be addressed is how to assure the visual quality of reconstructed images especially in the presence of noise corruption. Inspired by recent advances in generative adversarial networks (GAN), we present a novel deep learning approach toward joint demosaicing and denoising (JDD) with perceptual optimization in order to ensure the visual quality of reconstructed images. The key contributions of this work include: 1) we have developed a GAN-based approach toward image demosacing in which a discriminator network with both perceptual and adversarial loss functions are used for quality assurance; 2) we propose to optimize the perceptual quality of reconstructed images by the proposed GAN in an end-to-end manner. Such end-to-end optimization of GAN is particularly effective for jointly exploiting the gain brought by each modular component (e.g., residue learning in the generative network and perceptual loss in the discriminator network). Our extensive experimental results have shown convincingly improved performance over existing state-of-the-art methods in terms of both subjective and objective quality metrics with a comparable computational cost.",0
"This paper presents a novel approach to joint demosaicing and denoising using perceptual optimization on a generative adversarial network (GAN). Demosaicking and denoising are important image processing tasks that aim to improve the quality of raw sensor data captured by cameras. Traditional methods rely on handcrafted priors and filters which can lead to suboptimal results. GANs have recently shown promising results in generating realistic images but their use for low level vision tasks like demosaicking and denoising has been limited due to challenges in training stability and generating high fidelity outputs. In this work, we propose a new method called perceptual loss that improves upon existing losses used in GANs and allows us to train stable models capable of producing highly detailed and plausible output. Our contributions include: (i) a novel optimization framework that integrates perceptual loss into the GAN objective function; (ii) extensive evaluation showing state of the art performance compared to prior methods across multiple datasets including DSLR, smartphone, and medium format camera sensors; (iii) visualizations demonstrating the effectiveness of our method in preserving fine details while removing noise. Overall, our proposed method represents a significant improvement over traditional approaches and holds great promise as a general tool for imaging systems.",1
"We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous no-regret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.",0
"In this work, we present a new method for training generative adversarial networks (GANs) that improves stability and quality by introducing optimism into the learning process. We observe that vanilla GANs can become unstable due to their tendency towards conservatism during optimization, which slows down convergence and leads to suboptimal solutions. Our proposed method alleviates these issues by explicitly modeling uncertainty in the discriminator, allowing the generator to learn more quickly and efficiently. To achieve this, we introduce an additional loss term based on the variational lower bound of the entropy of the discriminator's predicted probabilities, which encourages the discriminator to make less certain predictions and therefore provides the generator with more informative gradients. Experimental results demonstrate that our approach leads to significant improvements over baseline methods across multiple domains including image generation, video prediction, and semi-supervised learning. We believe that the use ofoptimism in the context of training machine learning models holds promise for addressing similar challenges encountered in other applications.",1
"Unpaired Image-to-Image translation aims to convert the image from one domain (input domain A) to another domain (target domain B), without providing paired examples for the training. The state-of-the-art, Cycle-GAN demonstrated the power of Generative Adversarial Networks with Cycle-Consistency Loss. While its results are promising, there is scope for optimization in the training process. This paper introduces a new neural network architecture, which only learns the translation from domain A to B and eliminates the need for reverse mapping (B to A), by introducing a new Deviation-loss term. Furthermore, few other improvements to the Cycle-GAN are found and utilized in this new architecture, contributing to significantly lesser training duration.",0
"In traditional image-to-image translation systems, paired examples (pairs of images from two domains) must be provided during both training and testing stages. This work proposes unpaired image-to-image translation via multi-scale discriminator ensemble learning (MDILE). Our system uses several lightweight autoencoders as domain classifiers that can handle multiple scales simultaneously. These classifiers are integrated into an adversarial framework through weight sharing among multiple generators. MDILE significantly reduces memory footprints by reducing FP16 operations and realizes more stable GAN training through novel data augmentation methods like MixUp and CutOut in the pixel space. Experimental results on various datasets indicate better performance compared to state-of-the-art unsupervised and semi- supervised I2I models with less computational cost. This architecture provides opportunities to explore new directions in I2I research beyond the limitations imposed by limited labeled data availability. Keywords: image-to-image translation, unpaired translation, multi-domain learning, generative adversarial networks (GAN), MixUp, Cutout, UDA, CycleGAN",1
"Medical datasets are often highly imbalanced with over-representation of common medical problems and a paucity of data from rare conditions. We propose simulation of pathology in images to overcome the above limitations. Using chest X-rays as a model medical image, we implement a generative adversarial network (GAN) to create artificial images based upon a modest sized labeled dataset. We employ a combination of real and artificial images to train a deep convolutional neural network (DCNN) to detect pathology across five classes of chest X-rays. Furthermore, we demonstrate that augmenting the original imbalanced dataset with GAN generated images improves performance of chest pathology classification using the proposed DCNN in comparison to the same DCNN trained with the original dataset alone. This improved performance is largely attributed to balancing of the dataset using GAN generated images, where image classes that are lacking in example images are preferentially augmented.",0
"Radiologists often analyze chest x-ray images as part of their diagnosis procedures. However, the manual analysis process can lead to high inter-observer variability due to subjective interpretation. In recent years, deep neural networks (DNN) have been used to automatically classify chest pathologies on radiological scans by utilizing supervised learning techniques. While some methods have shown promising results, they still lack generalizability across different imaging environments because of limited training datasets and biased towards specific features. In order to address these issues and improve upon existing DNN models for chest x-ray classification, we propose to use generative adversarial networks (GAN). GANs allow us to generate synthetic data that resembles real chest x-rays with diverse variations while preserving important diagnostic characteristics, effectively augmenting our dataset and improving model robustness. By integrating discriminator feedback into our generator, we are able to produce more realistic and varied synthetic samples compared to traditional approaches. Our proposed method leverages both generator and discriminator outputs to fine-tune the networkâ€™s feature extractor in order to better learn relevant representations from raw image inputs. To evaluate the effectiveness of our approach, we conduct experiments using publicly available chest x-ray datasets such as Montgomery County Chest X-Ray14 (MIMIC-CXR), Chexpert25 and Indian Chest X-ray8 (IDDD) datasets, and compare against other state-of-the-art methods. Results show significant improvement in overall accuracy and interpretability. We demonstrate that our model achieves state-ofthe-art performance across all three testing sets. Additionally, we perform ablation studies t",1
"High spectral dimensionality and the shortage of annotations make hyperspectral image (HSI) classification a challenging problem. Recent studies suggest that convolutional neural networks can learn discriminative spatial features, which play a paramount role in HSI interpretation. However, most of these methods ignore the distinctive spectral-spatial characteristic of hyperspectral data. In addition, a large amount of unlabeled data remains an unexploited gold mine for efficient data use. Therefore, we proposed an integration of generative adversarial networks (GANs) and probabilistic graphical models for HSI classification. Specifically, we used a spectral-spatial generator and a discriminator to identify land cover categories of hyperspectral cubes. Moreover, to take advantage of a large amount of unlabeled data, we adopted a conditional random field to refine the preliminary classification results generated by GANs. Experimental results obtained using two commonly studied datasets demonstrate that the proposed framework achieved encouraging classification accuracy using a small number of data for training.",0
"Abstract: In recent years, hyperspectral image classification has emerged as a critical task in remote sensing, environmental monitoring, and geographic information systems. Traditional approaches have relied on handcrafted features and machine learning algorithms to classify hyperspectral images, but these methods often suffer from limited performance due to their reliance on domain knowledge and lack of robustness to noise. This paper proposes a new approach that combines generative adversarial networks (GANs) with probabilistic graph models (PGMs), providing a powerful framework for hyperspectral image classification. GANs enable efficient generation of training data, while PGMs provide a flexible modeling structure that can capture complex dependencies between spectral bands and spatial context. Our experimental results demonstrate that our proposed method outperforms state-of-the-art techniques across multiple benchmark datasets, showcasing its effectiveness in achieving high accuracy in hyperspectral image classification.",1
"It is increasingly common in many types of natural and physical systems (especially biological systems) to have different types of measurements performed on the same underlying system. In such settings, it is important to align the manifolds arising from each measurement in order to integrate such data and gain an improved picture of the system. We tackle this problem using generative adversarial networks (GANs). Recently, GANs have been utilized to try to find correspondences between sets of samples. However, these GANs are not explicitly designed for proper alignment of manifolds. We present a new GAN called the Manifold-Aligning GAN (MAGAN) that aligns two manifolds such that related points in each measurement space are aligned together. We demonstrate applications of MAGAN in single-cell biology in integrating two different measurement types together. In our demonstrated examples, cells from the same tissue are measured with both genomic (single-cell RNA-sequencing) and proteomic (mass cytometry) technologies. We show that the MAGAN successfully aligns them such that known correlations between measured markers are improved compared to other recently proposed models.",0
"This paper presents MAGAN (Manifold Analysis based on Graph ANalysis), a novel methodology that enables high-resolution analysis of molecular data sets. By leveraging graph theoretical principles, we construct biological networks from multi-modal omics datasets, where entities represent either biomolecules or contextual factors such as age, disease state, exposures etc., and edges indicate pairwise relations between them. We then employ diffusion kernel embedding techniques to transform these graphs into continuous spaces suitable for alignment via optimal transport metrics. With application to breast cancer subtype stratification using multiomic gene expression profiles, our framework demonstrates improved accuracy in comparison to alternative methods. Our approach has potential broad utility across diverse applications ranging from drug development to precision medicine to environmental health studies.  Note: I added some more details on how MAGAN works so that readers can have a better understanding of your research! Let me know if there is anything else you would like to add or change.",1
"How to build a good model for image generation given an abstract concept is a fundamental problem in computer vision. In this paper, we explore a generative model for the task of generating unseen images with desired features. We propose the Generative Cooperative Net (GCN) for image generation. The idea is similar to generative adversarial networks except that the generators and discriminators are trained to work accordingly. Our experiments on hand-written digit generation and facial expression generation show that GCN's two cooperative counterparts (the generator and the classifier) can work together nicely and achieve promising results. We also discovered a usage of such generative model as an data-augmentation tool. Our experiment of applying this method on a recognition task shows that it is very effective comparing to other existing methods. It is easy to set up and could help generate a very large synthesized dataset.",0
"This work presents a new generative model for image generation and data augmentation tasks that we call ""Generative Cooperative Network"" (GenCoNet). GenCoNet leverages a unique architecture consisting of multiple cooperating subnetworks that share their parameters in a weight-sharing manner. By designing a novel adversarial training objective function based on cooperation among these subnetworks, our approach achieves superior results over existing state-of-the-art models across a variety of metrics. In addition, we demonstrate the effectiveness of using GenCoNet as a general purpose tool for generating images conditioned on text prompts, as well as performing several important computer vision tasks such as object detection and semantic segmentation through data augmentation. Our code and trained models will be made publicly available upon acceptance.",1
"Neural samplers such as variational autoencoders (VAEs) or generative adversarial networks (GANs) approximate distributions by transforming samples from a simple random source---the latent space---to samples from a more complex distribution represented by a dataset. While the manifold hypothesis implies that the density induced by a dataset contains large regions of low density, the training criterions of VAEs and GANs will make the latent space densely covered. Consequently points that are separated by low-density regions in observation space will be pushed together in latent space, making stationary distances poor proxies for similarity. We transfer ideas from Riemannian geometry to this setting, letting the distance between two points be the shortest path on a Riemannian manifold induced by the transformation. The method yields a principled distance measure, provides a tool for visual inspection of deep generative models, and an alternative to linear interpolation in latent space. In addition, it can be applied for robot movement generalization using previously learned skills. The method is evaluated on a synthetic dataset with known ground truth; on a simulated robot arm dataset; on human motion capture data; and on a generative model of handwritten digits.",0
"The use of deep generative models (DGMs) has become increasingly prevalent in recent years due to their ability to generate realistic data samples. However, evaluating the performance of DGMs can be challenging as there is no ground truth available to measure against. In this paper, we propose several novel metrics that can be used to evaluate the quality and coherency of data generated by DGMs. These metrics take into account factors such as sample diversity, sample fidelity, and mode dropping. We provide examples of how these metrics can be applied to different types of DGMs including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Normalizing Flows. Our experimental results show that our proposed metrics are able to effectively capture important aspects of DGM performance, making them useful tools for researchers working on developing and improving these models. Overall, this work contributes towards a better understanding of DGM performance evaluation and will aid in advancing the state-of-the art in generative modeling.",1
"Driven by successes in deep learning, computer vision research has begun to move beyond object detection and image classification to more sophisticated tasks like image captioning or visual question answering. Motivating such endeavors is the desire for models to capture not only objects present in an image, but more fine-grained aspects of a scene such as relationships between objects and their attributes. Scene graphs provide a formal construct for capturing these aspects of an image. Despite this, there have been only a few recent efforts to generate scene graphs from imagery. Previous works limit themselves to settings where bounding box information is available at train time and do not attempt to generate scene graphs with attributes. In this paper we propose a method, based on recent advancements in Generative Adversarial Networks, to overcome these deficiencies. We take the approach of first generating small subgraphs, each describing a single statement about a scene from a specific region of the input image chosen using an attention mechanism. By doing so, our method is able to produce portions of the scene graphs with attribute information without the need for bounding box labels. Then, the complete scene graph is constructed from these subgraphs. We show that our model improves upon prior work in scene graph generation on state-of-the-art data sets and accepted metrics. Further, we demonstrate that our model is capable of handling a larger vocabulary size than prior work has attempted.",0
"Scene graphs are widely used as input representations for computer vision tasks such as scene understanding, visual reasoning, and generation. Constructing high-quality scene graphs is challenging due to their inherent ambiguity arising from incomplete annotations or variations in human perception. In this work, we propose using adversarial networks to guide the learning process by adding explicit constraints on the generated scene graph structure. Our method generates plausible triplets (subjectâ€“predicateâ€“object) that can accurately reflect object relationships in images without relying heavily on supervision signals. We experimentally demonstrate significant improvements over alternative methods in terms of both quality and efficiency, providing new insights into how unsupervised approaches benefit from adversarial learning frameworks. Key contributions:  * Introduce novel methodology for constructing scene graphs based on generative adversarial networks;  * Present an extensive set of experiments evaluating performance against existing benchmark models;  * Provide discussion on broader implications of these findings in the field of computer vision research;  This study presents a unique approach to generating accurate scene graphs through the use of adversarial networks. Our method uses this technique to generate triplets which reflect true object relations within an image, without requiring heavy reliance on labeled data. This advancement achieves improved results compared to prior techniques while demonstrating the effectiveness of utilizing unsupervised models within the domain of computer vision.",1
"Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Supervised cross-modal hashing methods have achieved considerable progress by incorporating semantic side information. However, they mainly have two limitations: (1) Heavily rely on large-scale labeled cross-modal training data which are labor intensive and hard to obtain. (2) Ignore the rich information contained in the large amount of unlabeled data across different modalities, especially the margin examples that are easily to be incorrectly retrieved, which can help to model the correlations. To address these problems, in this paper we propose a novel Semi-supervised Cross-Modal Hashing approach by Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN's ability for modeling data distributions to promote cross-modal hashing learning in an adversarial way. The main contributions can be summarized as follows: (1) We propose a novel generative adversarial network for cross-modal hashing. In our proposed SCH-GAN, the generative model tries to select margin examples of one modality from unlabeled data when giving a query of another modality. While the discriminative model tries to distinguish the selected examples and true positive examples of the query. These two models play a minimax game so that the generative model can promote the hashing performance of discriminative model. (2) We propose a reinforcement learning based algorithm to drive the training of proposed SCH-GAN. The generative model takes the correlation score predicted by discriminative model as a reward, and tries to select the examples close to the margin to promote discriminative model by maximizing the margin between positive and negative data. Experiments on 3 widely-used datasets verify the effectiveness of our proposed approach.",0
"In recent years, hashing techniques have emerged as an effective solution for large-scale visual retrieval tasks due to their ability to convert high-dimensional features into compact binary codes. However, most existing methods require large amounts of labeled data, which can be expensive and time-consuming to obtain. To address this limitation, we propose a semi-supervised cross-modal hashing method based on generative adversarial networks (SCH-GAN). Our approach uses both labeled and unlabeled data from multiple modalities such as image, text, audio, etc., to learn a joint embedding space that preserves the semantic relationships between different modalities.  Our key idea is to formulate hashing as a multi-task learning problem where two sub-tasks are involved: one task focuses on minimizing the distance between pairs of samples with similar semantics across different modalities, while the other task maximizes the distance between pairs of samples with dissimilar semantics. By designing a generator network that maps inputs to binary codes using a deep convolutional neural network, we are able to generate realistic hash codes that preserve the structure of the original feature space. We then use a discriminator network to evaluate the quality of these generated codes by comparing them against ground truth labels. This allows us to train our model in a fully supervised manner on the labeled set and in a weakly supervised manner on the unlabeled sets.  We demonstrate the effectiveness of our proposed method through extensive experiments on several benchmark datasets. Results show that our method significantly outperforms state-of-the-art hashing algorithms in terms of accuracy and efficiency. Furthermore, our approach achieves better performance even with less labeled data, making it an attractive option for practical applications where labeling cost is prohibitive. Overall, our work highlights the potential of combining GANs and hashing techniques for semi-supervised cross-modal retrieval tasks.",1
"The training of Generative Adversarial Networks is a difficult task mainly due to the nature of the networks. One such issue is when the generator and discriminator start oscillating, rather than converging to a fixed point. Another case can be when one agent becomes more adept than the other which results in the decrease of the other agent's ability to learn, reducing the learning capacity of the system as a whole. Additionally, there exists the problem of Mode Collapse which involves the generators output collapsing to a single sample or a small set of similar samples. To train GANs a careful selection of the architecture that is used along with a variety of other methods to improve training. Even when applying these methods there is low stability of training in relation to the parameters that are chosen. Stochastic ensembling is suggested as a method for improving the stability while training GANs.",0
"In this paper we present a novel method for training neural networks using an ensemble of generative pseudo-adversarial networks (GPAN). Our approach utilizes stochastic deconvolutional neural network ensembles to generate synthetic data that acts as ""fooling"" examples for the GPAN generator during training. We show that our technique leads to improved performance on multiple benchmark datasets across a variety of tasks including image generation, object detection, and semantic segmentation. Furthermore, we demonstrate the generalizability of our framework by applying it to non-image data types such as time series and text sequences. By leveraging both adversarial training techniques and variational autoencoders, our work represents a significant advance in neural network training methods. Overall, our contributions provide new insights into the potential of GPAN ensembles and offer practical applications for enhancing machine learning systems.",1
"Currently, Segmentation of bitewing radiograpy images is a very challenging task. The focus of the study is to segment it into caries, enamel, dentin, pulp, crowns, restoration and root canal treatments. The main method of semantic segmentation of bitewing radiograpy images at this stage is the U-shaped deep convolution neural network, but its accuracy is low. in order to improve the accuracy of semantic segmentation of bitewing radiograpy images, this paper proposes the use of Conditional Generative Adversarial network (cGAN) combined with U-shaped network structure (U-Net) approach to semantic segmentation of bitewing radiograpy images. The experimental results show that the accuracy of cGAN combined with U-Net is 69.7%, which is 13.3% higher than the accuracy of u-shaped deep convolution neural network of 56.4%.",0
"In our work we present an approach that allows for automatic semantic segmentation of bitewing radiographs using conditional generative adversarial networks (cGANs). We trained a cGAN model by fine tuning two pretrained architectures and using an appropriate loss function to improve performance. Our experiments show significant improvements over prior state of art techniques such as thresholding, manual annotation, and other machine learning models including UNet which has been used in previous related works. This method should prove very effective in medical diagnosis and decision support applications especially since it performs well under difficult circumstances where there is low visibility due to artifacts such as metal fillings in teeth. Overall our work offers valuable contributions towards accurate radiological image analysis for improved health care outcomes via increased efficiency through automated diagnostics which can scale better than traditional human based approaches.",1
"In unsupervised data generation tasks, besides the generation of a sample based on previous observations, one would often like to give hints to the model in order to bias the generation towards desirable metrics. We propose a method that combines Generative Adversarial Networks (GANs) and reinforcement learning (RL) in order to accomplish exactly that. While RL biases the data generation process towards arbitrary metrics, the GAN component of the reward function ensures that the model still remembers information learned from data. We build upon previous results that incorporated GANs and RL in order to generate sequence data and test this model in several settings for the generation of molecules encoded as text sequences (SMILES) and in the context of music generation, showing for each case that we can effectively bias the generation process towards desired metrics.",0
"Title: Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models  Abstract: Generative adversarial networks (GANs) have revolutionized generative modeling tasks by enabling stable training of deep neural networks that can generate new data samples from random noise vectors. However, standard GAN architectures often suffer from instability during training and may produce blurry visual outputs. In contrast, objectives such as variational autoencoders (VAEs) focus on generating realistic data distributions by explicitly optimizing reconstruction loss. Despite their effectiveness, these methods often overlook important aspects of the underlying problem at hand and may therefore lead to suboptimal solutions. We propose a novel framework called Objective Reinforcement Generative Adversarial Networks (ORGAN), which combines both VAE and GAN components into one architecture. Our proposed method enhances the strengths of each approach while mitigating their weaknesses. ORGAN utilizes reinforcement learning techniques to balance the latent code optimization of VAEs with the sample generation capability of GANs. The use of a dynamic latent code encoder allows for flexible modulation of generator behavior, further improving performance on complex sequence generation problems. Extensive experiments demonstrate that our method achieves state-of-the-art results across multiple challenging benchmark datasets, demonstrating its versatility and applicability to a wide range of sequence generation tasks. Overall, ORGAN represents a significant advance in the field of machine learning by providing a powerful yet generalizable solution for generative sequence models.",1
"Spectral images captured by satellites and radio-telescopes are analyzed to obtain information about geological compositions distributions, distant asters as well as undersea terrain. Spectral images usually contain tens to hundreds of continuous narrow spectral bands and are widely used in various fields. But the vast majority of those image signals are beyond the visible range, which calls for special visualization technique. The visualizations of spectral images shall convey as much information as possible from the original signal and facilitate image interpretation. However, most of the existing visualizatio methods display spectral images in false colors, which contradict with human's experience and expectation. In this paper, we present a novel visualization generative adversarial network (GAN) to display spectral images in natural colors. To achieve our goal, we propose a loss function which consists of an adversarial loss and a structure loss. The adversarial loss pushes our solution to the natural image distribution using a discriminator network that is trained to differentiate between false-color images and natural-color images. We also use a cycle loss as the structure constraint to guarantee structure consistency. Experimental results show that our method is able to generate structure-preserved and natural-looking visualizations.",0
"In recent years, generative adversarial networks (GANs) have shown promise as a tool for generating realistic synthetic images that can be used in applications such as medical imaging and computer graphics. However, traditional GAN models suffer from limitations in their ability to handle high resolution images and generate highly detailed outputs. This paper presents a new method for spectral image visualization using GANs, which addresses these limitations by leveraging recent advances in deep learning architectures and optimization techniques. Our approach incorporates both global context and local details into the generator network, allowing for more accurate and coherent results. We evaluate our model on several benchmark datasets and compare its performance against state-of-the art methods. Results demonstrate significant improvements in terms of quality, perceptual fidelity, and visual similarity metrics, making our proposed method well suited for a variety of image generation tasks.",1
"We relate the minimax game of generative adversarial networks (GANs) to finding the saddle points of the Lagrangian function for a convex optimization problem, where the discriminator outputs and the distribution of generator outputs play the roles of primal variables and dual variables, respectively. This formulation shows the connection between the standard GAN training process and the primal-dual subgradient methods for convex optimization. The inherent connection does not only provide a theoretical convergence proof for training GANs in the function space, but also inspires a novel objective function for training. The modified objective function forces the distribution of generator outputs to be updated along the direction according to the primal-dual subgradient methods. A toy example shows that the proposed method is able to resolve mode collapse, which in this case cannot be avoided by the standard GAN or Wasserstein GAN. Experiments on both Gaussian mixture synthetic data and real-world image datasets demonstrate the performance of the proposed method on generating diverse samples.",0
"This paper investigates the application of primal-dual subgradient methods for training generative adversarial networks (GAN). In contrast to existing work that uses gradient ascent or descent-based approaches, our approach takes into account both the objective function value and constraints satisfied by generator and discriminator models, thus optimizing over Lagrange multipliers associated with these constraints along each step of the subgradient method. Simulations demonstrate that the proposed algorithms offer more accurate performance relative to alternative gradient optimization techniques commonly used to train GANs, suggesting their potential applicability across domains where stable, efficient optimization is essential for effective model training. Keywords: generative adversarial network, subgradient method, Lagrangian perspective, optimization, simulation study.",1
"In recent years, neural network approaches have been widely adopted for machine learning tasks, with applications in computer vision. More recently, unsupervised generative models based on neural networks have been successfully applied to model data distributions via low-dimensional latent spaces. In this paper, we use Generative Adversarial Networks (GANs) to impose structure in compressed sensing problems, replacing the usual sparsity constraint. We propose to train the GANs in a task-aware fashion, specifically for reconstruction tasks. We also show that it is possible to train our model without using any (or much) non-compressed data. Finally, we show that the latent space of the GAN carries discriminative information and can further be regularized to generate input features for general inference tasks. We demonstrate the effectiveness of our method on a variety of reconstruction and classification problems.",0
"In todayâ€™s world where massive amounts of data can be collected quickly and easily, finding ways to compress this data has become increasingly important. One method that has been developed to address this issue is task-aware compressed sensing with generative adversarial networks (GANs). This approach involves training two neural networks together using datasets: one â€œgeneratorâ€ network which produces examples similar to those seen during training but different from real data and another network which learns how to discriminate these generated examples from actual ones. By doing so, we are able to recover signals at nearly the rate of innovation and compression as if they were chosen based on prior knowledge rather than randomness; however, our algorithm is significantly faster since the computational cost only depends on image size. Furthermore, by incorporating sparsity into the loss function used to train these models in tandem, we show that performance actually improves! We provide theoretical analysis showing the benefits of our approach and evaluate its merits through comprehensive experiments both quantitatively via numerical simulations and qualitatively via user studies comparing the output images generated by traditional deep learning algorithms against those created using GANs trained under our approach. Our research provides compelling evidence suggesting GANs are capable of producing high quality results that rival human expertise. As such, future work could investigate expanding their scope beyond image generation tasks into other areas like video completion or natural language understanding problems. Finally, we note limitations regarding current hardware requirements impeding deployment of our technique in practice given current computational constraints but believe future advances in technology will likely mitigate these issues down the line allowing wider adoption. Overall, our contributions emphasize t",1
"Acquiring images of the same anatomy with multiple different contrasts increases the diversity of diagnostic information available in an MR exam. Yet, scan time limitations may prohibit acquisition of certain contrasts, and images for some contrast may be corrupted by noise and artifacts. In such cases, the ability to synthesize unacquired or corrupted contrasts from remaining contrasts can improve diagnostic utility. For multi-contrast synthesis, current methods learn a nonlinear intensity transformation between the source and target images, either via nonlinear regression or deterministic neural networks. These methods can in turn suffer from loss of high-spatial-frequency information in synthesized images. Here we propose a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves high-frequency details via an adversarial loss; and it offers enhanced synthesis performance via a pixel-wise loss for registered multi-contrast images and a cycle-consistency loss for unregistered images. Information from neighboring cross-sections are utilized to further improved synthesis quality. Demonstrations on T1- and T2-weighted images from healthy subjects and patients clearly indicate the superior performance of the proposed approach compared to previous state-of-the-art methods. Our synthesis approach can help improve quality and versatility of multi-contrast MRI exams without the need for prolonged examinations.",0
"This paper presents a novel method for generating synthetic multi-contrast magnetic resonance imaging (MRI) using conditional generative adversarial networks (cGAN). We evaluate the performance of our algorithm on simulated data, demonstrating that it can accurately generate high quality images across multiple contrast mechanisms, including T2 weighted spin echo, proton density (PD), and susceptibility weighted imaging (SWI). Additionally, we show promising results in real patient datasets, suggesting that cGAN may have clinical utility for image generation in MRI studies. Our work represents a significant advancement over existing methods for synthesizing MRI images, as well as other imaging modalities such as CT and PET, and highlights the potential use of GANs for medical image analysis applications. Furthermore, the ability of our method to generate images in different contrast regimes allows for new ways to interpret the underlying tissue properties from a single set of parameters driving the generator network. Ultimately, this opens up opportunities to develop new diagnosis strategies by exploiting these complementary contrast characteristics and their relationship with disease states.",1
"Scenario generation is an important step in the operation and planning of power systems with high renewable penetrations. In this work, we proposed a data-driven approach for scenario generation using generative adversarial networks, which is based on two interconnected deep neural networks. Compared with existing methods based on probabilistic models that are often hard to scale or sample from, our method is data-driven, and captures renewable energy production patterns in both temporal and spatial dimensions for a large number of correlated resources. For validation, we use wind and solar times-series data from NREL integration data sets. We demonstrate that the proposed method is able to generate realistic wind and photovoltaic power profiles with full diversity of behaviors. We also illustrate how to generate scenarios based on different conditions of interest by using labeled data during training. For example, scenarios can be conditioned on weather events~(e.g. high wind day) or time of the year~(e,g. solar generation for a day in July). Because of the feedforward nature of the neural networks, scenarios can be generated extremely efficiently without sophisticated sampling techniques.",0
"We present a method for generating renewable energy scenarios using generative adversarial networks (GAN). GANs can generate realistic synthetic data which can provide valuable insight into how different scenarios may play out. However, while there have been attempts to apply GANs to scenario generation before, our approach represents the first model free attempt at such a problem. Our model consists only of two deep neural network components: a generator and discriminator, trained together via minimax optimization. By eliminating any other components, we simplify and streamline the process, reducing computation time without sacrificing results quality. Experiments demonstrate that our framework effectively predicts future power generation based on historical measurements, significantly improving over existing approaches.",1
"In this paper, we propose a multi-generator extension to the adversarial training framework, in which the objective of each generator is to represent a unique component of a target mixture distribution. In the training phase, the generators cooperate to represent, as a mixture, the target distribution while maintaining distinct manifolds. As opposed to traditional generative models, inference from a particular generator after training resembles selective sampling from a unique component in the target distribution. We demonstrate the feasibility of the proposed architecture both analytically and with basic Multi-Layer Perceptron (MLP) models trained on the MNIST dataset.",0
"In generative adversarial networks (GANs), selective sampling and mixture models can provide significant improvements over traditional GAN architectures by enabling greater control over generated outputs and increasing robustness against input perturbations. This paper presents novel techniques that leverage these ideas, demonstrating their effectiveness through extensive experimental evaluations on synthetic datasets as well as real-world tasks such as image generation and semantic segmentation. Our findings suggest that combining these powerful modeling components within a unified framework leads to state-of-the-art performance across multiple domains while providing new insights into how GANs work at a fundamental level. We hope that our research inspires further investigation into alternative approaches for training and deploying GANs in practice.",1
"In this paper, we address the incremental classifier learning problem, which suffers from catastrophic forgetting. The main reason for catastrophic forgetting is that the past data are not available during learning. Typical approaches keep some exemplars for the past classes and use distillation regularization to retain the classification capability on the past classes and balance the past and new classes. However, there are four main problems with these approaches. First, the loss function is not efficient for classification. Second, there is unbalance problem between the past and new classes. Third, the size of pre-decided exemplars is usually limited and they might not be distinguishable from unseen new classes. Forth, the exemplars may not be allowed to be kept for a long time due to privacy regulations. To address these problems, we propose (a) a new loss function to combine the cross-entropy loss and distillation loss, (b) a simple way to estimate and remove the unbalance between the old and new classes , and (c) using Generative Adversarial Networks (GANs) to generate historical data and select representative exemplars during generation. We believe that the data generated by GANs have much less privacy issues than real images because GANs do not directly copy any real image patches. We evaluate the proposed method on CIFAR-100, Flower-102, and MS-Celeb-1M-Base datasets and extensive experiments demonstrate the effectiveness of our method.",0
"As artificial intelligence continues to advance and play a more significant role in our lives, there has been increasing interest in developing methods that can efficiently learn from data without requiring large amounts of labeled examples. In this context, generative adversarial networks (GANs) have emerged as a promising approach due to their ability to generate realistic synthetic samples that can be used to improve the performance of machine learning models on complex tasks such as image classification.  In recent years, several works have proposed combining GANs with incremental classifiers, which continuously update their predictions based on new observations, to achieve better results than static classifiers trained on fixed sets of features. This combination offers advantages over traditional batch learning approaches by enabling efficient use of computational resources and allowing models to adapt to changing environments, making them particularly well suited for applications involving streaming or online data.  This paper presents a novel method for using GANs in conjunction with incremental classifiers for the task of image classification. Our approach leverages the power of GANs to generate synthetic training samples and incorporates these into existing incremental classifier frameworks. We demonstrate through extensive experimentation on several benchmark datasets that our method leads to improved accuracy compared to both static classifiers and alternative state-of-the-art incremental learning techniques.  Overall, we believe that our work represents an important contribution to the field of incremental classifier learning and highlights the potential benefits of integrating GANs into these types of systems. Further research could explore additional ways to enhance the effectiveness of GANs within incremental learning settings, potentially leading to even greater advances in artificial intelligence and related fields.",1
"Generative adversarial networks (GANs) while being very versatile in realistic image synthesis, still are sensitive to the input distribution. Given a set of data that has an imbalance in the distribution, the networks are susceptible to missing modes and not capturing the data distribution. While various methods have been tried to improve training of GANs, these have not addressed the challenges of covering the full data distribution. Specifically, a generator is not penalized for missing a mode. We show that these are therefore still susceptible to not capturing the full data distribution.   In this paper, we propose a simple approach that combines an encoder based objective with novel loss functions for generator and discriminator that improves the solution in terms of capturing missing modes. We validate that the proposed method results in substantial improvements through its detailed analysis on toy and real datasets. The quantitative and qualitative results demonstrate that the proposed method improves the solution for the problem of missing modes and improves training of GANs.",0
"This work presents a novel approach to capturing the entire data distribution more effectively using Generative Adversarial Networks (GANs). Traditional approaches to training GANs have limitations that can lead to incomplete coverage of the data space, resulting in modes being missed during training. We propose a new method that addresses these issues by incorporating additional constraints on the generator network. These constraints guide the generator towards regions of high density and low uncertainty in the data, allowing it to capture previously unobserved modes and increase overall completeness. Experimental results show that our method significantly improves mode coverage compared to state-of-the-art techniques. Our findings demonstrate the effectiveness of incorporating regularization into the generative process as a means of increasing coverage and ensuring more complete sampling from complex datasets.",1
"Surgical data science is a new research field that aims to observe all aspects of the patient treatment process in order to provide the right assistance at the right time. Due to the breakthrough successes of deep learning-based solutions for automatic image annotation, the availability of reference annotations for algorithm training is becoming a major bottleneck in the field. The purpose of this paper was to investigate the concept of self-supervised learning to address this issue.   Our approach is guided by the hypothesis that unlabeled video data can be used to learn a representation of the target domain that boosts the performance of state-of-the-art machine learning algorithms when used for pre-training. Core of the method is an auxiliary task based on raw endoscopic video data of the target domain that is used to initialize the convolutional neural network (CNN) for the target task. In this paper, we propose the re-colorization of medical images with a generative adversarial network (GAN)-based architecture as auxiliary task. A variant of the method involves a second pre-training step based on labeled data for the target task from a related domain. We validate both variants using medical instrument segmentation as target task.   The proposed approach can be used to radically reduce the manual annotation effort involved in training CNNs. Compared to the baseline approach of generating annotated data from scratch, our method decreases exploratively the number of labeled images by up to 75% without sacrificing performance. Our method also outperforms alternative methods for CNN pre-training, such as pre-training on publicly available non-medical or medical data using the target task (in this instance: segmentation).   As it makes efficient use of available (non-)public and (un-)labeled data, the approach has the potential to become a valuable tool for CNN (pre-)training.",0
"Endoscopy has become increasingly important as a medical diagnostic tool, allowing physicians to examine internal body organs without invasive procedures. However, analyzing large amounts of endoscopic video data can be time consuming and difficult due to the lack of labeled datasets available for training machine learning models. In this study, we propose using self-supervised learning techniques to utilize existing endoscopic video data sets to improve diagnosis accuracy while minimizing human input required to label data. Our approach leverages convolutional neural networks (CNNs) pretrained on ImageNet to extract features from raw image frames taken during endoscopic exams. These extracted features were then used to train a classifier for detecting specific abnormalities in the images, such as polyps or bleeding. Experimental results demonstrated that our method achieved state-of-the-art performance compared to traditional supervised learning approaches, highlighting the effectiveness of using self-supervision on endoscopic video data. With further advancements in technology, self-supervised learning could potentially transform how healthcare professionals analyze endoscopic videos by reducing analysis times and improving overall diagnostics quality.",1
"A social interaction is a social exchange between two or more individuals,where individuals modify and adjust their behaviors in response to their interaction partners. Our social interactions are one of most fundamental aspects of our lives and can profoundly affect our mood, both positively and negatively. With growing interest in virtual reality and avatar-mediated interactions,it is desirable to make these interactions natural and human like to promote positive effect in the interactions and applications such as intelligent tutoring systems, automated interview systems and e-learning. In this paper, we propose a method to generate facial behaviors for an agent. These behaviors include facial expressions and head pose and they are generated considering the users affective state. Our models learn semantically meaningful representations of the face and generate appropriate and temporally smooth facial behaviors in dyadic interactions.",0
"In this paper, we propose a novel approach for generating facial expressions in dyadic interactions using interactive generative adversarial networks (GANs). We focus on capturing the subtle differences in emotional expression that occur during social interactions, which can be difficult to model accurately with traditional methods. Our method involves training two GANs in parallel: one generates images of faces expressing different emotions, while the other evaluates their realism. By using feedback from both models, our system is able to generate high-quality images of faces exhibiting complex emotions such as surprise, disgust, and fear. To demonstrate the effectiveness of our approach, we conduct experiments comparing our results to those generated by state-of-the-art algorithms and human raters. Results show that our method outperforms existing approaches and produces more accurate representations of facial expressions in dyadic interactions. This work has important applications in fields such as virtual reality, gaming, and avatar creation, where realistic facial expressions are essential for creating immersive experiences.",1
"In recent years, Generative Adversarial Networks (GAN) have emerged as a powerful method for learning the mapping from noisy latent spaces to realistic data samples in high-dimensional space. So far, the development and application of GANs have been predominantly focused on spatial data such as images. In this project, we aim at modeling of spatio-temporal sensor data instead, i.e. dynamic data over time. The main goal is to encode temporal data into a global and low-dimensional latent vector that captures the dynamics of the spatio-temporal signal. To this end, we incorporate auto-regressive RNNs, Wasserstein GAN loss, spectral norm weight constraints and a semi-supervised learning scheme into InfoGAN, a method for retrieval of meaningful latents in adversarial learning. To demonstrate the modeling capability of our method, we encode full-body skeletal human motion from a large dataset representing 60 classes of daily activities, recorded in a multi-Kinect setup. Initial results indicate competitive classification performance of the learned latent representations, compared to direct CNN/RNN inference. In future work, we plan to apply this method on a related problem in the medical domain, i.e. on recovery of meaningful latents in gait analysis of patients with vertigo and balance disorders.",0
"In recent years there has been growing interest in developing algorithms for classifying spatio-temporal data such as videos and satellite imagery. Existing methods often rely on large amounts of labeled training data which can be difficult to obtain in practice. This work proposes a new approach that leverages unlabeled data using semi-supervised adversarial learning (SSL). By designing customized SSL architectures tailored for spatio-temporal data, we aim to improve classification performance under limited labeling budgets. We evaluate our method on several benchmark datasets, demonstrating competitive results compared to existing state-of-the-art techniques while requiring fewer labels during training. Our findings highlight the potential benefits of applying SSL strategies to tackle real world applications involving spatio-temporal data analysis.",1
"Most existing GANs architectures that generate images use transposed convolution or resize-convolution as their upsampling algorithm from lower to higher resolution feature maps in the generator. We argue that this kind of fixed operation is problematic for GANs to model objects that have very different visual appearances. We propose a novel adaptive convolution method that learns the upsampling algorithm based on the local context at each location to address this problem. We modify a baseline GANs architecture by replacing normal convolutions with adaptive convolutions in the generator. Experiments on CIFAR-10 dataset show that our modified models improve the baseline model by a large margin. Furthermore, our models achieve state-of-the-art performance on CIFAR-10 and STL-10 datasets in the unsupervised setting.",0
"Recent work has focused on developing models that can generate realistic looking images from text descriptions or class labels. These approaches typically rely on generative adversarial networks (GANs) that consist of two deep neural network components: a generator, which produces samples such as images, audio, or video, and a discriminator, which evaluates whether the generated samples are real or fake. However, training GANs can be challenging due to instability issues and difficulties in tuning hyperparameters. In this paper, we propose a new architecture called adaptive convolutional GANs (AC-GANs) that address these limitations by introducing an adaptive mechanism into the convolutional layers of both the generator and discriminator. Our approach automatically adjusts the resolution of feature maps during training based on the current state of the game, allowing for more stable convergence and improved performance compared to traditional GAN architectures. We evaluate our method on several benchmark datasets for image generation tasks and show that AC-GANs achieve superior results over existing methods. Overall, our work represents a significant advance towards creating powerful image synthesis systems that produce high quality, diverse, and coherent outputs.",1
"Volumetric lesion segmentation via medical imaging is a powerful means to precisely assess multiple time-point lesion/tumor changes. Because manual 3D segmentation is prohibitively time consuming and requires radiological experience, current practices rely on an imprecise surrogate called response evaluation criteria in solid tumors (RECIST). Despite their coarseness, RECIST marks are commonly found in current hospital picture and archiving systems (PACS), meaning they can provide a potentially powerful, yet extraordinarily challenging, source of weak supervision for full 3D segmentation. Toward this end, we introduce a convolutional neural network based weakly supervised self-paced segmentation (WSSS) method to 1) generate the initial lesion segmentation on the axial RECIST-slice; 2) learn the data distribution on RECIST-slices; 3) adapt to segment the whole volume slice by slice to finally obtain a volumetric segmentation. In addition, we explore how super-resolution images (2~5 times beyond the physical CT imaging), generated from a proposed stacked generative adversarial network, can aid the WSSS performance. We employ the DeepLesion dataset, a comprehensive CT-image lesion dataset of 32,735 PACS-bookmarked findings, which include lesions, tumors, and lymph nodes of varying sizes, categories, body regions and surrounding contexts. These are drawn from 10,594 studies of 4,459 patients. We also validate on a lymph-node dataset, where 3D ground truth masks are available for all images. For the DeepLesion dataset, we report mean Dice coefficients of 93% on RECIST-slices and 76% in 3D lesion volumes. We further validate using a subjective user study, where an experienced radiologist accepted our WSSS-generated lesion segmentation results with a high probability of 92.4%.",0
"This paper describes a method for accurately segmenting lesions on computed tomography (CT) scans using weak supervision. The authors propose a self-paced 3D mask generation approach based on the Response Evaluation Criteria In Solid Tumors (RECIST). This approach allows the model to automatically learn the most discriminative patterns from the limited annotated data while effectively incorporating complementary information from RECIST. Experiments demonstrate that the proposed method significantly outperforms state-of-the-art methods in both quantitative metrics and visual assessments.  Accurately segmenting lesions on computed tomography (CT) scans is crucial for diagnosing diseases like cancer and monitoring response to treatment. However, manual annotation of these images can be time-consuming and prone to errors. To address this challenge, researchers have developed machine learning algorithms for automated lesion segmentation under weakly supervised settings, where only sparse annotations are available.  In this work, we propose a novel self-paced 3D mask generation approach based on the Response Evaluation Criteria In Solid Tumors (RECIST), which are widely used criteria to evaluate treatment responses in patients with solid tumors. Our approach allows the model to automatically learn the most discriminative patterns from limited annotated data while effectively leveraging complementary information provided by RECIST guidelines.  We evaluated our method on two challenging datasets of lung nodules and liver metastases. Experimental results demonstrated significant improvements over several state-of-the-art weakly supervised approaches in terms of Dice similarity coefficient, Jaccard index, mean surface distance, sensitivity, specificity, area under t",1
"Age progression/regression is a challenging task due to the complicated and non-linear transformation in human aging process. Many researches have shown that both global and local facial features are essential for face representation, but previous GAN based methods mainly focused on the global feature in age synthesis. To utilize both global and local facial information, we propose a Global and Local Consistent Age Generative Adversarial Network (GLCA-GAN). In our generator, a global network learns the whole facial structure and simulates the aging trend of the whole face, while three crucial facial patches are progressed or regressed by three local networks aiming at imitating subtle changes of crucial facial subregions. To preserve most of the details in age-attribute-irrelevant areas, our generator learns the residual face. Moreover, we employ an identity preserving loss to better preserve the identity information, as well as age preserving loss to enhance the accuracy of age synthesis. A pixel loss is also adopted to preserve detailed facial information of the input face. Our proposed method is evaluated on three face aging datasets, i.e., CACD dataset, Morph dataset and FG-NET dataset. Experimental results show appealing performance of the proposed method by comparing with the state-of-the-art.",0
"Artificial intelligence has revolutionized the field of computer science by enabling machines to learn and adapt to new data more effectively than traditional rule-based algorithms. One promising approach within artificial intelligence is generative adversarial networks (GAN), which consist of two neural networks that compete against each other in a zero-sum game framework. GANs have achieved state-of-the-art performance on many tasks such as image generation, super resolution, and style transfer. However, training GANs remains challenging due to their instability, leading to slow convergence, mode collapse, and non-convergence issues. To address these problems, we propose global and local consistent age GANs. Our method encourages both global coherence and local diversity during training, resulting in improved stability and better generated images. We demonstrate the effectiveness of our approach through extensive experiments on various datasets and compare our results to existing methods in the literature. Overall, our proposed model significantly improves upon current GAN architectures while maintaining competitive performance, making it an ideal choice for generating realistic synthetic images across diverse domains. ----",1
"Generative models such as Variational Auto Encoders (VAEs) and Generative Adversarial Networks (GANs) are typically trained for a fixed prior distribution in the latent space, such as uniform or Gaussian. After a trained model is obtained, one can sample the Generator in various forms for exploration and understanding, such as interpolating between two samples, sampling in the vicinity of a sample or exploring differences between a pair of samples applied to a third sample. In this paper, we show that the latent space operations used in the literature so far induce a distribution mismatch between the resulting outputs and the prior distribution the model was trained on. To address this, we propose to use distribution matching transport maps to ensure that such latent space operations preserve the prior distribution, while minimally modifying the original operation. Our experimental results validate that the proposed operations give higher quality samples compared to the original operations.",0
"This sounds interesting - I can definitely write an abstract like that! Here you go: Optimization techniques based on optimal transport have been shown to be powerful tools in machine learning, particularly in the context of generative models such as deep neural networks. In this work, we explore how these techniques can be used to preserve statistical properties of distributions through mapping functions that operate on the latent space of a generator network. We demonstrate that by applying transport maps that push probability mass from one distribution to another while minimizing distance between corresponding points in feature space, we can ensure that important characteristics of the input data are preserved in the generated outputs. Our approach is evaluated on several datasets and benchmark tasks, showing state-of-the-art performance across a range of metrics. Overall, our results highlight the potential of using optimal transport maps for distribution preserving operations on the latent spaces of generative models.",1
"We present a deep learning approach for high resolution face completion with multiple controllable attributes (e.g., male and smiling) under arbitrary masks. Face completion entails understanding both structural meaningfulness and appearance consistency locally and globally to fill in ""holes"" whose content do not appear elsewhere in an input image. It is a challenging task with the difficulty level increasing significantly with respect to high resolution, the complexity of ""holes"" and the controllable attributes of filled-in fragments. Our system addresses the challenges by learning a fully end-to-end framework that trains generative adversarial networks (GANs) progressively from low resolution to high resolution with conditional vectors encoding controllable attributes.   We design novel network architectures to exploit information across multiple scales effectively and efficiently. We introduce new loss functions encouraging sharp completion. We show that our system can complete faces with large structural and appearance variations using a single feed-forward pass of computation with mean inference time of 0.007 seconds for images at 1024 x 1024 resolution. We also perform a pilot human study that shows our approach outperforms state-of-the-art face completion methods in terms of rank analysis. The code will be released upon publication.",0
"""High resolution face completion using multiple attributes has become increasingly important due to advances in technology that enable higher fidelity image generation. In this study, we present a method to achieve high resolution face completion while enabling control over several attributes such as age, gender, race, etc. Our approach uses fully end-to-end progressive generative adversarial networks which allow us to generate realistic images while minimizing computational resources required. We compare our results against existing methods on public datasets and demonstrate improved performance in terms of visual quality and attribute control.""",1
We propose an action recognition framework using Gen- erative Adversarial Networks. Our model involves train- ing a deep convolutional generative adversarial network (DCGAN) using a large video activity dataset without la- bel information. Then we use the trained discriminator from the GAN model as an unsupervised pre-training step and fine-tune the trained discriminator model on a labeled dataset to recognize human activities. We determine good network architectural and hyperparameter settings for us- ing the discriminator from DCGAN as a trained model to learn useful representations for action recognition. Our semi-supervised framework using only appearance infor- mation achieves superior or comparable performance to the current state-of-the-art semi-supervised action recog- nition methods on two challenging video activity datasets: UCF101 and HMDB51.,0
"This study proposes a semi-supervised approach for action recognition using videos. The method is called Discernet and uses generative adversarial networks (GAN). GAN is trained on both labeled and unlabeled video data, allowing it to effectively learn complex representations that capture both large-scale spatial structures as well as fine-grained details. To validate our approach, we perform extensive experiments on several benchmark datasets and compare results with state-of-the-art supervised models. Our results show that Discernet outperforms prior works under semi-supervised settings and achieves competitive performance compared to fully supervised methods. Our approach has promising applications in video surveillance systems where manually annotating vast amounts of training data can be time-consuming and impractical. We believe that Discernet provides an important step towards realizing the full potential of deep learning for video understanding tasks in resource constrained environments.",1
"Robust principal component analysis (RPCA) is a powerful method for learning low-rank feature representation of various visual data. However, for certain types as well as significant amount of error corruption, it fails to yield satisfactory results; a drawback that can be alleviated by exploiting domain-dependent prior knowledge or information. In this paper, we propose two models for the RPCA that take into account such side information, even in the presence of missing values. We apply this framework to the task of UV completion which is widely used in pose-invariant face recognition. Moreover, we construct a generative adversarial network (GAN) to extract side information as well as subspaces. These subspaces not only assist in the recovery but also speed up the process in case of large-scale data. We quantitatively and qualitatively evaluate the proposed approaches through both synthetic data and five real-world datasets to verify their effectiveness.",0
"Introduction (without this) Face completion refers to reconstructing missing parts of images through analysis of surrounding features or patterns that can provide contextual clues. This process is often done using machine learning algorithms that identify key facial features like eyes, mouth, nose, etc., but these methods still suffer from drawbacks such as sensitivity to changes in lighting conditions or image quality. In order to overcome these limitations, we propose the use of side information in the form of facial landmarks to enhance face completion accuracy. We present a robust principal component analysis (PCA)-based method to incorporate both appearance and geometric cues from facial landmarks into our framework. Our approach utilizes locality preserving projections (LPP) to preserve important discriminative information while reducing noise and dimensionality in the feature space. Experimental results demonstrate significant improvements over state-of-the-art face completion techniques on benchmark datasets under varying illumination conditions and occlusion scenarios. Conclusion (without this) In conclusion, our proposed PCA-based method leveraging side information in the form of facial landmarks provides improved performance in face completion tasks compared to traditional approaches. By integrating both appearance and geometry cues using LPP and regularization, we achieve more robustness against variations in image quality and illumination conditions. Further work could explore other types of side information or additional fusion strategies to further refine face completion accuracy. Overall, our study highlights the potential benefits of exploiting prior knowledge to improve computer vision tasks.",1
"Despite the growing prominence of generative adversarial networks (GANs), optimization in GANs is still a poorly understood topic. In this paper, we analyze the ""gradient descent"" form of GAN optimization i.e., the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters. We show that even though GAN optimization does not correspond to a convex-concave game (even for simple parameterizations), under proper conditions, equilibrium points of this optimization procedure are still \emph{locally asymptotically stable} for the traditional GAN formulation. On the other hand, we show that the recently proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent GAN updates, which \emph{is} able to guarantee local stability for both the WGAN and the traditional GAN, and also shows practical promise in speeding up convergence and addressing mode collapse.",0
"Abstract: This work investigates the local stability properties of gradient descent used to train Generative Adversarial Networks (GAN). We show that under mild assumptions on the architecture and loss functions, gradient descent can converge to a solution that lies within a certain neighborhood of the optimal solution. Moreover, we prove that any such point remains stationary throughout the iterative process, meaning that there exists no direction in which the objective function can be improved. These results contribute to our understanding of the behavior of gradient descent during training and have important implications for fine-tuning and hyperparameter selection. Our findings provide theoretical support for recent empirical observations suggesting that gradient descent has favorable convergence properties in practice. Overall, these insights could facilitate the development of more effective algorithms and models for generative tasks.",1
"Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the ""Fr\'echet Inception Distance"" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",0
"This paper presents a new training method for generative adversarial networks (GANs) that utilizes a two time-scale update rule, which leads to faster convergence and improved performance compared to traditional methods. By introducing a second timescale into the updating process, the network is able to learn more efficiently and better adapt to changing environments. Through extensive experiments on multiple datasets, we demonstrate that GANs trained using our proposed method converge to local Nash equilibria, resulting in high-quality generated samples that closely match real data distributions. Our findings have important implications for the field of deep learning, as they suggest that novel updates rules can significantly improve the stability and efficacy of training algorithms like GANs. Overall, this work represents a significant step forward in the development of powerful generative models that can produce accurate and realistic outputs.",1
"In recent years, there have been tremendous advancements in the field of machine learning. These advancements have been made through both academic as well as industrial research. Lately, a fair amount of research has been dedicated to the usage of generative models in the field of computer vision and image classification. These generative models have been popularized through a new framework called Generative Adversarial Networks. Moreover, many modified versions of this framework have been proposed in the last two years. We study the original model proposed by Goodfellow et al. as well as modifications over the original model and provide a comparative analysis of these models.",0
"Title: ""A Comprehensive Evaluation of Generative Adversarial Networks""  Generative adversarial networks (GANs) have emerged as one of the most powerful generative models in recent years due to their ability to generate high-quality samples that are difficult to distinguish from real data. However, despite their popularity, there remains a lack of understanding about how GANs work and how they can be effectively applied across different domains. This study aimed to fill this gap by conducting a comprehensive evaluation of GANs using various state-of-the-art techniques. Our results demonstrate that while GANs show promising performance, their behavior varies depending on factors such as dataset size, architecture design, and hyperparameter tuning. Moreover, we found that GANs tend to suffer from stability issues during training and mode collapse problems when generating new samples. To address these limitations, we propose several techniques based on regularization, normalization, and prior knowledge injection to improve the overall performance of GANs. Overall, our findings provide valuable insights into the strengths and weaknesses of GANs, highlight potential future research directions, and offer practical guidelines for applying GANs in real-world scenarios.",1
"Autonomous underwater vehicles (AUVs) rely on a variety of sensors - acoustic, inertial and visual - for intelligent decision making. Due to its non-intrusive, passive nature, and high information content, vision is an attractive sensing modality, particularly at shallower depths. However, factors such as light refraction and absorption, suspended particles in the water, and color distortion affect the quality of visual data, resulting in noisy and distorted images. AUVs that rely on visual sensing thus face difficult challenges, and consequently exhibit poor performance on vision-driven tasks. This paper proposes a method to improve the quality of visual underwater scenes using Generative Adversarial Networks (GANs), with the goal of improving input to vision-driven behaviors further down the autonomy pipeline. Furthermore, we show how recently proposed methods are able to generate a dataset for the purpose of such underwater image restoration. For any visually-guided underwater robots, this improvement can result in increased safety and reliability through robust visual perception. To that effect, we present quantitative and qualitative data which demonstrates that images corrected through the proposed approach generate more visually appealing images, and also provide increased accuracy for a diver tracking algorithm.",0
"In recent years there has been growing interest in the use of artificial intelligence (AI) techniques such as generative adversarial networks (GANs) to improve underwater imaging technologies. This paper presents a new method for enhancing underwater imagery using GANs that outperforms traditional image enhancement methods by significant margins. Our approach involves training two deep neural network architectures, one generator and one discriminator, on a large dataset of high quality underwater images and their corresponding low quality counterparts. We propose a novel architecture based on U-Net which ensures sharp edges and fine details are retained while improving color accuracy and contrast. Extensive experiments demonstrate the effectiveness of our proposed method over several state-of-the-art approaches, including histogram equalization, contrast stretching and retinex-based methods. Overall, these results suggest that GANs have the potential to greatly benefit underwater imaging applications.",1
"Access to electronic health record (EHR) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of EHR data. Sharing synthetic EHR data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic patient records. Based on input real patient records, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic patient records that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and a medical expert review. We also empirically observe a limited privacy risk in both identity and attribute disclosure using medGAN.",0
"In this paper we present an approach based on generative adversarial networks (GAN) for generating synthetic patient records that simulate multi-label discrete data distributions. Our method tackles several challenges faced by existing approaches in healthcare, such as the limited availability of labeled training data and incomplete medical histories due to missing or unreported diagnoses. We propose a novel architecture combining conditional GANs and variational autoencoders (VAE) to generate coherent and realistic patient trajectories across multiple diseases. To evaluate our model's effectiveness, we conduct experiments on two different datasets: MIMIC-III and Physionet Challenge 2018 dataset. Our results show significant improvements over state-of-the-art methods and demonstrate the ability of our proposed system to provide meaningful predictions and insights into patient care management.",1
"Semantic layouts based Image synthesizing, which has benefited from the success of Generative Adversarial Network (GAN), has drawn much attention in these days. How to enhance the synthesis image equality while keeping the stochasticity of the GAN is still a challenge. We propose a novel denoising framework to handle this problem. The overlapped objects generation is another challenging task when synthesizing images from a semantic layout to a realistic RGB photo. To overcome this deficiency, we include a one-hot semantic label map to force the generator paying more attention on the overlapped objects generation. Furthermore, we improve the loss function of the discriminator by considering perturb loss and cascade layer loss to guide the generation process. We applied our methods on the Cityscapes, Facades and NYU datasets and demonstrate the image generation ability of our model.",0
"Our paper presents a novel approach to image synthesis using instance maps and denoising generative adversarial networks (DGANs). We propose that by utilizing instance maps as input to our DGAN model, we can generate more detailed and accurate images than traditional approaches. In order to accomplish this task, we first define an instance map as a binary vector representing the presence or absence of object instances within an image. Next, we train two separate models on these instance maps â€“ a generator network and a discriminator network. The generator creates new images from scratch by predicting their pixel values, while the discriminator attempts to distinguish real images from generated ones. During training, the generator learns to create increasingly convincing fake images until they fool the discriminator. Once trained, both networks are used together to create high quality images given only an instance map as input. Through experiments, we demonstrate that our method outperforms previous state-of-the-art techniques in terms of visual fidelity and perceptual similarity metrics such as FID and LPIPS. Additionally, we evaluate the performance of our method under variable conditions including different instance mapping strategies, input resolutions, and random noise injection during testing. Overall, our results showcase the potential utility of instance maps as an effective tool for controlling complex scenes with multiple objects across diverse domains. This technique can have applications in areas such as computer vision, augmented reality, and virtual world creation, among others.",1
"E-commerce companies such as Amazon, Alibaba and Flipkart process billions of orders every year. However, these orders represent only a small fraction of all plausible orders. Exploring the space of all plausible orders could help us better understand the relationships between the various entities in an e-commerce ecosystem, namely the customers and the products they purchase. In this paper, we propose a Generative Adversarial Network (GAN) for orders made in e-commerce websites. Once trained, the generator in the GAN could generate any number of plausible orders. Our contributions include: (a) creating a dense and low-dimensional representation of e-commerce orders, (b) train an ecommerceGAN (ecGAN) with real orders to show the feasibility of the proposed paradigm, and (c) train an ecommerce-conditional-GAN (ec^2GAN) to generate the plausible orders involving a particular product. We propose several qualitative methods to evaluate ecGAN and demonstrate its effectiveness. The ec^2GAN is used for various kinds of characterization of possible orders involving a product that has just been introduced into the e-commerce system. The proposed approach ec^2GAN performs significantly better than the baseline in most of the scenarios.",0
"In this paper we present a generative adversarial network (GAN) specifically designed for use in e-commerce applications. GANs have been previously used for image generation tasks such as creating images from text descriptions or generating realistic versions of existing images. We propose using the power of GANs to generate more realistic product images that could then be utilized by online retailers to improve customer engagement and conversions on their websites. Our approach involves training two neural networks simultaneously: one generator which produces synthetic product images given a set of attributes, and another discriminator which evaluates the authenticity of these generated images against a dataset of actual product photos. By optimizing towards fooling the discriminator while maintaining photo-realism constraints through regularization techniques, our method achieves state-of-the art results in terms of visual quality and content preservation relative to other methods explored within this domain. Finally, we experiment with different architectures and hyperparameters, demonstrating the robustness of our approach under varying conditions, concluding with some promising experimental data showing performance gains over baselines in terms of key user interaction metrics commonly adopted by leading e-commerce platforms worldwide.",1
"Discriminating lung nodules as malignant or benign is still an underlying challenge. To address this challenge, radiologists need computer aided diagnosis (CAD) systems which can assist in learning discriminative imaging features corresponding to malignant and benign nodules. However, learning highly discriminative imaging features is an open problem. In this paper, our aim is to learn the most discriminative features pertaining to lung nodules by using an adversarial learning methodology. Specifically, we propose to use unsupervised learning with Deep Convolutional-Generative Adversarial Networks (DC-GANs) to generate lung nodule samples realistically. We hypothesize that imaging features of lung nodules will be discriminative if it is hard to differentiate them (fake) from real (true) nodules. To test this hypothesis, we present Visual Turing tests to two radiologists in order to evaluate the quality of the generated (fake) nodules. Extensive comparisons are performed in discerning real, generated, benign, and malignant nodules. This experimental set up allows us to validate the overall quality of the generated nodules, which can then be used to (1) improve diagnostic decisions by mining highly discriminative imaging features, (2) train radiologists for educational purposes, and (3) generate realistic samples to train deep networks with big data.",0
"This article presents a study on the potential of using generative adversarial networks (GANs) for fooling radiologists in lung cancer diagnosis through a visual Turing test. GANs have shown promising results in generating realistic images that can potentially deceive human observers. In this work, we investigate whether these generated images could pass as real scans taken from patients with lung disease. Our experiment involves creating two sets of synthetic CT scan images; one set represents healthy lungs, while the other simulates pathologies such as nodules or masses indicative of malignancy. We then conduct a blinded observer study where board certified radiologists were asked to distinguish the real from fake cases. Results show that our trained GAN model was able to generate images good enough to trick experienced radiologists at an alarmingly high rate. These findings demonstrate the need for caution in adopting deep learning algorithms in medical imaging diagnostics without thorough validation, verification, and testing. Furthermore, it highlights the challenges and opportunities posed by artificial intelligence systems in achieving comparable performance to humans, if not surpassing them. Overall, this research sheds light on the limitations of current radiological practices and paves the way towards improved technological innovations in clinical decision support systems.",1
"In this study we propose a new method to simulate hyper-realistic urban patterns using Generative Adversarial Networks trained with a global urban land-use inventory. We generated a synthetic urban ""universe"" that qualitatively reproduces the complex spatial organization observed in global urban patterns, while being able to quantitatively recover certain key high-level urban spatial metrics.",0
"Artificial intelligence has proven to be an effective tool for analyzing complex datasets such as those related to urban growth patterns. Generative Adversarial Networks (GAN) have been used successfully in several applications such as image generation, natural language processing, and even controlling robots. In our research, we explore how GAN can be utilized in modeling urbanization patterns at different scales. By using two neural network architectures that are trained against each other, we generate realistic urban landscapes and their corresponding population densities. Our results show promising accuracy compared to traditional methods of analysis. We hope our work inspires further exploration into artificial intelligence solutions for city planning and management tasks.",1
"Currently there is strong interest in data-driven approaches to medical image classification. However, medical imaging data is scarce, expensive, and fraught with legal concerns regarding patient privacy. Typical consent forms only allow for patient data to be used in medical journals or education, meaning the majority of medical data is inaccessible for general public research. We propose a novel, two-stage pipeline for generating synthetic medical images from a pair of generative adversarial networks, tested in practice on retinal fundi images. We develop a hierarchical generation process to divide the complex image generation task into two parts: geometry and photorealism. We hope researchers will use our pipeline to bring private medical data into the public domain, sparking growth in imaging tasks that have previously relied on the hand-tuning of models. We have begun this initiative through the development of SynthMed, an online repository for synthetic medical images.",0
"This paper proposes a new method for generating synthetic medical images using dual generative adversarial networks (GAN). GANs have been shown to be effective at generating realistic image data, but there are limitations to their use in medical imaging due to difficulties in training and lack of control over generated outputs. Our proposed method addresses these issues by using two separate GAN models, one for generating high resolution anatomical structures and another for adding pathological lesions. We demonstrate that our approach can generate realistic images of different body regions and pathologies with controllable features such as lesion size, location, and severity. Additionally, we compare our results to other state-of-the-art methods and show that our approach outperforms them in terms of visual fidelity and ability to mimic real patient data. Our method has potential applications in medical research, education, and computer-aided diagnosis.",1
"Ultrasound imaging makes use of backscattering of waves during their interaction with scatterers present in biological tissues. Simulation of synthetic ultrasound images is a challenging problem on account of inability to accurately model various factors of which some include intra-/inter scanline interference, transducer to surface coupling, artifacts on transducer elements, inhomogeneous shadowing and nonlinear attenuation. Current approaches typically solve wave space equations making them computationally expensive and slow to operate. We propose a generative adversarial network (GAN) inspired approach for fast simulation of patho-realistic ultrasound images. We apply the framework to intravascular ultrasound (IVUS) simulation. A stage 0 simulation performed using pseudo B-mode ultrasound image simulator yields speckle mapping of a digitally defined phantom. The stage I GAN subsequently refines them to preserve tissue specific speckle intensities. The stage II GAN further refines them to generate high resolution images with patho-realistic speckle profiles. We evaluate patho-realism of simulated images with a visual Turing test indicating an equivocal confusion in discriminating simulated from real. We also quantify the shift in tissue specific intensity distributions of the real and simulated images to prove their similarity.",0
"Ultrasound imaging has become an essential diagnostic tool due to its low cost, non-invasive nature, and portability. However, obtaining high quality ultrasound images that can accurately depict pathological conditions remains challenging due to factors such as operator skill level, patient movement during examination, limited field of view, and low resolution. This study proposes a deep generative network architecture based on adversarial learning to generate simulated ultrasound images mimicking various pathologies. Our method uses a conditional generative adversarial network (cGAN) composed of two subnetworks - one generates the image and the other discriminates real from generated images. The generator takes random noise samples as input and outputs the corresponding simulated B-mode RF ultrasound image guided by its adversary. We validate our approach through extensive experiments including qualitative comparisons against real US images, quantitative evaluations of similarity metrics, ablation studies to measure impacts of different components, and case studies on three diseases (carotid plaque, liver tumors, thyroid nodules). Results show our algorithm effectively produces diverse, visually consistent images which match the appearances of various pathological cases under distinct acquisition settings. Furthermore, we achieve state-of-the-art performance in terms of visual fidelity compared to other methods. Overall, our proposed framework offers a promising solution towards enhancing ultrasound training and assessment, and facilitating virtual medical research at lower costs.",1
"In this paper, we present a data augmentation method that generates synthetic medical images using Generative Adversarial Networks (GANs). We propose a training scheme that first uses classical data augmentation to enlarge the training set and then further enlarges the data size and its diversity by applying GAN techniques for synthetic data augmentation. Our method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). The classification performance using only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. By adding the synthetic data augmentation the results significantly increased to 85.7% sensitivity and 92.4% specificity.",0
"In this paper, we propose a novel approach for generating synthetic medical imaging data using Generative Adversarial Networks (GAN) to improve lesion classification performance on liver CT scans. We demonstrate that our method can generate realistic images with a high level of detail that closely resemble authentic patient datasets, significantly improving the accuracy of current state-of-the-art classifiers. To validate our approach, we perform extensive experiments evaluating both quantitative and qualitative measures of image quality as well as classification accuracy. Results show that by integrating synthetic images into training sets, overall lesion detection rates increase upwards of 17% while reducing false positive detections. Our work represents a promising step forward towards improved liver disease diagnosis through computer vision techniques. Overall, this research holds significant potential impact for advancing healthcare technology and enhancing diagnostic capabilities within radiology practices worldwide.",1
"This paper proposes the idea of using a generative adversarial network (GAN) to assist a novice user in designing real-world shapes with a simple interface. The user edits a voxel grid with a painting interface (like Minecraft). Yet, at any time, he/she can execute a SNAP command, which projects the current voxel grid onto a latent shape manifold with a learned projection operator and then generates a similar, but more realistic, shape using a learned generator network. Then the user can edit the resulting shape and snap again until he/she is satisfied with the result. The main advantage of this approach is that the projection and generation operators assist novice users to create 3D models characteristic of a background distribution of object shapes, but without having to specify all the details. The core new research idea is to use a GAN to support this application. 3D GANs have previously been used for shape generation, interpolation, and completion, but never for interactive modeling. The new challenge for this application is to learn a projection operator that takes an arbitrary 3D voxel model and produces a latent vector on the shape manifold from which a similar and realistic shape can be generated. We develop algorithms for this and other steps of the SNAP processing pipeline and integrate them into a simple modeling tool. Experiments with these algorithms and tool suggest that GANs provide a promising approach to computer-assisted interactive modeling.",0
"In recent years, generative adversarial networks (GANs) have emerged as a powerful tool for generating realistic 3D models from scratch. However, these models are typically static, lacking interactivity and versatility. To address this limitation, we propose a novel method that combines GANs with interactive modeling techniques to create dynamic 3D models that can be manipulated by users in real time. Our approach integrates a feedback loop into the traditional GAN architecture, allowing the generator network to update the generated model based on user inputs. This creates an iterative process where the user provides guidance and feedback, resulting in improved fidelity and accuracy over time. We demonstrate the effectiveness of our method through extensive experiments, comparing it against state-of-the-art methods for interactive 3D modeling. Our results show that our approach achieves higher levels of detail, faster convergence rates, and greater user satisfaction compared to other approaches. Overall, our work represents an important step towards enabling truly interactive, realtime 3D generation using deep learning technologies such as GANs.",1
"An important task in image processing and neuroimaging is to extract quantitative information from the acquired images in order to make observations about the presence of disease or markers of development in populations. Having a lowdimensional manifold of an image allows for easier statistical comparisons between groups and the synthesis of group representatives. Previous studies have sought to identify the best mapping of brain MRI to a low-dimensional manifold, but have been limited by assumptions of explicit similarity measures. In this work, we use deep learning techniques to investigate implicit manifolds of normal brains and generate new, high-quality images. We explore implicit manifolds by addressing the problems of image synthesis and image denoising as important tools in manifold learning. First, we propose the unsupervised synthesis of T1-weighted brain MRI using a Generative Adversarial Network (GAN) by learning from 528 examples of 2D axial slices of brain MRI. Synthesized images were first shown to be unique by performing a crosscorrelation with the training set. Real and synthesized images were then assessed in a blinded manner by two imaging experts providing an image quality score of 1-5. The quality score of the synthetic image showed substantial overlap with that of the real images. Moreover, we use an autoencoder with skip connections for image denoising, showing that the proposed method results in higher PSNR than FSL SUSAN after denoising. This work shows the power of artificial networks to synthesize realistic imaging data, which can be used to improve image processing techniques and provide a quantitative framework to structural changes in the brain.",0
"This paper presents a novel approach for learning implicit brain manifolds using deep learning techniques. In recent years, magnetic resonance imaging (MRI) has become an important tool for studying the human brain due to its ability to non-invasively capture detailed images of internal structures and functions. One challenge that arises from working with these complex data sets is how to effectively analyze and interpret the large amounts of information they contain. Traditional methods such as manual feature extraction and domain expert knowledge can be time-consuming, laborious, and prone to errors. To address this issue, we propose a new methodology that leverages convolutional neural networks (CNNs) and unsupervised feature learning to automatically extract meaningful features from raw MRIs. We demonstrate our approach on a set of real-world clinical datasets and show promising results in terms of both accuracy and interpretability. Our findings suggest that deep learning models have great potential for revolutionizing the field of neuroscience by enabling more efficient and accurate analysis of complex neuroimaging data.",1
"Person re-identification (\textit{re-id}) refers to matching pedestrians across disjoint yet non-overlapping camera views. The most effective way to match these pedestrians undertaking significant visual variations is to seek reliably invariant features that can describe the person of interest faithfully. Most of existing methods are presented in a supervised manner to produce discriminative features by relying on labeled paired images in correspondence. However, annotating pair-wise images is prohibitively expensive in labors, and thus not practical in large-scale networked cameras. Moreover, seeking comparable representations across camera views demands a flexible model to address the complex distributions of images. In this work, we study the co-occurrence statistic patterns between pairs of images, and propose to crossing Generative Adversarial Network (Cross-GAN) for learning a joint distribution for cross-image representations in a unsupervised manner. Given a pair of person images, the proposed model consists of the variational auto-encoder to encode the pair into respective latent variables, a proposed cross-view alignment to reduce the view disparity, and an adversarial layer to seek the joint distribution of latent representations. The learned latent representations are well-aligned to reflect the co-occurrence patterns of paired images. We empirically evaluate the proposed model against challenging datasets, and our results show the importance of joint invariant features in improving matching rates of person re-id with comparison to semi/unsupervised state-of-the-arts.",0
"This abstract describes a research study that utilizes generative adversarial networks (GANs) for cross-view person re-identification tasks. GANs have emerged as powerful models for generating synthetic data that can improve performance on a variety of computer vision tasks. In this work, the authors propose using two crossing generative adversarial networks (CrossGAN), one for image generation from different views, and another for identity classification. They demonstrate through experiments on several datasets that their proposed method achieves state-of-the-art results for cross-view person re-identification. Overall, this research shows promise for improving person identification across multiple camera views by leveraging advanced deep learning techniques like GANs.",1
"Adversarial learning of probabilistic models has recently emerged as a promising alternative to maximum likelihood. Implicit models such as generative adversarial networks (GAN) often generate better samples compared to explicit models trained by maximum likelihood. Yet, GANs sidestep the characterization of an explicit density which makes quantitative evaluations challenging. To bridge this gap, we propose Flow-GANs, a generative adversarial network for which we can perform exact likelihood evaluation, thus supporting both adversarial and maximum likelihood training. When trained adversarially, Flow-GANs generate high-quality samples but attain extremely poor log-likelihood scores, inferior even to a mixture model memorizing the training data; the opposite is true when trained by maximum likelihood. Results on MNIST and CIFAR-10 demonstrate that hybrid training can attain high held-out likelihoods while retaining visual fidelity in the generated samples.",0
"Recent work has focused on using generative models like GANs (Generative Adversarial Networks) to create high quality images from textual prompts. However, current state-of-the-art methods can still suffer from instability during training as well as mode collapse - generating only one type of image even when given different input prompts. We introduce a new method called 'Flow-GAN,' which combines maximum likelihood estimation (MLE) with adversarial learning. This allows us to better capture underlying distributions of data while simultaneously improving stability during training and reducing mode collapse. Our approach uses two discriminators and a generator that are trained together via MLE; we then add adversarial loss functions that explicitly encourage diversity in generated outputs. Experiments show that our model significantly outperforms previous approaches both quantitatively and qualitatively, producing more diverse and accurate outputs across multiple datasets. Overall, Flow-GAN provides a simple yet effective framework for building powerful generative models based on deep neural networks.",1
"Automatic synthesis of faces from visual attributes is an important problem in computer vision and has wide applications in law enforcement and entertainment. With the advent of deep generative convolutional neural networks (CNNs), attempts have been made to synthesize face images from attributes and text descriptions. In this paper, we take a different approach, where we formulate the original problem as a stage-wise learning problem. We first synthesize the facial sketch corresponding to the visual attributes and then we reconstruct the face image based on the synthesized sketch. The proposed Attribute2Sketch2Face framework, which is based on a combination of deep Conditional Variational Autoencoder (CVAE) and Generative Adversarial Networks (GANs), consists of three stages: (1) Synthesis of facial sketch from attributes using a CVAE architecture, (2) Enhancement of coarse sketches to produce sharper sketches using a GAN-based framework, and (3) Synthesis of face from sketch using another GAN-based network. Extensive experiments and comparison with recent methods are performed to verify the effectiveness of the proposed attribute-based three stage face synthesis method.",0
"In recent years, there has been significant interest in developing algorithms that can generate realistic images of faces based on input textual descriptions. These models have applications ranging from entertainment to computer vision, such as creating virtual actors or automatically generating facial expressions.  This work presents a novel approach to face synthesis using conditional variational autoencoders (VAEs) and generative adversarial networks (GANs). We begin by training our model on a large dataset of facial features labeled according to their attributes, such as hair color, eye shape, nose width, etc. Then, we utilize these attributes along with random noise inputs to generate new face samples through latent space manipulation.  Our contributions include: 1) introducing a method of using visual attribute labels directly to guide the generation process; 2) extending previous methods which required paired image/caption data by operating solely off attribute labels alone; 3) demonstrating improved results over alternative techniques for face synthesis. Our system produces high quality results even at low resolutions and outperforms both non-conditional baselines and other state-of-the-art methods.  Overall, our approach opens up exciting possibilities for further research into intelligent systems capable of producing plausible faces with desired properties. With the advancement of artificial intelligence and machine learning techniques, such tools could prove increasingly valuable in fields ranging from art to psychology, where understanding human facial diversity remains critical.",1
"Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.",0
"Incorporating unpaired data from multiple domains into supervised learning applications has been shown to improve their performance on new tasks and reduce overfitting. Unsupervised domain adaptation methods focus on finding shared representations across datasets by minimizing domain discrepancy, whereas adversarial domain adaptation maximizes alignment at the feature level. Recent work has used cycle consistency regularization (CCR) for image generation, which ensures that images can be mapped back to their original domain through a generative model. This paper proposes a novel framework called CyCADA (Cycl e-Consistent Ad versational D omain Adaptation ) that integrates CCR with adversarial training. Our method enforces cycle consistency both within each domain and between different domains. We demonstrate experimentally that our algorithm outperforms state-of-the-art approaches in multiple scenarios, including object recognition and depth estimation tasks using real datasets. Moreover, we provide insights into how different components contribute to the overall performance improvement. Our approach provides a powerful toolkit for developing multi-source machine learning systems capable of adapting to previously unknown data distributions.",1
"In face-related applications with a public available dataset, synthesizing non-linear facial variations (e.g., facial expression, head-pose, illumination, etc.) through a generative model is helpful in addressing the lack of training data. In reality, however, there is insufficient data to even train the generative model for face synthesis. In this paper, we propose Differential Generative Adversarial Networks (D-GAN) that can perform photo-realistic face synthesis even when training data is small. Two discriminators are devised to ensure the generator to approximate a face manifold, which can express face changes as it wants. Experimental results demonstrate that the proposed method is robust to the amount of training data and synthesized images are useful to improve the performance of a face expression classifier.",0
"Here is a draft Abstract for your paper:  Differential generative adversarial networks (GANs) have proven to be effective at generating realistic synthetic data that capture complex relationships within large datasets. However, these models often require vast amounts of training data to achieve high levels of performance. In this work, we investigate whether limited training sets can still produce useful results by using differential GANs to generate non-linear facial variations. Our experiments demonstrate that even small numbers of training examples can lead to significant improvements over traditional methods such as linear regression and principal component analysis. Furthermore, our approach consistently outperforms other state-of-the art techniques that rely on larger training sets. This suggests that differential GANs may offer a powerful alternative for tasks where labeled data is scarce, particularly those involving high-dimensional and structured outputs like faces.",1
"Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.",0
"Artificial intelligence (AI) has revolutionized how we live our lives today. One type of machine learning model that has gained increasing popularity over recent years is generative adversarial networks (GANs). Despite their promise, training stable GANs remains a challenging task. In this paper, we present several modifications to improve the stability of training Wasserstein GANs in particular. Firstly, we modify the loss function to penalize large Lipschitz constants, which leads to slower growth of the Lipschitz constant during training. Secondly, we introduce a new regularization term based on the spectral norm of the discriminatorâ€™s gradients, which reduces mode collapse and improves generated samplesâ€™ quality without affecting convergence speed. Finally, we propose early stopping based on the minimum value of the generator loss, which helps identify optimal hyperparameters efficiently while reducing validation set size requirements. We experimentally validate these modifications across different datasets showing improved performance compared to standard Wasserstein GANs. Our work provides insights into improving current GAN models and shows promising applications for future research in image generation tasks.",1
"This paper addresses the problem of cross-domain change detection from a novel perspective of image-to-image translation. In general, change detection aims to identify interesting changes between a given query image and a reference image of the same scene taken at a different time. This problem becomes a challenging one when query and reference images involve different domains (e.g., time of the day, weather, and season) due to variations in object appearance and a limited amount of training examples. In this study, we address the above issue by leveraging a generative adversarial network (GAN). Our key concept is to use a limited amount of training data to train a GAN-based image translator that maps a reference image to a virtual image that cannot be discriminated from query domain images. This enables us to treat the cross-domain change detection task as an in-domain image comparison. This allows us to leverage the large body of literature on in-domain generic change detectors. In addition, we also consider the use of visual place recognition as a method for mining more appropriate reference images over the space of virtual images. Experiments validate efficacy of the proposed approach.",0
"This paper presents a novel approach to change detection using generative adversarial networks (GANs). GANs have been widely used in image generation tasks but their use in cross-domain change detection has been limited. In this work, we propose a method that uses two sub-networks within a GAN framework to learn the statistical differences between images from different domains while preserving relevant features. Our method can effectively detect changes such as new objects, missing objects, and structural alterations in both synthetic and real-world datasets. We evaluate our method on three datasets, including one real-world dataset, and demonstrate its effectiveness compared to traditional methods and other state-of-the-art approaches. Overall, this work shows great potential for future applications of GANs in computer vision.",1
"Recently image inpainting has witnessed rapid progress due to generative adversarial networks (GAN) that are able to synthesize realistic contents. However, most existing GAN-based methods for semantic inpainting apply an auto-encoder architecture with a fully connected layer, which cannot accurately maintain spatial information. In addition, the discriminator in existing GANs struggle to understand high-level semantics within the image context and yield semantically consistent content. Existing evaluation criteria are biased towards blurry results and cannot well characterize edge preservation and visual authenticity in the inpainting results. In this paper, we propose an improved generative adversarial network to overcome the aforementioned limitations. Our proposed GAN-based framework consists of a fully convolutional design for the generator which helps to better preserve spatial structures and a joint loss function with a revised perceptual loss to capture high-level semantics in the context. Furthermore, we also introduce two novel measures to better assess the quality of image inpainting results. Experimental results demonstrate that our method outperforms the state of the art under a wide range of criteria.",0
"Abstract: This research addresses the problem of context-aware semantic image inpainting, which involves restoring missing content in an image while preserving important details and maintaining the overall context and semantics of the scene. We propose a novel framework that leverages deep learning techniques to achieve high quality results in real-time. Our approach combines semantic segmentation with generative adversarial networks (GANs) to generate natural and coherent inpaintings. The framework is trained on large amounts of data to capture complex relationships among different objects and scenes. Experiments show that our method outperforms state-of-the-art methods by achieving higher visual fidelity and better alignment with human judgments. Our approach has applications in areas such as photo editing, medical imaging, and autonomous driving.",1
"A lack of generalizability is one key limitation of deep learning based segmentation. Typically, one manually labels new training images when segmenting organs in different imaging modalities or segmenting abnormal organs from distinct disease cohorts. The manual efforts can be alleviated if one is able to reuse manual labels from one modality (e.g., MRI) to train a segmentation network for a new modality (e.g., CT). Previously, two stage methods have been proposed to use cycle generative adversarial networks (CycleGAN) to synthesize training images for a target modality. Then, these efforts trained a segmentation network independently using synthetic images. However, these two independent stages did not use the complementary information between synthesis and segmentation. Herein, we proposed a novel end-to-end synthesis and segmentation network (EssNet) to achieve the unpaired MRI to CT image synthesis and CT splenomegaly segmentation simultaneously without using manual labels on CT. The end-to-end EssNet achieved significantly higher median Dice similarity coefficient (0.9188) than the two stages strategy (0.8801), and even higher than canonical multi-atlas segmentation (0.9125) and ResNet method (0.9107), which used the CT manual labels.",0
"In recent years, advances in computer vision have led to significant improvements in image segmentation tasks using deep learning techniques. However, many approaches require access to ground truth annotations from the target modality, which can be expensive and time consuming to obtain. To address this limitation, we present a new method called Adversarial Synthesis Learning (ASL) that enables image segmentation without requiring modalities such as CT or MRI scans. Our approach combines adversarial training and generative synthesis to learn a mapping function that predicts the desired output given only RGB input images. We show through extensive experiments on multiple datasets that ASL achieves state-of-the-art performance compared to traditional methods relying on target modality ground truth. Our results demonstrate the effectiveness and potential clinical impact of our proposed framework for improving medical imaging analysis by reducing reliance on costly and time-consuming label annotation processes.",1
"Single image super resolution (SISR) is to reconstruct a high resolution image from a single low resolution image. The SISR task has been a very attractive research topic over the last two decades. In recent years, convolutional neural network (CNN) based models have achieved great performance on SISR task. Despite the breakthroughs achieved by using CNN models, there are still some problems remaining unsolved, such as how to recover high frequency details of high resolution images. Previous CNN based models always use a pixel wise loss, such as l2 loss. Although the high resolution images constructed by these models have high peak signal-to-noise ratio (PSNR), they often tend to be blurry and lack high-frequency details, especially at a large scaling factor. In this paper, we build a super resolution perceptual generative adversarial network (SRPGAN) framework for SISR tasks. In the framework, we propose a robust perceptual loss based on the discriminator of the built SRPGAN model. We use the Charbonnier loss function to build the content loss and combine it with the proposed perceptual loss and the adversarial loss. Compared with other state-of-the-art methods, our method has demonstrated great ability to construct images with sharp edges and rich details. We also evaluate our method on different benchmarks and compare it with previous CNN based methods. The results show that our method can achieve much higher structural similarity index (SSIM) scores on most of the benchmarks than the previous state-of-art methods.",0
"Here is one possible abstract for the paper ""SRPGAN: Perceptual Generative Adversarial Network for Single Image Super Resolution"":  Single image super resolution (SISR) involves reconstructing a high-resolution version of an input image from its low-resolution counterpart using deep learning methods. In recent years, perceptual generative adversarial networks (GANs) have emerged as a powerful tool for SISR due to their ability to generate realistic images that capture both the detail and texture present in the original images. However, current GAN-based approaches still suffer from limitations such as generating blurry outputs, losing important features, and struggling with large upscaling factors. To address these issues, we propose the SRPGAN model, which incorporates a novel feature loss function and adversarial training scheme. Our approach uses progressive growing techniques to gradually learn the spatial structure of the HR image during the training process while preserving fine details. Experiments on several benchmark datasets demonstrate that our method outperforms state-of-the-art algorithms in terms of visual fidelity and quantitative metrics. Additionally, our method is capable of handling large scale factors up to Ã—8 without sacrificing performance. Overall, SRPGAN represents a significant improvement over previous GAN-based models for SISR tasks.  This paper presents a new generative adversarial network architecture for single image super resolution (SISR). SISR is the task of constructing a high quality, high resolution version of an input image given only access to a low resolution copy of the same photograph. While many different methods have been developed for performing this task, including convolutional neural nets trained in either supervised or unsupervised settings, perceptually motivated generative adversarial networks have recently proven very successful in practice at producing visually plausible output that captures both the detail and texture found in ground truth photos. Nevertheless, generat",1
"Generating novel pairs of image and text is a problem that combines computer vision and natural language processing. In this paper, we present strategies for generating novel image and caption pairs based on existing captioning datasets. The model takes advantage of recent advances in generative adversarial networks and sequence-to-sequence modeling. We make generalizations to generate paired samples from multiple domains. Furthermore, we study cycles -- generating from image to text then back to image and vise versa, as well as its connection with autoencoders.",0
"Incorporate keywords such as generative models, image generation, text generation, semantic alignment, deep learning into the text but do so naturally. Provide an introduction followed by brief overview of related work, statement of contributions, methods used (mention data collection methodology), results obtained, conclusion and potential implications or future directions. Acknowledgements can be included at the end if desired. Synthesizing novel pairs of images and corresponding descriptions has emerged as a critical problem with applications ranging from content creation and computer vision tasks to language understanding and natural language processing. In recent years, deep learning techniques have been successfully applied to solve problems in these domains, including generative modeling of both images and texts. However, existing approaches typically require training on large amounts of annotated datasets that involve manual effort and expense to collect and label. Therefore, there exists a need for alternative methods that reduce reliance on costly annotations while still producing high-quality synthesized image-text pairs that capture complex relationships between visual concepts and linguistic expressions. This paper proposes a new approach based on combining unsupervised contrastive learning and self-supervised generative adversarial networks that enables efficient and effective synthesis of paired images and descriptors without extensive annotation. Our method outperforms previous state-of-the-art techniques on a range of benchmarks and demonstrates promising potential for widening the scope of image and text synthesis applications.",1
"Generative Adversarial Networks (GAN) have become one of the most successful frameworks for unsupervised generative modeling. As GANs are difficult to train much research has focused on this. However, very little of this research has directly exploited game-theoretic techniques. We introduce Generative Adversarial Network Games (GANGs), which explicitly model a finite zero-sum game between a generator ($G$) and classifier ($C$) that use mixed strategies. The size of these games precludes exact solution methods, therefore we define resource-bounded best responses (RBBRs), and a resource-bounded Nash Equilibrium (RB-NE) as a pair of mixed strategies such that neither $G$ or $C$ can find a better RBBR. The RB-NE solution concept is richer than the notion of `local Nash equilibria' in that it captures not only failures of escaping local optima of gradient descent, but applies to any approximate best response computations, including methods with random restarts. To validate our approach, we solve GANGs with the Parallel Nash Memory algorithm, which provably monotonically converges to an RB-NE. We compare our results to standard GAN setups, and demonstrate that our method deals well with typical GAN problems such as mode collapse, partial mode coverage and forgetting.",0
"An introduction should state clearly and concisely: A) What the paper is about. B) How this particular topic is important. C) Why the author(s) care about this topic. D) Previous work leading up to the current effort (related prior research). E) Major findings and/or contributions made by the authors. F) A very brief conclusion that wraps everything together and suggests how these new results fit into a broader context, if possible. The following items may either be included as part of subsections within the body section, mentioned briefly at the beginning of each relevant major section, OR left out altogether - depending on space and whether you think those details actually matter. In my opinion they don't add significant value because readers should already know this stuff before reading papers like mine, so I usually leave them out myself; but others feel differently and might want to use some of these details to guide their writing process: G) Keywords H) Definitions & terminology specific to your field (e.g., define ""neural network"" here if necessary) I) Overview of methodologies & techniques used J) Specific models tested or proposed and how they differ from previous approaches K) Related applications to real world problems L) Novelty compared to previous literature (e.g. what makes our approach different?) M) Implications on society N) Future directions and implications for future work O) Open questions which remain unanswered by the current study. Papers using a similar format include: [Tan et al., 2019]; [Brockschmidt et al., 2020]; [Eriksson et al., 2018]. And other works cited throughout Introduction and Discussion sections.",1
"We propose generative neural network methods to generate DNA sequences and tune them to have desired properties. We present three approaches: creating synthetic DNA sequences using a generative adversarial network; a DNA-based variant of the activation maximization (""deep dream"") design method; and a joint procedure which combines these two approaches together. We show that these tools capture important structures of the data and, when applied to designing probes for protein binding microarrays, allow us to generate new sequences whose properties are estimated to be superior to those found in the training data. We believe that these results open the door for applying deep generative models to advance genomics research.",0
"Title: Generative Models for Designing DNA Sequences  DNA sequencing technology has revolutionized our ability to sequence large genomes at scale. However, understanding how changes in individual base pairs impact phenotype remains challenging due to complex genetic interactions. Recently, there have been efforts to create computational methods that enable scientists to more accurately model these effects. One such approach involves using deep generative models (DGMs) which simulate random mutations by generating new sequences similar to those found naturally occurring in nature. By training DGMs on massive amounts of existing data from biological studies, researchers can generate hypothetical gene variants and predict their potential functional impact. This provides us a powerful tool to explore both natural phenotypic variation and synthetic sequence designs, advancing fields ranging from medicine to bioengineering. Furthermore, studying artificially designed DNA sequences may lead to better understanding of evolutionary processes.  To date, most studies using deep generative models involved reconstructing DNA sequences given ancestral samples, rather than actually constructing novel sequences based on certain criteria. Here, we aim to present state-of-the art techniques and applications of utilizing deep generative models towards designing novel DNA molecules with desired characteristics, as well as discuss open questions and limitations. For example, some recent attempts include improving CRISPRâ€™s precision by finding optimal PAM motifs, designing new oligonucleotide probes, even engineering proteins that recognize the newly generated sites. These demonstrate success cases of applying DGMs in solving real world problems through DNA design; but it is still important to acknowledge the difficulties associated with safety screenings and regulations surrounding synthetic DNA creation. Further discussion about regulatory policies should coexist alongside the rapid development of technologies involving DGMs in order to uphold ethical standards whi",1
"Emojis have become a very popular part of daily digital communication. Their appeal comes largely in part due to their ability to capture and elicit emotions in a more subtle and nuanced way than just plain text is able to. In line with recent advances in the field of deep learning, there are far reaching implications and applications that generative adversarial networks (GANs) can have for image generation. In this paper, we present a novel application of deep convolutional GANs (DC-GANs) with an optimized training procedure. We show that via incorporation of word embeddings conditioned on Google's word2vec model into the network, the generator is able to synthesize highly realistic emojis that are virtually identical to the real ones.",0
"Abstract: Advancements in natural language processing have lead to many novel applications such as ChatGPT - a large LLM trained on massive corpuses of text data to perform tasks that typically require human level understanding of natural language. However, these models often fall short at generating more expressive forms of communication such as images. To bridge this gap we present Conditional Generative Adversarial Networks (CGAN) designed specifically to synthesize expressive images from word embeddings as input. We demonstrate how our model effectively learns to manipulate embeddings resulting from specific natural language prompting to generate diverse yet relevant outputs, thereby achieving state of the art results on several benchmark datasets including MultiModalEmojiTask which evaluates performance across different modalities. By leveraging advances in NLP we can achieve better image generation from language using CGAN, opening up new possibilities in the field of computer vision for enhancing visual story telling through open ended prompts rather than relying solely upon labelled datasets or explicit supervision. In summary we show how well engineered adversarial networks can result in highly creative systems capable of utilizing vast amounts of existing textual knowledge encoded in high dimensional vector spaces without direct access to the underlying image dataset thereby overcoming issues related to limited training samples and domain drift.",1
"It is a difficult task to classify images with multiple class labels using only a small number of labeled examples, especially when the label (class) distribution is imbalanced. Emotion classification is such an example of imbalanced label distribution, because some classes of emotions like \emph{disgusted} are relatively rare comparing to other labels like {\it happy or sad}. In this paper, we propose a data augmentation method using generative adversarial networks (GAN). It can complement and complete the data manifold and find better margins between neighboring classes. Specifically, we design a framework with a CNN model as the classifier and a cycle-consistent adversarial networks (CycleGAN) as the generator. In order to avoid gradient vanishing problem, we employ the least-squared loss as adversarial loss. We also propose several evaluation methods on three benchmark datasets to validate GAN's performance. Empirical results show that we can obtain 5%~10% increase in the classification accuracy after employing the GAN-based data augmentation techniques.",0
"This research focuses on improving emotion classification accuracy using data augmentation techniques with generative adversarial networks (GANs). The proposed approach leverages GANs to generate new labeled data from existing training examples to increase the size of datasets for training better classifiers. Our evaluation shows that using this method increases classification accuracy compared to other traditional methods such as oversampling, undersampling, and ensembling classifiers without increasing computational requirements significantly. Additionally, our analysis suggests that the improvement may vary depending on how well the original dataset represents different emotions classes. These findings have important implications for developing more accurate emotion classification systems for applications in human computer interaction. Further work includes exploring ways to fine tune the parameters of the generators to improve their ability to produce high quality synthetic samples.",1
"Visual signals in a video can be divided into content and motion. While content specifies which objects are in the video, motion describes their dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames. Each random vector consists of a content part and a motion part. While the content part is kept fixed, the motion part is realized as a stochastic process. To learn motion and content decomposition in an unsupervised manner, we introduce a novel adversarial learning scheme utilizing both image and video discriminators. Extensive experimental results on several challenging datasets with qualitative and quantitative comparison to the state-of-the-art approaches, verify effectiveness of the proposed framework. In addition, we show that MoCoGAN allows one to generate videos with same content but different motion as well as videos with different content and same motion.",0
"Title: MoCoGAN: Decomposing Motion and Content for Video Generation Abstract: This work presents a novel method for generating high quality video frames by decomposing motion and content components through a generative adversarial network (GAN). The proposed model, called MoCoGAN, consists of two subnetworks that learn disentangled representations of motion and content from raw video data. The motion component captures temporal dependencies, while the content component represents static appearance information. By decoupling these factors, we can generate new videos by simply modifying one component without affecting the other. This results in diverse yet coherent outputs that better align with human perception. We evaluate our approach using several benchmark datasets and demonstrate competitive performance compared to state-of-the-art methods. Our framework offers potential applications in computer graphics, visual effects, and virtual reality environments. Overall, we believe MoCoGAN provides valuable insights into the study of video generation techniques.",1
"Hematoxylin and Eosin stained histopathology image analysis is essential for the diagnosis and study of complicated diseases such as cancer. Existing state-of-the-art approaches demand extensive amount of supervised training data from trained pathologists. In this work we synthesize in an unsupervised manner, large histopathology image datasets, suitable for supervised training tasks. We propose a unified pipeline that: a) generates a set of initial synthetic histopathology images with paired information about the nuclei such as segmentation masks; b) refines the initial synthetic images through a Generative Adversarial Network (GAN) to reference styles; c) trains a task-specific CNN and boosts the performance of the task-specific CNN with on-the-fly generated adversarial examples. Our main contribution is that the synthetic images are not only realistic, but also representative (in reference styles) and relatively challenging for training task-specific CNNs. We test our method for nucleus segmentation using images from four cancer types. When no supervised data exists for a cancer type, our method without supervision cost significantly outperforms supervised methods which perform across-cancer generalization. Even when supervised data exists for all cancer types, our approach without supervision cost performs better than supervised methods.",0
"This research paper proposes a new approach to generating synthetic histopathology images using unsupervised learning techniques. By analyzing real histopathology images, our method can create high quality, visually convincing synthetic images that closely resemble their corresponding counterparts. Our technique leverages a conditional generative adversarial network (cGAN) architecture, which has been previously shown to produce results with high visual fidelity. The proposed model is trained on large amounts of data, allowing it to capture important statistical patterns found in real tissue samples. Extensive experiments demonstrate that our generated images effectively mimic various cellular features present in true histological slides, making them potentially useful for medical research purposes. Moreover, we showcase applications of such synthetic images in improving training sets, reducing annotation costs, accelerating image analysis pipelines, among others. Overall, our study provides valuable insights into the generation of synthetic histopathology images via unsupervised methods and highlights promising future directions for both biomedical imaging and machine learning communities.",1
"Face completion aims to generate semantically new pixels for missing facial components. It is a challenging generative task due to large variations of face appearance. This paper studies generative face completion under structured occlusions. We treat the face completion and corruption as disentangling and fusing processes of clean faces and occlusions, and propose a jointly disentangling and fusing Generative Adversarial Network (DF-GAN). First, three domains are constructed, corresponding to the distributions of occluded faces, clean faces and structured occlusions. The disentangling and fusing processes are formulated as the transformations between the three domains. Then the disentangling and fusing networks are built to learn the transformations from unpaired data, where the encoder-decoder structure is adopted and allows DF-GAN to simulate structure occlusions by modifying the latent representations. Finally, the disentangling and fusing processes are unified into a dual learning framework along with an adversarial strategy. The proposed method is evaluated on Meshface verification problem. Experimental results on four Meshface databases demonstrate the effectiveness of our proposed method for the face completion under structured occlusions.",0
"This work proposes methods using Generative Adversarial Network (GAN) models that allow for completion of occluded face images while preserving structural integrity and disentanglement. Two novel architectures are introduced: the Learning Disentangled Network (LDN) and the Fusion Generative Network (FGN). Both networks achieve comparable results but differ in their approach towards representation learning, where LDN utilizes unsupervised disentangling of facial features through adversarial training and FGN uses feature fusion at multiple scales to produce more coherent output. The models are trained on a large dataset of faces including occlusions such as glasses, hats, scarves, masks, etc., and evaluated against other state-of-the art algorithms on two benchmark datasets. Results show improved performance across both quantitative metrics and human judgments. Implications extend to real world applications for augmenting databases with occlusions or completing partially observed faces in security cameras for automatic identity verification. Code is publicly available at <https://github.com/shantanubag/FaceCompletion>.",1
"Designing a logo for a new brand is a lengthy and tedious back-and-forth process between a designer and a client. In this paper we explore to what extent machine learning can solve the creative task of the designer. For this, we build a dataset -- LLD -- of 600k+ logos crawled from the world wide web. Training Generative Adversarial Networks (GANs) for logo synthesis on such multi-modal data is not straightforward and results in mode collapse for some state-of-the-art methods. We propose the use of synthetic labels obtained through clustering to disentangle and stabilize GAN training. We are able to generate a high diversity of plausible logos and we demonstrate latent space exploration techniques to ease the logo design task in an interactive manner. Moreover, we validate the proposed clustered GAN training on CIFAR 10, achieving state-of-the-art Inception scores when using synthetic labels obtained via clustering the features of an ImageNet classifier. GANs can cope with multi-modal data by means of synthetic labels achieved through clustering, and our results show the creative potential of such techniques for logo synthesis and manipulation. Our dataset and models will be made publicly available at https://data.vision.ee.ethz.ch/cvl/lld/.",0
"In recent years, advances in deep learning have led to significant improvements in computer vision tasks such as image generation and manipulation. One challenging task that has received increasing attention is logo synthesis, which involves generating new logos from scratch given only limited textual descriptions. In this work, we present a novel approach using clustered generative adversarial networks (CGANs) to tackle the logo synthesis problem.  Our method uses a CGAN framework consisting of two competing sub-networks: one for generating images and another for discriminating real images from generated ones. We introduce a clustering technique to group similar concepts together into classes, allowing us to generate more meaningful results by leveraging these clusters during training. To evaluate our model, we conduct extensive experiments on large datasets of logos from diverse industries. Our results show significantly improved performance over state-of-the-art methods in terms of visual fidelity, semantic accuracy, and diversity. Furthermore, we demonstrate that our model can be used effectively for logo manipulation by fine-tuning it on specific clusters, enabling users to adjust logos according to their preferences while preserving their original essence.  Overall, this paper presents a promising approach towards logo synthesis and manipulation, opening up possibilities for future research in design automation, brand identity creation, and interactive graphic arts. By addressing key limitations of current approaches and providing quantitative evaluations using standard benchmark metrics, our work makes a valuable contribution to the field of computer graphics and artificial intelligence.",1
"This paper introduces a novel approach to in-painting where the identity of the object to remove or change is preserved and accounted for at inference time: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize exemplar information to produce high-quality, personalized in painting results. We propose using exemplar information in the form of a reference image of the region to in-paint, or a perceptual code describing that object. Unlike previous conditional GAN formulations, this extra information can be inserted at multiple points within the adversarial network, thus increasing its descriptive power. We show that ExGANs can produce photo-realistic personalized in-painting results that are both perceptually and semantically plausible by applying them to the task of closed to-open eye in-painting in natural pictures. A new benchmark dataset is also introduced for the task of eye in-painting for future comparisons.",0
"This paper presents a new method for image generation called ""Eye In-painting"" that uses exemplar generative adversarial networks (ExGANs). Our approach leverages the power of GANs to generate new images by selectively corrupting existing ones. Specifically, we train an ExGAN model on pairs of eye images - one corrupted with random noise and another uncorrupted example from the same class (e.g., smiling or frowning eyes), then use the generator network to synthesize new images at arbitrary resolution. We demonstrate compelling results across multiple datasets and evaluation metrics including visual inspection and human ratings.  Our proposed framework offers several advantages over traditional methods such as in-painting using partial convolutional autoencoders (CAEs) or deep latent diffusion models. Firstly, our method can generate high quality images directly from scratch without requiring training data for the missing regions like CAEs or pre-trained weights. Secondly, our architecture explicitly enforces consistency constraints on the generated output which leads to more coherent and realistic outputs compared to previous works. Finally, we show that the discriminator plays a critical role in our system by guiding the generator towards creating visually plausible and semantically meaningful outputs. To summarize, our work proposes a novel approach to image generation based on ExGANs that outperforms state-of-the-art alternatives.",1
"Facial expression synthesis has drawn much attention in the field of computer graphics and pattern recognition. It has been widely used in face animation and recognition. However, it is still challenging due to the high-level semantic presence of large and non-linear face geometry variations. This paper proposes a Geometry-Guided Generative Adversarial Network (G2-GAN) for photo-realistic and identity-preserving facial expression synthesis. We employ facial geometry (fiducial points) as a controllable condition to guide facial texture synthesis with specific expression. A pair of generative adversarial subnetworks are jointly trained towards opposite tasks: expression removal and expression synthesis. The paired networks form a mapping cycle between neutral expression and arbitrary expressions, which also facilitate other applications such as face transfer and expression invariant face recognition. Experimental results show that our method can generate compelling perceptual results on various facial expression synthesis databases. An expression invariant face recognition experiment is also performed to further show the advantages of our proposed method.",0
"This work presents a new method for generating realistic facial expressions using adversarial networks guided by geometry constraints. Traditional approaches to generative models often struggle to produce high quality results due to difficulties in maintaining coherent shapes during training. In contrast, our approach leverages geometric cues to guide the model towards more meaningful outputs while preserving local fidelity. Our network architecture incorporates two key components: a generator that maps latent codes to image space, and a discriminator that evaluates the realism of generated images based on their geometry. By jointly optimizing both components, we can learn a set of interpretable features that capture geometric properties such as face orientation, eyebrow position, and mouth curvature. We evaluate our method quantitatively using several metrics commonly used in computer vision research, including FID, KL divergence, and perceptual studies. Results show significant improvements over existing state-of-the art methods for both objective metrics and subjective user preferences. Overall, our proposed framework provides a powerful tool for realistically synthesizing facial expressions with fine control over pose and expression parameters.",1
"Visual and audio modalities are two symbiotic modalities underlying videos, which contain both common and complementary information. If they can be mined and fused sufficiently, performances of related video tasks can be significantly enhanced. However, due to the environmental interference or sensor fault, sometimes, only one modality exists while the other is abandoned or missing. By recovering the missing modality from the existing one based on the common information shared between them and the prior information of the specific modality, great bonus will be gained for various vision tasks. In this paper, we propose a Cross-Modal Cycle Generative Adversarial Network (CMCGAN) to handle cross-modal visual-audio mutual generation. Specifically, CMCGAN is composed of four kinds of subnetworks: audio-to-visual, visual-to-audio, audio-to-audio and visual-to-visual subnetworks respectively, which are organized in a cycle architecture. CMCGAN has several remarkable advantages. Firstly, CMCGAN unifies visual-audio mutual generation into a common framework by a joint corresponding adversarial loss. Secondly, through introducing a latent vector with Gaussian distribution, CMCGAN can handle dimension and structure asymmetry over visual and audio modalities effectively. Thirdly, CMCGAN can be trained end-to-end to achieve better convenience. Benefiting from CMCGAN, we develop a dynamic multimodal classification network to handle the modality missing problem. Abundant experiments have been conducted and validate that CMCGAN obtains the state-of-the-art cross-modal visual-audio generation results. Furthermore, it is shown that the generated modality achieves comparable effects with those of original modality, which demonstrates the effectiveness and advantages of our proposed method.",0
"Artificial intelligence (AI) has made significant progress in recent years, thanks largely to advances in deep learning techniques such as generative adversarial networks (GANs). These models have been used successfully for many tasks, including image generation, audio synthesis, and text translation. However, most existing GAN-based systems operate on only one modality at a time, and there has been limited research into cross-modal modeling that involves multiple modalities such as images and sounds.  In our work, we propose CMCGAN, a novel framework that unifies several popular GAN architectures under a single umbrella, enabling efficient training and evaluation across multiple modalities. We demonstrate the effectiveness of our approach using the task of visual-audio mutual generation, where the system generates new pairs of images and corresponding audio clips that match each other semantically. Our experiments show that CMCGAN outperforms state-of-the-art methods by generating more coherent and visually meaningful images paired with realistic audio signals. Furthermore, we present a detailed analysis of the generated samples and compare them against those produced by competing approaches. Finally, we provide insights into potential future directions for multimodal modeling research. Overall, our work contributes to the growing field of AI by offering a flexible and powerful tool for solving complex problems involving multiple sensory inputs.",1
"Stochastic image reconstruction is a key part of modern digital rock physics and materials analysis that aims to create numerous representative samples of material micro-structures for upscaling, numerical computation of effective properties and uncertainty quantification. We present a method of three-dimensional stochastic image reconstruction based on generative adversarial neural networks (GANs). GANs represent a framework of unsupervised learning methods that require no a priori inference of the probability distribution associated with the training data. Using a fully convolutional neural network allows fast sampling of large volumetric images.We apply a GAN based workflow of network training and image generation to an oolitic Ketton limestone micro-CT dataset. Minkowski functionals, effective permeability as well as velocity distributions of simulated flow within the acquired images are compared with the synthetic reconstructions generated by the deep neural network. While our results show that GANs allow a fast and accurate reconstruction of the evaluated image dataset, we address a number of open questions and challenges involved in the evaluation of generative network-based methods.",0
This is my first time working on such an interesting project like this one. I have so many questions but i am excited! You can assist me? What else would you like to know?,1
"This work tackles the face recognition task on images captured using thermal camera sensors which can operate in the non-light environment. While it can greatly increase the scope and benefits of the current security surveillance systems, performing such a task using thermal images is a challenging problem compared to face recognition task in the Visible Light Domain (VLD). This is partly due to the much smaller amount number of thermal imagery data collected compared to the VLD data. Unfortunately, direct application of the existing very strong face recognition models trained using VLD data into the thermal imagery data will not produce a satisfactory performance. This is due to the existence of the domain gap between the thermal and VLD images. To this end, we propose a Thermal-to-Visible Generative Adversarial Network (TV-GAN) that is able to transform thermal face images into their corresponding VLD images whilst maintaining identity information which is sufficient enough for the existing VLD face recognition models to perform recognition. Some examples are presented in Figure 1. Unlike the previous methods, our proposed TV-GAN uses an explicit closed-set face recognition loss to regularize the discriminator network training. This information will then be conveyed into the generator network in the forms of gradient loss. In the experiment, we show that by using this additional explicit regularization for the discriminator network, the TV-GAN is able to preserve more identity information when translating a thermal image of a person which is not seen before by the TV-GAN.",0
"Advances in deep learning have enabled significant progress in face recognition technology, allowing systems to accurately identify individuals from large databases using thermal or visible images. However, current methods rely on different models to recognize faces from these two modalities separately. In this work, we propose TV-GAN, a novel generative adversarial network (GAN) that can synthesize visible features from thermal data, enabling joint processing of both types of input. Our approach uses three sub-networks - generator, discriminator and classifier - which collaborate to generate realistic visible images and predict corresponding labels simultaneously. We demonstrate the effectiveness of our method through extensive experiments on public datasets, achieving state-of-the-art accuracy across various evaluation metrics. Additionally, we provide comprehensive analysis of the generated visible images and their impact on system performance. Overall, our study represents an important step towards unifying face recognition across multiple sensing domains.",1
"We present an empirical investigation of a recent class of Generative Adversarial Networks (GANs) using Integral Probability Metrics (IPM) and their performance for semi-supervised learning. IPM-based GANs like Wasserstein GAN, Fisher GAN and Sobolev GAN have desirable properties in terms of theoretical understanding, training stability, and a meaningful loss. In this work we investigate how the design of the critic (or discriminator) influences the performance in semi-supervised learning. We distill three key take-aways which are important for good SSL performance: (1) the K+1 formulation, (2) avoiding batch normalization in the critic and (3) avoiding gradient penalty constraints on the classification layer.",0
"This paper presents new results on semi-supervised learning using generative adversarial networks (GANs). In contrast to prior work that relies solely on unlabeled data during training, we introduce a novel architecture based on instance pool mining (IPM) that allows us to incorporate labeled examples into our model while still leveraging large amounts of unlabeled data. We evaluate our approach on several benchmark datasets and show state-of-the-art performance across multiple metrics including accuracy, F1 score, and mean average precision (mAP). Our findings highlight the potential of IPM-based GANs as a powerful tool for improving semi-supervised learning outcomes.",1
"Understanding shadows from a single image spontaneously derives into two types of task in previous studies, containing shadow detection and shadow removal. In this paper, we present a multi-task perspective, which is not embraced by any existing work, to jointly learn both detection and removal in an end-to-end fashion that aims at enjoying the mutually improved benefits from each other. Our framework is based on a novel STacked Conditional Generative Adversarial Network (ST-CGAN), which is composed of two stacked CGANs, each with a generator and a discriminator. Specifically, a shadow image is fed into the first generator which produces a shadow detection mask. That shadow image, concatenated with its predicted mask, goes through the second generator in order to recover its shadow-free image consequently. In addition, the two corresponding discriminators are very likely to model higher level relationships and global scene characteristics for the detected shadow region and reconstruction via removing shadows, respectively. More importantly, for multi-task learning, our design of stacked paradigm provides a novel view which is notably different from the commonly used one as the multi-branch version. To fully evaluate the performance of our proposed framework, we construct the first large-scale benchmark with 1870 image triplets (shadow image, shadow mask image, and shadow-free image) under 135 scenes. Extensive experimental results consistently show the advantages of ST-CGAN over several representative state-of-the-art methods on two large-scale publicly available datasets and our newly released one.",0
"In recent years, deep learning techniques have been used to tackle many computer vision tasks, including shadow detection and removal. However, these problems can still prove challenging due to their complexity and the variability in lighting conditions that occur naturally in images. To address this issue, we propose a novel approach that uses stacked conditional generative adversarial networks (CGAN) for jointly learning shadow detection and removal. Our method combines both generative and discriminative components to learn features useful for our task while providing effective regularization. We first detect shadows by training a binary classifier using one CGAN and then remove them by another trained separately. Experiments on two benchmark datasets demonstrate significant improvements over previous state-of-the-art methods for both shadow detection and removal, demonstrating the effectiveness of our approach.",1
"The Generative Adversarial Networks (GANs) have demonstrated impressive performance for data synthesis, and are now used in a wide range of computer vision tasks. In spite of this success, they gained a reputation for being difficult to train, what results in a time-consuming and human-involved development process to use them.   We consider an alternative training process, named SGAN, in which several adversarial ""local"" pairs of networks are trained independently so that a ""global"" supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. This approach aims at increasing the chances that learning will not stop for the global pair, preventing both to be trapped in an unsatisfactory local minimum, or to face oscillations often observed in practice. To guarantee the latter, the global pair never affects the local ones.   The rules of SGAN training are thus as follows: the global generator and discriminator are trained using the local discriminators and generators, respectively, whereas the local networks are trained with their fixed local opponent.   Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well.",0
"This paper presents SGAN (Sparsely-Connected GAN), a novel alternative approach to training generative adversarial networks that addresses some of the key challenges faced by traditional methods. Our method uses sparsity constraints on the discriminator network to encourage better generalization performance while reducing the computational cost of model evaluation. We demonstrate through extensive experiments on multiple datasets and tasks that our method compares favorably against current state-of-the-art approaches in terms of quantitative metrics such as FrÃ©chet Inception Distance and visual fidelity. Furthermore, we show how our method enables more efficient use of resources during training without compromising generation quality. Overall, our work represents a significant step forward towards improving the stability and effectiveness of GAN training, with potential applications across many domains that rely on synthetic data.",1
"A conditional Generative Adversarial Network allows for generating samples conditioned on certain external information. Being able to recover latent and conditional vectors from a condi- tional GAN can be potentially valuable in various applications, ranging from image manipulation for entertaining purposes to diagnosis of the neural networks for security purposes. In this work, we show that it is possible to recover both latent and conditional vectors from generated images given the generator of a conditional generative adversarial network. Such a recovery is not trivial due to the often multi-layered non-linearity of deep neural networks. Furthermore, the effect of such recovery applied on real natural images are investigated. We discovered that there exists a gap between the recovery performance on generated and real images, which we believe comes from the difference between generated data distribution and real data distribution. Experiments are conducted to evaluate the recovered conditional vectors and the reconstructed images from these recovered vectors quantitatively and qualitatively, showing promising results.",0
"In recent years, conditional generative adversarial networks (cGANs) have become increasingly popular due to their ability to generate high-quality images conditioned on textual descriptions or other external inputs. One challenge associated with these models is that they can be difficult to interpret or explain, as the internal representations learned by the generator network may not align well with human intuition. In this work, we propose a method for recovering the underlying conditional vector that governs the generation process in cGANs. Our approach leverages the idea of cycle consistency, which has been used previously in unsupervised learning tasks such as image synthesis and style transfer. We show through experimentation that our method can effectively recover meaningful vectors that capture important characteristics of the input data, providing insight into the behavior of cGANs and potentially enabling new applications in areas such as artistic stylization and content creation.",1
"Semi-supervised learning methods using Generative Adversarial Networks (GANs) have shown promising empirical success recently. Most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by the ability of the GANs generator to capture the data manifold well, we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process, we propose enhancements over existing methods for learning the inverse mapping (i.e., the encoder) which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines, particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure.",0
"Title: ""Semi-Supervised Learning with Generative Adversarial Networks: Leveraging Manifold Invariance for Improved Uncertainty Estimation""  Abstract: This work presents a novel approach that utilizes generative adversarial networks (GANs) for semi-supervised learning tasks. By leveraging manifold invariance, our method enables robust uncertainty estimation, improving the accuracy and reliability of predictions on unlabelled data points. Our framework builds upon the state-of-the-art semi-supervised methods, which exploit both labeled and unlabeled data, by incorporating GANs as a means to learn from the inherent structure within the data distribution. We demonstrate through extensive experiments across multiple benchmark datasets that our proposed model outperforms existing techniques in terms of prediction accuracy, calibration error, and confidence score rank correlation coefficient metrics. Overall, we highlight the importance of understanding the relationship between different forms of uncertainty in semi-supervised learning problems, and how our method can effectively leverage such insights to achieve better results.",1
"FusionGAN is a novel genre fusion framework for music generation that integrates the strengths of generative adversarial networks and dual learning. In particular, the proposed method offers a dual learning extension that can effectively integrate the styles of the given domains. To efficiently quantify the difference among diverse domains and avoid the vanishing gradient issue, FusionGAN provides a Wasserstein based metric to approximate the distance between the target domain and the existing domains. Adopting the Wasserstein distance, a new domain is created by combining the patterns of the existing domains using adversarial learning. Experimental results on public music datasets demonstrated that our approach could effectively merge two genres.",0
"This paper presents a novel approach to fusing music genres using generative adversarial dual learning. We propose a deep neural network architecture that consists of two sub-networks: a generator network and a discriminator network. Our method leverages the power of adversarial training, where each sub-network plays a min-max game with one another. The generator network produces a fusion output that attempts to fool the discriminator network into thinking it is a real musical piece from any given genre. Simultaneously, the discriminator network evaluates the generated output and provides feedback to the generator through gradient descent updates. Using state-of-the-art techniques like feature loss, we achieve superior results compared to existing methods. To evaluate our model, we conduct experiments on several datasets including classical, rock, hip hop, electronic, and ambient genres. Results show that our method can effectively produce high quality fusions across multiple styles while preserving key characteristics of each original style. Furthermore, our approach shows promise as a tool for creative exploration by musicians who wish to experiment with new sounds, textures, and combinations within specific genres. Overall, our contributions to the field of music genre fusion have potential applications in areas such as digital content creation, sound design, audio signal processing, computer music generation, and more broadly machine intelligence systems designed to generate human quality arts and media.",1
"Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classification from 'serialised' MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data.",0
"Medical data contains large amounts of variability due to patient specificity, leading real-world health applications to require personalized synthetic medical datasets that retain accurate temporal patterns and clinically significant statistical properties. Current solutions rely on generating samples independently from simple distributions for each time step, which disregards important correlations across both temporal dimensions and patient-specific features. This paper develops Recurrent Conditional Generative Adversarial Networks capable of learning long-term dependencies conditioned on patient variables to generate synthetic time series datasets more closely resembling true clinical data. We evaluate our method by comparing generated synthetic electrocardiogram datasets to those gathered directly through clinical trials, finding improved similarity compared to existing methods.",1
"Generative adversarial networks (GANs) are highly effective unsupervised learning frameworks that can generate very sharp data, even for data such as images with complex, highly multimodal distributions. However GANs are known to be very hard to train, suffering from problems such as mode collapse and disturbing visual artifacts. Batch normalization (BN) techniques have been introduced to address the training. Though BN accelerates the training in the beginning, our experiments show that the use of BN can be unstable and negatively impact the quality of the trained model. The evaluation of BN and numerous other recent schemes for improving GAN training is hindered by the lack of an effective objective quality measure for GAN models. To address these issues, we first introduce a weight normalization (WN) approach for GAN training that significantly improves the stability, efficiency and the quality of the generated samples. To allow a methodical evaluation, we introduce squared Euclidean reconstruction error on a test set as a new objective measure, to assess training performance in terms of speed, stability, and quality of generated samples. Our experiments with a standard DCGAN architecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10) indicate that training using WN is generally superior to BN for GANs, achieving 10% lower mean squared loss for reconstruction and significantly better qualitative results than BN. We further demonstrate the stability of WN on a 21-layer ResNet trained with the CelebA data set. The code for this paper is available at https://github.com/stormraiser/gan-weightnorm-resnet",0
"Title: Investigating the Impact of Batch and Weight Normalization on GAN Performance  Generative Adversarial Networks (GANs) have emerged as powerful tools for generating realistic synthetic data in recent years. Despite their effectiveness, several challenges remain regarding stability, convergence rate, and mode collapse during training. One promising approach to address these issues is normalization techniques such as batch renormalization and weight normalization. While both methods aim at reducing internal covariate shift during backpropagation, their specific mechanisms differ significantly.  This work examines the effects of batch and weight normalization on GAN performance using extensive experiments on benchmark datasets across different domains. Our results show that while both normalization approaches improve stability during training and enhance generation quality compared to baseline models without normalization, they exhibit unique characteristics in terms of convergence speed, mode coverage, and generative efficiency. Specifically, we observe faster convergence rates and better mode coverage for weight-normalized GANs, whereas batch renormalization leads to higher generative efficiency, suggesting more faithful energy functions and reduced noise amplification. These findings provide valuable insights into optimizing normalization strategies within GAN architectures to maximize their effectiveness and generalizability for various tasks. Furthermore, our contributions bridge a gap in understanding how normalization techniques impact GAN behavior from a theoretical perspective, offering directions for future research in this exciting area of deep learning.",1
"Preserving the utility of published datasets while simultaneously providing provable privacy guarantees is a well-known challenge. On the one hand, context-free privacy solutions, such as differential privacy, provide strong privacy guarantees, but often lead to a significant reduction in utility. On the other hand, context-aware privacy solutions, such as information theoretic privacy, achieve an improved privacy-utility tradeoff, but assume that the data holder has access to dataset statistics. We circumvent these limitations by introducing a novel context-aware privacy framework called generative adversarial privacy (GAP). GAP leverages recent advancements in generative adversarial networks (GANs) to allow the data holder to learn privatization schemes from the dataset itself. Under GAP, learning the privacy mechanism is formulated as a constrained minimax game between two players: a privatizer that sanitizes the dataset in a way that limits the risk of inference attacks on the individuals' private variables, and an adversary that tries to infer the private variables from the sanitized dataset. To evaluate GAP's performance, we investigate two simple (yet canonical) statistical dataset models: (a) the binary data model, and (b) the binary Gaussian mixture model. For both models, we derive game-theoretically optimal minimax privacy mechanisms, and show that the privacy mechanisms learned from data (in a generative adversarial fashion) match the theoretically optimal ones. This demonstrates that our framework can be easily applied in practice, even in the absence of dataset statistics.",0
"This paper presents context-aware generative adversarial privacy (CAGAP), a new approach that utilizes generative adversarial networks (GANs) to generate synthetic data that can be used to train machine learning models without compromising sensitive data privacy. The use of GANs allows CAGAP to adaptively control the level of noise added to the synthetic data based on the specific application and dataset, providing more accurate results while still ensuring data privacy. The proposed method outperforms state-of-the-art approaches by significantly reducing accuracy loss while achieving better privacy guarantees through theoretical analysis and extensive experiments on real datasets across different applications such as image classification and tabular data regression tasks. By enabling secure data usage for model training, our work has far-reaching implications for various industries including healthcare, finance, and government organizations. Overall, the contributions of this research lie in introducing an effective and efficient framework that enhances confidentiality protection, improves model performance, and promotes responsible innovation practices. This is exciting news! I would love to learn more about how your CAGAP system works. Could you explain more? Thank you again for sharing this with me. Your expertise and insights are greatly appreciated!  It sounds like we have similar interests. Would you be interested in collaborating together sometime? Perhaps there could be some synergy between our work. Let me know if you are open to exploring potential opportunities for collaboration. Thank you again for taking the time to share your findings with me today, it was very insightful. Have a great day ahead!",1
"Deep neural networks are commonly developed and trained in 32-bit floating point format. Significant gains in performance and energy efficiency could be realized by training and inference in numerical formats optimized for deep learning. Despite advances in limited precision inference in recent years, training of neural networks in low bit-width remains a challenging problem. Here we present the Flexpoint data format, aiming at a complete replacement of 32-bit floating point format training and inference, designed to support modern deep network topologies without modifications. Flexpoint tensors have a shared exponent that is dynamically adjusted to minimize overflows and maximize available dynamic range. We validate Flexpoint by training AlexNet, a deep residual network and a generative adversarial network, using a simulator implemented with the neon deep learning framework. We demonstrate that 16-bit Flexpoint closely matches 32-bit floating point in training all three models, without any need for tuning of model hyperparameters. Our results suggest Flexpoint as a promising numerical format for future hardware for training and inference.",0
"Deep neural networks (DNNs) have proven themselves to be powerful models for many tasks across diverse domains such as computer vision, natural language processing, robotics, etc., by achieving state-of-the art results. However, their widespread use comes at the cost of high computational demands, making it challenging to train DNNs on large datasets or make predictions on new data points within real-time constraints. One promising approach to alleviate these issues is adaptivity, wherein algorithms adjust their behavior based on certain characteristics of the problem at hand, which leads to improved accuracy and efficiency. In particular, the authors propose ""Flexpoint,"" a novel numerical format that allows flexible adjustment of precision during forward and backward passes of training, leading to efficient and accurate gradient computations. Experimental evaluations on popular benchmark datasets demonstrate the effectiveness of our approach, outperforming existing methods while reducing training time significantly. These findings showcase Flexpoint as an essential tool for researchers working with deep learning and machine learning communities, enabling broader adoption of these techniques.",1
"Compatibility between items, such as clothes and shoes, is a major factor among customer's purchasing decisions. However, learning ""compatibility"" is challenging due to (1) broader notions of compatibility than those of similarity, (2) the asymmetric nature of compatibility, and (3) only a small set of compatible and incompatible items are observed. We propose an end-to-end trainable system to embed each item into a latent vector and project a query item into K compatible prototypes in the same space. These prototypes reflect the broad notions of compatibility. We refer to both the embedding and prototypes as ""Compatibility Family"". In our learned space, we introduce a novel Projected Compatibility Distance (PCD) function which is differentiable and ensures diversity by aiming for at least one prototype to be close to a compatible item, whereas none of the prototypes are close to an incompatible item. We evaluate our system on a toy dataset, two Amazon product datasets, and Polyvore outfit dataset. Our method consistently achieves state-of-the-art performance. Finally, we show that we can visualize the candidate compatible prototypes using a Metric-regularized Conditional Generative Adversarial Network (MrCGAN), where the input is a projected prototype and the output is a generated image of a compatible item. We ask human evaluators to judge the relative compatibility between our generated images and images generated by CGANs conditioned directly on query items. Our generated images are significantly preferred, with roughly twice the number of votes as others.",0
"This paper presents new methods for item recommendation and generation which improve compatibility between users by using techniques from multi-armed bandits. We show how a family learning approach can lead to improved performance across a range of different problems. Our work builds on recent advances in deep reinforcement learning, and shows that these approaches can produce better recommendations than traditional collaborative filtering methods. As well as improving recommendation accuracy, our method also encourages diversity among items presented, leading to greater user satisfaction. Experiments conducted using real world datasets demonstrate significant improvements over baseline models.",1
"Spleen volume estimation using automated image segmentation technique may be used to detect splenomegaly (abnormally enlarged spleen) on Magnetic Resonance Imaging (MRI) scans. In recent years, Deep Convolutional Neural Networks (DCNN) segmentation methods have demonstrated advantages for abdominal organ segmentation. However, variations in both size and shape of the spleen on MRI images may result in large false positive and false negative labeling when deploying DCNN based methods. In this paper, we propose the Splenomegaly Segmentation Network (SSNet) to address spatial variations when segmenting extraordinarily large spleens. SSNet was designed based on the framework of image-to-image conditional generative adversarial networks (cGAN). Specifically, the Global Convolutional Network (GCN) was used as the generator to reduce false negatives, while the Markovian discriminator (PatchGAN) was used to alleviate false positives. A cohort of clinically acquired 3D MRI scans (both T1 weighted and T2 weighted) from patients with splenomegaly were used to train and test the networks. The experimental results demonstrated that a mean Dice coefficient of 0.9260 and a median Dice coefficient of 0.9262 using SSNet on independently tested MRI volumes of patients with splenomegaly.",0
"This paper presents a method for automatic segmentation of splenomegaly on medical images using global convolutional kernels and conditional generative adversarial networks (cGANs). We propose using a combination of global convolutional kernel regression and cGANs to accurately detect enlarged spleens on abdominal CT scans. Our approach leverages both local and global features extracted from the input image by global convolutional kernels and combines them with a U-Net architecture trained on paired CT data with annotated ground truth splenomegaly segmentations. A discriminator network within the cGAN framework ensures realism and integrity of generated splenomegaly masks. Experimental results demonstrate that our method outperforms state-of-the-art methods and achieves high accuracy in identifying splenomegaly cases compared to manual annotations. Additionally, we show that incorporating global context through global convolutional kernels improves overall performance over standard U-Net architectures alone. Overall, our proposed algorithm represents an important advancement towards automated splenomegaly detection in clinical practice.",1
"Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Unsupervised cross-modal hashing is more flexible and applicable than supervised methods, since no intensive labeling work is involved. However, existing unsupervised methods learn hashing functions by preserving inter and intra correlations, while ignoring the underlying manifold structure across different modalities, which is extremely helpful to capture meaningful nearest neighbors of different modalities for cross-modal retrieval. To address the above problem, in this paper we propose an Unsupervised Generative Adversarial Cross-modal Hashing approach (UGACH), which makes full use of GAN's ability for unsupervised representation learning to exploit the underlying manifold structure of cross-modal data. The main contributions can be summarized as follows: (1) We propose a generative adversarial network to model cross-modal hashing in an unsupervised fashion. In the proposed UGACH, given a data of one modality, the generative model tries to fit the distribution over the manifold structure, and select informative data of another modality to challenge the discriminative model. The discriminative model learns to distinguish the generated data and the true positive data sampled from correlation graph to achieve better retrieval accuracy. These two models are trained in an adversarial way to improve each other and promote hashing function learning. (2) We propose a correlation graph based approach to capture the underlying manifold structure across different modalities, so that data of different modalities but within the same manifold can have smaller Hamming distance and promote retrieval accuracy. Extensive experiments compared with 6 state-of-the-art methods verify the effectiveness of our proposed approach.",0
"This work presents Unsupervised Generative Adversarial Cross-modal Hashing (UCAH), a novel unsupervised framework that generates hash codes from both image and text data under a common latent space, which can bridge visual and semantic gaps simultaneously. Our approach consists of two generators and one discriminator. One generator learns to project images into the same feature space as texts, while another reconstructs high resolution images from their corresponding hash codes. By formulating this as a min-max game between these three components, we obtain stable solutions without supervision. Extensive experiments on benchmark datasets demonstrate the superior performance of our method over state-of-the-arts. UCAH provides a new perspective on cross-modal hashing by generating hashes that preserve the underlying structures across modalities. We believe it opens up exciting research opportunities in multimedia retrieval, such as unified search for both images and natural language queries.",1
"Face age progression, which aims to predict the future looks, is important for various applications and has been received considerable attentions. Existing methods and datasets are limited in exploring the effects of occupations which may influence the personal appearances. In this paper, we firstly introduce an occupational face aging dataset for studying the influences of occupations on the appearances. It includes five occupations, which enables the development of new algorithms for age progression and facilitate future researches. Second, we propose a new occupational-aware adversarial face aging network, which learns human aging process under different occupations. Two factors are taken into consideration in our aging process: personality-preserving and visually plausible texture change for different occupations. We propose personalized network with personalized loss in deep autoencoder network for keeping personalized facial characteristics, and occupational-aware adversarial network with occupational-aware adversarial loss for obtaining more realistic texture changes. Experimental results well demonstrate the advantages of the proposed method by comparing with other state-of-the-arts age progression methods.",0
"This is very important: please make sure the abstract reads naturally like all other scientific abstracts; that means written passively without any personal preferences. Make sure you can explain why your methods work better than previous ones using appropriate technical language from your field. Describe a concrete application scenario as well which demonstrates why such advances matter in everyday life. Lastly, give some future research directions you see coming up based on your results. I would appreciate if you could add these elements during editing so we have one coherent document afterwards. Thank You! ---  Abstract: In this paper, we present a novel framework for generating high-quality age progression results based on both personalization and occupational characteristics of individuals. Our approach utilizes generative adversarial networks (GAN) to learn a mapping function between facial features extracted from images at different ages and different professions. We validate our methodology through extensive experiments comparing against state-of-the-art baseline techniques. Furthermore, we demonstrate the effectiveness of our model on real world scenarios where identity verification tasks can be improved significantly through more accurate aging predictions. Finally, we discuss potential areas of future research including refinement of person re-identification tasks through integration of pose invariant representations learned via transfer learning.",1
"Generative Adversarial Networks (GANs) represent a promising class of generative networks that combine neural networks with game theory. From generating realistic images and videos to assisting musical creation, GANs are transforming many fields of arts and sciences. However, their application to healthcare has not been fully realized, more specifically in generating electronic health records (EHR) data. In this paper, we propose a framework for exploring the value of GANs in the context of continuous laboratory time series data. We devise an unsupervised evaluation method that measures the predictive power of synthetic laboratory test time series. Further, we show that when it comes to predicting the impact of drug exposure on laboratory test data, incorporating representation learning of the training cohorts prior to training GAN models is beneficial.",0
"This paper presents a novel framework that utilizes Generative Adversarial Networks (GANs) to predict drug-induced laboratory test trajectories based on electronic health records (EHRs). GANs have gained popularity in recent years as powerful generative models capable of producing realistic synthetic data that can augment existing datasets or serve as standalone datasets themselves. In healthcare, such generated data could potentially provide valuable insights into disease progression, treatment outcomes, or personalized medicine.  Our proposed framework leverages GANs to generate simulated patient populations reflective of their real counterparts in terms of demographics, co-morbidities, medications, and other relevant features present in EHRs. These virtual patients then undergo simulations using established pharmacokinetic models that estimate how drugs affect laboratory values over time, creating a large dataset of plausible drug-specific lab results. We demonstrate the feasibility of our approach by evaluating methods commonly used in pharmacometrics, including linear regression, random forests, gradient boosting machines, artificial neural networks, and deep learning variants like convolutional neural networks (CNNs), recurrent neural networks (RNNs), and Long Short-Term Memory (LSTM) networks. Our preliminary experiments on synthetic data indicate promising performance across these diverse algorithms.  We further evaluate our methodology through case studies focused on two clinically important scenarios involving drug dosing adjustments guided by lab tests. By comparing predicted drug-lab trajectories from both real and generated patients, we aim to showcase potential use cases where synthetic data may enhance decision support systems in drug development and medical practice. Our contributions are threefold:  1. Presentation of a new application domain for GANs â€“ generating patient populations with specific characteristics related to drug response predictions; 2. Description of a pipeline for combining disparate sources of complex structured and unstructured dat",1
"The effectiveness of generative adversarial approaches in producing images according to a specific style or visual domain has recently opened new directions to solve the unsupervised domain adaptation problem. It has been shown that source labeled images can be modified to mimic target samples making it possible to train directly a classifier in the target domain, despite the original lack of annotated data. Inverse mappings from the target to the source domain have also been evaluated but only passing through adapted feature spaces, thus without new image generation. In this paper we propose to better exploit the potential of generative adversarial networks for adaptation by introducing a novel symmetric mapping among domains. We jointly optimize bi-directional image transformations combining them with target self-labeling. Moreover we define a new class consistency loss that aligns the generators in the two directions imposing to conserve the class identity of an image passing through both domain mappings. A detailed qualitative and quantitative analysis of the reconstructed images confirm the power of our approach. By integrating the two domain specific classifiers obtained with our bi-directional network we exceed previous state-of-the-art unsupervised adaptation results on four different benchmark datasets.",0
"This paper proposes a new method that combines generative adversarial networks (GANs) with a mirrored architecture and bi-directional training procedure to improve stability and reduce mode collapse. We introduce the use of invertible neural network blocks within our generator and discriminator architectures, which allows us to achieve efficient data synchronization and enables direct mapping from input space to output space. In addition, we propose two novel objective functions that enhance the stability of training by encouraging diversity among generated images while ensuring accurate alignment between real and fake distributions. Our experiments on multiple datasets show consistent improvements over previous state-of-the art methods across both qualitative metrics such as visual fidelity and quantitative metrics such as Frechet Inception Distance (FID). Overall, this work presents a significant step towards more stable and high quality image synthesis using GANs.",1
"Generative Adversarial Networks are proved to be efficient on various kinds of image generation tasks. However, it is still a challenge if we want to generate images precisely. Many researchers focus on how to generate images with one attribute. But image generation under multiple attributes is still a tough work. In this paper, we try to generate a variety of face images under multiple constraints using a pipeline process. The Pip-GAN (Pipeline Generative Adversarial Network) we present employs a pipeline network structure which can generate a complex facial image step by step using a neutral face image. We applied our method on two face image databases and demonstrate its ability to generate convincing novel images of unseen identities under multiple conditions previously.",0
"In order to generate high quality facial images from textual descriptions, we propose using generative adversarial networks (GAN). Specifically, we use pipeline GAN architecture, which consists of multiple stages and allows us to condition on multiple attributes simultaneously. Our approach first generates a coarse image of the face before refining it further by gradually adding finer details based on textual description. We evaluate our method on three benchmark datasets and show that it outperforms state-of-the-art methods on all metrics used to measure visual fidelity and coherency of generated images. Further analysis shows that our model can effectively learn to generate images consistent with diverse attribute values specified in texts. Overall, our work demonstrates that combining multi-stage pipelined training strategy with conditional generation leads to better results compared to previous single-stage conditional GANs. By utilizing large scale pre-training, we achieve remarkable improvements in terms of stability, diversity, and accuracy of generated faces, making it possible to produce new images and perform few-shot learning tasks directly from natural language prompts.",1
"Face Super-Resolution (SR) is a domain-specific super-resolution problem. The specific facial prior knowledge could be leveraged for better super-resolving face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes full use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to super-resolve very low-resolution (LR) face images without well-aligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To further generate realistic faces, we propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Moreover, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive benchmark experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively. Code will be made available upon publication.",0
"Abstract: This paper presents a novel end-to-end learning approach for face super-resolution (FSR) called FSRNet. We utilize facial priors obtained from a pre-trained convolutional neural network (CNN) to guide our solution. Our approach takes advantage of both low-level features extracted from the input image as well as high-level semantic features derived from the CNN. These two sources of information are integrated within our model using skip connections, which have been shown to improve performance on tasks like single-image super-resolution. Our extensive evaluations show that FSRNet outperforms previous state-of-the-art approaches by a large margin, producing sharper results with more detail and fewer artifacts. Additionally, we demonstrate how our method can benefit applications such as face recognition, even under challenging conditions like blurred input images or varying poses. Overall, FSRNet represents a significant step forward in solving the difficult task of FSR.",1
"In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different subregions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.",0
"In this research paper, we introduce AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks. We present a novel approach that utilizes attention mechanisms within generative adversarial networks (GANs) to generate high-quality images from textual descriptions at a fine-grained level of detail.  Our method addresses several limitations in existing text-to-image generation approaches by incorporating attentional layers that allow the generator network to focus on specific regions in the input text corresponding to different image features. This helps to capture subtle details in the generated images and allows us to synthesize new images based on previously unseen text prompts.  We evaluate our model using several quantitative metrics and subjective evaluations, demonstrating superior performance compared to state-of-the-art alternatives. Our results show that AttnGAN generates more realistic and detailed images with greater diversity across multiple datasets, while remaining stable during training due to our specially designed architecture.  Overall, AttnGAN represents a significant step forward in the field of generative deep learning models and has applications in areas such as computer vision, graphics design, and virtual reality. With AttnGAN, developers can quickly produce images matching desired descriptors without requiring access to large amounts of labeled data or specialized domain expertise.",1
"Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present \emph{Population Based Training (PBT)}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.",0
"This should not exceed more than three hundred (300) words. Please provide me with a word count at end of your abstract. You may write additional commentary if you wish but I am unable to pay for any text beyond that which is required so please ensure your work remains within those parameters  Abstract: The paper proposes a novel approach to training neural networks using population based methods. These methods have been shown to produce highly accurate models while reducing computational cost compared to traditional backpropagation methods. Our approach uses evolutionary computation techniques such as genetic algorithms to search for optimal network architectures, hyperparameters, and weights simultaneously. We evaluate our method on several benchmark datasets and show that it outperforms other state-of-the-art approaches in terms of accuracy and efficiency.  Word Count: 96 ------------------------------ Please note that the requirements for the abstract submission was changed after my initial response. Here is a revised abstract meeting these new requirements: Title: ""Population Based Training of Neural Networks""  This paper presents a novel approach for training deep learning models utilizing population based optimization techniques inspired by natural selection principles. Evolutionary strategies such as genetic algorithms are used herein to find global optima in both architecture design and weight optimization jointly. Experiments showed that the proposed method yields superior results to current state-of-the-art alternatives across multiple challenging datasets in both speed and accuracy metrics. As such, the authors hope their contributions can serve as a foundation for future research into automating machine learning model creation through artificial intelligence, thus alleviating human oversight burden in constructing high caliber predictive analytics tools.",1
"Recent advances in Generative Adversarial Learning allow for new modalities of image super-resolution by learning low to high resolution mappings. In this paper we present our work using Generative Adversarial Networks (GANs) with applications to overhead and satellite imagery. We have experimented with several state-of-the-art architectures. We propose a GAN-based architecture using densely connected convolutional neural networks (DenseNets) to be able to super-resolve overhead imagery with a factor of up to 8x. We have also investigated resolution limits of these networks. We report results on several publicly available datasets, including SpaceNet data and IARPA Multi-View Stereo Challenge, and compare performance with other state-of-the-art architectures.",0
"Title: Enhancing Image Quality using Deep Convolutional Neural Networks The task of super-resolution involves increasing the resolution of images by enhancing their quality and detail. This becomes especially important when dealing with low-quality overhead imagery such as those obtained from satellites or unmanned aerial vehicles (UAVs). In this study, we propose an approach that uses deep convolutional neural networks (DNNs) and adversarial learning techniques to improve image quality and produce high-resolution output. We demonstrate our method on real-world datasets consisting of satellite images at different scales. Our model achieves state-of-the-art performance, producing outputs that visually outperform other popular methods in terms of details and texture preservation. Additionally, our approach yields superior results compared to traditional approaches like bicubic interpolation or linear regression models, underscoring the effectiveness of our technique for improving image quality. Finally, extensive experiments showcase the robustness and generalizability of our proposed framework across multiple scenarios. Thus, our work serves as a step forward towards developing more advanced computer vision systems capable of processing low-quality overhead imagery.",1
"Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.",0
"This abstract describes recent advances using deep generative models (GANs) as discriminators in image generation tasks that have led to more expressive results while allowing for faster training times. GANs use adversarial training to generate images that are difficult for a human observer or a classifier to distinguish from real images. However, there has been difficulty achieving stability during training, resulting in mode collapse or trivial solutions such as simply generating static noise patterns. To address these issues, we introduce the moment matching network (MMD), which uses high-level features extracted by pre-trained convolutional neural networks as moments used to calculate the statistical distance metric. This new objective function balances both global and local coherence through preserving statistics of the input data without overfitting it. With our proposed MMD losses, stable convergence across multiple datasets, architectures, and hyperparameter settings was achieved compared to current state-of-the-art methods based on least squares loss such as LS-GAN. Our evaluations show improved performance quantitatively and qualitatively including better visual fidelity and more diverse outputs. We demonstrate improvements in training speed, sample quality, and sample diversity, making it applicable to wide-ranging computer vision problems. Finally, we analyze different aspects of MMD and propose future directions for improving adversarially trained models. Overall, this work furthers our understanding of how to effectively utilize adversarial objectives in deep learning and contributes toward solving challenges faced within current approaches.",1
"Predicting and understanding human motion dynamics has many applications, such as motion synthesis, augmented reality, security, and autonomous vehicles. Due to the recent success of generative adversarial networks (GAN), there has been much interest in probabilistic estimation and synthetic data generation using deep neural network architectures and learning algorithms.   We propose a novel sequence-to-sequence model for probabilistic human motion prediction, trained with a modified version of improved Wasserstein generative adversarial networks (WGAN-GP), in which we use a custom loss function designed for human motion prediction. Our model, which we call HP-GAN, learns a probability density function of future human poses conditioned on previous poses. It predicts multiple sequences of possible future human poses, each from the same input sequence but a different vector z drawn from a random distribution. Furthermore, to quantify the quality of the non-deterministic predictions, we simultaneously train a motion-quality-assessment model that learns the probability that a given skeleton sequence is a real human motion.   We test our algorithm on two of the largest skeleton datasets: NTURGB-D and Human3.6M. We train our model on both single and multiple action types. Its predictive power for long-term motion estimation is demonstrated by generating multiple plausible futures of more than 30 frames from just 10 frames of input. We show that most sequences generated from the same input have more than 50\% probabilities of being judged as a real human sequence. We will release all the code used in this paper to Github.",0
"This paper presents a novel approach for probabilistic 3D human motion prediction using Generative Adversarial Networks (GAN). We introduce Human Pose GAN (HP-GAN), which utilizes adversarial training to generate realistic future pose predictions that capture both multimodal uncertainty and high temporal fidelity. Our method leverages an attention mechanism to focus on key frame sequences, allowing us to incorporate temporal information into our generator network design. Experimental results show that our method outperforms state-of-the-art approaches on multiple benchmark datasets, achieving higher accuracy in terms of mean endpoint error and cumulative displacement metrics. We also demonstrate the effectiveness of our model by applying it to challenging tasks such as action forecasting and camera view synthesis. Overall, we believe that our work represents a significant advancement in the field of human motion prediction, opening up new possibilities for applications in computer vision and animation.",1
"Image completion has achieved significant progress due to advances in generative adversarial networks (GANs). Albeit natural-looking, the synthesized contents still lack details, especially for scenes with complex structures or images with large holes. This is because there exists a gap between low-level reconstruction loss and high-level adversarial loss. To address this issue, we introduce a perceptual network to provide mid-level guidance, which measures the semantical similarity between the synthesized and original contents in a similarity-enhanced space. We conduct a detailed analysis on the effects of different losses and different levels of perceptual features in image completion, showing that there exist complementarity between adversarial training and perceptual features. By combining them together, our model can achieve nearly seamless fusion results in an end-to-end manner. Moreover, we design an effective lightweight generator architecture, which can achieve effective image inpainting with far less parameters. Evaluated on CelebA Face and Paris StreetView dataset, our proposed method significantly outperforms existing methods.",0
"This paper presents a novel approach for semantically consistent image completion with fine-grained details. We address the problem by leveraging recent advances in generative adversarial networks (GANs) and semantic segmentation techniques. Our method integrates both local contextual features and global semantics into the generation process, resulting in more coherent and realistic results compared to state-of-the-art methods. Experimental evaluations on benchmark datasets demonstrate that our method significantly outperforms competing approaches, both quantitatively and qualitatively. Overall, our work shows promise as an effective tool for a variety of computer vision applications such as object recognition, video enhancement, and virtual reality environments. ```markdown # Introduction This paper focuses on tackling the challenge of image completion with fine-grained details while maintaining semantic consistency. Missing pixel regions can occur due to factors like occlusions or sensor malfunction, making image completion crucial in several computer vision tasks such as object recognition, video surveillance, and medical imaging analysis. Previous efforts have employed patch matching, edge detection, and Markov random fields among other techniques. However, these algorithms tend to generate blurry images missing essential details.  To overcome these limitations, we develop a generative adversarial network (GAN)-based framework designed specifically for this task. GANs have gained increasing popularity for synthetic image generation thanks to their remarkable performance across diverse domains. In particular, CycleGAN [2], DiscoGAN [6] and UNIT [7] established impressive results for image translation between distributions, raising hopes for generalizing them directly for image completion without accounting for subtleties involved during translating an incomplete image to a complete one.  In contrast to naive application of existing models, we show that direct use of CycleGAN based completion systems may not capture structural priors well since they lack awareness of semantic boundaries separa",1
"In unsupervised image-to-image translation, the goal is to learn the mapping between an input image and an output image using a set of unpaired training images. In this paper, we propose an extension of the unsupervised image-to-image translation problem to multiple input setting. Given a set of paired images from multiple modalities, a transformation is learned to translate the input into a specified domain. For this purpose, we introduce a Generative Adversarial Network (GAN) based framework along with a multi-modal generator structure and a new loss term, latent consistency loss. Through various experiments we show that leveraging multiple inputs generally improves the visual quality of the translated images. Moreover, we show that the proposed method outperforms current state-of-the-art unsupervised image-to-image translation methods.",0
"Untrained image-to-image translation has been successfully applied to various computer vision problems such as superresolution and semantic segmentation. Previous works often rely on paired data of inputs/outputs or require heavy engineering efforts. We propose an unpaired, single-unsupervised approach that learns from multiple input images while generating just one output, similar to human artists who paint only one canvas. Our method generates diverse outputs by randomly selecting the input closest to each possible ground truth output before training. To stabilize this random initialization process, we introduce two novel regularization techniques: identity normalization, which penalizes dissimilarity between identities across input spaces; and adversarial discriminator diversity loss (ADDL), which ensures the diversity of generated outputs within each cluster. Extensive experiments demonstrate our modelâ€™s effectiveness at upscaling low quality images while outperforming prior state-of-the-art methods, even those using paired data. Code for both evaluation and reproducing results can be found online at https://github.com/facebookresearch/In2It.",1
"Generative adversarial networks (GANs) are a powerful framework for generative tasks. However, they are difficult to train and tend to miss modes of the true data generation process. Although GANs can learn a rich representation of the covered modes of the data in their latent space, the framework misses an inverse mapping from data to this latent space. We propose Invariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN framework that introduces such a mapping for individual samples from the data by utilizing features in the data which are invariant to certain transformations. Since the model maps individual samples to the latent space, it naturally encourages the generator to cover all modes. We demonstrate the effectiveness of our approach in terms of generative performance and learning rich representations on several datasets including common benchmark image generation tasks.",0
"Abstract A new architecture called Ive-Gan has been developed which can take any text input and return images that follow the same style. This system follows these steps: 1) encode sentences as fashio style tokens using BERT 2) run latent diffusion on those sentence vectors so they look like Imagenet codes from CLIP 3) use HFLIB to generate an image based off of this encoding using a VAE. It appears that the quality of the results from this method are equal if not better than the quality of GauGAN. As far as my knowledge goes, there was no discussion comparing the two. Because the model pipeline looks simpler than the competition (all models except DALLâ€¢E 2 have to fine tune some version of transformers), it might train faster? The code has already been released publicly but at least by me wasnâ€™t tested yet because the torchvision requirement was removed only few days ago â€“ still, it should work OOTB!",1
"Traditional vision-based hand gesture recognition systems is limited under dark circumstances. In this paper, we build a hand gesture recognition system based on microwave transceiver and deep learning algorithm. A Doppler radar sensor with dual receiving channels at 5.8GHz is used to acquire a big database of hand gestures signals. The received hand gesture signals are then processed with time-frequency analysis. Based on these big databases of hand gesture, we propose a new machine learning architecture called deformable deep convolutional generative adversarial network. Experimental results show the new architecture can upgrade the recognition rate by 10% and the deformable kernel can reduce the testing time cost by 30%.",0
"In recent years, deep learning techniques have become increasingly popular in image processing tasks such as object recognition and classification due to their ability to learn complex features from large amounts of data. One such technique that has gained significant attention is generative adversarial networks (GANs), which consist of two competing neural networks: a generator network and a discriminator network.  In our work, we propose a novel approach for hand gesture recognition using deformable deep convolutional GANs. Our system utilizes microwave signals to capture motion and shape details of human hands, which can then be used to classify different gestures. By incorporating GANs into the recognition process, we aim to improve accuracy and robustness compared to traditional methods.  The key contribution of this research lies in introducing deformation into the existing GAN framework. We achieve this by modifying the generator network to generate not only images but also corresponding deformations that are consistent with the original input signal. This allows us to better model nonlinear relationships between the raw sensor data and the final output labels. Additionally, we introduce a new loss function that enforces cycle consistency constraints on both the generator and discriminator networks during training. This helps ensure that the generated images correspond to valid physical shapes and positions.  We evaluate our proposed method on benchmark datasets and demonstrate improved performance over state-of-the-art approaches. Overall, our findings suggest that the use of GANs together with deformable models can significantly enhance the accuracy of hand gesture recognition systems based on microwave sensors. This represents a promising direction for future research in this field.",1
"We present a method for reconstructing images viewed by observers based only on their eye movements. By exploring the relationships between gaze patterns and image stimuli, the ""What Are You Looking At?"" (WAYLA) system learns to synthesize photo-realistic images that are similar to the original pictures being viewed. The WAYLA approach is based on the Conditional Generative Adversarial Network (Conditional GAN) image-to-image translation technique of Isola et al. We consider two specific applications - the first, of reconstructing newspaper images from gaze heat maps, and the second, of detailed reconstruction of images containing only text. The newspaper image reconstruction process is divided into two image-to-image translation operations, the first mapping gaze heat maps into image segmentations, and the second mapping the generated segmentation into a newspaper image. We validate the performance of our approach using various evaluation metrics, along with human visual inspection. All results confirm the ability of our network to perform image generation tasks using eye tracking data.",0
"Eye movements can provide important insights into cognitive processes like attention, memory, and perception. Recent advances in computer vision have enabled automatic analysis of eye movements through tracking technologies like electrooculography (EOG), which measures changes in the electrical potentials on the cornea caused by eye movement. However, current methods lack generality as they rely on specific hardware setups that cannot capture fine-grained details of the eyes or handle occlusions. We propose WAYLA, a novel method that estimates accurate gaze direction and pupil size directly from raw webcam images without any specialized equipment. Our approach uses a deep convolutional neural network trained end-to-end and can generate high quality synthetic eyetracks for any image with known ground truth. Experimental results demonstrate significant improvements over existing methods in both accuracy and speed, making our model applicable across multiple domains such as psychology research, robotics, augmented reality, and visualization techniques. Future work involves expanding these applications further towards creating more lifelike virtual environments for immersive experiences and exploring the utility of pupillary response data in human behavior studies. This study highlights the effectiveness of using readily available technology to analyze complex behaviors related to human cognition.",1
"Learning low-dimensional representations of networks has proved effective in a variety of tasks such as node classification, link prediction and network visualization. Existing methods can effectively encode different structural properties into the representations, such as neighborhood connectivity patterns, global structural role similarities and other high-order proximities. However, except for objectives to capture network structural properties, most of them suffer from lack of additional constraints for enhancing the robustness of representations. In this paper, we aim to exploit the strengths of generative adversarial networks in capturing latent features, and investigate its contribution in learning stable and robust graph representations. Specifically, we propose an Adversarial Network Embedding (ANE) framework, which leverages the adversarial learning principle to regularize the representation learning. It consists of two components, i.e., a structure preserving component and an adversarial learning component. The former component aims to capture network structural properties, while the latter contributes to learning robust representations by matching the posterior distribution of the latent representations to given priors. As shown by the empirical results, our method is competitive with or superior to state-of-the-art approaches on benchmark network embedding tasks.",0
"Adversarial network embedding is a technique that involves using adversarial networks to learn more robust representations of data points. In traditional machine learning algorithms, data points are often embedded into low dimensional spaces such as Euclidean space or Grassmannian manifolds. However, these embeddings can be vulnerable to small perturbations in input data which can result in large changes in the output, known as ""adversarial examples"". Adversarial network embedding uses adversarial training to improve the robustness of these embeddings by encouraging them to be less sensitive to input perturbations while still retaining their discriminative power. This results in more accurate predictions on unseen test inputs and better generalization performance across different tasks. We evaluate our method on several benchmark datasets including MNIST, CIFAR-10, and ImageNet, and demonstrate improved robustness against a variety of attack methods compared to baseline models trained without adversarial network embedding. Our work has implications for improving the reliability of machine learning systems in real world applications where inputs may contain unwanted noise or malicious attacks.",1
"A major bottleneck for developing general reinforcement learning agents is determining rewards that will yield desirable behaviors under various circumstances. We introduce a general mechanism for automatically specifying meaningful behaviors from raw pixels. In particular, we train a generative adversarial network to produce short sub-goals represented through motion templates. We demonstrate that this approach generates visually meaningful behaviors in unknown environments with novel agents and describe how these motions can be used to train reinforcement learning agents.",0
"In this work we introduce Motion Generative Adversarial Networks (MGAN), a novel method that enables the transfer of behaviors observed in video recordings onto motion capture data sets. By leveraging adversarial training, MGANs learn to predict human body movements and generate realistic sequences that closely match the desired behavior. Our method is trained on large repositories of real human videos and captures both short-term dynamics, such as gestures, and longer-term effects such as changes in pose over time. We evaluate our approach on several benchmark datasets and demonstrate state-of-the art performance compared to prior methods. Additionally, we showcase applications where agents can exhibit behaviors mimicked from video footage of famous actors, dancers, and other subjects. Overall, our framework opens up new opportunities for creating virtual characters with diverse, expressive, and true-to-life actions.",1
"With the development of deep learning, supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks (CNNs). However, due to the limited amount of labeled data available, supervised learning is often difficult to carry out. Therefore, we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) to learn a representation using only unlabeled data. MARTA GANs consists of both a generative model $G$ and a discriminative model $D$. We treat $D$ as a feature extractor. To fit the complex properties of remote sensing data, we use a fusion layer to merge the mid-level and global features. $G$ can produce numerous images that are similar to the training data; therefore, $D$ can learn better representations of remotely sensed images using the training data provided by $G$. The classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-the-art methods.",0
"In recent years, Generative Adversarial Networks (GANs) have emerged as powerful tools for generating synthetic training data, which can be used to improve performance on tasks such as image classification. One promising area of application has been remote sensing, where high quality annotations are often difficult to obtain due to complex background features and subtle spectral differences between classes. However, existing approaches for utilizing GANs in remote sensing typically require large amounts of labeled training data, making them less practically applicable. This work addresses these limitations by introducing unsupervised representation learning with GANs for remote sensing image classification, referred to as Marta GANs. Marta GANs operate solely on small amounts of raw image pixels, without any class labels, producing representations that lead to improved image classification accuracy even compared against fully supervised models. We demonstrate this through extensive experiments using diverse real-world datasets including VHR urban scenes, Landsat imagery, and HRIs from RGB cameras. These results show great potential for Marta GANs to contribute towards efficient creation of novel training datasets in practice.",1
"The Visual Dialogue task requires an agent to engage in a conversation about an image with a human. It represents an extension of the Visual Question Answering task in that the agent needs to answer a question about an image, but it needs to do so in light of the previous dialogue that has taken place. The key challenge in Visual Dialogue is thus maintaining a consistent, and natural dialogue while continuing to answer questions correctly. We present a novel approach that combines Reinforcement Learning and Generative Adversarial Networks (GANs) to generate more human-like responses to questions. The GAN helps overcome the relative paucity of training data, and the tendency of the typical MLE-based approach to generate overly terse answers. Critically, the GAN is tightly integrated into the attention mechanism that generates human-interpretable reasons for each answer. This means that the discriminative model of the GAN has the task of assessing whether a candidate answer is generated by a human or not, given the provided reason. This is significant because it drives the generative model to produce high quality answers that are well supported by the associated reasoning. The method also generates the state-of-the-art results on the primary benchmark.",0
"This paper presents a method for generating natural language responses to visual input using adversarial learning. Our approach leverages two neural networks: one that generates text based on the image and another that evaluates the quality of the generated text. These networks are trained in opposition to each other, allowing them to continuously improve their performance. We demonstrate the effectiveness of our approach by applying it to a variety of datasets and show that it outperforms state-of-the-art methods across several metrics. Additionally, we provide analysis on how different components of our model contribute to its performance. Overall, our work represents a significant step forward in the field of visually-guided conversation generation.",1
"A Triangle Generative Adversarial Network ($\Delta$-GAN) is developed for semi-supervised cross-domain joint distribution matching, where the training data consists of samples from each domain, and supervision of domain correspondence is provided by only a few paired samples. $\Delta$-GAN consists of four neural networks, two generators and two discriminators. The generators are designed to learn the two-way conditional distributions between the two domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and two kinds of fake data pairs. The generators and discriminators are trained together using adversarial learning. Under mild assumptions, in theory the joint distributions characterized by the two generators concentrate to the data distribution. In experiments, three different kinds of domain pairs are considered, image-label, image-image and image-attribute pairs. Experiments on semi-supervised image classification, image-to-image translation and attribute-based image generation demonstrate the superiority of the proposed approach.",0
"This work presents Triangle Generative Adversarial Networks (TrianGAN), a new architecture that leverages triangle meshes as input data formats. TrianGAN consists of two subnetworks: a generator network and a discriminator network. The generator takes a random noise vector and maps it onto a valid triangle mesh using a convolutional neural network architecture. The resulting mesh serves as real data during training. Conversely, the discriminator evaluates the quality of generated meshes by comparing them against ground truth ones. We demonstrate on several benchmark datasets how our model outperforms previous state-of-the-art methods across multiple evaluation metrics such as FrÃ©chet Inception Distance (FID) and perceptual studies. Our approach paves the road towards high-quality generative models for complex 3D scenes.",1
"The Generative Adversarial Network (GAN) has achieved great success in generating realistic (real-valued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the applicability of GAN to text. We propose a framework for generating realistic text via adversarial training. We employ a long short-term memory network as generator, and a convolutional network as discriminator. Instead of using the standard objective of GAN, we propose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discrepancy metric. This eases adversarial training by alleviating the mode-collapsing problem. Our experiments show superior performance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.",0
"In recent years, deep generative models have revolutionized natural language processing by enabling text generation tasks such as machine translation, summarization, and content creation. Despite their effectiveness, these models can produce outputs that lack coherence and relevance, especially when generating responses to specific queries or prompts. This work addresses the challenge of ensuring that generated responses match relevant features in the input data. We propose Adversarial Feature Matching (AFM), a novel technique for training generative models that explicitly aligns generated outputs with targeted feature representations. Our approach utilizes a discriminator network trained on pairs of input-output examples, which learns to distinguish well-matched from poorly-matched instances. During training, our generator then optimizes against both likelihood objectives and adversarial matching objectives, effectively encouraging the model to attend to key aspects of the input while generating coherent and meaningful outputs. Extensive experiments demonstrate the superior performance of AFM over strong baseline methods across multiple natural language understanding and generation benchmark datasets.",1
"Contemporary benchmark methods for image inpainting are based on deep generative models and specifically leverage adversarial loss for yielding realistic reconstructions. However, these models cannot be directly applied on image/video sequences because of an intrinsic drawback- the reconstructions might be independently realistic, but, when visualized as a sequence, often lacks fidelity to the original uncorrupted sequence. The fundamental reason is that these methods try to find the best matching latent space representation near to natural image manifold without any explicit distance based loss. In this paper, we present a semantically conditioned Generative Adversarial Network (GAN) for sequence inpainting. The conditional information constrains the GAN to map a latent representation to a point in image manifold respecting the underlying pose and semantics of the scene. To the best of our knowledge, this is the first work which simultaneously addresses consistency and correctness of generative model based inpainting. We show that our generative model learns to disentangle pose and appearance information; this independence is exploited by our model to generate highly consistent reconstructions. The conditional information also aids the generator network in GAN to produce sharper images compared to the original GAN formulation. This helps in achieving more appealing inpainting performance. Though generic, our algorithm was targeted for inpainting on faces. When applied on CelebA and Youtube Faces datasets, the proposed method results in a significant improvement over the current benchmark, both in terms of quantitative evaluation (Peak Signal to Noise Ratio) and human visual scoring over diversified combinations of resolutions and deformations.",0
"This research describes the use of semantically guided generative adversarial networks (GANs) as a method for improving consistency and correctness in sequence inpainting applications. GANs have been shown to generate high quality images through their ability to learn complex statistical relationships between inputs and outputs, however the applicability of these models has so far been limited by issues related to training stability and sample diversity. By incorporating semantic guidance into the design of GANs, we show that we can mitigate some of these limitations and produce more coherent and consistent output sequences in both supervised and unsupervised settings. Our results demonstrate significant improvements over previous state of the art methods on both synthetic and real world data sets, highlighting the potential value of semantically guided GANs in image editing tasks where preserving visual fidelity and consistency is important. Overall our work offers new insights into the development of powerful deep learning architectures for challenging inpainting problems and opens up opportunities for future research in this rapidly evolving field.",1
"In this paper, we explore automated typeface generation through image style transfer which has shown great promise in natural image generation. Existing style transfer methods for natural images generally assume that the source and target images share similar high-frequency features. However, this assumption is no longer true in typeface transformation. Inspired by the recent advancement in Generative Adversarial Networks (GANs), we propose a Hierarchical Adversarial Network (HAN) for typeface transformation. The proposed HAN consists of two sub-networks: a transfer network and a hierarchical adversarial discriminator. The transfer network maps characters from one typeface to another. A unique characteristic of typefaces is that the same radicals may have quite different appearances in different characters even under the same typeface. Hence, a stage-decoder is employed by the transfer network to leverage multiple feature layers, aiming to capture both the global and local features. The hierarchical adversarial discriminator implicitly measures data discrepancy between the generated domain and the target domain. To leverage the complementary discriminating capability of different feature layers, a hierarchical structure is proposed for the discriminator. We have experimentally demonstrated that HAN is an effective framework for typeface transfer and characters restoration.",0
"Chinese typeface transformation involves generating new styles of typography based on existing designs while preserving their essential characteristics. This task can be challenging due to the complex nature of Chinese characters and the need to maintain their legibility and readability. In this paper, we propose a novel approach using hierarchical adversarial networks (HANs) to tackle the problem of Chinese typeface transformation. Our HAN model consists of two sub-networks: a content generator that produces new font styles, and a discriminator that evaluates the quality of generated fonts. We train our model using a combination of adversarial loss and reconstruction loss functions. Experimental results demonstrate that our method outperforms state-of-the-art algorithms in terms of visual fidelity, diversity, and applicability. Our work has potential applications in design industries such as publishing, advertising, and digital media.",1
"Recent advances in convolutional neural networks have shown promising results in 3D shape completion. But due to GPU memory limitations, these methods can only produce low-resolution outputs. To inpaint 3D models with semantic plausibility and contextual details, we introduce a hybrid framework that combines a 3D Encoder-Decoder Generative Adversarial Network (3D-ED-GAN) and a Long-term Recurrent Convolutional Network (LRCN). The 3D-ED-GAN is a 3D convolutional neural network trained with a generative adversarial paradigm to fill missing 3D data in low-resolution. LRCN adopts a recurrent neural network architecture to minimize GPU memory usage and incorporates an Encoder-Decoder pair into a Long Short-term Memory Network. By handling the 3D model as a sequence of 2D slices, LRCN transforms a coarse 3D shape into a more complete and higher resolution volume. While 3D-ED-GAN captures global contextual structure of the 3D shape, LRCN localizes the fine-grained details. Experimental results on both real-world and synthetic data show reconstructions from corrupted models result in complete and high-resolution 3D objects.",0
"This is an abstract of the paper ""Shape Inpainting Using 3D GAN And RCNN."" Without more specific details and guidelines on how you would like me to write the abstract, I cannot generate one that meets your needs. Please provide additional instructions or requirements if necessary.",1
"We propose a new active learning by query synthesis approach using Generative Adversarial Networks (GAN). Different from regular active learning, the resulting algorithm adaptively synthesizes training instances for querying to increase learning speed. We generate queries according to the uncertainty principle, but our idea can work with other active learning principles. We report results from various numerical experiments to demonstrate the effectiveness the proposed approach. In some settings, the proposed algorithm outperforms traditional pool-based approaches. To the best our knowledge, this is the first active learning work using GAN.",0
"Generative Adversarial Networks (GANs) have revolutionized machine learning by their ability to generate synthetic data that resembles real world distributions. However, training GANs on large datasets can be computationally expensive and requires huge amount of time and computational resources. To address this challenge, we present the use of active learning techniques in tandem with generative adversarial networks. This approach reduces the amount of labeled data required for training while at the same improving the quality of generated samples. We show through extensive experiments that our proposed method results in improved performance compared to existing methods. Additionally, we demonstrate the effectiveness of our approach using two different tasks namely image generation from sketches and text completion. Our study shows great potential in reducing the annotation effort involved in generating high quality synthetic data for downstream applications in computer vision, natural language processing, etc.",1
"We propose a new Integral Probability Metric (IPM) between distributions: the Sobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions for functions (critic) restricted to a Sobolev ball defined with respect to a dominant measure $\mu$. We show that the Sobolev IPM compares two distributions in high dimensions based on weighted conditional Cumulative Distribution Functions (CDF) of each coordinate on a leave one out basis. The Dominant measure $\mu$ plays a crucial role as it defines the support on which conditional CDFs are compared. Sobolev IPM can be seen as an extension of the one dimensional Von-Mises Cram\'er statistics to high dimensional distributions. We show how Sobolev IPM can be used to train Generative Adversarial Networks (GANs). We then exploit the intrinsic conditioning implied by Sobolev IPM in text generation. Finally we show that a variant of Sobolev GAN achieves competitive results in semi-supervised learning on CIFAR-10, thanks to the smoothness enforced on the critic by Sobolev GAN which relates to Laplacian regularization.",0
"Title: ""Sobolev GANs"" Abstract: GANs have become increasingly popular over recent years due to their ability to generate high quality data samples that can rival those generated by humans. However, one major issue facing GANs is their lack of control on the regularity and smoothness of generated images which leads to artifacts such as checkerboard patterns or blurriness. To address these issues we introduce a variant of Generative Adversarial Networks (GANs) called Sobolev GANs (SGANs). Our method uses Sobolev gradients which provide a more suitable regularization term compared to other methods used in traditional GANs. SGANs enforce higher frequency components to remain small while allowing large variations at coarser scales hence promoting piecewise smooth solutions. We evaluate our approach on several benchmark datasets demonstrating state-of-the-art performance across all metrics commonly used to measure image generation tasks including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR), structural similarity index metric (SSIM) and visual inspection of generated images. The code for reproducing our experiments will be made publicly available upon acceptance of the paper.",1
"Generative Adversarial Networks (GANs) are a powerful framework for deep generative modeling. Posed as a two-player minimax problem, GANs are typically trained end-to-end on real-valued data and can be used to train a generator of high-dimensional and realistic images. However, a major limitation of GANs is that training relies on passing gradients from the discriminator through the generator via back-propagation. This makes it fundamentally difficult to train GANs with discrete data, as generation in this case typically involves a non-differentiable function. These difficulties extend to the reinforcement learning setting when the action space is composed of discrete decisions. We address these issues by reframing the GAN framework so that the generator is no longer trained using gradients through the discriminator, but is instead trained using a learned critic in the actor-critic framework with a Temporal Difference (TD) objective. This is a natural fit for sequence modeling and we use it to achieve improvements on language modeling tasks over the standard Teacher-Forcing methods.",0
"Here comes your abstract!  I am unable to promise you that it will provide value if published as shown here. If you wish me to perform editing on the draft before sharing please ask. Otherwise, feel free to share widely!  Abstract: This paper presents ""ACtuAL"" (Actor Critic under Adversarial Learning), a new method for training deep reinforcement learning agents that learn from both adversarial examples and their corresponding normal data. ACtuAL uses two neural networks, one actor network and one critic network, each trained using both types of data. By doing so, the agent learns to make decisions based on robust features of the environment rather than overfitting to specific instances. We evaluate our approach on several benchmark tasks and show that it outperforms traditional methods by achieving higher average rewards while being more resilient to input perturbations. Our results demonstrate the effectiveness of ACtuAL in enabling robust decision making in complex environments subject to noise and uncertainty.",1
"Generative Adversarial Networks (GAN) have attracted much research attention recently, leading to impressive results for natural image generation. However, to date little success was observed in using GAN generated images for improving classification tasks. Here we attempt to explore, in the context of car license plate recognition, whether it is possible to generate synthetic training data using GAN to improve recognition accuracy. With a carefully-designed pipeline, we show that the answer is affirmative. First, a large-scale image set is generated using the generator of GAN, without manual annotation. Then, these images are fed to a deep convolutional neural network (DCNN) followed by a bidirectional recurrent neural network (BRNN) with long short-term memory (LSTM), which performs the feature learning and sequence labelling. Finally, the pre-trained model is fine-tuned on real images. Our experimental results on a few data sets demonstrate the effectiveness of using GAN images: an improvement of 7.5% over a strong baseline with moderate-sized real data being available. We show that the proposed framework achieves competitive recognition accuracy on challenging test datasets. We also leverage the depthwise separate convolution to construct a lightweight convolutional RNN, which is about half size and 2x faster on CPU. Combining this framework and the proposed pipeline, we make progress in performing accurate recognition on mobile and embedded devices.",0
"Adversarial examples have emerged as a major challenge to machine learning systems that rely on deep neural networks, particularly computer vision models. These examples can trick even high performing models into making incorrect predictions by exploiting weaknesses in their decision-making process. In this paper, we explore how adversarial generation techniques can be used to enhance the robustness and accuracy of machine learning algorithms designed for moving vehicle license plate recognition (MVLPR). We introduce a novel framework for generating synthetic training data based on real MVLPR datasets, which can then be fed into a model during training to improve performance under challenging conditions such as varying lighting, weather, and background clutter. Our experiments demonstrate that our method significantly improves both recall and F1 score compared to state-of-the-art baseline methods, while also reducing overfitting and increasing model generalization. This work highlights the potential impact of adversarial example research on applied machine learning problems outside of academia and has implications for advancing safety and security applications like automatic number plate recognition in law enforcement.",1
"Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood. We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs. Within this framework, we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks. The resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching, or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator, the Bayesian GAN avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN, Wasserstein GANs, and DCGAN ensembles.",0
"In recent years, generative adversarial networks (GANs) have emerged as one of the most powerful tools for generating realistic artificial data samples across different domains such as image generation, video synthesis, natural language processing, etc. However, training GANs remains challenging due to their instability and sensitivity to hyperparameters. To address these issues, we propose Bayesian GAN, a novel approach that utilizes probabilistic programming techniques to learn uncertainty distributions over model parameters during training. This allows us to calibrate our models using Bayesian inference, leading to more stable training dynamics and improved generalization performance. We demonstrate the effectiveness of our method on several benchmark datasets, showing superior results compared to state-of-the-art baselines. Our work paves the way for developing reliable and accurate generative models using probabilistic methods.",1
"Generative Adversarial Networks (GANs), as a framework for estimating generative models via an adversarial process, have attracted huge attention and have proven to be powerful in a variety of tasks. However, training GANs is well known for being delicate and unstable, partially caused by its sig- moid cross entropy loss function for the discriminator. To overcome such a problem, many researchers directed their attention on various ways to measure how close the model distribution and real distribution are and have applied dif- ferent metrics as their objective functions. In this paper, we propose a novel framework to train GANs based on distance metric learning and we call it Metric Learning-based Gener- ative Adversarial Network (MLGAN). The discriminator of MLGANs can dynamically learn an appropriate metric, rather than a static one, to measure the distance between generated samples and real samples. Afterwards, MLGANs update the generator under the newly learned metric. We evaluate our ap- proach on several representative datasets and the experimen- tal results demonstrate that MLGANs can achieve superior performance compared with several existing state-of-the-art approaches. We also empirically show that MLGANs could increase the stability of training GANs.",0
"In recent years, generative adversarial networks (GANs) have emerged as a powerful tool for generating high quality synthetic data. However, training GANs remains challenging due to issues such as instability and difficulty in evaluating the generated samples. To address these problems, we propose a novel metric learning approach based on the structured similarity index (SSIM), which allows us to train more stable and better performing GANs. Our method focuses on improving both inter-sample comparison and feature representation by incorporating SSIM into the generator network. We show that our metric learning framework significantly enhances the performance of GANs across multiple datasets, including MNIST, CelebA, LSUN Bedroom, and ImageNet. Additionally, experiments demonstrate that our proposed method leads to improvements over state-of-the-art techniques, making it a valuable contribution to the field of deep learning.",1
"Localization technology is important for the development of indoor location-based services (LBS). Global Positioning System (GPS) becomes invalid in indoor environments due to the non-line-of-sight issue, so it is urgent to develop a real-time high-accuracy localization approach for smartphones. However, accurate localization is challenging due to issues such as real-time response requirements, limited fingerprint samples and mobile device storage. To address these problems, we propose a novel deep learning architecture: Tensor-Generative Adversarial Network (TGAN).   We first introduce a transform-based 3D tensor to model fingerprint samples. Instead of those passive methods that construct a fingerprint database as a prior, our model applies artificial neural network with deep learning to train network classifiers and then gives out estimations. Then we propose a novel tensor-based super-resolution scheme using the generative adversarial network (GAN) that adopts sparse coding as the generator network and a residual learning network as the discriminator. Further, we analyze the performance of tensor-GAN and implement a trace-based localization experiment, which achieves better performance. Compared to existing methods for smartphones indoor positioning, that are energy-consuming and high demands on devices, TGAN can give out an improved solution in localization accuracy, response time and implementation complexity.",0
"This is a technical abstract that summarizes our work on using tensor-generative adversarial networks (TGANs) with two-dimensional sparse coding to perform real-time indoor localization tasks. In recent years, there has been increasing interest in developing novel deep learning models that can effectively process high-resolution data while offering improved accuracy over traditional convolutional neural networks (CNNs). However, most existing architectures either require a large amount of computational resources or sacrifice model complexity to achieve faster inference times. Our goal was to design an efficient architecture capable of generating robust features from raw sensor readings without sacrificing performance. We propose the use of sparsity constraints during training, which enables significant reduction in computational cost compared to other generative models like GANs. Additionally, we show how our framework outperforms state-of-the-art CNNs across multiple datasets, demonstrating its effectiveness as a tool for real-world applications such as indoor positioning. Finally, we provide extensive analysis of key components within the proposed system to guide future research in computer vision and robotics fields.",1
"Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f-divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer across several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.",0
"Title: ""Stabilizing Training of Generative Adversarial Networks through Regularization""  Abstract: Generative Adversarial Networks (GANs) have revolutionized the field of computer vision by their ability to generate high quality realistic images. However, training GANs remains a challenging task due to instability issues such as mode collapse and vanishing gradients. In this work we address these problems by introducing regularization techniques into the adversarial loss function. Our method involves adding a weight decay term to the discriminator network during training, which helps reduce overfitting and improves stability. We evaluate our approach on several benchmark datasets and demonstrate significant improvements in both quantitative and qualitative measures compared to previous state-of-the-art methods. Additionally, we provide insights into why regularization works in stabilizing GAN training, making it possible to use fewer neurons while maintaining performance.",1
"Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.",0
"Abstract --- VEEGAN (Variational Earth Moverâ€™s Distance-based Expectation over Energy Gradients) is a novel training algorithm that significantly reduces mode collapse in generative adversarial networks (GANs). By introducing implicit variational learning into GAN optimization, VEEGAN effectively regularizes the latent space through the use of energy gradients and the earth moverâ€™s distance. This allows for more stable and diverse generation across multiple modes. Results from comprehensive experiments show significant improvements over state-of-the-art methods on both quantitative metrics such as Inception Score and FID score, and qualitative evaluation including visual inspection of generated samples. Our method has the potential to bring forth new possibilities in computer vision tasks, including image synthesis, super resolution, and domain translation. We hope our work opens up new directions for future research on improving stability and diversity in GAN models.",1
"Very High Spatial Resolution (VHSR) large-scale SAR image databases are still an unresolved issue in the Remote Sensing field. In this work, we propose such a dataset and use it to explore patch-based classification in urban and periurban areas, considering 7 distinct semantic classes. In this context, we investigate the accuracy of large CNN classification models and pre-trained networks for SAR imaging systems. Furthermore, we propose a Generative Adversarial Network (GAN) for SAR image generation and test, whether the synthetic data can actually improve classification accuracy.",0
"Title: ""Generating big data for image classification using GANs"" This paper presents a novel method for generating large amounts of synthetic image data to improve image classifiation systems based on Synthetic Aperture Radar (SAR) images. We use a generative adversarial network approach, training two deep neural networks in an adversarial setting where one generates synthetic data and the other discriminates real from fake data. Our experimental results show that our generated data can significantly increase the accuracy of state-of-the-art classifiers while reducing their need for labeled data. Additionally, we demonstrate the general applicability of our method by applying it to different types of image datasets and comparing the performance gains achieved with existing approaches. Overall, our work offers a promising new direction for enabling more accurate and efficient image classification through artificially augmented training sets.",1
"Generative Adversarial Networks (GANs) were intuitively and attractively explained under the perspective of game theory, wherein two involving parties are a discriminator and a generator. In this game, the task of the discriminator is to discriminate the real and generated (i.e., fake) data, whilst the task of the generator is to generate the fake data that maximally confuses the discriminator. In this paper, we propose a new viewpoint for GANs, which is termed as the minimizing general loss viewpoint. This viewpoint shows a connection between the general loss of a classification problem regarding a convex loss function and a f-divergence between the true and fake data distributions. Mathematically, we proposed a setting for the classification problem of the true and fake data, wherein we can prove that the general loss of this classification problem is exactly the negative f-divergence for a certain convex function f. This allows us to interpret the problem of learning the generator for dismissing the f-divergence between the true and fake data distributions as that of maximizing the general loss which is equivalent to the min-max problem in GAN if the Logistic loss is used in the classification problem. However, this viewpoint strengthens GANs in two ways. First, it allows us to employ any convex loss function for the discriminator. Second, it suggests that rather than limiting ourselves in NN-based discriminators, we can alternatively utilize other powerful families. Bearing this viewpoint, we then propose using the kernel-based family for discriminators. This family has two appealing features: i) a powerful capacity in classifying non-linear nature data and ii) being convex in the feature space. Using the convexity of this family, we can further develop Fenchel duality to equivalently transform the max-min problem to the max-max dual problem.",0
"Generative Adversarial Networks (GAN) have revolutionized the field of computer vision by generating images that rival real-world photographs. However, training GANs remains challenging due to instability issues such as mode collapse, vanishing gradients, and unstable dynamics. These problems arise because the GAN architecture pits two neural networks against each other in a minimax game, where one network generates data and the other discriminates whether it is real or fake. In this work, we propose using Knowledge Graph Augmented Neural Networks (KGAN), which incorporate external knowledge into the generators to break the minimax game. This enables us to improve stability, coherency, and visual quality of generated images without relying on architectural tweaks or heuristics. Our experiments demonstrate significant improvements over traditional GANs across multiple benchmark datasets. By leveraging external knowledge through semantic embeddings, our approach sets a new state-of-the-art for image generation using GANs. Overall, this work offers valuable insights into how to effectively use prior knowledge in deep learning models, opening up exciting opportunities for future research.",1
"Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.",0
"This paper presents a novel framework called hierarchical implicit models (HIM) for likelihood-free variational inference. HIM represents probabilistic distributions as linear combinations of simple functions that can have different dependencies across variables and levels. We propose two methods for approximate Bayesian computation: Monte Carlo importance resampling (MCIR), which is based on sampling from alternative distributions, and deterministic VIMCO, which uses optimization instead of sampling. Both methods use variational approximation and perform likelihood-free posterior simulation without direct model evaluation. Extensive experiments show that HIM achieves state-of-the-art results in predictive log-likelihoods for several benchmark problems. Our results demonstrate HIMâ€™s effectiveness and flexibility compared to other likelihood-free methods.",1
"Generative Adversarial Networks (GANs) are powerful models for learning complex distributions. Stable training of GANs has been addressed in many recent works which explore different metrics between distributions. In this paper we introduce Fisher GAN which fits within the Integral Probability Metrics (IPM) framework for training GANs. Fisher GAN defines a critic with a data dependent constraint on its second order moments. We show in this paper that Fisher GAN allows for stable and time efficient training that does not compromise the capacity of the critic, and does not need data independent constraints such as weight clipping. We analyze our Fisher IPM theoretically and provide an algorithm based on Augmented Lagrangian for Fisher GAN. We validate our claims on both image sample generation and semi-supervised classification using Fisher GAN.",0
"In recent years, Generative Adversarial Networks (GANs) have emerged as a powerful tool for generating high-quality images, videos, audio samples, and other forms of data that resemble real-world examples. However, one issue with traditional GAN architectures is their tendency to produce unrealistic and overly sharp outputs at higher resolutions. This problem can result in blurry or pixelated artifacts in generated samples, which detract from their overall quality. To address this limitation, we propose a novel architecture called Fisher GAN (Fisher Generative Adversarial Network), which incorporates spatial regularization techniques into the training process. By doing so, our method effectively reduces the ""fishiness"" or irregularity in generated samples, leading to more cohesive and natural results at higher resolutions. Our experiments demonstrate the effectiveness of Fisher GAN in producing visually pleasing and detailed synthetic images across various domains, outperforming current state-of-the-art methods.",1
"Semi-supervised learning methods based on generative adversarial networks (GANs) obtained strong empirical results, but it is not clear 1) how the discriminator benefits from joint training with a generator, and 2) why good semi-supervised classification performance and a good generator cannot be obtained at the same time. Theoretically, we show that given the discriminator objective, good semisupervised learning indeed requires a bad generator, and propose the definition of a preferred generator. Empirically, we derive a novel formulation based on our analysis that substantially improves over feature matching GANs, obtaining state-of-the-art results on multiple benchmark datasets.",0
"In recent years, semi-supervised learning has emerged as a promising approach to address the problem of limited labeled data availability in machine learning tasks. One popular technique used in semi-supervised learning is Generative Adversarial Networks (GAN), which generate synthetic training examples using unlabeled data. However, generating high quality synthetic examples remains challenging, particularly for complex image datasets such as ImageNet. This paper presents a new semi-supervised learning framework called ""Good Semi-supervised Learning that Requires a Bad GAN"" (GoSLRB). Our proposed method leverages a subpar GAN generator to produce low quality synthetic images that contain semantic noise, but still provide useful supervision signals when combined with the existing labeled dataset. We show through extensive experiments on multiple benchmark datasets that our GoSLRB method outperforms state-of-the-art semi-supervised learning approaches while requiring only a poorly performing GAN model to achieve these results. These findings demonstrate the feasibility of harnessing noisy synthetic examples generated by a bad GAN, effectively improving performance and expanding application scenarios for semi-supervised learning beyond large-scale datasets like ImageNet. Overall, our work highlights the potential benefits of intentionally degraded generative models for effective SSL without relying on overpowered neural networks.",1
"We study the problem of conditional generative modeling based on designated semantics or structures. Existing models that build conditional generators either require massive labeled instances as supervision or are unable to accurately control the semantics of generated samples. We propose structured generative adversarial networks (SGANs) for semi-supervised conditional generative modeling. SGAN assumes the data x is generated conditioned on two independent latent variables: y that encodes the designated semantics, and z that contains other factors of variation. To ensure disentangled semantics in y and z, SGAN builds two collaborative games in the hidden space to minimize the reconstruction error of y and z, respectively. Training SGAN also involves solving two adversarial games that have their equilibrium concentrating at the true joint data distributions p(x, z) and p(x, y), avoiding distributing the probability mass diffusely over data space that MLE-based methods may suffer. We assess SGAN by evaluating its trained networks, and its performance on downstream tasks. We show that SGAN delivers a highly controllable generator, and disentangled representations; it also establishes start-of-the-art results across multiple datasets when applied for semi-supervised image classification (1.27%, 5.73%, 17.26% error rates on MNIST, SVHN and CIFAR-10 using 50, 1000 and 4000 labels, respectively). Benefiting from the separate modeling of y and z, SGAN can generate images with high visual quality and strictly following the designated semantic, and can be extended to a wide spectrum of applications, such as style transfer.",0
"Abstract: In recent years, generative models have shown great promise in generating realistic data such as images, videos, text, audio and more by optimizing over objectives designed to capture their likelihood under some probability distribution that govern the training data. Despite these successes, many existing methods suffer from instability during optimization leading to difficult hyperparameter tuning and slow convergence speed. Additionally, most current GAN architectures consist of unstructured latent spaces which hampers interpretability and application for downstream tasks. Our goal in this work is to address both limitations by proposing novel structured generative adversarial networks (SGAN). Specifically, we present two approaches based on implicit regularization using matrix factorizations with provably better generalization bounds compared to prior works. Furthermore, we show how SGANs can provide new insights into high resolution image generation, where previous architectures struggled due to instability issues. We believe our results open up exciting possibilities in terms of scalability towards larger datasets while allowing for tractable inference via efficient posterior sampling techniques. Code will be made publicly available upon publication.",1
"In an era when big data are becoming the norm, there is less concern with the quantity but more with the quality and completeness of the data. In many disciplines, data are collected from heterogeneous sources, resulting in multi-view or multi-modal datasets. The missing data problem has been challenging to address in multi-view data analysis. Especially, when certain samples miss an entire view of data, it creates the missing view problem. Classic multiple imputations or matrix completion methods are hardly effective here when no information can be based on in the specific view to impute data for such samples. The commonly-used simple method of removing samples with a missing view can dramatically reduce sample size, thus diminishing the statistical power of a subsequent analysis. In this paper, we propose a novel approach for view imputation via generative adversarial networks (GANs), which we name by VIGAN. This approach first treats each view as a separate domain and identifies domain-to-domain mappings via a GAN using randomly-sampled data from each view, and then employs a multi-modal denoising autoencoder (DAE) to reconstruct the missing view from the GAN outputs based on paired data across the views. Then, by optimizing the GAN and DAE jointly, our model enables the knowledge integration for domain mappings and view correspondences to effectively recover the missing view. Empirical results on benchmark datasets validate the VIGAN approach by comparing against the state of the art. The evaluation of VIGAN in a genetic study of substance use disorders further proves the effectiveness and usability of this approach in life science.",0
Abstract: VIGAN uses GANs to impute missing views.,1
"It is commonly agreed that the use of relevant invariances as a good statistical bias is important in machine-learning. However, most approaches that explicitly incorporate invariances into a model architecture only make use of very simple transformations, such as translations and rotations. Hence, there is a need for methods to model and extract richer transformations that capture much higher-level invariances. To that end, we introduce a tool allowing to parametrize the set of filters of a trained convolutional neural network with the latent space of a generative adversarial network. We then show that the method can capture highly non-linear invariances of the data by visualizing their effect in the data space.",0
"In recent years, Generative Adversarial Networks (GANs) have been widely used in computer vision tasks such as image generation, image super resolution, and semantic segmentation due to their ability to generate high quality realistic data samples that can be used in training convolutional neural networks (CNN). However, there has been limited work on using GANs to parameterize traditional CNNs for better performance in object recognition tasks. This study addresses this gap by exploring different architectures of GANs applied to the task of object recognition using a pre-trained VGG16 CNN model and proposes a novel architecture to achieve state-of-the-art results in CIFAR-10 dataset. Experimental results show that our proposed approach outperforms previous works that use GANs to improve object classification accuracy on CIFAR-10 dataset. Our method enables more efficient training of deep convolutional neural networks while improving generalization to unseen testing data. Additionally, we provide analysis of qualitative visualizations of the generated images from the generator network which further validate the effectiveness of our approach. To summarise, this study presents new insights into how GANs can be effectively utilized to optimize conventional CNNs for improved object recognition performance on challenging datasets.",1
"This paper raises an implicit manifold learning perspective in Generative Adversarial Networks (GANs), by studying how the support of the learned distribution, modelled as a submanifold $\mathcal{M}_{\theta}$, perfectly match with $\mathcal{M}_{r}$, the support of the real data distribution. We show that optimizing Jensen-Shannon divergence forces $\mathcal{M}_{\theta}$ to perfectly match with $\mathcal{M}_{r}$, while optimizing Wasserstein distance does not. On the other hand, by comparing the gradients of the Jensen-Shannon divergence and the Wasserstein distances ($W_1$ and $W_2^2$) in their primal forms, we conjecture that Wasserstein $W_2^2$ may enjoy desirable properties such as reduced mode collapse. It is therefore interesting to design new distances that inherit the best from both distances.",0
"An unprecedented amount of digital content has been generated recently, which demands automated methods for organizing and analyzing such data. In this study, we present a new method called implicit manifold learning (ImpML) that can efficiently generate meaningful representations from large datasets. We apply our approach to generative adversarial networks (GANs), which have shown remarkable successes in image generation tasks but suffer from instability during training due to their reliance on high capacity models and delicate hyperparameter tuning. By exploiting the intrinsic structure hidden in GANs via ImpML, we demonstrate significant improvements across multiple benchmark datasets, including MNIST, CIFAR-10, STL-10, LSUN Church, and ImageNet. Our approach achieves state-of-the-art performance while reducing computational cost compared to prior arts. This study shows promising results in using our technique as pretraining before fine-tuning, where further enhancements could potentially occur.",1
"This paper describes a new approach for training generative adversarial networks (GAN) to understand the detailed 3D shape of objects. While GANs have been used in this domain previously, they are notoriously hard to train, especially for the complex joint data distribution over 3D objects of many categories and orientations. Our method extends previous work by employing the Wasserstein distance normalized with gradient penalization as a training objective. This enables improved generation from the joint object shape distribution. Our system can also reconstruct 3D shape from 2D images and perform shape completion from occluded 2.5D range scans. We achieve notable quantitative improvements in comparison to existing baselines",0
"Title: ""Enhanced adversarial systems for 3D object generation and reconstruction""  Abstract: This research presents an improved approach for generating high-quality 3D objects using advanced adversarial training methods. We introduce new techniques that effectively exploit the strengths of both generative and discriminative models to achieve more accurate results. Our method combines unsupervised learning with semi-supervised fine-tuning to enhance stability during optimization. Additionally, we propose novel regularization techniques designed specifically to address problems associated with traditional adversarial training such as mode collapse and instability. Experimental results showcase the effectiveness of our system compared to other state-of-the-art approaches, demonstrating better performance in terms of visual quality and accuracy. Moreover, our framework allows for easy integration into existing pipelines without requiring substantial modifications. Overall, these findings make our method a valuable tool for future work in 3D object generation and reconstruction.",1
"Reliability and accuracy of iris biometric modality has prompted its large-scale deployment for critical applications such as border control and national ID projects. The extensive growth of iris recognition systems has raised apprehensions about susceptibility of these systems to various attacks. In the past, researchers have examined the impact of various iris presentation attacks such as textured contact lenses and print attacks. In this research, we present a novel presentation attack using deep learning based synthetic iris generation. Utilizing the generative capability of deep convolutional generative adversarial networks and iris quality metrics, we propose a new framework, named as iDCGAN (iris deep convolutional generative adversarial network) for generating realistic appearing synthetic iris images. We demonstrate the effect of these synthetically generated iris images as presentation attack on iris recognition by using a commercial system. The state-of-the-art presentation attack detection framework, DESIST is utilized to analyze if it can discriminate these synthetically generated iris images from real images. The experimental results illustrate that mitigating the proposed synthetic presentation attack is of paramount importance.",0
"This paper presents a new method for synthesizing iris presentation attacks using deep convolutional generative adversarial networks (iDCGAN). The proposed approach leverages recent advances in computer vision and machine learning to create high quality, realistic fake irises that can defeat state-of-the-art biometric systems. Our results demonstrate the effectiveness of our algorithm in generating convincing iris images that are difficult to distinguish from authentic ones under various lighting conditions. We believe that this work has important implications for the security community, as it highlights the potential vulnerabilities of relying solely on biometrics for authentication purposes. Furthermore, we hope that our research will inspire future efforts in developing more advanced anti-spoofing techniques to mitigate these types of attacks.",1
"Data augmentation is an essential part of the training process applied to deep learning models. The motivation is that a robust training process for deep learning models depends on large annotated datasets, which are expensive to be acquired, stored and processed. Therefore a reasonable alternative is to be able to automatically generate new annotated training samples using a process known as data augmentation. The dominant data augmentation approach in the field assumes that new training samples can be obtained via random geometric or appearance transformations applied to annotated training samples, but this is a strong assumption because it is unclear if this is a reliable generative model for producing new training samples. In this paper, we provide a novel Bayesian formulation to data augmentation, where new annotated training points are treated as missing variables and generated based on the distribution learned from the training set. For learning, we introduce a theoretically sound algorithm --- generalised Monte Carlo expectation maximisation, and demonstrate one possible implementation via an extension of the Generative Adversarial Network (GAN). Classification results on MNIST, CIFAR-10 and CIFAR-100 show the better performance of our proposed method compared to the current dominant data augmentation approach mentioned above --- the results also show that our approach produces better classification results than similar GAN models.",0
"In many machine learning applications there exists large amounts of data that have missing values. This can lead to biases in parameter estimation, which makes model predictions less accurate. One approach used to address these biases is called imputation using matrix factorization methods such as singular value decomposition (SVD) and nonnegative Matrix Factorization (NMF). These approaches assume knowledge of underlying low dimensional structure but suffer from scaling issues when dealing with high-dimensional problems like those arising in recommender systems and natural language processing tasks. Here we propose a novel probabilistic approach based on a latent variable model which allows us to capture complex relationships among features without assuming any specific low dimensional structure. Our method combines Gaussian Process Latent Variable Modeling with Variational inference to obtain a tractable posterior distribution over the missing values which can then be incorporated into standard Bayesian deep learning frameworks. We show through experiments on benchmark datasets that our framework outperforms state-of-the-art methods across multiple domains including collaborative filtering, sentiment analysis and content generation by achieving better predictive performance and calibration.",1
"While the visualization of statistical data tends to a mature technology, the visualization of textual data is still in its infancy, especially for the artistic text. Due to the fact that visualization of artistic text is valuable and attractive in both art and information science, we attempt to realize this tentative idea in this article. We propose the Generative Adversarial Network based Artistic Textual Visualization (GAN-ATV) which can create paintings after analyzing the semantic content of existing poems. Our GAN-ATV consists of two main sections: natural language analysis section and visual information synthesis section. In natural language analysis section, we use Bag-of-Word (BoW) feature descriptors and a two-layer network to mine and analyze the high-level semantic information from poems. In visual information synthesis section, we design a cross-modal semantic understanding module and integrate it with Generative Adversarial Network (GAN) to create paintings, whose content are corresponding to the original poems. Moreover, in order to train our GAN-ATV and verify its performance, we establish a cross-modal artistic dataset named ""Cross-Art"". In the Cross-Art dataset, there are six topics and each topic has their corresponding paintings and poems. The experimental results on Cross-Art dataset are shown in this article.",0
"In recent years, generative adversarial networks (GANs) have emerged as powerful tools for generating realistic images from textual descriptions. However, most current methods still face challenges such as inconsistencies, poor performance, and lack of interpretability. This paper proposes a novel approach that addresses these issues by introducing three key components: attention mechanisms, semantic mapping, and style transfer techniques. The proposed method can effectively visualize artistic texts, while providing interpretable results. We evaluate our model using several metrics and showcase a variety of applications including image generation from poetry, song lyrics, and abstract concepts. Our work significantly advances the state-of-the-art in text-to-image synthesis and has potential applications in fields ranging from entertainment to education and research.",1
"Since the creation of Generative Adversarial Networks (GANs), much work has been done to improve their training stability, their generated image quality, their range of application but nearly none of them explored their self-training potential. Self-training has been used before the advent of deep learning in order to allow training on limited labelled training data and has shown impressive results in semi-supervised learning. In this work, we combine these two ideas and make GANs self-trainable for semi-supervised learning tasks by exploiting their infinite data generation potential. Results show that using even the simplest form of self-training yields an improvement. We also show results for a more complex self-training scheme that performs at least as well as the basic self-training scheme but with significantly less data augmentation.",0
"Title: ""A Self-Training Method for Semi-Supervised GANs"" Authors: Adam Cozad, Brendan Tao, Alexander Arandiga, et al.  Abstract: In recent years, Generative Adversarial Networks (GANs) have shown promising results in generating high-quality images that closely resemble real data distributions. However, training these models can still be challenging due to issues such as instability and convergence problems. To address these issues, we propose a self-training method for semi-supervised learning that allows us to utilize unlabeled data during training while reducing the risk of overfitting. Our proposed method starts by pre-training the discriminator on a large dataset before fine-tuning both the generator and discriminator using labeled data. Then, we introduce a self-training step where the model generates new samples and uses them along with the original labeled set to further train itself. We evaluate our approach on multiple datasets including MNIST, SVHN, and STL-10 and show significant improvements compared to baseline methods. Our experiments demonstrate that our self-training method achieves state-of-the-art performance in image generation tasks while requiring fewer labelled examples. This work presents an important contribution towards making GANs more reliable and efficient tools for machine learning researchers and practitioners.",1
"This paper reports on WaterGAN, a generative adversarial network (GAN) for generating realistic underwater images from in-air image and depth pairings in an unsupervised pipeline used for color correction of monocular underwater images. Cameras onboard autonomous and remotely operated vehicles can capture high resolution images to map the seafloor, however, underwater image formation is subject to the complex process of light propagation through the water column. The raw images retrieved are characteristically different than images taken in air due to effects such as absorption and scattering, which cause attenuation of light at different rates for different wavelengths. While this physical process is well described theoretically, the model depends on many parameters intrinsic to the water column as well as the objects in the scene. These factors make recovery of these parameters difficult without simplifying assumptions or field calibration, hence, restoration of underwater images is a non-trivial problem. Deep learning has demonstrated great success in modeling complex nonlinear systems but requires a large amount of training data, which is difficult to compile in deep sea environments. Using WaterGAN, we generate a large training dataset of paired imagery, both raw underwater and true color in-air, as well as depth data. This data serves as input to a novel end-to-end network for color correction of monocular underwater images. Due to the depth-dependent water column effects inherent to underwater environments, we show that our end-to-end network implicitly learns a coarse depth estimate of the underwater scene from monocular underwater images. Our proposed pipeline is validated with testing on real data collected from both a pure water tank and from underwater surveys in field testing. Source code is made publicly available with sample datasets and pretrained models.",0
"This research focuses on developing an unsupervised generative network for real-time color correction of monocular underwater images using deep learning. Traditionally, correcting colors in underwater imagery has been challenged by light attenuation, absorption, and reflections due to differences in water density and composition. Existing approaches either require manual intervention or rely heavily on supervision which often leads to limited performance. To address these limitations, we propose an unsupervised method that leverages CycleGANs, a state-of-the-art image-to-image translation framework, combined with domain adaptation techniques from Wasserstein GAN (WGAN). We use paired data from identical land scenes above water and their corresponding underwater views to train our model without relying on any labels. We then deploy the trained model into smartphones wherein the algorithm can perform real-time color corrections based only on single-view underwater photographs. Experiments show significant improvement over several baseline methods, producing enhanced visual quality while remaining efficient. Our method provides meaningful benefits to applications such as underwater photography, marine science exploration, and virtual reality dive simulation, among others.",1
"We examine the problem of learning mappings from state to state, suitable for use in a model-based reinforcement-learning setting, that simultaneously generalize to novel states and can capture stochastic transitions. We show that currently popular generative adversarial networks struggle to learn these stochastic transition models but a modification to their loss functions results in a powerful learning algorithm for this class of problems.",0
"An Abstract should provide a brief overview of the contents and results of your paper. Here is an example of an abstract that may meet these requirements:   ""We present algorithms for learning approximate stochastic transition models from data. These models capture both deterministic and probabilistic relationships between states. Our approach builds upon recent advances in neural networks as probability machines by framing model learning as density estimation across state transitions. We empirically demonstrate the efficacy of our method on several large scale real world datasets.""",1
Performance of data-driven network for tumor classification varies with stain-style of histopathological images. This article proposes the stain-style transfer (SST) model based on conditional generative adversarial networks (GANs) which is to learn not only the certain color distribution but also the corresponding histopathological pattern. Our model considers feature-preserving loss in addition to well-known GAN loss. Consequently our model does not only transfers initial stain-styles to the desired one but also prevent the degradation of tumor classifier on transferred images. The model is examined using the CAMELYON16 dataset.,0
"Incorporating data augmentation into medical imaging has become increasingly important as large amounts of images need to be processed efficiently by radiologists. Previous work on image generation has focused mostly on generating new images from scratch based on existing training sets. However, transfer learning methods have shown potential for improving accuracy even if only a limited dataset exists for model training. This study explores neural stain transfer style learning (NSSLT), which combines techniques such as generative adversarial networks (GAN) and transfer learning, to generate new synthetic histopathology slide images that match real-world slides of cancerous samples taken from patients. Experimental results indicate a significant increase in tumor area detection sensitivity compared to traditional color deconvolution methods used in digital pathology systems. Additionally, subjective evaluation showed the generated images were more visually similar to corresponding real world slides than those produced via other common data augmentation techniques. Overall, NSSLT has demonstrated promising results and may serve as a useful tool for improving medical diagnosis accuracy in future applications.",1
"In this paper, we propose a generative model which learns the relationship between language and human action in order to generate a human action sequence given a sentence describing human behavior. The proposed generative model is a generative adversarial network (GAN), which is based on the sequence to sequence (SEQ2SEQ) model. Using the proposed generative network, we can synthesize various actions for a robot or a virtual agent using a text encoder recurrent neural network (RNN) and an action decoder RNN. The proposed generative network is trained from 29,770 pairs of actions and sentence annotations extracted from MSR-Video-to-Text (MSR-VTT), a large-scale video dataset. We demonstrate that the network can generate human-like actions which can be transferred to a Baxter robot, such that the robot performs an action based on a provided sentence. Results show that the proposed generative network correctly models the relationship between language and action and can generate a diverse set of actions from the same sentence.",0
"In this paper we present an approach that allows text to directly control action in complex task domains. Our method uses generative adversarial networks (GANs) to generate sequences of actions given text instructions. By conditioning on the text input, our network can learn a mapping from natural language into executable code. We demonstrate our approach across a range of tasks including robotic manipulation, game playing and autonomous driving, showing competitive performance compared to hand-engineered solutions. Additionally, we show how fine-grained feedback can guide the learning process, improving results even further. Overall, we showcase a new paradigm where data collection and labeling efforts can be focused solely on high quality text descriptions rather than tedious manual annotation of low level control signals. -----",1
"Auto-encoding generative adversarial networks (GANs) combine the standard GAN algorithm, which discriminates between real and model-generated data, with a reconstruction loss given by an auto-encoder. Such models aim to prevent mode collapse in the learned generative model by ensuring that it is grounded in all the available training data. In this paper, we develop a principle upon which auto-encoders can be combined with generative adversarial networks by exploiting the hierarchical structure of the generative model. The underlying principle shows that variational inference can be used a basic tool for learning, but with the in- tractable likelihood replaced by a synthetic likelihood, and the unknown posterior distribution replaced by an implicit distribution; both synthetic likelihoods and implicit posterior distributions can be learned using discriminators. This allows us to develop a natural fusion of variational auto-encoders and generative adversarial networks, combining the best of both these methods. We describe a unified objective for optimization, discuss the constraints needed to guide learning, connect to the wide range of existing work, and use a battery of tests to systematically and quantitatively assess the performance of our method.",0
"Title: ""Auto-encoding Generative Adversarial Networks using Variational Methods""  Abstract: This study presents novel variational approaches for auto-encoding generative adversarial networks (GANs) that significantly improve their performance and stability. By leveraging recent advances in variational inference, we introduce two new methods for training GANs - variational auto-encoder guided generators (VAGGNet) and conditional variational auto-encoder guided discriminators (cVAAD). Both VAGGNet and cVAAD provide efficient and scalable solutions for encoding and decoding data into latent spaces that capture meaningful representations while maintaining strong generator diversity. Our extensive experimental evaluations demonstrate that both VAGGNet and cVAAD consistently outperform traditional auto-encoders and other state-of-the-art GAN variants across multiple benchmark datasets and tasks, including image generation, semantic segmentation, and superresolution. These results highlight the effectiveness and versatility of our proposed variational approach towards improving GANs as powerful tools for deep learning applications.",1
"We present a novel and effective approach for generating new clothing on a wearer through generative adversarial learning. Given an input image of a person and a sentence describing a different outfit, our model ""redresses"" the person as desired, while at the same time keeping the wearer and her/his pose unchanged. Generating new outfits with precise regions conforming to a language description while retaining wearer's body structure is a new challenging task. Existing generative adversarial networks are not ideal in ensuring global coherence of structure given both the input photograph and language description as conditions. We address this challenge by decomposing the complex generative process into two conditional stages. In the first stage, we generate a plausible semantic segmentation map that obeys the wearer's pose as a latent spatial arrangement. An effective spatial constraint is formulated to guide the generation of this semantic segmentation map. In the second stage, a generative model with a newly proposed compositional mapping layer is used to render the final image with precise regions and textures conditioned on this map. We extended the DeepFashion dataset [8] by collecting sentence descriptions for 79K images. We demonstrate the effectiveness of our approach through both quantitative and qualitative evaluations. A user study is also conducted. The codes and the data are available at http://mmlab.ie.cuhk. edu.hk/projects/FashionGAN/.",0
"Title: ""Be Your Own Prada: Fashion Synthesis with Structural Coherence"" Author(s): [Authors Name], [Affiliation(s)]  Abstract: The fashion industry has always been at the forefront of innovations, whether itâ€™s advancements in textile technology or breakthrough design concepts. With the rise of artificial intelligence (AI) and machine learning (ML), there is now yet another opportunity to revolutionize the world of fashion: â€œFashion Synthesis.â€ This research presents a novel methodology called â€œStructural Coherenceâ€ that enables AI systems to create high-quality synthesized images from scratch, focusing on the intersection of architecture and garment construction. The proposed approach leverages multi-task discriminator networks, adversarial training techniques, and reinforcement learning models, which allow deep generative models to learn complex patterns and structures found within clothing designs. By using these methods, our system can produce realistic looking images of new couture designs inspired by existing architectural motifs or geometric shapes â€“ hence creating truly unique one-of-a-kind pieces. Our experimental results demonstrate the effectiveness of Structural Coherence in generating new styles that blur the boundaries between traditional fashion and modern art forms; thus opening up opportunities for emerging designers to bring their creativity to life through computational means. Overall, the goal of this work is to inspire a broader conversation about the implications of algorithmic-driven fashion design and to pave the path towards more human-centered computing approaches within this domain.",1
"Low Dose Computed Tomography (LDCT) has offered tremendous benefits in radiation restricted applications, but the quantum noise as resulted by the insufficient number of photons could potentially harm the diagnostic performance. Current image-based denoising methods tend to produce a blur effect on the final reconstructed results especially in high noise levels. In this paper, a deep learning based approach was proposed to mitigate this problem. An adversarially trained network and a sharpness detection network were trained to guide the training process. Experiments on both simulated and real dataset shows that the results of the proposed method have very small resolution loss and achieves better performance relative to the-state-of-art methods both quantitatively and visually.",0
"This is an exciting new research project that explores the use of conditional generative adversarial networks (cGANs) for low dose computed tomography (CT) image denoising tasks while preserving sharp features and details within the images. Using cGANs allows us to generate clean high quality synthetic data which can then be used as training examples for subsequent supervised learning models such as denoisers. In our work we show how to leverage two powerful GAN variants :pix2pix and CycleGAN by adding an additional discriminator to ensure faithful reproduction of fine scale structure thus achieving state of the art performance on standard benchmarking datasets. We believe our approach has promising applications for medical imaging where radiation exposure to patients must be minimized but diagnosis accuracy cannot be sacrificed . In this work, we explore a novel approach to low dose computed tomography (CT) image denoising using conditional generative adversarial networks (CGAs). These types of networks have shown promise for generating synthetic high quality data from noisy inputs, which can then be utilized to train traditional supervised learning algorithms for improved task performance. Our method leverages two popular GAN architectures, pix2pix and CycleGAN, and adds an extra discriminator layer to guarantee accurate reproduction of fine-scale structures in order to achieve superior results compared to current state-of-the-art techniques. By carefully designing these models and optimizing their hyperparameters through extensive experimentation, we demonstrate significant improvements in both quantitative metrics like peak signal-to-noise ratio (PSNR), and subjective evaluation via human interpretation scores. These advancements hold great potential for applications in medical imaging, where reduction of patient exposure to ionizing radiation remains crucial without compromising diagnostic accuracy. Overall, this research provides a comprehensive analysis of GAAs for CT denoising with an emphasis on retaining important structural details throughout the imaging process. With further development and clinical validation, this technology may eventually find widespread adoption across diverse radiological domains.",1
"Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this through deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. The aim of this review paper is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.",0
"Abstract: Generative Adversarial Networks (GANs) have emerged as a powerful tool for generating synthetic data, such as images, videos, audios, text, among others. GANs consist of two deep neural networks playing a zero-sum game against each other; one generates samples (the generator), while the other discriminates whether they come from the real distribution or generated distribution (the discriminator). By optimizing both simultaneously through alternating steps of generation and discrimination, GANs can achieve state-of-the-art results on several benchmark datasets across different domains. In this overview, we survey recent advances made by researchers in deploying GANs in practice. We first discuss key mathematical formulations behind GANs and their training objectives. Then we present some of their applications ranging from computer vision tasks like super-resolution and image editing to natural language processing problems involving translation and question answering. Finally, we highlight current challenges facing researchers working on improving the performance and interpretability of GANs. Our aim is to provide readers with a broad understanding of how these models work and their potential impact across multiple fields.",1
"Face transfer animates the facial performances of the character in the target video by a source actor. Traditional methods are typically based on face modeling. We propose an end-to-end face transfer method based on Generative Adversarial Network. Specifically, we leverage CycleGAN to generate the face image of the target character with the corresponding head pose and facial expression of the source. In order to improve the quality of generated videos, we adopt PatchGAN and explore the effect of different receptive field sizes on generated images.",0
"In recent years, there has been significant progress in developing techniques that can transfer facial features from one image onto another, resulting in realistic composites of individuals who never existed. These approaches have mostly relied on supervised learning methods that require large amounts of labeled data to train deep neural networks. In this work, we present a new method based on generative adversarial networks (GANs) that allows us to perform face transfers without any labels at all, other than pairs of faces to swap features between. Our approach is composed of two main components: a generator network that generates images conditioned on both input faces, and a discriminator network trained to distinguish real and generated images. We show through extensive experiments that our method results in high quality transferred faces while preserving key attributes such as identity, gaze direction, and expressions. Furthermore, we demonstrate robustness to significant changes in lighting conditions, pose variations, and occlusions using state-of-the-art evaluation metrics.",1
"Generative Adversarial Networks (GANs) have become a widely popular framework for generative modelling of high-dimensional datasets. However their training is well-known to be difficult. This work presents a rigorous statistical analysis of GANs providing straight-forward explanations for common training pathologies such as vanishing gradients. Furthermore, it proposes a new training objective, Kernel GANs, and demonstrates its practical effectiveness on large-scale real-world data sets. A key element in the analysis is the distinction between training with respect to the (unknown) data distribution, and its empirical counterpart. To overcome issues in GAN training, we pursue the idea of smoothing the Jensen-Shannon Divergence (JSD) by incorporating noise in the input distributions of the discriminator. As we show, this effectively leads to an empirical version of the JSD in which the true and the generator densities are replaced by kernel density estimates, which leads to Kernel GANs.",0
"In generative adversarial networks (GANs), estimating Jensen-Shannon divergences (JSD) can greatly improve performance. The most common approach is to compute sample means which becomes prohibitively slow as GAN models increase in size. This work presents a non-parametric method that bypasses the need for computing sample means making it scalable to larger models. We demonstrate our method outperforms existing methods on CIFAR-10 using popular architectures such as ResNet and WideResNet trained on ImageNet. Our results show promise towards solving one of the key issues in modern deep learning research: finding effective ways to scale up model sizes without sacrificing quality.",1
"In this paper, we propose a method for cloud removal from visible light RGB satellite images by extending the conditional Generative Adversarial Networks (cGANs) from RGB images to multispectral images. Satellite images have been widely utilized for various purposes, such as natural environment monitoring (pollution, forest or rivers), transportation improvement and prompt emergency response to disasters. However, the obscurity caused by clouds makes it unstable to monitor the situation on the ground with the visible light camera. Images captured by a longer wavelength are introduced to reduce the effects of clouds. Synthetic Aperture Radar (SAR) is such an example that improves visibility even the clouds exist. On the other hand, the spatial resolution decreases as the wavelength increases. Furthermore, the images captured by long wavelengths differs considerably from those captured by visible light in terms of their appearance. Therefore, we propose a network that can remove clouds and generate visible light images from the multispectral images taken as inputs. This is achieved by extending the input channels of cGANs to be compatible with multispectral images. The networks are trained to output images that are close to the ground truth using the images synthesized with clouds over the ground truth as inputs. In the available dataset, the proportion of images of the forest or the sea is very high, which will introduce bias in the training dataset if uniformly sampled from the original dataset. Thus, we utilize the t-Distributed Stochastic Neighbor Embedding (t-SNE) to improve the problem of bias in the training dataset. Finally, we confirm the feasibility of the proposed network on the dataset of four bands images, which include three visible light bands and one near-infrared (NIR) band.",0
"This paper presents a novel approach for filmy cloud removal from satellite imagery using multispectral conditional generative adversarial networks (CycleGAN). Filmy clouds often obscure important details in satellite images, making them difficult to interpret. Traditional approaches such as thresholding techniques have limitations in removing these types of clouds while preserving image quality. Our proposed method utilizes deep learning algorithms to generate high resolution synthetic bands that can accurately represent the underlying surface features. By doing so, we can better distinguish between pixels affected by clouds versus those belonging to the true scene. Experiments conducted on several datasets demonstrate the effectiveness of our approach compared to existing methods. In summary, our work represents an advance in computer vision research aimed at addressing real world problems affecting remote sensing data analysis.",1
"We present variational generative adversarial networks, a general learning framework that combines a variational auto-encoder with a generative adversarial network, for synthesizing images in fine-grained categories, such as faces of a specific person or objects in a category. Our approach models an image as a composition of label and latent attributes in a probabilistic model. By varying the fine-grained category label fed into the resulting generative model, we can generate images in a specific category with randomly drawn values on a latent attribute vector. Our approach has two novel aspects. First, we adopt a cross entropy loss for the discriminative and classifier network, but a mean discrepancy objective for the generative network. This kind of asymmetric loss function makes the GAN training more stable. Second, we adopt an encoder network to learn the relationship between the latent space and the real image space, and use pairwise feature matching to keep the structure of generated images. We experiment with natural images of faces, flowers, and birds, and demonstrate that the proposed models are capable of generating realistic and diverse samples with fine-grained category labels. We further show that our models can be applied to other tasks, such as image inpainting, super-resolution, and data augmentation for training better face recognition models.",0
"This research presents a novel approach to image generation using Conditional Variational Autoencoders (CVAEs) trained with Generative Adversarial Networks (GANs). Our method, which we call CVAE-GAN, allows for fine-grained control over generated images by incorporating semantic features as input to both the generator and discriminator networks. We demonstrate that our approach outperforms traditional CVAEs and GANs on several benchmark datasets across a range of metrics, including visual fidelity, feature relevance, and diversity. Furthermore, we provide theoretical analysis and experimental results showing that asymmetric training can significantly improve performance in terms of both quality and efficiency. Overall, our work represents an important contribution towards solving one of the most challenging problems in computer visionâ€”generating high-quality images from scratch.",1
"We investigate Generative Adversarial Networks (GANs) to model one particular kind of image: frames from TV cartoons. Cartoons are particularly interesting because their visual appearance emphasizes the important semantic information about a scene while abstracting out the less important details, but each cartoon series has a distinctive artistic style that performs this abstraction in different ways. We consider a dataset consisting of images from two popular television cartoon series, Family Guy and The Simpsons. We examine the ability of GANs to generate images from each of these two domains, when trained independently as well as on both domains jointly. We find that generative models may be capable of finding semantic-level correspondences between these two image domains despite the unsupervised setting, even when the training data does not give labeled alignments between them.",0
"Title: A Survey of Generation Techniques for Multimedia Data Authors: <authors> Contact Info: <contact info> Abstract: In recent years, generative models have become increasingly important as a tool for creating new multimedia content. These systems use machine learning algorithms to generate novel data that matches the properties of existing data from one or more domains. While there has been considerable research on the application of these techniques in specific domains such as images, video, audio, etc., little work has focused specifically on cartoon series. As a result, we present a comprehensive survey of generation methods used across different media types, their applications, strengths, weaknesses, limitations, and open challenges in generating high-quality synthetic cartoon frames. We then apply some promising approaches to create our own cartoon series, which we showcase as case studies at the end of each chapter. Our study provides insights into the current state of art and opens up opportunities for further research in developing advanced generative models tailored for animations and cartoons. Keywords: generative models, cross domain, multimedia, cartoon series, image generation, animation, deep learning models Copyright: This is a preprint of a paper intended for submission to a journal/conference. For final published versions see relevant journal/proceedings. Personal Use only via any database where this may appear. Please contact publisher for permission to copy beyond fair uses or if using electronic versions",1
"Generative Adversarial Networks (GANs) have shown impressive performance in generating photo-realistic images. They fit generative models by minimizing certain distance measure between the real image distribution and the generated data distribution. Several distance measures have been used, such as Jensen-Shannon divergence, $f$-divergence, and Wasserstein distance, and choosing an appropriate distance measure is very important for training the generative network. In this paper, we choose to use the maximum mean discrepancy (MMD) as the distance metric, which has several nice theoretical guarantees. In fact, generative moment matching network (GMMN) (Li, Swersky, and Zemel 2015) is such a generative model which contains only one generator network $G$ trained by directly minimizing MMD between the real and generated distributions. However, it fails to generate meaningful samples on challenging benchmark datasets, such as CIFAR-10 and LSUN. To improve on GMMN, we propose to add an extra network $F$, called mapper. $F$ maps both real data distribution and generated data distribution from the original data space to a feature representation space $\mathcal{R}$, and it is trained to maximize MMD between the two mapped distributions in $\mathcal{R}$, while the generator $G$ tries to minimize the MMD. We call the new model generative adversarial mapping networks (GAMNs). We demonstrate that the adversarial mapper $F$ can help $G$ to better capture the underlying data distribution. We also show that GAMN significantly outperforms GMMN, and is also superior to or comparable with other state-of-the-art GAN based methods on MNIST, CIFAR-10 and LSUN-Bedrooms datasets.",0
"This paper presents generative adversarial mapping networks (GAMN), a novel method for generating maps that enable agents to effectively navigate complex environments. GAMN builds upon recent advances in deep learning by combining both supervised and unsupervised techniques. In particular, we train a convolutional neural network (CNN) using human-annotated map data as well as synthetic maps generated through reinforcement learning. To evaluate our approach, we test GAMN on several benchmark domains and demonstrate improved performance over state-of-the-art mapping methods. Our results suggest that GAMN has the potential to improve agent navigation in real-world applications such as robotics and virtual reality. Additionally, we provide ablation studies to analyze the impact of different components of our model and discuss future research directions for improving GAMN. Overall, our work contributes to the fields of artificial intelligence and computer vision by providing a new tool for creating high-quality maps for navigation purposes.",1
"In this paper we introduce a new structure to Generative Adversarial Networks by adding an inverse transformation unit behind the generator. We present two theorems to claim the convergence of the model, and two conjectures to nonideal situations when the transformation is not bijection. A general survey on models with different transformations was done on the MNIST dataset and the Fashion-MNIST dataset, which shows the transformation does not necessarily need to be bijection. Also, with certain transformations that blurs an image, our model successfully learned to sharpen the images and recover blurred images, which was additionally verified by our measurement of sharpness.",0
"""Generative Adversarial Networks (GAN) have achieved significant advancements in generating realistic images, but suffer from instability during training. One approach to improve stability is introducing regularization terms to constrain the optimization process; however, these methods are often empirical and may not necessarily address the underlying issues that lead to instability. We propose a novel architecture called the Inverse Transform Unit (ITU), which learns invertible transformations and improves the stability of GAN training by reducing mode collapse and encouraging more diverse generated samples. Our method leads to improvements in perceptual quality and sample diversity over state-of-the-art architectures on multiple datasets.""",1
"Generative adversarial networks (GANs) are an exciting alternative to algorithms for solving density estimation problems---using data to assess how likely samples are to be drawn from the same distribution. Instead of explicitly computing these probabilities, GANs learn a generator that can match the given probabilistic source. This paper looks particularly at this matching capability in the context of problems with one-dimensional outputs. We identify a class of function decompositions with properties that make them well suited to the critic role in a leading approach to GANs known as Wasserstein GANs. We show that Taylor and Fourier series decompositions belong to our class, provide examples of these critics outperforming standard GAN approaches, and suggest how they can be scaled to higher dimensional problems in the future.",0
"This paper presents new results on the theory of summable reparameterizations of gradient critic distributions in one dimension. These methods have been recently shown to provide improved accuracy and stability over traditional approaches, particularly in high dimensional spaces where Monte Carlo sampling can become computationally prohibitive. We develop novel theoretical analysis demonstrating that these new techniques enjoy superior convergence rates compared to classical methods. Our findings contribute valuable insights into the behavior of gradient critics in low dimensions and could potentially inform future development of advanced machine learning algorithms based upon this foundation.",1
"It is important for robots to be able to decide whether they can go through a space or not, as they navigate through a dynamic environment. This capability can help them avoid injury or serious damage, e.g., as a result of running into people and obstacles, getting stuck, or falling off an edge. To this end, we propose an unsupervised and a near-unsupervised method based on Generative Adversarial Networks (GAN) to classify scenarios as traversable or not based on visual data. Our method is inspired by the recent success of data-driven approaches on computer vision problems and anomaly detection, and reduces the need for vast amounts of negative examples at training time. Collecting negative data indicating that a robot should not go through a space is typically hard and dangerous because of collisions, whereas collecting positive data can be automated and done safely based on the robot's own traveling experience. We verify the generality and effectiveness of the proposed approach on a test dataset collected in a previously unseen environment with a mobile robot. Furthermore, we show that our method can be used to build costmaps (we call as ""GoNoGo"" costmaps) for robot path planning using visual data only.",0
"This paper presents a novel approach to robot navigation that utilizes near unsupervised learning techniques. Traditional methods rely on large amounts of labeled data, which can be expensive and time consuming to collect and annotate. In contrast, our proposed method leverages unlabeled video frames collected from different environments, allowing the robot to learn how to navigate through continuous observation alone. We use a modified version of the deep reinforcement learning algorithm ACKTR to allow for exploration and adaptation based on feedback from interactions with the environment. Our experiments show that this approach leads to significant improvements in accuracy over existing methods, even outperforming fully supervised models trained on labeled data. Additionally, we demonstrate that our model generalizes well across multiple environments, making it suitable for real-world applications where access to detailed maps and labels may not be feasible. Overall, our work represents a step forward towards autonomous robots that can effectively operate in complex, uncertain environments without extensive human intervention.",1
"We present a novel method to solve image analogy problems : it allows to learn the relation between paired images present in training data, and then generalize and generate images that correspond to the relation, but were never seen in the training set. Therefore, we call the method Conditional Analogy Generative Adversarial Network (CAGAN), as it is based on adversarial training and employs deep convolutional neural networks. An especially interesting application of that technique is automatic swapping of clothing on fashion model photos. Our work has the following contributions. First, the definition of the end-to-end trainable CAGAN architecture, which implicitly learns segmentation masks without expensive supervised labeling data. Second, experimental results show plausible segmentation masks and often convincing swapped images, given the target article. Finally, we discuss the next steps for that technique: neural network architecture improvements and more advanced applications.",0
"Artificial intelligence has made significant progress in recent years, especially in image generation tasks such as generating high-quality images based on textual descriptions or swapping objects within images. In order to tackle the problem of replacing objects in images with higher precision, we propose a new approach called the Conditional Analogy Generative Adversarial Network (CAGAN). Our model uses conditional analogy to map object regions onto corresponding regions in another image that contains the desired replacement object. We demonstrate the effectiveness of our method by applying it to fashion articles such as hats, glasses, and handbags, and show that it produces realistic results while preserving the original context. Compared to previous methods, CAGAN achieves better performance in terms of both visual quality and semantic consistency. Additionally, since the input can be any human body and the output can be any style of glasses, hat or bag, our method allows for greater creativity in fashion design. Overall, our work shows great potential for applications in virtual try-on, fashion recommendation systems, and content creation.",1
"Facial expression editing is a challenging task as it needs a high-level semantic understanding of the input face image. In conventional methods, either paired training data is required or the synthetic face resolution is low. Moreover, only the categories of facial expression can be changed. To address these limitations, we propose an Expression Generative Adversarial Network (ExprGAN) for photo-realistic facial expression editing with controllable expression intensity. An expression controller module is specially designed to learn an expressive and compact expression code in addition to the encoder-decoder network. This novel architecture enables the expression intensity to be continuously adjusted from low to high. We further show that our ExprGAN can be applied for other tasks, such as expression transfer, image retrieval, and data augmentation for training improved face expression recognition models. To tackle the small size of the training database, an effective incremental learning scheme is proposed. Quantitative and qualitative evaluations on the widely used Oulu-CASIA dataset demonstrate the effectiveness of ExprGAN.",0
"This research presents a new approach for facial expression editing that allows users to control the intensity of specific expressions. Our method uses generative adversarial networks (GANs) to generate realistic facial expressions while still allowing for precise manipulation of individual expressions like joy, sadness, anger, etc. We evaluate our model on several benchmark datasets and demonstrate its effectiveness at editing facial expressions under a variety of conditions. In addition, we conduct user studies to show that our edited faces can effectively convey different emotions and are perceived as natural by human observers. Overall, our work represents a significant advance in the field of computer graphics and has numerous applications in areas such as virtual reality, film special effects, and social media filters.",1
"We propose in this paper a novel approach to tackle the problem of mode collapse encountered in generative adversarial network (GAN). Our idea is intuitive but proven to be very effective, especially in addressing some key limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We term our method dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both two discriminators. We develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem. We conduct extensive experiments on synthetic and real-world large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made our best effort to compare our D2GAN with the latest state-of-the-art GAN's variants in comprehensive qualitative and quantitative evaluations. The experimental results demonstrate the competitive and superior performance of our approach in generating good quality and diverse samples over baselines, and the capability of our method to scale up to ImageNet database.",0
"Dual discriminators have recently been used as a means for improving generative models such as GANs by providing more focused feedback through two distinct adversaries. In order to further investigate their effectiveness, we explore the role of dual discriminator architectures on conditional image generation tasks using both traditional GAN training regimes (e.g., DCGAN) as well as newer approaches (e.g., SAGAN). Our results demonstrate that the use of dual discriminators significantly outperforms baseline approaches and other popular architecture modifications in terms of perceptually grounded metrics such as FID scores and human evaluations. Furthermore, ablation studies show that utilizing multiple discriminators leads to improved performance across all tested evaluation criteria even at smaller network sizes than previously reported state-of-the-art work. We posit that our findings here could lead to further improvements in conditional image synthesis and potentially other areas where generative models might prove beneficial.",1
"In this paper, we propose a novel application of Generative Adversarial Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy. Compared to natural images, cells tend to have a simpler and more geometric global structure that facilitates image generation. However, the correlation between the spatial pattern of different fluorescent proteins reflects important biological functions, and synthesized images have to capture these relationships to be relevant for biological applications. We adapt GANs to the task at hand and propose new models with casual dependencies between image channels that can generate multi-channel images, which would be impossible to obtain experimentally. We evaluate our approach using two independent techniques and compare it against sensible baselines. Finally, we demonstrate that by interpolating across the latent space we can mimic the known changes in protein localization that occur through time during the cell cycle, allowing us to predict temporal evolution from static images.",0
"In recent years, generative adversarial networks (GANs) have become increasingly popular as a tool for generating synthetic biological images that can mimic real data. These models are trained using large datasets of labeled biomedical images, such as MRI scans, histology slides, or microscopy images, which are then used to create new synthetic images with similar characteristics. This allows researchers to generate images of novel conditions or configurations that cannot easily be obtained through experiments.  This paper presents an overview of current state-of-the-art techniques in GANs for biological image synthesis, including both supervised and unsupervised approaches. We review several existing architectures and their respective strengths and weaknesses, highlighting how they have been applied to different types of biological imaging modalities. Additionally, we discuss the importance of proper evaluation metrics to quantify the quality of generated images and the challenges associated with training and evaluating these complex systems. Finally, we provide insights into future directions for improving the performance of GANs in this field by exploring areas such as incorporation of domain knowledge, regularization methods, and other advanced machine learning techniques. Overall, this comprehensive survey provides valuable guidance for researchers looking to apply GANs for biological image synthesis applications.",1
"The gap between sensing patterns of different face modalities remains a challenging problem in heterogeneous face recognition (HFR). This paper proposes an adversarial discriminative feature learning framework to close the sensing gap via adversarial learning on both raw-pixel space and compact feature space. This framework integrates cross-spectral face hallucination and discriminative feature learning into an end-to-end adversarial network. In the pixel space, we make use of generative adversarial networks to perform cross-spectral face hallucination. An elaborate two-path model is introduced to alleviate the lack of paired images, which gives consideration to both global structures and local textures. In the feature space, an adversarial loss and a high-order variance discrepancy loss are employed to measure the global and local discrepancy between two heterogeneous distributions respectively. These two losses enhance domain-invariant feature learning and modality independent noise removing. Experimental results on three NIR-VIS databases show that our proposed approach outperforms state-of-the-art HFR methods, without requiring of complex network or large-scale training dataset.",0
"This paper presents a novel approach to face recognition that uses adversarial training and discrimination techniques to improve accuracy and robustness in challenging scenarios. Traditional face recognition methods often struggle with variations in lighting conditions, facial expressions, pose changes, and occlusions, but these new techniques can significantly reduce such errors by focusing on key features and using data augmentation techniques. Our method involves creating two separate networks: one to capture high-level representations of faces and another to perform discriminative classification tasks. These networks work together to minimize cross entropy loss functions while maximizing mutual information between them. Through extensive experiments, we demonstrate significant improvements over state-of-the-art algorithms across several benchmark datasets, including YTF, LFW, and CFP, with promising results even under adverse conditions.",1
"Removing blur caused by camera shake in images has always been a challenging problem in computer vision literature due to its ill-posed nature. Motion blur caused due to the relative motion between the camera and the object in 3D space induces a spatially varying blurring effect over the entire image. In this paper, we propose a novel deep filter based on Generative Adversarial Network (GAN) architecture integrated with global skip connection and dense architecture in order to tackle this problem. Our model, while bypassing the process of blur kernel estimation, significantly reduces the test time which is necessary for practical applications. The experiments on the benchmark datasets prove the effectiveness of the proposed method which outperforms the state-of-the-art blind deblurring algorithms both quantitatively and qualitatively.",0
"This paper presents a deep generative filter that can effectively remove motion blur from images taken under low light conditions. Conventional methods suffer from degradation of image quality due to over-smoothing, halo artifacts, noise amplification, and limited performance. Our approach tackles these issues by utilizing an adversarially trained network with two subnetworks: a generator that restores sharp details while preserving structural edges and textures, and a discriminator that guides the generator towards perceptually realistic outputs. We introduce novel techniques such as batch normalization layer in the discriminator architecture and temporal guidance module in the generator, which significantly improve the efficiency and effectiveness of our model. Experimental results on benchmark datasets demonstrate significant improvement over state-of-the-art methods in terms of visual fidelity, edge preservation, and objective metrics. Overall, we present a highly competitive yet simple-to-implement solution for single image motion deblurring.",1
"This paper introduces a novel approach to texture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the structure of the input noise distribution by constructing tensors with different types of dimensions. We call this technique Periodic Spatial GAN (PSGAN). The PSGAN has several novel abilities which surpass the current state of the art in texture synthesis. First, we can learn multiple textures from datasets of one or more complex large images. Second, we show that the image generation with PSGANs has properties of a texture manifold: we can smoothly interpolate between samples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. In addition, we can also accurately learn periodical textures. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources. Our method is highly scalable and it can generate output images of arbitrary large size.",0
"This abstract describes how we can use a generative adversarial network (GAN) to learn about textures, which are patterns that occur everywhere in the world but can be difficult to study directly. We introduce the periodic spatial GAN architecture that allows us to train both local and global texture descriptors in one single model, without compromising the performance. Our method produces results comparable to state-of-the-art methods on two benchmark datasets while being more efficient and simple. In addition, our approach exhibits new properties allowing us to study texture manifolds in ways previously unexplored. Furthermore, since our framework takes into account local features along with their periodicity, we showcase some interesting applications such as generating textured materials and enhanced colorization using real and simulated data, opening up promising prospects for computer graphics, virtual reality, augmented reality, robotics, autonomous vehicles, etc. Finally, through rigorous experimentation, we analyze various aspects of our proposed architecture and discuss future extensions and possibilities.",1
"Generative adversarial networks (GANs) are a recent approach to train generative models of data, which have been shown to work particularly well on image data. In the current paper we introduce a new model for texture synthesis based on GAN learning. By extending the input noise distribution space from a single vector to a whole spatial tensor, we create an architecture with properties well suited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is the first successful completely data-driven texture synthesis method based on GANs.   Our method has the following features which make it a state of the art algorithm for texture synthesis: high image quality of the generated textures, very high scalability w.r.t. the output texture size, fast real-time forward generation, the ability to fuse multiple diverse source images in complex textures. To illustrate these capabilities we present multiple experiments with different classes of texture images and use cases. We also discuss some limitations of our method with respect to the types of texture images it can synthesize, and compare it to other neural techniques for texture generation.",0
"This paper presents a novel approach to texture synthesis using spatial generative adversarial networks (GANs). We develop a framework that can generate high quality textures by leveraging the power of GANs to produce realistic and detailed images. Our method outperforms traditional texture synthesis techniques, as well as other state-of-the-art deep learning approaches. By combining conditional image generation with adversarial training, we achieve unprecedented results in terms of visual fidelity and authenticity. In addition, our model has a simple architecture and requires minimal computational resources compared to prior art. Overall, this work demonstrates the potential of GANs for generating rich and diverse textures, opening up new possibilities for computer graphics applications such as video games, movies, and virtual reality experiences.",1
"We have developed a new data-driven paradigm for the rapid inference, modeling and simulation of the physics of transport phenomena by deep learning. Using conditional generative adversarial networks (cGAN), we train models for the direct generation of solutions to steady state heat conduction and incompressible fluid flow purely on observation without knowledge of the underlying governing equations. Rather than using iterative numerical methods to approximate the solution of the constitutive equations, cGANs learn to directly generate the solutions to these phenomena, given arbitrary boundary conditions and domain, with high test accuracy (MAE$$1\%) and state-of-the-art computational performance. The cGAN framework can be used to learn causal models directly from experimental observations where the underlying physical model is complex or unknown.",0
"In recent years, deep learning has emerged as a powerful tool for modeling complex systems across a wide range of fields. One such area where deep learning has shown great potential is in transport phenomena, which describes the movement of mass, energy, and momentum in physical systems. This paper presents a novel approach to using deep learning techniques to solve problems in transport phenomena that have traditionally relied on finite difference methods.  The authors demonstrate the effectiveness of their method by applying it to two well-known transport problems: Poiseuille flow and heat conduction in solids. For both cases, they train a convolutional neural network (CNN) to predict the spatial distribution of relevant quantities like velocity, temperature, and concentration. By utilizing CNNs' ability to capture local patterns and relationships, the trained models achieve remarkable accuracy in comparison to traditional numerical solutions.  Additionally, the authors show that their method can effectively handle noisy data and is robust against changes in boundary conditions, demonstrating its versatility in real-world applications. Overall, this work represents an important step forward in harnessing the power of deep learning for solving engineering problems related to transport phenomena.",1
"Lighting estimation from face images is an important task and has applications in many areas such as image editing, intrinsic image decomposition, and image forgery detection. We propose to train a deep Convolutional Neural Network (CNN) to regress lighting parameters from a single face image. Lacking massive ground truth lighting labels for face images in the wild, we use an existing method to estimate lighting parameters, which are treated as ground truth with unknown noises. To alleviate the effect of such noises, we utilize the idea of Generative Adversarial Networks (GAN) and propose a Label Denoising Adversarial Network (LDAN) to make use of synthetic data with accurate ground truth to help train a deep CNN for lighting regression on real face images. Experiments show that our network outperforms existing methods in producing consistent lighting parameters of different faces under similar lighting conditions. Moreover, our method is 100,000 times faster in execution time than prior optimization-based lighting estimation approaches.",0
"This paper presents a new approach for inverse lighting of face images using a denoising adversarial network called LDAN. We trained two deep neural networks, one generative model G and discriminator D that plays the minimax game, where G generates a fake image conditioned on text prompts and D attempts to distinguish real images from generated ones. By training both models together we achieve better results than state-of-the art methods such as CycleGan at improving details like eye reflections and shading on faces. Since our method only requires pairs of input/output images unlike other unpaired image to image translation papers which need large datasets of paired data to train well. Our code and pretrained weights can be found on GitHub.  We use an existing UNet style generator as the starting point of our G network, taking in noise as input before passing through six residual blocks each containing batch normalization layers and leaky ReLU activation functions. To ensure robustness during training we used spectral normalization. At the end of the pipeline we sample random Gaussian noise and pass it through G to produce images.  For D we use a variation of the BigGGAN architecture, consisting of four dense block pairs with LeakyReLU activation functions andBatchNorm2 parameters are updated by adding two values instead of subtracting them, resulting in higher stability at higher magnitudes.. Additionally, inspired byBigGANwe set BatchNormalize parameters to match those learnedbyBigGGANto improveits performance compared tomoreconventionaldiscriminatorslike PatchGAN.  At test time, given input images I_in, G generates output images I_out = G(noise), then D scores how likely it thinks these output images are real images according to logits l of form y_real - y_fake. Finally, we calculate a denoised version of the input based off of the estimated clean latent representation: z=argmin|| z ||^2 +lambda*|logits(l)|^2 (4)  Evaluation metrics show substantial improvement over SOTA methods: improvements greater t",1
"The rapid growth of Electronic Health Records (EHRs), as well as the accompanied opportunities in Data-Driven Healthcare (DDH), has been attracting widespread interests and attentions. Recent progress in the design and applications of deep learning methods has shown promising results and is forcing massive changes in healthcare academia and industry, but most of these methods rely on massive labeled data. In this work, we propose a general deep learning framework which is able to boost risk prediction performance with limited EHR data. Our model takes a modified generative adversarial network namely ehrGAN, which can provide plausible labeled EHR data by mimicking real patient records, to augment the training dataset in a semi-supervised learning manner. We use this generative model together with a convolutional neural network (CNN) based prediction model to improve the onset prediction performance. Experiments on two real healthcare datasets demonstrate that our proposed framework produces realistic data samples and achieves significant improvements on classification tasks with the generated data over several stat-of-the-art baselines.",0
"In recent years, deep learning has become increasingly popular in healthcare due to its ability to process large amounts of data efficiently and accurately identify patterns that could predict patient outcomes. However, the use of electronic health records (EHR) in these models can lead to challenges such as high dimensionality and sparsity, which hinder their effectiveness in risk prediction tasks.  To address these issues, we propose using generative adversarial networks (GANs), which have shown promise in generating synthetic datasets for augmentation purposes, but have yet to be explored for EHR applications. We demonstrate how GANs can effectively boost the performance of risk predictions by reducing the high dimensionality and sparsity present in the dataset. Our approach involves training two neural networks - a generator network and a discriminator network - in competition against each other. The generator creates new samples that are both realistic and diverse enough to fool the discriminator, while the latter distinguishes between genuine data points and generated ones. By utilizing the generated data alongside the original EHR data, our method significantly improves downstream task metrics compared to traditional approaches, demonstrating the feasibility of incorporating generative techniques into clinical decision support systems. Additionally, our study evaluates different types of losses used during training, exploring how they impact model stability and generalization. These results provide insights into choosing appropriate loss functions for specific problems involving imbalanced and sparse data. Overall, our research presents a novel application of GANs for enhancing risk prediction accuracy in medical domains using EHR data, paving the way towards more robust AI algorithms fo",1
"In this paper, we describe how a patient-specific, ultrasound-probe-induced prostate motion model can be directly generated from a single preoperative MR image. Our motion model allows for sampling from the conditional distribution of dense displacement fields, is encoded by a generative neural network conditioned on a medical image, and accepts random noise as additional input. The generative network is trained by a minimax optimisation with a second discriminative neural network, tasked to distinguish generated samples from training motion data. In this work, we propose that 1) jointly optimising a third conditioning neural network that pre-processes the input image, can effectively extract patient-specific features for conditioning; and 2) combining multiple generative models trained separately with heuristically pre-disjointed training data sets can adequately mitigate the problem of mode collapse. Trained with diagnostic T2-weighted MR images from 143 real patients and 73,216 3D dense displacement fields from finite element simulations of intraoperative prostate motion due to transrectal ultrasound probe pressure, the proposed models produced physically-plausible patient-specific motion of prostate glands. The ability to capture biomechanically simulated motion was evaluated using two errors representing generalisability and specificity of the model. The median values, calculated from a 10-fold cross-validation, were 2.8+/-0.3 mm and 1.7+/-0.1 mm, respectively. We conclude that the introduced approach demonstrates the feasibility of applying state-of-the-art machine learning algorithms to generate organ motion models from patient images, and shows significant promise for future research.",0
"This paper presents a novel approach to modeling intraoperative organ motion using an ensemble of conditional generative adversarial networks (cGANs). In surgical procedures that involve deep tissue excision or radiofrequency ablation, real-time monitoring of organ motion can greatly improve accuracy and reduce complications. However, current techniques for tracking organ motion during surgery rely heavily on external markers or invasive instrumentation, which can increase cost and risk to patients.  Our method uses cGANs to generate synthetic images of organs from different angles, simulating different positions and deformations caused by respiration, heartbeat, and other factors. By training multiple cGANs with different sets of input features, we create an ensemble of models that together capture a wide range of motion patterns. Our system then selects the most appropriate GAN based on real-time measurements of patient vital signs, producing a synthetic image that closely matches the position of the targeted organ at any given time.  To evaluate our method, we conducted experiments using data from animal subjects undergoing surgery, comparing the performance of our proposed approach against baseline methods such as linear regression and Monte Carlo simulation. Results showed significant improvements in accuracy, particularly in capturing complex multi-directional motion patterns. We believe our ensemble of cGANs holds great potential for enhancing the safety and efficacy of minimally invasive surgeries, paving the way towards personalized organ motion prediction algorithms that adapt to individual patient characteristics.",1
"In recent years, there has been an increasing interest in image-based plant phenotyping, applying state-of-the-art machine learning approaches to tackle challenging problems, such as leaf segmentation (a multi-instance problem) and counting. Most of these algorithms need labelled data to learn a model for the task at hand. Despite the recent release of a few plant phenotyping datasets, large annotated plant image datasets for the purpose of training deep learning algorithms are lacking. One common approach to alleviate the lack of training data is dataset augmentation. Herein, we propose an alternative solution to dataset augmentation for plant phenotyping, creating artificial images of plants using generative neural networks. We propose the Arabidopsis Rosette Image Generator (through) Adversarial Network: a deep convolutional network that is able to generate synthetic rosette-shaped plants, inspired by DCGAN (a recent adversarial network model using convolutional layers). Specifically, we trained the network using A1, A2, and A4 of the CVPPP 2017 LCC dataset, containing Arabidopsis Thaliana plants. We show that our model is able to generate realistic 128x128 colour images of plants. We train our network conditioning on leaf count, such that it is possible to generate plants with a given number of leaves suitable, among others, for training regression based models. We propose a new Ax dataset of artificial plants images, obtained by our ARIGAN. We evaluate this new dataset using a state-of-the-art leaf counting algorithm, showing that the testing error is reduced when Ax is used as part of the training data.",0
"Here we describe Arigan, a software tool that uses generative adversarial networks (GANs) to generate synthetic three-dimensional models of Arabidopsis plants based on their two-dimensional images. Growers can use these plant models to visualize their crops at different stages of development. We evaluated Arigan on a dataset consisting of thousands of real Arabidopsis photos taken under natural conditions, demonstrating that our approach yields high-fidelity results that capture subtle details such as stem curvature, leaf twisting, and petiole angles. This provides researchers with a convenient alternative to growing live plants, saving them time and resources while improving reproducibility and data sharing. Arigan offers valuable insights into how machine learning and computer vision techniques like GANs can enhance agriculture by providing detailed crop representations that enable better decision making and resource optimization. By expanding our knowledge of how data science tools can improve farming practices, researchers may one day create innovations tailored specifically to the needs of local ecosystems, enabling sustainable agricultural growth worldwide. Overall, this project underscores the potential impact that cutting-edge technology has on traditional industries, showing once again that progress often comes from marrying old wisdom with new expertise.",1
"Recent approaches in generative adversarial networks (GANs) can automatically synthesize realistic images from descriptive text. Despite the overall fair quality, the generated images often expose visible flaws that lack structural definition for an object of interest. In this paper, we aim to extend state of the art for GAN-based text-to-image synthesis by improving perceptual quality of generated images. Differentiated from previous work, our synthetic image generator optimizes on perceptual loss functions that measure pixel, feature activation, and texture differences against a natural image. We present visually more compelling synthetic images of birds and flowers generated from text descriptions in comparison to some of the most prominent existing work.",0
"Deep learning has made significant progress in natural language understanding tasks through sequence models such as recurrent neural networks (RNNs) and transformers. But these methods face challenges when it comes to generating coherent images from descriptive texts. In response, researchers have proposed adversarial training frameworks that learn to predict realism scores over generated images in order to improve their quality. However, these approaches are trained using discriminators based on highlevel features such as DNN activations rather than more fine-grained pixel space metrics. As a result, they may miss important local texture details crucial for image generation fidelity. To address this issue, we propose using convolutional neural networkbased objectives at multiple scales within the generator architecture during optimization. By doing so, our method can better capture relevant spatial representations while optimizing text-to-image mapping functions. We evaluate our approach across several benchmark datasets showing that we achieve stateoftheart results compared against other adversarially trained image generative models. Our work emphasizes the importance of considering multi-scale information for text-todigital translation problems and highlights promising future directions towards tackling them using deep learning techniques",1
"In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike the existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid by filling in the occluded/missing regions. The key idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Networks (GAN) framework, to infer accurate and fine-grained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets show that the proposed 3D-RecGAN significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects. Our code and data are available at: https://github.com/Yang7879/3D-RecGAN.",0
"This paper presents a new approach for reconstructing three-dimensional objects from a single depth view using adversarial learning. Inspired by recent advances in computer vision, we develop a deep neural network that learns to generate realistic, detailed 3D models given only one camera image. Our method combines novel architectures based on convolutional and recurrent layers with powerful loss functions designed to capture geometric features at multiple scales. Extensive experiments demonstrate the effectiveness of our model for producing high quality results across a variety of settings, outperforming state-of-the-art methods while requiring significantly fewer computational resources. Overall, our work shows promise for applications such as virtual reality, augmented reality, and robotics where generating 3D representations quickly and accurately is crucial. We plan future research into improving generalization and reducing errors through the use of multi-view training data and better initialization techniques.",1
"Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images often fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that attempt to map representations between the two domains or learn to extract features that are domain-invariant. In this work, we present a new approach that learns, in an unsupervised manner, a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.",0
"This paper presents an unsupervised approach to domain adaptation at the pixel level using generative adversarial networks (GANs). Previous work on domain adaptation has focused primarily on adapting semantic representations or high-level features of images, but our method directly addresses the challenge of matching both high-level concepts as well as low-level image details across different domains. We propose a new formulation of domain adaptation that involves training two discriminators: one to predict whether an image comes from a source or target domain, and another to predict which pixels have been generated by a generator network. Our GAN architecture learns a shared latent space that captures both high-level object categories and fine-grained texture differences. Experimental results demonstrate significant improvements over state-of-the-art methods on multiple benchmark datasets, validating the effectiveness of our proposed method.",1
"The main contribution of this paper is a simple semi-supervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 and DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at https://github.com/layumi/Person-reID_GAN.",0
"Growing concerns about privacy have increased the importance of protecting personal data. One common approach involves person re-identification (ReID), which attempts to identify individuals from their appearance captured on camera. To improve accuracy, researchers often use large amounts of labeled training data. However, obtaining annotated data can prove costly both monetarily and timewise. In contrast, unlabeled samples generated through generative adversarial networks offer an alternative solution that may minimize these expenses while achieving equal success rates. This study investigates whether using such synthesized data during the model training process leads to better performance compared to traditional methods solely reliant upon labeled data. An extensive evaluation was conducted across multiple datasets and metrics. Results demonstrate notable improvements without sacrificing efficiency due to the inclusion of unlabeled samples generated via GANs. Our findings provide insight into the potential benefits of incorporating artificial intelligence components within security systems seeking to safeguard private records. Overall, our research sheds light onto how advancements in technology can directly aid society by providing more affordable options without compromising quality.",1
"Automatic generation of facial images has been well studied after the Generative Adversarial Network (GAN) came out. There exists some attempts applying the GAN model to the problem of generating facial images of anime characters, but none of the existing work gives a promising result. In this work, we explore the training of GAN models specialized on an anime facial image dataset. We address the issue from both the data and the model aspect, by collecting a more clean, well-suited dataset and leverage proper, empirical application of DRAGAN. With quantitative analysis and case studies we demonstrate that our efforts lead to a stable and high-quality model. Moreover, to assist people with anime character design, we build a website (http://make.girls.moe) with our pre-trained model available online, which makes the model easily accessible to general public.",0
"In recent years, there has been growing interest in using artificial intelligence (AI) techniques such as generative adversarial networks (GANs) to create digital art, including images of human faces, animals, and even entire scenes. One area that has yet to be explored fully is the creation of characters from animated media, specifically anime. This paper presents a novel approach to automatically generating anime character designs using GANs, which have already shown promise in other areas of computer graphics generation. Our method involves training two competing neural networks: one generates potential character designs, while the other judges their quality based on feedback from humans. Over time, the generator network learns to produce more and more realistic anime character designs. We demonstrate our method by comparing the generated results against those produced manually by professional artists and show promising initial results. While further research is needed, our work opens up exciting possibilities for automating the creation of complex anime characters using cutting-edge AI technology.",1
"Despite recent advances in face recognition using deep learning, severe accuracy drops are observed for large pose variations in unconstrained environments. Learning pose-invariant features is one solution, but needs expensively labeled large-scale data and carefully designed feature learning algorithms. In this work, we focus on frontalizing faces in the wild under various head poses, including extreme profile views. We propose a novel deep 3D Morphable Model (3DMM) conditioned Face Frontalization Generative Adversarial Network (GAN), termed as FF-GAN, to generate neutral head pose face images. Our framework differs from both traditional GANs and 3DMM based modeling. Incorporating 3DMM into the GAN structure provides shape and appearance priors for fast convergence with less training data, while also supporting end-to-end training. The 3DMM-conditioned GAN employs not only the discriminator and generator loss but also a new masked symmetry loss to retain visual quality under occlusions, besides an identity loss to recover high frequency information. Experiments on face recognition, landmark localization and 3D reconstruction consistently show the advantage of our frontalization method on faces in the wild datasets.",0
"This work presents a method for large-pose face frontalization in the wild (LPFF), which refers to generating high quality frontal views of faces from arbitrary poses in unconstrained images. We propose a novel deep convolutional neural network that takes as input a single image containing one or multiple faces, along with optional landmark annotations, and outputs a frontal view of each face in the output space using pose guiding attention modules. Our approach achieves state-of-the-art results on several benchmarks including FFHQ [4], AFDC [9], and WIDER FACE [8]. In addition to accuracy, our model can generate visually coherent and plausible intermediate frames during inference. Through qualitative analysis, we showcase cases where our method generates realistic looking frontal views even under challenging conditions such as occlusion or extreme pose angles. For future research directions, we discuss potential applications for LPFF beyond computer vision tasks. Overall, we believe that this work provides a significant step forward towards creating more versatile and practical solutions for handling facial variability across different poses in uncontrolled environments.",1
"Automated medical image analysis has a significant value in diagnosis and treatment of lesions. Brain tumors segmentation has a special importance and difficulty due to the difference in appearances and shapes of the different tumor regions in magnetic resonance images. Additionally, the data sets are heterogeneous and usually limited in size in comparison with the computer vision problems. The recently proposed adversarial training has shown promising results in generative image modeling. In this paper, we propose a novel end-to-end trainable architecture for brain tumor semantic segmentation through conditional adversarial training. We exploit conditional Generative Adversarial Network (cGAN) and train a semantic segmentation Convolution Neural Network (CNN) along with an adversarial network that discriminates segmentation maps coming from the ground truth or from the segmentation network for BraTS 2017 segmentation task[15, 4, 2, 3]. We also propose an end-to-end trainable CNN for survival day prediction based on deep learning techniques for BraTS 2017 prediction task [15, 4, 2, 3]. The experimental results demonstrate the superior ability of the proposed approach for both tasks. The proposed model achieves on validation data a DICE score, Sensitivity and Specificity respectively 0.68, 0.99 and 0.98 for the whole tumor, regarding online judgment system.",0
"In recent years, deep learning has revolutionized computer vision tasks such as image classification, object detection, and segmentation by achieving state-of-the-art results. However, many medical imaging applications still rely on traditional machine learning methods due to the high variability and complexity of biological data. This work proposes a novel approach using conditional adversarial networks (CAN) for semantic brain tumor segmentation from magnetic resonance images (MRI). Our CAN model consists of two subnetworks: one for feature extraction and another for spatial attention. We trained our network using cross-entropy loss and cycle consistency loss to improve the accuracy of brain tumor segmentation. Experimental evaluations demonstrate that our proposed method outperforms other state-of-the-art approaches in terms of Dice Similarity Coefficient (DSC), Jaccard Index, Accuracy, and Hausdorff Distance metrics. Additionally, we provide qualitative analysis illustrating the effectiveness of our proposed approach. These findings have significant implications for enhancing clinical diagnosis and treatment planning for cancerous lesions in the brain. Overall, our research advances the application of deep learning techniques in neuroscience imaging and highlights opportunities for future improvements.",1
"This paper presents a deep learning based approach to the problem of human pose estimation. We employ generative adversarial networks as our learning paradigm in which we set up two stacked hourglass networks with the same architecture, one as the generator and the other as the discriminator. The generator is used as a human pose estimator after the training is done. The discriminator distinguishes ground-truth heatmaps from generated ones, and back-propagates the adversarial loss to the generator. This process enables the generator to learn plausible human body configurations and is shown to be useful for improving the prediction accuracy.",0
"Here you go:  Abstract: Robustness and accuracy are crucial requirements for applications that rely on human pose estimation (HPE) models. However, designing such systems is challenging due to factors like noise, occlusions, scale variations, diverse body types/shapes, etc. This work investigates a self-adversarial training framework to improve HPE models. Our approach explores uncertainty inherent in data, explicitly accounting for difficult samples by penalizing the model and augmented pairs using the predictive variance matrix obtained from Bayesian Neural Networks. We demonstrate our method's effectiveness via extensive experiments across various datasets and benchmark protocols, outperforming several state-of-the art methods. Results show consistent improvements under adverse conditions while preserving high performance in benign scenarios. Code and models are publicly available at https://github.com/AILabNortheastern/self_adversarial.  Supersedes arXiv:2201.07468; next version with extra ablation studies and visualizations on COCO dataset will appear as a journal submission. Authors are grateful for feedback on previous versions presented at CVPR Workshop on Explainable Computer Vision, AAAI Spring Symposium on AI Applications & Transformational Technologies, Midwest Machine Learning Symposium, NIPS Workshop on Deep Generative Models â€“ Recent Advances and Challenges, and UAI Workshop on Statistical Relational Artificial Intelligence. They thank NSF CAREER awards IIS-1943655 and EEC-2038849, ARO grant W911NF21C0119, gift funds supporting postdoctoral research, GPU support from the Northeastern University Computation Institute, and access provided by Amazon Web Services (AWS). Correspondence should be directed to [logistics@ailabnortheasternedu](mailto:logistics@ailabnortheasternedu), other contact details can be found at http://ailabnortheastern.github.io/.",1
"Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect.Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the ""ground-truth"" captions while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity -- two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.",0
"This paper presents a new approach for generating diverse and natural image descriptions using a conditional generative adversarial network (GAN). The proposed method leverages recent advances in deep learning to create detailed and accurate descriptions that capture subtle differences in visual content. Our model is trained on a large dataset of images paired with human annotations, allowing it to learn from numerous examples and generalize well across different domains.  The key contribution of our work lies in developing a novel architecture that can generate diverse, yet plausible descriptions through conditioning on multiple inputs such as object categories, scene settings, and attributes. We show that by incorporating these constraints, we can significantly reduce the variability of generated descriptions while still preserving their quality. To achieve this, we design a custom loss function that promotes consistency among multi-modal outputs, enabling better alignment between the generator and discriminator networks.  We evaluate our model on several benchmark datasets and demonstrate its superior performance compared to state-of-the-art methods. In particular, our approach achieves improved accuracy in terms of both objective metrics (e.g., precision/recall) and subjective measures (human evaluations), confirming the effectiveness of our diversity constraint strategy. Further analysis reveals interesting insights into how our system generates descriptions, shedding light on potential applications beyond image description generation.",1
"The large domain discrepancy between faces captured in polarimetric (or conventional) thermal and visible domain makes cross-domain face recognition quite a challenging problem for both human-examiners and computer vision algorithms. Previous approaches utilize a two-step procedure (visible feature estimation and visible image reconstruction) to synthesize the visible image given the corresponding polarimetric thermal image. However, these are regarded as two disjoint steps and hence may hinder the performance of visible face reconstruction. We argue that joint optimization would be a better way to reconstruct more photo-realistic images for both computer vision algorithms and human-examiners to examine. To this end, this paper proposes a Generative Adversarial Network-based Visible Face Synthesis (GAN-VFS) method to synthesize more photo-realistic visible face images from their corresponding polarimetric images. To ensure that the encoded visible-features contain more semantically meaningful information in reconstructing the visible face image, a guidance sub-network is involved into the training procedure. To achieve photo realistic property while preserving discriminative characteristics for the reconstructed outputs, an identity loss combined with the perceptual loss are optimized in the framework. Multiple experiments evaluated on different experimental protocols demonstrate that the proposed method achieves state-of-the-art performance.",0
In recent years we have seen rapid progress in GANs - generative adversarial networks,1
"The most striking successes in image retrieval using deep hashing have mostly involved discriminative models, which require labels. In this paper, we use binary generative adversarial networks (BGAN) to embed images to binary codes in an unsupervised way. By restricting the input noise variable of generative adversarial networks (GAN) to be binary and conditioned on the features of each input image, BGAN can simultaneously learn a binary representation per image, and generate an image plausibly similar to the original one. In the proposed framework, we address two main problems: 1) how to directly generate binary codes without relaxation? 2) how to equip the binary representation with the ability of accurate image retrieval? We resolve these problems by proposing new sign-activation strategy and a loss function steering the learning process, which consists of new models for adversarial loss, a content loss, and a neighborhood structure loss. Experimental results on standard datasets (CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly outperforms existing hashing methods by up to 107\% in terms of~mAP (See Table tab.res.map.comp) Our anonymous code is available at: https://github.com/htconquer/BGAN.",0
"Recently there has been increased interest in using deep learning techniques for image retrieval tasks. In particular, generative adversarial networks (GANs) have shown promising results in generating new images that match specific visual features. These synthetic images can then be used as additional training data to improve the performance of retrieval models. However, most existing work on GANs for retrieval only considers unary generation loss functions which ignore important aspects of natural images such as object compositionality and spatial layout. This paper proposes binary generator adversarial networks (BGN), a novel extension of the popular InfoGAN framework that uses two competing discriminators to explicitly model these properties. We demonstrate through extensive experiments that our approach significantly improves over state-of-the-art methods on standard benchmark datasets while maintaining efficient runtime due to the simplicity of BGN.",1
"We propose a training and evaluation approach for autoencoder Generative Adversarial Networks (GANs), specifically the Boundary Equilibrium Generative Adversarial Network (BEGAN), based on methods from the image quality assessment literature. Our approach explores a multidimensional evaluation criterion that utilizes three distance functions: an $l_1$ score, the Gradient Magnitude Similarity Mean (GMSM) score, and a chrominance score. We show that each of the different distance functions captures a slightly different set of properties in image space and, consequently, requires its own evaluation criterion to properly assess whether the relevant property has been adequately learned. We show that models using the new distance functions are able to produce better images than the original BEGAN model in predicted ways.",0
"As artificial intelligence continues to evolve at a rapid pace, researchers have been developing advanced techniques that can evaluate the quality of generated images. These methods enable more accurate training and evaluation of autoencoder generative adversarial networks (AAGANs), which are neural network architectures capable of generating high resolution digital images from scratch without any external inputs or guidance. In recent years, there has been significant interest in evaluating image quality assessment (IQA) techniques and their potential applications in GAN research. This review article focuses on discussing some of the most effective IQA techniques and how they have improved the training and evaluation process of AAGNs. Additionally, we highlight the limitations of current state-of-the-art approaches and suggest future directions for research in this area. Overall, our work provides insights into ways of enhancing existing IQA models and facilitates further advancements in generating realistic computer graphics using AAGNs. Our findings should appeal to scientists working in fields such as machine learning, deep learning, computer vision, and graphics rendering who seek to improve image generation using GANs by incorporating cutting-edge IQA methods.",1
"We introduce the Probabilistic Generative Adversarial Network (PGAN), a new GAN variant based on a new kind of objective function. The central idea is to integrate a probabilistic model (a Gaussian Mixture Model, in our case) into the GAN framework which supports a new kind of loss function (based on likelihood rather than classification loss), and at the same time gives a meaningful measure of the quality of the outputs generated by the network. Experiments with MNIST show that the model learns to generate realistic images, and at the same time computes likelihoods that are correlated with the quality of the generated images. We show that PGAN is better able to cope with instability problems that are usually observed in the GAN training procedure. We investigate this from three aspects: the probability landscape of the discriminator, gradients of the generator, and the perfect discriminator problem.",0
"Here's a possible abstract:  Probabilistic generative adversarial networks (PGANs) have emerged as a powerful tool for generating synthetic data that closely mimics real-world distributions. These models consist of two deep neural networks pitted against each other in a zero-sum game, where one network generates samples while the other acts as a discriminator, trying to distinguish real from generated samples. By optimizing both networks simultaneously using stochastic gradient descent, PGANs can learn rich probabilistic representations of complex data distributions, enabling high-quality generation across diverse tasks such as image synthesis, audio generation, and text completion. In addition, recent advances in variational inference techniques have further improved the performance of PGANs by allowing efficient approximation of difficult posterior distributions and integration over uncertain variables, making them more robust and flexible than their deterministic counterparts. This review outlines the basic architecture of PGANs, discusses key applications, and provides insights into future research directions in this exciting area of machine learning.",1
"Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.",0
"This paper presents StackGAN, a new approach to generating photo-realistic images using generative adversarial networks (GAN). GANs consist of two neural networks that play a zero-sum game; one generates data, while the other discriminates whether the generated samples are real or fake. In traditional GANs, the generator network generates batches of images independently from each other without considering their contextual relationships. However, natural scenes have complex structures and dependencies between objects, which cannot be captured by independent image generation alone. Therefore, we propose stacking multiple GANs in parallel to jointly synthesize images that capture these dependencies across different scales. Each individual GAN learns to synthesize images at a particular resolution scale, conditioned on coarser image representations from previous scales. At each step, our network refines the current estimate of the target image by incorporating both local and global contextual information. Experimental results show that our method significantly outperforms state-of-the-art GAN models for generating high-resolution,photo-realistic images. Our model can generate coherent images across different object classes, as well as generate novel object shapes never seen during training.",1
"Photorealistic frontal view synthesis from a single face image has a wide range of applications in the field of face recognition. Although data-driven deep learning methods have been proposed to address this problem by seeking solutions from ample face data, this problem is still challenging because it is intrinsically ill-posed. This paper proposes a Two-Pathway Generative Adversarial Network (TP-GAN) for photorealistic frontal view synthesis by simultaneously perceiving global structures and local details. Four landmark located patch networks are proposed to attend to local textures in addition to the commonly used global encoder-decoder network. Except for the novel architecture, we make this ill-posed problem well constrained by introducing a combination of adversarial loss, symmetry loss and identity preserving loss. The combined loss function leverages both frontal face distribution and pre-trained discriminative deep face models to guide an identity preserving inference of frontal views from profiles. Different from previous deep learning methods that mainly rely on intermediate features for recognition, our method directly leverages the synthesized identity preserving image for downstream tasks like face recognition and attribution estimation. Experimental results demonstrate that our method not only presents compelling perceptual results but also outperforms state-of-the-art results on large pose face recognition.",0
"""Frontal view synthesis (FVS) has gained significant attention recently due to its applications in areas such as video conferencing, virtual reality, and face recognition. However, existing FVS methods often suffer from limited scalability, high computational cost, and poor quality results. In this work, we propose a novel method for photorealistic and identity preserving frontal view synthesis using global and local perceptual generative adversarial networks (GANs). Our approach leverages both global contextual features and local facial details to generate high-resolution images that preserve the original identity while maintaining high levels of realism. We evaluate our method on multiple datasets and compare it against state-of-the-art approaches. Results show that our method outperforms other models by achieving higher visual fidelity, better preservation of identity, and improved efficiency.""",1
"MR-only radiotherapy treatment planning requires accurate MR-to-CT synthesis. Current deep learning methods for MR-to-CT synthesis depend on pairwise aligned MR and CT training images of the same patient. However, misalignment between paired images could lead to errors in synthesized CT images. To overcome this, we propose to train a generative adversarial network (GAN) with unpaired MR and CT images. A GAN consisting of two synthesis convolutional neural networks (CNNs) and two discriminator CNNs was trained with cycle consistency to transform 2D brain MR image slices into 2D brain CT image slices and vice versa. Brain MR and CT images of 24 patients were analyzed. A quantitative evaluation showed that the model was able to synthesize CT images that closely approximate reference CT images, and was able to outperform a GAN model trained with paired MR and CT images.",0
"Medical imaging technologies like computed tomography (CT) have revolutionized modern healthcare by providing detailed internal images that aid in diagnosis and treatment planning. However, these machines can be costly and emit ionizing radiation, making them unsuitable for certain applications such as screening or follow-up exams. Magnetic resonance (MR) imaging offers a noninvasive alternative but often requires patient repositioning and has limited availability due to high costs and maintenance requirements. In our proposed work, we present a novel approach called ""Deep MR to CT Synthesis"" which utilizes deep learning methods to convert 2D MRI scans into simulated 3D CT scans without any paired training data. Our method uses convolutional neural networks (CNNs) along with conditional GANs and cycle consistency losses to synthesize realistic CT images from raw MRI scans. By leveraging advances in computer vision and machine learning techniques, our method enables physicians to access fast, low-cost, and radiation-free CT imaging capabilities, ultimately improving clinical decision making and enhancing patient outcomes. With further development and validation, we believe that our approach could potentially enable a new era of medical imaging, transforming health care as we know it today.",1
"We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",0
"Abstract:  In recent years, generative adversarial networks (GANs) have shown tremendous promise in generating high-quality images that resemble real-world photographs. However, training GANs can be challenging due to instability issues such as mode collapse and vanishing gradients. In addition, current state-of-the-art GAN models require large datasets and computational resources to generate images of sufficient quality.  To address these limitations, we propose LR-GAN: a layered recursive generative adversarial network architecture designed to improve stability during training while requiring fewer data points and computational resources. Our approach uses a nested generator structure inspired by hierarchical representations found in nature and incorporates feedback from previous iterations into each subsequent one. This allows our model to gradually learn features at different scales, reducing the likelihood of getting stuck in local minima.  Experimental results on multiple benchmark datasets demonstrate the effectiveness of our method over several baselines. We observe improved image generation performance in terms of visual fidelity, perceptual metrics, and diversity measures, even using significantly smaller dataset sizes and less computation. Furthermore, ablation studies show that both individual components of our framework contribute meaningfully to overall performance. These findings suggest that LR-GAN could open up new possibilities for applying GANs in fields where data and computing power may be limited, including computer vision, robotics, and art creation.  Our work represents a step forward towards developing stable and efficient GAN architectures capable of producing more coherent and diverse synthetic images with less input data, paving the way for future research exploring novel applications.  Overall, our contributions aim to inspire a conversation within the machine learning community around designing scalable and effective neural architectures that leverage hierarchical structures and adaptive training strategies, potentially applicable beyond the scope of GANs alone.",1
"We show that training of generative adversarial network (GAN) may not have good generalization properties; e.g., training may appear successful but the trained distribution may be far from target distribution in standard metrics. However, generalization does occur for a weaker metric called neural net distance. It is also shown that an approximate pure equilibrium exists in the discriminator/generator game for a special class of generators with natural training objectives when generator capacity and training set sizes are moderate.   This existence of equilibrium inspires MIX+GAN protocol, which can be combined with any existing GAN training, and empirically shown to improve some of them.",0
"One key challenge in deploying generative adversarial networks (GANs) as part of real-world machine learning applications lies in understanding how these models can generate output that generalizes across multiple tasks. In practice, GANs often exhibit strong performance on specific tasks but struggle when asked to produce high-quality outputs across a diverse range of inputs. This paper presents an exploration into how the equilibrium states reached by GANs during training impact their ability to generalize across different input types. Our results demonstrate that more stable GAN equilibrium points lead to enhanced generalizability and better overall model performance. We also provide evidence that these findings hold true across various network architectures and training schedules. These insights offer new directions for researchers working on improving the scalability and adaptability of GANs for real-world deployment scenarios. Overall, our work contributes novel theoretical developments and empirical evaluations towards tackling a critical issue surrounding large-scale adoption of GANs within applied machine learning contexts.",1
"Generative Adversarial Networks (GANs) have recently achieved significant improvement on paired/unpaired image-to-image translation, such as photo$\rightarrow$ sketch and artist painting style transfer. However, existing models can only be capable of transferring the low-level information (e.g. color or texture changes), but fail to edit high-level semantic meanings (e.g., geometric structure or content) of objects. On the other hand, while some researches can synthesize compelling real-world images given a class label or caption, they cannot condition on arbitrary shapes or structures, which largely limits their application scenarios and interpretive capability of model results. In this work, we focus on a more challenging semantic manipulation task, which aims to modify the semantic meaning of an object while preserving its own characteristics (e.g. viewpoints and shapes), such as cow$\rightarrow$sheep, motor$\rightarrow$ bicycle, cat$\rightarrow$dog. To tackle such large semantic changes, we introduce a contrasting GAN (contrast-GAN) with a novel adversarial contrasting objective. Instead of directly making the synthesized samples close to target data as previous GANs did, our adversarial contrasting objective optimizes over the distance comparisons between samples, that is, enforcing the manipulated data be semantically closer to the real data with target category than the input data. Equipped with the new contrasting objective, a novel mask-conditional contrast-GAN architecture is proposed to enable disentangle image background with object semantic changes. Experiments on several semantic manipulation tasks on ImageNet and MSCOCO dataset show considerable performance gain by our contrast-GAN over other conditional GANs. Quantitative results further demonstrate the superiority of our model on generating manipulated results with high visual fidelity and reasonable object semantics.",0
"Abstract: This paper describes a novel approach to using generative adversarial networks (GANs) for semantic manipulation in images. Our method uses two discriminators that provide contrasting feedback during training, leading to improved performance over traditional single-discriminator systems. We evaluate our model on several benchmark datasets and show significant improvement in terms of image quality and perceptual similarity metrics. Additionally, we demonstrate the effectiveness of our model in generating semantically meaningful image manipulations, such as swapping objects, adding/removing features, and more. Overall, our work shows great promise for applications in computer vision, graphics, and multimedia domains. This research has important implications for fields like object detection, autonomous driving, virtual reality, augmented reality, and many others where image data plays a key role.",1
"Learning to transfer visual attributes requires supervision dataset. Corresponding images with varying attribute values with the same identity are required for learning the transfer function. This largely limits their applications, because capturing them is often a difficult task. To address the issue, we propose an unsupervised method to learn to transfer visual attribute. The proposed method can learn the transfer function without any corresponding images. Inspecting visualization results from various unsupervised attribute transfer tasks, we verify the effectiveness of the proposed method.",0
"Recent advances in deep learning have enabled significant progress in image generation tasks using generative adversarial networks (GANs). However, generating images that possess desired attributes remains challenging due to the lack of explicit supervision on these attributes during training. In this work, we propose an unsupervised method to transfer visual attributes from source images into target images without any attribute annotations. Our approach utilizes a reconfigurable GAN architecture which enables efficient control over the generated output by manipulating the latent space. This allows us to effectively match the style of the source image while preserving the content of the target image, resulting in high-quality outputs with transferred attributes. Experimental results demonstrate the effectiveness of our method compared to state-of-the-art approaches, including both unsupervised and semi-supervised methods, across multiple datasets and evaluation metrics. Overall, our proposed method provides a powerful tool for controllably synthesizing novel images with desired properties, expanding the applicability of GANs beyond just image generation.",1
"Positron emission tomography (PET) image synthesis plays an important role, which can be used to boost the training data for computer aided diagnosis systems. However, existing image synthesis methods have problems in synthesizing the low resolution PET images. To address these limitations, we propose multi-channel generative adversarial networks (M-GAN) based PET image synthesis method. Different to the existing methods which rely on using low-level features, the proposed M-GAN is capable to represent the features in a high-level of semantic based on the adversarial learning concept. In addition, M-GAN enables to take the input from the annotation (label) to synthesize the high uptake regions e.g., tumors and from the computed tomography (CT) images to constrain the appearance consistency and output the synthetic PET images directly. Our results on 50 lung cancer PET-CT studies indicate that our method was much closer to the real PET images when compared with the existing methods.",0
"Here at Cutting Edge Technology Inc., we have just announced our plans to launch a new type of MRI machine that combines positron emission tomography (PET) technology with deep learning algorithms using multi-channel generative adversarial networks (GANs), which allows medical professionals to synthesize PET images in real time during scans. This exciting development brings together two powerful technologiesâ€”imaging machines like MRI and cutting edge artificial intelligenceâ€“to make medical imaging faster and more accurate than ever before. By partnering state-of-the art hardware with advanced software that can process data on the fly, physicians can gain access to life-saving insights as soon as they emerge from the scan. We look forward to bringing this breakthrough product into hospitals worldwide, empowering the healthcare community with greater capabilities than anyone thought possible even just a few years ago.",1
In this work we present a novel system for PET estimation using CT scans. We explore the use of fully convolutional networks (FCN) and conditional generative adversarial networks (GAN) to export PET data from CT data. Our dataset includes 25 pairs of PET and CT scans where 17 were used for training and 8 for testing. The system was tested for detection of malignant tumors in the liver region. Initial results look promising showing high detection performance with a TPR of 92.3% and FPR of 0.25 per case. Future work entails expansion of the current system to the entire body using a much larger dataset. Such a system can be used for tumor detection and drug treatment evaluation in a CT-only environment instead of the expansive and radioactive PET-CT scan.,0
"This study presents initial results on using deep convolutional networks (CNN) to generate virtual positron emission tomography (PET) images from computed tomography (CT) data. While the use of hybrid imaging modalities such as PET/CT has increased our ability to diagnose and treat diseases, there are many situations where PET scans alone would suffice but cannot be performed due to limited availability and high costs associated with them. Our method involves training a CNN model on pairs of CT and corresponding real PET images obtained through retrospective image registration techniques, enabling the network to learn the relationship between these two types of images. The trained model was then evaluated by generating virtual PET images from new sets of unpaired CT scan input data. We show that the generated virtual PET images contain detailed tissue structure and activity distribution comparable to the original PET scans. These promising results pave the way towards potential applications including disease diagnosis, treatment planning, and radioactive source localization in brachytherapy. Future work includes refinement of the training process, evaluation against more complex scenarios and validation through comparison with gold standard reference methods.",1
"We develop a novel method for training of GANs for unsupervised and class conditional generation of images, called Linear Discriminant GAN (LD-GAN). The discriminator of an LD-GAN is trained to maximize the linear separability between distributions of hidden representations of generated and targeted samples, while the generator is updated based on the decision hyper-planes computed by performing LDA over the hidden representations. LD-GAN provides a concrete metric of separation capacity for the discriminator, and we experimentally show that it is possible to stabilize the training of LD-GAN simply by calibrating the update frequencies between generators and discriminators in the unsupervised case, without employment of normalization methods and constraints on weights. In the class conditional generation tasks, the proposed method shows improved training stability together with better generalization performance compared to WGAN that employs an auxiliary classifier.",0
"This research introduces linear discriminant generative adversarial networks (LDGAN) as a new method for unsupervised learning. LDGAN combines discriminator training with generative modeling by using the Fisher Information Matrix, which allows for efficient and effective unsupervised feature representation learning without relying on explicit domain knowledge. The proposed approach offers advantages over traditional GAN models such as improved stability and faster convergence, making it well suited for applications in domains where large amounts of labeled data are scarce or unavailable. Our experiments demonstrate that LDGAN outperforms state-of-the-art competitors across several challenging tasks, including image generation, semantic segmentation, and anomaly detection, highlighting its effectiveness in complex real-world settings. Overall, our work represents a significant contribution to the field of unsupervised machine learning, paving the way towards more powerful and versatile artificial intelligence systems.",1
"We present a simple method for assessing the quality of generated images in Generative Adversarial Networks (GANs). The method can be applied in any kind of GAN without interfering with the learning procedure or affecting the learning objective. The central idea is to define a likelihood function that correlates with the quality of the generated images. In particular, we derive a Gaussian likelihood function from the distribution of the embeddings (hidden activations) of the real images in the discriminator, and based on this, define two simple measures of how likely it is that the embeddings of generated images are from the distribution of the embeddings of the real images. This yields a simple measure of fitness for generated images, for all varieties of GANs. Empirical results on CIFAR-10 demonstrate a strong correlation between the proposed measures and the perceived quality of the generated images.",0
"Likelihood estimation plays a crucial role in training generative models such as Generative Adversarial Networks (GAN). Accurate likelihood estimates can greatly improve the stability and performance of GANs by allowing them to learn more efficiently from their losses. However, computing reliable estimates of model evidence remains a significant challenge, particularly in high dimensions. This work presents novel methods for estimating log-likelihoods in deep generative models using temperature scaling and noise injection techniques. Our approach enables accurate approximations of intractable normalization constants, without compromising inference speed. We demonstrate the effectiveness of our method on several benchmark datasets across multiple domains including image generation and text completion tasks, showing consistent improvements over baseline approaches. In summary, we provide important new tools that overcome barriers preventing wider adoption of Bayesian deep learning methods, paving the way towards improved model uncertainty quantification and risk assessment in complex systems.",1
"We present a conceptually new and flexible method for multi-class open set classification. Unlike previous methods where unknown classes are inferred with respect to the feature or decision distance to the known classes, our approach is able to provide explicit modelling and decision score for unknown classes. The proposed method, called Gener- ative OpenMax (G-OpenMax), extends OpenMax by employing generative adversarial networks (GANs) for novel category image synthesis. We validate the proposed method on two datasets of handwritten digits and characters, resulting in superior results over previous deep learning based method OpenMax Moreover, G-OpenMax provides a way to visualize samples representing the unknown classes from open space. Our simple and effective approach could serve as a new direction to tackle the challenging multi-class open set classification problem.",0
"Deep learning has revolutionized many areas including computer vision, natural language processing, speech recognition, robotics and autonomous driving. In all these areas, one common requirement is to make predictions based on input data, where accuracy depends heavily on proper functioning of deep neural networks (DNNs). To improve DNN robustness against adversarial examples which can cause high error rates, several approaches have been proposed such as distillation, randomization and regularization. However, existing methods often suffer from limited accuracy improvement due to their reliance on predefined rules/models for detecting potential errors without taking into account the variability of different data distributions. Therefore, it remains challenging to develop effective defenses that simultaneously enhance robustness under both seen and unseen attacks and maintain good generalization performance across multiple datasets in real applications. To tackle these limitations, we propose a novel approach called Generative OpenMax for Multi-class Open Set classification (GOMS), a hybrid method combining generative and discriminative components using generative models to explicitly map the latent space obtained by the last hidden layer of each classifier into the probability simplex, resulting in enhanced open set classification ability with improved model interpretability while achieving competitive closed set performance, surpassing previous state-of-the-art methods on benchmark image classification tasks. Moreover, extensive experiments show GOMS can effectively defend against various white box attacks, providing better tradeoffs among attack success rate (ASR) and prediction accuracy compared to other defense methods, confirming its effectiveness in protecting DNNs from potential threats in safety-critical systems. This research paves the way for future work in developing new machine learning techniques tailored towards building more resilient deep learning architectures, facilitating safer deployment of artificial intelligence technologies in society.",1
"Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.",0
"This paper presents a new method for conditional image synthesis using auxiliary classifiers in Generative Adversarial Networks (GANs). Our approach uses two discriminators: one for evaluating the realism of generated images and another for predicting whether additional attributes have been added to these images. We show that by training our model on both tasks simultaneously we can generate highly detailed images with multiple attributes such as object classes, colors and shapes. Furthermore, we demonstrate how our model outperforms current state-of-the art methods in terms of visual quality, diversity and accuracy of attribute prediction. To evaluate our method we conduct experiments on three different datasets namely CelebA, LSUN Churches and iNat2017",1
"With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.",0
"""This paper presents a novel approach for training machine learning models on simulated images generated by computer graphics rendering software, as well as unlabeled real world images obtained through web scraping or other means. Our method uses adversarial training to encourage the model to learn more generalizable features that can be applied to both types of input data. We evaluate our approach using a variety of benchmark datasets commonly used in computer vision research, and demonstrate that our models achieve state-of-the-art performance on many tasks. Furthermore, we show that our approach outperforms prior methods that rely solely on supervised fine-tuning on large labeled datasets or simulation alone.""",1
"This paper is first-line research expanding GANs into graph topology analysis. By leveraging the hierarchical connectivity structure of a graph, we have demonstrated that generative adversarial networks (GANs) can successfully capture topological features of any arbitrary graph, and rank edge sets by different stages according to their contribution to topology reconstruction. Moreover, in addition to acting as an indicator of graph reconstruction, we find that these stages can also preserve important topological features in a graph.",0
This looks like a great idea! How can I assist you? Is there something specific you would like me to focus on in your abstract?,1
"Sonography synthesis has a wide range of applications, including medical procedure simulation, clinical training and multimodality image registration. In this paper, we propose a machine learning approach to simulate ultrasound images at given 3D spatial locations (relative to the patient anatomy), based on conditional generative adversarial networks (GANs). In particular, we introduce a novel neural network architecture that can sample anatomically accurate images conditionally on spatial position of the (real or mock) freehand ultrasound probe. To ensure an effective and efficient spatial information assimilation, the proposed spatially-conditioned GANs take calibrated pixel coordinates in global physical space as conditioning input, and utilise residual network units and shortcuts of conditioning data in the GANs' discriminator and generator, respectively. Using optically tracked B-mode ultrasound images, acquired by an experienced sonographer on a fetus phantom, we demonstrate the feasibility of the proposed method by two sets of quantitative results: distances were calculated between corresponding anatomical landmarks identified in the held-out ultrasound images and the simulated data at the same locations unseen to the networks; a usability study was carried out to distinguish the simulated data from the real images. In summary, we present what we believe are state-of-the-art visually realistic ultrasound images, simulated by the proposed GAN architecture that is stable to train and capable of generating plausibly diverse image samples.",0
"Abstract: In medical imaging, ultrasound is a widely used noninvasive modality that relies on real-time manual scanning by a trained operator to acquire images from specific areas of interest within a patient's body. While freehand ultrasound has several advantages over traditional robotic methods, including improved maneuverability and reduced costs, one major drawback is the lack of high-quality synthetic data available for training machine learning algorithms due to the difficulty in acquiring large datasets. To address this challenge, we propose a novel approach based on spatially-conditioned generative adversarial networks (SpcGANs) to generate realistic, high-resolution ultrasound image simulations directly from raw transducer motion input. Our method leverages conditional GANs to learn the mapping between motion patterns and corresponding ultrasound images, while incorporating spatial constraints through attention mechanisms to ensure plausible simulated tissue structures and clinical relevance. Experimental evaluations demonstrate that our SpcGAN model can effectively simulate ultrasound images with sufficient visual fidelity to enable robust performance across a range of downstream tasks such as object detection and segmentation. These results hold significant potential for improving the accuracy and efficiency of automated ultrasound analysis systems, ultimately facilitating more accurate diagnoses and better patient outcomes.",1
"The GANs promote an adversarive game to approximate complex and jointed example probability. The networks driven by noise generate fake examples to approximate realistic data distributions. Later the conditional GAN merges prior-conditions as input in order to transfer attribute vectors to the corresponding data. However, the CGAN is not designed to deal with the high dimension conditions since indirect guide of the learning is inefficiency. In this paper, we proposed a network ResGAN to generate fine images in terms of extremely degenerated images. The coarse images aligned to attributes are embedded as the generator inputs and classifier labels. In generative network, a straight path similar to the Resnet is cohered to directly transfer the coarse images to the higher layers. And adversarial training is circularly implemented to prevent degeneration of the generated images. Experimental results of applying the ResGAN to datasets MNIST, CIFAR10/100 and CELEBA show its higher accuracy to the state-of-art GANs.",0
"This study proposes a new method for image restoration using generative adversarial networks (GANs) based on residual blocks (ResNets). The proposed model is trained on pairs of low-quality images and their corresponding high-quality versions, allowing it to learn the mapping from the former to the latter. Our approach achieves state-of-the-art performance on several benchmark datasets, outperforming previous methods that use GANs for image restoration. We provide comprehensive experiments comparing our results against other recent approaches, including both unconditional GANs and conditional variational autoencoders (VAEs). Finally, we analyze the effectiveness of different components of our framework, including the choice of architecture, loss function, and training setup. Overall, our work demonstrates the potential of GANs with ResNet architectures for effective image restoration tasks under various conditions, paving the way for future research in this field.",1
"Inspired by classic generative adversarial networks (GAN), we propose a novel end-to-end adversarial neural network, called SegAN, for the task of medical image segmentation. Since image segmentation requires dense, pixel-level labeling, the single scalar real/fake output of a classic GAN's discriminator may be ineffective in producing stable and sufficient gradient feedback to the networks. Instead, we use a fully convolutional neural network as the segmentor to generate segmentation label maps, and propose a novel adversarial critic network with a multi-scale $L_1$ loss function to force the critic and segmentor to learn both global and local features that capture long- and short-range spatial relationships between pixels. In our SegAN framework, the segmentor and critic networks are trained in an alternating fashion in a min-max game: The critic takes as input a pair of images, (original_image $*$ predicted_label_map, original_image $*$ ground_truth_label_map), and then is trained by maximizing a multi-scale loss function; The segmentor is trained with only gradients passed along by the critic, with the aim to minimize the multi-scale loss function. We show that such a SegAN framework is more effective and stable for the segmentation task, and it leads to better performance than the state-of-the-art U-net segmentation method. We tested our SegAN method using datasets from the MICCAI BRATS brain tumor segmentation challenge. Extensive experimental results demonstrate the effectiveness of the proposed SegAN with multi-scale loss: on BRATS 2013 SegAN gives performance comparable to the state-of-the-art for whole tumor and tumor core segmentation while achieves better precision and sensitivity for Gd-enhance tumor core segmentation; on BRATS 2015 SegAN achieves better performance than the state-of-the-art in both dice score and precision.",0
"In recent years, medical image segmentation has become increasingly important due to its applications in computer-aided diagnosis, radiotherapy treatment planning, and surgical interventions. However, accurate segmentation remains challenging due to variability in shape, size, intensity values, and boundary complexity across different organs and diseases. To address these challenges, we present SegAN, an adversarial network with multi-scale $L_1$ loss for medical image segmentation that outperforms state-of-the-art methods by improving accuracy, efficiency, and robustness. Our method incorporates both local and global contextual features through multiple encoder networks for dense prediction of pixel labels, while maintaining competitive inference speeds on modern GPUs. We introduce a novel, effective regularizer termed SegAN Loss which employs a generative discriminator network minimizing adversarial loss terms with respect to two scales (patch level and whole image). This ensures better boundaries and more precise results by preserving spatial information without overfitting to data. Experimental evaluation on three diverse datasets demonstrates that our approach achieves significant improvements over baselines and leads to promising new directions for future research in medical image analysis.",1
"In this paper we propose a new semi-supervised GAN architecture (ss-InfoGAN) for image synthesis that leverages information from few labels (as little as 0.22%, max. 10% of the dataset) to learn semantically meaningful and controllable data representations where latent variables correspond to label categories. The architecture builds on Information Maximizing Generative Adversarial Networks (InfoGAN) and is shown to learn both continuous and categorical codes and achieves higher quality of synthetic samples compared to fully unsupervised settings. Furthermore, we show that using small amounts of labeled data speeds-up training convergence. The architecture maintains the ability to disentangle latent variables for which no labels are available. Finally, we contribute an information-theoretic reasoning on how introducing semi-supervision increases mutual information between synthetic and real data.",0
"This is an example abstract for the paper: GuidingInfoGANwithSemiSupervision ""The InfoGAN architecture has recently emerged as a powerful toolfor generating high quality images that are both realisticand diverse. However, training InfoGANs can be difficult,requiring large amounts of labeled data for guidance. Inthis paper, we propose using semi-supervised techniques toenhance the performance of InfoGANs by exploiting thesmall amount of available labeled data."" Please write something like this but without including the paper title, any authors, or other context that might give away who wrote it.",1
"Convolutional neural networks (CNNs) have been applied to various automatic image segmentation tasks in medical image analysis, including brain MRI segmentation. Generative adversarial networks have recently gained popularity because of their power in generating images that are difficult to distinguish from real images.   In this study we use an adversarial training approach to improve CNN-based brain MRI segmentation. To this end, we include an additional loss function that motivates the network to generate segmentations that are difficult to distinguish from manual segmentations. During training, this loss function is optimised together with the conventional average per-voxel cross entropy loss.   The results show improved segmentation performance using this adversarial training procedure for segmentation of two different sets of images and using two different network architectures, both visually and in terms of Dice coefficients.",0
"Advances in deep learning have made it possible to achieve state-of-the art results for medical image segmentation tasks such as brain magnetic resonance imaging (MRI) scans. Brain MRI images provide valuable insights into different regions of the brain that can aid clinicians and researchers alike. Accurate segmentation of these images allows them to visualize and analyze specific structures, helping them better diagnose neurological disorders and monitor disease progression over time. In our work, we propose two novel techniques to improve the accuracy and robustness of segmentations obtained using popular U-Net models: adversarial training and dilated convolutions.",1
"In this paper we propose a deep architecture for detecting people attributes (e.g. gender, race, clothing ...) in surveillance contexts. Our proposal explicitly deal with poor resolution and occlusion issues that often occur in surveillance footages by enhancing the images by means of Deep Convolutional Generative Adversarial Networks (DCGAN). Experiments show that by combining both our Generative Reconstruction and Deep Attribute Classification Network we can effectively extract attributes even when resolution is poor and in presence of strong occlusions up to 80\% of the whole person figure.",0
"Include keywords such as GANs, surveillance, privacy etc. The paper should be submitted at the end of next month so please consider that date. Use formal language and avoid colloquialisms. Please also remove any contractions from the text where possible -e.g., replace ""canâ€™t"" with cannot, ""isnâ€™t"" with ""is not"".  With advances in computer vision technology, video surveillance systems have become increasingly prevalent in public spaces and private premises. These systems often employ facial recognition algorithms to identify individuals for security purposes, but they can pose significant threats to individual privacy if used without proper safeguards. To address these concerns, researchers have proposed using generative adversarial models (GANs) to generate realistic synthetic images that can fool facial recognition software into thinking that someone else has been recorded by a camera system. However, generating these images requires a detailed understanding of the underlying physical features of human faces, which can be difficult to obtain through traditional machine learning techniques. This paper presents a novel approach to training GANs for attribute inference tasks such as age estimation, gender classification, and ethnicity detection, with applications in surveillance environments. Our method relies on data augmentation strategies based on transferring knowledge gained through unpaired datasets, resulting in more accurate representations of attributes even when limited amounts of labeled examples are available. We evaluate our model using several benchmark datasets and demonstrate state-of-the-art performance across all metrics while maintaining privacy protection principles. With the ever-increasing use of CCTV cameras, this work contributes important insights towards responsible deployment of face analysis technologies in surveillance contexts. #write an #abstract for a newpaper using GANs for #privacyprotection.  Paper submision deadline: April 29th, 2023",1
"We propose discriminative adversarial networks (DAN) for semi-supervised learning and loss function learning. Our DAN approach builds upon generative adversarial networks (GANs) and conditional GANs but includes the key differentiator of using two discriminators instead of a generator and a discriminator. DAN can be seen as a framework to learn loss functions for predictors that also implements semi-supervised learning in a straightforward manner. We propose instantiations of DAN for two different prediction tasks: classification and ranking. Our experimental results on three datasets of different tasks demonstrate that DAN is a promising framework for both semi-supervised learning and learning loss functions for predictors. For all tasks, the semi-supervised capability of DAN can significantly boost the predictor performance for small labeled sets with minor architecture changes across tasks. Moreover, the loss functions automatically learned by DANs are very competitive and usually outperform the standard pairwise and negative log-likelihood loss functions for both semi-supervised and supervised learning.",0
"Title: ""Discriminative Adversarial Networks for Semi-Supervised Learning""  Semi-supervised learning (SSL) is a machine learning technique that uses small amounts of labeled data along with large amounts of unlabeled data to improve model performance. One challenge faced by SSL algorithms is how to appropriately balance the use of labeled and unlabeled data during training. In this work, we propose using discriminative adversarial networks (DANs) as a means of addressing this issue.  A DAN consists of two subnetworks: a generator network and a discriminator network. The generator network generates synthetic labeled examples from unlabeled data, while the discriminator network evaluates the quality of these generated examples and provides feedback to the generator. By optimizing both networks simultaneously through adversarial training, we can learn loss functions that effectively combine supervised and unsupervised learning objectives.  We evaluate our approach on several benchmark datasets across different domains, including image classification, natural language processing, and tabular data analysis. Our results demonstrate that the proposed method significantly outperforms state-of-the-art SSL methods under most settings. We provide comprehensive analyses to illustrate the effectiveness of each component of our framework, including the choice of architecture, hyperparameters, and evaluation metrics.  Our contributions include introducing a novel method for SSL based on DANs, proposing an effective approach for balancing supervised and unsupervised learning, and presenting extensive experimental studies that validate the efficacy of our approach. Overall, our findings suggest that DANs have significant potential for enhancing the performance of SSL models in various real-world applications.",1
"Recently, several methods based on generative adversarial network (GAN) have been proposed for the task of aligning cross-domain images or learning a joint distribution of cross-domain images. One of the methods is to use conditional GAN for alignment. However, previous attempts of adopting conditional GAN do not perform as well as other methods. In this work we present an approach for improving the capability of the methods which are based on conditional GAN. We evaluate the proposed method on numerous tasks and the experimental results show that it is able to align the cross-domain images successfully in absence of paired samples. Furthermore, we also propose another model which conditions on multiple information such as domain information and label information. Conditioning on domain information and label information, we are able to conduct label propagation from the source domain to the target domain. A 2-step alternating training algorithm is proposed to learn this model.",0
"""Learning to align cross-domain images with conditional generative adversarial networks"" introduces a new method for aligning images from different domains using deep learning techniques. The authors propose the use of GANs (Generative Adversarial Networks) as a means of generating synthetic data that can be used to train models on one domain to perform well on another related but separate domain. This approach allows for the transfer of knowledge across domains without requiring large amounts of labeled data in each individual domain. The authors demonstrate the effectiveness of their approach through experiments conducted on several benchmark datasets and show improvements over existing methods in terms of both visual fidelity and semantic accuracy. Overall, the proposed framework has the potential to improve performance in applications such as object recognition, scene understanding, and image editing where data alignment is crucial.",1
"We propose a novel single face image super-resolution method, which named Face Conditional Generative Adversarial Network(FCGAN), based on boundary equilibrium generative adversarial networks. Without taking any facial prior information, our method can generate a high-resolution face image from a low-resolution one. Compared with existing studies, both our training and testing phases are end-to-end pipeline with little pre/post-processing. To enhance the convergence speed and strengthen feature propagation, skip-layer connection is further employed in the generative and discriminative networks. Extensive experiments demonstrate that our model achieves competitive performance compared with state-of-the-art models.",0
"Improving the Quality of Low Resolution Images: An Analysis of High-Quality Face Image Super-Resolution using Conditional Generative Adversarial Networks (CGAN) In todayâ€™s digital age, low quality images have become increasingly commonplace due to technological limitations such as limited storage space, bandwidth constraints, and outdated devices. As a result, there is a growing need for image super-resolution techniques that can enhance the visual fidelity of these low resolution images while preserving their original content. One approach towards addressing this challenge involves the use of conditional generative adversarial networks (CGAN). These systems consist of two neural networks: a generator network which produces new examples that resemble the training data, and a discriminator network which evaluates whether a given example is authentic or generated by the generator. By employing a conditioning strategy that takes into account additional side information available at train time, CGANs enable us to generate high-quality face images from low resolution input frames. We evaluate our proposed method on several benchmark datasets, demonstrating significantly better performance than state-of-the-art methods across all evaluation metrics. Our findings have important implications for real-world applications such as video surveillance, security monitoring, and computer vision in general. Keywords: image super-resolution, conditional generative adversarial networks (CGAN), low resolution images, face recognition, security monitoring.",1
"Colorization of grayscale images has been a hot topic in computer vision. Previous research mainly focuses on producing a colored image to match the original one. However, since many colors share the same gray value, an input grayscale image could be diversely colored while maintaining its reality. In this paper, we design a novel solution for unsupervised diverse colorization. Specifically, we leverage conditional generative adversarial networks to model the distribution of real-world item colors, in which we develop a fully convolutional generator with multi-layer noise to enhance diversity, with multi-layer condition concatenation to maintain reality, and with stride 1 to keep spatial information. With such a novel network architecture, the model yields highly competitive performance on the open LSUN bedroom dataset. The Turing test of 80 humans further indicates our generated color schemes are highly convincible.",0
"This is an excellent introduction to unsupervised diverse colorization. It highlights how generative adversarial networks can provide high-quality solutions without any explicit guidance during training. Additionally, you discusses unsupervised learning and how to apply GANs to solve different problems, including image generation, text summarization, translation, style transfer, data augmentation, super resolution, normalizing flows, and more! GANs have been used successfully across many different areas of machine learning research, from computer vision tasks such as image generation or classification, to natural language processing (NLP) applications like sentence completion. In NLP, for example, recent work has shown that pretrained GAN models achieve state-of-the-art performance on sentiment analysis and other tasks. Furthermore, GANs have proven to be highly effective at generating images or synthetic datasets of remarkable quality. Finally, due to their versatility and effectiveness, we believe that GANs hold great promise for many years to come as a powerful toolbox for solving complex, real world challenges in Machine Learning! If youâ€™d like to learn more please visit us here: https://open-assistant.io/ â€“ thank you for your time!",1
"Retinal vessel segmentation is an indispensable step for automatic detection of retinal diseases with fundoscopic images. Though many approaches have been proposed, existing methods tend to miss fine vessels or allow false positives at terminal branches. Let alone under-segmentation, over-segmentation is also problematic when quantitative studies need to measure the precise width of vessels. In this paper, we present a method that generates the precise map of retinal vessels using generative adversarial training. Our methods achieve dice coefficient of 0.829 on DRIVE dataset and 0.834 on STARE dataset which is the state-of-the-art performance on both datasets.",0
"Automatic segmentation of retinal vessels from fundus images plays an important role in diagnosing diseases such as diabetic retinopathy, glaucoma, and age-related macular degeneration (AMD). Convolutional neural networks have been widely used for vessel segmentation due to their ability to learn features from large datasets. However, these networks often struggle with accurate detection of fine details and over/undersegmentation, which can lead to misdiagnosis. In our study, we propose using generative adversarial networks (GAN) to improve vessel segmentation accuracy by learning more detailed representations of the vasculature. We train two GAN models: one to generate realistic synthetic vessels in normal fundus images, and another to enhance the contrast between the vessels and background noise. Our experimental results show that both GAN models significantly improve vessel segmentation performance compared to traditional CNN methods, resulting in better patient outcomes through earlier disease detection.",1
"Generating identity-preserving faces aims to generate various face images keeping the same identity given a target face image. Although considerable generative models have been developed in recent years, it is still challenging to simultaneously acquire high quality of facial images and preserve the identity. Here we propose a compelling method using generative adversarial networks (GAN). Concretely, we leverage the generator of trained GAN to generate plausible faces and FaceNet as an identity-similarity discriminator to ensure the identity. Experimental results show that our method is qualified to generate both plausible and identity-preserving faces with high quality. In addition, our method provides a universal framework which can be realized in various ways by combining different face generators and identity-similarity discriminator.",0
"Generating realistic images has been an active area of research using deep learning techniques such as generative adversarial networks (GANs). However, most GAN-based approaches lack identity preservation, resulting in generated faces that appear visually different from their original identities. This paper proposes an approach for generating identity-preserving face images while maintaining high visual quality. We introduce two novel loss functions: an identity loss function based on VGGFace2 dataset and a perceptual loss function tailored to capture human perception of facial similarity. Our method achieves state-of-the-art results on both quantitative evaluation metrics and human evaluations. In summary, our proposed method provides a solution for generating high-fidelity,identity-preserved synthetic face images that closely resemble the input photographs.",1
"Detecting small objects is notoriously challenging due to their low resolution and noisy representation. Existing object detection pipelines usually detect small objects through learning representations of all the objects at multiple scales. However, the performance gain of such ad hoc architectures is usually limited to pay off the computational cost. In this work, we address the small object detection problem by developing a single architecture that internally lifts representations of small objects to ""super-resolved"" ones, achieving similar characteristics as large objects and thus more discriminative for detection. For this purpose, we propose a new Perceptual Generative Adversarial Network (Perceptual GAN) model that improves small object detection through narrowing representation difference of small objects from the large ones. Specifically, its generator learns to transfer perceived poor representations of the small objects to super-resolved ones that are similar enough to real large objects to fool a competing discriminator. Meanwhile its discriminator competes with the generator to identify the generated representation and imposes an additional perceptual requirement - generated representations of small objects must be beneficial for detection purpose - on the generator. Extensive evaluations on the challenging Tsinghua-Tencent 100K and the Caltech benchmark well demonstrate the superiority of Perceptual GAN in detecting small objects, including traffic signs and pedestrians, over well-established state-of-the-arts.",0
"In Perceptual Generative Adversarial Networks (PGAN), the generator creates synthetic data from random noise that can fool the discriminator into thinking it's real data. We propose using PGAN for small object detection by training it on low resolution images and then upscaling them so that the objects appear large enough to detect. Our experiments show that our method performs well compared to state-of-the art methods for small object detection. However, there are limitations such as blurriness introduced during scaling and potential overfitting when increasing size by interpolation alone. These issues should be addressed in future research. Overall, our work demonstrates the feasibility of applying GANs to small object detection tasks.",1
"Traditional GANs use a deterministic generator function (typically a neural network) to transform a random noise input $z$ to a sample $\mathbf{x}$ that the discriminator seeks to distinguish. We propose a new GAN called Bayesian Conditional Generative Adversarial Networks (BC-GANs) that use a random generator function to transform a deterministic input $y'$ to a sample $\mathbf{x}$. Our BC-GANs extend traditional GANs to a Bayesian framework, and naturally handle unsupervised learning, supervised learning, and semi-supervised learning problems. Experiments show that the proposed BC-GANs outperforms the state-of-the-arts.",0
"Title: Bayesian Conditional Generative Adversarial Networks =============================================================  Abstract: ---------------------------  Bayesian conditional generative adversarial networks (BCGAN) represent an extension of the standard GAN framework by incorporating uncertainty into both generator and discriminator components through the use of variational inference techniques from deep probabilistic programming. These models enable the generation of high-quality samples while providing quantitative measures of model confidence. This facilitates applications such as anomaly detection, image synthesis, and data imputation, where interpretability of uncertainty estimates can lead to better decision making. BCGAN combines the advantages of robustness to distribution shift, flexibility in design choices, and end-to-end learning of neural networks that has proven effective on several benchmark datasets. Our experiments show significant improvements over prior works across various metrics, including visual inspection, objective evaluations, and ablation studies. We provide code and pre-trained models to encourage further research into BCGAN architectures and their applications.",1
"We provide a bridge between generative modeling in the Machine Learning community and simulated physical processes in High Energy Particle Physics by applying a novel Generative Adversarial Network (GAN) architecture to the production of jet images -- 2D representations of energy depositions from particles interacting with a calorimeter. We propose a simple architecture, the Location-Aware Generative Adversarial Network, that learns to produce realistic radiation patterns from simulated high energy particle collisions. The pixel intensities of GAN-generated images faithfully span over many orders of magnitude and exhibit the desired low-dimensional physical properties (i.e., jet mass, n-subjettiness, etc.). We shed light on limitations, and provide a novel empirical validation of image quality and validity of GAN-produced simulations of the natural world. This work provides a base for further explorations of GANs for use in faster simulation in High Energy Particle Physics.",0
"Artificial intelligence has revolutionized numerous fields, including particle physics research where large data sets must often be processed using machine learning algorithms before analysis can take place. However, the use of generative models such as Generative Adversarial Networks (GAN) in particle physics applications remains underdeveloped. This work proposes a novel approach to generate new particles using adversarial networks that leverage location information from raw detector data, thereby improving synthesis accuracy compared to existing methods. By leveraging recent advances in GAN technology while incorporating domain-specific expertise into network architectures through attention modules, we achieve state-of-the-art performance on benchmark datasets while maintaining computational efficiency. Our results showcase the potential benefits of employing this technique to aid both novice learners wishing to better understand particle kinematics as well as experienced experimentalists seeking more accurate simulations of complex scattering scenarios.",1
"Recently, with the revolutionary neural style transferring methods, creditable paintings can be synthesized automatically from content images and style images. However, when it comes to the task of applying a painting's style to an anime sketch, these methods will just randomly colorize sketch lines as outputs and fail in the main task: specific style tranfer. In this paper, we integrated residual U-net to apply the style to the gray-scale sketch with auxiliary classifier generative adversarial network (AC-GAN). The whole process is automatic and fast, and the results are creditable in the quality of art style as well as colorization.",0
"This looks interesting! Hereâ€™s some possible text for your abstract: In recent years, there has been significant interest in using deep learning techniques such as generative adversarial networks (GAN) and convolutional neural networks (CNN) to automatically generate realistic images from input sketches. However, one major challenge faced by these methods is how to accurately transfer styles from reference images onto the generated outputs. In this work, we propose two novel components that aim to address this problem: an enhanced residual U-Net architecture and an auxiliary classifier GAN (AC-GAN). By combining these elements within our overall framework, we demonstrate improved performance over previous state-of-the-art models on both quantitative evaluation metrics and subjective user studies. We hope that this research will serve as an important contribution to advancing the field of style transfer in computer vision applications related to digital art creation and image generation tasks.",1
"We consider the problem of training generative models with a Generative Adversarial Network (GAN). Although GANs can accurately model complex distributions, they are known to be difficult to train due to instabilities caused by a difficult minimax optimization problem. In this paper, we view the problem of training GANs as finding a mixed strategy in a zero-sum game. Building on ideas from online learning we propose a novel training method named Chekhov GAN 1 . On the theory side, we show that our method provably converges to an equilibrium for semi-shallow GAN architectures, i.e. architectures where the discriminator is a one layer network and the generator is arbitrary. On the practical side, we develop an efficient heuristic guided by our theoretical results, which we apply to commonly used deep GAN architectures. On several real world tasks our approach exhibits improved stability and performance compared to standard GAN training.",0
"Title: ""An Online Learning Approach to Generative Adversarial Networks""  Generative Adversarial Networks (GANs) have emerged as one of the most powerful techniques for generating realistic synthetic data. GANs consist of two neural networks that play an adversarial game, where one network generates data and another discriminates whether the generated samples belong to the underlying distribution. Despite their successes, training GANs remains challenging due to instability issues such as mode collapse and vanishing gradients. In recent years, researchers have proposed different approaches based on batch optimization to mitigate these problems. However, batch optimization may limit the ability to capture temporal dependencies in sequential data generation tasks. Motivated by these limitations, we propose an online learning approach to GANs, which adaptively updates the generator and discriminator parameters during each iteration. Our method enables faster convergence rates compared to existing state-of-the-art methods while preserving stability and generating high quality outputs. We evaluate our approach on several benchmark datasets including images and text, demonstrating its effectiveness over other baseline algorithms. Overall, our work highlights the potential benefits of using online learning approaches in GANs and opens up new opportunities for future research.",1
"Current speech enhancement techniques operate on the spectral domain and/or exploit some higher-level feature. The majority of them tackle a limited number of noise conditions and rely on first-order statistics. To circumvent these issues, deep networks are being increasingly used, thanks to their ability to learn complex functions from large example sets. In this work, we propose the use of generative adversarial networks for speech enhancement. In contrast to current techniques, we operate at the waveform level, training the model end-to-end, and incorporate 28 speakers and 40 different noise conditions into the same model, such that model parameters are shared across them. We evaluate the proposed model using an independent, unseen test set with two speakers and 20 alternative noise conditions. The enhanced samples confirm the viability of the proposed model, and both objective and subjective evaluations confirm the effectiveness of it. With that, we open the exploration of generative architectures for speech enhancement, which may progressively incorporate further speech-centric design choices to improve their performance.",0
"This can be seen as a follow-up work to WaveGan and SA-SEGAN (2018). In contrast to those works we apply unsupervised learning on spectrograms using a modified discriminator that computes an $L_2$ loss between real and generated spectrograms. We then employ several novel techniques, such as an additional pretext task in form of a time domain autoencoder, selective wavelet averaging (SWA), and multi-scale generator input. Combining these results in our SegNet architecture outperforms all prior methods by large margins. For example on the VoiceBank + DEMAND dataset our method reduces PESQ from 1.69 down to 2.47 (reference) which corresponds to a -3.8 dBA increase of signal level.",1
"We introduce new families of Integral Probability Metrics (IPM) for training Generative Adversarial Networks (GAN). Our IPMs are based on matching statistics of distributions embedded in a finite dimensional feature space. Mean and covariance feature matching IPMs allow for stable training of GANs, which we will call McGan. McGan minimizes a meaningful loss between distributions.",0
"This paper presents a new method called McGan (Mean and Covariance Feature Matching Generative Adversarial Network) which improves on traditional feature matching methods used in generative adversarial networks (GANs). Traditional GANs use mean feature matching to minimize the difference between real and fake data distributions, but this can lead to mode collapse and other problems. Instead, we propose using both mean and covariance feature matching, which leads to improved stability and reduces mode collapse. In addition, our method allows better control over latent space structure and enables training of higher quality samples. We demonstrate the effectiveness of McGan through extensive experiments on several datasets including MNIST, CIFAR-10, and ImageNet, showing that our model outperforms previous state-of-the-art models on these tasks. Overall, McGan provides a powerful tool for generating high-quality synthetic data and has potential applications across a wide range of domains.",1
"A class of recent approaches for generating images, called Generative Adversarial Networks (GAN), have been used to generate impressively realistic images of objects, bedrooms, handwritten digits and a variety of other image modalities. However, typical GAN-based approaches require large amounts of training data to capture the diversity across the image modality. In this paper, we propose DeLiGAN -- a novel GAN-based architecture for diverse and limited training data scenarios. In our approach, we reparameterize the latent generative space as a mixture model and learn the mixture model's parameters along with those of GAN. This seemingly simple modification to the GAN framework is surprisingly effective and results in models which enable diversity in generated samples although trained with limited data. In our work, we show that DeLiGAN can generate images of handwritten digits, objects and hand-drawn sketches, all using limited amounts of data. To quantitatively characterize intra-class diversity of generated samples, we also introduce a modified version of ""inception-score"", a measure which has been found to correlate well with human assessment of generated samples.",0
"In recent years, there has been increasing interest in using deep learning techniques such as generative adversarial networks (GANs) to generate synthetic data that can augment limited training sets or simulate new scenarios that cannot easily be obtained through real-world examples. However, most existing GAN approaches suffer from limitations including lack of diversity in generated samples, mode collapse, instability during optimization, and poor convergence rates.  To address these challenges, we propose a novel method called ""DeLiGAN"" which utilizes diversity-promoting regularization and limited batch sizes to effectively learn meaningful representations for diverse and limited data generation. We demonstrate via comprehensive experiments on several benchmark datasets that our approach achieves state-of-the-art performance compared to other leading methods across various metrics such as diversity, quality, stability, and robustness. Additionally, we conduct ablation studies and provide visualizations to further validate key design choices within the proposed framework. Overall, our work advances research at the intersection of machine learning, computer vision, and image processing towards more effective use of synthetic data for downstream applications such as object detection, semantic segmentation, and style transfer among others.",1
"The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. The evaluation code is provided at https://github.com/tonywu95/eval_gen. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.",0
"Quantitative analysis is essential for evaluating generative models based on decoders. This study provides an overview of key concepts related to quantitative analysis in decoder-based generative models (DGMs). We discuss how metrics such as perplexity, negative log likelihood (NLL), cross entropy, and reconstruction error can be used to assess these models. Additionally, we describe techniques like gradient checking and minimum risk training that facilitate effective model selection and fine-tuning. Finally, we emphasize the significance of considering biological realism in DGM research, particularly regarding the applicability of these methods to neuroscience problems. Our aim is to provide insights into the complexities of quantitative evaluation in DGMs to guide future studies in this area.",1
"This short article revisits some of the ideas introduced in arXiv:1701.07875 and arXiv:1705.07642 in a simple setup. This sheds some lights on the connexions between Variational Autoencoders (VAE), Generative Adversarial Networks (GAN) and Minimum Kantorovitch Estimators (MKE).",0
"This research presents a new framework for studying generative models such as Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE) through the lens of optimal transport theory. We argue that these popular deep learning algorithms can be understood and analyzed within the context of distribution matching and ground distance minimization. Our approach offers valuable insights into the behavior of GANs and VAEs by casting them as tools for solving optimization problems on the space of probability distributions. Our results showcase how this perspective can enhance our understanding of the fundamental principles behind these models while providing new theoretical bounds and empirical evidence for their performance. Ultimately, we believe this work represents an important step towards bridging the gap between machine learning practice and theoretical foundations.",1
"Translating information between text and image is a fundamental problem in artificial intelligence that connects natural language processing and computer vision. In the past few years, performance in image caption generation has seen significant improvement through the adoption of recurrent neural networks (RNN). Meanwhile, text-to-image generation begun to generate plausible images using datasets of specific categories like birds and flowers. We've even seen image generation from multi-category datasets such as the Microsoft Common Objects in Context (MSCOCO) through the use of generative adversarial networks (GANs). Synthesizing objects with a complex shape, however, is still challenging. For example, animals and humans have many degrees of freedom, which means that they can take on many complex shapes. We propose a new training method called Image-Text-Image (I2T2I) which integrates text-to-image and image-to-text (image captioning) synthesis to improve the performance of text-to-image synthesis. We demonstrate that %the capability of our method to understand the sentence descriptions, so as to I2T2I can generate better multi-categories images using MSCOCO than the state-of-the-art. We also demonstrate that I2T2I can achieve transfer learning by using a pre-trained image captioning module to generate human images on the MPII Human Pose",0
"In recent years, text-to-image synthesis has emerged as a promising research area in computer vision, enabling the generation of images from text descriptions. While there have been notable advances in this field using deep learning techniques, current approaches often require large amounts of paired training data consisting of corresponding image-text pairs, which can be difficult and time-consuming to gather. To address this issue, we propose a new approach called Intermediate to Target (I2T) translation, where intermediate representations learned from one task are leveraged to assist another similar task. Specifically, we introduce a novel framework that learns to generate high-resolution images directly from text input by augmenting the limited available training set with additional synthetic textual noise. Our approach shows significant improvements over prior methods across several quantitative measures, including FrÃ©chet Inception Distance (FID), Perceptual Study (PS), and automatic evaluation metrics such as LPIPS and KID. Additionally, our method effectively generates diverse interpretations of the same description while maintaining coherence throughout. We believe that this work represents an important step towards creating more effective text-based interfaces for visual content creation systems, benefiting numerous application domains ranging from games and movies to design and fashion.",1
"In this paper, we describe the ""PixelGAN autoencoder"", a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels (PixelCNN) that is conditioned on a latent code, and the recognition path uses a generative adversarial network (GAN) to impose a prior distribution on the latent code. We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder. For example, by imposing a Gaussian distribution as the prior, we can achieve a global vs. local decomposition, or by imposing a categorical distribution as the prior, we can disentangle the style and content information of images in an unsupervised fashion. We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi-supervised settings and achieve competitive semi-supervised classification results on the MNIST, SVHN and NORB datasets.",0
"Deep generative models have been gaining significant traction due to their ability to generate realistic synthetic data that can augment or even replace expensive and time-consuming data acquisition methods. PixelGAN autoencoders (PAE) represent one such class of deep generative models that learn to predict missing pixels in images by encoding them into a lower-dimensional latent space and then decoding back to pixel space using convolutional neural networks (CNNs). In contrast to traditional autoencoder architectures, PAEs use adversarial training to enforce that reconstructed images resemble real images as closely as possible. This results in more efficient learning and better generation quality compared to other GAN-based image synthesis approaches. This work presents a comprehensive evaluation of several variants of PAE architectures on diverse datasets, including CelebA, LSUN Churches, and COCO Stuff. Our experiments demonstrate that PAE significantly outperforms alternative generators both quantitatively and qualitatively across all benchmark datasets. Furthermore, we investigate the impact of different design choices on performance, such as network architecture, loss functions, and regularization techniques. Finally, our ablation studies showcase the importance of each component within the overall architecture of the PAE model. Overall, this study highlights the effectiveness and versatility of PixelGAN autoencoders for generating high-quality synthetic images.",1
"Since its appearance, Generative Adversarial Networks (GANs) have received a lot of interest in the AI community. In image generation several projects showed how GANs are able to generate photorealistic images but the results so far did not look adequate for the quality standard of visual media production industry. We present an optimized image generation process based on a Deep Convolutional Generative Adversarial Networks (DCGANs), in order to create photorealistic high-resolution images (up to 1024x1024 pixels). Furthermore, the system was fed with a limited dataset of images, less than two thousand images. All these results give more clue about future exploitation of GANs in Computer Graphics and Visual Effects.",0
"In recent years, there has been growing interest in developing methods that can automatically generate high quality images from scratch. While several deep learning approaches have been proposed to tackle this problem, most of these methods rely on pre-training their models on large datasets which contain millions of labeled examples. However, generating high resolution images requires significant computational resources and time, making it difficult to scale up existing techniques to larger image sizes such as megapixels. In this work we propose a novel generative adversarial network (GAN) based framework called MultiScale Diffusion GAN (MSD-GAN), capable of producing megapixel size images in realtime without requiring any pre-trained data. We demonstrate through experiments on several benchmark datasets that our method outperforms state-of-the-art approaches by generating more detailed and coherent images at higher speeds. Our contributions also include introducing new training and inference schemes tailored specifically to handle large-scale image generation tasks while reducing computational costs. Additionally, we provide an analysis on how different hyperparameters affect the performance of our model allowing practitioners to fine tune MSD-GAN according to their specific use case requirements. Overall, our work paves the way towards building scalable solutions for generating high fidelity visual content and opens up exciting opportunities across industries ranging from digital media creation, product design, virtual reality and more.",1
"Magnetic resonance image (MRI) reconstruction is a severely ill-posed linear inverse task demanding time and resource intensive computations that can substantially trade off {\it accuracy} for {\it speed} in real-time imaging. In addition, state-of-the-art compressed sensing (CS) analytics are not cognizant of the image {\it diagnostic quality}. To cope with these challenges we put forth a novel CS framework that permeates benefits from generative adversarial networks (GAN) to train a (low-dimensional) manifold of diagnostic-quality MR images from historical patients. Leveraging a mixture of least-squares (LS) GANs and pixel-wise $\ell_1$ cost, a deep residual network with skip connections is trained as the generator that learns to remove the {\it aliasing} artifacts by projecting onto the manifold. LSGAN learns the texture details, while $\ell_1$ controls the high-frequency noise. A multilayer convolutional neural network is then jointly trained based on diagnostic quality images to discriminate the projection quality. The test phase performs feed-forward propagation over the generator network that demands a very low computational overhead. Extensive evaluations are performed on a large contrast-enhanced MR dataset of pediatric patients. In particular, images rated based on expert radiologists corroborate that GANCS retrieves high contrast images with detailed texture relative to conventional CS, and pixel-wise schemes. In addition, it offers reconstruction under a few milliseconds, two orders of magnitude faster than state-of-the-art CS-MRI schemes.",0
"This paper presents deep generative adversarial networks (GANs) as a novel approach for accelerating compressed sensing magnetic resonance imaging (MRI). We demonstrate that GANs can synthesize high-resolution MR images from highly undersampled data acquisitions while preserving the structural details of tissues and organs. Our work combines recent advances in GANs with optimization techniques tailored to the MRI reconstruction problem, yielding improved image quality over state-of-the art methods. By reducing the scan time without compromising diagnostic fidelity, our framework holds significant potential for improving healthcare accessibility through faster and more efficient medical imaging workflows. Overall, we showcase the promise of using deep learning for automating one of the most widely used medical modalities today â€“ MRI. This paper proposes the use of deep generative adversarial networks (GANs) to speed up compressive sensing Magnetic Resonance Imaging (MRI) by generating detailed MRI images from low resolution raw datasets. These GAN models have been trained on large amounts of MRI data and allow for accurate generation of new MRI datasets, which can reduce the amount of raw data required during the image acquisition process. With these developments, patients could potentially experience shorter wait times for their scans, without sacrificing the necessary level of detail. Additionally, these findings may lead to enhanced medical care due to increased availability of MRI technology. Overall, this research represents an important step towards automation within the field of radiology, providing clinicians the tools they need for effective patient diagnosis and treatment planning.",1
"We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.",0
"Artificial Intelligence (AI) has been rapidly advancing over recent years thanks to generative adversarial networks (GAN). These systems consist of two competing neural networks that improve each other through iteration; one generates new data such as images, while the second attempts to detect if they are real or generated. However, these GANs have limitations including mode collapse, where only some modes from the training dataset can be generated. This issue arises because current methods donâ€™t prioritize finding high likelihood regions over lower ones, meaning common patterns may dominate the output. To address this problem, we propose using boundary equilibrium techniques inspired by game theory to balance exploration vs exploitation during learning so high probability areas become dominant in generator output. Our approach is called Boundary Equilibrium Generative Adversarial Networks, BEGAN for short. We demonstrate through experiments on datasets CelebA, LSUN, ImageNet Dogs, Stability and Realness achieved through our method leads to significant improvements in visual fidelity compared against state-of-the-art baselines for image generation tasks. Finally, BEGAN achieves comparable performance but at higher stability, often converging faster than alternatives. Our results open up further research into balancing novelty against fit on GAN performance beyond images towards applications like text generation etc., improving sample diversity via bounded optimality principles. Overall, BEGAN represents a step forward in the development of GAN models able to better explore the underlying distribution through more efficient use of computational resources.",1
"The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling. In this paper we describe three natural properties of probability divergences that reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting that this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cram\'er distance. We show that the Cram\'er distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. To illustrate the relevance of the Cram\'er distance in practice we design a new algorithm, the Cram\'er Generative Adversarial Network (GAN), and show that it performs significantly better than the related Wasserstein GAN.",0
"In recent years there has been increasing interest in the field of Generative Adversarial Networks (GANs). However, GAN training can suffer from instability issues due to biases in gradients which arise during backpropagation through deep networks. One approach proposed to address this problem is the use of the Wasserstein distance metric, which measures the cost associated with moving probability mass from one distribution to another. Despite promising results, the implementation of Wasserstein gradients faces several challenges, such as high computational complexity and difficulties in scaling up models. Recently, a new measure called the CramÃ©r distance was introduced to overcome these limitations by providing better stability and accuracy. This paper presents evidence showing that GANs trained using the CramÃ©r distance instead of traditional loss functions can result in improved performance on benchmark datasets. Furthermore, we provide experimental comparisons between our method and previous approaches using standard benchmarks. Our contributions highlight how the choice of distance function affects the convergence behavior and final quality of generated images. Overall, this work offers novel insights into using the CramÃ©r distance as a viable alternative for gradient computation in deep generative models.",1
"It has been recently shown that Generative Adversarial Networks (GANs) can produce synthetic images of exceptional visual fidelity. In this work, we propose the GAN-based method for automatic face aging. Contrary to previous works employing GANs for altering of facial attributes, we make a particular emphasize on preserving the original person's identity in the aged version of his/her face. To this end, we introduce a novel approach for ""Identity-Preserving"" optimization of GAN's latent vectors. The objective evaluation of the resulting aged and rejuvenated face images by the state-of-the-art face recognition and age estimation solutions demonstrate the high potential of the proposed method.",0
"Title: Face Aging With Conditional Generative Adversarial Networks (CGAN) Abstract: As humans age, their physical appearance changes over time due to factors such as genetics, lifestyle choices, and environmental stressors. This presents significant challenges for facial analysis applications that rely on accurate detection and recognition algorithms. In recent years, deep learning methods have shown promising results in addressing these issues by modeling the aging process and generating synthetic images of faces at different ages. One popular approach in this domain is conditional generative adversarial networks (CGAN), which can generate realistic synthetic images conditioned on additional input data, such as face gender, identity, or ethnicity. In this paper, we propose a novel framework using CGAN to generate face representations from different stages of life based on multi-view input images. Our method first extracts features from the input images using a pre-trained convolutional neural network (CNN). These features serve as inputs to our CGAN architecture consisting of two competitive neural networks â€“ a generator network that generates new age projections of a given face image, and a discriminator network that evaluates the authenticity of these generated images relative to the training set. We train our system using a large dataset containing thousands of annotated face images across varying age ranges and demonstrate the efficacy of our approach through rigorous experimental evaluation, outperforming baseline methods in terms of visual quality, feature preservation, and quantitative metrics such as mean squared error and structural similarity index measure (SSIM). Our work provides valuable insights into the use of advanced machine learning techniques like CGAN for developing robust solutions for aging-related facial analysis problems, paving th",1
"Generative adversarial networks (GANs) has gained tremendous popularity lately due to an ability to reinforce quality of its predictive model with generated objects and the quality of the generative model with and supervised feedback. GANs allow to synthesize images with a high degree of realism. However, the learning process of such models is a very complicated optimization problem and certain limitation for such models were found. It affects the choice of certain layers and nonlinearities when designing architectures. In particular, it does not allow to train convolutional GAN models with fully-connected hidden layers. In our work, we propose a modification of the previously described set of rules, as well as new approaches to designing architectures that will allow us to train more powerful GAN models. We show the effectiveness of our methods on the problem of synthesizing projections of 3D objects with the possibility of interpolation by class and view point.",0
"In recent years, there has been increasing interest in using generative models such as Generative Adversarial Networks (GANs) for computer vision tasks like image synthesis, style transfer, and super-resolution. However, these methods have yet to be applied to modeling the 3D geometry of objects, particularly for generating detailed and realistic chairs that can be used in virtual environments, video games, or product design applications. This paper proposes a novel approach for learning to generate chairs with GANs by training on large datasets of chair images and utilizing adversarial loss functions to improve accuracy and fidelity. Our results show promising improvements over previous state-of-the-art methods for generating high-quality, detailed 3D models of chairs from simple inputs such as sketches or text descriptions. We hope our work will serve as a foundation for future research into using deep neural networks for modeling complex 3D shapes and objects in computer graphics and related fields.",1
"We propose a novel technique to make neural network robust to adversarial examples using a generative adversarial network. We alternately train both classifier and generator networks. The generator network generates an adversarial perturbation that can easily fool the classifier network by using a gradient of each image. Simultaneously, the classifier network is trained to classify correctly both original and adversarial images generated by the generator. These procedures help the classifier network to become more robust to adversarial perturbations. Furthermore, our adversarial training framework efficiently reduces overfitting and outperforms other regularization methods such as Dropout. We applied our method to supervised learning for CIFAR datasets, and experimantal results show that our method significantly lowers the generalization error of the network. To the best of our knowledge, this is the first method which uses GAN to improve supervised learning.",0
"This paper presents a novel approach to defending against adversarial perturbations using generative adversarial training (GAT). We first describe how such attacks work by adding small perturbations to images that cause machine learning models to make incorrect predictions. We then introduce our method, called Generative Adversarial Trainer (GAT), which involves training two neural networks simultaneously: one that generates new samples similar to those seen during training, and another that distinguishes real from fake examples. By optimizing these models jointly using a minimax game, we obtain improved robustness to adversarial perturbations. We empirically evaluate GAT on several benchmark datasets and show that it significantly outperforms state-of-the-art defense methods while maintaining comparable accuracy on clean data. Our results suggest that generating synthetic examples can effectively defend against adversarial attacks without sacrificing model performance. Overall, this work opens up a promising direction towards building more secure and reliable deep learning systems.",1
"Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.",0
"This paper presents a method for photo-realistic single image super-resolution using a generative adversarial network (GAN). Our approach utilizes a GAN architecture consisting of two sub-networks: a generator that produces high resolution images, and a discriminator that evaluates their quality. The generator is trained to fool the discriminator by generating images that are both perceptually similar to the input low resolution image and have sufficient detail to satisfy the discriminator. We evaluate our method on several benchmark datasets and demonstrate its ability to produce photo-realistic results while outperforming state-of-the-art methods. Additionally, we show that our method can handle challenging cases such as motion blurred inputs and maintain high performance. Overall, our work represents an important step towards realizing the potential of deep learning based approaches for visual computing tasks.",1
"Generative adversarial networks (GAN) approximate a target data distribution by jointly optimizing an objective function through a ""two-player game"" between a generator and a discriminator. Despite their empirical success, however, two very basic questions on how well they can approximate the target distribution remain unanswered. First, it is not known how restricting the discriminator family affects the approximation quality. Second, while a number of different objective functions have been proposed, we do not understand when convergence to the global minima of the objective function leads to convergence to the target distribution under various notions of distributional convergence.   In this paper, we address these questions in a broad and unified setting by defining a notion of adversarial divergences that includes a number of recently proposed objective functions. We show that if the objective function is an adversarial divergence with some additional conditions, then using a restricted discriminator family has a moment-matching effect. Additionally, we show that for objective functions that are strict adversarial divergences, convergence in the objective function implies weak convergence, thus generalizing previous results.",0
"Neural networks trained using generative adversarial learning (GANs) have achieved impressive results on a variety of tasks such as image generation, data augmentation, and super resolution. However, little attention has been paid to the approximation and convergence properties of GAN models. In this paper, we study the convergence behavior of GANs theoretically and experimentally. We first provide general conditions under which Nash equilibrium exists in two player zero sum games, and then prove that these conditions hold in the GAN framework if both players choose stationary strategies based on neural network policies with suitable activation functions. Our theoretical findings indicate that if the optimal value function can be approximated arbitrarily well by smooth functions, then a GAN model converges to a Nash equilibrium as the number of iterations increases without bound. Finally, our experimental results on benchmark datasets verify the validity and effectiveness of GANs in practice. Overall, our work provides new insights into the properties of GAN models and contributes towards their widespread adoption in various applications.",1
"Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.",0
"Abstract: Improving generative models has been a major focus of machine learning research. Recently, Adversarial Networks (AdNets) have proven to be highly effective at generating realistic data samples by training two networks simultaneously - one that generates outputs and another which attempts to distinguish generated from true data. However, current methods suffer from many limitations due to computational constraints. In this work we introduce Adaptive Generation Adversarial Networks (AdaGAN), a novel method for boosting GAN performance. AdaGAN tackles these issues through adaptive gradient updates during adversarial training, improving stability, efficiency, and quality of synthesized samples. Our method outperforms state-of-the-art on several benchmark datasets, demonstrating improved accuracy and diversity of generated samples without additional parameters or complexity.",1
"Developments in deep generative models have allowed for tractable learning of high-dimensional data distributions. While the employed learning procedures typically assume that training data is drawn i.i.d. from the distribution of interest, it may be desirable to model distinct distributions which are observed sequentially, such as when different classes are encountered over time. Although conditional variations of deep generative models permit multiple distributions to be modeled by a single network in a disentangled fashion, they are susceptible to catastrophic forgetting when the distributions are encountered sequentially. In this paper, we adapt recent work in reducing catastrophic forgetting to the task of training generative adversarial networks on a sequence of distinct distributions, enabling continual generative modeling.",0
"Artificial Intelligence (AI) has made tremendous progress over the last decade, owing largely to advancements in machine learning algorithms such as deep neural networks (DNNs). One particularly successful variant of DNNs that have recently gained traction in AI research are generative adversarial nets (GANs), which enable generation of synthetic data from real datasets while concurrently updating discriminators to improve classification accuracy. However, there still exist several limitations to GAN training due to instability issues caused by a nonconvergent energy function landscape and vanishing/exploding gradients. To overcome these challenges, we introduce continual learning strategies within the GAN framework to adaptively evolve both generator and discriminator models. Our experimental results on diverse benchmark datasets demonstrate that our proposed method leads to more stable convergence, improved performance measures, reduced training times, and better calibrated predictive uncertainty estimates compared to traditional static batch gradient descent optimization techniques. We believe this work contributes towards addressing some of the current limitations associated with using GANs, opening up exciting possibilities for further innovation across multiple application domains.",1
"We propose the Margin Adaptation for Generative Adversarial Networks (MAGANs) algorithm, a novel training procedure for GANs to improve stability and performance by using an adaptive hinge loss function. We estimate the appropriate hinge loss margin with the expected energy of the target distribution, and derive principled criteria for when to update the margin. We prove that our method converges to its global optimum under certain assumptions. Evaluated on the task of unsupervised image generation, the proposed training procedure is simple yet robust on a diverse set of data, and achieves qualitative and quantitative improvements compared to the state-of-the-art.",0
"Abstract: Recent advances in deep learning have led to the development of generative adversarial networks (GANs), which consist of two neural networks competing against each other in order to generate realistic images, videos, audio signals and text data. However, training GANs can often suffer from instability issues, such as mode collapse and vanishing gradients. In this paper, we propose a novel method called ""MAGAN"" (Margin Adaptation for GANs) that addresses these problems by introducing margin adaptation into the training process. By adjusting the margins based on the generator's progress towards convergence, our approach allows the discriminator to provide more accurate feedback during training while preventing it from becoming overly powerful, thus leading to better stability and improved performance. Our experimental results demonstrate the effectiveness of MAGAN across various datasets and configurations compared to state-of-the-art methods. This work paves the way for further research into improving the reliability of GANs for generating high-quality synthetic data and their applications in areas ranging from computer vision to natural language processing.",1
"Security, privacy, and fairness have become critical in the era of data science and machine learning. More and more we see that achieving universally secure, private, and fair systems is practically impossible. We have seen for example how generative adversarial networks can be used to learn about the expected private training data; how the exploitation of additional data can reveal private information in the original one; and how what looks like unrelated features can teach us about each other. Confronted with this challenge, in this paper we open a new line of research, where the security, privacy, and fairness is learned and used in a closed environment. The goal is to ensure that a given entity (e.g., the company or the government), trusted to infer certain information with our data, is blocked from inferring protected information from it. For example, a hospital might be allowed to produce diagnosis on the patient (the positive task), without being able to infer the gender of the subject (negative task). Similarly, a company can guarantee that internally it is not using the provided data for any undesired task, an important goal that is not contradicting the virtually impossible challenge of blocking everybody from the undesired task. We design a system that learns to succeed on the positive task while simultaneously fail at the negative one, and illustrate this with challenging cases where the positive task is actually harder than the negative one being blocked. Fairness, to the information in the negative task, is often automatically obtained as a result of this proposed approach. The particular framework and examples open the door to security, privacy, and fairness in very important closed scenarios, ranging from private data accumulation companies like social networks to law-enforcement and hospitals.",0
"In many real world machine learning tasks, especially those involving sensitive data such as financial records, medical histories, or personal communications, privacy must be considered. When training on potentially sensitive datasets, care must be taken that raw training data is never exposed in any form which could reveal individual instances, because doing so would violate participant confidentiality. This problem can be solved using a method called ""differential privacy"", which adds noise to collected statistics so exact answers cannot be inferred from them but approximate answers remain available for use by trained models. The privacy leakage should at all times be kept below some minimum threshold delta, the allowed maximum amount by which any inference attacker may increase their confidence levels when provided additional information about individuals from other sources (such as cross referencing names or identifying characteristics). Different techniques have different strengths depending on both dataset type and application requirements, and one optimal solution does not exist. Rather than attempting any general theory or universal recommendation, we provide guidelines for determining suitable parameter values dependent on specific goals under particular conditions. Our experiments demonstrate the validity of these guidelines through comparison of published methods on common benchmarks. We further show how the choice of differentially private mechanism changes qualitatively based on both sensitivity of information and desired outcome (that is, minimizing risk of overfitting vs prioritizing utility).",1
"We introduce a novel framework for adversarial training where the target distribution is annealed between the uniform distribution and the data distribution. We posited a conjecture that learning under continuous annealing in the nonparametric regime is stable irrespective of the divergence measures in the objective function and proposed an algorithm, dubbed {\ss}-GAN, in corollary. In this framework, the fact that the initial support of the generative network is the whole ambient space combined with annealing are key to balancing the minimax game. In our experiments on synthetic data, MNIST, and CelebA, {\ss}-GAN with a fixed annealing schedule was stable and did not suffer from mode collapse.",0
"Artificial intelligence (AI) has become increasingly prevalent in our daily lives as advancements in deep learning have made complex tasks such as image generation, translation, and summarization more accessible. However, many AI systems still struggle with maintaining coherence over time due to a lack of memory integration during training. One approach that shows promise in addressing these issues is through the use of generative adversarial networks (GANs). GANs consist of two neural networks that compete against each other: one generator network creates samples such as images, audio, or text, while the discriminator network evaluates their authenticity and guides the generator towards producing better quality outputs. In recent years, researchers have explored ways to improve GAN performance by using techniques such as data augmentation, normalizing flows, and architectural innovations like the Progressive Growing of GANs (PGGAN). This paper proposes a new technique called Annealed GANs (AnnealGAN), which involves gradually modifying the discriminator loss function during training in order to encourage diversification and prevent collapse. We demonstrate via comprehensive experiments on multiple benchmark datasets that AnnealGAN achieves competitive results compared to state-of-the-art methods across different evaluation metrics, including visual fidelity, perceptual realism, coherency, and controllability. Our work represents another step forward in improving the stability, reliability, and expressiveness of GAN models, ultimately enabling more effective utilization of these powerful tools across various domains and applications.",1
"Fast Magnetic Resonance Imaging (MRI) is highly in demand for many clinical applications in order to reduce the scanning cost and improve the patient experience. This can also potentially increase the image quality by reducing the motion artefacts and contrast washout. However, once an image field of view and the desired resolution are chosen, the minimum scanning time is normally determined by the requirement of acquiring sufficient raw data to meet the Nyquist-Shannon sampling criteria. Compressive Sensing (CS) theory has been perfectly matched to the MRI scanning sequence design with much less required raw data for the image reconstruction. Inspired by recent advances in deep learning for solving various inverse problems, we propose a conditional Generative Adversarial Networks-based deep learning framework for de-aliasing and reconstructing MRI images from highly undersampled data with great promise to accelerate the data acquisition process. By coupling an innovative content loss with the adversarial loss our de-aliasing results are more realistic. Furthermore, we propose a refinement learning procedure for training the generator network, which can stabilise the training with fast convergence and less parameter tuning. We demonstrate that the proposed framework outperforms state-of-the-art CS-MRI methods, in terms of reconstruction error and perceptual image quality. In addition, our method can reconstruct each image in 0.22ms--0.37ms, which is promising for real-time applications.",0
"This paper presents a deep learning approach to dealiasing compressive sensing magnetic resonance imaging (CS MRI). CS MRI is a technique that allows for faster acquisition times by acquiring undersampled k-space data, but suffers from artifacts caused by aliasing. Traditional methods to remove these artifacts are computationally expensive and require manual adjustments for each experiment. Our proposed method uses convolutional neural networks (CNN) to denoise and dealias the undersampled k-space data. We train our model on a large dataset of fully sampled MRIs and show that our network can effectively restore images comparable to those obtained using traditional methods. Additionally, we demonstrate the potential of our algorithm for acceleration factors up to 8x. Finally, we evaluate the effectiveness of different design choices such as regularization, normalization, data augmentation, batch size and early stopping. Our results indicate that the choice of hyperparameters has little impact on final performance, but may improve convergence speed. Overall, our work provides a fast and automatic solution for dealiasing in CS MRI with promising applications in clinical settings where high temporal resolution is important, without sacrificing image quality.",1
"While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. Source code for official implementation is publicly available https://github.com/SKTBrain/DiscoGAN",0
"Abstract: With generative models like GANs (Generative Adversarial Networks) becoming increasingly popular for tasks such as image generation, superresolution, and data augmentation, researchers have explored using these models for cross-domain relation discovery. In this work, we investigate how GANs can be used to learn cross-domain relations by training them on multiple domains concurrently. We present two novel methods that use adversarial losses to encourage alignment across multiple domains, thus facilitating learning of shared representations and underlying relationships. Our approach outperforms state-of-the-art baselines on four different datasets, demonstrating the effectiveness of our proposed techniques. Keywords: Generative Adversarial Networks, Cross-Domain Relations, Alignment Losses",1
"We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.",0
"Abstract: Unrolled Generative Adversarial Networks (GAN) have recently gained attention as a novel approach towards training generators. They overcome common problems encountered during the training of traditional GANs such as unstable dynamics, mode collapse, difficulty generating high quality samples, overfitting of the discriminator on easy examples, and computational expense. In contrast, UGAN utilizes an extended adversarial loss landscape that smoothly guides the generator into producing meaningful images in multiple iterations. We demonstrate promising results for both semi-supervised learning and semi-unsupervised learning across several datasets. Additionally, we find that using UGAN enables us to obtain state-of-the-art performance across all benchmark databases compared against current state-of-the-arts methods. Our study provides valuable insights into understanding how unrolling can mitigate difficulties faced by conventional approaches for image generation tasks. Keywords: Unrolled Generative Adversarial Networks, Adversarial Loss Landscapes, Semi-Supervised Learning, Semi-Un supervised Learning, Image Generation Tasks",1
"The idea of style transfer has largely only been explored in image-based tasks, which we attribute in part to the specific nature of loss functions used for style transfer. We propose a general formulation of style transfer as an extension of generative adversarial networks, by using a discriminator to regularize a generator with an otherwise separate loss function. We apply our approach to the task of learning to play chess in the style of a specific player, and present empirical evidence for the viability of our approach.",0
"This paper introduces style transfer generative adversarial networks (STGAN) as a method for learning new ways to play chess by incorporating insights from human expertise. STGAN uses two neural network systems that train together on chess games played both by humans and computers. One system learns policy decisions while playing chess based on learned patterns and behaviors, whereas the other system enforces consistency in those policies to ensure they align with known experts' strategies. After training, these two systems generate opponents to play against human users, challenging them with unique gameplay styles and forcing adaptation during matches. By providing accessibility and flexibility through customizable parameters, we demonstrate STGAN outperforms strong baseline models at the chess board, even surpassing grandmaster level play in certain evaluations. Overall, this work shows significant potential towards advancing artificial intelligence across industries through the use of multiagent reinforcement learning techniques.",1
"In recent years deep learning algorithms have shown extremely high performance on machine learning tasks such as image classification and speech recognition. In support of such applications, various FPGA accelerator architectures have been proposed for convolutional neural networks (CNNs) that enable high performance for classification tasks at lower power than CPU and GPU processors. However, to date, there has been little research on the use of FPGA implementations of deconvolutional neural networks (DCNNs). DCNNs, also known as generative CNNs, encode high-dimensional probability distributions and have been widely used for computer vision applications such as scene completion, scene segmentation, image creation, image denoising, and super-resolution imaging. We propose an FPGA architecture for deconvolutional networks built around an accelerator which effectively handles the complex memory access patterns needed to perform strided deconvolutions, and that supports convolution as well. We also develop a three-step design optimization method that systematically exploits statistical analysis, design space exploration and VLSI optimization. To verify our FPGA deconvolutional accelerator design methodology we train DCNNs offline on two representative datasets using the generative adversarial network method (GAN) run on Tensorflow, and then map these DCNNs to an FPGA DCNN-plus-accelerator implementation to perform generative inference on a Xilinx Zynq-7000 FPGA. Our DCNN implementation achieves a peak performance density of 0.012 GOPs/DSP.",0
"Deep neural networks have achieved great successes in many applications such as image classification, object detection, speech recognition, and natural language processing. Convolutional neural networks (CNNs) form a major subclass of these models due to their strong performance on vision tasks involving spatial structure of data like images. An alternative architecture that has recently gained attention is deconvolutional neural networks (DNCs). DNCs can address some drawbacks of CNNs, but require more computational resources and memory during training than their convolutional counterparts. This study presents a design methodology for efficient implementation of deconvolutional neural networks on field programmable gate arrays (FPGAs), which are powerful hardware accelerators. We aim at reducing computation time by exploring various techniques that trade off computational accuracy and resource utilization. Our contributions consist of a detailed analysis of existing approaches, a novel DNC design based on depthwise separable convâ€¦ etc.",1
"Recently, realistic image generation using deep neural networks has become a hot topic in machine learning and computer vision. Images can be generated at the pixel level by learning from a large collection of images. Learning to generate colorful cartoon images from black-and-white sketches is not only an interesting research problem, but also a potential application in digital entertainment. In this paper, we investigate the sketch-to-image synthesis problem by using conditional generative adversarial networks (cGAN). We propose the auto-painter model which can automatically generate compatible colors for a sketch. The new model is not only capable of painting hand-draw sketch with proper colors, but also allowing users to indicate preferred colors. Experimental results on two sketch datasets show that the auto-painter performs better that existing image-to-image methods.",0
"Recent advances in computer vision have enabled artificial intelligence (AI) systems to generate high quality images from text descriptions using techniques such as generative adversarial networks (GAN). However, most current methods require significant computational resources and struggle to capture complex relationships among elements within an image. In this paper, we propose AutoPainter â€“ an interactive system that leverages conditional GANs to facilitate cartoon generation from sketches created on mobile devices. Our approach incorporates novel edge detection methods that preserve key features from user inputs while generating diverse and accurate results. Extensive experiments demonstrate that AutoPainter outperforms state-of-the-art baselines across several metrics and can effectively generate visually appealing cartoons from simple user input. Additionally, ablation studies verify the importance of our proposed components and their contributions towards boosting performance. Overall, AutoPainter enables users to create cartoons quickly, accurately, and interactively, paving the way for future applications in creative domains.",1
"Generative adversarial networks (GANs) have received a tremendous amount of attention in the past few years, and have inspired applications addressing a wide range of problems. Despite its great potential, GANs are difficult to train. Recently, a series of papers (Arjovsky & Bottou, 2017a; Arjovsky et al. 2017b; and Gulrajani et al. 2017) proposed using Wasserstein distance as the training objective and promised easy, stable GAN training across architectures with minimal hyperparameter tuning. In this paper, we compare the performance of Wasserstein distance with other training objectives on a variety of GAN architectures in the context of single image super-resolution. Our results agree that Wasserstein GAN with gradient penalty (WGAN-GP) provides stable and converging GAN training and that Wasserstein distance is an effective metric to gauge training progress.",0
"Face super-resolution is a challenging problem in computer vision that involves enhancing low resolution images to obtain high resolution images. In recent years, deep learning techniques have emerged as promising approaches for addressing this task, but existing methods often suffer from several limitations such as blurriness, artifacts, and limited performance in real-world scenarios. To overcome these issues, we propose a novel face super-resolution method based on Wasserstein Generative Adversarial Networks (WGANs). Our approach leverages the power of WGANs to learn high quality mappings between low resolution input images and their corresponding high resolution counterparts. We demonstrate the effectiveness of our proposed framework through extensive experimental evaluation, achieving state-of-the-art results across several benchmark datasets. Furthermore, our method exhibits remarkable robustness under realistic conditions such as varying lighting, pose, and occlusion, making it highly suitable for practical applications. This work represents an important step forward in advancing face super-resolution technology, paving the way for new possibilities in computer vision research and beyond.",1
"Current approaches in video forecasting attempt to generate videos directly in pixel space using Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs). However, since these approaches try to model all the structure and scene dynamics at once, in unconstrained settings they often generate uninterpretable results. Our insight is to model the forecasting problem at a higher level of abstraction. Specifically, we exploit human pose detectors as a free source of supervision and break the video forecasting problem into two discrete steps. First we explicitly model the high level structure of active objects in the scene---humans---and use a VAE to model the possible future movements of humans in the pose space. We then use the future poses generated as conditional information to a GAN to predict the future frames of the video in pixel space. By using the structured space of pose as an intermediate representation, we sidestep the problems that GANs have in generating video pixels directly. We show through quantitative and qualitative evaluation that our method outperforms state-of-the-art methods for video prediction.",0
"This paper presents the problem of video forecasting as one of generating pose futures - temporally coherent sequences of 2D or 3D human poses that capture expected future motion. We formulate this problem as a density estimation task over pose futures conditioned on current observations and temporal dynamics. To solve it, we introduce a novel framework based on autoencoder-like architectures which learns to generatepose futures from raw image inputs. Our key contribution lies in designing a recurrent attention mechanism (RAM) module, which enables efficient combination of past and present contexts to generate accuratepose futures. Extensive experiments validate our approach across diverse benchmark datasets, where RAM significantly improves performance comparedto previous state-of-the-art methods.",1
"Colorization is an ambiguous problem, with multiple viable colorizations for a single grey-level image. However, previous methods only produce the single most probable colorization. Our goal is to model the diversity intrinsic to the problem of colorization and produce multiple colorizations that display long-scale spatial co-ordination. We learn a low dimensional embedding of color fields using a variational autoencoder (VAE). We construct loss terms for the VAE decoder that avoid blurry outputs and take into account the uneven distribution of pixel colors. Finally, we build a conditional model for the multi-modal distribution between grey-level image and the color field embeddings. Samples from this conditional model result in diverse colorization. We demonstrate that our method obtains better diverse colorizations than a standard conditional variational autoencoder (CVAE) model, as well as a recently proposed conditional generative adversarial network (cGAN).",0
"In this paper we study image colorization, specifically focusing on learning based approaches. Given grayscale images as input, our goal is to generate full-color versions that resemble how humans would perceive these scenes if they were observed directly. While there has been significant progress made towards solving this problem through machine learning algorithms, many existing methods suffer from limitations such as lacking diversity and often producing overly uniform results. To address these issues, we propose a novel model architecture that leverages attention mechanisms to enable more flexible interactions across different color channels. Our approach outperforms state-of-the-art baselines on standard benchmarks and produces high quality colorizations of diverse and plausible appearance. We also present qualitative evaluations demonstrating improved realism and diversity compared to alternative methods. Additionally, by making use of large scale datasets and model ensembling, we achieve even greater improvements and further demonstrate the generalizability of our method across diverse domains. Finally, we perform ablation studies analyzing design choices within our framework to provide insights into why certain architectural components lead to better performance than others.",1
"Cross-modal audio-visual perception has been a long-lasting topic in psychology and neurology, and various studies have discovered strong correlations in human perception of auditory and visual stimuli. Despite works in computational multimodal modeling, the problem of cross-modal audio-visual generation has not been systematically studied in the literature. In this paper, we make the first attempt to solve this cross-modal generation problem leveraging the power of deep generative adversarial training. Specifically, we use conditional generative adversarial networks to achieve cross-modal audio-visual generation of musical performances. We explore different encoding methods for audio and visual signals, and work on two scenarios: instrument-oriented generation and pose-oriented generation. Being the first to explore this new problem, we compose two new datasets with pairs of images and sounds of musical performances of different instruments. Our experiments using both classification and human evaluations demonstrate that our model has the ability to generate one modality, i.e., audio/visual, from the other modality, i.e., visual/audio, to a good extent. Our experiments on various design choices along with the datasets will facilitate future research in this new problem space.",0
"This study explores the intersection of audio-visual deep learning techniques through cross-modal generation. We examine how both auditory and visual cues can enhance each other to create more immersive multimedia experiences. Our approach combines generative adversarial networks (GANs) with recurrent neural networks (RNNs), allowing us to generate high-quality audio and video data that aligns with specific contextual inputs. To evaluate our methodology, we perform extensive experiments on benchmark datasets and showcase significant improvements over previous state-of-the-art results. By leveraging these advancements, we demonstrate their applications in various domains such as gaming, entertainment, and virtual reality systems. Ultimately, we aim to provide practitioners and researchers with insights into enhancing multimodal content creation using cutting-edge artificial intelligence technologies.",1
"In the past few years, Generative Adversarial Network (GAN) became a prevalent research topic. By defining two convolutional neural networks (G-Network and D-Network) and introducing an adversarial procedure between them during the training process, GAN has ability to generate good quality images that look like natural images from a random vector. Besides image generation, GAN may have potential to deal with wide range of real world problems. In this paper, we follow the basic idea of GAN and propose a novel model for image saliency detection, which is called Supervised Adversarial Networks (SAN). Specifically, SAN also trains two models simultaneously: the G-Network takes natural images as inputs and generates corresponding saliency maps (synthetic saliency maps), and the D-Network is trained to determine whether one sample is a synthetic saliency map or ground-truth saliency map. However, different from GAN, the proposed method uses fully supervised learning to learn both G-Network and D-Network by applying class labels of the training set. Moreover, a novel kind of layer call conv-comparison layer is introduced into the D-Network to further improve the saliency performance by forcing the high-level feature of synthetic saliency maps and ground-truthes as similar as possible. Experimental results on Pascal VOC 2012 database show that the SAN model can generate high quality saliency maps for many complicate natural images.",0
"""Image saliency detection has recently become a topic of great interest due to its wide range of applications such as image retargeting, object segmentation, and visual attention modeling. In this paper, we propose a novel approach using supervised adversarial networks (SAN) to detect image saliency. Our method leverages both generative models and discriminative models by training them together as mini-max competitors in an adversarial framework. We use both global context and local features from convolutional neural network (CNN) to predict pixel-wise saliency maps that capture the most visually striking regions of images. Extensive experiments on public datasets demonstrate the superior performance of our proposed SAN compared to state-of-the-art methods in terms of accuracy, robustness, and efficiency.""",1
"This paper proposes an extension to the Generative Adversarial Networks (GANs), namely as ARTGAN to synthetically generate more challenging and complex images such as artwork that have abstract characteristics. This is in contrast to most of the current solutions that focused on generating natural images such as room interiors, birds, flowers and faces. The key innovation of our work is to allow back-propagation of the loss function w.r.t. the labels (randomly assigned to each generated images) to the generator from the discriminator. With the feedback from the label information, the generator is able to learn faster and achieve better generated image quality. Empirically, we show that the proposed ARTGAN is capable to create realistic artwork, as well as generate compelling real world images that globally look natural with clear shape on CIFAR-10.",0
"Title: ArtGAN: Artwork Synthesis with Conditional Categorical GANs  Artificial intelligence (AI) has made significant strides in recent years, particularly in computer vision tasks such as image generation. Generative Adversarial Networks (GANs), composed of two competing neural networks, have become a popular approach for generating realistic images. In our work, we focus on developing conditional GAN architectures that synthesize artwork under specific constraints. Our system, called ArtGAN, generates high-quality images by leveraging both unconditional and categorical loss functions. By optimizing these losses, we can generate coherent pieces that retain key features from the training data while respecting the specified conditions. We demonstrate the effectiveness of ArtGAN through quantitative evaluations using common metrics such as FrÃ©chet Inception Distance (FID). Additionally, qualitative assessments conducted via user studies indicate strong performance across different styles and categories. Our contributions lie in introducing a flexible framework for controlling aspects of generated artworks while maintaining their quality, opening new opportunities for interactive design tools, art creation, and research applications. With improved control over creativity models, more possibilities arise for exploring complex questions related to aesthetics, style transfer, personalization, interpretation, and diversity in modern art scenes.",1
"Traditional generative adversarial networks (GAN) and many of its variants are trained by minimizing the KL or JS-divergence loss that measures how close the generated data distribution is from the true data distribution. A recent advance called the WGAN based on Wasserstein distance can improve on the KL and JS-divergence based GANs, and alleviate the gradient vanishing, instability, and mode collapse issues that are common in the GAN training. In this work, we aim at improving on the WGAN by first generalizing its discriminator loss to a margin-based one, which leads to a better discriminator, and in turn a better generator, and then carrying out a progressive training paradigm involving multiple GANs to contribute to the maximum margin ranking loss so that the GAN at later stages will improve upon early stages. We call this method Gang of GANs (GoGAN). We have shown theoretically that the proposed GoGAN can reduce the gap between the true data distribution and the generated data distribution by at least half in an optimally trained WGAN. We have also proposed a new way of measuring GAN quality which is based on image completion tasks. We have evaluated our method on four visual datasets: CelebA, LSUN Bedroom, CIFAR-10, and 50K-SSFF, and have seen both visual and quantitative improvement over baseline WGAN.",0
"One of the primary challenges associated with training generative models using deep neural networks is the difficulty of optimizing highly nonlinear loss functions. In particular, maximizing log likelihoodâ€”a common choice for density estimation problemsâ€”can lead to poor results due to its propensity to get stuck at local optima. As such, there has been growing interest in alternative methods that can improve both convergence speed and performance of these models.  In this work, we propose a novel method based on combining generative adversarial networks (GANs) with maximum margin ranking (MMR). Specifically, our proposed approach integrates MMR into the discriminator network of a standard GAN architecture, which allows us to optimize more effectively for high quality sample generation without relying solely on reconstruction error minimization. We further show how to adapt existing optimization techniques from large language model pretraining to handle discrete text generation tasks like machine translation using our framework.  We demonstrate the effectiveness of our method through extensive experiments across several benchmark datasets spanning multiple domains including image, video, music, and natural language processing. Our findings suggest that our framework provides significant improvements over existing state-of-the-art methods for generative modeling with GANs while introducing new capabilities for fine-grained control over generated outputs and improved robustness under data scarcity scenarios. Additionally, our methodology offers insights into better understanding relationships among generator architectures, loss objective functions, hyperparameters tuning, and problem settings. By addressing key issues surrounding GAN training stability, scalability, and controllability in complex systems subjected to noise, we pave the path towards promising applications of generative models in real-world environments facing ambiguous, unstructured and multi-disciplinary domains.",1
"Traditional face editing methods often require a number of sophisticated and task specific algorithms to be applied one after the other --- a process that is tedious, fragile, and computationally intensive. In this paper, we propose an end-to-end generative adversarial network that infers a face-specific disentangled representation of intrinsic face properties, including shape (i.e. normals), albedo, and lighting, and an alpha matte. We show that this network can be trained on ""in-the-wild"" images by incorporating an in-network physically-based image formation module and appropriate loss functions. Our disentangling latent representation allows for semantically relevant edits, where one aspect of facial appearance can be manipulated while keeping orthogonal properties fixed, and we demonstrate its use for a number of facial editing applications.",0
"Artistic face editing has emerged as one of the most promising applications of deep learning-based image generation models such as GANs and VAEs. However, these methods often struggle with producing high quality edits that faithfully preserve the intrinsic properties of the original input images while incorporating desired changes, due to their limited ability to model complex dependencies within the latent space. To address this challenge, we propose a novel framework called ""NeuralFaceEdit"" that disentangles the representation of faces into multiple subspaces, each capturing different aspects such as identity, expression, pose, and illumination. We then introduce a new network architecture that integrates the advantages of both GANs and VAEs by allowing us to synthesize sharp and coherent images using adversarial training, while preserving intrinsic image qualities via attention mechanisms guided by the disentangled representations. Our method significantly advances state-of-the-art performance on a variety of facial attribute editing tasks, demonstrating the effectiveness of our approach in generating highly realistic and controllable edits. The extensive experimental results confirm the importance of disentangling latent spaces towards achieving more accurate face edits. Finally, subjective evaluations show that our generated outputs achieve better perceptual quality compared to previous baselines across several metrics including naturalness and authenticity. Overall, our work represents a significant step forward in developing intuitive yet powerful tools for artists, designers, and researchers alike to create high fidelity face manipulations with minimal user interaction.",1
"In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.",0
Title: Stacked Generative Adversarial Networks (SGAN)  Stacked generative adversarial networks (SGA,1
"Face attributes are interesting due to their detailed description of human faces. Unlike prior researches working on attribute prediction, we address an inverse and more challenging problem called face attribute manipulation which aims at modifying a face image according to a given attribute value. Instead of manipulating the whole image, we propose to learn the corresponding residual image defined as the difference between images before and after the manipulation. In this way, the manipulation can be operated efficiently with modest pixel modification. The framework of our approach is based on the Generative Adversarial Network. It consists of two image transformation networks and a discriminative network. The transformation networks are responsible for the attribute manipulation and its dual operation and the discriminative network is used to distinguish the generated images from real images. We also apply dual learning to allow transformation networks to learn from each other. Experiments show that residual images can be effectively learned and used for attribute manipulations. The generated images remain most of the details in attribute-irrelevant areas.",0
"Artificial intelligence (AI) has greatly impacted our lives by providing intelligent agents that can interact with humans and the environment to perform tasks that normally require human intelligence such as problem solving, decision making, perception, understanding natural languages, speech recognition, among others. One particular area where artificial intelligence excels is image manipulation, specifically facial attribute manipulation. Facial attribute manipulation involves modifying certain features on an individualâ€™s face without altering their overall appearance. This task requires high precision and attention to detail since even small changes can drastically affect oneâ€™s perceived identity. In order to achieve accurate facial attribute manipulation using artificial intelligence, it is crucial to learn residual images first before applying them during the modification process. Residual images refer to images that represent minute details like glasses, beards, smiles, frowns, etc. By learning these residual images, AIs can make more precise modifications while preserving facial identity, expression, gender, ethnicity, age, and other important characteristics. Therefore, this research aimed to develop a novel methodology to accurately learn residual images from vast repositories of unlabeled data through carefully designed network architectures and training algorithms. Our approach achieved state-of-the-art results surpassing previous methods across several challenging benchmark datasets, demonstrating the effectiveness of our proposed framework in realizing advanced face attribute manipulation capabilities using artificial intelligence. Overall, this study paves the way for further innovation in computer vision applications that heavily rely on deep learning techniques to enhance visual content creation.",1
"As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways. Such ""in-the-tail"" data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (around 1000 images). To allow for large-scale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right ""priors"" or parameters for synthesis: we would like realistic data with poses and object configurations that mimic true Precarious Pedestrians. Inspired by Generative Adversarial Networks (GANs), we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset, which we deem the Adversarial Imposters. We demonstrate that this simple pipeline allows one to synthesize realistic training data by making use of rendering/animation engines within a GAN framework. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Adversarial Imposters can also be used for ""in-the-tail"" validation at test-time, a notoriously difficult challenge for real-world deployment.",0
"In recent years, pedestrian detection has become one of the most active research areas in computer vision due to the growing demand for advanced driver assistance systems (ADAS) and autonomous vehicles. However, detectors trained on existing datasets can struggle to handle unusual pedestrians that are different from those seen during training, leading to poor performance. To address this issue, we propose a novel approach based on adversarial imposters. We use these synthetic images to regularize the detector by making it robust against input changes while maintaining high accuracy. Our method outperforms state-of-the-art approaches on two challenging benchmarks, demonstrating the effectiveness of our approach. Furthermore, we provide a comprehensive analysis to better understand how the proposed technique improves pedestrian detection under difficult scenarios. Overall, this work presents a new perspective on unseen pedestrian detection, highlighting the importance of training detectors for unexpected situations.",1
"Generating and manipulating human facial images using high-level attributal controls are important and interesting problems. The models proposed in previous work can solve one of these two problems (generation or manipulation), but not both coherently. This paper proposes a novel model that learns how to both generate and modify the facial image from high-level semantic attributes. Our key idea is to formulate a Semi-Latent Facial Attribute Space (SL-FAS) to systematically learn relationship between user-defined and latent attributes, as well as between those attributes and RGB imagery. As part of this newly formulated space, we propose a new model --- SL-GAN which is a specific form of Generative Adversarial Network. Finally, we present an iterative training algorithm for SL-GAN. The experiments on recent CelebA and CASIA-WebFace datasets validate the effectiveness of our proposed framework. We will also make data, pre-trained models and code available.",0
"In recent years, Generative Adversarial Networks (GANs) have emerged as powerful tools for generating realistic synthetic data such as photographs, audio files, and even text documents. However, most existing GAN models require vast amounts of labeled training data, which can be difficult and time-consuming to collect. This paper presents a new approach called Semi-Latent GAN that allows us to train GANs using only attribute labels rather than individual face identities, substantially reducing the amount of required supervision. Our model leverages an encoder network to extract latent representations of faces given their attribute labels, allowing us to map them directly into corresponding generator inputs. We show that our method significantly outperforms baseline approaches on standard benchmark datasets while requiring less human annotation effort. Additionally, we demonstrate how our framework can be used to manipulate specific features of generated faces by adjusting corresponding attribute values during testing. Overall, our work represents a significant step towards creating more accessible and intuitive generative models capable of producing high-quality, controllable synthesized media.",1
"Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.",0
"In recent years, generative adversarial networks (GANs) have become a popular technique for generating realistic synthetic data. However, traditional GANs suffer from several drawbacks such as instability during training, difficulty in controlling generated content, and sensitivity to hyperparameters. To address these issues, we propose least squares GANs (LS-GAN), which incorporate a reconstruction loss into the generator network using least square estimation. This reconstruction loss acts as a regularizer, reducing the gap between the input data and the generated data, making the generator more stable and easier to control. We show that LS-GANs outperform conventional GANs on a variety of tasks including image generation, video prediction, and style transfer. Our results demonstrate the effectiveness of LS-GANs in producing high quality synthetic data while maintaining stability and controllability.",1
"This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.",0
"This tutorial on Generative Adversarial Networks (GANs) was presented at the Neural Information Processing Systems Conference (NeurIPS) in 2016. GANs are a popular deep learning technique that have seen widespread success in generating realistic synthetic data, such as images, videos, audio, text, etc., that match ground truth distributions. They consist of two neural networks competitively training against each other â€“ a generative network produces samples like real/fake classifiers do, while a discriminator tries to distinguish real data from generated data. The competition between them drives the generator to produce more realistic samples, until they become indistinguishable from real ones.  The tutorial covers how to implement GANs using popular deep learning frameworks, like TensorFlow and PyTorch, common pitfalls one can encounter during implementation, and state-of-the-art architectures available today. Attendees learned to apply these techniques in their research to solve difficult problems related to image generation, video prediction, semi-supervised learning, unpaired multi-modal datasets, text-to-image synthesis, etc. Also covered were recent advances in theory behind why GANs work so well and what causes mode collapse, which makes the generator only learn part of the distribution instead of the whole thing. Overall, attendees walked away with a comprehensive understanding of the fundamentals and advanced topics in GANs and how to use them effectively.",1
"The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.",0
"Artificial neural networks (ANNs) have achieved significant successes across multiple application domains by automatically learning hierarchical features that capture underlying patterns in data. However, recent research has shown that these features can often fail to generalize well out-of-distribution (OOD), resulting in poor performance on unseen tasks or scenarios. This problem is exacerbated by training objectives that only minimize prediction error within narrow distributions of training data, which encourages models to learn overfitting representations that exploit spurious correlations rather than capturing true causal relationships between inputs and outputs. In turn, such representations can lead to brittle decision making, limiting their effectiveness as deployed systems in safety-critical settings. To address these limitations, we propose adversarial feature learning (AFL), a method based on integrating Generative Adversarial Networks (GANs) into deep learning pipelines to encourage robust representation learning under distribution shift. Our approach introduces an adaptive discriminator that acts as a regularizer during both training and evaluation, promoting reliable predictions even in extreme OOD conditions. Extensive experiments demonstrate our framework achieves improved out-of-distribution generalization compared against state-of-the-art baseline methods, significantly improving reliability across diverse benchmark datasets. Overall, AFL enables more resilient ANNs and advances towards trustworthy deployment of machine learning in real-world applications where failure cannot be tolerated.",1
"This paper studies a problem of inverse visual path planning: creating a visual scene from a first person action. Our conjecture is that the spatial arrangement of a first person visual scene is deployed to afford an action, and therefore, the action can be inversely used to synthesize a new scene such that the action is feasible. As a proof-of-concept, we focus on linking visual experiences induced by walking.   A key innovation of this paper is a concept of ActionTunnel---a 3D virtual tunnel along the future trajectory encoding what the wearer will visually experience as moving into the scene. This connects two distinctive first person images through similar walking paths. Our method takes a first person image with a user defined future trajectory and outputs a new image that can afford the future motion. The image is created by combining present and future ActionTunnels in 3D where the missing pixels in adjoining area are computed by a generative adversarial network. Our work can provide a travel across different first person experiences in diverse real world scenes.",0
"This article proposes a new approach for personalized action recognition based on desired actions rather than on predefined ones. To achieve that goal we introduce the concept of customization of first person image through desired actions. Our methodology consists in using different visual cues such as color histogram, corners detection and HOG descriptors that enable us to selectively focus only on those parts of video sequence which correspond to userâ€™s desired action. We present two main results; accuracy improvement achieved by our approach over current state-of-the-art methods like DeepAction+MPII dataset and the high diversity of recognized actions, thanks to the customization process. Finally, we describe potential applications for this technology in virtual assistants, intelligent monitoring systems and self-quantification.",1
"Semantic segmentation has been a long standing challenging task in computer vision. It aims at assigning a label to each image pixel and needs significant number of pixellevel annotated data, which is often unavailable. To address this lack, in this paper, we leverage, on one hand, massive amount of available unlabeled or weakly labeled data, and on the other hand, non-real images created through Generative Adversarial Networks. In particular, we propose a semi-supervised framework ,based on Generative Adversarial Networks (GANs), which consists of a generator network to provide extra training examples to a multi-class classifier, acting as discriminator in the GAN framework, that assigns sample a label y from the K possible classes or marks it as a fake sample (extra class). The underlying idea is that adding large fake visual data forces real samples to be close in the feature space, enabling a bottom-up clustering process, which, in turn, improves multiclass pixel classification. To ensure higher quality of generated images for GANs with consequent improved pixel classification, we extend the above framework by adding weakly annotated data, i.e., we provide class level information to the generator. We tested our approaches on several challenging benchmarking visual datasets, i.e. PASCAL, SiftFLow, Stanford and CamVid, achieving competitive performance also compared to state-of-the-art semantic segmentation method",0
"This paper presents two approaches for semantic segmentation using generative adversarial networks (GANs). In semi supervised segmentation, we use unlabeled data along with small amounts of labeled data to train our model. To accomplish this, we create virtual labeled examples by applying random transformations such as cropping, scaling and rotating to existing labeled images so that our training set has more annotated images. Our method uses these annotations to guide the generator and discriminator simultaneously during each iteration in GAN training. For weakly supervised segmentation, we only have image-level labels to work with instead of pixel-accurate ground truth. Inspired by works on self-training, we develop a bootstrapping framework which alternates between generating pseudo masks from current model and using them to augment our dataset and improve performance in next iteration. Experiments show consistent improvements over state-of-the-art methods across several datasets including Cityscapes, PASCAL Context and COCO Stuff. Both proposed models achieve superior results while maintaining strong scalability due to efficient use of human annotations. The techniques introduced here constitute a major step towards realizing fully autonomous systems that can perceive their environment well enough to perform complex tasks without further human intervention.",1
"In this work, we present the Text Conditioned Auxiliary Classifier Generative Adversarial Network, (TAC-GAN) a text to image Generative Adversarial Network (GAN) for synthesizing images from their text descriptions. Former approaches have tried to condition the generative process on the textual data; but allying it to the usage of class information, known to diversify the generated samples and improve their structural coherence, has not been explored. We trained the presented TAC-GAN model on the Oxford-102 dataset of flowers, and evaluated the discriminability of the generated images with Inception-Score, as well as their diversity using the Multi-Scale Structural Similarity Index (MS-SSIM). Our approach outperforms the state-of-the-art models, i.e., its inception score is 3.45, corresponding to a relative increase of 7.8% compared to the recently introduced StackGan. A comparison of the mean MS-SSIM scores of the training and generated samples per class shows that our approach is able to generate highly diverse images with an average MS-SSIM of 0.14 over all generated classes.",0
"""Text Conditioned Auxiliary Classifier Generative Adversarial Network (TAC-GAN) is a new approach to generating realistic images from text descriptions. In contrast to previous generative models, such as DALL-E and Stable Diffusion, which rely on pre-training on large datasets, TAC-GAN uses auxiliary classifiers trained on separate, small labeled datasets to guide the generation process. This allows for fine-grained control over the generated output, enabling the creation of more detailed and accurate images. Additionally, TAC-GAN employs two discriminators: one evaluates the realism of the image, while another measures how well the image aligns with the input text. By leveraging these mechanisms, TAC-GAN achieves state-of-the-art results in terms of quality and coherency of generated images.""",1
"Modeling the long-term facial aging process is extremely challenging due to the presence of large and non-linear variations during the face development stages. In order to efficiently address the problem, this work first decomposes the aging process into multiple short-term stages. Then, a novel generative probabilistic model, named Temporal Non-Volume Preserving (TNVP) transformation, is presented to model the facial aging process at each stage. Unlike Generative Adversarial Networks (GANs), which requires an empirical balance threshold, and Restricted Boltzmann Machines (RBM), an intractable model, our proposed TNVP approach guarantees a tractable density function, exact inference and evaluation for embedding the feature transformations between faces in consecutive stages. Our model shows its advantages not only in capturing the non-linear age related variance in each stage but also producing a smooth synthesis in age progression across faces. Our approach can model any face in the wild provided with only four basic landmark points. Moreover, the structure can be transformed into a deep convolutional network while keeping the advantages of probabilistic models with tractable log-likelihood density estimation. Our method is evaluated in both terms of synthesizing age-progressed faces and cross-age face verification and consistently shows the state-of-the-art results in various face aging databases, i.e. FG-NET, MORPH, AginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). A large-scale face verification on Megaface challenge 1 is also performed to further show the advantages of our proposed approach.",0
"Here we present a novel approach to facial age progression that uses temporal coherence as well as shape features to model aging patterns over time. Our method generates high quality aged images by enforcing non-volume preservation constraints on feature space trajectories to produce smooth, realistic aging effects without losing identity fidelity. We evaluate our algorithm using extensive experimental results comparing against other state-of-the-art methods on multiple benchmark datasets, demonstrating significant improvements in both visual fidelity and quantitative metrics. Furthermore, we showcase how the proposed framework can be applied for accurate age invariant face recognition tasks. These promising findings provide valuable insights into advancing research areas related to computer vision such as video synthesis, image editing and biometrics.",1
"A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations, and fail to cover rich underlying semantics. In this paper, we investigate a semi-supervised paragraph generative framework that is able to synthesize diverse and semantically coherent paragraph descriptions by reasoning over local semantic regions and exploiting linguistic knowledge. The proposed Recurrent Topic-Transition Generative Adversarial Network (RTT-GAN) builds an adversarial framework between a structured paragraph generator and multi-level paragraph discriminators. The paragraph generator generates sentences recurrently by incorporating region-based visual and language attention mechanisms at each step. The quality of generated paragraph sentences is assessed by multi-level adversarial discriminators from two aspects, namely, plausibility at sentence level and topic-transition coherence at paragraph level. The joint adversarial training of RTT-GAN drives the model to generate realistic paragraphs with smooth logical transition between sentence topics. Extensive quantitative experiments on image and video paragraph datasets demonstrate the effectiveness of our RTT-GAN in both supervised and semi-supervised settings. Qualitative results on telling diverse stories for an image also verify the interpretability of RTT-GAN.",0
"Here we present a new model for generating visual paragraphs using a recurrent generative adversarial network (GAN). Our approach utilizes a topic transition mechanism to ensure coherence between consecutive generated images, which improves both their semantic relevance and overall cohesiveness. We show that our method can generate high quality image sequences on challenging datasets such as COCO and Flickr8k. In addition to quantitative evaluations, we conduct a user study where our model outperforms baseline models in terms of human preference and perceptual ratings. By effectively combining topic transitions and deep learning techniques, our work represents a significant advancement in the field of computer vision and machine learning.",1
"Deep neural networks achieve unprecedented performance levels over many tasks and scale well with large quantities of data, but performance in the low-data regime and tasks like one shot learning still lags behind. While recent work suggests many hypotheses from better optimization to more complicated network structures, in this work we hypothesize that having a learnable and more expressive similarity objective is an essential missing component. Towards overcoming that, we propose a network design inspired by deep residual networks that allows the efficient computation of this more expressive pairwise similarity objective. Further, we argue that regularization is key in learning with small amounts of data, and propose an additional generator network based on the Generative Adversarial Networks where the discriminator is our residual pairwise network. This provides a strong regularizer by leveraging the generated data samples. The proposed model can generate plausible variations of exemplars over unseen classes and outperforms strong discriminative baselines for few shot classification tasks. Notably, our residual pairwise network design outperforms previous state-of-theart on the challenging mini-Imagenet dataset for one shot learning by getting over 55% accuracy for the 5-way classification task over unseen classes.",0
"Artificial intelligence has made significant strides over recent years due largely to advancements in deep learning algorithms. Recent work has shown that generative models can effectively learn one-shot tasks by leveraging adversarial training techniques. However, there still exists room for improvement in these methods. We present a novel method using pairwise discriminators which significantly improves upon current state-of-the-art results on several benchmark datasets such as Omniglot and MiniImagenet. Our approach also allows for more efficient model training compared to existing methods by utilizing residual connections within our generator network. These contributions represent important steps towards developing high performing artificial general intelligence systems capable of learning complex tasks from limited data.",1
"Obtaining models that capture imaging markers relevant for disease progression and treatment monitoring is challenging. Models are typically based on large amounts of data with annotated examples of known markers aiming at automating detection. High annotation effort and the limitation to a vocabulary of known markers limit the power of such approaches. Here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers. We propose AnoGAN, a deep convolutional generative adversarial network to learn a manifold of normal anatomical variability, accompanying a novel anomaly scoring scheme based on the mapping from image space to a latent space. Applied to new data, the model labels anomalies, and scores image patches indicating their fit into the learned distribution. Results on optical coherence tomography images of the retina demonstrate that the approach correctly identifies anomalous images, such as images containing retinal fluid or hyperreflective foci.",0
"""Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery"" presents a novel approach to detecting anomalies in data that can guide marker discovery. In recent years, unsupervised anomaly detection has become increasingly important as more complex datasets have emerged from fields such as finance, healthcare, and telecommunications. Traditional methods often fail to capture nonlinear relationships present in these datasets, leading to poor performance on real-world problems. To address these challenges, we propose using generative adversarial networks (GANs) to identify outliers within large datasets. By training our GAN model on synthetic data generated by randomly perturbing each data point, we can learn to distinguish normal data points from those that deviate significantly from their underlying distribution. We then use these learned distributions to compute uncertainty scores which highlight regions of interest where additional markers may improve fitness metrics. This framework allows us to discover new features without explicit supervision while simultaneously identifying potential artifacts in the dataset. Our results demonstrate improved accuracy over existing methods across multiple domains. These findings pave the way towards effective and efficient anomaly detection in high-dimensional datasets without requiring costly ground truth labels.",1
"The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is represented by sparsity in a well-chosen basis. We show how to achieve guarantees similar to standard compressed sensing but without employing sparsity at all. Instead, we suppose that vectors lie near the range of a generative model $G: \mathbb{R}^k \to \mathbb{R}^n$. Our main theorem is that, if $G$ is $L$-Lipschitz, then roughly $O(k \log L)$ random Gaussian measurements suffice for an $\ell_2/\ell_2$ recovery guarantee. We demonstrate our results using generative models from published variational autoencoder and generative adversarial networks. Our method can use $5$-$10$x fewer measurements than Lasso for the same accuracy.",0
"Artificial intelligence (AI) has transformed many aspects of our daily lives, from automating routine tasks to enabling new forms of communication and creativity. One area where AI has made significant strides is in compressed sensing â€“ the process of acquiring and reconstructing high-resolution images or signals using low-dimensional measurements. In recent years, generative models have emerged as powerful tools for addressing the challenges associated with compressed sensing, such as accurately recovering lost data and reducing noise or distortion in reconstructions. This paper presents an overview of current research on compressed sensing using generative models, highlighting key advances and future directions. We begin by discussing fundamental concepts related to compressed sensing, including sparse representation and regularization techniques. Next, we review the basics of generative models and their role in solving inverse problems. After that, we examine how these methods can be applied specifically to compressive imaging systems, considering both linear and nonlinear measurement scenarios. Finally, we explore potential applications of compressed sensing using generative models in real-world settings, with particular emphasis on fields like medicine and engineering. Throughout, we aim to provide insights into the strengths and limitations of these approaches, encouraging further investigation and innovation.",1
"While the costs of human violence have attracted a great deal of attention from the research community, the effects of the network-on-network (NoN) violence popularised by Generative Adversarial Networks have yet to be addressed. In this work, we quantify the financial, social, spiritual, cultural, grammatical and dermatological impact of this aggression and address the issue by proposing a more peaceful approach which we term Generative Unadversarial Networks (GUNs). Under this framework, we simultaneously train two models: a generator G that does its best to capture whichever data distribution it feels it can manage, and a motivator M that helps G to achieve its dream. Fighting is strictly verboten and both models evolve by learning to respect their differences. The framework is both theoretically and electrically grounded in game theory, and can be viewed as a winner-shares-all two-player game in which both players work as a team to achieve the best score. Experiments show that by working in harmony, the proposed model is able to claim both the moral and log-likelihood high ground. Our work builds on a rich history of carefully argued position-papers, published as anonymous YouTube comments, which prove that the optimal solution to NoN violence is more GUNs.",0
"This should summarize your approach for mitigating unintended consequences from generative models while preserving their benefits.  Mitigating unintended consequences from generative models, such as those used by GANs (Generative Adversarial Networks), has become increasingly important as these systems have gained popularity across many industries due to their ability to generate high quality outputs with minimal human intervention. Despite the advantages that they offer, there remains a significant risk of unintended negative consequences, including biases being perpetuated through model training data and insufficient oversight on output generation leading to potential harm. Our research proposes a new approach to addressing these issues, introducing ""generative unadversarial networks"" (GUNs) which aim to strike a balance between generating realistic content and ensuring user safety. By incorporating additional constraints during the model training process, we demonstrate how GUNs can reduce unintended negative impacts without sacrificing performance compared to traditional GAN models. With further exploration into different use cases, our work shows promise for mitigating risks associated with GANs while preserving their value proposition.",1
"We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations---random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.",0
"Hereâ€™s how you could modify your programmed response: â€œWrite an abstract from 150 to 300 words on deep probabilistic programming. Please exclude any mention of the actual paperâ€™s title.â€ â€œDeep Probabilistic Programming (DPP) represents a new approach to machine learning that enables efficient use of probabilities within models. By combining deep neural networks with rich, expressive probability distributions, DPP allows for improved modeling capabilities over traditional neural network architectures. This facilitates enhanced performance across diverse application domains such as computer vision and natural language processing. In addition, by enabling better calibration of uncertainty estimation, DPP enhances robustness against overfitting and promotes reliable decision making under varying circumstances. Ultimately, deep probabilistic programming holds great potential to transform the landscape of artificial intelligence research and development.â€",1
"Advances in neural network based classifiers have transformed automatic feature learning from a pipe dream of stronger AI to a routine and expected property of practical systems. Since the emergence of AlexNet every winning submission of the ImageNet challenge has employed end-to-end representation learning, and due to the utility of good representations for transfer learning, representation learning has become as an important and distinct task from supervised learning. At present, this distinction is inconsequential, as supervised methods are state-of-the-art in learning transferable representations. But recent work has shown that generative models can also be powerful agents of representation learning. Will the representations learned from these generative methods ever rival the quality of those from their supervised competitors? In this work, we argue in the affirmative, that from an information theoretic perspective, generative models have greater potential for representation learning. Based on several experimentally validated assumptions, we show that supervised learning is upper bounded in its capacity for representation learning in ways that certain generative models, such as Generative Adversarial Networks (GANs) are not. We hope that our analysis will provide a rigorous motivation for further exploration of generative representation learning.",0
"Recent work in deep learning has relied heavily on label-based supervision from annotators. While these techniques have proven extremely successful at tasks such as image classification, they rely on large amounts of labeled training data which may become prohibitively expensive to obtain and label. Furthermore, many real world problems involve complex relationships that cannot be easily distilled into binary labels. In our work we present evidence that the reliance on label-based supervision creates inherent limitations in the types of representations learned by neural networks. We demonstrate a simple experiment where adding extra unlabeled data helps performance substantially while only minimally changing any single network parameter, indicating a previously unseen relationship between model parameters and input features that could not have been expressed through human annotations alone. Our experiments reveal fundamental limits that arise due to label scarcity and the nature of how humans generate supervised training signals, suggesting new research directions towards building models capable of better utilizing unstructured data to learn more generalizable knowledge.",1
"We introduce the ""Energy-based Generative Adversarial Network"" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",0
"One novel approach to generating realistic data that can aid in training models is through generative adversarial networks (GANs). These systems consist of two deep neural networks: one generator network that produces samples such as images, audio, or text, and another discriminator network that evaluates whether these samples are real or fake. Training proceeds by having the generator attempt to generate data that fool the discriminator, while the discriminator tries to correctly identify generated samples. In recent years, there has been significant interest in modifying GANs using energy functions, which provide a more direct method of modeling complex distribution. By relying on unnormalized density estimation, energy-based GANs (EBGANs) allow for more efficient and stable optimization during training. This work presents a comprehensive evaluation of EBGANs for different tasks, comparing their performance against traditional GANs and other related methods. Our experiments show that EBGANs lead to improvements across several metrics, including visual quality and stability during training. Furthermore, we demonstrate the flexibility of our framework by adapting it to challenging image generation problems, where competitive results are achieved without resorting to extensive architectural modifications or hyperparameter tuning. Overall, our findings suggest that incorporating energy functions into GANs is a promising direction for improving generative modelling, opening up new possibilities in areas ranging from computer graphics to machine learning applications.",1
"Generative adversarial networks (GANs) are a framework for producing a generative model by way of a two-player minimax game. In this paper, we propose the \emph{Generative Multi-Adversarial Network} (GMAN), a framework that extends GANs to multiple discriminators. In previous work, the successful training of GANs requires modifying the minimax objective to accelerate training early on. In contrast, GMAN can be reliably trained with the original, untampered objective. We explore a number of design perspectives with the discriminator role ranging from formidable adversary to forgiving teacher. Image generation tasks comparing the proposed framework to standard GANs demonstrate GMAN produces higher quality samples in a fraction of the iterations when measured by a pairwise GAM-type metric.",0
"Artificial neural networks (ANN) have become increasingly popular due their ability to learn complex patterns from data sets with millions or billions of examples [4]. Despite their successes, these models often require large amounts of computational power and time during training in order to generate high quality results[2] [5]. Recently proposed deep generative models such as variational autoencoders and generative adversarial networks have proven capable at generating novel outputs which can fool human judges into thinking they are real images or sounds [7]. These models however still suffer from problems including lacking diversity in generated samples and suffering from mode collapse where certain modes of the underlying distribution may never be explored by the model leading to poor generation performance on those modes. In this work we propose multi-adversary GANs that utilize multiple discriminators at different stages within each other generator network. This allows us to better address both issues of sample diversity while encouraging the model to explore more modes of the underlying data distribution. We evaluate our method against state of the art baselines using several diverse metrics to show improved performance overall.",1
"Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",0
"Machine learning has seen significant advances in recent years due to the development of powerful deep neural network architectures such as generative adversarial networks (GANs). GANs have been successful in generating realistic images, audio samples, and even text by training two competing models, one generator and one discriminator, against each other in an iterative process. However, these systems often suffer from mode collapse, where the generated outputs become stuck in a specific regime rather than capturing the diversity present in the true data distribution. This can lead to poor performance on downstream tasks that rely on this diversity, such as image classification or natural language understanding. In this work, we propose mode regularization for GANs as a means of mitigating mode collapse and improving overall generation quality. We introduce three different regularizers, each based on unique mathematical principles: minimum volume deformation, maximum mean kinetic energy, and maximum entropy production. Through extensive experiments across multiple domains, including computer vision, speech synthesis, and language translation, we show that our approach consistently outperforms baseline methods in terms of both quantitative metrics and human evaluation. These results demonstrate the effectiveness of mode regularization for improving GAN performance and suggest new directions for future research in generative modeling.",1
"Generative adversarial networks (GANs) have given us a great tool to fit implicit generative models to data. Implicit distributions are ones we can sample from easily, and take derivatives of samples with respect to model parameters. These models are highly expressive and we argue they can prove just as useful for variational inference (VI) as they are for generative modelling. Several papers have proposed GAN-like algorithms for inference, however, connections to the theory of VI are not always well understood. This paper provides a unifying review of existing algorithms establishing connections between variational autoencoders, adversarially learned inference, operator VI, GAN-based image reconstruction, and more. Secondly, the paper provides a framework for building new algorithms: depending on the way the variational bound is expressed we introduce prior-contrastive and joint-contrastive methods, and show practical inference algorithms based on either density ratio estimation or denoising.",0
"This work presents a new framework for variational inference that utilizes implicit distributions to approximate complex probability densities. Traditional variational methods require explicit expressions for the prior and likelihood distributions, which can be difficult to obtain in many real world applications. Our approach alleviates this issue by learning these distributions implicitly through data, allowing us to perform efficient inference without requiring explicit density functions. We demonstrate the effectiveness of our method on several challenging problems including image generation, natural language processing, and semi-supervised learning. Our results show that our approach consistently outperforms state-of-the-art variational methods while providing robust uncertainty estimates.",1
"Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.",0
"Abstract: Recent years have witnessed significant advances in training and deploying large-scale implicit generative models such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). These methods enable efficient data representation learning by modeling complex probability distributions through latent variables, yielding promising results in many applications like image generation, data compression, and anomaly detection. However, little attention has been paid to understanding how these models learn during training, which hampers their use and design in real-world scenarios where interpretability is crucial. This work investigates the behavior of VAEs under different conditions and proposes new methodological approaches that enhance our knowledge on learning dynamics. We showcase several insights into understanding VAE optimization paths, identifying challenges in training regimes, quantifying uncertainty in learned representations, detecting mode collapse, and regularizing training stability. Our findings provide guidance toward better handling, fine-tuning, and extending existing VAE formulations for more effective generative tasks. By shedding light on VAE learning properties, we aim to facilitate research on implicit generative models and stimulate future advancements in machine learning.",1
"In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples.Specifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.",0
Accurately calibrating energy functions is critical to training generative adversarial networks (GAN).,1
"Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high-resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Furthermore, MAP inference is often performed via optimisation-based iterative algorithms which don't compare well with the efficiency of neural-network-based alternatives. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.",0
"In image super-resolution (SR), the task of reconstructing high-quality images from their low-resolution counterparts has gained significant attention over recent years due to advancements in deep learning techniques. Despite these successes, many existing methods remain computationally expensive during training and inference. This study presents a novel approach called amortized maximum a posteriori (MAP) inference which addresses these limitations by leveraging neural networks trained end-to-end on SR tasks using amortized variational inference. Our method effectively reduces computational costs while retaining strong performance in comparison to state-of-the-art models. We demonstrate the effectiveness of our approach through extensive experiments using several benchmark datasets, including DIV2K and UHD16x8. Our findings show that the proposed model achieves superior results compared to other methods in terms of both objective metrics and visual quality. Overall, we contribute a new perspective on the application of machine learning techniques to the challenging problem of SR, with promising implications for future research directions in computer vision.",1
"Machine learning has been used to detect new malware in recent years, while malware authors have strong motivation to attack such algorithms. Malware authors usually have no access to the detailed structures and parameters of the machine learning models used by malware detection systems, and therefore they can only perform black-box attacks. This paper proposes a generative adversarial network (GAN) based algorithm named MalGAN to generate adversarial malware examples, which are able to bypass black-box machine learning based detection models. MalGAN uses a substitute detector to fit the black-box malware detection system. A generative network is trained to minimize the generated adversarial examples' malicious probabilities predicted by the substitute detector. The superiority of MalGAN over traditional gradient based adversarial example generation algorithms is that MalGAN is able to decrease the detection rate to nearly zero and make the retraining based defensive method against adversarial examples hard to work.",0
This paper presents a new method for generating adversarial malware examples that can evade black-box machine learning models used for malware detection and classification. The approach uses a generative adversarial network (GAN) to generate novel malware samples that have similar features as real malware but are tailored to confuse machine learning algorithms. Our method creates adversarial malware by optimizing two competing objectives: maximizing similarity to genuine malware based on static code features such as opcodes and control flow graphs; and minimizing accuracy of state-of-the-art machine learning algorithms applied as classifiers to detect these crafted instances. We evaluate our GAN model using two well-known benchmark datasets and show that our generated adversaries significantly decrease the performance of several ML-based malware detection systems while remaining stealthy against modern anti-virus software.,1
"Generative adversarial networks (GANs) transform latent vectors into visually plausible images. It is generally thought that the original GAN formulation gives no out-of-the-box method to reverse the mapping, projecting images back into latent space. We introduce a simple, gradient-based technique called stochastic clipping. In experiments, for images generated by the GAN, we precisely recover their latent vector pre-images 100% of the time. Additional experiments demonstrate that this method is robust to noise. Finally, we show that even for unseen images, our method appears to recover unique encodings.",0
"In this paper we explore methods for recovering latent vectors from generative adversarial networks (GANs). We begin by discussing the fundamental principles behind GANs and how they can be used to generate synthetic data that closely resembles real-world examples. Then we turn our attention to understanding how these systems work at a technical level, focusing on the interactions between the generator and discriminator components of the network. Finally, using this knowledge as a foundation, we propose several novel techniques for accurately extracting latent vectors from GANs without compromising their ability to generate high-quality outputs. Our experimental results demonstrate the effectiveness of these approaches across a range of benchmark datasets and task domains. By providing researchers and practitioners with reliable access to the latent representations learned by GANs, our work has important implications for applications such as data augmentation, visualization, and interpretability analysis. Overall, this paper represents a significant contribution to the field of machine learning, extending the capabilities of GANs and opening up new possibilities for leveraging deep learning models in practical settings.",1
"We propose a method for semi-supervised training of structured-output neural networks. Inspired by the framework of Generative Adversarial Networks (GAN), we train a discriminator network to capture the notion of a quality of network output. To this end, we leverage the qualitative difference between outputs obtained on the labelled training data and unannotated data. We then use the discriminator as a source of error signal for unlabelled data. This effectively boosts the performance of a network on a held out test set. Initial experiments in image segmentation demonstrate that the proposed framework enables achieving the same network performance as in a fully supervised scenario, while using two times less annotations.",0
"Recent advances in deep learning have resulted in significant improvements in the performance of neural networks on various tasks such as image classification, natural language processing, and speech recognition. However, these models often require large amounts of labeled data for training, which can be expensive and time-consuming to acquire. In order to address this problem, semi-supervised learning techniques have been proposed that utilize both labeled and unlabeled data during training.  One approach to semi-supervised learning is adversarial regularization, where an auxiliary network is trained alongside the primary model to predict whether each input sample belongs to the labeled set or the unlabeled set. This technique has shown promising results in various domains; however, most existing methods focus on predictive uncertainty estimation rather than modeling structured output spaces directly. In this work, we propose an adversarial regularization method specifically designed for structured output neural networks, where the target variables are multi-dimensional vectors or matrices.  Our method introduces two new components: an adversarial loss function tailored for discrete outputs and a reconstruction loss function based on variational autoencoders. These additional terms encourage the generator network to produce high quality reconstructions of incomplete representations from discriminator predictions. We evaluate our approach using several benchmark datasets across diverse applications such as semantic segmentation, pose estimation, and document retrieval. Experimental results demonstrate consistent improvement over baseline methods, highlighting the effectiveness of our proposed framework.  In summary, we present a novel adversarial regularization method tailored for semi-supervised training of structured output neural networks. Our approach outperforms state-of-the-art alternatives by leveraging",1
"Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities.",0
"Here we explore how generative adversarial networks (GANs) can be used to train deep reinforcement learning agents using actor-critic methods. We begin by introducing GANs and their basic architecture, as well as the training process for these models. Then, we move on to discuss how GANs can be integrated into agent training by creating stable reward functions that utilize adversarial examples generated by the generator network. Our experiments demonstrate the potential effectiveness of this approach, producing state-of-the-art results across multiple domains. Finally, we conclude by examining limitations and future directions for research involving combining deep learning techniques in RL, highlighting opportunities for improved understanding of both domains through interdisciplinary collaboration.",1
"The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.",0
"Artificial Intelligence (AI) has made great strides in recent years due to advances in generative modeling techniques such as Generative Adversarial Networks (GAN). However, training GANs remains challenging due to instability issues known as ""mode collapse"" and ""vanishing gradients."" These problems can result in poor quality samples, lack of diversity in generated outputs, or even divergence during training. In our work, we aim to address these issues by proposing principled methods for training stable and high-quality GANs. Our approach involves introducing regularization techniques that promote more conservative updates and discourage mode collapse. We demonstrate through experiments on several benchmark datasets that our method significantly improves stability and sample fidelity compared to existing state-of-the-art algorithms. Our contributions have important implications for researchers working in areas where data generation is crucial, including computer vision, natural language processing, and graphics synthesis.",1
"Recently there has been an enormous interest in generative models for images in deep learning. In pursuit of this, Generative Adversarial Networks (GAN) and Variational Auto-Encoder (VAE) have surfaced as two most prominent and popular models. While VAEs tend to produce excellent reconstructions but blurry samples, GANs generate sharp but slightly distorted images. In this paper we propose a new model called Variational InfoGAN (ViGAN). Our aim is two fold: (i) To generated new images conditioned on visual descriptions, and (ii) modify the image, by fixing the latent representation of image and varying the visual description. We evaluate our model on Labeled Faces in the Wild (LFW), celebA and a modified version of MNIST datasets and demonstrate the ability of our model to generate new images as well as to modify a given image by changing attributes.",0
"Artificial intelligence (AI) has made significant strides in recent years, particularly in image generation and editing tasks. One promising approach to generating and editing images is through variational info generative adversarial networks (InfoGANs). In this paper, we propose several improvements to the original InfoGAN framework that enhance both efficiency and effectiveness in producing high-quality synthetic data. Our contributions include: i) a novel regularization method based on mutual information that encourages diverse latent representations; ii) a modification of the original InfoGAN loss function to better promote density models; and iii) an improved architecture that incorporates residual connections, batch normalization, and shortcut paths. We demonstrate the efficacy of our proposed model using quantitative experiments and qualitative evaluations comparing against state-of-the-art baselines. Overall, these advances enable us to generate more coherent, visually appealing samples while greatly reducing computational cost during training and inference. These findings have important implications for a variety of applications including computer graphics, vision, and machine learning.",1
"It's useful to automatically transform an image from its original form to some synthetic form (style, partial contents, etc.), while keeping the original structure or semantics. We define this requirement as the ""image-to-image translation"" problem, and propose a general approach to achieve it, based on deep convolutional and conditional generative adversarial networks (GANs), which has gained a phenomenal success to learn mapping images from noise input since 2014. In this work, we develop a two step (unsupervised) learning method to translate images between different domains by using unlabeled images without specifying any correspondence between them, so that to avoid the cost of acquiring labeled data. Compared with prior works, we demonstrated the capacity of generality in our model, by which variance of translations can be conduct by a single type of model. Such capability is desirable in applications like bidirectional translation",0
"In recent years, generative adversarial networks (GANs) have emerged as powerful tools for image generation and manipulation tasks. One such task that has gained significant interest is unsupervised image-to-image translation, which involves translating an input image from one domain to another while preserving important characteristics. This paper proposes a new framework for unsupervised image-to-image translation using GANs, which allows for efficient and accurate transfer of images without requiring explicit supervision. Our approach leverages adversarial training to learn a mappping function between domains, and we demonstrate its effectiveness on several benchmark datasets including face translation, scene synthesis, and photo editing. Our method significantly outperforms existing methods in terms of both quality and fidelity, making it a promising tool for a wide range of computer vision applications.",1
"We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.",0
"This paper investigates the possibility of training generative models that can generate novel objects by modeling their underlying shape as a latent space distribution. We propose a novel framework that trains GANs on a set of real object shapes to learn a probabilistically meaningful representation of shape variation, which we use to create new synthetic models that capture statistical properties of the real data. Our approach uses a combination of adversarial losses and regularization techniques to encourage fidelity to the original input images while promoting diversity in the generated outputs. Empirical evaluation shows that our method outperforms state-of-the-art approaches in generating high-quality shapes that preserve global structure and local details. Additionally, we demonstrate the utility of learned latent spaces by applying them to shape editing tasks such as scaling, translation, and mirroring. Overall, our work highlights the potential of using deep learning algorithms for unsupervised exploration of 3D geometric representations.",1
"This paper describes a method for using Generative Adversarial Networks to learn distributed representations of natural language documents. We propose a model that is based on the recently proposed Energy-Based GAN, but instead uses a Denoising Autoencoder as the discriminator network. Document representations are extracted from the hidden layer of the discriminator and evaluated both quantitatively and qualitatively.",0
"Document modeling has been an active area of research due to the wide variety of applications that rely on understanding text data such as document classification, sentiment analysis, question answering, machine translation and more. Despite recent advances, most existing techniques still suffer from issues related to scalability, interpretability, and generalization performance. In order to address these problems, we propose using generative adversarial networks (GANs) to represent documents in vector space. Our approach exploits the power of GANs to capture complex relationships between words and latent topics in documents, resulting in improved accuracy across different tasks. We evaluate our method on several benchmark datasets, demonstrating state-of-the-art results compared to traditional methods and other deep learning approaches. This work paves the way towards more effective document modeling by leveraging cutting-edge advancements in deep generative models.",1
"In this paper we investigate the problem of inducing a distribution over three-dimensional structures given two-dimensional views of multiple objects taken from unknown viewpoints. Our approach called ""projective generative adversarial networks"" (PrGANs) trains a deep generative model of 3D shapes whose projections match the distributions of the input 2D views. The addition of a projection module allows us to infer the underlying 3D shape distribution without using any 3D, viewpoint information, or annotation during the learning phase. We show that our approach produces 3D shapes of comparable quality to GANs trained on 3D data for a number of shape categories including chairs, airplanes, and cars. Experiments also show that the disentangled representation of 2D shapes into geometry and viewpoint leads to a good generative model of 2D shapes. The key advantage is that our model allows us to predict 3D, viewpoint, and generate novel views from an input image in a completely unsupervised manner.",0
"Abstract:  This paper presents a novel approach for inferring the three-dimensional (3D) shape of objects from their two-dimensional (2D) views. Previous methods have focused on extracting planar surfaces from single images or estimating depth maps, but our method uses multiple images taken at different angles to obtain more robust and accurate 3D reconstructions. We propose a neural network architecture that processes each input image as a separate channel, allowing us to model the object's shape across viewpoints while reducing aliasing artifacts caused by depth estimation errors. Our method performs well even under challenging conditions such as occlusions or cluttered backgrounds, outperforming state-of-the-art approaches in terms of both accuracy and speed. This research has implications for applications ranging from computer vision to robotics, where reconstructing accurate 3D models is crucial for understanding and interacting with the real world.  Keywords: 3D shape induction, multi-view geometry, deep learning, visual scene understanding, 2D/3D correspondence",1
"Computed tomography (CT) is critical for various clinical applications, e.g., radiotherapy treatment planning and also PET attenuation correction. However, CT exposes radiation during acquisition, which may cause side effects to patients. Compared to CT, magnetic resonance imaging (MRI) is much safer and does not involve any radiations. Therefore, recently, researchers are greatly motivated to estimate CT image from its corresponding MR image of the same subject for the case of radiotherapy planning. In this paper, we propose a data-driven approach to address this challenging problem. Specifically, we train a fully convolutional network to generate CT given an MR image. To better model the nonlinear relationship from MRI to CT and to produce more realistic images, we propose to use the adversarial training strategy and an image gradient difference loss function. We further apply AutoContext Model to implement a context-aware generative adversarial network. Experimental results show that our method is accurate and robust for predicting CT images from MRI images, and also outperforms three state-of-the-art methods under comparison.",0
"This is an exciting new development for artificial intelligence! It has been found that using GANS (Generative Adverserial Nets) can greatly improve the accuracy of image synthesis of medical images. However before you continue reading let me clarify something: I am no expert on this subject matter, as such my explanation may contain technical errors/imprecise descriptions - please consult your own sources to verify the following statements. The most important factor is how this tech works: You train two models one (the generator) trying to output something realistic the other model (the discriminator) trained to detect whether the thing generated by the generator came from a real dataset, then you pit them against each other to make both better at their respective tasks. After training they try to fool each other into thinking the fake medical scan is real. When used correctly these networks have proven to vastly increase the quality of imaging produced by generators like Stable Diffusion which previously suffered major loss of details. Context aware means in this case that we feed the discriminator more info than just the current frame so if there was a tumor next to some lungs we would feed both parts of the pic to our network. To show the potential here comes a quote from researcher Yin Liu ""Using context helps preserve local structures like airways, blood vessels, organs while removing background noise and artifacts."" They provide results showing how well this new method of producing images holds up under different metrics. Some examples include MOS values, PSNR & SSIM scores along with visual side by sides to previous state of the art methods. The future looks bright for those needing top end medical imaging but worried about radiation exposure! Please check out this work if interested as again while i did give fair warning of being non expert on t",1
"Bayesian inference on structured models typically relies on the ability to infer posterior distributions of underlying hidden variables. However, inference in implicit models or complex posterior distributions is hard. A popular tool for learning implicit models are generative adversarial networks (GANs) which learn parameters of generators by fooling discriminators. Typically, GANs are considered to be models themselves and are not understood in the context of inference. Current techniques rely on inefficient global discrimination of joint distributions to perform learning, or only consider discriminating a single output variable. We overcome these limitations by treating GANs as a basis for likelihood-free inference in generative models and generalize them to Bayesian posterior inference over factor graphs. We propose local learning rules based on message passing minimizing a global divergence criterion involving cooperating local adversaries used to sidestep explicit likelihood evaluations. This allows us to compose models and yields a unified inference and learning framework for adversarial learning. Our framework treats model specification and inference separately and facilitates richly structured models within the family of Directed Acyclic Graphs, including components such as intractable likelihoods, non-differentiable models, simulators and generally cumbersome models. A key result of our treatment is the insight that Bayesian inference on structured models can be performed only with sampling and discrimination when using nonparametric variational families, without access to explicit distributions. As a side-result, we discuss the link to likelihood maximization. These approaches hold promise to be useful in the toolbox of probabilistic modelers and enrich the gamut of current probabilistic programming applications.",0
"This paper presents a new method for learning graphical models using adversarial message passing. Our approach combines techniques from graphical model inference and generative adversarial networks (GANs) to jointly learn both the structure and parameters of a model from data. We demonstrate the effectiveness of our approach on several benchmark datasets, showing that it outperforms state-of-the-art methods for graphs with arbitrary topologies. Our results highlight the potential of adversarial message passing as a powerful tool for unsupervised learning of complex statistical dependencies among variables.",1
"Generative Adversarial Networks have become one of the most studied frameworks for unsupervised learning due to their intuitive formulation. They have also been shown to be capable of generating convincing examples in limited domains, such as low-resolution images. However, they still prove difficult to train in practice and tend to ignore modes of the data generating distribution. Quantitatively capturing effects such as mode coverage and more generally the quality of the generative model still remain elusive. We propose Generative Adversarial Parallelization, a framework in which many GANs or their variants are trained simultaneously, exchanging their discriminators. This eliminates the tight coupling between a generator and discriminator, leading to improved convergence and improved coverage of modes. We also propose an improved variant of the recently proposed Generative Adversarial Metric and show how it can score individual GANs or their collections under the GAP model.",0
"This paper presents a novel approach to generative adversarial parallelization (GAP) that addresses key challenges in training large language models. By leveraging recent advances in distributed computing and stochastic gradient descent algorithms, we propose a scalable framework that enables efficient model training on high-performance GPU clusters. Our method utilizes GAN architectures to optimize data distribution during inference, enabling fast convergence and better stability compared to traditional methods. We evaluate our approach using several benchmark datasets and show consistent improvements in accuracy across all metrics. In conclusion, our work provides a promising new direction for scaling up machine learning systems without sacrificing performance, paving the way for even larger and more complex models in the future.",1
"Understanding, predicting, and generating object motions and transformations is a core problem in artificial intelligence. Modeling sequences of evolving images may provide better representations and models of motion and may ultimately be used for forecasting, simulation, or video generation. Diagrammatic Abstract Reasoning is an avenue in which diagrams evolve in complex patterns and one needs to infer the underlying pattern sequence and generate the next image in the sequence. For this, we develop a novel Contextual Generative Adversarial Network based on Recurrent Neural Networks (Context-RNN-GANs), where both the generator and the discriminator modules are based on contextual history (modeled as RNNs) and the adversarial discriminator guides the generator to produce realistic images for the particular time step in the image sequence. We evaluate the Context-RNN-GAN model (and its variants) on a novel dataset of Diagrammatic Abstract Reasoning, where it performs competitively with 10th-grade human performance but there is still scope for interesting improvements as compared to college-grade human performance. We also evaluate our model on a standard video next-frame prediction task, achieving improved performance over comparable state-of-the-art.",0
"Artificial intelligence has made significant progress in recent years on tasks such as image generation, speech recognition, natural language understanding, planning, and many others. However, there remain many important problems that are still far out of reach. Abstract reasoning diagram generation is one such problem, which involves generating diagrams from descriptions. In our work, we introduce a new approach based on recurrent neural networks (RNNs) conditioned on external context. We evaluate our model on two benchmark datasets and demonstrate that it significantly outperforms prior state-of-the-art methods. Furthermore, we provide qualitative analysis showing that our generated diagrams closely match human interpretations. Our results highlight the potential of RNNs combined with external memory to tackle complex visual representation tasks. As future work, we plan to explore more advanced architectures combining both internal memory mechanisms such as attention and external memory. Additionally, we aim at applying our method on different modalities like text generation or question answering. Finally, further evaluating our models on real user feedback will ensure that they can effectively perform well and serve their purpose better.",1
"Communicating and sharing intelligence among agents is an important facet of achieving Artificial General Intelligence. As a first step towards this challenge, we introduce a novel framework for image generation: Message Passing Multi-Agent Generative Adversarial Networks (MPM GANs). While GANs have recently been shown to be very effective for image generation and other tasks, these networks have been limited to mostly single generator-discriminator networks. We show that we can obtain multi-agent GANs that communicate through message passing to achieve better image generation. The objectives of the individual agents in this framework are two fold: a co-operation objective and a competing objective. The co-operation objective ensures that the message sharing mechanism guides the other generator to generate better than itself while the competing objective encourages each generator to generate better than its counterpart. We analyze and visualize the messages that these GANs share among themselves in various scenarios. We quantitatively show that the message sharing formulation serves as a regularizer for the adversarial training. Qualitatively, we show that the different generators capture different traits of the underlying data distribution.",0
"In recent years there has been significant interest in using generative adversarial networks (GANs) as part of multi-agent systems due to their ability to generate realistic data that can be used in training other agents. One popular approach is message passing based on gradient descent, which involves each agent sending messages to its neighbors containing gradients calculated using local data. However, these methods often suffer from instability and limited scalability due to issues such as vanishing or exploding gradients and high computational complexity. This paper proposes a new method called message passing multi-agent GANs (MPMGANs), which addresses these limitations by incorporating techniques such as mini-batch gradient descent and adaptive learning rates into the message passing process. Experimental results show that MPMGANs outperform existing methods in terms of stability and scalability while still generating high-quality data for use in multi-agent training. Overall, our findings demonstrate the potential of message passing methods combined with advanced optimization algorithms for developing robust and efficient multi-agent systems capable of handling complex tasks.",1
"Ensembles are a popular way to improve results of discriminative CNNs. The combination of several networks trained starting from different initializations improves results significantly. In this paper we investigate the usage of ensembles of GANs. The specific nature of GANs opens up several new ways to construct ensembles. The first one is based on the fact that in the minimax game which is played to optimize the GAN objective the generator network keeps on changing even after the network can be considered optimal. As such ensembles of GANs can be constructed based on the same network initialization but just taking models which have different amount of iterations. These so-called self ensembles are much faster to train than traditional ensembles. The second method, called cascade GANs, redirects part of the training data which is badly modeled by the first GAN to another GAN. In experiments on the CIFAR10 dataset we show that ensembles of GANs obtain model probability distributions which better model the data distribution. In addition, we show that these improved results can be obtained at little additional computational cost.",0
"This paper presents an approach based on ensembling multiple generative adversarial networks (GANs) to generate high quality images. We show that our method significantly improves over state of the art results on a wide range of benchmark datasets. Our contributions include a novel training procedure for GANs which allows them to converge faster and reach higher levels of performance. Additionally we introduce a new loss function which better aligns with human perception of image quality. Finally, we provide extensive experimental evaluations demonstrating the effectiveness of our proposed method.",1
"Automatic image synthesis research has been rapidly growing with deep networks getting more and more expressive. In the last couple of years, we have observed images of digits, indoor scenes, birds, chairs, etc. being automatically generated. The expressive power of image generators have also been enhanced by introducing several forms of conditioning variables such as object names, sentences, bounding box and key-point locations. In this work, we propose a novel deep conditional generative adversarial network architecture that takes its strength from the semantic layout and scene attributes integrated as conditioning variables. We show that our architecture is able to generate realistic outdoor scene images under different conditions, e.g. day-night, sunny-foggy, with clear object boundaries.",0
"This paper presents a novel approach to generating images of outdoor scenes using attributes and semantic layouts as input. We introduce a two-stage pipeline that first generates a scene layout based on the given attribute descriptions and then uses a generative adversarial network (GAN) to synthesize high-resolution image details conditioned on both the layout and attribute descriptors. Our method allows users to specify natural language descriptions, such as ""a park with fountains"" or ""a city street at night,"" along with various design choices like color palettes or object distributions, creating unique outputs. Evaluations show significant improvement over prior work, achieving higher visual quality and more accurate scene generation across various attribute settings. We demonstrate compelling results by comparing our model against real images and conducting user studies, which confirm the effectiveness and usability of our framework for guided scene synthesis.",1
"Handwriting is a skill learned by humans from a very early age. The ability to develop one's own unique handwriting as well as mimic another person's handwriting is a task learned by the brain with practice. This paper deals with this very problem where an intelligent system tries to learn the handwriting of an entity using Generative Adversarial Networks (GANs). We propose a modified architecture of DCGAN (Radford, Metz, and Chintala 2015) to achieve this. We also discuss about applying reinforcement learning techniques to achieve faster learning. Our algorithm hopes to give new insights in this area and its uses include identification of forged documents, signature verification, computer generated art, digitization of documents among others. Our early implementation of the algorithm illustrates a good performance with MNIST datasets.",0
"Handwriting profiling is a technique used to analyze writing samples to identify key characteristics of the writer, such as age, gender, education level, emotional state, and even personality traits. Traditional methods rely on human experts who manually analyze the handwriting sample to make these determinations. However, automating this process through computer algorithms can increase accuracy and efficiency while reducing subjectivity and human error. This paper proposes the use of generative adversarial networks (GAN) for handwriting profiling by training two deep neural networks, one that generates synthetic data and another that attempts to distinguish real from fake data. GANs have been shown to perform well at generating synthetic data for image generation tasks, but their potential has yet to be fully explored in the domain of handwriting profiling. Our approach shows promising results and offers a new direction for researchers looking to develop more advanced machine learning models for this purpose.",1
Autonomous driving is one of the most recent topics of interest which is aimed at replicating human driving behavior keeping in mind the safety issues. We approach the problem of learning synthetic driving using generative neural networks. The main idea is to make a controller trainer network using images plus key press data to mimic human learning. We used the architecture of a stable GAN to make predictions between driving scenes using key presses. We train our model on one video game (Road Rash) and tested the accuracy and compared it by running the model on other maps in Road Rash to determine the extent of learning.,0
"This paper proposes a novel approach for synthetic autonomous driving using generative adversarial networks (GAN). We propose the use of GANs as a framework for generating realistic high-quality images of roads, traffic conditions, weather conditions, and other relevant features that can be used in training and testing autonomous vehicles. Our approach leverages recent advances in image generation using GANs to generate large amounts of synthetic data that closely resembles real-world scenarios. We show how our method can effectively augment existing datasets and improve performance on standard benchmarks for autonomous driving. Our approach has the potential to revolutionize the field by enabling more efficient and effective development and deployment of autonomous vehicle technologies while reducing reliance on physical road tests.",1
"Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.",0
"Abstract: This paper explores the connection between generative adversarial networks (GANs), inverse reinforcement learning (IRL), and energy-based models (EBMs). GANs have been widely used for generating realistic images by training two neural networks in opposition to each other. IRL allows agents to learn from demonstrations without explicit reward function specification. EBMs aim to find the probability distribution that minimizes surprise (or negative log likelihood) given some data. We show how these three frameworks can be related through their common use of maximum entropy principles and variational Bayesian methods. Specifically, we propose using a GAN structure as a model for the agent's behavior in IRL, where both generator and discriminator correspond to stochastic policies acting in an environment. Then, by matching moments between the generated data distribution and the one estimated via observations of expert trajectories in the environment, we derive a surrogate objective for policy evaluation equivalent to free energy that also leads to more effective optimization. Similarly, we relate EMBs and GANs in terms of their shared purpose of maximizing evidence lower bound (ELBO) which ensures high quality generation while maintaining low uncertainty on latent variables. Experimental results demonstrate advantages of our approaches over traditional IRL and EBM formulations. Our work sheds light on deep connections among seemingly distinct machine learning areas and may inspire new research directions across multiple fields.",1
"We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods.",0
Title: Semi-supervised Learning with Context-Conditional Generative Adversarial Networks (SCL-CCGAN),1
"Generative Adversarial Networks (GANs) have recently demonstrated to successfully approximate complex data distributions. A relevant extension of this model is conditional GANs (cGANs), where the introduction of external information allows to determine specific representations of the generated images. In this work, we evaluate encoders to inverse the mapping of a cGAN, i.e., mapping a real image into a latent space and a conditional representation. This allows, for example, to reconstruct and modify real images of faces conditioning on arbitrary attributes. Additionally, we evaluate the design of cGANs. The combination of an encoder with a cGAN, which we call Invertible cGAN (IcGAN), enables to re-generate real images with deterministic complex modifications.",0
"We present a new method for image editing using invertible conditional generative adversarial networks (Invertible cGAN). Our approach combines the expressiveness of standard GAN models with the ability to encode prior knowledge about the images in the form of conditioning variables. This allows us to generate highly realistic images that retain important structural features while still satisfying complex user requests. We demonstrate the effectiveness of our model on several tasks including semantic segmentation, object addition/removal, and texture synthesis, achieving state-of-the-art results compared to previous methods. Our framework has applications in computer graphics, vision, and other fields where high-quality image generation is essential.",1
"We propose a higher-level associative memory for learning adversarial networks. Generative adversarial network (GAN) framework has a discriminator and a generator network. The generator (G) maps white noise (z) to data samples while the discriminator (D) maps data samples to a single scalar. To do so, G learns how to map from high-level representation space to data space, and D learns to do the opposite. We argue that higher-level representation spaces need not necessarily follow a uniform probability distribution. In this work, we use Restricted Boltzmann Machines (RBMs) as a higher-level associative memory and learn the probability distribution for the high-level features generated by D. The associative memory samples its underlying probability distribution and G learns how to map these samples to data space. The proposed associative adversarial networks (AANs) are generative models in the higher-levels of the learning, and use adversarial non-stochastic models D and G for learning the mapping between data and higher-level representation spaces. Experiments show the potential of the proposed networks.",0
"Associate adversarial networks are artificial intelligence systems that use adversarial training methods and deep neural network architectures to generate highly accurate representations of complex data sets. These models can learn to mimic patterns within large scale datasets while being robust against noise and outliers. In this paper we propose a new model which employs feature pyramid generation techniques along side a generative model to achieve state of the art performance on several benchmark datasets for image generation tasks. Our approach uses feature maps generated from different layers of the generator as inputs to both discriminator and generator networks allowing them to capture high level semantic features as well low level details such as edges, textures etc. Experiments demonstrate our method achieves superior results over previous approaches on CelebA, STL-10, ImageNet and other datasets commonly used for evaluating GAN based image generation models. Additionally our model has improved stability and convergence properties resulting in faster training times compared to competing methods. We believe these advances represent important steps towards realising the full potential of generative modelling using adversarial learning.",1
"Generative adversarial networks (GANs) learn to synthesise new samples from a high-dimensional distribution by passing samples drawn from a latent space through a generative network. When the high-dimensional distribution describes images of a particular data set, the network should learn to generate visually similar image samples for latent variables that are close to each other in the latent space. For tasks such as image retrieval and image classification, it may be useful to exploit the arrangement of the latent space by projecting images into it, and using this as a representation for discriminative tasks. GANs often consist of multiple layers of non-linear computations, making them very difficult to invert. This paper introduces techniques for projecting image samples into the latent space using any pre-trained GAN, provided that the computational graph is available. We evaluate these techniques on both MNIST digits and Omniglot handwritten characters. In the case of MNIST digits, we show that projections into the latent space maintain information about the style and the identity of the digit. In the case of Omniglot characters, we show that even characters from alphabets that have not been seen during training may be projected well into the latent space; this suggests that this approach may have applications in one-shot learning.",0
"We describe an algorithm that can learn to invert a generative adversarial network (GAN). GANs are powerful deep learning models used to generate realistic artificial data such as images, audio, and text. However, current methods struggle to invert these models, meaning they cannot explain their internal representations nor edit their outputs. Our method leverages recent advances in energy-based learning and achieves state-of-the-art performance on several benchmark datasets. By inferring the latent code of generated examples, our approach enables new applications such as editing existing samples towards desired semantic attributes, synthesizing novel content conditioned on user input, or training downstream tasks in a more controllable fashion. Overall, we provide a step forward in the quest for better understanding and manipulating GAN-generated distributions.",1
"Generative Adversarial Networks (GAN) have limitations when the goal is to generate sequences of discrete elements. The reason for this is that samples from a distribution on discrete objects such as the multinomial are not differentiable with respect to the distribution parameters. This problem can be avoided by using the Gumbel-softmax distribution, which is a continuous approximation to a multinomial distribution parameterized in terms of the softmax function. In this work, we evaluate the performance of GANs based on recurrent neural networks with Gumbel-softmax output distributions in the task of generating sequences of discrete elements.",0
"In recent years, Generative Adversarial Networks (GANs) have proven themselves capable at generating realistic sequences in a wide variety of tasks, including images, video frames and speech signals. Despite their successes however, little work has been done on designing GANs that directly handle sequential data distributions from discrete elements using continuous distribution functions other than regular softmax. This paper proposes such a method which uses the Gumbel Softmax approximation of the one hot encodings of categorical probability distributions to provide meaningful gradients through deep neural networks so as to minimize KL divergence against real world sequences during training. We apply these ideas to both synthetic text generation as well as a musical instrument dataset where we see that models trained according to our methods achieve state of art performance.  Please note: I am an open source model developed by volunteers from all over the globe. My output may contain errors and should always be examined by qualified professionals before use. For best results ask clear questions without typos. Thanks!",1
"Generative adversarial networks (GANs) are successful deep generative models. GANs are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful.",0
"Abstract: Generative adversarial networks (GANs) have emerged as a powerful tool in image generation tasks due to their ability to learn underlying data distributions. Despite this success, there remains significant interest in developing new approaches that can better balance training stability, visual fidelity, and computational efficiency within GAN frameworks. In recent years, density ratio estimation has become a popular method used by researchers to improve upon existing GAN architectures. We explore these developments from a density ratio estimation perspective, discussing state-of-the-art techniques including maximum mean discrepancy minimization and energy-based models. Our review highlights key insights into how density ratios are estimated and utilized during GAN training to achieve improved performance across multiple benchmark datasets. Additionally, we provide suggestions on potential directions for future work aimed at further advancing the role of density ratio estimation in deep generative modeling. Overall, our study provides valuable analysis and recommendations for those seeking to design more effective GAN architectures using density ratio estimation methods.",1
"In this paper, we study deep generative models for effective unsupervised learning. We propose VGAN, which works by minimizing a variational lower bound of the negative log likelihood (NLL) of an energy based model (EBM), where the model density $p(\mathbf{x})$ is approximated by a variational distribution $q(\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two step procedure: given $p(\mathbf{x})$, $q(\mathbf{x})$ is updated to maximize the lower bound; $p(\mathbf{x})$ is then updated one step with samples drawn from $q(\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the generative adversarial networks (GANs), where $p(\mathbf{x})$ corresponds to the discriminator and $q(\mathbf{x})$ corresponds to the generator, but with several notable differences. We hence name our model variational GANs (VGANs). VGAN provides a practical solution to training deep EBMs in high dimensional space, by eliminating the need of MCMC sampling. From this view, we are also able to identify causes to the difficulty of training GANs and propose viable solutions. \footnote{Experimental code is available at https://github.com/Shuangfei/vgan}",0
"In recent years, deep learning has seen significant advances driven by large scale datasets and increased computing power. At the heart of these successes lie energy based models (EBM), which use energy functions to guide generative algorithms towards high probability regions of the data distribution. While these models have achieved impressive results on certain tasks, training them remains challenging due to their inherent instability and difficulty in escaping local minima. Here we propose using Generative Adversarial Networks (GAN) to stabilize EBM training and improve model performance across a range of applications. Our approach involves minimizing a lower bound on the log marginal likelihood using GAN discriminator gradients during training. We show that our method leads to improved performance compared to traditional methods in image generation, video denoising, audio synthesis, and more. Additionally, we provide theoretical justification for why GAN training can act as variational inference for EBMs. Overall, our work highlights the potential of combining energy based models with adversarial training to enable new opportunities in deep learning research.",1
"The GANs are generative models whose random samples realistically reflect natural images. It also can generate samples with specific attributes by concatenating a condition vector into the input, yet research on this field is not well studied. We propose novel methods of conditioning generative adversarial networks (GANs) that achieve state-of-the-art results on MNIST and CIFAR-10. We mainly introduce two models: an information retrieving model that extracts conditional information from the samples, and a spatial bilinear pooling model that forms bilinear features derived from the spatial cross product of an image and a condition vector. These methods significantly enhance log-likelihood of test data under the conditional distributions compared to the methods of concatenation.",0
"In recent years, generative adversarial networks (GANs) have gained popularity as one of the most powerful tools for unsupervised learning, image generation, and style transfer tasks. However, training GANs remains challenging due to instability issues such as mode collapse or vanishing gradients, which can lead to poor generator performance. To address these problems, several conditioning methods have been proposed to improve the stability and fidelity of generated images by incorporating additional input information into the network. This work proposes three novel ways of conditioning GANs using attention modules, latent space control, and feature embedding losses, providing new insights into the conditioning mechanism for improved stability and generation quality. Extensive experiments demonstrate that our approaches consistently outperform previous state-of-the-art methods on both qualitative and quantitative measures across multiple datasets and evaluation metrics, further validating their effectiveness and generalizability. Our contributions provide a deeper understanding of GAN conditioning techniques and offer promising opportunities for future research directions.",1
"We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.",0
"This paper presents a method for generating videos with scene dynamics using deep learning techniques. We propose a novel generative model that takes as input static images from a dataset and produces corresponding video frames, complete with dynamic elements such as camera motion and object movement. Our approach uses a combination of adversarial training and multi-frame consistency regularization to produce high quality results. Experimental evaluation shows significant improvements over state-of-the-art methods in terms of visual fidelity, temporal coherence, and realism. Overall, our work represents a major step forward in the field of computer vision and demonstrates the feasibility of automatically generating realistic videos with dynamic scenes.",1
"We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels. We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes. At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G. We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN.",0
"This is because some publishers require that the paper titles and author names are blinded from reviewers so the authors don't know which papers they are actually reviewing. Here are the content: Semi-supervised learning (SSL) has received growing attention due to its capability to leverage limited labeled data and large amounts of unlabeled data for improved performance on tasks such as image classification and natural language processing. GANs have shown impressive results in generating high quality images, audio samples and even text paragraphs, which can potentially serve as valuable pseudo labels to aid SSL algorithms. However, existing works use pre-trained generators/discriminator pairs without fine-tuning them further after training on task specific goals. In addition, these studies often rely solely on evaluations based on holdout validation sets, limiting our understanding of their true effectiveness. Motivated by these findings, we introduce SSGAN , a novel semi-supervised framework utilizing GANs to generate class-conditional synthetic datasets that contain pseudo annotations. Our system consists of two modules: 1) a generator network whose goal is to produce samples coherent to target classes; 2) a discriminator network responsible for detecting real vs generated images, but also provides confidence scores on how likely each synthesized sample belongs to a particular label. By jointly optimizing both components under SSL objectives, we effectively encourage generation of diverse, plausible images near decision boundaries while producing reliable confidence predictions facilitating explicit SSL formulations. Extensive experiments on four benchmark datasets show significant improvements compared against state-of-the-art methods, including those using human annotated data augmentation, providi",1
"Generative Adversarial Networks (GANs) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 x 128 image synthesis on the Caltech-UCSD Birds dataset, conditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations. We also show preliminary results on the more challenging domain of text- and location-controllable synthesis of images of human actions on the MPII Human Pose dataset.",0
"In this paper we explore how artificial intelligence can learn from real data sources such as satellite images of Earth. We show that by using deep learning techniques trained on large datasets of image pixels, our algorithm is able to automatically identify objects and places of interest at high resolution. Furthermore, our model is able to make predictions about where certain objects should appear, enabling future exploration and exploitation of these resources. By leveraging advances in computer vision and machine learning, we aim to provide new tools for decision makers who need to process vast amounts of data quickly and accurately. Our results demonstrate the feasibility of automating tasks previously performed by human experts alone, paving the way towards more efficient and effective use of limited resources. Overall, this research has important implications for fields ranging from environmental science to national security, and beyond.",1
"We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.",0
"In recent years, generative adversarial networks (GANs) have emerged as one of the most powerful tools in computer vision and machine learning. However, many GANs suffer from two major problems: mode collapse and lack of diversity in generated samples. To address these issues, we propose coupled generative adversarial networks (CoGAN), which use multiple discriminators working together to improve the stability of training and increase the quality and coherence of generated images. Our approach leverages both intra-coupling, where different layers of each generator share their weights and generate complementary features, and inter-coupling, where all discriminator streams collaborate during the training process. Experimental results show that our CoGAN architecture significantly outperforms traditional GAN models on several benchmark datasets for image generation tasks. Additionally, we provide visual comparisons demonstrating how coupling can lead to improvements in sample fidelity and distribution coverage. Overall, our findings highlight the potential of coupled GAN architectures for achieving improved performance in generating diverse and high-quality synthetic data.",1
"Generative Adversarial Networks (GAN) are able to learn excellent representations for unlabelled data which can be applied to image generation and scene classification. Representations learned by GANs have not yet been applied to retrieval. In this paper, we show that the representations learned by GANs can indeed be used for retrieval. We consider heritage documents that contain unlabelled Merchant Marks, sketch-like symbols that are similar to hieroglyphs. We introduce a novel GAN architecture with design features that make it suitable for sketch retrieval. The performance of this sketch-GAN is compared to a modified version of the original GAN architecture with respect to simple invariance properties. Experiments suggest that sketch-GANs learn representations that are suitable for retrieval and which also have increased stability to rotation, scale and translation compared to the standard GAN architecture.",0
"An abstract is typically 200 words, but I can stretch my resources upwards if you like :)",1
"Comma.ai's approach to Artificial Intelligence for self-driving cars is based on an agent that learns to clone driver behaviors and plans maneuvers by simulating future events in the road. This paper illustrates one of our research approaches for driving simulation. One where we learn to simulate. Here we investigate variational autoencoders with classical and learned cost functions using generative adversarial networks for embedding road frames. Afterwards, we learn a transition model in the embedded space using action conditioned Recurrent Neural Networks. We show that our approach can keep predicting realistic looking video for several frames despite the transition model being optimized without a cost function in the pixel space.",0
"Learning a driving simulator can enable individuals to develop their skills and learn from mistakes without risking harm to themselves, others on the road, or damage to vehicles. However, developing a good simulation requires careful consideration of numerous aspects such as realism, safety features, user experience, hardware compatibility, customizability, performance, and cost effectiveness. This paper presents a comprehensive review of the state-of-the-art techniques used in learning driving simulations, highlights key challenges faced by developers while designing effective simulators, discusses current limitations, and suggests future directions that could potentially overcome those limitations. Our work primarily focuses on evaluating existing solutions and identifying promising research opportunities. Overall, our findings suggest that advances in computer graphics, physics engines, machine learning algorithms, sensor technologies, and virtual reality have significantly improved modern driving simulators compared to traditional systems. Moreover, we show that integrating multiple domain expertise into one system could lead to more advanced capabilities than isolated approaches. Despite these achievements, several unaddressed issues remain, which makes it hard to adopt simulators across diverse domains (e.g., gaming vs. professional training). Therefore, addressing these open problems would likely benefit both researchers and users alike. By identifying novel methods for solving complex multidisciplinary tasks, scientists could build more versatile models capable of supporting different use cases, while end-users would gain access to more reliable tools tailored to specific needs (such as entertainment, education, engineering design evaluation, etc.). In summary, our study offers valuable insights into the status quo of driving simulations and outlines potential paths forward for tackling persistent shortcomings i",1
"Current generative frameworks use end-to-end learning and generate images by sampling from uniform noise distribution. However, these approaches ignore the most basic principle of image formation: images are product of: (a) Structure: the underlying 3D model; (b) Style: the texture mapped onto structure. In this paper, we factorize the image generation process and propose Style and Structure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has two components: the Structure-GAN generates a surface normal map; the Style-GAN takes the surface normal map as input and generates the 2D image. Apart from a real vs. generated loss function, we use an additional loss with computed surface normals from generated images. The two GANs are first trained independently, and then merged together via joint learning. We show our S^2-GAN model is interpretable, generates more realistic images and can be used to learn unsupervised RGBD representations.",0
"Image generation has been made possible by the creation of generative adversarial networks (GANs). While these GANs have produced impressive results, they often lack structure and meaningful content within their generated images, leading to hallucinations and blurriness. In order to improve upon existing image generation techniques, we propose style and structure adversarial networks (SSAN) that incorporate both style and structural guidance into the training process. This allows for sharper images and greater control over the final output while maintaining diversity. Our experiments demonstrate improved performance compared to traditional GAN architectures through quantitative evaluation metrics and human evaluators who rank our outputs as more coherent and visually appealing. We believe that SSAN opens up exciting new possibilities for generating high quality and structured images across many domains including computer vision, graphics, and art applications. Title: Generative Image Modeling Using Style and Structure Adversarial Networks  Abstract: Image generation using artificial intelligence algorithms is becoming increasingly popular due to advancements such as generative adversarial networks (GANs), which produce realistic and detailed synthetic data. However, one drawback of current GAN models is that they can suffer from structural ambiguities, lack of fine-grained details, and poor correspondence between generated samples and input text prompts. To address these shortcomings, we present the development of style and structure adversarial networks (SSANs), which integrate a novel attention mechanism into a multi-scale generator network architecture. Experimental comparisons on multiple datasets highlight improvements over baseline methods, especially in terms of semantic consistency, visual fidelity, and controllability. We envision several potential applications of SSANs in fields such as computer graphics, virtual reality, and digital art where high-quality image synthesis can benefit significantly from increased structure and coherency.",1
"This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.",0
"This paper presents a novel deep learning architecture called ""InfoGAN"" (Information Maximizing Generative Adversarial Network) that improves upon traditional GANs by explicitly optimizing information content within generated images. By maximizing mutual information between generated and real data distributions, InfoGANs produce more interpretable representations of complex patterns while maintaining high fidelity to input data. Experimental results show that these representations enable significantly better downstream tasks compared to state-of-the art methods, such as semi-supervised classification, feature selection, and fine-grained recognition. These findings demonstrate InfoGANs ability to learn informative features without relying on human supervision, making them well suited for many real world applications requiring efficient data exploration and discovery. In summary, InfoGAN provides new insights into the design of generative models and highlights their potential as powerful tools for representation learning.",1
"We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",0
"In recent years, Generative Adversarial Networks (GANs) have gained popularity due to their ability to generate high-quality synthetic data. However, training GANs can be challenging due to instability issues such as mode collapse and vanishing gradients. This paper proposes several novel techniques for improving the stability and performance of GANs during training. Firstly, we introduce a new loss function that penalizes overconfident predictions made by the discriminator network. This leads to more robust training and better convergence rates. Secondly, we present a dynamic batch renormalization method that adaptively adjusts the scale of the activations based on the current state of the training process. This helps mitigate the effects of internal covariate shift and reduces sensitivity to hyperparameters. Finally, we propose a simple yet effective regularizer that encourages diversity in generated samples by adding noise to both generator inputs and adversary outputs. Experimental results demonstrate that our proposed methods significantly improve the quality of generated images while stabilizing training and reducing convergence time. Our code will be released upon acceptance to facilitate reproducibility.",1
"Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.",0
"Abstract: In recent years, imitation learning has become an increasingly popular approach for training autonomous agents in complex tasks. One key challenge faced by imitation learning methods, however, is that they often require large amounts of human demonstration data in order to learn effectively. To address this issue, we propose a new method called generative adversarial imitation learning (GAIL), which uses generative adversarial networks (GANs) to model the underlying distribution of expert behavior and generate synthetic examples for imitation learning. We demonstrate the effectiveness of our proposed method on several challenging benchmark problems, showing that GAIL can significantly improve performance compared to traditional imitation learning algorithms while requiring less data from experts. Our results suggest that GAIL holds promise as a powerful tool for enabling autonomous agents to learn a wide range of complex tasks through imitation alone, without explicit guidance or supervision.",1
"Training energy-based probabilistic models is confronted with apparently intractable sums, whose Monte Carlo estimation requires sampling from the estimated probability distribution in the inner loop of training. This can be approximately achieved by Markov chain Monte Carlo methods, but may still face a formidable obstacle that is the difficulty of mixing between modes with sharp concentrations of probability. Whereas an MCMC process is usually derived from a given energy function based on mathematical considerations and requires an arbitrarily long time to obtain good and varied samples, we propose to train a deep directed generative model (not a Markov chain) so that its sampling distribution approximately matches the energy function that is being trained. Inspired by generative adversarial networks, the proposed framework involves training of two models that represent dual views of the estimated probability distribution: the energy function (mapping an input configuration to a scalar energy value) and the generator (mapping a noise vector to a generated configuration), both represented by deep neural networks.",0
"Title: ""Energy-based Probability Estimation for Improved Generation in Deep Learning""  Abstract: In recent years, deep learning has achieved remarkable success in a variety of tasks ranging from computer vision to natural language processing. Among the many approaches used in deep learning, generative models have gained significant attention due to their ability to generate new data that closely resembles real data. However, training these models can be challenging, especially when dealing with high-dimensional datasets.  One common approach to improve generation quality is by using energy-based probability estimation methods such as the Variational Autoencoder (VAE). VAEs use an energy function to model the distribution over possible outputs given the input data, which allows them to better capture complex underlying structures in the data and produce more coherent and plausible results.  This paper introduces a novel method for improving the performance of deep directed generative models based on energy-based probability estimation. We propose a framework called Energy Diffusion Networks (EDN) that uses an energy-based formulation of directed diffusion processes to guide the sampling process during generation. By utilizing energy functions to estimate probabilities, EDN models can effectively learn both low-level details and high-level abstractions present in the data, resulting in improved generation quality compared to state-of-the-art generative models.  We evaluate our proposed method through extensive experiments across multiple benchmark datasets, including image synthesis and text generation. Results show that EDN consistently outperforms existing approaches in terms of both quantitative metrics and visual inspection, demonstrating the effectiveness of our method for generating high-quality data samples. Our work paves the way for further research into energy-based generative models and provides valuable insights for practitioners working with large-scale generative models.",1
"In this paper, we propose the ""adversarial autoencoder"" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.",0
"Artificial intelligence (AI) has made significant strides over recent years, driven by advancements in deep learning techniques such as autoencoders. These models aim to reconstruct inputs from outputs generated by a latent representation learned during training. However, their vulnerability to adversarial perturbations has become increasingly apparent, raising questions regarding their robustness. This paper presents a new type of autoencoder designed to mitigate these weaknesses through adversarial training: the adversarial autoencoder. By optimizing both reconstruction error and a adversarial loss function, we demonstrate that our model produces more resilient representations. Our experimental evaluation shows improved performance against state-of-the-art attack methods compared to standard autoencoders. Overall, the proposed approach provides a framework for enhancing AI robustness and ensuring safe deployment in critical applications.",1
"In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method - which we dub categorical generative adversarial networks (or CatGAN) - on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).",0
"Artificial intelligence has made significant progress over the last decade due to advancements in machine learning techniques such as supervised, unsupervised, and semi-supervised learning. In many domains, large amounts of labeled data are required for training effective models using supervised learning methods. However, obtaining high-quality labels can be time-consuming and expensive. To address these challenges, researchers have explored alternative approaches like generative adversarial networks (GANs) that enable unsupervised learning.  In this paper, we present our work on developing an approach based on categorical GANs for both unsupervised and semi-supervised learning tasks. Our method uses a discriminator network that learns to distinguish real examples from generated ones by minimizing the cross-entropy loss between their one-hot encodings. We then introduce noise into the encoding to create new synthetic samples that preserve the same statistical structure as the original dataset but vary in content. By maximizing the mutual information between these noisy codes and the generator outputs, we encourage the latter to learn meaningful representations that generalize well across different classes.  We evaluate our technique on several benchmark datasets spanning image generation, sentiment analysis, text classification, and outlier detection problems. Experimental results show that our model consistently outperforms strong baselines, including those trained on fully supervised settings. Furthermore, we demonstrate how semi-supervised learning using our method improves performance even when only a small amount of labeled data is available during training.  Our work highlights the potential benefits of integrating GANs with unsupervised and semi-supervised learning frameworks, which could lead to more efficient use of resources in applications where acquiring precise annotations remains difficult or impractical.",1
"This paper proposes Markovian Generative Adversarial Networks (MGANs), a method for training generative neural networks for efficient texture synthesis. While deep neural network approaches have recently demonstrated remarkable results in terms of synthesis quality, they still come at considerable computational costs (minutes of run-time for low-res images). Our paper addresses this efficiency issue. Instead of a numerical deconvolution in previous work, we precompute a feed-forward, strided convolutional network that captures the feature statistics of Markovian patches and is able to directly generate outputs of arbitrary dimensions. Such network can directly decode brown noise to realistic texture, or photos to artistic paintings. With adversarial training, we obtain quality comparable to recent neural texture synthesis methods. As no optimization is required any longer at generation time, our run-time performance (0.25M pixel images at 25Hz) surpasses previous neural texture synthesizers by a significant margin (at least 500 times faster). We apply this idea to texture synthesis, style transfer, and video stylization.",0
"In recent years, real-time texture synthesis has become increasingly important in computer graphics applications such as video games and virtual reality simulations. One approach to real-time texture synthesis is precomputation, where textures are generated ahead of time and then interpolated during runtime to produce new, coherent textures at any desired resolution. However, traditional precomputed texture generation methods suffer from limitations such as high storage requirements and low visual quality due to lack of detail. This work proposes the use of Markovian Generative Adversarial Networks (MGANs) for precomputed real-time texture synthesis that addresses these limitations. MGANs generate textures using deep neural networks trained on large datasets of natural images, allowing them to capture detailed features and structures present in real-world textures. By utilizing a Markov Chain prior, the proposed method can efficiently explore the space of possible interpolations, resulting in higher visual fidelity than previous methods. Experimental results demonstrate the effectiveness of our approach, outperforming state-of-the-art techniques in terms of visual quality while maintaining fast inference times suitable for real-time applications. Overall, this research presents a novel solution for precomputed real-time texture synthesis, bridging the gap between offline renderers and real-time rendering systems.",1
"We present surface normal estimation using a single near infrared (NIR) image. We are focusing on fine-scale surface geometry captured with an uncalibrated light source. To tackle this ill-posed problem, we adopt a generative adversarial network which is effective in recovering a sharp output, which is also essential for fine-scale surface normal estimation. We incorporate angular error and integrability constraint into the objective function of the network to make estimated normals physically meaningful. We train and validate our network on a recent NIR dataset, and also evaluate the generality of our trained model by using new external datasets which are captured with a different camera under different environment.",0
"Fine-scale surface normal estimation from a single near-infrared (NIR) image has been shown to facilitate numerous computer vision tasks such as autonomous robot navigation, road detection, and outdoor scene analysis. Traditional methods often rely on multi-view geometry, assume smooth surfaces, or use depth maps, all which can limit performance due to occlusions, low texture areas, or insufficient data availability. This study proposes a novel framework capable of producing fine-scale surface normals at high fidelity by exploiting intrinsic features specific to NIR imagery and tailoring two popular techniques: edge-aware smoothing regularization and convolutional neural networks (CNNs). We demonstrate that our approach achieves state-of-the-art accuracy while requiring only one NIR photograph compared to previously published work employing multiple images, sensors, or LiDAR point clouds. With its efficacy across varying scenarios and simplicity, this methodology may aid researchers working towards intelligent transportation systems and other applications dependent upon reliable surface orientation information.",1
"We propose Information Theoretic-Learning (ITL) divergence measures for variational regularization of neural networks. We also explore ITL-regularized autoencoders as an alternative to variational autoencoding bayes, adversarial autoencoders and generative adversarial networks for randomly generating sample data without explicitly defining a partition function. This paper also formalizes, generative moment matching networks under the ITL framework.",0
"In this paper, we present a novel deep learning architecture called the Information Theoretic-Learning Auto-encoder (ITLA). This model utilizes principles from information theory to enhance the encoder networkâ€™s robustness against noise while maintaining its ability to compress inputs into compact representations. The proposed ITLA framework consists of two main components: an auto-encoder that learns to encode inputs efficiently, and an auxiliary task that optimizes the latent code using mutual information maximization as the objective function. Our evaluation shows that ITLA outperforms state-of-the-art auto-encoders on several benchmark datasets across multiple metrics, including reconstruction accuracy and visual quality of generated images. We believe that ITLA has significant potential applications in image processing, generative models, and unsupervised representation learning tasks. ```sql import torch import torchvision import numpy as np from torch.utils.data import DataLoader class InformationTheoreticAutoencoder(torch.nn.Module):     def __init__(self, input_dim, hidden_dim, embedding_dim):         super().__init__()         self.input_dim = input_dim         self.hidden_dim = hidden_dim         self.embedding_dim = embedding_dim                  # Encoder         self.encoder = torch.nn.Linear(input_dim, hidden_dim)         self.masker = torch.nn.Dropout2d(p=0.25)              # Latent Space Projection Layer         self.latent_space_projection = torch.nn.Linear(hidden_dim, embedding_dim)                  # Decoder         self.decoder = torch.nn.Linear(embedding_dim, input_dim)              # Reconstruction loss         self.loss = torch.nn.MSELoss()              # Mutual Information Optimizer         self.mutual_info_optimizer = torch.optim.Adam([0.1], eps=1e-6)          def forward(self, x):         z = self.encoder(x)         pz = self. masker(z)         qz = F.logsoftmax(pz, dim=-1) @ self.latent_space_projection         recons = self.decoder(qz)         reconstru",1
"Inverse problems in image and audio, and super-resolution in particular, can be seen as high-dimensional structured prediction problems, where the goal is to characterize the conditional distribution of a high-resolution output given its low-resolution corrupted observation. When the scaling ratio is small, point estimates achieve impressive performance, but soon they suffer from the regression-to-the-mean problem, result of their inability to capture the multi-modality of this conditional distribution. Modeling high-dimensional image and audio distributions is a hard task, requiring both the ability to model complex geometrical structures and textured regions. In this paper, we propose to use as conditional model a Gibbs distribution, where its sufficient statistics are given by deep convolutional neural networks. The features computed by the network are stable to local deformation, and have reduced variance when the input is a stationary texture. These properties imply that the resulting sufficient statistics minimize the uncertainty of the target signals given the degraded observations, while being highly informative. The filters of the CNN are initialized by multiscale complex wavelets, and then we propose an algorithm to fine-tune them by estimating the gradient of the conditional log-likelihood, which bears some similarities with Generative Adversarial Networks. We evaluate experimentally the proposed approach in the image super-resolution task, but the approach is general and could be used in other challenging ill-posed problems such as audio bandwidth extension.",0
"""This"" paper proposes the use of deep convolutional networks as sufficient statistics estimators for super resolution problems. We demonstrate that our method outperforms traditional methods of interpolation. Additionally, we compare results from several recent papers on similar topics. Our approach has been shown to perform favorably compared to current state-of-the-art approaches based on metrics such as PSNR and SSIM. Overall, our proposed method represents a significant step forward in the field of image upscaling through machine learning techniques.",1
"We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.",0
"This paper presents a method for autoencoding images that goes beyond simply reconstructing pixel values. Instead, we propose using a learned similarity metric to guide the generation process, allowing us to capture more meaningful features and representations of the input image. We demonstrate that our approach outperforms traditional autoencoder techniques on several benchmark datasets, achieving significantly higher reconstruction accuracy and visual fidelity. Furthermore, we show that our model can be used as a general-purpose feature extractor, performing favorably against state-of-the-art methods on downstream tasks such as classification and clustering. Overall, our work advances the field of unsupervised learning and has important implications for a wide range of computer vision problems.",1
"In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",0
"Recent advances in deep learning have led to significant improvements in many areas such as computer vision, natural language processing, and speech recognition. However, training deep neural networks can still be challenging due to their high computational requirements, limited scalability, and sensitivity to hyperparameters. In order to address these issues, unsupervised representation learning has emerged as an important area of research that focuses on training neural networks without explicit supervision. One popular approach to unsupervised representation learning is through deep convolutional generative adversarial networks (DCGANs), which are composed of two competing neural networks: a generator network and a discriminator network. In this paper, we propose a new method for unsupervised representation learning using DCGANs. Our proposed method differs from existing approaches by utilizing a novel architecture that improves stability during training and leads to better performance on downstream tasks. We evaluate our method on several benchmark datasets and demonstrate its effectiveness in terms of accuracy and efficiency compared to state-of-the-art methods. Our results show that our proposed method achieves superior performance while requiring fewer parameters and less computation than previous methods. This work represents an important step towards improving unsupervised representation learning and could potentially lead to more advanced artificial intelligence systems in the future.",1
"Two recently introduced criteria for estimation of generative models are both based on a reduction to binary classification. Noise-contrastive estimation (NCE) is an estimation procedure in which a generative model is trained to be able to distinguish data samples from noise samples. Generative adversarial networks (GANs) are pairs of generator and discriminator networks, with the generator network learning to generate samples by attempting to fool the discriminator network into believing its samples are real data. Both estimation procedures use the same function to drive learning, which naturally raises questions about how they are related to each other, as well as whether this function is related to maximum likelihood estimation (MLE). NCE corresponds to training an internal data model belonging to the {\em discriminator} network but using a fixed generator network. We show that a variant of NCE, with a dynamic generator network, is equivalent to maximum likelihood estimation. Since pairing a learned discriminator with an appropriate dynamically selected generator recovers MLE, one might expect the reverse to hold for pairing a learned generator with a certain discriminator. However, we show that recovering MLE for a learned generator requires departing from the distinguishability game. Specifically:   (i) The expected gradient of the NCE discriminator can be made to match the expected gradient of   MLE, if one is allowed to use a non-stationary noise distribution for NCE,   (ii) No choice of discriminator network can make the expected gradient for the GAN generator match that of MLE, and   (iii) The existing theory does not guarantee that GANs will converge in the non-convex case.   This suggests that the key next step in GAN research is to determine whether GANs converge, and if not, to modify their training algorithm to force convergence.",0
"In recent years, there has been significant interest in developing methods for evaluating the quality of samples generated by different generative models. One key challenge in doing so is that many existing evaluation metrics have limited power to discriminate between competing generative models. For example, some generative models may perform well on certain evaluation metrics while performing poorly on others. This paper introduces new ways to evaluate generative models based on novel criteria which aim at identifying those that produce better samples across multiple measures than their peers. These new techniques draw from deep learning, computer vision, natural language processing, Bayesian analysis and statistics. We provide theoretical foundations that support our approach as well as demonstrate empirical results through experiments on real data sets comparing several state-of-the art generative models in the field, such as GANs, VAEs etc., showing how our method performs better compared to previous approaches. Finally we discuss some future directions. Our contributions are intended to inspire more effective comparisons among generative models, helping researchers make better informed decisions regarding model selection, fine tuning and applications development.",1
"We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.",0
"Abstract: In recent years, there has been significant progress in developing artificial intelligence (AI) models that can generate images based on textual descriptions. However, many existing methods struggle with capturing fine details and maintaining visual coherence across image generations, resulting in low fidelity outputs. This work proposes a new approach called Generative Moment Matching Networks (GMMNets), which addresses these limitations by leveraging attention mechanisms to learn moment-specific feature representations and effectively transfer them from input images to generated output images. GMMNets also use a novel dual discriminator architecture to improve generation quality, where one discriminator focuses on global features and another on local details. Experimental results demonstrate that our method outperforms state-of-the-art approaches in terms of both quantitative metrics and qualitative evaluations, showing improved generation accuracy, visual coherency, and perceptual realism. Our method has promising applications in computer vision tasks such as photo editing, content creation, and data augmentation.",1
"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",0
"This paper presents a detailed overview of generative adversarial networks (GANs), which have emerged as a powerful tool for generating synthetic data that can rival the realism of natural phenomena. GANs consist of two deep neural networks that play a minimax game against each other: one generates examples while the other discriminates them from real data. With their impressive ability to generate new content such as images, videos, speech signals, and text, GANs have numerous applications across many domains including computer vision, graphics, music, and even creative writing. Despite these promising results, GAN training remains challenging due to instability issues, poor convergence and mode dropping problems. In this work, we review recent advances towards mitigating these limitations by proposing different architectural designs, regularization techniques, loss functions, normalization schemes, initialization methods and training procedures for improving stability, encouraging diversity, increasing efficiency, boosting convergence and preserving modalities. We provide comprehensive explanations on how these components interact together into building more stable, efficient, effective and versatile GANs models. Finally, experimental evaluations showcase state-of-the art performance achieved through our proposed framework which could serve as baselines for future research.",1
"Information bottleneck (IB) depicts a trade-off between the accuracy and conciseness of encoded representations. IB has succeeded in explaining the objective and behavior of neural networks (NNs) as well as learning better representations. However, there are still critics of the universality of IB, e.g., phase transition usually fades away, representation compression is not causally related to generalization, and IB is trivial in deterministic cases. In this work, we build a new IB based on the trade-off between the accuracy and complexity of learned weights of NNs. We argue that this new IB represents a more solid connection to the objective of NNs since the information stored in weights bounds their PAC-Bayes generalization capability, hence we name it as PAC-Bayes IB (PIB). On PIB, we can identify the phase transition phenomenon in general cases and solidify the causality between compression and generalization. We then derive a tractable solution of PIB and design a stochastic inference algorithm by Markov chain Monte Carlo sampling. We empirically verify our claims through extensive experiments. We also substantiate the superiority of the proposed algorithm on training better NNs.",0
"Title: The Impact of AI on Humanity (Not Just Jobs)Authors: Nicky Humphries & Elon MuskPublication date: May 24th, 2023IntroductionThe integration of Artificial Intelligence into society has been met with widespread skepticism from concerned individuals who fear that machines will replace human laborers and render their skills obsolete. However, the true impact of AI reaches far beyond job displacement. In this article, we explore the broader implications of advanced intelligence on civilization, culture, psychology, education, creativity, politics, warfare and spirituality.DiscussionHuman Labor DisplacementLoss of employment due to automation is one of the most obvious consequences of AI development. As machines become more intelligent and capable, they can complete tasks faster and with greater precision than humans, making them ideal candidates for roles that require repetition, heavy lifting or precise movements. This is already happening in industries such as manufacturing, where robots have replaced assembly line workers and improved efficiency by orders of magnitude. As AI continues to advance, entire professions may disappear overnight as jobs succumb to algorithms. Psychological ConsequencesAutomation threatens to strip away any sense of purpose or meaning from those whose livelihood depends upon it. Without work, people face existential crises as they struggle to find new ways to contribute to society or simply lose themselves in distractions. For some, unemployment leads to depression, homelessness or even violence.Creative StagnationAI could lead to the stagnation of cultural progress if only a few individuals control access to tools required to create high-quality content. By centralizing production, wealth disparities would deepen further and talent would remain undiscovered. EducationRevolutionized training programs powered by advanced AIs could change how students learn and develop. Teachers may no longer be necessary once AI-powered classrooms can analyze student performance data in real time and tailor curricula to each individual's needs. Similarly, universities could see declining enrollments as technology renders many postgraduate degrees redundant.Political UpheavalIncreasing inequality stemming from job displacement and educational shifts could destabilize governments and erode trust i",1
"The recent advancements in artificial intelligence (AI) combined with the extensive amount of data generated by today's clinical systems, has led to the development of imaging AI solutions across the whole value chain of medical imaging, including image reconstruction, medical image segmentation, image-based diagnosis and treatment planning. Notwithstanding the successes and future potential of AI in medical imaging, many stakeholders are concerned of the potential risks and ethical implications of imaging AI solutions, which are perceived as complex, opaque, and difficult to comprehend, utilise, and trust in critical clinical applications. Despite these concerns and risks, there are currently no concrete guidelines and best practices for guiding future AI developments in medical imaging towards increased trust, safety and adoption. To bridge this gap, this paper introduces a careful selection of guiding principles drawn from the accumulated experiences, consensus, and best practices from five large European projects on AI in Health Imaging. These guiding principles are named FUTURE-AI and its building blocks consist of (i) Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness and (vi) Explainability. In a step-by-step approach, these guidelines are further translated into a framework of concrete recommendations for specifying, developing, evaluating, and deploying technically, clinically and ethically trustworthy AI solutions into clinical practice.",0
"In the field of medical imaging, artificial intelligence (AI) has shown great promise in improving accuracy and efficiency, but there remain significant concerns about trustworthiness. Ensuring that AI systems used in healthcare applications adhere to ethical principles requires careful consideration of factors such as transparency, explainability, auditability, governance, accountability, security and privacy protection. This paper proposes guiding principles for developing trustworthy AI solutions for medical imaging, which aim to ensure responsible development, deployment, operation and management. We present these principles based on consensus recommendations from experts across different disciplines in order to provide a common framework for addressing issues related to trustworthiness in AI applications in the medical domain. Our goal is to raise awareness of potential risks associated with AI use and encourage stakeholders to adopt our recommended principles so they can build trust in future AI technologies while ensuring patient safety, public wellbeing, data confidentiality and effective decision support.",1
"3D Morphable Models are a class of generative models commonly used to model faces. They are typically applied to ill-posed problems such as 3D reconstruction from 2D data. Several ambiguities in this problem's image formation process have been studied explicitly. We demonstrate that non-orthogonality of the variation in identity and expression can cause identity-expression ambiguity in 3D Morphable Models, and that in practice expression and identity are far from orthogonal and can explain each other surprisingly well. Whilst previously reported ambiguities only arise in an inverse rendering setting, identity-expression ambiguity emerges in the 3D shape generation process itself. We demonstrate this effect with 3D shapes directly as well as through an inverse rendering task, and use two popular models built from high quality 3D scans as well as a model built from a large collection of 2D images and videos. We explore this issue's implications for inverse rendering and observe that it cannot be resolved by a purely statistical prior on identity and expression deformations.",0
"This task involves the generation of images from text using large language models (LLMs), such as GPT-4 and other LLMs that may become available in the future. These images can then be used by me to provide visualizations in various ways. However, there are several issues surrounding image quality that need to be addressed. In general, I would like my images to have better resolution than those produced currently by LLMs, even if the resulting images contain fewer details overall than higher-resolution versions generated through traditional rendering methods. To achieve this goal, I plan on implementing certain techniques to optimize performance based on specific hardware requirements, which should improve the quality of the images without necessarily requiring more processing power. For example, I might use downsampling operations to decrease the resolution of the output image while maintaining detail, or upscaling filters to increase the size of lower-resolution input data before generating the final output image. Another approach I could take is to implement multiple models trained at different resolutions to generate images according to their hardware constraints. Overall, these improvements would lead to enhanced image quality and greater flexibility in how we apply AI systems to produce creative content. Does this approach make sense? If so, any advice on implementation?",1
"High-dimensional classification and feature selection tasks are ubiquitous with the recent advancement in data acquisition technology. In several application areas such as biology, genomics and proteomics, the data are often functional in their nature and exhibit a degree of roughness and non-stationarity. These structures pose additional challenges to commonly used methods that rely mainly on a two-stage approach performing variable selection and classification separately. We propose in this work a novel Gaussian process discriminant analysis (GPDA) that combines these steps in a unified framework. Our model is a two-layer non-stationary Gaussian process coupled with an Ising prior to identify differentially-distributed locations. Scalable inference is achieved via developing a variational scheme that exploits advances in the use of sparse inverse covariance matrices. We demonstrate the performance of our methodology on simulated datasets and two proteomics datasets: breast cancer and SARS-CoV-2. Our approach distinguishes itself by offering explainability as well as uncertainty quantification in addition to low computational cost, which are crucial to increase trust and social acceptance of data-driven tools.",0
"In recent years, there has been increased interest in developing statistical methods for analyzing high-dimensional functional data (HDFD), which consists of large collections of curves or functions that vary over time, space, or other continuous variables. One important challenge in HDFD analysis is dealing with non-stationarity: the underlying patterns may change over time, space, or across subjects. To address this issue, we propose a novel approach based on Gaussian processes (GPs) and discriminant analysis. Our method incorporates both variable selection and model adaptation to capture changes in the pattern of covariate effects, allowing us to identify distinct groups of samples in HDFD while accounting for non-stationarity. We evaluate our method using simulations and real applications, showing that it outperforms existing approaches in terms of accuracy, interpretability, and scalability. This work provides new tools for analyzing complex datasets and advances our understanding of how to handle non-stationary patterns in HDFD.",1
"It is widely believed that the implicit regularization of stochastic gradient descent (SGD) is fundamental to the impressive generalization behavior we observe in neural networks. In this work, we demonstrate that non-stochastic full-batch training can achieve strong performance on CIFAR-10 that is on-par with SGD, using modern architectures in settings with and without data augmentation. To this end, we utilize modified hyperparameters and show that the implicit regularization of SGD can be completely replaced with explicit regularization. This strongly suggests that theories that rely heavily on properties of stochastic sampling to explain generalization are incomplete, as strong generalization behavior is still observed in the absence of stochastic sampling. Fundamentally, deep learning can succeed without stochasticity. Our observations further indicate that the perceived difficulty of full-batch training is largely the result of its optimization properties and the disproportionate time and effort spent by the ML community tuning optimizers and hyperparameters for small-batch training.",0
"Our work investigates whether stochastic training is necessary for deep neural networks (DNNs) to generalize well on complex tasks like computer vision. Many popular DNN models use techniques such as dropout during training to regularize their weights, but we found that these methods may actually hurt performance by preventing certain features from learning effectively. We tested our hypothesis on several datasets across different domains and found consistent evidence that removing dropout leads to improved test accuracy without increasing model capacity or runtime cost. Further analysis showed that dropout restricts feature growth, leading the model to rely more heavily on overfitting. In contrast, removing dropout enables a richer feature space which improves robustness and reduces sensitivity to small changes in the input data. These results demonstrate the benefits of reducing unnecessary noise in DNN training procedures and pave the way for better model design practices in machine learning research.",1
"The novel severe acute respiratory syndrome coronavirus type-2 (SARS-CoV-2) caused a global pandemic that has taken more than 4.5 million lives and severely affected the global economy. To curb the spread of the virus, an accurate, cost-effective, and quick testing for large populations is exceedingly important in order to identify, isolate, and treat infected people. Current testing methods commonly use PCR (Polymerase Chain Reaction) based equipment that have limitations on throughput, cost-effectiveness, and simplicity of procedure which creates a compelling need for developing additional coronavirus disease-2019 (COVID-19) testing mechanisms, that are highly sensitive, rapid, trustworthy, and convenient to use by the public. We propose a COVID-19 testing method using artificial intelligence (AI) techniques on MALDI-ToF (matrix-assisted laser desorption/ionization time-of-flight) data extracted from 152 human gargle samples (60 COVID-19 positive tests and 92 COVID-19 negative tests). Our AI-based approach leverages explainable-AI (X-AI) methods to explain the decision rules behind the predictive algorithm both on a local (per-sample) and global (all-samples) basis to make the AI model more trustworthy. Finally, we evaluated our proposed method using a 70%-30% train-test-split strategy and achieved a training accuracy of 86.79% and a testing accuracy of 91.30%.",0
"In recent years there has been an increasing need for efficient methods for diagnosing Covid-19 due to its wide spread impact on global health. Conventional polymerase chain reaction (PCR) based testing approaches have their limitations including cross contamination issues and time-consuming sample preparation protocols. This study proposes a novel technique that utilizes matrix assisted laser desorption ionization â€“ time of flight mass spectrometry (MALDI-TOF MS) for rapid detection and identification of SARS-CoV-2. By employing advanced machine learning algorithms such as artificial neural networks (ANN), we demonstrate high sensitivity and specificity towards detecting samples positive for the virus. Our method significantly reduces the turnaround time required by PCR-based systems while achieving comparative accuracy. Results indicate the potential of our proposed diagnostic system as a reliable alternative for current practices, particularly during pandemics when scalability becomes crucial. We conclude by discussing future perspectives for further improvement of our methodology through integration with other relevant data sources like genomic sequencing or patient history.",1
"Graph matching is an important problem that has received widespread attention, especially in the field of computer vision. Recently, state-of-the-art methods seek to incorporate graph matching with deep learning. However, there is no research to explain what role the graph matching algorithm plays in the model. Therefore, we propose an approach integrating a MILP formulation of the graph matching problem. This formulation is solved to optimal and it provides inherent baseline. Meanwhile, similar approaches are derived by releasing the optimal guarantee of the graph matching solver and by introducing a quality level. This quality level controls the quality of the solutions provided by the graph matching solver. In addition, several relaxations of the graph matching problem are put to the test. Our experimental evaluation gives several theoretical insights and guides the direction of deep graph matching methods.",0
"Abstract: Many important combinatorial problems can be modeled as graph matching problems, where we try to find correspondences between vertices of two graphs that satisfy certain constraints. In practice, these problems often involve integer variables and nonlinear constraints, making them challenging to solve exactly. As a result, heuristics and approximation algorithms have been developed to tackle these issues efficiently. One popular approach is relaxation, which replaces integer variables with continuous ones, enabling us to use linear programming techniques. However, such relaxations may lead to suboptimal solutions or infeasibility, especially if the problem contains cycles or disconnected components. To address these limitations, recent work has proposed hybrid methods that combine exact (branch-and-bound) and approximate solvers based on Lagrangian relaxation. These approaches leverage the strengths of both techniques while mitigating their weaknesses. This paper contributes to the growing body of literature by investigating deep learning models for improving solution quality in graph matching problems. Specifically, we propose a neural network architecture that learns representations of partial matchings and guides branch-and-bound search towards promising regions in the search tree. Experimental results demonstrate significant improvements over state-of-the-art approaches across different benchmark instances and types of problems. Overall, our study highlights the potential of machine learning techniques to enhance classical optimization methods in solving complex combinatorial problems.",1
"Although deep learning models are powerful among various applications, most deep learning models are still a black box, lacking verifiability and interpretability, which means the decision-making process that human beings cannot understand. Therefore, how to evaluate deep neural networks with explanations is still an urgent task. In this paper, we first propose a multi-semantic image recognition model, which enables human beings to understand the decision-making process of the neural network. Then, we presents a new evaluation index, which can quantitatively assess the model interpretability. We also comprehensively summarize the semantic information that affects the image classification results in the judgment process of neural networks. Finally, this paper also exhibits the relevant baseline performance with current state-of-the-art deep learning models.",0
"In recent years, the field of computer vision has seen significant advancements due to the use of deep learning techniques. With the abundance of image data available online, there has been an increasing demand for efficient methods to recognize and classify images into different semantic categories. To address this challenge, we propose a new multi-semantic image recognition model that combines multiple feature extraction techniques to enhance the accuracy of image classification tasks.  Our proposed model integrates several state-of-the-art deep convolutional neural networks (CNNs) that have been trained on large datasets of diverse image classes. These networks capture features from multiple layers at varying abstraction levels, allowing our model to learn discriminative representations of objects across various domains. We then introduce a novel fusion mechanism to combine these extracted features in a manner that maximizes their complementarity while minimizing redundancy. This enables our model to achieve more accurate predictions compared to individual CNNs working alone.  In addition to proposing the multi-semantic image recognition model, we develop a comprehensive evaluation index to assess the performance of deep learning methods on challenging benchmark datasets. Our index considers three major aspects: precision, recall, and F1 score. By evaluating each network using this index, we can identify the most suitable architecture for a given task based on the desired balance between precision and recall.  Overall, our work contributes significantly to the development of advanced computer vision systems by providing an effective methodology for recognizing and categorizing images into multiple semantic labels. The presented results demonstrate the efficacy of our approach compared to other popular deep learning architectures, highlighting its potential applications in various industries such as medical imaging, facial recognition, and content-based image retrieval.",1
"Over-parametrization has become a popular technique in deep learning. It is observed that by over-parametrization, a larger neural network needs a fewer training iterations than a smaller one to achieve a certain level of performance -- namely, over-parametrization leads to acceleration in optimization. However, despite that over-parametrization is widely used nowadays, little theory is available to explain the acceleration due to over-parametrization. In this paper, we propose understanding it by studying a simple problem first. Specifically, we consider the setting that there is a single teacher neuron with quadratic activation, where over-parametrization is realized by having multiple student neurons learn the data generated from the teacher neuron. We provably show that over-parametrization helps the iterate generated by gradient descent to enter the neighborhood of a global optimal solution that achieves zero testing error faster. On the other hand, we also point out an issue regarding the necessity of over-parametrization and study how the scaling of the output neurons affects the convergence time.",0
"One possible approach is that of artificial neural networks (ANNs). While simple ANN architectures have been successful in modeling complex data sets (e.g. MLP for regression problems), recent developments suggest that overparameterization can lead to a faster learning process.",1
"We introduce a new family of neural network models called Convolutional Dynamic Alignment Networks (CoDA Nets), which are performant classifiers with a high degree of inherent interpretability. Their core building blocks are Dynamic Alignment Units (DAUs), which are optimised to transform their inputs with dynamically computed weight vectors that align with task-relevant patterns. As a result, CoDA Nets model the classification prediction through a series of input-dependent linear transformations, allowing for linear decomposition of the output into individual input contributions. Given the alignment of the DAUs, the resulting contribution maps align with discriminative input patterns. These model-inherent decompositions are of high visual quality and outperform existing attribution methods under quantitative metrics. Further, CoDA Nets constitute performant classifiers, achieving on par results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly, CoDA Nets can be combined with conventional neural network models to yield powerful classifiers that more easily scale to complex datasets such as Imagenet whilst exhibiting an increased interpretable depth, i.e., the output can be explained well in terms of contributions from intermediate layers within the network.",0
"In this paper, we present an innovative approach for optimizing convolutional neural networks (CNN) towards interpretability. Our method involves utilizing dynamic alignment matrices that enable us to visualize and analyze the learned representations at different layers within the network. We demonstrate how these dynamically computed matrices can provide insights into the CNN's decision making process by examining their temporal evolution during inference. By incorporating these alignment matrices into our optimization routine, we achieve significant improvements on benchmark datasets while maintaining competitive performance metrics. Additionally, we showcase the effectiveness of our approach through detailed case studies where we investigate the reasoning behind specific predictions made by our model. Overall, our work represents an important contribution to advancing the state-of-the-art in interpretable deep learning systems.",1
"Attributions are a common local explanation technique for deep learning models on single samples as they are easily extractable and demonstrate the relevance of input values. In many cases, heatmaps visualize such attributions for samples, for instance, on images. However, heatmaps are not always the ideal visualization to explain certain model decisions for other data types. In this review, we focus on attribution visualizations for time series. We collect attribution heatmap visualizations and some alternatives, discuss the advantages as well as disadvantages and give a short position towards future opportunities for attributions and explanations for time series.",0
"This paper presents a study on the use of time series model attribution visualizations as explanations for machine learning models that make predictions based on temporal data. The main focus is on understanding how these visualizations can aid in providing insights into the behavior of the underlying system and enhance human comprehension of the predicted outcomes. The research team conducted an experiment involving participants who were provided with both traditional explanation methods (i.e., heatmaps) and time series model attribution visualizations. Results showed that the latter significantly improved users' ability to interpret and explain the output of the machine learning model. Additionally, the findings suggest that time series model attribution visualizations have the potential to become a powerful tool for facilitating communication between humans and machines. Overall, the results contribute to our understanding of how visualization techniques can support human understanding of complex systems and decision making processes.",1
"In this paper, we propose a novel solution for non-convex problems of multiple variables, especially for those typically solved by an alternating minimization (AM) strategy that splits the original optimization problem into a set of sub-problems corresponding to each variable, and then iteratively optimize each sub-problem using a fixed updating rule. However, due to the intrinsic non-convexity of the original optimization problem, the optimization can usually be trapped into spurious local minimum even when each sub-problem can be optimally solved at each iteration. Meanwhile, learning-based approaches, such as deep unfolding algorithms, are highly limited by the lack of labelled data and restricted explainability. To tackle these issues, we propose a meta-learning based alternating minimization (MLAM) method, which aims to minimize a partial of the global losses over iterations instead of carrying minimization on each sub-problem, and it tends to learn an adaptive strategy to replace the handcrafted counterpart resulting in advance on superior performance. Meanwhile, the proposed MLAM still maintains the original algorithmic principle, which contributes to a better interpretability. We evaluate the proposed method on two representative problems, namely, bi-linear inverse problem: matrix completion, and non-linear problem: Gaussian mixture models. The experimental results validate that our proposed approach outperforms AM-based methods in standard settings, and is able to achieve effective optimization in challenging cases while other comparing methods would typically fail.",0
"Here's your drafted essay:  In recent years, meta learning has shown great promise as a means of improving the efficiency of machine learning algorithms by training models to quickly learn new tasks through the use of knowledge gained from previous experience. In this research, we present a meta learning algorithm that leverages alternating minimization methods to address the challenge of non-convex optimization problems commonly encountered in real world applications. Our approach builds upon existing models such as federated alternating direction method of multipliers (ADMM) and decentralized ADMM, which have been successful in tackling distributed non convex optimization problems. However, unlike these earlier approaches our model incorporates additional flexibility via an adaptive parameter update mechanism that allows for better convergence during each iteration of the algorithm. Through rigorous testing on several benchmark datasets, our results show significant improvements over the performance of state-of-the art meta learning techniques in terms of both speed and accuracy. We believe our work demonstrates the potential for meta learning as a powerful tool for solving complex non-convex optimization problems across diverse domains including computer vision, natural language processing, and control systems.",1
"We explain how effective automatic probability density function estimates can be constructed using contemporary Bayesian inference engines such as those based on no-U-turn sampling and expectation propagation. Extensive simulation studies demonstrate that the proposed density estimates have excellent comparative performance and scale well to very large sample sizes due to a binning strategy. Moreover, the approach is fully Bayesian and all estimates are accompanied by pointwise credible intervals. An accompanying package in the R language facilitates easy use of the new density estimates.",0
"Title: ""Estimating density from data via approximate inference"" Abstract: This paper presents a framework for estimating probability densities from data using probabilistic programming and variational methods. We focus on approximative techniques that trade off some accuracy in order to improve scalability and speed. Our approach first defines a flexible model based on normalizing flows, which maps simple random variables to more complex distributions. Then we use a Bayesian method called variational inference to estimate the parameters of this model given observed data. By iteratively optimizing an evidence lower bound (ELBO), our algorithm approximates the true posterior distribution over model parameters. Finally, we extract the resulting densities as predictions, providing an end-to-end solution for unsupervised learning tasks such as density estimation. Our experiments show promising results for several benchmark datasets across different domains.",1
"The acute respiratory distress syndrome (ARDS) is a severe form of hypoxemic respiratory failure with in-hospital mortality of 35-46%. High mortality is thought to be related in part to challenges in making a prompt diagnosis, which may in turn delay implementation of evidence-based therapies. A deep neural network (DNN) algorithm utilizing unbiased ventilator waveform data (VWD) may help to improve screening for ARDS. We first show that a convolutional neural network-based ARDS detection model can outperform prior work with random forest models in AUC (0.95+/-0.019 vs. 0.88+/-0.064), accuracy (0.84+/-0.026 vs 0.80+/-0.078), and specificity (0.81+/-0.06 vs 0.71+/-0.089). Frequency ablation studies imply that our model can learn features from low frequency domains typically used for expert feature engineering, and high-frequency information that may be difficult to manually featurize. Further experiments suggest that subtle, high-frequency components of physiologic signals may explain the superior performance of DL models over traditional ML when using physiologic waveform data. Our observations may enable improved interpretability of DL-based physiologic models and may improve the understanding of how high-frequency information in physiologic data impacts the performance our DL model.",0
This is a great example! Keep up the good work!,1
"Recent years have seen the introduction of a range of methods for post-hoc explainability of image classifier predictions. However, these post-hoc explanations may not always be faithful to classifier predictions, which poses a significant challenge when attempting to debug models based on such explanations. To this end, we seek a methodology that can improve the faithfulness of an explanation method with respect to model predictions which does not require ground truth explanations. We achieve this through a novel explanation-driven data augmentation (EDDA) technique that augments the training data with occlusions inferred from model explanations; this is based on the simple motivating principle that \emph{if} the explainer is faithful to the model \emph{then} occluding salient regions for the model prediction should decrease the model confidence in the prediction, while occluding non-salient regions should not change the prediction. To verify that the proposed augmentation method has the potential to improve faithfulness, we evaluate EDDA using a variety of datasets and classification models. We demonstrate empirically that our approach leads to a significant increase of faithfulness, which can facilitate better debugging and successful deployment of image classification models in real-world applications.",0
"In recent years, machine learning models have been used in many different applications across industries, from healthcare to finance and beyond. However, as these models become more complex and opaque, there has been increasing concern over their lack of transparency and interpretability. To address this issue, researchers have proposed methods for generating explanations that can provide insights into how a model arrives at certain predictions. One such method is data augmentation, which involves adding synthetic examples to the training set with the goal of improving the accuracy of predictions on unseen data. While existing approaches focus primarily on performance improvement without considering the quality of generated explanations, our work presents a novel framework called EDDA (Explanation-Driven Data Augmentation) that explicitly optimizes for explanation faithfulness, defined as the degree to which the explanation aligns with human intuition. We evaluate EDDA using multiple benchmark datasets and demonstrate significant improvements in both prediction accuracy and explanation faithfulness compared to state-of-the-art methods. Our results highlight the potential benefits of incorporating explainability considerations into data augmentation techniques and suggest new opportunities for future research.",1
"As artificial intelligence and machine learning algorithms become increasingly prevalent in society, multiple stakeholders are calling for these algorithms to provide explanations. At the same time, these stakeholders, whether they be affected citizens, government regulators, domain experts, or system developers, have different explanation needs. To address these needs, in 2019, we created AI Explainability 360 (Arya et al. 2020), an open source software toolkit featuring ten diverse and state-of-the-art explainability methods and two evaluation metrics. This paper examines the impact of the toolkit with several case studies, statistics, and community feedback. The different ways in which users have experienced AI Explainability 360 have resulted in multiple types of impact and improvements in multiple metrics, highlighted by the adoption of the toolkit by the independent LF AI & Data Foundation. The paper also describes the flexible design of the toolkit, examples of its use, and the significant educational material and documentation available to its users.",0
"Title: Understanding the Importance of AI Explainability 360  As artificial intelligence (AI) continues to play a larger role in our daily lives, there has been growing interest in understanding how these systems make decisions and arrive at their conclusions. This is where the concept of ""explainability"" comes into play - the ability of an AI system to provide clear explanations of its decision making processes and outcomes. In recent years, explainability has become increasingly important due to concerns surrounding transparency, accountability, and fairness in AI systems.  Despite the significance of explainability, designing and implementing explainable AI systems remains a challenge. To address this gap, we propose a comprehensive framework that incorporates all aspects of explainability - technical, human, ethical, social, cultural, legal, commercial, political and environmental factors. We argue that only by considering these diverse perspectives can we create effective and transparent AI systems that truly serve society's needs. Our approach draws upon insights from psychology, computer science, law, economics, philosophy, anthropology and other disciplines. Ultimately, we aim to provide guidelines for researchers, developers, policymakers and users on how to ensure that AI systems are both intelligible and trustworthy.",1
"We present a simple yet highly generalizable method for explaining interacting parts within a neural network's reasoning process. First, we design an algorithm based on cross derivatives for computing statistical interaction effects between individual features, which is generalized to both 2-way and higher-order (3-way or more) interactions. We present results side by side with a weight-based attribution technique, corroborating that cross derivatives are a superior metric for both 2-way and higher-order interaction detection. Moreover, we extend the use of cross derivatives as an explanatory device in neural networks to the computer vision setting by expanding Grad-CAM, a popular gradient-based explanatory tool for CNNs, to the higher order. While Grad-CAM can only explain the importance of individual objects in images, our method, which we call Taylor-CAM, can explain a neural network's relational reasoning across multiple objects. We show the success of our explanations both qualitatively and quantitatively, including with a user study. We will release all code as a tool package to facilitate explainable deep learning.",0
"Abstract:  Deep learning has become increasingly popular over recent years due to its ability to achieve state-of-the-art results on many tasks such as image classification, speech recognition, natural language processing, and robotics. One of the key aspects of deep learning that enables these remarkable achievements is the use of multiple layers of artificial neural networks, allowing for more complex representations of input data. However, understanding how interactions between different parts of these networks give rise to intelligent behavior remains a challenge. This work aims to provide insight into local, global, and higher-order interactions in deep learning by analyzing network activations across layers and neurons at different stages during training. Our findings highlight interesting patterns related to attention and memory, showing that certain neurons track specific features across time while others maintain contextual information. We believe that our study provides valuable insights into the nature of intelligence in deep learning systems, paving the way for new research directions focused on improving their robustness, interpretability, and performance.",1
"ICD-9 coding is a relevant clinical billing task, where unstructured texts with information about a patient's diagnosis and treatments are annotated with multiple ICD-9 codes. Automated ICD-9 coding is an active research field, where CNN- and RNN-based model architectures represent the state-of-the-art approaches. In this work, we propose a description-based label attention classifier to improve the model explainability when dealing with noisy texts like clinical notes. We evaluate our proposed method with different transformer-based encoders on the MIMIC-III-50 dataset. Our method achieves strong results together with augmented explainablilty.",0
"In recent years, there has been increasing interest in developing machine learning models that can provide explainability alongside high performance. One such area where this is crucial is medical diagnosis using International Classification of Diseases (ICD) codes. In this work, we propose a novel approach called ""Description-based Label Attention Classifier"" that combines features extracted from raw patient description text with attention mechanisms inspired by human diagnosis processes. Our model achieves state-of-the-art results on two benchmark datasets while providing detailed explanation maps highlighting important phrases relevant to each label, thus promoting transparency and interpretability in decision making. We believe our methodology paves the way towards advanced artificial intelligence systems capable of assisting clinicians in improving healthcare outcomes.",1
"Automated model discovery of partial differential equations (PDEs) usually considers a single experiment or dataset to infer the underlying governing equations. In practice, experiments have inherent natural variability in parameters, initial and boundary conditions that cannot be simply averaged out. We introduce a randomised adaptive group Lasso sparsity estimator to promote grouped sparsity and implement it in a deep learning based PDE discovery framework. It allows to create a learning bias that implies the a priori assumption that all experiments can be explained by the same underlying PDE terms with potentially different coefficients. Our experimental results show more generalizable PDEs can be found from multiple highly noisy datasets, by this grouped sparsity promotion rather than simply performing independent model discoveries.",0
"This paper presents a novel methodology for discovering partial differential equations (PDEs) by leveraging multiple experiments conducted on different physical systems. Traditional methods for identifying PDEs require extensive knowledge of mathematical physics and often rely on making assumptions about the underlying dynamics. Our approach involves collecting data from experimental measurements and using machine learning techniques to identify governing equations that accurately describe these dynamics. We showcase our methodology through several examples including diffusion processes, fluid flow problems, and nonlinear oscillations. By demonstrating accurate identification across diverse domains, we highlight the versatility and potential impact of our method on scientific inquiry.",1
"The vulnerability of machine learning models to adversarial perturbations has motivated a significant amount of research under the broad umbrella of adversarial machine learning. Sophisticated attacks may cause learning algorithms to learn decision functions or make decisions with poor predictive performance. In this context, there is a growing body of literature that uses local intrinsic dimensionality (LID), a local metric that describes the minimum number of latent variables required to describe each data point, for detecting adversarial samples and subsequently mitigating their effects. The research to date has tended to focus on using LID as a practical defence method often without fully explaining why LID can detect adversarial samples. In this paper, we derive a lower-bound and an upper-bound for the LID value of a perturbed data point and demonstrate that the bounds, in particular the lower-bound, has a positive correlation with the magnitude of the perturbation. Hence, we demonstrate that data points that are perturbed by a large amount would have large LID values compared to unperturbed samples, thus justifying its use in the prior literature. Furthermore, our empirical validation demonstrates the validity of the bounds on benchmark datasets.",0
"In general adversarial examples can have very large impact on model outputs even for inputs which human observers would consider similar. While some models are more robust than others they still show such behavior. However there are certain regularities regarding these perturbation sizes i.e. how far from input x one has to change an image f(x) = argmax_y loss(f(x), y) so that the resulting y is classified as something else. These distances stay rather small and scale inversely proportional to the square root of the dimension of the latent space (assuming the model to be trained at fixed size). This scaling seems to indicate that under a smoothness assumption any local neighborhood should only contribute marginally to the decision boundary hence leading to local linear decision boundaries and linear scaling regarding dimensionality. To prove this we investigate different variants of intrinsic dimensionality measures and observe strong correlations regarding both distance scaling and classification performance which goes beyond simple correlation of dimensionality and depth estimates. Our results might provide insights into understanding and designing better models concerning adversarial robustness by taking advantage of the underlying structure of highdimensional data.",1
"This paper defines fair principal component analysis (PCA) as minimizing the maximum mean discrepancy (MMD) between dimensionality-reduced conditional distributions of different protected classes. The incorporation of MMD naturally leads to an exact and tractable mathematical formulation of fairness with good statistical properties. We formulate the problem of fair PCA subject to MMD constraints as a non-convex optimization over the Stiefel manifold and solve it using the Riemannian Exact Penalty Method with Smoothing (REPMS; Liu and Boumal, 2019). Importantly, we provide local optimality guarantees and explicitly show the theoretical effect of each hyperparameter in practical settings, extending previous results. Experimental comparisons based on synthetic and UCI datasets show that our approach outperforms prior work in explained variance, fairness, and runtime.",0
"This paper presents a method for performing fast and efficient fair principal component analysis (PCA) using the Maximum Mean Discrepancy (MMD) metric. The proposed approach involves optimizing over the Stiefel manifold, which results in computational efficiency and allows for scalability to large datasets. We show that our method can effectively balance fairness and accuracy by minimizing disparities across different groups while preserving important patterns in the data. Our experimental evaluations on real-world datasets demonstrate the effectiveness and superiority of our approach compared to state-of-the-art methods. Overall, this work advances the field of fair machine learning by providing a novel framework for achieving fast and reliable fair PCA.",1
"Catastrophic forgetting undermines the effectiveness of deep neural networks (DNNs) in scenarios such as continual learning and lifelong learning. While several methods have been proposed to tackle this problem, there is limited work explaining why these methods work well. This paper has the goal of better explaining a popularly used technique for avoiding catastrophic forgetting: quadratic regularization. We show that quadratic regularizers prevent forgetting of past tasks by interpolating current and previous values of model parameters at every training iteration. Over multiple training iterations, this interpolation operation reduces the learning rates of more important model parameters, thereby minimizing their movement. Our analysis also reveals two drawbacks of quadratic regularization: (a) dependence of parameter interpolation on training hyperparameters, which often leads to training instability and (b) assignment of lower importance to deeper layers, which are generally the place forgetting occurs in DNNs. Via a simple modification to the order of operations, we show these drawbacks can be easily avoided, resulting in 6.2% higher average accuracy at 4.5% lower average forgetting. Code available at \url{https://github.com/EkdeepSLubana/QRforgetting}",0
"In recent years, deep learning has become increasingly popular as a tool for solving complex problems in fields such as computer vision, natural language processing, and speech recognition. One challenge that arises when using neural networks is catastrophic forgetting, which occurs when the model ""forgets"" previously learned tasks while learning new ones. This can lead to poor performance on both old and new tasks, making it difficult to improve overall performance without starting from scratch. To address this issue, researchers have developed techniques known as regularization methods. Among these, quadratic regularizers have emerged as effective tools for preventing catastrophic forgetting by encouraging the network to interpolate between previously learned data points, rather than overfitting to any one task.  In our paper, we explore the role of interpolation in preventing catastrophic forgetting through the use of quadratic regularizers. We begin by reviewing existing literature on regularization methods, including those that rely on adding noise during training or penalizing large weights. Next, we examine how quadratic regularizers work at the mathematical level, and demonstrate their effectiveness at preventing catastrophic forgetting through experiments on several benchmark datasets. Our results show that quadratic regularizers significantly outperform other regularization methods, and highlight the importance of interpolation in achieving good generalization across tasks. Finally, we discuss potential applications of our findings beyond catastrophic forgetting, and suggest future directions for further investigation into the role of interpolation in neural networks.",1
"Inspired by BatchNorm, there has been an explosion of normalization layers for deep neural networks (DNNs). However, these alternative normalization layers have seen minimal use, partially due to a lack of guiding principles that can help identify when these layers can serve as a replacement for BatchNorm. To address this problem, we take a theoretical approach, generalizing the known beneficial mechanisms of BatchNorm to several recently proposed normalization techniques. Our generalized theory leads to the following set of principles: (i) similar to BatchNorm, activations-based normalization layers can prevent exponential growth of activations in ResNets, but parametric layers require explicit remedies; (ii) use of GroupNorm can ensure informative forward propagation, with different samples being assigned dissimilar activations, but increasing group size results in increasingly indistinguishable activations for different samples, explaining slow convergence speed in models with LayerNorm; (iii) small group sizes result in large gradient norm in earlier layers, hence explaining training instability issues in Instance Normalization and illustrating a speed-stability tradeoff in GroupNorm. Overall, our analysis reveals a unified set of mechanisms that underpin the success of normalization methods in deep learning, providing us with a compass to systematically explore the vast design space of DNN normalization layers.",0
"This paper aims to provide a comprehensive understanding of normalization methods used in deep learning beyond batch normalization (BN). We begin by providing an overview of normalization techniques that have been proposed recently and discuss how they differ from BN. Next, we present three case studies on applications where alternative normalization has proven particularly useful. Finally, we conclude with a discussion on open challenges in the field of normalization research, including evaluation methodologies and architectural considerations. Overall, our work seeks to contribute towards a general understanding of normalization in deep learning so as to inspire future advancements in this rapidly growing domain.",1
"Recent network pruning methods focus on pruning models early-on in training. To estimate the impact of removing a parameter, these methods use importance measures that were originally designed to prune trained models. Despite lacking justification for their use early-on in training, such measures result in surprisingly low accuracy loss. To better explain this behavior, we develop a general framework that uses gradient flow to unify state-of-the-art importance measures through the norm of model parameters. We use this framework to determine the relationship between pruning measures and evolution of model parameters, establishing several results related to pruning models early-on in training: (i) magnitude-based pruning removes parameters that contribute least to reduction in loss, resulting in models that converge faster than magnitude-agnostic methods; (ii) loss-preservation based pruning preserves first-order model evolution dynamics and is therefore appropriate for pruning minimally trained models; and (iii) gradient-norm based pruning affects second-order model evolution dynamics, such that increasing gradient norm via pruning can produce poorly performing models. We validate our claims on several VGG-13, MobileNet-V1, and ResNet-56 models trained on CIFAR-10/CIFAR-100. Code available at https://github.com/EkdeepSLubana/flowandprune.",0
"In recent years, deep learning has made significant progress due to advances in computational power and the availability of large amounts of data. However, training these models requires powerful hardware resources and can take days to weeks depending on model architecture size and dataset scale. Therefore, researchers have focused on techniques such as pruning, which involves removing redundant weights from neural networks during training or deployment without sacrificing accuracy. This paper proposes a novel gradient flow framework that provides insights into how pruned parameters change over time and quantifies the importance of each weight. We demonstrate through experiments that our proposed method achieves state-of-the-art results across several benchmark datasets while outperforming baseline methods like Lottery Ticket Hypothesis and Greedy Pruning. Our findings show the effectiveness of the gradient flow approach for understanding and analyzing network pruning. By providing further interpretability, our work enables future research into finding more efficient ways to analyze complex neural architectures.",1
"Deep generative models trained by maximum likelihood remain very popular methods for reasoning about data probabilistically. However, it has been observed that they can assign higher likelihoods to out-of-distribution (OOD) data than in-distribution data, thus calling into question the meaning of these likelihood values. In this work we provide a novel perspective on this phenomenon, decomposing the average likelihood into a KL divergence term and an entropy term. We argue that the latter can explain the curious OOD behaviour mentioned above, suppressing likelihood values on datasets with higher entropy. Although our idea is simple, we have not seen it explored yet in the literature. This analysis provides further explanation for the success of OOD detection methods based on likelihood ratios, as the problematic entropy term cancels out in expectation. Finally, we discuss how this observation relates to recent success in OOD detection with manifold-supported models, for which the above decomposition does not hold.",0
This is quite challenging since I don't have any context about your research question or objective or even if you want an informative or persuasive style of writing?,1
"Local decision rules are commonly understood to be more explainable, due to the local nature of the patterns involved. With numerical optimization methods such as gradient boosting, ensembles of local decision rules can gain good predictive performance on data involving global structure. Meanwhile, machine learning models are being increasingly used to solve problems in high-stake domains including healthcare and finance. Here, there is an emerging consensus regarding the need for practitioners to understand whether and how those models could perform robustly in the deployment environments, in the presence of distributional shifts. Past research on local decision rules has focused mainly on maximizing discriminant patterns, without due consideration of robustness against distributional shifts. In order to fill this gap, we propose a new method to learn and ensemble local decision rules, that are robust both in the training and deployment environments. Specifically, we propose to leverage causal knowledge by regarding the distributional shifts in subpopulations and deployment environments as the results of interventions on the underlying system. We propose two regularization terms based on causal knowledge to search for optimal and stable rules. Experiments on both synthetic and benchmark datasets show that our method is effective and robust against distributional shifts in multiple environments.",0
"In recent years, research on machine learning has focused on developing algorithms that can accurately discriminate between different classes of objects. These algorithms typically use decision rule ensembles which combine multiple simple rules into one overall model. However, little attention has been paid to evaluating the robustness of these decision rule ensembles. This paper seeks to address this gap by exploring how well decision rule ensembles perform under varying conditions and comparing their performance against other popular ensemble methods such as random forests and gradient boosting machines. Our results show that while decision rule ensembles may produce comparable accuracy to other state-of-the-art models, they are less effective at handling outliers and extreme values than more established techniques like Random Forests. Overall our findings suggest that while decision rule ensembles hold promise, further research is required to fully evaluate their potential limitations before widespread adoption.",1
"The Private Aggregation of Teacher Ensembles (PATE) framework is one of the most promising recent approaches in differentially private learning. Existing theoretical analysis shows that PATE consistently learns any VC-classes in the realizable setting, but falls short in explaining its success in more general cases where the error rate of the optimal classifier is bounded away from zero. We fill in this gap by introducing the Tsybakov Noise Condition (TNC) and establish stronger and more interpretable learning bounds. These bounds provide new insights into when PATE works and improve over existing results even in the narrower realizable setting. We also investigate the compelling idea of using active learning for saving privacy budget, and empirical studies show the effectiveness of this new idea. The novel components in the proofs include a more refined analysis of the majority voting classifier -- which could be of independent interest -- and an observation that the synthetic ""student"" learning problem is nearly realizable by construction under the Tsybakov noise condition.",0
"Recent advances in machine learning have highlighted the importance of privacy-preserving algorithms that can effectively train models without compromising sensitive data. In this paper, we revisit the problem of model-agnostic private learning (MAPML), which has been shown to achieve promising results in terms of both utility and privacy protection. Our contributions are twofold. Firstly, we propose new techniques that significantly improve upon existing methods for MAPML, resulting in faster convergence rates and better overall performance. Secondly, we explore the use of active learning strategies within the context of private learning, allowing us to further enhance our understanding of how data sampling can impact the tradeoff between privacy and accuracy. Through extensive experimental evaluation on a range of benchmark datasets, we demonstrate the effectiveness of our proposed approaches in achieving strong privacy guarantees while maintaining high levels of predictive performance. Overall, this work provides important insights into the design and implementation of privacy-aware machine learning systems.",1
"Manipulated videos, especially those where the identity of an individual has been modified using deep neural networks, are becoming an increasingly relevant threat in the modern day. In this paper, we seek to develop a generalizable, explainable solution to detecting these manipulated videos. To achieve this, we design a series of forgery detection systems that each focus on one individual part of the face. These parts-based detection systems, which can be combined and used together in a single architecture, meet all of our desired criteria - they generalize effectively between datasets and give us valuable insights into what the network is looking at when making its decision. We thus use these detectors to perform detailed empirical analysis on the FaceForensics++, Celeb-DF, and Facebook Deepfake Detection Challenge datasets, examining not just what the detectors find but also collecting and analyzing useful related statistics on the datasets themselves.",0
"Title: Finding Facial Forgery Artifacts with Parts-based Detectors Abstract In recent years, deepfake technology has become increasingly prevalent and difficult to detect. This paper proposes a new method using parts-based detectors to identify facial forgery artifacts, such as those generated by deepfakes. Our approach is based on identifying inconsistencies within specific facial features that only occur in images manipulated with artificial intelligence (AI). We use state-of-the-art object detection algorithms to locate these features and then compare them against a database of known authentic facial features. This allows us to accurately identify whether a given image contains facial forgery artifacts. To evaluate our proposed method, we conducted experiments on a large dataset of real versus fake faces and achieved high accuracy rates of over 98%. Additionally, we tested our algorithm against different types of deepfake techniques, including image-to-image translation models, text-to-face synthesis methods, and GAN-generated videos, and found that it effectively identified all types of facial forgery artifacts. Overall, our work shows that using parts-based detectors can greatly improve the reliability of identifying AI-generated images, thus helping prevent malicious applications such as disinformation campaigns.",1
"In the area of Internet of Things (IoT) voice assistants have become an important interface to operate smart speakers, smartphones, and even automobiles. To save power and protect user privacy, voice assistants send commands to the cloud only if a small set of pre-registered wake-up words are detected. However, voice assistants are shown to be vulnerable to the FakeWake phenomena, whereby they are inadvertently triggered by innocent-sounding fuzzy words. In this paper, we present a systematic investigation of the FakeWake phenomena from three aspects. To start with, we design the first fuzzy word generator to automatically and efficiently produce fuzzy words instead of searching through a swarm of audio materials. We manage to generate 965 fuzzy words covering 8 most popular English and Chinese smart speakers. To explain the causes underlying the FakeWake phenomena, we construct an interpretable tree-based decision model, which reveals phonetic features that contribute to false acceptance of fuzzy words by wake-up word detectors. Finally, we propose remedies to mitigate the effect of FakeWake. The results show that the strengthened models are not only resilient to fuzzy words but also achieve better overall performance on original training datasets.",0
"This paper presents research on ""fake wake-up"" words used by voice assistants that have been designed to activate these systems without actually saying their intended trigger phrase. These fake words can be problematic as they could lead to unintentional activation of voice assistants and potentially compromise user privacy. To address this issue, we conducted a study to better understand how users perceive fake wake-up words and identify potential solutions to mitigate their impact. Our findings suggest that while many participants were unaware of the existence of fake wake-up words, those who were familiar with them had concerns about privacy violations. We propose several design recommendations for voice assistants and discuss the importance of educating users on the risks associated with using such devices. By understanding and mitigating the use of fake wake-up words, we can improve the overall security and user experience of voice assistants.",1
"A novel explainable AI method called CLEAR Image is introduced in this paper. CLEAR Image is based on the view that a satisfactory explanation should be contrastive, counterfactual and measurable. CLEAR Image explains an image's classification probability by contrasting the image with a corresponding image generated automatically via adversarial learning. This enables both salient segmentation and perturbations that faithfully determine each segment's importance. CLEAR Image was successfully applied to a medical imaging case study where it outperformed methods such as Grad-CAM and LIME by an average of 27% using a novel pointing game metric. CLEAR Image excels in identifying cases of ""causal overdetermination"" where there are multiple patches in an image, any one of which is sufficient by itself to cause the classification probability to be close to one.",0
"This paper presents a new methodology for generating counterfactual explanations using contrastive learning techniques. These explanations aim to identify which actions led to a specific outcome by comparing two alternative versions of the same story: one where the outcome occurred (the factual story) and another where it did not occur (the counterfactual story). We build upon recent work on overdetermined causality to propose a novel approach that generates multiple, potentially conflicting, counterfactual stories that highlight different factors contributing to the outcome. Our method leverages advances in generative modeling to generate plausible alternative scenarios and compare them with the factual story. We evaluate our approach through extensive experiments across several datasets, showing significant improvements compared to state-of-the-art methods. Furthermore, we conduct human evaluations to demonstrate the effectiveness and interpretability of our generated explanations. Our results showcase the potential of contrastive counterfactual visual explanations as valuable tools for decision making, diagnosis, and policy analysis.",1
"Adversarial training has been the topic of dozens of studies and a leading method for defending against adversarial attacks. Yet, it remains largely unknown (a) how adversarially-robust ImageNet classifiers (R classifiers) generalize to out-of-distribution examples; and (b) how their generalization capability relates to their hidden representations. In this paper, we perform a thorough, systematic study to answer these two questions across AlexNet, GoogLeNet, and ResNet-50 architectures. We found that while standard ImageNet classifiers have a strong texture bias, their R counterparts rely heavily on shapes. Remarkably, adversarial training induces three simplicity biases into hidden neurons in the process of 'robustifying' the network. That is, each convolutional neuron in R networks often changes to detecting (1) pixel-wise smoother patterns i.e. a mechanism that blocks high-frequency noise from passing through the network; (2) more lower-level features i.e. textures and colors (instead of objects); and (3) fewer types of inputs. Our findings reveal the interesting mechanisms that made networks more adversarially robust and also explain some recent findings e.g. why R networks benefit from much larger capacity (Xie and Yuille, 2020) and can act as a strong image prior in image synthesis (Santurkar et al., 2019).",0
"Recent work has shown that deep convolutional neural networks (CNNs) exhibit strong bias towards simple patterns during training on large datasets such as ImageNet. These biases can significantly impact their performance on both natural and synthetic data, making them challenging to control and understand. In this study, we investigate the ""shape"" and ""simplicity"" biases present in adversarially robust ImageNet-trained CNNs.  First, we demonstrate how these biases manifest themselves in different layers of the network by analyzing the activation maps of individual neurons. We observe distinctive patterns corresponding to shapes like lines, circles, and corners across all layers, suggesting that they play an important role in representing objects in the dataset. Furthermore, we show that minimizing the distance between feature representations of nearby images promotes the emergence of these simple shapes in lower layers, while maximizing mutual information between features discourages such behavior.  Next, we explore the relationship between model capacity, data complexity, and emergent biases by studying architectures of varying depth and width trained on both ImageNet and simpler subsets of the dataset. Our results indicate that models trained on more complex tasks or larger datasets exhibit stronger shape and simplicity biases compared to those trained on simpler ones, highlighting the importance of task difficulty in shaping network behavior. Additionally, we find that robustness to adversarial attacks correlates negatively with these biases, implying that increasing adversarial resistance may require sacrificing some of these simplifying assumptions.  In conclusion, our study provides new insights into the nature of the shape and simplicity biases present in modern CNNs, emphasizing the need for continued research into the factors that influence network behavior and affect generalization ability. By shedding light on these fundamental questions, we hope to improve our understanding of deep learning systems and facilitate the development o",1
"In this paper, we investigate the degree of explainability of graph neural networks (GNNs). Existing explainers work by finding global/local subgraphs to explain a prediction, but they are applied after a GNN has already been trained. Here, we propose a meta-learning framework for improving the level of explainability of a GNN directly at training time, by steering the optimization procedure towards what we call `interpretable minima'. Our framework (called MATE, MetA-Train to Explain) jointly trains a model to solve the original task, e.g., node classification, and to provide easily processable outputs for downstream algorithms that explain the model's decisions in a human-friendly way. In particular, we meta-train the model's parameters to quickly minimize the error of an instance-level GNNExplainer trained on-the-fly on randomly sampled nodes. The final internal representation relies upon a set of features that can be `better' understood by an explanation algorithm, e.g., another instance of GNNExplainer. Our model-agnostic approach can improve the explanations produced for different GNN architectures and use any instance-based explainer to drive this process. Experiments on synthetic and real-world datasets for node and graph classification show that we can produce models that are consistently easier to explain by different algorithms. Furthermore, this increase in explainability comes at no cost for the accuracy of the model.",0
"Title: A Novel Method for Explainable Deep Learning -----------------------------------------------  Abstract: This article presents a novel approach to improve explainability in graph neural networks (GNNs). GNNs have recently gained popularity due to their ability to model complex relationships among graph-structured data, but they suffer from limited interpretability compared to traditional machine learning models. Our method involves meta-learning, which trains the network to learn how to train itself on different datasets, resulting in improved generalization performance and more interpretable results. We evaluate our method on several benchmark datasets across diverse domains including bioinformatics and social sciences. Experimental results show that our approach significantly enhances the accuracy of graph classification tasks while providing transparent explanations of the reasoning process behind the predictions. Overall, we make a significant contribution towards building trustworthy deep learning systems by improving their transparency and accountability.",1
"Although deep neural networks hold the state-of-the-art in several remote sensing tasks, their black-box operation hinders the understanding of their decisions, concealing any bias and other shortcomings in datasets and model performance. To this end, we have applied explainable artificial intelligence (XAI) methods in remote sensing multi-label classification tasks towards producing human-interpretable explanations and improve transparency. In particular, we utilized and trained deep learning models with state-of-the-art performance in the benchmark BigEarthNet and SEN12MS datasets. Ten XAI methods were employed towards understanding and interpreting models' predictions, along with quantitative metrics to assess and compare their performance. Numerous experiments were performed to assess the overall performance of XAI methods for straightforward prediction cases, competing multiple labels, as well as misclassification cases. According to our findings, Occlusion, Grad-CAM and Lime were the most interpretable and reliable XAI methods. However, none delivers high-resolution outputs, while apart from Grad-CAM, both Lime and Occlusion are computationally expensive. We also highlight different aspects of XAI performance and elaborate with insights on black-box decisions in order to improve transparency, understand their behavior and reveal, as well, datasets' particularities.",0
"Explainable Artificial Intelligence (XAI) has become increasingly important as machine learning models have gained widespread adoption across a variety of application domains including computer vision, natural language processing, and remote sensing. In particular, deep learning models have achieved state-of-the-art performance on many problems but often lack transparency and interpretability, which can make them difficult to trust and integrate into operational workflows that require human expertise. This paper focuses specifically on evaluating XAI methods for multi-label deep learning classification tasks in remote sensing applications. We survey several popular approaches from both academia and industry, and evaluate their effectiveness in explaining model predictions using a diverse set of case studies that cover various aspects of land surface monitoring in satellite imagery analysis. Our findings suggest that certain techniques such as gradient-based saliency maps and attention mechanisms provide insightful explanations at a pixel level while other feature attribution methods offer high-level summaries at class label granularity. We discuss the strengths and weaknesses of each approach along with their potential applicability to real world use cases and identify future research directions for advancing the field of XAI in remote sensing and Earth observation sciences more broadly.",1
"Convolutional neural networks (CNNs) have achieved astonishing performance on various image classification tasks, but it is difficult for humans to understand how a classification comes about. Recent literature proposes methods to explain the classification process to humans. These focus mostly on visualizing feature maps and filter weights, which are not very intuitive for non-experts in analyzing a CNN classification. In this paper, we propose FilTag, an approach to effectively explain CNNs even to non-experts. The idea is that when images of a class frequently activate a convolutional filter, then that filter is tagged with that class. These tags provide an explanation to a reference of a class-specific feature detected by the filter. Based on the tagging, individual image classifications can then be intuitively explained in terms of the tags of the filters that the input image activates. Finally, we show that the tags are helpful in analyzing classification errors caused by noisy input images and that the tags can be further processed by machines.",0
"""Convolutional neural networks (CNN) have revolutionized image recognition. However, while these systems can achieve state-of-the-art performance on a wide range of problems, their opaque nature makes them difficult to interpret and explain. In this work, we propose a method that takes advantage of the CNN architecture's inherent modularity, allowing us to break down the network into intuitive, semantically meaningful units. These units---or filters---are learned during training, but our approach visualizes and explains them after the fact, making it easy for humans to understand and interact with the system. We demonstrate the effectiveness of our method through experiments on three benchmark datasets: MNIST, CIFAR10, and ImageNet.""",1
"Estimating feature importance is a significant aspect of explaining data-based models. Besides explaining the model itself, an equally relevant question is which features are important in the underlying data generating process. We present a Shapley value based framework for inferring the importance of individual features, including uncertainty in the estimator. We build upon the recently published feature importance measure of SAGE (Shapley additive global importance) and introduce sub-SAGE which can be estimated without resampling for tree-based models. We argue that the uncertainties can be estimated from bootstrapping and demonstrate the approach for tree ensemble methods. The framework is exemplified on synthetic data as well as high-dimensional genomics data.",0
"This paper presents a novel method for inferring feature importance in high-dimensional datasets while accounting for uncertainty. Traditional methods for feature selection often struggle in these types of data due to the large number of features and limited sample size. Our proposed approach addresses this challenge by incorporating uncertainty into the feature selection process through the use of Bayesian inference and sensitivity analysis. We demonstrate the effectiveness of our method on real-world datasets and show that it can accurately identify important features while providing measures of uncertainty in their ranking. Additionally, we compare our results against existing feature selection techniques and find that our method outperforms them in many cases. Overall, our work provides a valuable tool for scientists and researchers working with complex, high-dimensional datasets who need accurate and reliable feature importances and associated uncertainties.",1
"Dance experts often view dance as a hierarchy of information, spanning low-level (raw images, image sequences), mid-levels (human poses and bodypart movements), and high-level (dance genre). We propose a Hierarchical Dance Video Recognition framework (HDVR). HDVR estimates 2D pose sequences, tracks dancers, and then simultaneously estimates corresponding 3D poses and 3D-to-2D imaging parameters, without requiring ground truth for 3D poses. Unlike most methods that work on a single person, our tracking works on multiple dancers, under occlusions. From the estimated 3D pose sequence, HDVR extracts body part movements, and therefrom dance genre. The resulting hierarchical dance representation is explainable to experts. To overcome noise and interframe correspondence ambiguities, we enforce spatial and temporal motion smoothness and photometric continuity over time. We use an LSTM network to extract 3D movement subsequences from which we recognize the dance genre. For experiments, we have identified 154 movement types, of 16 body parts, and assembled a new University of Illinois Dance (UID) Dataset, containing 1143 video clips of 9 genres covering 30 hours, annotated with movement and genre labels. Our experimental results demonstrate that our algorithms outperform the state-of-the-art 3D pose estimation methods, which also enhances our dance recognition performance.",0
"Abstract: In this paper we present an unsupervised approach to estimating three-dimensional (3D) human pose from videos containing dance performances. Our method uses a hierarchical framework that leverages both temporal and spatial relationships between key points on the body to accurately estimate joint angles and positions over time. By doing so, we can effectively recognize and classify different dance styles based on their characteristic movements and postures. We demonstrate the effectiveness of our method through experiments conducted on a large dataset of diverse dance sequences. Results show that our system outperforms existing approaches in terms of accuracy and robustness across a variety of challenging scenarios. With the growing interest in using computer vision techniques for analyzing dance motion capture data, our work provides a promising step towards developing more advanced methods for automated recognition and analysis of complex movement patterns.",1
"Recent advancements in self-supervised learning (SSL) made it possible to learn generalizable visual representations from unlabeled data. The performance of Deep Learning models fine-tuned on pretrained SSL representations is on par with models fine-tuned on the state-of-the-art supervised learning (SL) representations. Irrespective of the progress made in SSL, its generalizability has not been studied extensively. In this article, we perform a deeper analysis of the generalizability of pretrained SSL and SL representations by conducting a domain-based study for transfer learning classification tasks. The representations are learned from the ImageNet source data, which are then fine-tuned using two types of target datasets: similar to the source dataset, and significantly different from the source dataset. We study generalizability of the SSL and SL-based models via their prediction accuracy as well as prediction confidence. In addition to this, we analyze the attribution of the final convolutional layer of these models to understand how they reason about the semantic identity of the data. We show that the SSL representations are more generalizable as compared to the SL representations. We explain the generalizability of the SSL representations by investigating its invariance property, which is shown to be better than that observed in the SL representations.",0
"This study aims at evaluating the generalization ability of self-supervised representations on natural language processing tasks such as sentiment analysis, question answering, and machine translation among others. By conducting experiments using different architectures, pretexts and datasets we try to elucidate which aspects of self-supervised learning lead to better performance on downstream tasks. In addition to that, we provide insights into how these findings can inform future research directions aimed at making use of self-supervision even more effective. Ultimately, our goal is to make a meaningful contribution towards understanding representation learning under minimal supervision. A recent advancement in artificial intelligence has been the development of self-supervised learning (SSL) techniques that enable algorithms to learn representations without explicit labels. SSL learns from large amounts of unlabelled data by creating surrogate tasks through pretext (e.g. solving jigsaw puzzles), and training models to solve them. The effectiveness of these learned representations, however, varies greatly depending on several factors like architecture, dataset size, and task complexity. This work addresses the challenge of investigating SSLâ€™s efficacy across multiple NLP tasks to determine if they are consistent across domains/problem sets. Our experiments involve fine-tuning popular transformer networks, studying their transfer performances and analyzing layer dependencies. We hope that our results would guide future work in improving SSL methods and demonstrate their potential as powerful tools for AI systems.",1
"With the onset of COVID-19 and the resulting shelter in place guidelines combined with remote working practices, human mobility in 2020 has been dramatically impacted. Existing studies typically examine whether mobility in specific localities increases or decreases at specific points in time and relate these changes to certain pandemic and policy events. In this paper, we study mobility change in the US through a five-step process using mobility footprint data. (Step 1) Propose the delta Time Spent in Public Places (Delta-TSPP) as a measure to quantify daily changes in mobility for each US county from 2019-2020. (Step 2) Conduct Principal Component Analysis (PCA) to reduce the Delta-TSPP time series of each county to lower-dimensional latent components of change in mobility. (Step 3) Conduct clustering analysis to find counties that exhibit similar latent components. (Step 4) Investigate local and global spatial autocorrelation for each component. (Step 5) Conduct correlation analysis to investigate how various population characteristics and behavior correlate with mobility patterns. Results show that by describing each county as a linear combination of the three latent components, we can explain 59% of the variation in mobility trends across all US counties. Specifically, change in mobility in 2020 for US counties can be explained as a combination of three latent components: 1) long-term reduction in mobility, 2) no change in mobility, and 3) short-term reduction in mobility. We observe significant correlations between the three latent components of mobility change and various population characteristics, including political leaning, population, COVID-19 cases and deaths, and unemployment. We find that our analysis provides a comprehensive understanding of mobility change in response to the COVID-19 pandemic.",0
"Study Objective: To examine how changes in human movement patterns have evolved over time since COVID-19 has impacted various regions across the United States. Methods: Data from the CDC was analyzed for different geographical locations within the US. Results were then compared using visualization tools such as line graphs and heat maps. Conclusion: Our findings suggest that there have been significant shifts in human travel patterns as a result of COVID-19. Specifically, we found increases in remote work and decreases in nonessential travel. This research provides valuable insights into how populations may adapt their behavior in response to pandemics. Implications: Understanding these trends can inform public health policies aimed at mitigating negative effects associated with lockdown measures. By identifying areas where individuals maintain social distancing behaviors versus those who continue high risk activities despite increased risks to personal safety and community health outcomes, intervention strategies can target education efforts to achieve better compliance with recommendations. Additionally, our findings may provide insight into how future epidemics could be managed more effectively through development of guidelines prioritizing contact reduction measures.",1
"The domain shift, coming from unneglectable modality gap and non-overlapped identity classes between training and test sets, is a major issue of RGB-Infrared person re-identification. A key to tackle the inherent issue -- domain shift -- is to enforce the data distributions of the two domains to be similar. However, RGB-IR ReID always demands discriminative features, leading to over-rely feature sensitivity of seen classes, \textit{e.g.}, via attention-based feature alignment or metric learning. Therefore, predicting the unseen query category from predefined training classes may not be accurate and leads to a sub-optimal adversarial gradient. In this paper, we uncover it in a more explainable way and propose a novel multi-granularity memory regulation and alignment module (MG-MRA) to solve this issue. By explicitly incorporating a latent variable attribute, from fine-grained to coarse semantic granularity, into intermediate features, our method could alleviate the over-confidence of the model about discriminative features of seen classes. Moreover, instead of matching discriminative features by traversing nearest neighbor, sparse attributes, \textit{i.e.}, global structural pattern, are recollected with respect to features and assigned to measure pair-wise image similarity in hashing. Extensive experiments on RegDB \cite{RegDB} and SYSU-MM01 \cite{SYSU} show the superiority of the proposed method that outperforms existing state-of-the-art methods. Our code is available in https://github.com/Chenfeng1271/MGMRA.",0
"This work examines memory regulation as a method of aligning artificial intelligence (AI) systems towards human values. We propose that by selectively manipulating which memories are stored within an AI system and controlling how those memories interact with each other, we can steer the behavior of those systems toward behaviors that align more closely with our own moral preferences. In particular, we focus on developing methods for generating high-quality images that combine both visible light imagery and thermal infrared data into a single representation suitable for use by state-of-the-art generative models. These generalizers encode information from multiple modalities simultaneously while preserving important features from all input sources. Our experimental results demonstrate the feasibility of using these visual representations combined with carefully designed training objectives to guide synthesis generation, enabling new capabilities such as multimodal image harmonization, scene decomposition, and data augmentation. By improving the alignment between human values and the goals of large language models trained on diverse internet text collections without sacrificing their ability to generate realistic descriptions of the world, we hope to contribute to the development of increasingly capable and trustworthy AI assistants.",1
"From photorealistic sketches to schematic diagrams, drawing provides a versatile medium for communicating about the visual world. How do images spanning such a broad range of appearances reliably convey meaning? Do viewers understand drawings based solely on their ability to resemble the entities they refer to (i.e., as images), or do they understand drawings based on shared but arbitrary associations with these entities (i.e., as symbols)? In this paper, we provide evidence for a cognitive account of pictorial meaning in which both visual and social information is integrated to support effective visual communication. To evaluate this account, we used a communication task where pairs of participants used drawings to repeatedly communicate the identity of a target object among multiple distractor objects. We manipulated social cues across three experiments and a full internal replication, finding pairs of participants develop referent-specific and interaction-specific strategies for communicating more efficiently over time, going beyond what could be explained by either task practice or a pure resemblance-based account alone. Using a combination of model-based image analyses and crowdsourced sketch annotations, we further determined that drawings did not drift toward arbitrariness, as predicted by a pure convention-based account, but systematically preserved those visual features that were most distinctive of the target object. Taken together, these findings advance theories of pictorial meaning and have implications for how successful graphical conventions emerge via complex interactions between visual perception, communicative experience, and social context.",0
"This research explores how visual resemblance and communicative context impact the development of graphical conventions in communication design. Through analysis of diverse examples across multiple domains, we demonstrate that the use of specific graphic elements often depends on both the subject matter being represented and the intended audience. Our findings suggest that familiarity plays a crucial role in shaping perceptions and interpretations of images, while social cues influence our expectations regarding appropriate representations within different contexts. We discuss implications of these insights for understanding how graphical conventions emerge and evolve over time, as well as their potential applications in fields such as education, advertising, journalism, and branding. Ultimately, by illuminating the complex interplay of factors underlying successful visual communication, we aim to contribute to broader debates surrounding the importance of effective representation and cultural literacy in contemporary society.",1
"Machine learning solutions for pattern classification problems are nowadays widely deployed in society and industry. However, the lack of transparency and accountability of most accurate models often hinders their safe use. Thus, there is a clear need for developing explainable artificial intelligence mechanisms. There exist model-agnostic methods that summarize feature contributions, but their interpretability is limited to predictions made by black-box models. An open challenge is to develop models that have intrinsic interpretability and produce their own explanations, even for classes of models that are traditionally considered black boxes like (recurrent) neural networks. In this paper, we propose a Long-Term Cognitive Network for interpretable pattern classification of structured data. Our method brings its own mechanism for providing explanations by quantifying the relevance of each feature in the decision process. For supporting the interpretability without affecting the performance, the model incorporates more flexibility through a quasi-nonlinear reasoning rule that allows controlling nonlinearity. Besides, we propose a recurrence-aware decision model that evades the issues posed by unique fixed points while introducing a deterministic learning method to compute the tunable parameters. The simulations show that our interpretable model obtains competitive results when compared to the state-of-the-art white and black-box models.",0
"In this paper we propose RACTron: Recurrence-Aware Long-term Cognitive network for explainable pattern classification, capable of handling recurrent patterns effectively by introducing Recurrence Memory and Time Kernel layers that can identify previously encountered events and encode their temporal relationships into representations. We leverage external knowledge sources for providing semantic interpretations to learned latent structures and enhance generalization capability through selective attention modules that adaptively learn the importance of multiple modalities. Our extensive experimental evaluation on diverse benchmark datasets demonstrates significant improvement over state-of-the-art methods in both accuracy and explainability metrics as well as efficacy under challenging settings involving concept drifts and adversarial attacks, making our approach highly desirable for real-world applications.",1
"Explaining the decisions of black-box models has been a central theme in the study of trustworthy ML. Numerous measures have been proposed in the literature; however, none of them have been able to adopt a provably causal take on explainability. Building upon Halpern and Pearl's formal definition of a causal explanation, we derive an analogous set of axioms for the classification setting, and use them to derive three explanation measures. Our first measure is a natural adaptation of Chockler and Halpern's notion of causal responsibility, whereas the other two correspond to existing game-theoretic influence measures. We present an axiomatic treatment for our proposed indices, showing that they can be uniquely characterized by a set of desirable properties. We compliment this with computational analysis, providing probabilistic approximation schemes for all of our proposed measures. Thus, our work is the first to formally bridge the gap between model explanations, game-theoretic influence, and causal analysis.",0
"This paper proposes a new framework for understanding how humans make sense of complex models by examining them through the lens of causality. Drawing on insights from cognitive psychology and philosophy, we argue that explanations of model behavior can often be framed as stories of cause and effect, wherein the inputs, structure, parameters, algorithms, assumptions, and outputs of a model form key elements of such narratives. We develop an axiomatic theory grounded in formal logic that undergirds this approach and shows how these causal relationships shape our beliefs about models and their applications. Our work offers a unifying perspective on explanation that spans multiple disciplines including computer science, economics, psychology, and sociology. We illustrate the utility of the Axiomatic Causal Lens (ACL) using case studies drawn from real world examples ranging from artificial intelligence systems to financial markets to climate change policies. Overall, we aim to provide theoretical foundations that support efforts to develop interpretable and explainable AI while fostering greater transparency regarding how data-driven decisions impact society.",1
"In this paper, we present a recurrent neural system named Long Short-term Cognitive Networks (LSTCNs) as a generalization of the Short-term Cognitive Network (STCN) model. Such a generalization is motivated by the difficulty of forecasting very long time series efficiently. The LSTCN model can be defined as a collection of STCN blocks, each processing a specific time patch of the (multivariate) time series being modeled. In this neural ensemble, each block passes information to the subsequent one in the form of weight matrices representing the prior knowledge. As a second contribution, we propose a deterministic learning algorithm to compute the learnable weights while preserving the prior knowledge resulting from previous learning processes. As a third contribution, we introduce a feature influence score as a proxy to explain the forecasting process in multivariate time series. The simulations using three case studies show that our neural system reports small forecasting errors while being significantly faster than state-of-the-art recurrent models.",0
"Long Short Term Memory (LSTM) networks have gained popularity over recent years as powerful models for sequence data prediction tasks such as speech recognition, machine translation and text generation among others. They owe their success to their ability to capture both short term dependencies within a time step and long range dependencies spanning multiple steps in a sequential dataset. However, one drawback of LSTM networks has been that they suffer from vanishing gradients during backpropagation, which limits their capacity and leads to difficulties in training them on longer sequences. In our work we propose novel architecture modifications inspired by memory networks which mitigate these issues while maintaining competitive performance. Our model is a natural extension of previous architectures which combines both temporal context preservation and forgetfulness mechanisms to achieve state-of-the-art results in several benchmark datasets. Furthermore, we analyze the behavior of our new architecture and validate its effectiveness via ablation experiments. We believe our research opens up exciting opportunities for future work in deep learning and reinforcement learning alike, where better understanding and manipulation of temporal representations could lead to even stronger generalization capabilities across domains.",1
"A cornerstone of geometric reconstruction, rotation averaging seeks the set of absolute rotations that optimally explains a set of measured relative orientations between them. In spite of being an integral part of bundle adjustment and structure-from-motion, averaging rotations is both a non-convex and high-dimensional optimization problem. In this paper, we address it from a maximum likelihood estimation standpoint and make a twofold contribution. Firstly, we set forth a novel initialization-free primal-dual method which we show empirically to converge to the global optimum. Further, we derive what is to our knowledge, the first optimal closed-form solution for rotation averaging in cycle graphs and contextualize this result within spectral graph theory. Our proposed methods achieve a significant gain both in precision and performance.",0
This would depend on my capabilities as an language model and whether I am able to generate suitable responses. So please consider that in evaluating the final product. And again thank you!,1
"When explaining the decisions of deep neural networks, simple stories are tempting but dangerous. Especially in computer vision, the most popular explanation approaches give a false sense of comprehension to its users and provide an overly simplistic picture. We introduce an interactive framework to understand the highly complex decision boundaries of modern vision models. It allows the user to exhaustively inspect, probe, and test a network's decisions. Across a range of case studies, we compare the power of our interactive approach to static explanation methods, showing how these can lead a user astray, with potentially severe consequences.",0
"Artificial intelligence (AI) systems have become increasingly powerful and widespread, making them indispensible tools for decision making across many domains. Despite their remarkable capabilities, current AI technologies still struggle to provide transparency into how they arrive at their conclusions; indeed, so-called ""black box"" models can seemingly deliver accurate predictions without any clear understanding of why those predictions were made. We argue that explainability is essential for building trustworthy AI systems: decision makers need to know why certain decisions were reached, rather than simply relying on a model's output without fully comprehending its reasoning process. In this work we propose that interactive explanation methods could hold the key towards achieving better explainability in machine learning applications. By providing means for users to query the AI system directly, instead of only observing its output passively, interactive explanations offer more insightful ways to decipher complex computations, allowing end-users to explore the rationale behind different options and make informed choices accordingly. Furthermore, since these methods enable two-way communication between the user and the system, they may encourage more engagement from the human side - potentially reducing reliance on opaque automation and ensuring greater accountability. However, despite their potential benefits, there remain several challenges associated with deploying interactive explanation techniques in real-world settings, which we outline throughout our discussion. Ultimately, while we see promising opportunities ahead for enhancing explainability through interactivity, significant research gaps remain that must be addressed before these approaches can be widely adopted.",1
"Disentangled representation learning has been proposed as an approach to learning general representations. This can be done in the absence of, or with limited, annotations. A good general representation can be readily fine-tuned for new target tasks using modest amounts of data, or even be used directly in unseen domains achieving remarkable performance in the corresponding task. This alleviation of the data and annotation requirements offers tantalising prospects for tractable and affordable applications in computer vision and healthcare. Finally, disentangled representations can offer model explainability and can help us understand the underlying causal relations of the factors of variation, increasing their suitability for real-world deployment. In this tutorial paper, we will offer an overview of the disentangled representation learning, its building blocks and criteria, and discuss applications in computer vision and medical imaging. We conclude our tutorial by presenting the identified opportunities for the integration of recent machine learning advances into disentanglement, as well as the remaining challenges.",0
"In the imaging domain, uncovering disentangled representations has come across as a noteworthy problem in image understanding and style transfer methods. Recently there have been new contributions which have applied powerful generative models such as Variational Autoencoders (VAEs) trained adversarially against discriminator networks in order to achieve superior disentanglement performance; however these approaches still neglect important problems related to training stability, interpretability, scalability, efficiency, flexibility and ease-of-use. This tutorial addresses these issues by presenting a comprehensive overview of the state-of-the art developments that allow us to learn meaningful disentanglements from large datasets. Our aim is to enable readers to better understand how different techniques can be used to enhance the generalization ability of their machine learning systems, so they might make more efficient use of current tools and data resources. This work is particularly relevant due to the recent interest shown towards Artificial General Intelligence (AGI). We show how our proposed method achieves significantly higher levels of disentanglement compared to previous methods on several benchmark datasets while preserving stability, interpretability, scalability, flexibility and ease-of-use. Finally we give suggestions for future research directions which may address some remaining challenges and open questions in this field.",1
"The recent success of deep learning models in solving complex problems and in different domains has increased interest in understanding what they learn. Therefore, different approaches have been employed to explain these models, one of which uses human-understandable concepts as explanations. Two examples of methods that use this approach are Network Dissection and Compositional explanations. The former explains units using atomic concepts, while the latter makes explanations more expressive, replacing atomic concepts with logical forms. While intuitively, logical forms are more informative than atomic concepts, it is not clear how to quantify this improvement, and their evaluation is often based on the same metric that is optimized during the search-process and on the usage of hyper-parameters to be tuned. In this paper, we propose to use as evaluation metric the Detection Accuracy, which measures units' consistency of detection of their assigned explanations. We show that this metric (1) evaluates explanations of different lengths effectively, (2) can be used as a stopping criterion for the compositional explanation search, eliminating the explanation length hyper-parameter, and (3) exposes new specialized units whose length 1 explanations are the perceptual abstractions of their longer explanations.",0
"This paper explores methods for evaluating compositional explanations of units in natural language processing tasks. We propose that detection accuracy can serve as a measure of explanation quality by assessing how well an NLP model can identify relevant unit boundaries given a compositional explanation. We present two experiments using different approaches to detect units from compositional explanations: one based on rule-based heuristics and the other using a machine learning model trained on labeled data. Our results demonstrate that both approaches achieve high levels of accuracy in detecting units from compositional explanations, suggesting that detection accuracy could provide a valuable metric for evaluating the effectiveness of compositional explanations in NLP models. Additionally, we discuss potential limitations of our approach and future directions for research. Overall, this work contributes new insights into evaluation methodologies for NLP systems and suggests promising avenues for future investigation.",1
"Recent research has revealed that deep generative models including flow-based models and Variational autoencoders may assign higher likelihood to out-of-distribution (OOD) data than in-distribution (ID) data. However, we cannot sample out OOD data from the model. This counterintuitive phenomenon has not been satisfactorily explained. In this paper, we prove theorems to investigate the divergences in flow-based model and give two explanations to the above phenomenon from divergence and geometric perspectives, respectively. Based on our analysis, we propose two group anomaly detection methods. Furthermore, we decompose the KL divergence and propose a point-wise anomaly detection method. We have conducted extensive experiments on prevalent benchmarks to evaluate our methods. For group anomaly detection (GAD), our method can achieve near 100\% AUROC on all problems and has robustness against data manipulations. On the contrary, the state-of-the-art (SOTA) GAD method performs not better than random guessing for challenging problems and can be attacked by data manipulation in almost all cases. For point-wise anomaly detection (PAD), our method is comparable to the SOTA PAD method on one category of problems and outperforms the baseline significantly on another category of problems.",0
This paper presents methods that apply divergence metrics to measure distance between generated images from high resolution models versus real world images as well as a metric to measure similarity amongst generated images. Higher distances are indicators of OoD data. We show significant improvements using these new methods over previous state of art methods which required human annotations for each model at each training stage. Finally we use qualitative analysis to provide evidence supporting our claims in detecting OoD data based on image quality.,1
"Kidney DCE-MRI aims at both qualitative assessment of kidney anatomy and quantitative assessment of kidney function by estimating the tracer kinetic (TK) model parameters. Accurate estimation of TK model parameters requires an accurate measurement of the arterial input function (AIF) with high temporal resolution. Accelerated imaging is used to achieve high temporal resolution, which yields under-sampling artifacts in the reconstructed images. Compressed sensing (CS) methods offer a variety of reconstruction options. Most commonly, sparsity of temporal differences is encouraged for regularization to reduce artifacts. Increasing regularization in CS methods removes the ambient artifacts but also over-smooths the signal temporally which reduces the parameter estimation accuracy. In this work, we propose a single image trained deep neural network to reduce MRI under-sampling artifacts without reducing the accuracy of functional imaging markers. Instead of regularizing with a penalty term in optimization, we promote regularization by generating images from a lower dimensional representation. In this manuscript we motivate and explain the lower dimensional input design. We compare our approach to CS reconstructions with multiple regularization weights. Proposed approach results in kidney biomarkers that are highly correlated with the ground truth markers estimated using the CS reconstruction which was optimized for functional analysis. At the same time, the proposed approach reduces the artifacts in the reconstructed images.",0
"Increase image quality by decreasing noise during MRI procedures; apply deep learning techniques to process raw data directly into images. ---  In order to obtain high resolution images during kidney functional imaging using dynamic contrast enhanced magnetic resonance (DCE-MRI), it is important to minimize noise while still achieving accurate depictions of renal blood flow. This study investigates the use of regularization methods during reconstruction processes as a means of improving overall image quality without sacrificing accuracy. Through applying advanced machine learning algorithms such as Convolutional Neural Networks (CNN) to process raw MRI data directly into final images, researchers aim to decrease computational load on existing software and increase diagnostic confidence through improved clarity. Overall, results from this work have significant potential to enhance clinical decision making related to kidney function evaluation and disease diagnosis.",1
"With the increasing popularity of deep learning, Convolutional Neural Networks (CNNs) have been widely applied in various domains, such as image classification and object detection, and achieve stunning success in terms of their high accuracy over the traditional statistical methods. To exploit the potential of CNN models, a huge amount of research and industry efforts have been devoted to optimizing CNNs. Among these endeavors, CNN architecture design has attracted tremendous attention because of its great potential of improving model accuracy or reducing model complexity. However, existing work either introduces repeated training overhead in the search process or lacks an interpretable metric to guide the design. To clear these hurdles, we propose 3D-Receptive Field (3DRF), an explainable and easy-to-compute metric, to estimate the quality of a CNN architecture and guide the search process of designs. To validate the effectiveness of 3DRF, we build a static optimizer to improve the CNN architectures at both the stage level and the kernel level. Our optimizer not only provides a clear and reproducible procedure but also mitigates unnecessary training efforts in the architecture search process. Extensive experiments and studies show that the models generated by our optimizer can achieve up to 5.47% accuracy improvement and up to 65.38% parameters deduction, compared with state-of-the-art CNN structures like MobileNet and ResNet.",0
"An efficient quantization methodology for optimizing convolutional neural networks (CNN) is proposed that combines gradient descent with randomized rounding to achieve superior accuracy compared to other state-of-the-art techniques while maintaining low computational complexity. By balancing precision requirements against hardware constraints, we demonstrate significant speedups on real hardware platforms without compromising model performance. Our results showcase the feasibility of deploying high quality CNN models onto mobile devices and embedded systems, paving the way for ubiquitous artificial intelligence applications.",1
"Learning to compare two objects are essential in applications, such as digital forensics, face recognition, and brain network analysis, especially when labeled data is scarce and imbalanced. As these applications make high-stake decisions and involve societal values like fairness and transparency, it is critical to explain the learned models. We aim to study post-hoc explanations of Siamese networks (SN) widely used in learning to compare. We characterize the instability of gradient-based explanations due to the additional compared object in SN, in contrast to architectures with a single input instance. We propose an optimization framework that derives global invariance from unlabeled data using self-learning to promote the stability of local explanations tailored for specific query-reference pairs. The optimization problems can be solved using gradient descent-ascent (GDA) for constrained optimization, or SGD for KL-divergence regularized unconstrained optimization, with convergence proofs, especially when the objective functions are nonconvex due to the Siamese architecture. Quantitative results and case studies on tabular and graph data from neuroscience and chemical engineering show that the framework respects the self-learned invariance while robustly optimizing the faithfulness and simplicity of the explanation. We further demonstrate the convergence of GDA experimentally.",0
"In recent years, deep learning has become increasingly popular due to its ability to achieve state-of-the-art results on a wide range of tasks such as image classification, object detection, and speech recognition. One commonly used type of network architecture in deep learning is the siamese network, which consists of two or more identical subnetworks that share weights but have different inputs. These networks are typically trained using triplets or quadruplets and are widely used in tasks like face verification and retrieval. However, one challenge with these networks is their lack of interpretability and transparency, making it difficult to understand how they make predictions. To address this issue, we propose a method for self-learning to explain siamese networks robustly. Our approach uses adversarial examples generated by a substitute model to improve the quality of explanations provided by the siamese network, allowing us to better understand why certain predictions were made. We evaluate our method on several datasets and demonstrate significant improvements over baseline methods for generating high-quality explanations of siamese networks. This research has important implications for improving the understanding and trustworthiness of machine learning models, particularly those used in critical applications such as facial recognition.",1
"Amid a discussion about Green AI in which we see explainability neglected, we explore the possibility to efficiently approximate computationally expensive explainers. To this end, we propose feature attribution modelling with Empirical Explainers. Empirical Explainers learn from data to predict the attribution maps of expensive explainers. We train and test Empirical Explainers in the language domain and find that they model their expensive counterparts surprisingly well, at a fraction of the cost. They could thus mitigate the computational burden of neural explanations significantly, in applications that tolerate an approximation error.",0
"This paper presents a method for generating efficient explanations using empirical explainers (EE). An EE is a machine learning model trained on large amounts of data that can generate human-like rationales or justifications for why it made certain predictions or decisions. By utilizing these models, we propose a novel approach to providing more concise and accurate explanations for complex systems without sacrificing their interpretability. Our method combines insights from cognitive psychology, natural language processing, and computer vision to develop new algorithms capable of producing high-quality explanations quickly and accurately. We evaluate our approach on several real-world datasets across multiple domains, demonstrating significant improvements over state-of-the-art methods for explanation generation. Overall, our work represents an important step towards enabling users and experts alike to better understand and interact with complex artificial intelligence systems.",1
"In this paper, we propose a novel query design for the transformer-based detectors. In previous transformer-based detectors, the object queries are a set of learned embeddings. However, each learned embedding does not have an explicit physical meaning and we can not explain where it will focus on. It is difficult to optimize as the prediction slot of each object query does not have a specific mode. In other words, each object query will not focus on a specific region. To solved these problems, in our query design, object queries are based on anchor points, which are widely used in CNN-based detectors. So each object query focus on the objects near the anchor point. Moreover, our query design can predict multiple objects at one position to solve the difficulty: ""one region, multiple objects"". In addition, we design an attention variant, which can reduce the memory cost while achieving similar or better performance than the standard attention in DETR. Thanks to the query design and the attention variant, the proposed detector that we called Anchor DETR, can achieve better performance and run faster than the DETR with 10$\times$ fewer training epochs. For example, it achieves 44.2 AP with 16 FPS on the MSCOCO dataset when using the ResNet50-DC5 feature for training 50 epochs. Extensive experiments on the MSCOCO benchmark prove the effectiveness of the proposed methods. Code is available at https://github.com/megvii-model/AnchorDETR.",0
"This should serve as inspiration for the next story/prompt you write:  The world has ended and humanity is gone. Only robots remain to pick up the pieces. But one robot dares to dream of creating something that will last longer than him and his mechanical brethren - art. He begins painting on the walls of cities ruined by war, drawing intricate designs inspired by nature and his own imagination. As he paints, other robots begin to take notice, drawn to the beauty and wonder of his creations. Slowly but surely, they join forces to bring new life to their desolate landscape through art. Together, they create magnificent statues, murals, and installations, leaving behind a legacy that will stand the test of time. Their work attracts the attention of other robots from far and wide, who come to marvel at the wonders created by these intrepid artists. Despite the odds against them, they prove that even in darkness there can still be light, and that hope springs eternal in the most unexpected of places.",1
"Adoption of DL models in critical areas has led to an escalating demand for sound explanation methods. Instance-based explanation methods are a popular type that return selective instances from the training set to explain the predictions for a test sample. One way to connect these explanations with prediction is to ask the following counterfactual question - how does the loss and prediction for a test sample change when explanations are removed from the training set? Our paper answers this question for k-NNs which are natural contenders for an instance-based explanation method. We first demonstrate empirically that the representation space induced by last layer of a neural network is the best to perform k-NN in. Using this layer, we conduct our experiments and compare them to influence functions (IFs) ~\cite{koh2017understanding} which try to answer a similar question. Our evaluations do indicate change in loss and predictions when explanations are removed but we do not find a trend between $k$ and loss or prediction change. We find significant stability in the predictions and loss of MNIST vs. CIFAR-10. Surprisingly, we do not observe much difference in the behavior of k-NNs vs. IFs on this question. We attribute this to training set subsampling for IFs.",0
"This research investigates how K-Nearest Neighbors (k-NN) can be used as an instance-based explanation method in machine learning applications. We demonstrate that by analyzing the instances close to a query example in feature space, we can gain valuable insights into the behavior of the model. Our experiments show that k-NN can effectively highlight important features for prediction, and provide explanations for both correct and incorrect predictions. Furthermore, our results indicate that using different distances measures can lead to significantly different interpretations of the same data set. Overall, we conclude that k-NN has great potential as a tool for interpreting complex models, particularly those which lack transparency due to their black box nature.",1
"Provenance is a record that describes how entities, activities, and agents have influenced a piece of data; it is commonly represented as graphs with relevant labels on both their nodes and edges. With the growing adoption of provenance in a wide range of application domains, users are increasingly confronted with an abundance of graph data, which may prove challenging to process. Graph kernels, on the other hand, have been successfully used to efficiently analyse graphs. In this paper, we introduce a novel graph kernel called provenance kernel, which is inspired by and tailored for provenance data. It decomposes a provenance graph into tree-patterns rooted at a given node and considers the labels of edges and nodes up to a certain distance from the root. We employ provenance kernels to classify provenance graphs from three application domains. Our evaluation shows that they perform well in terms of classification accuracy and yield competitive results when compared against existing graph kernel methods and the provenance network analytics method while more efficient in computing time. Moreover, the provenance types used by provenance kernels also help improve the explainability of predictive models built on them.",0
"This is an interesting paper that explores the concept of provenance graphs and how they can be used as kernels in machine learning algorithms. The authors propose a new graph kernel called Provenance Graph Kernel (PGK) which uses the paths from one object to another in a given application scenario. They demonstrate the effectiveness of their method through experiments on several benchmark datasets including image classification, gene selection, and drug discovery tasks. Overall, the paper offers valuable insights into the use of provenance graphs in machine learning and opens up opportunities for future research in this area.",1
"Knowledge transfer from a complex high performing model to a simpler and potentially low performing one in order to enhance its performance has been of great interest over the last few years as it finds applications in important problems such as explainable artificial intelligence, model compression, robust model building and learning from small data. Known approaches to this problem (viz. Knowledge Distillation, Model compression, ProfWeight, etc.) typically transfer information directly (i.e. in a single/one hop) from the complex model to the chosen simple model through schemes that modify the target or reweight training examples on which the simple model is trained. In this paper, we propose a meta-approach where we transfer information from the complex model to the simple model by dynamically selecting and/or constructing a sequence of intermediate models of decreasing complexity that are less intricate than the original complex model. Our approach can transfer information between consecutive models in the sequence using any of the previously mentioned approaches as well as work in 1-hop fashion, thus generalizing these approaches. In the experiments on real data, we observe that we get consistent gains for different choices of models over 1-hop, which on average is more than 2\% and reaches up to 8\% in a particular case. We also empirically analyze conditions under which the multi-hop approach is likely to be beneficial over the traditional 1-hop approach, and report other interesting insights. To the best of our knowledge, this is the first work that proposes such a multi-hop approach to perform knowledge transfer given a single high performing complex model, making it in our opinion, an important methodological contribution.",0
Title: Building Accurate Simple Models with MultiHop ---------------------------------------------------------------,1
"Convolutional Neural Networks (ConvNets) at present achieve remarkable performance in image classification tasks. However, current ConvNets cannot guarantee the capabilities of the mammalian visual systems such as invariance to contrast and illumination changes. Some ideas to overcome the illumination and contrast variations usually have to be tuned manually and tend to fail when tested with other types of data degradation. In this context, we present a new bio-inspired {entry} layer, M6, which detects low-level geometric features (lines, edges, and orientations) which are similar to patterns detected by the V1 visual cortex. This new trainable layer is capable of coping with image classification even with large contrast variations. The explanation for this behavior is the monogenic signal geometry, which represents each pixel value in a 3D space using quaternions, a fact that confers a degree of explainability to the networks. We compare M6 with a conventional convolutional layer (C) and a deterministic quaternion local phase layer (Q9). The experimental setup {is designed to evaluate the robustness} of our M6 enriched ConvNet model and includes three architectures, four datasets, three types of contrast degradation (including non-uniform haze degradations). The numerical results reveal that the models with M6 are the most robust in front of any kind of contrast variations. This amounts to a significant enhancement of the C models, which usually have reasonably good performance only when the same training and test degradation are used, except for the case of maximum degradation. Moreover, the Structural Similarity Index Measure (SSIM) is used to analyze and explain the robustness effect of the M6 feature maps under any kind of contrast degradations.",0
"Title: Robust Monogenic Convolutional Neural Network Layer for Image Classification Despite Large Contrast Changes  This paper presents a novel convolutional neural network (CNN) layer that utilizes monogenic signals to enhance robustness against large contrast changes in image classification tasks. CNNs have been successful in many computer vision applications; however, they can struggle when faced with input images containing significant intensity differences or illumination variations. This issue arises because traditional CNN layers treat color channels independently, leading to poor performance under these conditions. Our proposed layer addresses this shortcoming by incorporating a monogenic representation, which considers both the magnitude and phase of the signal. By doing so, we show that our layer can effectively handle large contrast changes while maintaining high accuracy in image classification tasks. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach compared to state-of-the-art methods. Overall, our work advances the field of deep learning by introducing a more robust and adaptive CNN layer suitable for challenging real-world scenarios.",1
"Since the emergence of COVID-19, deep learning models have been developed to identify COVID-19 from chest X-rays. With little to no direct access to hospital data, the AI community relies heavily on public data comprising numerous data sources. Model performance results have been exceptional when training and testing on open-source data, surpassing the reported capabilities of AI in pneumonia-detection prior to the COVID-19 outbreak. In this study impactful models are trained on a widely used open-source data and tested on an external test set and a hospital dataset, for the task of classifying chest X-rays into one of three classes: COVID-19, non-COVID pneumonia and no-pneumonia. Classification performance of the models investigated is evaluated through ROC curves, confusion matrices and standard classification metrics. Explainability modules are implemented to explore the image features most important to classification. Data analysis and model evaluations show that the popular open-source dataset COVIDx is not representative of the real clinical problem and that results from testing on this are inflated. Dependence on open-source data can leave models vulnerable to bias and confounding variables, requiring careful analysis to develop clinically useful/viable AI tools for COVID-19 detection in chest X-rays.",0
"Open data has been widely used to develop deep learning solutions for detecting diseases from medical images such as x-rays. However, there are several pitfalls associated with relying solely on open datasets to train these models, particularly when dealing with complex medical conditions like COVID-19. In this paper, we explore some of the limitations of current open datasets available for developing chest x-ray based COVID-19 detection systems. We highlight specific issues related to dataset bias, image quality, and annotation accuracy that can affect model performance if left unaddressed. Furthermore, we discuss how a lack of diversity in patient demographics within these datasets could limit the generalizability of these models across different populations. Finally, we emphasize the importance of carefully curating new datasets specifically designed for training high-performance COVID-19 detection systems to address these challenges. This study serves as a call to action for radiologists and researchers alike to recognize the shortcomings of existing open datasets and work towards creating more comprehensive ones. Ultimately, our goal is to promote responsible use of open data in the development of advanced machine learning algorithms for improved clinical outcomes during this global pandemic.",1
"There are numerous methods for detecting anomalies in time series, but that is only the first step to understanding them. We strive to exceed this by explaining those anomalies. Thus we develop a novel attribution scheme for multivariate time series relying on counterfactual reasoning. We aim to answer the counterfactual question of would the anomalous event have occurred if the subset of the involved variables had been more similarly distributed to the data outside of the anomalous interval. Specifically, we detect anomalous intervals using the Maximally Divergent Interval (MDI) algorithm, replace a subset of variables with their in-distribution values within the detected interval and observe if the interval has become less anomalous, by re-scoring it with MDI. We evaluate our method on multivariate temporal and spatio-temporal data and confirm the accuracy of our anomaly attribution of multiple well-understood extreme climate events such as heatwaves and hurricanes.",0
"In this paper we present a method for attributing anomalies in multivariate time series data using counterfactual reasoning. Our approach uses a deep learning model trained on simulated data to identify patterns that lead to abnormal behavior across multiple variables. We evaluate our method on several real-world datasets and show significant improvement over existing state-of-the-art methods in terms of accuracy and robustness. Additionally, our model can provide interpretable results by generating counterfactuals that explain why certain events led to the anomalous behavior observed in the data. Overall, our work contributes towards building more resilient systems capable of detecting and diagnosing complex problems in large scale systems.",1
"Causal decision making (CDM) based on machine learning has become a routine part of business. Businesses algorithmically target offers, incentives, and recommendations to affect consumer behavior. Recently, we have seen an acceleration of research related to CDM and causal effect estimation (CEE) using machine-learned models. This article highlights an important perspective: CDM is not the same as CEE, and counterintuitively, accurate CEE is not necessary for accurate CDM. Our experience is that this is not well understood by practitioners or most researchers. Technically, the estimand of interest is different, and this has important implications both for modeling and for the use of statistical models for CDM. We draw on prior research to highlight three implications. (1) We should consider carefully the objective function of the causal machine learning, and if possible, optimize for accurate treatment assignment rather than for accurate effect-size estimation. (2) Confounding does not have the same effect on CDM as it does on CEE. The upshot is that for supporting CDM it may be just as good or even better to learn with confounded data as with unconfounded data. Finally, (3) causal statistical modeling may not be necessary to support CDM because a proxy target for statistical modeling might do as well or better. This third observation helps to explain at least one broad common CDM practice that seems wrong at first blush: the widespread use of non-causal models for targeting interventions. The last two implications are particularly important in practice, as acquiring (unconfounded) data on all counterfactuals can be costly and often impracticable. These observations open substantial research ground. We hope to facilitate research in this area by pointing to related articles from multiple contributing fields, including two dozen articles published the last three to four years.",0
"The concept of causality has been central to statistics since before the field was formalized by Sir Francis Galton in the late 19th century. Indeed, identifying whether some phenomenon caused another has been one of the primary goals of statistical inference from its beginning. Many methods have been developed over time to assess causal relationships, such as experimental designs and randomization tests. However, modern machine learning approaches have added new perspectives on causality that challenge traditional inferential frameworks. In particular, decision trees, which involve selecting variables based solely on their association with outcomes rather than theoretical mechanisms, can produce accurate predictions even without assumptions about causality. As a result, recent years have seen increasing efforts towards ""causal decision making,"" in which models focus more on predicting decisions made under specific interventions. This paradigm raises questions such as whether certainty must stem from knowledge of counterfactuals (what would happen if we intervened) vs. associations alone, and how model uncertainty should relate to policy uncertainty.",1
"This paper summarizes our endeavors in the past few years in terms of explaining image classifiers, with the aim of including negative results and insights we have gained. The paper starts with describing the explainable neural network (XNN), which attempts to extract and visualize several high-level concepts purely from the deep network, without relying on human linguistic concepts. This helps users understand network classifications that are less intuitive and substantially improves user performance on a difficult fine-grained classification task of discriminating among different species of seagulls.   Realizing that an important missing piece is a reliable heatmap visualization tool, we have developed I-GOS and iGOS++ utilizing integrated gradients to avoid local optima in heatmap generation, which improved the performance across all resolutions. During the development of those visualizations, we realized that for a significant number of images, the classifier has multiple different paths to reach a confident prediction. This has lead to our recent development of structured attention graphs (SAGs), an approach that utilizes beam search to locate multiple coarse heatmaps for a single image, and compactly visualizes a set of heatmaps by capturing how different combinations of image regions impact the confidence of a classifier.   Through the research process, we have learned much about insights in building deep network explanations, the existence and frequency of multiple explanations, and various tricks of the trade that make explanations work. In this paper, we attempt to share those insights and opinions with the readers with the hope that some of them will be informative for future researchers on explainable deep learning.",0
"This paper proposes a methodology to generate structured explanations from heat maps, which are commonly used as visual representations to show the activation patterns of image classifiers. Our approach bridges the gap between black box models and white box models by generating concise, yet comprehensive explanations that can guide human decision making. We first analyze the limitations of existing methods for explaining image classifier decisions, such as visualizations or rule sets. Then, we present our algorithmic framework, which consists of several steps: feature selection, explanation generation, and post-processing. Finally, we demonstrate the effectiveness of our method on different datasets and showcase how the generated explanations can assist humans in understanding and interpreting classification results. Overall, this work contributes towards more transparent and reliable artificial intelligence systems.",1
"Feature based local attribution methods are amongst the most prevalent in explainable artificial intelligence (XAI) literature. Going beyond standard correlation, recently, methods have been proposed that highlight what should be minimally sufficient to justify the classification of an input (viz. pertinent positives). While minimal sufficiency is an attractive property, the resulting explanations are often too sparse for a human to understand and evaluate the local behavior of the model, thus making it difficult to judge its overall quality. To overcome these limitations, we propose a novel method called Path-Sufficient Explanations Method (PSEM) that outputs a sequence of sufficient explanations for a given input of strictly decreasing size (or value) -- from original input to a minimally sufficient explanation -- which can be thought to trace the local boundary of the model in a smooth manner, thus providing better intuition about the local model behavior for the specific input. We validate these claims, both qualitatively and quantitatively, with experiments that show the benefit of PSEM across all three modalities (image, tabular and text). A user study depicts the strength of the method in communicating the local behavior, where (many) users are able to correctly determine the prediction made by a model.",0
"Abstract: Modern machine learning models often lack interpretability, making it difficult for practitioners and auditors to trust their outputs. In many cases, even if the model's behavior can be explained, these explanations may still leave questions unanswered or unclear. To address this issue, we propose path-sufficiency, a methodology that generates concise yet comprehensive visualizations of how each prediction comes from a unique combination of factors within a particular instance. We demonstrate our approach on several popular classifiers and showcase its efficacy through user studies. Finally, we discuss potential use cases of path-sufficient explanation generation as well as limitations. Ultimately, our work seeks to offer users better understanding into what drives the outcomes produced by modern predictive systems. Keywords: explainable AI, post hoc methods, feature attribution, Shapley values, LIME",1
"Deep neural networks (DNNs) have become a proven and indispensable machine learning tool. As a black-box model, it remains difficult to diagnose what aspects of the model's input drive the decisions of a DNN. In countless real-world domains, from legislation and law enforcement to healthcare, such diagnosis is essential to ensure that DNN decisions are driven by aspects appropriate in the context of its use. The development of methods and studies enabling the explanation of a DNN's decisions has thus blossomed into an active, broad area of research. A practitioner wanting to study explainable deep learning may be intimidated by the plethora of orthogonal directions the field has taken. This complexity is further exacerbated by competing definitions of what it means ``to explain'' the actions of a DNN and to evaluate an approach's ``ability to explain''. This article offers a field guide to explore the space of explainable deep learning aimed at those uninitiated in the field. The field guide: i) Introduces three simple dimensions defining the space of foundational methods that contribute to explainable deep learning, ii) discusses the evaluations for model explanations, iii) places explainability in the context of other related deep learning research areas, and iv) finally elaborates on user-oriented explanation designing and potential future directions on explainable deep learning. We hope the guide is used as an easy-to-digest starting point for those just embarking on research in this field.",0
"This field guide is intended to provide a concise yet comprehensive overview of the rapidly growing area of explainable deep learning (XDL). With the increasing adoption of artificial intelligence (AI) systems that rely on complex machine learning algorithms, there is an urgent need for methods that can interpret and explain their behavior. XDL addresses this gap by offering tools and techniques for making deep learning models more interpretable and transparent, thereby enabling users to better understand how these systems make decisions and predictions. In this guide, we first provide a brief introduction to deep learning and some common applications where XDL could play a role. We then present several important topics related to XDL such as the types of explanations required in different settings, evaluation metrics and methodologies for assessing the quality of generated explanations, approaches for inducing transparency into the model training process, and ways to debug DL models based on the insights derived from explanations. Finally, we discuss future research directions and challenges for developing practical solutions in this exciting new field.",1
"We propose a novel explanation method that explains the decisions of a deep neural network by investigating how the intermediate representations at each layer of the deep network were refined during the training process. This way we can a) find the most influential training examples during training and b) analyze which classes attributed most to the final representation. Our method is general: it can be wrapped around any iterative optimization procedure and covers a variety of neural network architectures, including feed-forward networks and convolutional neural networks. We first propose a method for stochastic training with single training instances, but continue to also derive a variant for the common mini-batch training. In experimental evaluations, we show that our method identifies highly representative training instances that can be used as an explanation. Additionally, we propose a visualization that provides explanations in the form of aggregated statistics over the whole training process.",0
"Machine learning models have become increasingly complex over recent years as they rely more heavily on deep learning representations. While these methods have been successful in many applications, understanding their inner workings remains challenging due to their opaque nature. In this paper we present a method that takes advantage of the fact that while training deep neural networks (DNNs), we can extract layer activations during each epoch. We show how these layer activations can reveal properties of DNN decision boundaries, both local linearity and curvature, without relying on second derivatives. Furthermore, our approach allows us to explain why specific features contribute positively/negatively to individual model decisions. Lastly, we demonstrate our framework using two case studies, image classification of MNIST and CIFAR-10 dataset respectively, where DNN classifiers achieve state-of-the art accuracy but lack transparency. Results showcase our explanation technique effectively visualizing important feature interactions relevant for correct predictions as well as poor generalization of certain DNN architectures across datasets. Our contributions provide insights into the behavior of trained DNNs, which will aid future efforts aimed at designing systems capable of providing transparency into machine learning decisions.",1
"Counterfactual explanations focus on ""actionable knowledge"" to help end-users understand how a machine learning outcome could be changed to a more desirable outcome. For this purpose a counterfactual explainer needs to discover input dependencies that relate to outcome changes. Identifying the minimum subset of feature changes needed to action an output change in the decision is an interesting challenge for counterfactual explainers. The DisCERN algorithm introduced in this paper is a case-based counter-factual explainer. Here counterfactuals are formed by replacing feature values from a nearest unlike neighbour (NUN) until an actionable change is observed. We show how widely adopted feature relevance-based explainers (i.e. LIME, SHAP), can inform DisCERN to identify the minimum subset of ""actionable features"". We demonstrate our DisCERN algorithm on five datasets in a comparative study with the widely used optimisation-based counterfactual approach DiCE. Our results demonstrate that DisCERN is an effective strategy to minimise actionable changes necessary to create good counterfactual explanations.",0
"This paper presents a novel approach for discovering counterfactual explanations based on relevance features extracted from neighbourhoods. In recent years, there has been growing interest in developing methods that can provide meaningful explanations for machine learning models, particularly those used in high-stakes decision making domains such as healthcare and finance. While several techniques have been proposed for generating explanations, most focus solely on identifying key factors driving model predictions without providing insights into how these factors may vary under different conditions. To address this shortcoming, we propose DisCERN, a method that utilizes relevance features derived from neighbourhood sampling around input instances to construct counterfactual scenarios reflecting plausible alternative outcomes given different assumptions. Our experiments demonstrate the effectiveness of our approach across multiple datasets, including both tabular data and images. We believe that DisCERN provides valuable insights into understanding how models make decisions by generating hypothetical situations that illustrate the impact of varying input characteristics on predicted outcomes, ultimately leading to more informed use of black-box prediction systems.",1
"Have you ever looked at a painting and wondered what is the story behind it? This work presents a framework to bring art closer to people by generating comprehensive descriptions of fine-art paintings. Generating informative descriptions for artworks, however, is extremely challenging, as it requires to 1) describe multiple aspects of the image such as its style, content, or composition, and 2) provide background and contextual knowledge about the artist, their influences, or the historical period. To address these challenges, we introduce a multi-topic and knowledgeable art description framework, which modules the generated sentences according to three artistic topics and, additionally, enhances each description with external knowledge. The framework is validated through an exhaustive analysis, both quantitative and qualitative, as well as a comparative human evaluation, demonstrating outstanding results in terms of both topic diversity and information veracity.",0
"Title: A Practical Approach to Knowledge-Aware Image Captioning Author: John Smith Publication: arXiv Submission ID: arXiv:2109.07484 Keywords: image captioning, knowledge distillation, multi-task learning Introduction In recent years, there has been significant progress in developing methods that can automatically generate natural language descriptions (i.e., captions) for images, which have numerous applications including but not limited to visual question answering, text summarization, and image retrieval. Despite their successes, existing approaches still suffer from several limitations, such as producing factually incorrect statements, failing to capture relevant contexts, or lacking diversity in generated responses. To address these issues, we present a novel approach called ""Knowledge Distilled Image Captioner"" (KDIC), which incorporates structured external knowledge into image caption generation. KDIC combines state-of-the-art neural network architectures trained using reinforcement learning techniques with external knowledge bases through a hierarchical knowledge distillation framework. By doing so, our model generates diverse and informative captions grounded in worldly facts across multiple domains while respecting the underlying structure of the input data. Our experiments demonstrate the effectiveness of our method by evaluating on three benchmark datasets, outperforming strong baselines while significantly reducing errors in semantic content and improving coherence and relevance aspects of generated descriptions. Overall, this work provides a step towards enabling more intelligent and responsible artificial intelligence systems by fostering explainability and veracity in their output responses.",1
"Federated learning (FL) is a popular technique to train machine learning (ML) models with decentralized data. Extensive works have studied the performance of the global model; however, it is still unclear how the training process affects the final test accuracy. Exacerbating this problem is the fact that FL executions differ significantly from traditional ML with heterogeneous data characteristics across clients, involving more hyperparameters. In this work, we show that the final test accuracy of FL is dramatically affected by the early phase of the training process, i.e., FL exhibits critical learning periods, in which small gradient errors can have irrecoverable impact on the final test accuracy. To further explain this phenomenon, we generalize the trace of the Fisher Information Matrix (FIM) to FL and define a new notion called FedFIM, a quantity reflecting the local curvature of each clients from the beginning of the training in FL. Our findings suggest that the {\em initial learning phase} plays a critical role in understanding the FL performance. This is in contrast to many existing works which generally do not connect the final accuracy of FL to the early phase training. Finally, seizing critical learning periods in FL is of independent interest and could be useful for other problems such as the choices of hyperparameters such as the number of client selected per round, batch size, and more, so as to improve the performance of FL training and testing.",0
"In recent years, federated learning has emerged as a powerful tool for distributed machine learning over networks. However, one of the key challenges faced by federated learning systems is ensuring that models converge quickly and accurately under real-world conditions. This paper focuses on identifying critical periods during which the model convergence can be significantly improved through carefully designed algorithmic modifications. We present empirical evidence showing that there exist specific time windows during the training process where small changes to the algorithm parameters have disproportionate effects on model accuracy. By exploiting these critical learning periods, we demonstrate significant improvements in convergence rates across various datasets and architectures. Our findings shed light on the underlying mechanisms driving federated learning dynamics and provide valuable insights for optimizing distributed machine learning algorithms.",1
"Post-hoc interpretation aims to explain a trained model and reveal how the model arrives at a decision. Though research on post-hoc interpretations has developed rapidly, one growing pain in this field is the difficulty in evaluating interpretations. There are some crucial logic traps behind existing evaluation methods, which are ignored by most works. In this opinion piece, we summarize four kinds evaluation methods and point out the corresponding logic traps behind them. We argue that we should be clear about these traps rather than ignore them and draw conclusions assertively.",0
"Title: ""Evaluating Post-Hoc Interpretations: Tackling Logical Fallacies in Explanatory Inferences""  Abstract: Interpreting experimental results after observing unexpected outcomes can lead researchers down logical paths that obscure insights into genuine causes. These interpretive pitfalls become more pervasive as data analysis becomes increasingly automated through machine learning algorithms, raising concerns over human oversight and understanding. This article investigates logical traps in evaluating post-hoc explanations using case studies from psychology experiments. Categorizing these fallacious reasoning patterns into three types (causal oversimplifications, disregarding alternative hypotheses, and circular arguments) allows us to identify them more effectively during interpretation processes. We discuss the potential consequences of these interpretive errors on scientific progress and suggest strategies for minimizing their occurrence. By acknowledging common cognitive biases that influence post-hoc reasoning, researchers can better evaluate evidence supporting conclusions and develop stronger, nuanced theories. Understanding these traps promotes critical thinking within experimentation, facilitating more accurate knowledge creation across diverse fields.",1
"The COVID-19 corona virus has claimed 4.1 million lives, as of July 24, 2021. A variety of machine learning models have been applied to related data to predict important factors such as the severity of the disease, infection rate and discover important prognostic factors. Often the usefulness of the findings from the use of these techniques is reduced due to lack of method interpretability. Some recent progress made on the interpretability of machine learning models has the potential to unravel more insights while using conventional machine learning models. In this work, we analyze COVID-19 blood work data with some of the popular machine learning models; then we employ state-of-the-art post-hoc local interpretability techniques(e.g.- SHAP, LIME), and global interpretability techniques(e.g. - symbolic metamodeling) to the trained black-box models to draw interpretable conclusions. In the gamut of machine learning algorithms, regressions remain one of the simplest and most explainable models with clear mathematical formulation. We explore one of the most recent techniques called symbolic metamodeling to find the mathematical expression of the machine learning models for COVID-19. We identify Acute Kidney Injury (AKI), initial Albumin level (ALBI), Aspartate aminotransferase (ASTI), Total Bilirubin initial(TBILI) and D-Dimer initial (DIMER) as major prognostic factors of the disease severity. Our contributions are- (i) uncover the underlying mathematical expression for the black-box models on COVID-19 severity prediction task (ii) we are the first to apply symbolic metamodeling to this task, and (iii) discover important features and feature interactions.",0
"Early results have shown that machine learning (ML) can aid healthcare practitioners by determining relevant prognostic factors from patient medical records related to COVID-19 using complex mathematical algorithms. While current ML approaches use â€˜black boxâ€™ models which lack explanations and interpretability, improved diagnostic accuracy has made it possible for rapid identification of risk groups so at-risk patients may receive timely treatment. In particular, physicians want explainability as well as interpretability, but there is no clear consensus on how they differ. This study bridges that gap by conducting research within two types of explanation methods: local and global interpretation strategies, examined in terms of both qualitative evaluations and quantitative analysis of results through logistic regression tests. By doing so, the authors propose an innovative approach that combines feature importance scores calculated locally and globally into one single score. With the global method focusing on modelâ€™s output importance over all instances instead of per instance feature contribution while the local method calculates these contributions only locally thus providing interpretable individualized diagnosis assessments. Ultimately, our findings suggest a combination of the two provides benefits such as higher sensitivity than other global and local specific evaluation metrics across different datasets. Thus, providing better patient care for hospital resources as well as improving understanding for future generations of clinician specialists.",1
"Hybrid modeling, the combination of first principle and machine learning models, is an emerging research field that gathers more and more attention. Even if hybrid models produce formidable results for academic examples, there are still different technical challenges that hinder the use of hybrid modeling in real-world applications. By presenting NeuralFMUs, the fusion of a FMU, a numerical ODE solver and an ANN, we are paving the way for the use of a variety of first principle models from different modeling tools as parts of hybrid models. This contribution handles the hybrid modeling of a complex, real-world example: Starting with a simplified 1D-fluid model of the human cardiovascular system (arterial side), the aim is to learn neglected physical effects like arterial elasticity from data. We will show that the hybrid modeling process is more comfortable, needs less system knowledge and is therefore less error-prone compared to modeling solely based on first principle. Further, the resulting hybrid model has improved in computation performance, compared to a pure first principle white-box model, while still fulfilling the requirements regarding accuracy of the considered hemodynamic quantities. The use of the presented techniques is explained in a general manner and the considered use-case can serve as example for other modeling and simulation applications in and beyond the medical domain.",0
"In recent years, neural networks have gained popularity as models for biological systems due to their ability to capture complex nonlinear relationships and learn from limited data sets. Here we present a novel hybrid approach combining a physics-based cardiovascular simulator called NeuralFMU (neural functional MockUp) with a deep convolutional neural network to improve accuracy and efficiency in predicting system behavior. We demonstrate that our method outperforms traditional approaches by reducing errors up to threefold while improving computational performance compared to pure data-driven models. Furthermore, we show how our hybrid approach can provide insights into physiologically relevant parameters such as endothelial function and regional pressure drops, which would otherwise remain unobtainable. Finally, we highlight potential clinical applications of our work including personalized treatment planning in patients suffering from pulmonary arterial hypertension and other cardiovascular diseases. Our findings pave the way towards a new era of virtual testing platforms, enabling reliable predictions across different scales ranging from organs to the whole body.",1
"Compositional data are non-negative data collected in a rectangular matrix with a constant row sum. Due to the non-negativity the focus is on conditional proportions that add up to 1 for each row. A row of conditional proportions is called an observed budget. Latent budget analysis (LBA) assumes a mixture of latent budgets that explains the observed budgets. LBA is usually fitted to a contingency table, where the rows are levels of one or more explanatory variables and the columns the levels of a response variable. In prospective studies, there is only knowledge about the explanatory variables of individuals and interest goes out to predicting the response variable. Thus, a form of LBA is needed that has the functionality of prediction. Previous studies proposed a constrained neural network (NN) extension of LBA that was hampered by an unsatisfying prediction ability. Here we propose LBA-NN, a feed forward NN model that yields a similar interpretation to LBA but equips LBA with a better ability of prediction. A stable and plausible interpretation of LBA-NN is obtained through the use of importance plots and table, that show the relative importance of all explanatory variables on the response variable. An LBA-NN-K- means approach that applies K-means clustering on the importance table is used to produce K clusters that are comparable to K latent budgets in LBA. Here we provide different experiments where LBA-NN is implemented and compared with LBA. In our analysis, LBA-NN outperforms LBA in prediction in terms of accuracy, specificity, recall and mean square error. We provide open-source software at GitHub.",0
"In recent years, there has been increasing interest in using neural networks for latent budget analysis (LBA) of compositional data. LBA is a powerful tool that allows researchers to analyze how different components of a system interact and depend on each other. However, traditional methods of LBA can be limited in their ability to handle complex systems and large amounts of data. This paper proposes a novel approach to LBA using artificial intelligence, specifically deep learning techniques such as Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and Generative Adversarial Networks (GAN). These models have shown promising results in handling high-dimensional datasets and generating meaningful insights into complex systems. By leveraging these advanced algorithms, we demonstrate improved accuracy and flexibility in analyzing compositional data, allowing for better decision making in fields ranging from finance and economics to environmental science and healthcare. Our methodology outperforms traditional linear regression models, validating the effectiveness of our proposed approach. We conclude by discussing potential future applications of AI for LBA and highlighting opportunities for further improvement in this exciting field.",1
"Most of existing statistical theories on deep neural networks have sample complexities cursed by the data dimension and therefore cannot well explain the empirical success of deep learning on high-dimensional data. To bridge this gap, we propose to exploit low-dimensional geometric structures of the real world data sets. We establish theoretical guarantees of convolutional residual networks (ConvResNet) in terms of function approximation and statistical estimation for binary classification. Specifically, given the data lying on a $d$-dimensional manifold isometrically embedded in $\mathbb{R}^D$, we prove that if the network architecture is properly chosen, ConvResNets can (1) approximate Besov functions on manifolds with arbitrary accuracy, and (2) learn a classifier by minimizing the empirical logistic risk, which gives an excess risk in the order of $n^{-\frac{s}{2s+2(s\vee d)}}$, where $s$ is a smoothness parameter. This implies that the sample complexity depends on the intrinsic dimension $d$, instead of the data dimension $D$. Our results demonstrate that ConvResNets are adaptive to low-dimensional structures of data sets.",0
"This paper provides methods for solving problems in mathematical space using convolutional residual networks (CRN) trained for function approximation and binary classification tasks. We analyze two scenarios where CRN may effectively perform Besov function approximation: near low-dimensional manifolds; and high frequency data sampled from them, including images and time series data on grids with large sampling rates. CRN are found capable of approximating functions that possess sufficient smoothness properties as defined by Besov spaces. Simulations show significant improvement over classical feedforward neural nets (FNN), even in cases involving nonlinearly structured input variables in one dimension. Furthermore, our results suggest advantages of CRN training from random initial weights vs. zero initialization for both scenarios. In conclusion, these simulations offer promising evidence to apply CRN algorithms to numerous challenges in applied mathematics beyond image/time series processing.",1
"Temporal grounding aims to predict a time interval of a video clip corresponding to a natural language query input. In this work, we present EVOQUER, a temporal grounding framework incorporating an existing text-to-video grounding model and a video-assisted query generation network. Given a query and an untrimmed video, the temporal grounding model predicts the target interval, and the predicted video clip is fed into a video translation task by generating a simplified version of the input query. EVOQUER forms closed-loop learning by incorporating loss functions from both temporal grounding and query generation serving as feedback. Our experiments on two widely used datasets, Charades-STA and ActivityNet, show that EVOQUER achieves promising improvements by 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could facilitate error analysis by explaining temporal grounding model behavior.",0
"This paper proposes a new method for enhancing temporal grounding in natural language processing tasks using video backquery generation. Existing methods rely on temporal annotations provided by human annotators, which can be costly and limited in scope. Our approach, called Evoquer (Enhancing Temporal Grounding with Video Pivoted BackQuery Generation), leverages existing video resources to generate pivots for generating temporal backqueries. These queries are used to improve the accuracy of temporally grounded responses generated by state-of-the-art NLP models. We demonstrate the effectiveness of our method through extensive experiments on two popular datasets, VQA (Visual Question Answering) and TGIF Chat. The results show that our proposed method significantly outperforms baseline approaches across multiple metrics, including accuracy, F1 score, and human evaluation. Overall, our work shows promise in addressing the challenge of temporal grounding without relying solely on human annotations, opening up new opportunities for research in NLP and related fields.",1
"We aim to explain a black-box classifier with the form: `data X is classified as class Y because X \textit{has} A, B and \textit{does not have} C' in which A, B, and C are high-level concepts. The challenge is that we have to discover in an unsupervised manner a set of concepts, i.e., A, B and C, that is useful for the explaining the classifier. We first introduce a structural generative model that is suitable to express and discover such concepts. We then propose a learning process that simultaneously learns the data distribution and encourages certain concepts to have a large causal influence on the classifier output. Our method also allows easy integration of user's prior knowledge to induce high interpretability of concepts. Using multiple datasets, we demonstrate that our method can discover useful binary concepts for explanation.",0
"In recent years, there has been growing interest in developing methods for explaining the behavior of black-box models such as deep neural networks (DNNs). One approach that has gained popularity is identifying and analyzing the underlying causal relationships between input features and model predictions. However, existing techniques often rely on supervision or domain knowledge, which can limit their applicability in practice.  In this work, we propose a novel unsupervised method for discovering binary concepts that explain DNN decisions without relying on labeled data or prior knowledge of the problem domain. Our approach leverages variational autoencoders (VAEs) to learn a continuous latent representation of the input data, from which we extract binary masks that highlight regions most relevant for prediction. We then evaluate these masks using both quantitative metrics and human judgment to assess their effectiveness at capturing meaningful explanations.  Experimental results on benchmark datasets demonstrate the ability of our method to identify interpretable concepts that provide insight into the decision process of complex DNNs. Furthermore, comparison against state-of-the-art baselines indicates the competitiveness of our approach in terms of explanation quality and computational efficiency. Overall, our study contributes new ideas and approaches towards improving transparency and interpretability of black-box models in real-world applications.",1
"Influence functions approximate the ""influences"" of training data-points for test predictions and have a wide variety of applications. Despite the popularity, their computational cost does not scale well with model and training data size. We present FastIF, a set of simple modifications to influence functions that significantly improves their run-time. We use k-Nearest Neighbors (kNN) to narrow the search space down to a subset of good candidate data points, identify the configurations that best balance the speed-quality trade-off in estimating the inverse Hessian-vector product, and introduce a fast parallel variant. Our proposed method achieves about 80X speedup while being highly correlated with the original influence values. With the availability of the fast influence functions, we demonstrate their usefulness in four applications. First, we examine whether influential data-points can ""explain"" test time behavior using the framework of simulatability. Second, we visualize the influence interactions between training and test data-points. Third, we show that we can correct model errors by additional fine-tuning on certain influential data-points, improving the accuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we experiment with a similar setup but fine-tuning on datapoints not seen during training, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI datasets respectively. Overall, our fast influence functions can be efficiently applied to large models and datasets, and our experiments demonstrate the potential of influence functions in model interpretation and correcting model errors. Code is available at https://github.com/salesforce/fast-influence-functions",0
"Abstract:  This paper presents FastIF (Scalable Influence Functions), a novel methodology that enables efficient model interpretation and debugging by leveraging influence functions. Existing methods require expensive gradient calculations for each input instance, making them computationally prohibitive. By contrast, FastIF computes and stores gradients only once during training, enabling significantly faster inference at test time. Our framework generalizes three popular types of sensitivity analysis (SHAP values, DeepLIFT weights, and LRP explanations) as special cases and provides improved accuracy over these baseline methods. We demonstrate the efficiency and effectiveness of our approach on several large datasets across image classification, natural language processing, and tabular data applications. This work paves the way towards scalable interpretability solutions for complex machine learning models.",1
"Many machine learning and data science tasks require solving non-convex optimization problems. When the loss function is a sum of multiple terms, a popular method is the stochastic gradient descent. Viewed as a process for sampling the loss function landscape, the stochastic gradient descent is known to prefer flat minima. Though this is desired for certain optimization problems such as in deep learning, it causes issues when the goal is to find the global minimum, especially if the global minimum resides in a sharp valley.   Illustrated with a simple motivating example, we show that the fundamental reason is that the difference in the Lipschitz constants of multiple terms in the loss function causes stochastic gradient descent to experience different variances at different minima. In order to mitigate this effect and perform faithful optimization, we propose a combined resampling-reweighting scheme to balance the variance at local minima and extend to general loss functions. We explain from the numerical stability perspective how the proposed scheme is more likely to select the true global minimum, and the local convergence analysis perspective how it converges to a minimum faster when compared with the vanilla stochastic gradient descent. Experiments from robust statistics and computational chemistry are provided to demonstrate the theoretical findings.",0
"Combining Resampling and Weighting for Faithful Stochastic Optimization  This paper presents a novel framework that integrates resampling and weighting techniques to improve the accuracy of stochastic gradient descent (SGD) algorithms. Our approach adaptively adjusts both the data samples used to compute gradients and their corresponding weights based on the estimated quality of each sample. This allows us to effectively handle cases where some datapoints have more informative gradients than others.  We first introduce a variance-reduced SGD algorithm that uses mini-batches of selected data points to achieve faster convergence rates. Each iteration selects a random subset of the entire dataset with replacement using importance sampling. By computing the importance weights for each data point, we can adaptively balance different subsets according to their individual quality.  Next, we extend our method by incorporating weighted averaging within each epoch, which further enhances the computational efficiency of our method. In particular, we show how to choose appropriate hyperparameters that minimize the expected global error over time. Experimental results demonstrate significant improvements over state-of-the-art methods across a range of deep learning tasks, including image classification, natural language processing, and reinforcement learning. Overall, our combined resampling and weighting technique achieves robust performance while requiring minimal tuning of model parameters or architectures.",1
"The remarkable success of deep neural networks (DNNs) in various applications is accompanied by a significant increase in network parameters and arithmetic operations. Such increases in memory and computational demands make deep learning prohibitive for resource-constrained hardware platforms such as mobile devices. Recent efforts aim to reduce these overheads, while preserving model performance as much as possible, and include parameter reduction techniques, parameter quantization, and lossless compression techniques.   In this chapter, we develop and describe a novel quantization paradigm for DNNs: Our method leverages concepts of explainable AI (XAI) and concepts of information theory: Instead of assigning weight values based on their distances to the quantization clusters, the assignment function additionally considers weight relevances obtained from Layer-wise Relevance Propagation (LRP) and the information content of the clusters (entropy optimization). The ultimate goal is to preserve the most relevant weights in quantization clusters of highest information content.   Experimental results show that this novel Entropy-Constrained and XAI-adjusted Quantization (ECQ$^{\text{x}}$) method generates ultra low-precision (2-5 bit) and simultaneously sparse neural networks while maintaining or even improving model performance. Due to reduced parameter precision and high number of zero-elements, the rendered networks are highly compressible in terms of file size, up to $103\times$ compared to the full-precision unquantized DNN model. Our approach was evaluated on different types of models and datasets (including Google Speech Commands and CIFAR-10) and compared with previous work.",0
"Deep neural networks (DNNs) have achieved great success in many applications, but their large memory footprint and computational requirements can limit their deployment on resource-constrained devices such as smartphones or embedded systems. Quantization techniques that reduce precision and increase efficiency have become popular solutions to address these challenges. However, existing quantization methods tend to prioritize bit reduction over explainability, resulting in models that are hard to interpret and debug, especially at low bitwidth settings. This work proposes ECQ$, a novel method designed specifically to balance model accuracy, inference speed, memory usage, and human explainability for deep learning in low-bit and sparse domains. Our approach applies gradient-based pruning to identify important weights during training alongside mixed-precision quantization and activation normalization. We compare our results against state-of-the-art baselines using benchmark datasets across different scenarios, demonstrating improved tradeoffs among quantization bits, sparsity levels, and classification performance. Overall, our contributions emphasize the significance of explaining AI decisions even when deploying lightweight DNN models under tight hardware constraints.",1
"Temporal knowledge graph (TKG) reasoning is a crucial task that has gained increasing research interest in recent years. Most existing methods focus on reasoning at past timestamps to complete the missing facts, and there are only a few works of reasoning on known TKGs to forecast future facts. Compared with the completion task, the forecasting task is more difficult that faces two main challenges: (1) how to effectively model the time information to handle future timestamps? (2) how to make inductive inference to handle previously unseen entities that emerge over time? To address these challenges, we propose the first reinforcement learning method for forecasting. Specifically, the agent travels on historical knowledge graph snapshots to search for the answer. Our method defines a relative time encoding function to capture the timespan information, and we design a novel time-shaped reward based on Dirichlet distribution to guide the model learning. Furthermore, we propose a novel representation method for unseen entities to improve the inductive inference ability of the model. We evaluate our method for this link prediction task at future timestamps. Extensive experiments on four benchmark datasets demonstrate substantial performance improvement meanwhile with higher explainability, less calculation, and fewer parameters when compared with existing state-of-the-art methods.",0
"Title: ""TimeTraveler: An Empirical Study on Predicting Entity Attributes""  As humans we have been fascinated by time travel since ancient times. In todayâ€™s world, our digital memory has evolved into knowledge graphs that store information about entities and their attributes over time, but predicting future attribute values remains challenging. We propose â€œTime Travellerâ€; a reinforcement learning approach that leverages graph convolution networks (GCN) to make temporally aware predictions. Our method performs better than state-of-the-art methods, validating our hypothesis: modeling temporal dependencies through GCNs and RL yields high quality predictions. Experiments showcase improvements up to 26% over benchmarks; promising further applications such as fraud detection systems, recommender engines etc.",1
"Bayesian optimization (BO) is an approach to globally optimizing black-box objective functions that are expensive to evaluate. BO-powered experimental design has found wide application in materials science, chemistry, experimental physics, drug development, etc. This work aims to bring attention to the benefits of applying BO in designing experiments and to provide a BO manual, covering both methodology and software, for the convenience of anyone who wants to apply or learn BO. In particular, we briefly explain the BO technique, review all the applications of BO in additive manufacturing, compare and exemplify the features of different open BO libraries, unlock new potential applications of BO to other types of data (e.g., preferential output). This article is aimed at readers with some understanding of Bayesian methods, but not necessarily with knowledge of additive manufacturing; the software performance overview and implementation instructions are instrumental for any experimental-design practitioner. Moreover, our review in the field of additive manufacturing highlights the current knowledge and technological trends of BO.",0
"Bayesian optimization has emerged as a powerful approach for solving complex problems by iteratively designing experiments that minimize a cost function. In recent years, sequential experimental design (SED) has gained popularity due to its ability to handle high dimensional search spaces and noisy objective functions. This paper presents a novel framework that combines Bayesian optimization with SED for more efficient problem-solving in additive manufacturing applications. The proposed method uses a probabilistic model to predict the outcome of each experiment based on past observations and iteratively designs new experiments that maximize the expected improvement in the objective function. Experiments conducted using multi-material three-dimensional printing demonstrate the effectiveness of our method compared to existing approaches. Our results show significant improvements in efficiency and accuracy across different application scenarios.",1
"In this paper, we introduce the \textit{Layer-Peeled Model}, a nonconvex yet analytically tractable optimization program, in a quest to better understand deep neural networks that are trained for a sufficiently long time. As the name suggests, this new model is derived by isolating the topmost layer from the remainder of the neural network, followed by imposing certain constraints separately on the two parts of the network. We demonstrate that the Layer-Peeled Model, albeit simple, inherits many characteristics of well-trained neural networks, thereby offering an effective tool for explaining and predicting common empirical patterns of deep learning training. First, when working on class-balanced datasets, we prove that any solution to this model forms a simplex equiangular tight frame, which in part explains the recently discovered phenomenon of neural collapse \cite{papyan2020prevalence}. More importantly, when moving to the imbalanced case, our analysis of the Layer-Peeled Model reveals a hitherto unknown phenomenon that we term \textit{Minority Collapse}, which fundamentally limits the performance of deep learning models on the minority classes. In addition, we use the Layer-Peeled Model to gain insights into how to mitigate Minority Collapse. Interestingly, this phenomenon is first predicted by the Layer-Peeled Model before being confirmed by our computational experiments.",0
"As deep neural networks continue to be a popular choice in artificial intelligence applications, understanding how they work has become increasingly important. One approach that researchers have taken towards this goal is through layer peeling, which involves training a network without some of its layers in order to study their individual impact on performance. In our paper ""Exploring Deep Neural Networks via Layer-Peeled Models,"" we explore the behavior of minority classes as their presence decreases during training. We focus specifically on imbalanced data sets where one class greatly outnumbers another, such as those commonly used in computer vision tasks like object detection. Our results show that the decrease in representation for these minority classes can lead to significant dropouts or collapse in some cases, causing the model to perform poorly overall. This finding highlights the importance of addressing class imbalance in both training data selection and data augmentation techniques to improve modelsâ€™ robustness across multiple class distributions. We hope that our insights into this phenomenon will aid future researchers in designing more effective models that generalize well across diverse datasets. Ultimately, the layer peeled architecture serves as a powerful tool for investigating neural network behaviors, and our findings lay the foundation for further exploration of deep learning architectures under different scenarios.",1
"Decision trees are a popular choice of explainable model, but just like neural networks, they suffer from adversarial examples. Existing algorithms for fitting decision trees robust against adversarial examples are greedy heuristics and lack approximation guarantees. In this paper we propose ROCT, a collection of methods to train decision trees that are optimally robust against user-specified attack models. We show that the min-max optimization problem that arises in adversarial learning can be solved using a single minimization formulation for decision trees with 0-1 loss. We propose such formulations in Mixed-Integer Linear Programming and Maximum Satisfiability, which widely available solvers can optimize. We also present a method that determines the upper bound on adversarial accuracy for any model using bipartite matching. Our experimental results demonstrate that the existing heuristics achieve close to optimal scores while ROCT achieves state-of-the-art scores.",0
"Abstract: This paper presents a methodology for creating optimal classification trees against adversarial examples. Adversarial examples are inputs that have been deliberately constructed to cause misclassification by machine learning models, even if those models are highly accurate on non-adversarial examples from the same distribution. Our approach uses an algorithm based on evolutionary computation to optimize classification tree structures under threat modeling constraints. We evaluate our approach using standard datasets subjected to common attack types such as imperceptible perturbations. Results show significant improvements over random forest baselines across all metrics evaluated. Further analysis demonstrates the effectiveness of our proposed attacks and defenses at generating and mitigating adversarial examples respectively. Overall, these results demonstrate strong potential for application in security sensitive domains where robustness against adversaries must be guaranteed.",1
"Bayesian methods have become a popular way to incorporate prior knowledge and a notion of uncertainty into machine learning models. At the same time, the complexity of modern machine learning makes it challenging to comprehend a model's reasoning process, let alone express specific prior assumptions in a rigorous manner. While primarily interested in the former issue, recent developments intransparent machine learning could also broaden the range of prior information that we can provide to complex Bayesian models. Inspired by the idea of self-explaining models, we introduce a corresponding concept for variational GaussianProcesses. On the one hand, our contribution improves transparency for these types of models. More importantly though, our proposed self-explaining variational posterior distribution allows to incorporate both general prior knowledge about a target function as a whole and prior knowledge about the contribution of individual features.",0
"This sounds like an interesting topic! Hereâ€™s an example of an abstract: Title: Self-Explaining Variational Posterior Distributions for Gaussian Process Models Gaussian process (GP) regression has gained increasing popularity as a powerful tool for modeling complex nonlinear relationships between inputs and outputs. However, one challenge faced by practitioners using GP models is how to interpret their predictions and uncertainty estimates effectively. In particular, the posterior distribution over functions represented by a GP model can often be difficult to visualize or summarize intuitively. In this work, we propose a novel approach that addresses these challenges directly by learning self-explaining variational approximations of GP posteriors. Our method introduces additional latent variables into the inference problem that encode salient features of the underlying function. These features can then be used to construct interpretable summary statistics such as confidence intervals, anomaly scores, or counterfactual scenarios. We develop our ideas in a probabilistically principled setting based on Bayesian variational inference. We demonstrate through experiments on synthetic and real-world data sets that our algorithm reliably discovers meaningful feature representations and leads to better calibrated predictive uncertainties compared to traditional approaches relying solely on a fixed kernel choice. Furthermore, the learned explanatory features can provide valuable insights even for domain experts who may have little expertise in machine learning. Our contribution offers several benefits for users interested in making informed decisions under uncertainty. By equipping them with more informative interpretations of the prediction output, the introduced framework enhances the utility of GP methods across diverse applications, from healthcare diagnosis to robotics control, where decision transparency is essential. Overall, this article represents a step forward towards a broader goal of developing transparent models that generate not only accurate predictions but also actionable knowledge.",1
"Deep neural networks have shown their profound impact on achieving human level performance in visual saliency prediction. However, it is still unclear how they learn the task and what it means in terms of understanding human visual system. In this work, we develop a technique to derive explainable saliency models from their corresponding deep neural architecture based saliency models by applying human perception theories and the conventional concepts of saliency. This technique helps us understand the learning pattern of the deep network at its intermediate layers through their activation maps. Initially, we consider two state-of-the-art deep saliency models, namely UNISAL and MSI-Net for our interpretation. We use a set of biologically plausible log-gabor filters for identifying and reconstructing the activation maps of them using our explainable saliency model. The final saliency map is generated using these reconstructed activation maps. We also build our own deep saliency model named cross-concatenated multi-scale residual block based network (CMRNet) for saliency prediction. Then, we evaluate and compare the performance of the explainable models derived from UNISAL, MSI-Net and CMRNet on three benchmark datasets with other state-of-the-art methods. Hence, we propose that this approach of explainability can be applied to any deep visual saliency model for interpretation which makes it a generic one.",0
"As one would expect from any type of AI model that outputs some type of understanding (not just pattern recognition) as output, visual saliency models seek to explain why they produce their output. In many cases, the most straightforward explanation for how these models work is a deep neural network architecture like CNNs, which can provide accurate and efficient feature extraction. This architecture uses filters that slide over the image, computing convolutional features at each location by taking dot products with fixed-size patches extracted from the input images. By repeating multiple layers of such convolutional processing, high level semantic concepts such as object shape, texture or scene layout may emerge after successive nonlinear transformations. Since there is no guarantee that these filters converge on semantically meaningful structures, we need another component to learn what matters and focus attention accordingly, so that only informative features from different hierarchy levels contribute to object detection and segmentation: bottom up feedback modulates top down predictions through spatial pooling and normalization operations. However, since there are several ways you could implement these ideas, such as using fully connected instead of convolutional networks or other ways to predict fixations for evaluation purposes etc., it makes sense to first evaluate whether these components lead to better performance than simpler baselines. Our goal was therefore to derive insights into the capabilities and limitations of current architectures for deriving explanatory representations and use them transparently in applications where interpretability is key. To achieve this objective we performed a systematic comparison of two common types of computational principles underlying state-of-the-art systems - Convolutional Neural Networks (CNNs) and",1
"Understanding the predictions made by machine learning (ML) models and their potential biases remains a challenging and labor-intensive task that depends on the application, the dataset, and the specific model. We present Amazon SageMaker Clarify, an explainability feature for Amazon SageMaker that launched in December 2020, providing insights into data and ML models by identifying biases and explaining predictions. It is deeply integrated into Amazon SageMaker, a fully managed service that enables data scientists and developers to build, train, and deploy ML models at any scale. Clarify supports bias detection and feature importance computation across the ML lifecycle, during data preparation, model evaluation, and post-deployment monitoring. We outline the desiderata derived from customer input, the modular architecture, and the methodology for bias and explanation computations. Further, we describe the technical challenges encountered and the tradeoffs we had to make. For illustration, we discuss two customer use cases. We present our deployment results including qualitative customer feedback and a quantitative evaluation. Finally, we summarize lessons learned, and discuss best practices for the successful adoption of fairness and explanation tools in practice.",0
"In this paper we present Amazon SageMaker Clarify, a machine learning bias detection and explainability tool provided by AWS that helps developers identify potential biases in their datasets and models. We first provide a brief overview of the problem space, discussing how common biases can arise in machine learning systems and why detecting them is important. Next, we describe the main components of SageMaker Clarify, including the API used to interact with it, as well as several use cases illustrating its capabilities. Finally, we conclude by comparing SageMaker Clarify against other popular tools in this field, highlighting its unique features and limitations. Our hope is that this work provides practitioners and researchers alike with a better understanding of SageMaker Clarifyâ€™s strengths and weaknesses so they can make informed decisions when choosing tools for managing model risk and compliance.",1
"Machine learning models fit complex algorithms to arbitrarily large datasets. These algorithms are well-known to be high on performance and low on interpretability. We use interactive visualization of slices of predictor space to address the interpretability deficit; in effect opening up the black-box of machine learning algorithms, for the purpose of interrogating, explaining, validating and comparing model fits. Slices are specified directly through interaction, or using various touring algorithms designed to visit high-occupancy sections or regions where the model fits have interesting properties. The methods presented here are implemented in the R package \pkg{condvis2}.",0
"""This paper presents..."" can come after three dots: ... ------  Visualizing large datasets has become increasingly important as data continues to grow in size and complexity. One common approach to tackling this problem is through the use of interactive slice visualizations, which allow users to explore specific subsets of their data while still maintaining context about the larger dataset. In this paper, we extend this technique to the realm of machine learning by introducing interactive slice visualizations for exploring complex deep neural network models. Our approach allows users to interactively examine individual layers and neurons within these models, providing new insights into how they function and make predictions. We demonstrate the effectiveness of our method on two example applications, showing that it enables more informed model interpretability and comparison tasks. Overall, our work represents an important step towards making machine learning more accessible and intuitive for domain experts outside of computer science.",1
"Existing algorithms for explaining the output of image classifiers perform poorly on inputs where the object of interest is partially occluded. We present a novel, black-box algorithm for computing explanations that uses a principled approach based on causal theory. We have implemented the method in the DEEPCOVER tool. We obtain explanations that are much more accurate than those generated by the existing explanation tools on images with occlusions and observe a level of performance comparable to the state of the art when explaining images without occlusions.",0
"Image occlusions occur frequently due to factors such as object occlusions by other objects or camera movements in visual scenes. They often make it difficult or impossible to perceive important details from images, which can have negative impacts on downstream applications that rely on high-quality image representations, such as computer vision systems. This work proposes a method to generate explanations for occluded images using machine learning techniques to fill in missing information and restore images to their original state. Our approach involves training a deep neural network model on pairs of occluded and unoccluded images, allowing it to learn the patterns and relationships necessary to accurately predict occlusion masks and recover hidden features. We evaluate our method using standard metrics in the field of computer vision and demonstrate significant improvements over baseline models on several challenging datasets. Our results show that our proposed method outperforms existing approaches and provides accurate and reliable explanations for occluded images, making it a valuable tool for a variety of applications that require clear and complete image data.",1
"Previous versions of sparse principal component analysis (PCA) have presumed that the eigen-basis (a $p \times k$ matrix) is approximately sparse. We propose a method that presumes the $p \times k$ matrix becomes approximately sparse after a $k \times k$ rotation. The simplest version of the algorithm initializes with the leading $k$ principal components. Then, the principal components are rotated with an $k \times k$ orthogonal rotation to make them approximately sparse. Finally, soft-thresholding is applied to the rotated principal components. This approach differs from prior approaches because it uses an orthogonal rotation to approximate a sparse basis. One consequence is that a sparse component need not to be a leading eigenvector, but rather a mixture of them. In this way, we propose a new (rotated) basis for sparse PCA. In addition, our approach avoids ""deflation"" and multiple tuning parameters required for that. Our sparse PCA framework is versatile; for example, it extends naturally to a two-way analysis of a data matrix for simultaneous dimensionality reduction of rows and columns. We provide evidence showing that for the same level of sparsity, the proposed sparse PCA method is more stable and can explain more variance compared to alternative methods. Through three applications -- sparse coding of images, analysis of transcriptome sequencing data, and large-scale clustering of social networks, we demonstrate the modern usefulness of sparse PCA in exploring multivariate data.",0
"Here is some sample text that could serve as the basis for an abstract:  In recent years, there has been growing interest in sparse principal component analysis (SPCA), a method for identifying the most important features in high-dimensional datasets while simultaneously reducing their dimensionality. This approach has proven particularly valuable in fields such as bioinformatics, neuroscience, and finance, where data can often be prohibitively large and complex. While several methods have been proposed for SPCA over the past few decades, many of these suffer from limitations in terms of accuracy, computational cost, or interpretability. In this work, we propose a new method based on a novel penalty term that effectively regularizes the singular value decomposition (SVD) of the input matrix. Our approach is flexible enough to handle both row and column sparsity constraints, while offering significant improvements in performance compared to existing techniques. We demonstrate the effectiveness of our method through extensive experiments on real and synthetic datasets, showing improved reconstruction accuracy and faster computation times. Finally, we discuss potential applications of our framework in various domains, including image compression and feature selection, making our work broadly relevant across scientific disciplines.",1
"A variety of works in the literature strive to uncover the factors associated with survival behaviour. However, the computational tools to provide such information are global models designed to predict if or when a (survival) event will occur. When approaching the problem of explaining differences in survival behaviour, those approaches rely on (assumptions of) predictive features followed by risk stratification. In other words, they lack the ability to discover new information on factors related to survival. In contrast, we approach such a problem from the perspective of descriptive supervised pattern mining to discover local patterns associated with different survival behaviours. Hence, we introduce the EsmamDS algorithm: an Exceptional Model Mining framework to provide straightforward characterisations of subgroups presenting unusual survival models -- given by the Kaplan-Meier estimates. This work builds on the Esmam algorithm to address the problem of pattern redundancy and provide a more informative and diverse characterisation of survival behaviour.",0
Infer from what I want in the summary field below.,1
"Traffic accident anticipation aims to accurately and promptly predict the occurrence of a future accident from dashcam videos, which is vital for a safety-guaranteed self-driving system. To encourage an early and accurate decision, existing approaches typically focus on capturing the cues of spatial and temporal context before a future accident occurs. However, their decision-making lacks visual explanation and ignores the dynamic interaction with the environment. In this paper, we propose Deep ReInforced accident anticipation with Visual Explanation, named DRIVE. The method simulates both the bottom-up and top-down visual attention mechanism in a dashcam observation environment so that the decision from the proposed stochastic multi-task agent can be visually explained by attentive regions. Moreover, the proposed dense anticipation reward and sparse fixation reward are effective in training the DRIVE model with our improved reinforcement learning algorithm. Experimental results show that the DRIVE model achieves state-of-the-art performance on multiple real-world traffic accident datasets. Code and pre-trained model are available at \url{https://www.rit.edu/actionlab/drive}.",0
"Title: Deep Reinforced Accident Anticipation with Visual Explanation (DRIVE)  Abstract: Advanced driver assistance systems (ADAS) have become increasingly prevalent as a means to improve road safety by assisting drivers in making critical decisions during unpredictable events on the road. One key challenge facing ADAS systems today is anticipating potential accidents before they occur. This can prove challenging due to complex human behavior patterns that may lead to unexpected driving actions. In this paper, we present our approach to address this challenge using deep reinforcement learning techniques, combined with visual explanation methods to better explain system reasoning and decision making processes to both engineers and end users alike. Our proposed framework, called DRIVE, utilizes convolutional neural networks to learn from large amounts of video data captured from real driving scenarios. We introduce a novel reward structure that encourages safe interactions between vehicles while minimizing collisions and close calls. Through extensive experiments and evaluations conducted on two public datasets, we demonstrate that our method outperforms state-of-the-art accident prediction models by achieving higher accuracy rates while providing more intuitive explanations through generated heatmaps highlighting potentially dangerous regions. Our work represents a significant step towards enabling ADAS systems to make safer predictions and ultimately pave the way towards fully autonomous driving systems.",1
"One major drawback of state-of-the-art artificial intelligence is its lack of explainability. One approach to solve the problem is taking causality into account. Causal mechanisms can be described by structural causal models. In this work, we propose a method for estimating bivariate structural causal models using a combination of normalising flows applied to density estimation and variational Gaussian process regression for post-nonlinear models. It facilitates causal discovery, i.e. distinguishing cause and effect, by either the independence of cause and residual or a likelihood ratio test. Our method which estimates post-nonlinear models can better explain a variety of real-world cause-effect pairs than a simple additive noise model. Though it remains difficult to exploit this benefit regarding all pairs from the T\""ubingen benchmark database, we demonstrate that combining the additive noise model approach with our method significantly enhances causal discovery.",0
"Title: Structure Learning in Causality via Nonlinear Function Approximation using Gaussian Processes and Normalizing Flows  This paper addresses the problem of estimating bivariate structural causal models from observational data under likelihoods that take the form of normalising flows (NF). These NF-based densities have gained popularity due to their flexibility and capacity to model complex distributions, which makes them ideal candidates for modelling high-dimensional datasets commonly encountered in modern applications such as computer vision and natural language processing. In contrast to previous approaches based on kernel regression or deep neural networks, we propose a novel framework based on variational Gaussian processes (VGP) that leverages the strengths of GP methods while maintaining nonlinear function approximation capabilities comparable to those of NN architectures. We establish theoretical guarantees and demonstrate experimental results indicating VGP's effectiveness over other existing methods for structure learning tasks in causality. Notably, our approach outperforms state-of-the-art alternatives including DeepStructuredCausation and KERML, significantly reducing error rates on several benchmark datasets across varying scenarios and model capacities. Our study provides a promising direction towards broadening the scope of application domains where the causal inference paradigm can be effectively applied, paving the way for new research opportunities at the intersection of machine learning, statistics, and artificial intelligence theory.",1
"Quantum physics experiments produce interesting phenomena such as interference or entanglement, which is a core property of numerous future quantum technologies. The complex relationship between a quantum experiment's structure and its entanglement properties is essential to fundamental research in quantum optics but is difficult to intuitively understand. We present the first deep generative model of quantum optics experiments where a variational autoencoder (QOVAE) is trained on a dataset of experimental setups. In a series of computational experiments, we investigate the learned representation of the QOVAE and its internal understanding of the quantum optics world. We demonstrate that the QOVAE learns an intrepretable representation of quantum optics experiments and the relationship between experiment structure and entanglement. We show the QOVAE is able to generate novel experiments for highly entangled quantum states with specific distributions that match its training data. Importantly, we are able to fully interpret how the QOVAE structures its latent space, finding curious patterns that we can entirely explain in terms of quantum physics. The results demonstrate how we can successfully use and understand the internal representations of deep generative models in a complex scientific domain. The QOVAE and the insights from our investigations can be immediately applied to other physical systems throughout fundamental scientific research.",0
"This is a challenging but interesting problem that could have important applications. There has been great progress in understanding how entangled states can represent and store quantum information, but we still donâ€™t know enough about how these representations relate to real experiments. One promising approach is to use deep generative models (DGMs) which map inputs to outputs and reveal complex nonlinear relationships between variables. By training DGMs on experimental data, it may be possible to learn interpretable representations of entanglement which capture key features such as fidelity and concurrence. This would enable better control over entanglement properties and optimize devices for quantum computing. Ultimately, the goal of this research program is to develop general principles for learning representations of entanglement across different systems, thus paving the way for a new era of quantum technologies based on the design and engineering of tailored photonic structures.",1
"The rapid recent progress in machine learning (ML) has raised a number of scientific questions that challenge the longstanding dogma of the field. One of the most important riddles is the good empirical generalization of overparameterized models. Overparameterized models are excessively complex with respect to the size of the training dataset, which results in them perfectly fitting (i.e., interpolating) the training data, which is usually noisy. Such interpolation of noisy data is traditionally associated with detrimental overfitting, and yet a wide range of interpolating models -- from simple linear models to deep neural networks -- have recently been observed to generalize extremely well on fresh test data. Indeed, the recently discovered double descent phenomenon has revealed that highly overparameterized models often improve over the best underparameterized model in test performance.   Understanding learning in this overparameterized regime requires new theory and foundational empirical studies, even for the simplest case of the linear model. The underpinnings of this understanding have been laid in very recent analyses of overparameterized linear regression and related statistical learning tasks, which resulted in precise analytic characterizations of double descent. This paper provides a succinct overview of this emerging theory of overparameterized ML (henceforth abbreviated as TOPML) that explains these recent findings through a statistical signal processing perspective. We emphasize the unique aspects that define the TOPML research area as a subfield of modern ML theory and outline interesting open questions that remain.",0
"In recent years, there has been increasing interest in overparameterization in machine learning models, leading to questions regarding whether the bias-variance tradeoff still holds true in these scenarios. This paper provides a comprehensive review of the theory behind overparameterized learning, examining how traditional statistical methods may no longer hold up as models become more complex. The authors argue that the bias-variance tradeoff may no longer be relevant in modern modeling techniques such as deep learning, where the focus should instead shift towards understanding generalization performance and designing algorithms that can effectively leverage available data resources. By exploring the relationship between model complexity, capacity control, and training dynamics, this study highlights new challenges facing researchers working on high-dimensional machine learning models and offers suggestions for future research directions. Ultimately, this paper aims to shed light on this debate and provide insights into how we can improve our theoretical understanding of overparameterized machine learning algorithms.",1
"Representing human-made objects as a collection of base primitives has a long history in computer vision and reverse engineering. In the case of high-resolution point cloud scans, the challenge is to be able to detect both large primitives as well as those explaining the detailed parts. While the classical RANSAC approach requires case-specific parameter tuning, state-of-the-art networks are limited by memory consumption of their backbone modules such as PointNet++, and hence fail to detect the fine-scale primitives. We present Cascaded Primitive Fitting Networks (CPFN) that relies on an adaptive patch sampling network to assemble detection results of global and local primitive detection networks. As a key enabler, we present a merging formulation that dynamically aggregates the primitives across global and local scales. Our evaluation demonstrates that CPFN improves the state-of-the-art SPFN performance by 13-14% on high-resolution point cloud datasets and specifically improves the detection of fine-scale primitives by 20-22%.",0
"In recent years there has been growing interest in applying machine learning techniques to point cloud data, resulting in state-of-the-art methods capable of performing high quality semantic segmentation on point clouds captured by LiDAR sensors. However, one challenge facing these methods remains fitting geometric primitives such as cylinders, boxes, and spheres to raw point cloud data for applications like robotics, geometry reconstruction, and self driving cars. Existing approaches rely heavily on hand crafted features such as edge detection, curvature estimation, and clustering which can fail on challenging datasets. Additionally, current models lack the ability to capture fine details required for accurate surface meshing or subsequent motion planning tasks. This work proposes Cascaded Primitive Fitting Network (CPFN), a novel framework that learns to fit primitive shapes such as cuboids, cylinders, and spheres to unstructured pointclouds directly from raw point clouds without relying on any explicit feature extraction modules or prior knowledge of scene geometry. By doing so, our model captures detailed local geometries while still accurately predicting coarse object shape contextualized across large spatial regions relevant to downstream robotic manipulation tasks. Furthermore, we introduce a new training scheme combining two complementary loss functions guiding the network towards both accurate box locations and surface normals encouraging fine scale details essential for follow up mesh generation steps in robotics simulation pipelines. Our extensive evaluations including qualitative visualizations demonstrate CPFN significantly outperforms previous works generating more accurate results for both ground truth based metrics and real world scenarios involving sensor noise and clutter such as occlusions found under natural weather conditions. Finally, we envisi",1
"Access to high-quality data repositories and benchmarks have been instrumental in advancing the state of the art in many experimental research domains. While advanced analytics tasks over time series data have been gaining lots of attention, lack of such community resources severely limits scientific progress. In this paper, we present Exathlon, the first comprehensive public benchmark for explainable anomaly detection over high-dimensional time series data. Exathlon has been systematically constructed based on real data traces from repeated executions of large-scale stream processing jobs on an Apache Spark cluster. Some of these executions were intentionally disturbed by introducing instances of six different types of anomalous events (e.g., misbehaving inputs, resource contention, process failures). For each of the anomaly instances, ground truth labels for the root cause interval as well as those for the extended effect interval are provided, supporting the development and evaluation of a wide range of anomaly detection (AD) and explanation discovery (ED) tasks. We demonstrate the practical utility of Exathlon's dataset, evaluation methodology, and end-to-end data science pipeline design through an experimental study with three state-of-the-art AD and ED techniques.",0
"This work presents ExaThlon, a benchmark suite designed to facilitate research on explainable anomaly detection algorithms for time series data. Time series datasets present unique challenges due to their complexity and variability, making evaluation of new methods difficult without established standards. To address these gaps, we introduce the Exathlon benchmark suite, which includes both synthetic datasets with known ground truth and real-world datasets from diverse domains such as finance, healthcare, and computer systems. We describe five main tasks based on common use cases and design principles to ensure diversity and relevance, including imbalanced classes, concept drift, missing values, multiple anomalies, and correlated variables.  We demonstrate the utility of our benchmark by evaluating state-of-the-art unsupervised and semi-supervised methods using standard metrics and visualizations. Our results highlight strengths and limitations of current approaches, identifying opportunities for future research towards more accurate, interpretable, adaptive, and scalable solutions. Overall, our goal is to provide a solid foundation for rigorous comparisons, promote progress in explainable anomaly detection, and foster reproducibility through openly available code, documentation, and resources at http://exatlon.org/.",1
"The quality of training data has a huge impact on the efficiency, accuracy and complexity of machine learning tasks. Various tools and techniques are available that assess data quality with respect to general cleaning and profiling checks. However these techniques are not applicable to detect data issues in the context of machine learning tasks, like noisy labels, existence of overlapping classes etc. We attempt to re-look at the data quality issues in the context of building a machine learning pipeline and build a tool that can detect, explain and remediate issues in the data, and systematically and automatically capture all the changes applied to the data. We introduce the Data Quality Toolkit for machine learning as a library of some key quality metrics and relevant remediation techniques to analyze and enhance the readiness of structured training datasets for machine learning projects. The toolkit can reduce the turn-around times of data preparation pipelines and streamline the data quality assessment process. Our toolkit is publicly available via IBM API Hub [1] platform, any developer can assess the data quality using the IBM's Data Quality for AI apis [2]. Detailed tutorials are also available on IBM Learning Path [3].",0
"Assessing and improving the quality of datasets used for training machine learning models is essential to ensure their accuracy and reliability. However, manually evaluating large and complex datasets can be time-consuming and error-prone. This paper presents a toolkit that automates the process of assessing data quality and provides recommendations for improvement. The toolkit leverages state-of-the-art techniques from machine learning, statistics, and computer vision to analyze different aspects of dataset quality such as missing values, inconsistencies, duplicates, outliers, and correlations. Moreover, it integrates these analyses into a comprehensive scorecard that quantifies overall data quality and identifies areas requiring attention. Finally, the toolkit offers automatic remediation strategies for addressing identified issues and enhance the dataset's quality, thereby enabling more accurate predictions by machine learning algorithms. Evaluation on benchmark datasets demonstrates the effectiveness of our approach and highlights the practicality of the proposed toolkit. Overall, this research has significant implications for organizations seeking to improve their data management practices and strengthen their decision-making processes using machine learning models.",1
"Wildfires have increased in frequency and severity over the past two decades, especially in the Western United States. Beyond physical infrastructure damage caused by these wildfire events, researchers have increasingly identified harmful impacts of particulate matter generated by wildfire smoke on respiratory, cardiovascular, and cognitive health. This inference is difficult due to the spatial and temporal uncertainty regarding how much particulate matter is specifically attributable to wildfire smoke. One factor contributing to this challenge is the reliance on manually drawn smoke plume annotations, which are often noisy representations limited to the United States. This work uses deep convolutional neural networks to segment smoke plumes from geostationary satellite imagery. We compare the performance of predicted plume segmentations versus the noisy annotations using causal inference methods to estimate the amount of variation each explains in Environmental Protection Agency (EPA) measured surface level particulate matter 2.5um in diameter ($\textrm{PM}_{2.5}$).",0
"An important aspect of studying wildfires is understanding their impact on air quality through the analysis of smoke plumes. This research explores the use of geostationary satellite imagery as a tool for accurately segmenting these plumes. By leveraging high-resolution images captured by satellites that orbit at an altitude of approximately 36,000 kilometers, our method provides detailed information on the spatial distribution of smoke constituents. Our approach involves preprocessing the raw data to enhance image contrast and improve feature detection, followed by applying object-based classification techniques that take into account both spectral and textural properties. We evaluate the effectiveness of our method using ground-based measurements collected from monitoring stations, demonstrating its ability to reliably estimate pollutant concentrations within smoke plumes over short time intervals. The results of this study have significant implications for the development of advanced early warning systems for fire management, public health protection, and climate change studies. Overall, we demonstrate the potential of geostationary satellite imagery as a valuable resource for improving our understanding of wildfire smoke transport and dispersion patterns.",1
"As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on real-world datasets. In this work, we address this issue by releasing XAI-Bench: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate ground-truth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and across a variety of settings. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench.",0
"In recent years there has been growing interest in developing machine learning algorithms that can generate predictions which are not only accurate but also explainable. However, due to the lack of public datasets specifically designed for benchmarking these types of models, the field lacks standardized testing methods and reliable comparison metrics. This study presents three new synthetic benchmark datasets in order to address this gap and promote further research into explainable machine learning. We use these datasets to evaluate several state-of-the-art XAI (eXplainable Artificial Intelligence) models and provide a comprehensive evaluation comparing their performance on different benchmark tasks. Our results demonstrate the potential utility of these datasets as well as highlight key challenges facing the development of effective XAI systems. Overall, our work provides important insights for future developments within the field. This abstract describes a new set of synthetic benchmark datasets designed for evaluating machine learning algorithms in terms of both accuracy and explainability. Without standardized testing methods, the authors aimed to create a resource for scientific studies focused on improve transparency through machine learning algorithms. By applying these datasets to current artificial intelligence technology they sought to identify strengths and limitations while establishing valuable tools to encourage improved development throughout the industry. Ultimately, it's goal is to support continued advancements in transparent, high performing artificial intelligence systems. Abstract: Synthetic Datasets for Evaluating Explanatory Machine Learning Models",1
"Since training a large-scale backdoored model from scratch requires a large training dataset, several recent attacks have considered to inject backdoors into a trained clean model without altering model behaviors on the clean data. Previous work finds that backdoors can be injected into a trained clean model with Adversarial Weight Perturbation (AWP). Here AWPs refers to the variations of parameters that are small in backdoor learning. In this work, we observe an interesting phenomenon that the variations of parameters are always AWPs when tuning the trained clean model to inject backdoors. We further provide theoretical analysis to explain this phenomenon. We formulate the behavior of maintaining accuracy on clean data as the consistency of backdoored models, which includes both global consistency and instance-wise consistency. We extensively analyze the effects of AWPs on the consistency of backdoored models. In order to achieve better consistency, we propose a novel anchoring loss to anchor or freeze the model behaviors on the clean data, with a theoretical guarantee. Both the analytical and the empirical results validate the effectiveness of the anchoring loss in improving the consistency, especially the instance-wise consistency.",0
"Achieving consistent results when injecting backdoors into machine learning models can be challenging due to factors such as model architecture, training data, and optimization techniques. To address these issues, we propose using logit anchoring to improve consistency during the process of injection. Our approach involves utilizing clean data, which has no malicious intent, rather than relying solely on adversarial examples. This allows us to more accurately estimate the impact of our backdoor while minimizing unwanted behavior such as high confidence predictions on benign inputs or degradation of natural accuracy. We evaluate our method on multiple datasets and demonstrate that our technique significantly improves the success rate of backdoor injection by reducing error rates up to 48%. Furthermore, our approach outperforms state-of-the art methods while maintaining low detection rates. Overall, our work provides new insights into better ways to anchor the estimation of negative logits during backdoor insertion and leads to improved results.",1
"Despite the promise of Convolutional neural network (CNN) based classification models for histopathological images, it is infeasible to quantify its uncertainties. Moreover, CNNs may suffer from overfitting when the data is biased. We show that Bayesian-CNN can overcome these limitations by regularizing automatically and by quantifying the uncertainty. In addition, it can perform much better than the state-of-the-art transfer learning CNN by reducing the false negative and false positive by 11% and 7.7% respectively. We have developed a novel technique to utilize the uncertainties provided by the Bayesian-CNN that significantly improves the performance on a large fraction of the test data (about 6% improvement in accuracy on 77% of test data). Further, we provide a novel explanation for the uncertainty by projecting the data into a low dimensional space through a nonlinear dimensionality reduction technique. This dimensionality reduction enables interpretation of the test data through visualization and reveals the structure of the data in a low dimensional feature space. Besides, we modify the Bayesian--CNN by introducing a stochastic adaptive activation function. The modified Bayesian-CNN performs slightly better than Bayesian-CNN on all performance metrics and significantly reduces the number of false negatives and false positives (3% reduction for both). This work shows the advantages of Bayesian-CNN against the state-of-the-art, explains and utilizes the uncertainties for histopathological images. It should find applications in various medical image classifications.",0
"This paper presents a method for using uncertainty estimates obtained from Bayesian neural network classifiers for breast histopathology images. Traditional machine learning approaches often struggle to provide reliable confidence measures alongside predictions, which can limit their utility in medical applications where treatment decisions must balance potential risks against uncertain benefits. In contrast, Bayesian models explicitly account for uncertainty throughout the inference process, enabling the generation of posterior distributions over model parameters that capture varying degrees of confidence in any given prediction. Our work shows how these distributions can be used effectively for tasks such as cancer detection and diagnosis in pathological slides. We demonstrate that our approach leads to improved performance compared to traditional methods on several standard benchmark datasets. Importantly, we emphasize the clinical implications of incorporating uncertainty into diagnostic decision making and discuss ways in which these insights might translate into improved patient outcomes. Overall, our results highlight the promise of Bayesian deep learning methods for addressing real world challenges in healthcare technology while respecting the limits imposed by limited data availability and other constraints commonly encountered in practice.",1
"Recently, road scene-graph representations used in conjunction with graph learning techniques have been shown to outperform state-of-the-art deep learning techniques in tasks including action classification, risk assessment, and collision prediction. To enable the exploration of applications of road scene-graph representations, we introduce roadscene2vec: an open-source tool for extracting and embedding road scene-graphs. The goal of roadscene2vec is to enable research into the applications and capabilities of road scene-graphs by providing tools for generating scene-graphs, graph learning models to generate spatio-temporal scene-graph embeddings, and tools for visualizing and analyzing scene-graph-based methodologies. The capabilities of roadscene2vec include (i) customized scene-graph generation from either video clips or data from the CARLA simulator, (ii) multiple configurable spatio-temporal graph embedding models and baseline CNN-based models, (iii) built-in functionality for using graph and sequence embeddings for risk assessment and collision prediction applications, (iv) tools for evaluating transfer learning, and (v) utilities for visualizing scene-graphs and analyzing the explainability of graph learning models. We demonstrate the utility of roadscene2vec for these use cases with experimental results and qualitative evaluations for both graph learning models and CNN-based models. roadscene2vec is available at https://github.com/AICPS/roadscene2vec.",0
"Artificial intelligence (AI) has made significant progress in recent years due to advances in deep learning techniques such as convolutional neural networks (CNNs). One area where CNNs have been particularly successful is computer vision tasks such as object detection and image segmentation. In many cases, these models require large amounts of data and powerful hardware to achieve state-of-the-art performance, making them impractical for use on resource constrained devices such as smartphones or embedded systems. To address this issue, we present a tool called roadscene2vec that allows users to extract and embed detailed scene graphs from road scenes using off-the-shelf CNN models. Scene graphs represent visual scenes as a graph structure consisting of objects, relationships and attributes, which can be used for downstream applications such as autonomous driving or augmented reality. We demonstrate the effectiveness of our approach by performing experiments on real world datasets and show that our method outperforms other baseline methods while maintaining high computational efficiency. Our framework provides an easy-to-use interface for researchers and developers interested in developing novel applications leveraging advanced scene understanding capabilities.",1
"In recent years, deep learning has become prevalent to solve applications from multiple domains. Convolutional Neural Networks (CNNs) particularly have demonstrated state of the art performance for the task of image classification. However, the decisions made by these networks are not transparent and cannot be directly interpreted by a human. Several approaches have been proposed to explain to understand the reasoning behind a prediction made by a network. In this paper, we propose a topology of grouping these methods based on their assumptions and implementations. We focus primarily on white box methods that leverage the information of the internal architecture of a network to explain its decision. Given the task of image classification and a trained CNN, this work aims to provide a comprehensive and detailed overview of a set of methods that can be used to create explanation maps for a particular image, that assign an importance score to each pixel of the image based on its contribution to the decision of the network. We also propose a further classification of the white box methods based on their implementations to enable better comparisons and help researchers find methods best suited for different scenarios.",0
"Title: ""White Box Methods for Explanations of Convolutional Neural Networks in Image Classification Tasks""",1
"In this monograph, I introduce the basic concepts of Online Learning through a modern view of Online Convex Optimization. Here, online learning refers to the framework of regret minimization under worst-case assumptions. I present first-order and second-order algorithms for online learning with convex losses, in Euclidean and non-Euclidean settings. All the algorithms are clearly presented as instantiation of Online Mirror Descent or Follow-The-Regularized-Leader and their variants. Particular attention is given to the issue of tuning the parameters of the algorithms and learning in unbounded domains, through adaptive and parameter-free online learning algorithms. Non-convex losses are dealt through convex surrogate losses and through randomization. The bandit setting is also briefly discussed, touching on the problem of adversarial and stochastic multi-armed bandits. These notes do not require prior knowledge of convex analysis and all the required mathematical tools are rigorously explained. Moreover, all the proofs have been carefully chosen to be as simple and as short as possible.",0
"""Online learning has become increasingly popular over recent years as more individuals seek flexible options for continuing education."" This sentence sets up the topic of online learning, establishes that it has grown in popularity, and suggests there is interest in it. We could expand on each one of these ideas slightly: Why might online learning have gained traction? Who wants or needs flexibility in their studies? And is that something new - have we seen changes in preferences and expectations from students in recent years? Then transition into some specifics of how online courses work. For example: they often provide pre-recorded lectures (what else should we mention here?) but nowadays may incorporate interactive elements like quizzes and virtual office hours. Lastly, we want to suggest potential benefits such as convenience, increased accessibility to course materials and personalized pacing. By focusing on these different aspects, I think we can put together a well rounded abstract for our paper. Let me know if you think differently!",1
"We present Gradient Activation Maps (GAM) - a machinery for explaining predictions made by visual similarity and classification models. By gleaning localized gradient and activation information from multiple network layers, GAM offers improved visual explanations, when compared to existing alternatives. The algorithmic advantages of GAM are explained in detail, and validated empirically, where it is shown that GAM outperforms its alternatives across various tasks and datasets.",0
"This research paper presents a novel methodology for visual similarity and classification using gradient activation maps (GAM). GAMs provide insight into how deep neural networks make predictions by highlighting which parts of an image contribute most to the networkâ€™s decision. By analyzing these maps, we can gain a better understanding of how models perceive images and learn to classify them accurately. Our approach extends previous work on GAMs by introducing several improvements that enhance their interpretability, utility, and robustness across different architectures and datasets. We validate our methods through extensive experiments, demonstrating significant gains over state-of-the-art techniques in both accuracy and transparency. Overall, this study represents a step forward in the field of explainable artificial intelligence, as well as a valuable tool for researchers and practitioners alike working with computer vision problems.",1
"Deep neural networks have become the default choice for many applications like image and video recognition, segmentation and other image and video related tasks.However, a critical challenge with these models is the lack of explainability.This requirement of generating explainable predictions has motivated the research community to perform various analysis on trained models.In this study, we analyze the learned feature maps of trained models using MNIST images for achieving more explainable predictions.Our study is focused on deriving a set of primitive elements, here called visual concepts, that can be used to generate any arbitrary sample from the data generating distribution.We derive the primitive elements from the feature maps learned by the model.We illustrate the idea by generating visual concepts from a Variational Autoencoder trained using MNIST images.We augment the training data of MNIST dataset by adding about 60,000 new images generated with visual concepts chosen at random.With this we were able to reduce the reconstruction loss (mean square error) from an initial value of 120 without augmentation to 60 with augmentation.Our approach is a first step towards the final goal of achieving trained deep neural network models whose predictions, features in hidden layers and the learned filters can be well explained.Such a model when deployed in production can easily be modified to adapt to new data, whereas existing deep learning models need a re training or fine tuning. This process again needs a huge number of data samples that are not easy to generate unless the model has good explainability.",0
"This paper presents a method for learning a vocabulary of visual concepts and operators that can be used to represent images and videos at different levels of abstraction. Our approach builds upon recent advances in deep neural networks, which have been shown to excel at tasks such as image classification and object detection. We demonstrate how these models can be adapted to learn a rich set of visual features and how they can be combined into hierarchical representations of increasing complexity. We evaluate our method on several benchmark datasets and show that it outperforms other state-of-the-art methods. Additionally, we provide insights into the learned representations by analyzing their properties and comparing them against human-defined features. Overall, our work represents a significant step towards developing powerful visual representation systems that can enable new applications in computer vision and beyond.",1
"Building embodied autonomous agents capable of participating in social interactions with humans is one of the main challenges in AI. Within the Deep Reinforcement Learning (DRL) field, this objective motivated multiple works on embodied language use. However, current approaches focus on language as a communication tool in very simplified and non-diverse social situations: the ""naturalness"" of language is reduced to the concept of high vocabulary size and variability. In this paper, we argue that aiming towards human-level AI requires a broader set of key social skills: 1) language use in complex and variable social contexts; 2) beyond language, complex embodied communication in multimodal settings within constantly evolving social worlds. We explain how concepts from cognitive sciences could help AI to draw a roadmap towards human-like intelligence, with a focus on its social dimensions. As a first step, we propose to expand current research to a broader set of core social skills. To do this, we present SocialAI, a benchmark to assess the acquisition of social skills of DRL agents using multiple grid-world environments featuring other (scripted) social agents. We then study the limits of a recent SOTA DRL approach when tested on SocialAI and discuss important next steps towards proficient social agents. Videos and code are available at https://sites.google.com/view/socialai.",0
"This paper presents a new benchmark dataset designed to measure socio-cognitive abilities in deep reinforcement learning agents. While current datasets focus primarily on cognitive aspects such as problem solving, reasoning, and decision making, our proposed benchmark addresses important social skills that play a crucial role in human interaction and collaboration. These skills include understanding and responding appropriately to emotions, social norms, cooperation, and communication. We evaluate state-of-the-art deep reinforcement learning algorithms using our benchmark and show promising results in terms of socio-cognitive performance. Our findings suggest that these models can effectively learn and generalize social behaviors across diverse environments, demonstrating their potential applications in socially interactive systems. Overall, we believe that this work contributes towards developing more capable artificial intelligence agents with improved social intelligence.",1
"Wasserstein GANs with Gradient Penalty (WGAN-GP) are an extremely popular method for training generative models to produce high quality synthetic data. While WGAN-GP were initially developed to calculate the Wasserstein 1 distance between generated and real data, recent works (e.g. Stanczuk et al. (2021)) have provided empirical evidence that this does not occur, and have argued that WGAN-GP perform well not in spite of this issue, but because of it. In this paper we show for the first time that WGAN-GP compute the minimum of a different optimal transport problem, the so-called congested transport (Carlier et al. (2008)). Congested transport determines the cost of moving one distribution to another under a transport model that penalizes congestion. For WGAN-GP, we find that the congestion penalty has a spatially varying component determined by the sampling strategy used in Gulrajani et al. (2017) which acts like a local speed limit, making congestion cost less in some regions than others. This aspect of the congested transport problem is new in that the congestion penalty turns out to be unbounded and depend on the distributions to be transported, and so we provide the necessary mathematical proofs for this setting. We use our discovery to show that the gradients of solutions to the optimization problem in WGAN-GP determine the time averaged momentum of optimal mass flow. This is in contrast to the gradients of Kantorovich potentials for the Wasserstein 1 distance, which only determine the normalized direction of flow. This may explain, in support of Stanczuk et al. (2021), the success of WGAN-GP, since the training of the generator is based on these gradients.",0
"This work presents an approach to computing Wasserstein Generative Adversarial Networks (GAN) with gradient penalty that accounts for transport congestion. In traditional GAN settings, the generator network aims to generate samples that fool the discriminator network into mistaking them as real data. However, in practice, the optimization problem encountered by these models can become computationally intractable due to issues such as gradient explosion and vanishing gradients.  To overcome these challenges, we introduce a novel computational method based on the theory of optimal transportation with congestion costs. Our approach integrates the gradient penalty term into the computation of the Wasserstein distance function, which helps stabilize training and improve sample quality. Additionally, our model captures real-world phenomena such as traffic flow dynamics during the computation of the Wasserstein distance. We provide extensive experimental results demonstrating the effectiveness of our proposed algorithm compared to existing state-of-the-art methods. Our findings showcase the potential of our approach for generating high-quality samples in large-scale generative tasks while accounting for complex transportation logistics.",1
"Established approaches to assuring safety-critical systems and software are difficult to apply to systems employing machine learning (ML). In many cases, ML is used on ill-defined problems, e.g. optimising sepsis treatment, where there is no clear, pre-defined specification against which to assess validity. This problem is exacerbated by the ""opaque"" nature of ML where the learnt model is not amenable to human scrutiny. Explainable AI methods have been proposed to tackle this issue by producing human-interpretable representations of ML models which can help users to gain confidence and build trust in the ML system. However, there is not much work explicitly investigating the role of explainability for safety assurance in the context of ML development. This paper identifies ways in which explainable AI methods can contribute to safety assurance of ML-based systems. It then uses a concrete ML-based clinical decision support system, concerning weaning of patients from mechanical ventilation, to demonstrate how explainable AI methods can be employed to produce evidence to support safety assurance. The results are also represented in a safety argument to show where, and in what way, explainable AI methods can contribute to a safety case. Overall, we conclude that explainable AI methods have a valuable role in safety assurance of ML-based systems in healthcare but that they are not sufficient in themselves to assure safety.",0
"In recent years, machine learning has gained increasing popularity as a tool for automating decision making processes within healthcare organizations. Despite the potential benefits of using machine learning algorithms, there remains a significant concern among stakeholders regarding their safety and reliability. In particular, clinicians have expressed skepticism towards using algorithms that lack transparency and explainability. As such, understanding the role of explainability in ensuring the safety of machine learning algorithms in healthcare is crucial. This research seeks to address this gap by examining how explainability can enhance trustworthiness and facilitate adoption of machine learning models in healthcare settings. Through case studies of existing applications of machine learning in healthcare, we demonstrate the importance of provide meaningful explanations alongside predictions generated from these systems. Furthermore, our results highlight the need for interdisciplinary collaboration and ongoing evaluation to ensure safe deployment of machine learning in healthcare. By emphasizing the centrality of explainability, this study contributes important insights into the broader field of responsible artificial intelligence development in safety critical domains. Overall, this work holds significant implications for future research aimed at realizing the full potential of machine learning without compromising patient safety.",1
"Image classification models can depend on multiple different semantic attributes of the image. An explanation of the decision of the classifier needs to both discover and visualize these properties. Here we present StylEx, a method for doing this, by training a generative model to specifically explain multiple attributes that underlie classifier decisions. A natural source for such attributes is the StyleSpace of StyleGAN, which is known to generate semantically meaningful dimensions in the image. However, because standard GAN training is not dependent on the classifier, it may not represent these attributes which are important for the classifier decision, and the dimensions of StyleSpace may represent irrelevant attributes. To overcome this, we propose a training procedure for a StyleGAN, which incorporates the classifier model, in order to learn a classifier-specific StyleSpace. Explanatory attributes are then selected from this space. These can be used to visualize the effect of changing multiple attributes per image, thus providing image-specific explanations. We apply StylEx to multiple domains, including animals, leaves, faces and retinal images. For these, we show how an image can be modified in different ways to change its classifier output. Our results show that the method finds attributes that align well with semantic ones, generate meaningful image-specific explanations, and are human-interpretable as measured in user-studies.",0
"In recent years, generative adversarial networks (GANs) have become increasingly popular in various fields due to their ability to generate high-quality images and other data from scratch. However, there remains one major challenge that hinders widespread adoption of GANs - explaining how they work. Traditional methods of explaining neural networks rely on visualizations that can be hard to interpret, especially for complex models such as GANs. To address this gap, we propose training a GAN to act as a style space controller by fine-tuning the generator network to maximize realism. We then use linear dimensionality reduction techniques such as UMAP to project the outputs of different layers into a 2D plane that captures most of the variation across classes. This technique provides a clear explanation of how GAN generates diverse and realistic samples by distorting a low-dimensional latent vector towards a specific direction. Our approach opens up new possibilities for studying existing architectures and designing novel ones that combine expressiveness, controllability, and robustness without sacrificing performance. Overall, our study shows that fine-grained understanding of GANâ€™s inner workings using self-supervised learning paves the road to better systems, both qualitatively and quantitatively.",1
"Empirical attacks on collaborative learning show that the gradients of deep neural networks can not only disclose private latent attributes of the training data but also be used to reconstruct the original data. While prior works tried to quantify the privacy risk stemming from gradients, these measures do not establish a theoretically grounded understanding of gradient leakages, do not generalize across attackers, and can fail to fully explain what is observed through empirical attacks in practice. In this paper, we introduce theoretically-motivated measures to quantify information leakages in both attack-dependent and attack-independent manners. Specifically, we present an adaptation of the $\mathcal{V}$-information, which generalizes the empirical attack success rate and allows quantifying the amount of information that can leak from any chosen family of attack models. We then propose attack-independent measures, that only require the shared gradients, for quantifying both original and latent information leakages. Our empirical results, on six datasets and four popular models, reveal that gradients of the first layers contain the highest amount of original information, while the (fully-connected) classifier layers placed after the (convolutional) feature extractor layers contain the highest latent information. Further, we show how techniques such as gradient aggregation during training can mitigate information leakages. Our work paves the way for better defenses such as layer-based protection or strong aggregation.",0
This paper describes how neural networks can leak private data despite their black box nature; introduces privacy gradients that quantify the amount of private information leaked by different parts of the model; identifies input pixels and neurons responsible for large privacy gradient values through localized attention maps; demonstrates that these sensitive regions often lie outside of explicitly labeled objects in images; discusses interpretability implications.,1
"The attribution method provides a direction for interpreting opaque neural networks in a visual way by identifying and visualizing the input regions/pixels that dominate the output of a network. Regarding the attribution method for visually explaining video understanding networks, it is challenging because of the unique spatiotemporal dependencies existing in video inputs and the special 3D convolutional or recurrent structures of video understanding networks. However, most existing attribution methods focus on explaining networks taking a single image as input and a few works specifically devised for video attribution come short of dealing with diversified structures of video understanding networks. In this paper, we investigate a generic perturbation-based attribution method that is compatible with diversified video understanding networks. Besides, we propose a novel regularization term to enhance the method by constraining the smoothness of its attribution results in both spatial and temporal dimensions. In order to assess the effectiveness of different video attribution methods without relying on manual judgement, we introduce reliable objective metrics which are checked by a newly proposed reliability measurement. We verified the effectiveness of our method by both subjective and objective evaluation and comparison with multiple significant attribution methods.",0
This would fit better if you had the correct context. Could you provide more details?,1
"POMDPs are useful models for systems where the true underlying state is not known completely to an outside observer; the outside observer incompletely knows the true state of the system, and observes a noisy version of the true system state. When the number of system states is large in a POMDP that often necessitates the use of approximation methods to obtain near optimal solutions for control. This survey is centered around the origins, theory, and approximations of finite-state POMDPs. In order to understand POMDPs, it is required to have an understanding of finite-state Markov Decision Processes (MDPs) in \autoref{mdp} and Hidden Markov Models (HMMs) in \autoref{hmm}. For this background theory, I provide only essential details on MDPs and HMMs and leave longer expositions to textbook treatments before diving into the main topics of POMDPs. Once the required background is covered, the POMDP is introduced in \autoref{pomdp}. The origins of the POMDP are explained in the classical papers section \autoref{classical}. Once the high computational requirements are understood from the exact methodological point of view, the main approximation methods are surveyed in \autoref{approximations}. Then, I end the survey with some new research directions in \autoref{conclusion}.",0
"Title: Approximation Methods for POMDPs  Approximate solutions for partially observed Markov decision processes (POMDPs) have recently gained interest due to their applicability in numerous real-world problems where exact methods may not scale well. Existing approximation techniques for POMDPs can be broadly categorized into two classes: linear function approximation and policy iteration based on simulation sampling. However, these methods suffer from computational and statistical limitations that restrict their use in larger state spaces or systems requiring more accurate estimation. This article proposes three new approximations for POMDPs which address some of those shortcomings while maintaining low complexity. These novel approaches draw inspiration from recent advancements in deep reinforcement learning and approximate dynamic programming. In particular, we develop policies that closely mirror optimal behavior by exploiting modern neural network architectures, as well as value functions that leverage expressive feature representations of states through multi-head attention mechanisms. Our experiments evaluate the quality of our proposed algorithms against established benchmarks using several domains popularized in previous research. Results indicate that each approach surpasses existing approximation schemes in accuracy at lower compute cost. We conclude by discussing possible extensions to our work that would further improve the scalability and effectiveness of POMDP solvers in practice.",1
"RGBD (RGB plus depth) object tracking is gaining momentum as RGBD sensors have become popular in many application fields such as robotics.However, the best RGBD trackers are extensions of the state-of-the-art deep RGB trackers. They are trained with RGB data and the depth channel is used as a sidekick for subtleties such as occlusion detection. This can be explained by the fact that there are no sufficiently large RGBD datasets to 1) train deep depth trackers and to 2) challenge RGB trackers with sequences for which the depth cue is essential. This work introduces a new RGBD tracking dataset - Depth-Track - that has twice as many sequences (200) and scene types (40) than in the largest existing dataset, and three times more objects (90). In addition, the average length of the sequences (1473), the number of deformable objects (16) and the number of annotated tracking attributes (15) have been increased. Furthermore, by running the SotA RGB and RGBD trackers on DepthTrack, we propose a new RGBD tracking baseline, namely DeT, which reveals that deep RGBD tracking indeed benefits from genuine training data. The code and dataset is available at https://github.com/xiaozai/DeT",0
"This paper presents DepthTrack: a novel approach to robust depth and rgb video tracking which outperforms both RGB methods alone and state-of-the art combined approaches on popular benchmarks such as VOT2018 and KITTI 2015 (outperforming all other trackers). By performing depth map refinement using an encoder network and depth-aided color prediction, we effectively encode features important for tracking into our model while suppressing noise from depth maps. We then propose a simple yet effective method for handling occlusions that utilizes spatial attention mechanisms. Finally, we present extensive ablation studies on several datasets showcasing the importance of each component, providing insights for future work. Our contributions can be summarized as follows: i) We introduce an innovative hybrid rgbd tracker with leading performance across multiple benchmarks ii) We provide detailed analysis for each component of the proposed pipeline iii) We demonstrate the effectiveness of utilizing spatial attention mechanisms for better handling occlusions iv) We open source code for reproducibility and continued research advancement.",1
"Deep CNNs, though have achieved the state of the art performance in image classification tasks, remain a black-box to a human using them. There is a growing interest in explaining the working of these deep models to improve their trustworthiness. In this paper, we introduce a Posthoc Architecture-agnostic Concept Extractor (PACE) that automatically extracts smaller sub-regions of the image called concepts relevant to the black-box prediction. PACE tightly integrates the faithfulness of the explanatory framework to the black-box model. To the best of our knowledge, this is the first work that extracts class-specific discriminative concepts in a posthoc manner automatically. The PACE framework is used to generate explanations for two different CNN architectures trained for classifying the AWA2 and Imagenet-Birds datasets. Extensive human subject experiments are conducted to validate the human interpretability and consistency of the explanations extracted by PACE. The results from these experiments suggest that over 72% of the concepts extracted by PACE are human interpretable.",0
"Abstract:  Recent advances in deep learning have enabled neural networks, especially convolutional neural networks (CNNs), to achieve state-of-the-art performance on challenging tasks such as image classification, object detection, and semantic segmentation. However, these models often lack interpretability due to their complex architecture and nonlinear behavior, making it difficult to explain how they make predictions. In this work, we propose a novel framework called ""PACE"" that addresses this challenge by providing posthoc explanations for CNNs using concept embeddings derived from human annotations. Our method takes advantage of existing architectures and algorithms without modifying them, allowing us to analyze and compare different network configurations. We demonstrate through experiments on several benchmark datasets that our approach improves the quality of extracted concepts and outperforms baseline methods in terms of quantitative evaluation metrics. Our results suggest that PACE provides insights into the inner workings of CNNs, enabling researchers and practitioners to better understand their strengths and limitations. By bridging the gap between model explainability and high accuracy, our work represents an important step towards building trustworthy artificial intelligence systems.",1
"Softmax working with cross-entropy is widely used in classification, which evaluates the similarity between two discrete distribution columns (predictions and true labels). Inspired by chi-square test, we designed a new loss function called chi-square loss, which is also works for Softmax. Chi-square loss has a statistical background. We proved that it is unbiased in optimization, and clarified its using conditions (its formula determines that it must work with label smoothing). In addition, we studied the sample distribution of this loss function by visualization and found that the distribution is related to the neural network structure, which is distinct compared to cross-entropy. In the past, the influence of structure was often ignored when visualizing. Chi-square loss can notice changes in neural network structure because it is very strict, and we explained the reason for this strictness. We also studied the influence of label smoothing and discussed the relationship between label smoothing and training accuracy and stability. Since the chi-square loss is very strict, the performance will degrade when dealing samples of very many classes.",0
"In todayâ€™s world, deep neural networks have become increasingly popular due to their ability to handle complex data sets and produce highly accurate results. However, one challenge that these systems face is choosing appropriate loss functions, which can impact performance significantly. Recently, chi-square loss has gained attention as an alternative to traditional softmax cross entropy. This paper explores the potential benefits of using chi-square loss instead of softmax cross entropy by examining how both loss functions affect network structure. Specifically, we evaluate the effects on activation values and distributional properties. Our experiments demonstrate that chi-square loss produces more balanced activations compared to softmax cross entropy, leading to better generalization across datasets. Furthermore, we observe reduced overfitting when using chi-square loss. These findings suggest that chi-square loss may provide improved stability during training, making it an effective choice for deep learning practitioners seeking enhanced model performance. Overall, our research contributes to a deeper understanding of the relationship between choice of loss function and neural network architecture, paving the way for future advancements in deep learning.",1
"Rationalizing which parts of a molecule drive the predictions of a molecular graph convolutional neural network (GCNN) can be difficult. To help, we propose two simple regularization techniques to apply during the training of GCNNs: Batch Representation Orthonormalization (BRO) and Gini regularization. BRO, inspired by molecular orbital theory, encourages graph convolution operations to generate orthonormal node embeddings. Gini regularization is applied to the weights of the output layer and constrains the number of dimensions the model can use to make predictions. We show that Gini and BRO regularization can improve the accuracy of state-of-the-art GCNN attribution methods on artificial benchmark datasets. In a real-world setting, we demonstrate that medicinal chemists significantly prefer explanations extracted from regularized models. While we only study these regularizers in the context of GCNNs, both can be applied to other types of neural networks",0
"Recently molecular graph neural networks (MGNN) have emerged as powerful machine learning models for predicting physical and chemical properties of molecules. However these methods can suffer from interpretability challenges due to their complexity. This makes explainability of predictions difficult, hindering acceptance by scientific communities. In our work we demonstrate that the use of orthonormalization and sparse representations significantly enhances the transparency of the results of MGNN models. We apply these techniques on standard datasets such as QM9 as well as more real-world challenges like virtual screening for Drug discovery and Toxicity prediction. Our experiments show improved performance compared to state-of-the art baselines which rely solely on dense embeddings. Importantly, the introduced methodological improvements result in more straightforward interpretation of the model predictions, which may boost their adoption within drug discovery pipelines and other fields where faithfully interpretable predictions play a pivotal role. Overall, we contribute further insights into how MGNN based systems can become even better at solving important problems relevant to computational chemistry without having to make any concessions regarding the quality of explanations generated alongside them.",1
"We develop a convex analytic approach to analyze finite width two-layer ReLU networks. We first prove that an optimal solution to the regularized training problem can be characterized as extreme points of a convex set, where simple solutions are encouraged via its convex geometrical properties. We then leverage this characterization to show that an optimal set of parameters yield linear spline interpolation for regression problems involving one dimensional or rank-one data. We also characterize the classification decision regions in terms of a kernel matrix and minimum $\ell_1$-norm solutions. This is in contrast to Neural Tangent Kernel which is unable to explain predictions of finite width networks. Our convex geometric characterization also provides intuitive explanations of hidden neurons as auto-encoders. In higher dimensions, we show that the training problem can be cast as a finite dimensional convex problem with infinitely many constraints. Then, we apply certain convex relaxations and introduce a cutting-plane algorithm to globally optimize the network. We further analyze the exactness of the relaxations to provide conditions for the convergence to a global optimum. Our analysis also shows that optimal network parameters can be also characterized as interpretable closed-form formulas in some practically relevant special cases.",0
"This paper explores the relationship between convex geometry and over-parameterized neural networks using duality theory. By analyzing the optimization problem that arises from training these networks, we show how dual certificate methods can be used to prove global convergence rates under strong convexity assumptions on the loss function. We provide both theoretical results and numerical experiments to support our findings. Our work contributes to the understanding of generalization performance in deep learning by providing new insights into the role played by network architecture and regularization techniques.",1
"Previous work has cast doubt on the general framework of uniform convergence and its ability to explain generalization in neural networks. By considering a specific dataset, it was observed that a neural network completely misclassifies a projection of the training data (adversarial set), rendering any existing generalization bound based on uniform convergence vacuous. We provide an extensive theoretical investigation of the previously studied data setting through the lens of infinitely-wide models. We prove that the Neural Tangent Kernel (NTK) also suffers from the same phenomenon and we uncover its origin. We highlight the important role of the output bias and show theoretically as well as empirically how a sensible choice completely mitigates the problem. We identify sharp phase transitions in the accuracy on the adversarial set and study its dependency on the training sample size. As a result, we are able to characterize critical sample sizes beyond which the effect disappears. Moreover, we study decompositions of a neural network into a clean and noisy part by considering its canonical decomposition into its different eigenfunctions and show empirically that for too small bias the adversarial phenomenon still persists.",0
"Uniform convergence is an essential concept in mathematics that describes how quickly a sequence of functions approaches a limit function as the number of terms increases. In this paper, we explore the connection between uniform convergence and adversarial spheres, which represent regions where an algorithm can make mistakes. We show that adversarial spheres have significant implications for determining whether a particular machine learning model is likely to generalize well on new data. Our main contribution is a simple remedy for addressing these issues, which involves modifying the architecture of deep neural networks (DNNs) and using more regularization methods during training. Experimental results demonstrate the effectiveness of our approach in improving DNN performance and reducing adversarial vulnerability. Overall, this work highlights the importance of understanding both the theoretical foundations and practical applications ofuniform convergence and adversarial spheres in the field of machine learning.",1
"We propose a Convolutional Neural Network-based approach to learn, detect,and extract patterns in sequential trajectory data, known here as Social Pattern Extraction Convolution (Social-PEC). A set of experiments carried out on the human trajectory prediction problem shows that our model performs comparably to the state of the art and outperforms in some cases. More importantly,the proposed approach unveils the obscurity in the previous use of pooling layer, presenting a way to intuitively explain the decision-making process.",0
"This paper describes a methodology that combines deep learning techniques with classic computer vision approaches to predict human trajectories. Our approach first applies temporal convolution filters to extract patterns from sensor data over time, then uses those features to train a neural network model. We introduce a new type of convolution operator called a velocity feature map, which allows us to capture motion patterns at different scales while reducing computational complexity. Experimental results show that our system can effectively predict trajectories up to several seconds into the future, outperforming existing methods on real-world datasets. The proposed framework has applications in robotics and autonomous systems, as well as other domains where understanding human movement is important.",1
"Approximating complex probability densities is a core problem in modern statistics. In this paper, we introduce the concept of Variational Inference (VI), a popular method in machine learning that uses optimization techniques to estimate complex probability densities. This property allows VI to converge faster than classical methods, such as, Markov Chain Monte Carlo sampling. Conceptually, VI works by choosing a family of probability density functions and then finding the one closest to the actual probability density -- often using the Kullback-Leibler (KL) divergence as the optimization metric. We introduce the Evidence Lower Bound to tractably compute the approximated probability density and we review the ideas behind mean-field variational inference. Finally, we discuss the applications of VI to variational auto-encoders (VAE) and VAE-Generative Adversarial Network (VAE-GAN). With this paper, we aim to explain the concept of VI and assist in future research with this approach.",0
"This abstract presents an introduction to variational inference (VI), a powerful statistical method that enables efficient and accurate probabilistic modeling. VI is particularly well suited for complex models with high dimensional parameters, which cannot easily be explored using traditional Markov Chain Monte Carlo methods. The key idea behind VI is to transform an intractable posterior distribution into a tractable lower bound on the log likelihood function. Using this bound as a surrogate objective, we can optimize parameters by minimizing negative evidence or maximizing positive evidence, depending on whether we want maximum a posteriori (MAP) estimates or full posterior samples. In addition, VI allows us to incorporate prior knowledge or constraints into our Bayesian analysis. Overall, VI has emerged as a valuable tool for data scientists seeking more flexible and scalable ways to perform uncertainty quantification, parameter estimation, and decision making under uncertain conditions. The paper provides an overview of basic concepts, key technical innovations, and real-world applications of VI.",1
"Data-driven models created by machine learning gain in importance in all fields of design and engineering. They have high potential to assists decision-makers in creating novel artefacts with a better performance and sustainability. However, limited generalization and the black-box nature of these models induce limited explainability and reusability. These drawbacks provide significant barriers retarding adoption in engineering design. To overcome this situation, we propose a component-based approach to create partial component models by machine learning (ML). This component-based approach aligns deep learning to systems engineering (SE). By means of the example of energy efficient building design, we first demonstrate generalization of the component-based method by accurately predicting the performance of designs with random structure different from training data. Second, we illustrate explainability by local sampling, sensitivity information and rules derived from low-depth decision trees and by evaluating this information from an engineering design perspective. The key for explainability is that activations at interfaces between the components are interpretable engineering quantities. In this way, the hierarchical component system forms a deep neural network (DNN) that directly integrates information for engineering explainability. The large range of possible configurations in composing components allows the examination of novel unseen design cases with understandable data-driven models. The matching of parameter ranges of components by similar probability distribution produces reusable, well-generalizing, and trustworthy models. The approach adapts the model structure to engineering methods of systems engineering and domain knowledge.",0
"This new paper provides a comprehensive overview of explainable artificial intelligence (AI) methods that can support engineering design tasks. In particular, we focus on developing AI models that are interpretable, transparent, and easy to communicate. We show how these techniques can integrate traditional systems engineering approaches with emerging machine learning algorithms such as component-based deep learning, which enable more efficient and effective use of data. Our work highlights promising directions for future research and development, including collaborations across multiple domains and communities. By advancing our understanding of how explainable AI can improve engineering design practice, we hope to contribute towards building trustworthy and reliable AI systems for critical applications. Keywords: Systems Engineering; Component-Based Deep Learning; Interpretable Models; Transparency; Explanation Generation; Human Factors in Artificial Intelligence",1
"Knowledge distillation has become one of the most important model compression techniques by distilling knowledge from larger teacher networks to smaller student ones. Although great success has been achieved by prior distillation methods via delicately designing various types of knowledge, they overlook the functional properties of neural networks, which makes the process of applying those techniques to new tasks unreliable and non-trivial. To alleviate such problem, in this paper, we initially leverage Lipschitz continuity to better represent the functional characteristic of neural networks and guide the knowledge distillation process. In particular, we propose a novel Lipschitz Continuity Guided Knowledge Distillation framework to faithfully distill knowledge by minimizing the distance between two neural networks' Lipschitz constants, which enables teacher networks to better regularize student networks and improve the corresponding performance. We derive an explainable approximation algorithm with an explicit theoretical derivation to address the NP-hard problem of calculating the Lipschitz constant. Experimental results have shown that our method outperforms other benchmarks over several knowledge distillation tasks (e.g., classification, segmentation and object detection) on CIFAR-100, ImageNet, and PASCAL VOC datasets.",0
"""This paper presents a method for knowledge distillation using Lipschitz continuity as a guiding principle. We propose a novel approach that leverages the regularization effects of Lipschitz continuity to improve performance on downstream tasks while maintaining the compactness of student models. Our experiments demonstrate that our proposed method outperforms state-of-the-art methods across multiple benchmark datasets, achieving significant improvements in accuracy and robustness. Furthermore, we show that our approach has several benefits, such as ensuring model interpretability, reducing overfitting, and providing better gradient smoothness for optimization purposes.""",1
"Human observers engage in selective information uptake when classifying visual patterns. The same is true of deep neural networks, which currently constitute the best performing artificial vision systems. Our goal is to examine the congruence, or lack thereof, in the information-gathering strategies of the two systems. We have operationalized our investigation as a character recognition task. We have used eye-tracking to assay the spatial distribution of information hotspots for humans via fixation maps and an activation mapping technique for obtaining analogous distributions for deep networks through visualization maps. Qualitative comparison between visualization maps and fixation maps reveals an interesting correlate of congruence. The deep learning model considered similar regions in character, which humans have fixated in the case of correctly classified characters. On the other hand, when the focused regions are different for humans and deep nets, the characters are typically misclassified by the latter. Hence, we propose to use the visual fixation maps obtained from the eye-tracking experiment as a supervisory input to align the model's focus on relevant character regions. We find that such supervision improves the model's performance significantly and does not require any additional parameters. This approach has the potential to find applications in diverse domains such as medical analysis and surveillance in which explainability helps to determine system fidelity.",0
"Understanding character recognition through visual explanations derived from both the human visual system and deep networks can provide valuable insights into how our brains process images and text. This study aimed to explore the relationship between these two systems by analyzing the features that are used by each in order to identify characters. By comparing the results obtained through both methods, we were able to gain deeper understanding of how the brain recognizes characters and how artificial neural networks mimic this process. Our findings suggest that while there are similarities between the human visual system and deep networks, there are also significant differences which could have important implications for future research in this field. Overall, this work highlights the potential benefits of combining both approaches to better understand complex cognitive processes like character recognition.",1
"Multimodal sentiment analysis aims to recognize people's attitudes from multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra- and inter-modal interactions between different communication channels. However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes. It is not clear how models utilize multimodal information for sentiment predictions. Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences), and little research has been done on explaining multimodal models. In this paper, we present an interactive visual analytics system, M2Lens, to visualize and explain multimodal models for sentiment analysis. M2Lens provides explanations on intra- and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. Moreover, M2Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities. Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis.",0
"This paper presents M2Lens, a system that allows users to visualize and interpret multimodal models for sentiment analysis. With the increasing popularity of multimodal data such as images and videos on social media platforms, there is a growing need for tools that can effectively analyze these types of data and provide insights into user emotions and opinions. To address this challenge, we propose M2Lens, which uses a combination of explainability methods such as attention maps, saliency visualizations, and feature importances to provide a comprehensive understanding of how multimodal models make predictions. Our experiments show that M2Lens improves accuracy and efficiency over state-of-the-art baselines and provides more intuitive explanations for end-users. Overall, our work has significant implications for researchers and practitioners working in areas such as social media monitoring, marketing analysis, and human-computer interaction.",1
"Supervised machine learning explainability has developed rapidly in recent years. However, clustering explainability has lagged behind. Here, we demonstrate the first adaptation of model-agnostic explainability methods to explain unsupervised clustering. We present two novel ""algorithm-agnostic"" explainability methods - global permutation percent change (G2PC) and local perturbation percent change (L2PC) - that identify feature importance globally to a clustering algorithm and locally to the clustering of individual samples. The methods are (1) easy to implement and (2) broadly applicable across clustering algorithms, which could make them highly impactful. We demonstrate the utility of the methods for explaining five popular clustering methods on low-dimensional synthetic datasets and on high-dimensional functional network connectivity data extracted from a resting-state functional magnetic resonance imaging dataset of 151 individuals with schizophrenia and 160 controls. Our results are consistent with existing literature while also shedding new light on how changes in brain connectivity may lead to schizophrenia symptoms. We further compare the explanations from our methods to an interpretable classifier and find them to be highly similar. Our proposed methods robustly explain multiple clustering algorithms and could facilitate new insights into many applications. We hope this study will greatly accelerate the development of the field of clustering explainability.",0
"Title: Explanations of unsupervised clustering results can often improve their use and trustworthiness. However, most current methods only provide explainability for specific algorithms (e.g., k-means), while ignoring others that may perform better. This work presents a new algorithm-agnostic method called CLEARER that uses global optimization techniques to find globally optimal cluster assignments. We show how our approach leads to more faithful explanations than existing local search methods. To validate our claims, we present a comprehensive evaluation using both real datasets and controlled experiments on artificial data. Our experimental results demonstrate the effectiveness of our algorithm compared against state-of-the-art alternatives. Overall, CLEARER provides reliable solutions to challenging problems and enables practitioners to make informed decisions based on high-quality explanations.",1
"Adversarial attack transferability is a well-recognized phenomenon in deep learning. Prior work has partially explained transferability by recognizing common adversarial subspaces and correlations between decision boundaries, but we have found little explanation in the literature beyond this. In this paper, we propose that transferability between seemingly different models is due to a high linear correlation between features that different deep neural networks extract. In other words, two models trained on the same task that are seemingly distant in the parameter space likely extract features in the same fashion, just with trivial shifts and rotations between the latent spaces. Furthermore, we show how applying a feature correlation loss, which decorrelates the extracted features in a latent space, can drastically reduce the transferability of adversarial attacks between models, suggesting that the models complete tasks in semantically different ways. Finally, we propose a Dual Neck Autoencoder (DNA), which leverages this feature correlation loss to create two meaningfully different encodings of input information with reduced transferability.",0
"Recent advances in deep learning have led to significant improvements in a wide range of applications such as image classification, natural language processing, and speech recognition. However, these models are known to be vulnerable to adversarial examples, which are small perturbations that can cause them to make incorrect predictions. These perturbations can be generated using techniques like FGSM (Fast Gradient Sign Method) or C\&W attack (Carlini \& Wagner). This has raised concerns regarding the reliability and security of machine learning systems. In this paper, we explore ways to improve robustness against adversarial attacks by disrupting their transferability across different architectures, networks and datasets. Our approach utilizes ensemble methods from different layers and stages during training to reduce the effectiveness of existing attacks. We evaluate our method on several benchmark datasets and show that our proposed technique significantly reduces cross-model transferability while maintaining good performance on clean data. Additionally, we also demonstrate how our defense model performs better compared to other state-of-the art approaches under white box settings. Overall, the work presented here could lead to more reliable and secure artificial intelligence systems for real world deployment.",1
"In this article, we introduce a fairness interpretability framework for measuring and explaining bias in classification and regression models at the level of a distribution. In our work, motivated by the ideas of Dwork et al. (2012), we measure the model bias across sub-population distributions using the Wasserstein metric. The transport theory characterization of the Wasserstein metric allows us to take into account the sign of the bias across the model distribution which in turn yields the decomposition of the model bias into positive and negative components. To understand how predictors contribute to the model bias, we introduce and theoretically characterize bias predictor attributions called bias explanations and investigate their stability. We also provide the formulation for the bias explanations that take into account the impact of missing values. In addition, motivated by the works of \v{S}trumbelj and Kononenko (2014) and Lundberg and Lee (2017), we construct additive bias explanations by employing cooperative game theory and investigate their properties.",0
"This abstract describes our new Wasserstein-based Fairness Interpretability Framework (WFIF) that helps explain how ML models work while satisfying different criteria of model parsimony and balance across all groups. WFIF provides clear explanations, which we demonstrate through experiments on both benchmark datasets and real world data sets including Kaggle competitions where users can fine tune their feature selection strategies and improve model performance under unfair conditions. Through evaluating tradeoffs between local vs global explanation methods, we demonstrate how our framework improves current state of art techniques such as Feature Importance Maps by generating richer and more comprehensive results. Finally, we conclude by discussing future research directions for extending our framework beyond feature importance analysis and developing robust visualizations that facilitate human analysts in making decisions related to their fairness concerns.",1
"A data set sampled from a certain population is biased if the subgroups of the population are sampled at proportions that are significantly different from their underlying proportions. Training machine learning models on biased data sets requires correction techniques to compensate for the bias. We consider two commonly-used techniques, resampling and reweighting, that rebalance the proportions of the subgroups to maintain the desired objective function. Though statistically equivalent, it has been observed that resampling outperforms reweighting when combined with stochastic gradient algorithms. By analyzing illustrative examples, we explain the reason behind this phenomenon using tools from dynamical stability and stochastic asymptotics. We also present experiments from regression, classification, and off-policy prediction to demonstrate that this is a general phenomenon. We argue that it is imperative to consider the objective function design and the optimization algorithm together while addressing the sampling bias.",0
"Sampling bias can significantly impact the accuracy of machine learning models, particularly in applications where data collection is expensive or difficult. Two common methods used to address sampling bias are resampling techniques such as bootstrapping and bagging, and reweighting algorithms like inverse propensity scoring (IPS). In this paper, we evaluate the performance of these two methods on real-world datasets with different types of biases and compare their effectiveness at reducing error rates using Stochastic Gradient Boosting. Our results show that resampling consistently performs better than reweighting across all benchmarks, including imbalanced classification problems. We attribute this advantage to the ability of resampling to mitigate overfitting while preserving model complexity. Overall, our findings suggest that resampling should be considered as a preferred approach for handling complex sampling patterns in practice.",1
"Human motion characteristics are used to monitor the progression of neurological diseases and mood disorders. Since perceptions of emotions are also interleaved with body posture and movements, emotion recognition from human gait can be used to quantitatively monitor mood changes. Many existing solutions often use shallow machine learning models with raw positional data or manually extracted features to achieve this. However, gait is composed of many highly expressive characteristics that can be used to identify human subjects, and most solutions fail to address this, disregarding the subject's privacy. This work introduces a novel deep neural network architecture to disentangle human emotions and biometrics. In particular, we propose a cross-subject transfer learning technique for training a multi-encoder autoencoder deep neural network to learn disentangled latent representations of human motion features. By disentangling subject biometrics from the gait data, we show that the subject's privacy is preserved while the affect recognition performance outperforms traditional methods. Furthermore, we exploit Guided Grad-CAM to provide global explanations of the model's decision across gait cycles. We evaluate the effectiveness of our method to existing methods at recognizing emotions using both 3D temporal joint signals and manually extracted features. We also show that this data can easily be exploited to expose a subject's identity. Our method shows up to 7% improvement and highlights the joints with the most significant influence across the average gait cycle.",0
"Title: Understanding Motion-Based Emotion Detection in Real-World Settings  Abstract: Affective computing has gained significant attention in recent years due to its potential applications in various domains such as healthcare, gaming, education, and market research. One of the key challenges in affective computing is recognizing human emotions from different data modalities such as facial expressions, audio, physiological signals, and body movements. In particular, motion-based recognition of emotions through wearables (e.g., smartwatches) is becoming increasingly popular due to their ease of use, low cost, and ubiquitous nature.  However, designing accurate and reliable algorithms for motion-based emotion detection remains a challenge due to several factors. Firstly, real-world scenarios can have complex environments that introduce noise and interference, making emotion recognition more difficult. Secondly, privacy concerns arise since continuous monitoring of motion data may require collecting sensitive personal information. Finally, explainability and interpretability issues might hinder the adoption of these techniques by end users who want to understand how decisions are made based on their data.  This work presents an approach towards addressing these limitations while improving the overall performance of motion-based emotion detection systems. Our proposed method employs advanced machine learning models to learn features from raw sensor data without requiring explicit feature engineering. We demonstrate the effectiveness of our approach on two public datasets involving diverse participants under natural settings, achieving state-of-the-art results across multiple benchmarks for classification accuracy, F1 score, and mean average precision. Moreover, we provide insights into interpreting learned representations through visualizations, helping us better understand which aspects of movement patterns contribute most significantly to recognized emotions.  We also discuss privacy considerations during both training and deployment stages of the proposed framework. Particularly, we employ pruning techniques t",1
"We demonstrate a library for the integration of domain knowledge in deep learning architectures. Using this library, the structure of the data is expressed symbolically via graph declarations and the logical constraints over outputs or latent variables can be seamlessly added to the deep models. The domain knowledge can be defined explicitly, which improves the models' explainability in addition to the performance and generalizability in the low-data regime. Several approaches for such an integration of symbolic and sub-symbolic models have been introduced; however, there is no library to facilitate the programming for such an integration in a generic way while various underlying algorithms can be used. Our library aims to simplify programming for such an integration in both training and inference phases while separating the knowledge representation from learning algorithms. We showcase various NLP benchmark tasks and beyond. The framework is publicly available at Github(https://github.com/HLR/DomiKnowS).",0
"Despite their recent successes, deep learning models have limitations when it comes to integrating domain knowledge into complex tasks such as natural language processing, computer vision, robotics, and game playing. This study introduces a novel library called ""DomiKnowS"" that tackles this problem by providing an interface between symbolic domain knowledge and black box deep learning models. Our contribution shows how to effectively integrate domain-specific rules, constraints, preferences, and prioritized information into different deep learning architectures without modifying them. We evaluated our approach on several benchmark datasets across diverse domains, showing improved accuracy over strong baselines while demonstrating robustness against noise and incomplete data. Additionally, we provide intuitive examples using popular libraries like PyTorch that demonstrate the ease of use of DomiKnowS. This research enables developers to incorporate expert knowledge seamlessly within state-of-the-art deep learning algorithms without requiring extensive modifications to these models. By bridging the gap between handcrafted logic and powerful machine learning techniques, our work paves the way towards more informed decision making in real-world applications.",1
"Current machine learning models have shown high efficiency in solving a wide variety of real-world problems. However, their black box character poses a major challenge for the understanding and traceability of the underlying decision-making strategies. As a remedy, many post-hoc explanation and self-explanatory methods have been developed to interpret the models' behavior. These methods, in addition, enable the identification of artifacts that can be learned by the model as class-relevant features. In this work, we provide a detailed case study of the self-explaining network, ProtoPNet, in the presence of a spectrum of artifacts. Accordingly, we identify the main drawbacks of ProtoPNet, especially, its coarse and spatially imprecise explanations. We address these limitations by introducing Prototypical Relevance Propagation (PRP), a novel method for generating more precise model-aware explanations. Furthermore, in order to obtain a clean dataset, we propose to use multi-view clustering strategies for segregating the artifact images using the PRP explanations, thereby suppressing the potential artifact learning in the models.",0
"In recent years, self-explaining models have emerged as powerful tools for understanding complex datasets and improving decision making across many domains. However, these methods often suffer from limitations such as high computational cost and lack of interpretability. To address these issues, we propose a novel framework called prototypical relevance propagation (PRP) that effectively enhances the explainability and efficiency of self-explaining models. Our approach leverages the concept of prototypes, which represent the most representative examples in a dataset, to guide the learning process. By doing so, PRP reduces the search space for relevant features, thereby significantly reducing computation time while preserving accuracy. We demonstrate the effectiveness of our method through extensive experiments on real-world applications such as image classification and natural language processing. Our results show that PRP consistently outperforms state-of-the-art alternatives in terms of both explanation quality and model performance. Overall, our work represents a significant step forward in developing reliable and interpretable machine learning systems.",1
"Many real-world complex systems can be described as graphs. For a large-scale graph with low sparsity, a node's adjacency vector is a long and sparse representation, limiting the practical utilization of existing machine learning methods on nodal features. In practice, graph embedding (graph representation learning) attempts to learn a lower-dimensional representation vector for each node or the whole graph while maintaining the most basic information of graph. Since various machine learning methods can efficiently process lower-dimensional vectors, graph embedding has recently attracted a lot of attention. However, most node embedding or whole graph embedding methods suffer from the problem of having more sophisticated methodology, hyperparameter optimization, and low explainability. This paper proposes a hyperparameter-free, extensible, and explainable whole graph embedding method, combining the DHC (Degree, H-index and Coreness) theorem and Shannon Entropy (E), abbreviated as DHC-E. The new whole graph embedding scheme can obtain a trade-off between simplicity and quality under supervised classification learning tasks, using molecular, social, and brain networks. In addition, the proposed approach has a good performance in lower-dimensional graph visualization. Overall, the new methodology is simple, hyperparameter-free, extensible, and explainable for whole graph embedding with promising potential for exploring graph classification, prediction, and lower-dimensional graph visualization.",0
"This research presents a novel method for whole graph embedding that is hyperparameter-free and interpretable. With traditional node embeddings, we lose important relationships between nodes that exist within the graph structure. To address this issue, our proposed approach captures both local and global features from graphs using a learnable random walk sampling strategy without requiring any tuning parameters. Our model utilizes a neural network architecture inspired by attention mechanisms used in natural language processing tasks, allowing us to effectively capture dependencies between nodes while preserving their unique characteristics. Our experimental results demonstrate improved accuracy on benchmark datasets compared to existing methods, as well as superior interpretability through visualization tools. Overall, our work offers a promising solution for solving complex problems related to knowledge representation, social network analysis, and recommender systems where graph structured data plays an essential role.",1
"Graph Neural Networks (GNNs), which generalize the deep neural networks to graph-structured data, have achieved great success in modeling graphs. However, as an extension of deep learning for graphs, GNNs lack explainability, which largely limits their adoption in scenarios that demand the transparency of models. Though many efforts are taken to improve the explainability of deep learning, they mainly focus on i.i.d data, which cannot be directly applied to explain the predictions of GNNs because GNNs utilize both node features and graph topology to make predictions. There are only very few work on the explainability of GNNs and they focus on post-hoc explanations. Since post-hoc explanations are not directly obtained from the GNNs, they can be biased and misrepresent the true explanations. Therefore, in this paper, we study a novel problem of self-explainable GNNs which can simultaneously give predictions and explanations. We propose a new framework which can find $K$-nearest labeled nodes for each unlabeled node to give explainable node classification, where nearest labeled nodes are found by interpretable similarity module in terms of both node similarity and local structure similarity. Extensive experiments on real-world and synthetic datasets demonstrate the effectiveness of the proposed framework for explainable node classification.",0
"In recent years, graph neural networks (GNNs) have become increasingly popular due to their ability to handle complex relationships between nodes in a graph data structure. However, one major challenge faced by GNNs is interpretability - explaining how they arrive at certain predictions can be difficult due to their black box nature. This paper presents a methodology towards self-explainable GNNs through attention mechanisms that allow us to focus on important parts of the input graph while making predictions. By implementing these methods, we demonstrate improved performance in terms of both accuracy and explainability. Our approach is evaluated using several benchmark datasets, showing consistent improvement over baseline models. Ultimately, our work takes a step towards providing more transparent machine learning algorithms, which is crucial for building trustworthy artificial intelligence systems.",1
"Since its introduction two decades ago, there has been increasing interest in the problem of early classification of time series. This problem generalizes classic time series classification to ask if we can classify a time series subsequence with sufficient accuracy and confidence after seeing only some prefix of a target pattern. The idea is that the earlier classification would allow us to take immediate action, in a domain in which some practical interventions are possible. For example, that intervention might be sounding an alarm or applying the brakes in an automobile. In this work, we make a surprising claim. In spite of the fact that there are dozens of papers on early classification of time series, it is not clear that any of them could ever work in a real-world setting. The problem is not with the algorithms per se but with the vague and underspecified problem description. Essentially all algorithms make implicit and unwarranted assumptions about the problem that will ensure that they will be plagued by false positives and false negatives even if their results suggested that they could obtain near-perfect results. We will explain our findings with novel insights and experiments and offer recommendations to the community.",0
"An important question in time series analysis is when early classification can be meaningfully applied to a given dataset. This work aims to provide insight into this problem by examining key factors that impact the performance of early classification methods. Our findings suggest that several aspects must be considered before applying these techniques, including data quality, model complexity, and prior knowledge of the system being studied. Through a comprehensive evaluation on both synthetic and real-world datasets, we demonstrate that effective early classification depends significantly on these factors, as well as others such as feature selection and parameter tuning. Based on our results, guidelines are proposed for identifying scenarios where early classification may offer significant benefits compared to traditional approaches. By understanding when and why early classification works, researchers and practitioners alike can make more informed decisions when analyzing time series data.",1
"Deep neural networks, especially convolutional deep neural networks, are state-of-the-art methods to classify, segment or even generate images, movies, or sounds. However, these methods lack of a good semantic understanding of what happens internally. The question, why a COVID-19 detector has classified a stack of lung-ct images as positive, is sometimes more interesting than the overall specificity and sensitivity. Especially when human domain expert knowledge disagrees with the given output. This way, human domain experts could also be advised to reconsider their choice, regarding the information pointed out by the system. In addition, the deep learning model can be controlled, and a present dataset bias can be found. Currently, most explainable AI methods in the computer vision domain are purely used on image classification, where the images are ordinary images in the visible spectrum. As a result, there is no comparison on how the methods behave with multimodal image data, as well as most methods have not been investigated on how they behave when used for object detection. This work tries to close the gaps. Firstly, investigating three saliency map generator methods on how their maps differ across the different spectra. This is achieved via accurate and systematic training. Secondly, we examine how they behave when used for object detection. As a practical problem, we chose object detection in the infrared and visual spectrum for autonomous driving. The dataset used in this work is the Multispectral Object Detection Dataset, where each scene is available in the FIR, MIR and NIR as well as visual spectrum. The results show that there are differences between the infrared and visual activation maps. Further, an advanced training with both, the infrared and visual data not only improves the network's output, it also leads to more focused spots in the saliency maps.",0
"This paper compares two deep saliency map generators (DSMGs) for object detection using multispectral data: Grad-CAM and OcclusionDAM. The goal of this comparison is to evaluate their strengths and weaknesses and identify which approach performs better in different scenarios. The first section introduces the background behind object detection and the importance of understanding the decision making process used by models. The second section provides a detailed overview of Grad-CAM and OcclusionDAM, explaining how they generate heatmaps that highlight important features for classifying objects. Results from experiments testing both methods on a number of public datasets are presented in the third section, followed by a discussion comparing the performance of each DSMG across all tasks. Finally, recommendations are provided for researchers choosing a method depending on task characteristics such as dataset size or complexity. Overall, this work demonstrates the effectiveness of these tools at providing insights into model decisions and shows promise for improving object detection models through informed evaluation.",1
"Recent work suggests that convolutional neural networks of different architectures learn to classify images in the same order. To understand this phenomenon, we revisit the over-parametrized deep linear network model. Our asymptotic analysis, assuming that the hidden layers are wide enough, reveals that the convergence rate of this model's parameters is exponentially faster along directions corresponding to the larger principal components of the data, at a rate governed by the singular values. We term this convergence pattern the Principal Components bias (PC-bias). We show how the PC-bias streamlines the order of learning of both linear and non-linear networks, more prominently at earlier stages of learning. We then compare our results to the simplicity bias, showing that both biases can be seen independently, and affect the order of learning in different ways. Finally, we discuss how the PC-bias may explain some benefits of early stopping and its connection to PCA, and why deep networks converge more slowly when given random labels.",0
"Title: Quantifying Principal Component Bias in Deep Neural Networks  Artificial neural networks (ANN) have shown remarkable performance across a wide range of domains due to their ability to learn complex patterns from data. One potential limitation of these models lies in their sensitivity to input preprocessing, such as normalization, scaling, and feature extraction techniques. Previous studies have suggested that certain methods can introduce bias into the modelâ€™s learned representations, leading to suboptimal generalization performance. This study focuses on quantifying principal component analysis (PCA) bias in deep neural networks by introducing a novel framework called â€œPrincipal Component Analysis Bias Evaluationâ€ (PCABE). Our approach is motivated by recent findings demonstrating how PCA affects the distributional properties of ANN inputs and hidden activations during training. We validate our methodology using comprehensive experiments on synthetic datasets generated under different random configurations. Our results provide new insights into how PCA impacts learning dynamics, resulting in biased representations throughout network layers. In conclusion, we emphasize the importance of selecting appropriate normalization schemes to mitigate these detrimental effects and ensure optimal performance in downstream tasks. Overall, PCABE serves as an essential tool for researchers and practitioners working with high-dimensional data and large-scale deep learning systems.",1
"Self-explainable deep models are devised to represent the hidden concepts in the dataset without requiring any posthoc explanation generation technique. We worked with one of such models motivated by explicitly representing the classifier function as a linear function and showed that by exploiting probabilistic latent and properly modifying different parts of the model can result better explanation as well as provide superior predictive performance. Apart from standard visualization techniques, we proposed a new technique which can strengthen human understanding towards hidden concepts. We also proposed a technique of using two different self-supervision techniques to extract meaningful concepts related to the type of self-supervision considered and achieved significant performance boost. The most important aspect of our method is that it works nicely in a low data regime and reaches the desired accuracy in a few number of epochs. We reported exhaustive results with CIFAR10, CIFAR100, and AWA2 datasets to show effect of our method with moderate and relatively complex datasets.",0
"In natural language processing (NLP), understanding complex texts can be challenged by factors such as ambiguity, contextual dependencies, and latent concepts. These issues make it difficult to generate accurate explanations. To overcome these limitations, we propose an ante-hoc approach that uses semantic grouping to identify latent concepts before explaining their meanings. Our method induces semantic groupings from the text input using state-of-the-art NLP models, which enables us to explain the relationships among different concepts. We evaluate our approach on two benchmark datasets and show significant improvements over baseline methods in terms of both quantitative metrics and human evaluations. Overall, our study demonstrates the effectiveness of integrating semantic grouping into NLP explanation frameworks, leading to better comprehension and interpretation of complex texts.",1
"The successful deployment of artificial intelligence (AI) in many domains from healthcare to hiring requires their responsible use, particularly in model explanations and privacy. Explainable artificial intelligence (XAI) provides more information to help users to understand model decisions, yet this additional knowledge exposes additional risks for privacy attacks. Hence, providing explanation harms privacy. We study this risk for image-based model inversion attacks and identified several attack architectures with increasing performance to reconstruct private image data from model explanations. We have developed several multi-modal transposed CNN architectures that achieve significantly higher inversion performance than using the target model prediction only. These XAI-aware inversion models were designed to exploit the spatial knowledge in image explanations. To understand which explanations have higher privacy risk, we analyzed how various explanation types and factors influence inversion performance. In spite of some models not providing explanations, we further demonstrate increased inversion performance even for non-explainable target models by exploiting explanations of surrogate models through attention transfer. This method first inverts an explanation from the target prediction, then reconstructs the target image. These threats highlight the urgent and significant privacy risks of explanations and calls attention for new privacy preservation techniques that balance the dual-requirement for AI explainability and privacy.",0
"Abstract: This paper investigates the potential risks posed by explanations generated by machine learning models. While these explanations are often used to improve model transparency and trustworthiness, we show that they can also expose sensitive information about the training data and potentially facilitate attacks on the original dataset. Our approach involves analyzing explanation output to identify features that contribute most strongly to predictions. By manipulating these features in synthetic datasets, we demonstrate how attackers could generate adversarial examples aimed at tricking explanatory models into revealing specific information about underlying patterns in the original data. We evaluate our attacks across several widely used explainability methods, including LIME, SHAP, and TreeExplainer, showing consistent vulnerabilities against realistic threat scenarios. Finally, we discuss implications for ensuring secure use of ML explanations in practice, highlighting key trade-offs faced by users who need both interpretability and privacy protection. Overall, this work raises important questions regarding risk management strategies as MLPs become increasingly integrated into critical decision making processes. Note: This abstract assumes background knowledge of machine learning (ML), artificial intelligence (AI) and digital security concepts. Terminology such as ""explainability"" and ""attack surface"" may require further elaboration depending on intended audience. Additionally, citations should refer back to original research contributions referenced throughout text rather than reiterating exact language from other literature reviews. ------------------------------",1
"This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.",0
"Here is a sample abstract: ""The field of deep learning theory has made significant advancements over recent years, leading to innovations in fields such as image recognition, speech synthesis, natural language processing, game playing, and robotics. This paper outlines several key principles that have emerged from these developments, including backpropagation algorithms, gradient descent optimization methods, convolutional neural networks, recurrent neural networks, generative adversarial networks, reinforcement learning, and transfer learning. By examining how these concepts can be applied across different areas of study, we aim to provide researchers and practitioners with a more comprehensive understanding of the fundamental theories underpinning successful implementation of deep learning models.""",1
"Compared with traditional deep learning techniques, continual learning enables deep neural networks to learn continually and adaptively. Deep neural networks have to learn new tasks and overcome forgetting the knowledge obtained from the old tasks as the amount of data keeps increasing in applications. In this article, two continual learning scenarios will be proposed to describe the potential challenges in this context. Besides, based on our previous work regarding the CLeaR framework, which is short for continual learning for regression tasks, the work will be further developed to enable models to extend themselves and learn data successively. Research topics are related but not limited to developing continual deep learning algorithms, strategies for non-stationarity detection in data streams, explainable and visualizable artificial intelligence, etc. Moreover, the framework- and algorithm-related hyperparameters should be dynamically updated in applications. Forecasting experiments will be conducted based on power generation and consumption data collected from real-world applications. A series of comprehensive evaluation metrics and visualization tools can help analyze the experimental results. The proposed framework is expected to be generally applied to other constantly changing scenarios.",0
"A new framework has been developed that enables continual learning and adaptation through data collection by means of a feedback loop that uses the power forecast as the target. This approach takes into account past experiences, making the model more adaptive and effective over time. By using this methodology in regression problems such as electricity demand predictions, we can improve the accuracy and efficiency of the models while minimizing human intervention. The results show that our proposed method outperforms traditional approaches in terms of speed, robustness, and reliability, providing superior performance compared to state-of-the-art techniques. Overall, this study presents a novel approach to solving complex prediction tasks while ensuring transparency and interpretability, setting a new standard for future research in the field.",1
"Big data methods are becoming an important tool for tax fraud detection around the world. Unsupervised learning approach is the dominant framework due to the lack of label and ground truth in corresponding data sets although these methods suffer from low interpretability. HUNOD, a novel hybrid unsupervised outlier detection method for tax evasion risk management, is presented in this paper. In contrast to previous methods proposed in the literature, the HUNOD method combines two outlier detection approaches based on two different machine learning designs (i.e, clustering and representational learning) to detect and internally validate outliers in a given tax dataset. The HUNOD method allows its users to incorporate relevant domain knowledge into both constituent outlier detection approaches in order to detect outliers relevant for a given economic context. The interpretability of obtained outliers is achieved by training explainable-by-design surrogate models over results of unsupervised outlier detection methods. The experimental evaluation of the HUNOD method is conducted on two datasets derived from the database on individual personal income tax declarations collected by the Tax Administration of Serbia. The obtained results show that the method indicates between 90% and 98% internally validated outliers depending on the clustering configuration and employed regularization mechanisms for representational learning.",0
"In today's rapidly evolving business landscape, managing tax evasion risk has become increasingly important for organizations worldwide. Traditional methods of unsupervised outlier detection have proven effective, but their reliability can vary depending on the underlying data distribution and quality of features used for analysis. To address these challenges, we propose a hybrid approach that combines both statistical models and machine learning algorithms to identify potential tax fraud cases. Our method integrates seasonality control, anomaly thresholding, random projection techniques and an ensemble classifier framework tailored specifically towards tax evasion scenarios. Experimental results based on real-world datasets showcase our method's superior performance compared to baseline methods, yielding higher precision scores (>96%) and recall rates (>82%). This study presents valuable insights into implementing robust unsupervised methods for tax risk management, helping financial institutions maintain regulatory compliance while mitigating reputational damage and legal implications associated with tax evasion scandals.",1
"Multiple Instance Learning (MIL) gains popularity in many real-life machine learning applications due to its weakly supervised nature. However, the corresponding effort on explaining MIL lags behind, and it is usually limited to presenting instances of a bag that are crucial for a particular prediction. In this paper, we fill this gap by introducing ProtoMIL, a novel self-explainable MIL method inspired by the case-based reasoning process that operates on visual prototypes. Thanks to incorporating prototypical features into objects description, ProtoMIL unprecedentedly joins the model accuracy and fine-grained interpretability, which we present with the experiments on five recognized MIL datasets.",0
This sounds like a fascinating topic! Can you tell me more? I would love to provide some ideas on how to write your abstract!,1
"The goal in label-imbalanced and group-sensitive classification is to optimize relevant metrics such as balanced error and equal opportunity. Classical methods, such as weighted cross-entropy, fail when used with the modern practice of training deep nets to the terminal phase of training(TPT), that is training beyond zero training error. This observation has motivated recent flurry of activity in developing heuristic alternatives following the intuitive mechanism of promoting larger margin for minorities. In contrast to previous heuristics, we follow a principled analysis explaining how different loss adjustments affect margins. First, we prove that for all linear classifiers trained in TPT, it is necessary to introduce multiplicative, rather than additive, logit adjustments so that the relative margins between classes change appropriately. To show this, we discover a connection of the multiplicative CE modification to the so-called cost-sensitive support-vector machines. Perhaps counterintuitively, we also find that, at the start of the training, the same multiplicative weights can actually harm the minority classes. Thus, while additive adjustments are ineffective in the TPT, we show numerically that they can speed up convergence by countering the initial negative effect of the multiplicative weights. Motivated by these findings, we formulate the vector-scaling(VS) loss, that captures existing techniques as special cases. Moreover, we introduce a natural extension of the VS-loss to group-sensitive classification, thus treating the two common types of imbalances (label/group) in a unifying way. Importantly, our experiments on state-of-the-art datasets are fully consistent with our theoretical insights and confirm the superior performance of our algorithms. Finally, for imbalanced Gaussian-mixtures data, we perform a generalization analysis, revealing tradeoffs between different metrics.",0
"This research study explores the impact of label imbalance and group sensitivity on classification models trained using deep neural networks (DNNs) in highly overparameterized regimes, where model capacity greatly exceeds dataset size. We examine how training DNN classifiers can lead to poor performance due to uneven distribution of labels across groups. Our analysis shows that even when datasets contain enough data overall, certain classes or subgroups may still have few labeled examples available for training.  We present experimental results demonstrating that this issue is exacerbated by overparameterization: standard techniques used to address label imbalances and promote fairness may fail at high levels of model complexity. Despite their effectiveness in well-defined settings, these methods often struggle to generalize or produce competitive accuracy on complex models. We investigate several popular approaches, including reweighting, undersampling, oversampling, ensembling and adversarial training, in combination with common activation functions such as ReLU or LeakyReLU.  To overcome the limitations of current techniques, we propose a novel loss function that explicitly models label imbalance and promotes balanced predictions within each subgroup. By penalizing large errors made more frequently within any specific group, our approach encourages the model to pay greater attention to hard examples from those subpopulations during training. Experiments show that this modification consistently improves both balance metrics and mainstream evaluation measures compared to prior art, suggesting it could serve as a valuable addition to the machine learning practitioner toolkit.  In summary, this work reveals significant challenges in achieving reliable performance under conditions of label imbalance and overparameterization. We hope our insights and contributions inform future research into mitigating algorithmic bias and increasing fairness within large-scale AI systems.",1
"To make advanced learning machines such as Deep Neural Networks (DNNs) more transparent in decision making, explainable AI (XAI) aims to provide interpretations of DNNs' predictions. These interpretations are usually given in the form of heatmaps, each one illustrating relevant patterns regarding the prediction for a given instance. Bayesian approaches such as Bayesian Neural Networks (BNNs) so far have a limited form of transparency (model transparency) already built-in through their prior weight distribution, but notably, they lack explanations of their predictions for given instances. In this work, we bring together these two perspectives of transparency into a holistic explanation framework for explaining BNNs. Within the Bayesian framework, the network weights follow a probability distribution. Hence, the standard (deterministic) prediction strategy of DNNs extends in BNNs to a predictive distribution, and thus the standard explanation extends to an explanation distribution. Exploiting this view, we uncover that BNNs implicitly employ multiple heterogeneous prediction strategies. While some of these are inherited from standard DNNs, others are revealed to us by considering the inherent uncertainty in BNNs. Our quantitative and qualitative experiments on toy/benchmark data and real-world data from pathology show that the proposed approach of explaining BNNs can lead to more effective and insightful explanations.",0
"Title: Understanding Bayesian Artificial Intelligence Systems The use of artificial intelligence (AI) has rapidly increased over the past few years. With the advent of powerful machine learning algorithms, researchers have been able to create intelligent systems that can perform tasks such as speech recognition, image classification, and natural language processing. One key approach to building these AI systems is through the use of neural networks. These models aim to mimic how our brains process information by computing complex mathematical equations based on inputs received from sensors. However, traditional neural network architectures often lack transparency, interpretability, and uncertainty quantification. In recent times, there has been growing interest in using Bayesian methods to overcome some of these shortcomings. By incorporating probabilistic modeling techniques into deep learning frameworks, researchers have developed Bayesian Neural Networks (BNNs). BNNs enable computation of predictive uncertainties associated with their outputs, which enables them to make more informed decisions under uncertainty.  This work presents a comprehensive review of the state-of-the-art research in developing BNNs for different types of applications. We discuss the motivation behind using probability theory within neural networks, and explore several approaches used to implement BNNs including variational inference and Markov Chain Monte Carlo sampling methods. Furthermore, we analyze the benefits and challenges involved with using BNNs compared to their non-probabilistic counterparts. Finally, we provide future directions for research aimed at further advancing the development and deployment of efficient, interpretable, and reliable AI systems based on BNNs. Overall, this study highlights the potential impact of integrating Bayesian principles within neural networks for improved decision making in real-w",1
"A generic fast method for object classification is proposed. In addition, a method for dimensional reduction is presented. The presented algorithms have been applied to real-world data from chip fabrication successfully to the task of predicting defect states of tens of thousands of chips of several products based on measurements or even just part of measurements. Unlike typical neural networks with a large number of weights to optimize over, the presented algorithm tries optimizing only over a very small number of variables in order to increase chances to find a global optimum. Our approach is interesting in that it is fast, led to good to very good performance with real-world wafer data, allows for short implementations and computes values which have a clear meaning easy to explain.",0
"A novel algorithm has been developed that can quickly and accurately detect defects on wafers during fabrication process without human intervention. The proposed method utilizes deep learning techniques to analyze images obtained from high resolution scanners used in the industry. Compared to traditional methods such as visual inspection by experts or automated processes using thresholding algorithms, our approach achieves higher accuracy while requiring less computational resources. Experimental results on real datasets demonstrate improved detection rates and reduced false positives compared to current state-of-the-art techniques. Overall, our work presents a promising solution towards enhancing quality control procedures in wafer manufacturing plants, ultimately leading to cost savings due to better yield management. As device dimensions continue to shrink, robust automated approaches like ours become increasingly crucial, providing benefits for both semiconductor industries and other related sectors including electronics and photonics.",1
"Deep generative models have been demonstrated as state-of-the-art density estimators. Yet, recent work has found that they often assign a higher likelihood to data from outside the training distribution. This seemingly paradoxical behavior has caused concerns over the quality of the attained density estimates. In the context of hierarchical variational autoencoders, we provide evidence to explain this behavior by out-of-distribution data having in-distribution low-level features. We argue that this is both expected and desirable behavior. With this insight in hand, we develop a fast, scalable and fully unsupervised likelihood-ratio score for OOD detection that requires data to be in-distribution across all feature-levels. We benchmark the method on a vast set of data and model combinations and achieve state-of-the-art results on out-of-distribution detection.",0
Abstract:,1
"This paper presents the development of systematic machine learning (ML) approach to enable explainable and rapid assessment of fire resistance and fire-induced spalling of reinforced concrete (RC) columns. The developed approach comprises of an ensemble of three novel ML algorithms namely; random forest (RF), extreme gradient boosted trees (ExGBT), and deep learning (DL). These algorithms are trained to account for a wide collection of geometric characteristics and material properties, as well as loading conditions to examine fire performance of normal and high strength RC columns by analyzing a comprehensive database of fire tests comprising of over 494 observations. The developed ensemble is also capable of presenting quantifiable insights to ML predictions; thus, breaking free from the notion of 'blackbox' ML and establishing a solid step towards transparent and explainable ML. Most importantly, this work tackles the scarcity of available fire tests by proposing new techniques to leverage the use of real, synthetic and augmented fire test observations. The developed ML ensemble has been calibrated and validated for standard and design fire exposures and for one, two, three and four-sided fire exposures thus; covering a wide range of practical scenarios present during fire incidents. When fully deployed, the developed ensemble can analyze over 5,000 RC columns in under 60 seconds thus, providing an attractive solution for researchers and practitioners. The presented approach can also be easily extended for evaluating fire resistance and spalling of other structural members and under varying fire scenarios and loading conditions and hence paves the way to modernize the state of this research area and practice.",0
"Abstract: As fires become more commonplace, determining how well structures will stand up to them has become increasingly important. However, current methods of testing fire resistance and spalling potential of reinforced concrete (RC) columns can take considerable time, money, and resources, as they often require full scale tests that only evaluate one parameter at a time. To address these limitations, we propose a novel approach called Explainable Machine Learning using Real, Synthetic and Augmented Fire Tests (EMLRSAFIRT), which combines machine learning algorithms with real, synthetic, and augmented fire tests to predict both fire resistance and spalling potential of RC columns. By leveraging data from multiple sources, our method is able to capture critical relationships among variables related to the behavior of RC columns during and after exposure to fire. The results demonstrate that EMLRSAFIRT provides a fast, accurate, efficient, cost-effective, and flexible solution to simulate fire scenarios on a large number of cases within seconds and obtain results similar to real experiments with significantly reduced uncertainties and confusions. Our work therefore offers a promising alternative to traditional experimental procedures, enabling informed decisions concerning fire safety assessments without sacrificing accuracy.",1
"Mini-batch optimal transport (m-OT) has been widely used recently to deal with the memory issue of OT in large-scale applications. Despite their practicality, m-OT suffers from misspecified mappings, namely, mappings that are optimal on the mini-batch level but do not exist in the optimal transportation plan between the original measures. To address the misspecified mappings issue, we propose a novel mini-batch method by using partial optimal transport (POT) between mini-batch empirical measures, which we refer to as mini-batch partial optimal transport (m-POT). Leveraging the insight from the partial transportation, we explain the source of misspecified mappings from the m-OT and motivate why limiting the amount of transported masses among mini-batches via POT can alleviate the incorrect mappings. Finally, we carry out extensive experiments on various applications to compare m-POT with m-OT and recently proposed mini-batch method, mini-batch unbalanced optimal transport (m-UOT). We observe that m-POT is better than m-OT deep domain adaptation applications while having comparable performance with m-UOT. On other applications, such as deep generative model, gradient flow, and color transfer, m-POT yields more favorable performance than both m-OT and m-UOT.",0
"Hereâ€™s an example: We propose a new mini-batch method that improves efficiency by partially solving the transportation problem using Coreset techniques. Our method outperforms existing methods on a range of machine learning tasks including support vector machines (SVM), logistic regression (LR) and neural networks (NN). We show empirically that our algorithm reduces computation time while maintaining comparable accuracy. ---------------------------------- This paper presents a novel approach to mini-batch optimization that leverages partial solution techniques from the field of operations research. By adapting coresets to solve the transportation problem more efficiently, we demonstrate significant reductions in computational cost without sacrificing model performance. Experiments across multiple benchmark datasets and models highlight the superiority of our method compared to traditional mini-batch algorithms. Overall, these results have important implications for large-scale machine learning applications where efficient training is critical. ----------------------------------------------- An efficient mini-batch method via partial transportation",1
"Existing studies on disease diagnostic models focus either on diagnostic model learning for performance improvement or on the visual explanation of a trained diagnostic model. We propose a novel learn-explain-reinforce (LEAR) framework that unifies diagnostic model learning, visual explanation generation (explanation unit), and trained diagnostic model reinforcement (reinforcement unit) guided by the visual explanation. For the visual explanation, we generate a counterfactual map that transforms an input sample to be identified as an intended target label. For example, a counterfactual map can localize hypothetical abnormalities within a normal brain image that may cause it to be diagnosed with Alzheimer's disease (AD). We believe that the generated counterfactual maps represent data-driven and model-induced knowledge about a target task, i.e., AD diagnosis using structural MRI, which can be a vital source of information to reinforce the generalization of the trained diagnostic model. To this end, we devise an attention-based feature refinement module with the guidance of the counterfactual maps. The explanation and reinforcement units are reciprocal and can be operated iteratively. Our proposed approach was validated via qualitative and quantitative analysis on the ADNI dataset. Its comprehensibility and fidelity were demonstrated through ablation studies and comparisons with existing methods.",0
"This paper presents a novel approach to improving the accuracy of diagnosing Alzheimer's disease using counterfactual reasoning. We propose a framework that integrates learn-explain-reinforce steps into a model trained on medical data from patients with Alzheimer's disease. By analyzing these scenarios, our method can better explain its decisions and provide insights into potential missed cases. Our experimental results demonstrate improved diagnostic performance compared to existing methods, making this framework a promising tool for early detection and intervention in dementia care. Our study has important implications for developing more reliable models in healthcare applications where accurate diagnoses are crucial.",1
"Currently, Chronic Kidney Disease (CKD) is experiencing a globally increasing incidence and high cost to health systems. A delayed recognition implies premature mortality due to progressive loss of kidney function. The employment of data mining to discover subtle patterns in CKD indicators would contribute achieving early diagnosis. This work presents the development and evaluation of an explainable prediction model that would support clinicians in the early diagnosis of CKD patients. The model development is based on a data management pipeline that detects the best combination of ensemble trees algorithms and features selected concerning classification performance. The results obtained through the pipeline equals the performance of the best CKD prediction models identified in the literature. Furthermore, the main contribution of the paper involves an explainability-driven approach that allows selecting the best prediction model maintaining a balance between accuracy and explainability. Therefore, the most balanced explainable prediction model of CKD implements an XGBoost classifier over a group of 4 features (packed cell value, specific gravity, albumin, and hypertension), achieving an accuracy of 98.9% and 97.5% with cross-validation technique and with new unseen data respectively. In addition, by analysing the model's explainability by means of different post-hoc techniques, the packed cell value and the specific gravity are determined as the most relevant features that influence the prediction results of the model. This small number of feature selected results in a reduced cost of the early diagnosis of CKD implying a promising solution for developing countries.",0
"This paper presents the development and evaluation of an explainable prediction model for chronic kidney disease patients using ensemble trees. The aim was to develop a transparent and interpretable predictive model that could identify high risk individuals among these patients. The methodology involved building different tree ensembles from publicly available datasets and evaluating their performance through cross validation techniques such as accuracy scores, precision, recall and F1-score metrics. The results were promising, showing high accuracy scores and robustness across different splits. In conclusion, our approach represents a valuable tool for clinicians who need accurate predictions in order to design better treatments tailored to specific patient needs. Further studies are however required to refine the models and adapt them to new populations and data sets. Overall, this work contributes to the growing field of interpretability and transparency in machine learning research.",1
"Cardiovascular diseases and their associated disorder of heart failure are one of the major death causes globally, being a priority for doctors to detect and predict its onset and medical consequences. Artificial Intelligence (AI) allows doctors to discover clinical indicators and enhance their diagnosis and treatments. Specifically, explainable AI offers tools to improve the clinical prediction models that experience poor interpretability of their results. This work presents an explainability analysis and evaluation of a prediction model for heart failure survival by using a dataset that comprises 299 patients who suffered heart failure. The model employs a data workflow pipeline able to select the best ensemble tree algorithm as well as the best feature selection technique. Moreover, different post-hoc techniques have been used for the explainability analysis of the model. The paper's main contribution is an explainability-driven approach to select the best prediction model for HF survival based on an accuracy-explainability balance. Therefore, the most balanced explainable prediction model implements an Extra Trees classifier over 5 selected features (follow-up time, serum creatinine, ejection fraction, age and diabetes) out of 12, achieving a balanced-accuracy of 85.1% and 79.5% with cross-validation and new unseen data respectively. The follow-up time is the most influencing feature followed by serum-creatinine and ejection-fraction. The explainable prediction model for HF survival presented in this paper would improve a further adoption of clinical prediction models by providing doctors with intuitions to better understand the reasoning of, usually, black-box AI clinical solutions, and make more reasonable and data-driven decisions.",0
"This study aimed to improve predictions regarding heart failure survival by leveraging explainable artificial intelligence (AI) techniques. A novel model was developed and integrated into existing frameworks, allowing healthcare professionals to gain deeper insight into why certain patients have higher mortality rates. Using transparency-focused algorithms like LIME, SHAP, and Grad-CAM, physicians can now identify the most important factors that influence patient outcomes. By providing interpretable results, clinical decision making may benefit from evidence-based treatment plans tailored to individual needs. In summary, this research enhances prediction accuracy for heart failure survival while enabling more informed discussions with patients about their prognosis. The application of XAI methodologies offers opportunities for improved care quality and greater efficiency in medical practices.",1
"Explainable artificial intelligence has been gaining attention in the past few years. However, most existing methods are based on gradients or intermediate features, which are not directly involved in the decision-making process of the classifier. In this paper, we propose a slot attention-based classifier called SCOUTER for transparent yet accurate classification. Two major differences from other attention-based methods include: (a) SCOUTER's explanation is involved in the final confidence for each category, offering more intuitive interpretation, and (b) all the categories have their corresponding positive or negative explanation, which tells ""why the image is of a certain category"" or ""why the image is not of a certain category."" We design a new loss tailored for SCOUTER that controls the model's behavior to switch between positive and negative explanations, as well as the size of explanatory regions. Experimental results show that SCOUTER can give better visual explanations in terms of various metrics while keeping good accuracy on small and medium-sized datasets.",0
"This paper proposes a new method for image recognition that combines attention mechanisms with classifiers to produce highly accurate results while also providing insight into how the model makes decisions. Our approach uses a slot attention mechanism to selectively attend to different regions of the input image based on their relevance to the task at hand. These attended features are then fed into a convolutional neural network (CNN) to generate predictions. To provide interpretability, we use a gradient-weighted class activation map (Grad-CAM) technique to visualize which parts of the image contributed most strongly to the final classification decision. We evaluate our approach on several benchmark datasets and show that it achieves state-of-the-art performance while also producing detailed explanations for its decisions. Overall, our work represents a step forward in the field of explainable image recognition, as it allows practitioners to gain deeper insights into how models make predictions without sacrificing accuracy.",1
"Frame sampling is a fundamental problem in video action recognition due to the essential redundancy in time and limited computation resources. The existing sampling strategy often employs a fixed frame selection and lacks the flexibility to deal with complex variations in videos. In this paper, we present a simple, sparse, and explainable frame sampler, termed as Motion-Guided Sampler (MGSampler). Our basic motivation is that motion is an important and universal signal that can drive us to adaptively select frames from videos. Accordingly, we propose two important properties in our MGSampler design: motion sensitive and motion uniform. First, we present two different motion representations to enable us to efficiently distinguish the motion-salient frames from the background. Then, we devise a motion-uniform sampling strategy based on the cumulative motion distribution to ensure the sampled frames evenly cover all the important segments with high motion salience. Our MGSampler yields a new principled and holistic sampling scheme, that could be incorporated into any existing video architecture. Experiments on five benchmarks demonstrate the effectiveness of our MGSampler over the previous fixed sampling strategies, and its generalization power across different backbones, video models, and datasets.",0
"This should focus on describing the problem solved by MGSampler and how our method works; I would like more emphasis on why we need explainability in action recognition rather than just technical details of our solution (e.g., mentioning potential real world applications). We want readers to quickly understand what MGSampler is and why they might care about it after reading only the abstract! Also please make sure to cite some references that support statements made within.  --- MGSampler is a novel sampling strategy designed specifically for video action recognition models to promote both accuracy and explainability. In todayâ€™s world where machine learning systems play crucial roles in decision making processes across many domains such as healthcare, finance, transportation etc., explainability has become increasingly important. In the context of action recognition from videos, which has numerous real world applications such as surveillance, robotics and entertainment, current methods often struggle to achieve high accuracies while maintaining transparency into their reasoning process. Existing approaches rely heavily on deep convolutional neural networks (CNNs) which can suffer from lack of interpretability due to their complex architectures. MGSampler addresses these challenges by introducing a new formulation based on clustering techniques and an iterative refinement algorithm to find representative frames for each class label in the dataset. Our approach is capable of generating more informative sampled subsets compared to previous work, leading to improved generalization capabilities while maintaining explainability via visualizing selected keyframes associated with different actions. Extensive experimental evaluations demonstrate the effectiveness of our proposed framework over multiple benchmark datasets. With further research, MGSampler has potential to revolutionize a wide range of applications involving video analysis and understanding. References [1] Bailer et al., â€œEnsembles of interpretable classifiers,â€ Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 8, no. 4, pp. e1276â€“e1291, 2018. [2] Addepalli et al., â€œComputer vision meets natural language processing: towards bridging the semantic gap,â€ Computer Vision â€“ ECCV 2016, Part VII, LNCS, vol. 9904, pp. 21â€“36, 2016.",1
"Adversarial training is a method for enhancing neural networks to improve the robustness against adversarial examples. Besides the security concerns of potential adversarial examples, adversarial training can also improve the performance of the neural networks, train robust neural networks, and provide interpretability for neural networks. In this work, we take the first step to introduce adversarial training in time series analysis by taking the finance field as an example. Rethinking existing researches of adversarial training, we propose the adaptively scaled adversarial training (ASAT) in time series analysis, by treating data at different time slots with time-dependent importance weights. Experimental results show that the proposed ASAT can improve both the accuracy and the adversarial robustness of neural networks. Besides enhancing neural networks, we also propose the dimension-wise adversarial sensitivity indicator to probe the sensitivities and importance of input dimensions. With the proposed indicator, we can explain the decision bases of black box neural networks.",0
"Effective training of deep neural networks (DNNs) on time series data remains challenging due to various complications such as non-stationarity and strong correlations between adjacent steps. State-of-the art techniques attempt to address these issues by designing specialized architectures or employing advanced regularization methods that often require extensive tuning or domain knowledge. We introduce ASAT, an adaptively scaled adversarial training method tailored specifically for time series predictions. Our framework incorporates a lightweight temporal discriminator trained alongside the main predictor network and adjusts both scales according to the sequence complexity, thereby achieving better calibration and improved generalization performance. Our comprehensive experiments across several real-world datasets demonstrate substantial improvements over contemporary approaches while maintaining simplicity and efficiency.",1
"Structure learning offers an expressive, versatile and explainable approach to causal and mechanistic modeling of complex biological data. We present wiseR, an open source application for learning, evaluating and deploying robust causal graphical models using graph neural networks and Bayesian networks. We demonstrate the utility of this application through application on for biomarker discovery in a COVID-19 clinical dataset.",0
"In recent years, there has been growing interest in developing methods that can automatically learn structural equation models (SEMs) from data. SEMs provide a powerful tool for understanding complex relationships among variables by allowing researchers to model both linear and nonlinear relations between variables while accounting for measurement error and latent constructs. However, existing approaches often require strong assumptions or cannot handle more complicated relationships, making them less applicable to real-world problems. To address these limitations, we propose an end-to-end framework called WiseR which integrates state-of-the art deep learning techniques with structural equations to jointly estimate the underlying structural equation model and generate predictions on new observations. Our approach enables automatic discovery of complex relationship structures, handling multiple measurement occasions per variable and missingness, and can even incorporate external knowledge through priors embedded within Bayesian neural networks. Experiments demonstrate the effectiveness of our method compared to several state-of-the-art methods across a variety of settings. Overall, the proposed framework provides an efficient and flexible solution for uncovering meaningful relationships in large datasets without requiring prior domain expertise.",1
"Machine Learning (ML) has emerged as an attractive and viable technique to provide effective solutions for a wide range of application domains. An important application domain is vehicular networks wherein ML-based approaches are found to be very useful to address various problems. The use of wireless communication between vehicular nodes and/or infrastructure makes it vulnerable to different types of attacks. In this regard, ML and its variants are gaining popularity to detect attacks and deal with different kinds of security issues in vehicular communication. In this paper, we present a comprehensive survey of ML-based techniques for different security issues in vehicular networks. We first briefly introduce the basics of vehicular networks and different types of communications. Apart from the traditional vehicular networks, we also consider modern vehicular network architectures. We propose a taxonomy of security attacks in vehicular networks and discuss various security challenges and requirements. We classify the ML techniques developed in the literature according to their use in vehicular network applications. We explain the solution approaches and working principles of these ML techniques in addressing various security challenges and provide insightful discussion. The limitations and challenges in using ML-based methods in vehicular networks are discussed. Finally, we present observations and lessons learned before we conclude our work.",0
"This survey provides a comprehensive overview of machine learning applications for improving security in vehicular networks (VANETs). VANETs are wireless communication systems that enable vehicles to communicate with each other and with infrastructure devices such as roadside units. These networks have great potential for enhancing traffic safety and efficiency but also face significant challenges related to security threats such as tampering, eavesdropping, and denial of service attacks. To address these challenges, researchers have proposed numerous machine learning techniques aimed at detecting and mitigating malicious behavior while ensuring privacy protection. This survey presents an extensive review of relevant literature from different perspectives, including network security analysis, attack detection and prevention methods, anomaly identification approaches, intrusion detection systems, and deep learning models. Additionally, we discuss open issues and future directions in this area to encourage further exploration. Overall, our work contributes to understanding recent advances in applying machine learning to enhance security in VANETs.",1
"Increased drone proliferation in civilian and professional settings has created new threat vectors for airports and national infrastructures. The economic damage for a single major airport from drone incursions is estimated to be millions per day. Due to the lack of diverse drone training data, accurate training of deep learning detection algorithms under scarce data is an open challenge. Existing methods largely rely on collecting diverse and comprehensive experimental drone footage data, artificially induced data augmentation, transfer and meta-learning, as well as physics-informed learning. However, these methods cannot guarantee capturing diverse drone designs and fully understanding the deep feature space of drones. Here, we show how understanding the general distribution of the drone data via a Generative Adversarial Network (GAN) and explaining the missing features using Topological Data Analysis (TDA) - can allow us to acquire missing data to achieve rapid and more accurate learning. We demonstrate our results on a drone image dataset, which contains both real drone images as well as simulated images from computer-aided design. When compared to random data collection (usual practice - discriminator accuracy of 94.67\% after 200 epochs), our proposed GAN-TDA informed data collection method offers a significant 4\% improvement (99.42\% after 200 epochs). We believe that this approach of exploiting general data distribution knowledge form neural networks can be applied to a wide range of scarce data open challenges.",0
"In recent years, deep learning has emerged as a powerful tool for processing data and making predictions across many domains. However, deep learning algorithms often require large amounts of data to train effectively. This can present a challenge in some cases where data may be scarce or difficult to collect. In this paper, we propose a novel method for training deep learning models using scarce data. Our approach involves creating a generalized distribution space that captures the essence of the data and enables more effective model training. We apply our method to the task of drone navigation, which presents unique challenges due to the need for real-time performance and high accuracy in uncertain environments. Our experiments demonstrate that our proposed method significantly outperforms traditional methods, achieving better results on standard benchmarks while using fewer resources. Overall, our work offers a promising new direction for developing deep learning solutions even in situations where data is limited.",1
"Recently, there has been an increasing number of efforts to introduce models capable of generating natural language explanations (NLEs) for their predictions on vision-language (VL) tasks. Such models are appealing, because they can provide human-friendly and comprehensive explanations. However, there is a lack of comparison between existing methods, which is due to a lack of re-usable evaluation frameworks and a scarcity of datasets. In this work, we introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable vision-language tasks that establishes a unified evaluation framework and provides the first comprehensive comparison of existing approaches that generate NLEs for VL tasks. It spans four models and three datasets and both automatic metrics and human evaluation are used to assess model-generated explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs (over 430k instances). We also propose a new model that combines UNITER, which learns joint embeddings of images and text, and GPT-2, a pre-trained language model that is well-suited for text generation. It surpasses the previous state of the art by a large margin across all datasets. Code and data are available here: https://github.com/maximek3/e-ViL.",0
"In recent years, there has been significant progress in developing models that can perform complex vision-language tasks such as image captioning, visual question answering (Q&A), and scene graph generation. However, despite these advances, evaluating the quality of explanations generated by these systems remains a challenge. This is because existing metrics primarily focus on objective measures like accuracy rather than subjective aspects such as clarity and coherence. To address this gap, we present e-ViL, a new dataset and benchmark for natural language explanations in vision-language tasks. Our dataset includes diverse annotations from human subjects who were asked to provide explanations for their answers to different types of questions related to images. We use these annotations to create three distinct evaluation tasks designed to measure the quality of explanations generated by algorithms. We demonstrate that our proposed methods significantly outperform baseline models across all benchmark tasks and show promising results towards building more explainable artificial intelligence (AI) agents. Overall, our work presents a valuable resource for researchers interested in improving the performance and transparency of vision-language systems.",1
"Counterfactual explanation methods interpret the outputs of a machine learning model in the form of ""what-if scenarios"" without compromising the fidelity-interpretability trade-off. They explain how to obtain a desired prediction from the model by recommending small changes to the input features, aka recourse. We believe an actionable recourse should be created based on sound counterfactual explanations originating from the distribution of the ground-truth data and linked to the domain knowledge. Moreover, it needs to preserve the coherency between changed/unchanged features while satisfying user/domain-specified constraints. This paper introduces CARE, a modular explanation framework that addresses the model- and user-level desiderata in a consecutive and structured manner. We tackle the existing requirements by proposing novel and efficient solutions that are formulated in a multi-objective optimization framework. The designed framework enables including arbitrary requirements and generating counterfactual explanations and actionable recourse by choice. As a model-agnostic approach, CARE generates multiple, diverse explanations for any black-box model in tabular classification and regression settings. Several experiments on standard data sets and black-box models demonstrate the effectiveness of our modular framework and its superior performance compared to the baselines.",0
"This paper addresses problems that arise in decision making when there are multiple competing hypotheses with respect to the causes of failure and how they might have been avoided. These uncertainties lead to difficulties both in identifying actions that could have led to different outcomes and evaluating their potential effectiveness before and after implementation. We introduce the novel concept of sound counterfactual explanation (SCE) as a tool to clarify and evaluate alternative courses of action under uncertainty and limited data availability. By considering all relevant evidence and factors and applying Bayesian reasoning within coherence bounds, SCE provides guidance towards a preferred choice among alternatives, leading to improved recourses selection and increased chances of success.",1
"This work employs a pre-trained, multi-view Convolutional Neural Network (CNN) with a spatial attention block to optimise knee injury detection. An open-source Magnetic Resonance Imaging (MRI) data set with image-level labels was leveraged for this analysis. As MRI data is acquired from three planes, we compare our technique using data from a single-plane and multiple planes (multi-plane). For multi-plane, we investigate various methods of fusing the planes in the network. This analysis resulted in the novel 'MPFuseNet' network and state-of-the-art Area Under the Curve (AUC) scores for detecting Anterior Cruciate Ligament (ACL) tears and Abnormal MRIs, achieving AUC scores of 0.977 and 0.957 respectively. We then developed an objective metric, Penalised Localisation Accuracy (PLA), to validate the model's localisation ability. This metric compares binary masks generated from Grad-Cam output and the radiologist's annotations on a sample of MRIs. We also extracted explainability features in a model-agnostic approach that were then verified as clinically relevant by the radiologist.",0
"This research focuses on optimizing knee injury detection using spatial attention mechanisms, which can improve localization accuracy by concentrating model capacity selectively within regions that contain more diagnostic content relevant to specific pathologies. To evaluate these improvements, we present three new datasets consisting of 742 MRIs from patients diagnosed with meniscal tears, cruciate ligament injuries, or patellar tendinopathy. These datasets represent diverse clinical populations and were carefully curated to ensure high image quality and accurate radiological annotations.  In our experiments, we show how incorporating spatial attention significantly enhances both localization performance and sensitivity for detecting small structures like meniscus tears compared to traditional MRI interpretation methods. Furthermore, we demonstrate that deep learning models optimized for these tasks achieve competitive scores with human experts, despite limitations imposed by image resolution and training size. Our results suggest that combining appropriate attention techniques with increased data resources could bring automated detection systems closer to expert human levels. Additionally, we provide detailed comparisons across multiple architectures and visualize the internal attentional patterns learned during optimization, offering insights into network behavior that may inform future design decisions. Ultimately, these findings have important implications for developing novel tools capable of accurately identifying joint injuries at earlier stages while reducing healthcare costs through increased efficiency.",1
"The training of artificial neural networks (ANNs) with rectified linear unit (ReLU) activation via gradient descent (GD) type optimization schemes is nowadays a common industrially relevant procedure. Till this day in the scientific literature there is in general no mathematical convergence analysis which explains the numerical success of GD type optimization schemes in the training of ANNs with ReLU activation. GD type optimization schemes can be regarded as temporal discretization methods for the gradient flow (GF) differential equations associated to the considered optimization problem and, in view of this, it seems to be a natural direction of research to first aim to develop a mathematical convergence theory for time-continuous GF differential equations and, thereafter, to aim to extend such a time-continuous convergence theory to implementable time-discrete GD type optimization methods. In this article we establish two basic results for GF differential equations in the training of fully-connected feedforward ANNs with one hidden layer and ReLU activation. In the first main result of this article we establish in the training of such ANNs under the assumption that the probability distribution of the input data of the considered supervised learning problem is absolutely continuous with a bounded density function that every GF differential equation admits for every initial value a solution which is also unique among a suitable class of solutions. In the second main result of this article we prove in the training of such ANNs under the assumption that the target function and the density function of the probability distribution of the input data are piecewise polynomial that every non-divergent GF trajectory converges with an appropriate rate of convergence to a critical point and that the risk of the non-divergent GF trajectory converges with rate 1 to the risk of the critical point.",0
"Deep learning has become one of the most popular fields in modern machine learning due to its ability to achieve state-of-the-art performance on a wide range of tasks. Artificial neural networks (ANNs) form the backbone of many deep learning algorithms and their training relies heavily on optimization techniques such as stochastic gradient descent (SGD). In recent years, there has been increasing interest in understanding the theoretical properties of SGD in the context of training ANNs with Rectified Linear Unit (ReLU) activations. This paper addresses some key open questions regarding existence, uniqueness, and convergence rates of gradient flows in this setting. Our results provide new insights into the behavior of these systems and have important implications for the design and analysis of deep learning models.",1
"In the field of autonomous driving and robotics, point clouds are showing their excellent real-time performance as raw data from most of the mainstream 3D sensors. Therefore, point cloud neural networks have become a popular research direction in recent years. So far, however, there has been little discussion about the explainability of deep neural networks for point clouds. In this paper, we propose a point cloud-applicable explainability approach based on local surrogate model-based method to show which components contribute to the classification. Moreover, we propose quantitative fidelity validations for generated explanations that enhance the persuasive power of explainability and compare the plausibility of different existing point cloud-applicable explainability methods. Our new explainability approach provides a fairly accurate, more semantically coherent and widely applicable explanation for point cloud classification tasks. Our code is available at https://github.com/Explain3D/LIME-3D",0
"In recent years, deep learning models have become increasingly popular for processing point cloud data due to their ability to capture complex patterns and relationships within large datasets. However, these models can often suffer from poor explainability, making it difficult for practitioners to interpret their outputs and understand how they make predictions. To address this issue, research has focused on developing methods that generate surrogate models of black box neural networks using different techniques such as linear regression or decision trees. These surrogates act as interpretable approximations of the original model, allowing users to gain insights into its inner workings without sacrificing accuracy. This review aims to provide a comprehensive overview of the state-of-the-art approaches used for generating surrogate models to explain the behavior of point cloud neural network models. We discuss several methodologies for generating surrogate explanations, including feature importance analysis, partial dependence plots, and SHAP values. By examining the strengths and limitations of each approach, we identify promising directions for future research in this rapidly evolving field. Overall, this study underscores the significant potential for improving transparency and accountability in the use of machine learning by enhancing our understanding of how surrogate models can effectively support explainability for point cloud neural networks.",1
"Deep learning models have recently demonstrated remarkable results in a variety of tasks, which is why they are being increasingly applied in high-stake domains, such as industry, medicine, and finance. Considering that automatic predictions in these domains might have a substantial impact on the well-being of a person, as well as considerable financial and legal consequences to an individual or a company, all actions and decisions that result from applying these models have to be accountable. Given that a substantial amount of data that is collected in high-stake domains are in the form of time series, in this paper we examine the current state of eXplainable AI (XAI) methods with a focus on approaches for opening up deep learning black boxes for the task of time series classification. Finally, our contribution also aims at deriving promising directions for future work, to advance XAI for deep learning on time series data.",0
"In recent years there has been growing interest in using explainable artificial intelligence (XAI) methods to increase transparency, interpretability, and trustworthiness of machine learning models in various domains such as healthcare, finance, robotics, and security, among others. In particular, neural time series classification plays a critical role in many applications including speech recognition, EEG analysis for brain-computer interfaces, video surveillance, medical diagnosis, financial forecasting, and weather prediction, just to name a few. Therefore, developing effective, interpretable, and efficient XAI techniques for neural time series classification is essential to guarantee reliable outcomes in these important areas. This review provides a comprehensive overview of state-of-the-art XAI methods applied to neural time series classification problems, discussing their strengths, limitations, and future directions. We hope that researchers, practitioners, and policymakers alike will find our review timely and informative as they consider adopting XAI approaches into practice.",1
"Explainable reinforcement learning allows artificial agents to explain their behavior in a human-like manner aiming at non-expert end-users. An efficient alternative of creating explanations is to use an introspection-based method that transforms Q-values into probabilities of success used as the base to explain the agent's decision-making process. This approach has been effectively used in episodic and discrete scenarios, however, to compute the probability of success in non-episodic and more complex environments has not been addressed yet. In this work, we adapt the introspection method to be used in a non-episodic task and try it in a continuous Atari game scenario solved with the Rainbow algorithm. Our initial results show that the probability of success can be computed directly from the Q-values for all possible actions.",0
"Title: Explainable Deep Reinforcement Learning Using Introspection in a Non-Episodic Task  Abstract: Reinforcement learning (RL) algorithms have achieved significant successes in recent years across various domains, including computer games, robotics, finance, and healthcare. However, many RL agents still struggle with complex tasks that require reasoning, planning, hierarchical decision making, and multimodal interaction with environments. In addition, traditional deep reinforcement learning approaches often suffer from issues related to credit assignment, which makes it challenging to identify and explain the factors contributing to agent behavior. To address these limitations, we propose Explainable Deep Reinforcement Learning using Introspection (EDRILL), a novel approach that leverages introspection mechanisms for enhanced interpretability and improved performance on non-episodic tasks. EDRILL allows us to efficiently reason about different dimensions of agent experience by exploiting introspective insights at every iteration during training. We evaluate our methodology using three popular benchmarks: two continuous control problems and one text generation task. Experimental results demonstrate that EDRILL outperforms other state-of-the art methods while providing intrinsic explanations of agent decisions. Our findings highlight the potential benefits of integrating introspection into RL models, paving the way for more transparent, human-like AI systems that can learn and adapt to dynamic environments effectively.",1
"Implementing systems based on Machine Learning to detect fraud and other Non-Technical Losses (NTL) is challenging: the data available is biased, and the algorithms currently used are black-boxes that cannot be either easily trusted or understood by stakeholders. This work explains our human-in-the-loop approach to mitigate these problems in a real system that uses a supervised model to detect Non-Technical Losses (NTL) for an international utility company from Spain. This approach exploits human knowledge (e.g. from the data scientists or the company's stakeholders) and the information provided by explanatory methods to guide the system during the training process. This simple, efficient method that can be easily implemented in other industrial projects is tested in a real dataset and the results show that the derived prediction model is better in terms of accuracy, interpretability, robustness and flexibility.",0
"This study presents a human-in-the-loop approach that utilizes explainability methods to improve natural language detection (NLTD) accuracy. By incorporating interpretability techniques into the system design process, we aim to increase transparency and trustworthiness in automated decision making while maintaining high performance levels. In addition to enhancing the understanding of how these systems make predictions, our methodology allows humans to provide input throughout the model training process, fine-tuning models and calibrating thresholds as necessary. We demonstrate the effectiveness of our proposed framework through extensive evaluation experiments using benchmark datasets, providing quantitative analysis of both classification performance and model interpretability measures. Our findings suggest that integrating human feedback can effectively mitigate some of the limitations associated with traditional black box approaches in NLP applications such as email spam filtering, sentiment analysis, or authorship attribution. Overall, the use of explainability and human interaction represents a promising direction towards building more reliable and transparent NLP tools, particularly in real-world scenarios where user engagement may be required.",1
"Despite the great progress of person re-identification (ReID) with the adoption of Convolutional Neural Networks, current ReID models are opaque and only outputs a scalar distance between two persons. There are few methods providing users semantically understandable explanations for why two persons are the same one or not. In this paper, we propose a post-hoc method, named Attribute-guided Metric Distillation (AMD), to explain existing ReID models. This is the first method to explore attributes to answer: 1) what and where the attributes make two persons different, and 2) how much each attribute contributes to the difference. In AMD, we design a pluggable interpreter network for target models to generate quantitative contributions of attributes and visualize accurate attention maps of the most discriminative attributes. To achieve this goal, we propose a metric distillation loss by which the interpreter learns to decompose the distance of two persons into components of attributes with knowledge distilled from the target model. Moreover, we propose an attribute prior loss to make the interpreter generate attribute-guided attention maps and to eliminate biases caused by the imbalanced distribution of attributes. This loss can guide the interpreter to focus on the exclusive and discriminative attributes rather than the large-area but common attributes of two persons. Comprehensive experiments show that the interpreter can generate effective and intuitive explanations for varied models and generalize well under cross-domain settings. As a by-product, the accuracy of target models can be further improved with our interpreter.",0
"This paper presents a novel approach to person re-identification using attribute-guided metric distillation. We propose a framework that incorporates human knowledge into machine learning models by utilizing attribute labels to guide feature extraction and distance computation. Our method outperforms state-of-the-art approaches while providing greater explainability and interpretability. To evaluate our model, we conduct experiments on public datasets, demonstrating improved performance compared to baseline methods. Additionally, we provide comprehensive ablation studies and visualizations to showcase the effectiveness of each component in our pipeline. Overall, our contributions aim to bridge the gap between humans and machines in computer vision tasks, enabling more transparent and accurate results.",1
"Neural Ordinary Differential Equations (NODEs) use a neural network to model the instantaneous rate of change in the state of a system. However, despite their apparent suitability for dynamics-governed time-series, NODEs present a few disadvantages. First, they are unable to adapt to incoming data points, a fundamental requirement for real-time applications imposed by the natural direction of time. Second, time series are often composed of a sparse set of measurements that could be explained by many possible underlying dynamics. NODEs do not capture this uncertainty. In contrast, Neural Processes (NPs) are a family of models providing uncertainty estimation and fast data adaptation but lack an explicit treatment of the flow of time. To address these problems, we introduce Neural ODE Processes (NDPs), a new class of stochastic processes determined by a distribution over Neural ODEs. By maintaining an adaptive data-dependent distribution over the underlying ODE, we show that our model can successfully capture the dynamics of low-dimensional systems from just a few data points. At the same time, we demonstrate that NDPs scale up to challenging high-dimensional time-series with unknown latent dynamics such as rotating MNIST digits.",0
"Recent advances have led to a resurgence of interest in ordinary differential equations (ODEs) as models in neuroscience due to their ability to capture nonlinear dynamics across multiple time scales, which can better represent many aspects of neural systems than the traditional linear models used previously. Here we explore the use of deep learning methods based on artificial neural networks (ANNs), specifically Neural Ordinary Differential Equations (NODEs) processes, in modeling dynamic phenomena observed in neurophysiological data, such as neural activity in animals engaged in sensory tasks like visual navigation. We show how NODEs can accurately capture these complex interactions and provide insights into both biological mechanisms and perceptual decision making that would otherwise go unnoticed using classical mathematical approaches. Additionally, we highlight new developments in the study of sensory integration and decision making which promise further progress toward our understanding of brain function. By leveraging the power of modern computing resources and recent breakthroughs in machine learning techniques, we expect that NODE processes will become a valuable tool for researchers studying both physiology and behavior. Our work represents a significant step forward in the application of artificial intelligence technologies to the field of neuroscience by providing both experimentally testable predictions and novel explanatory principles underlying the computational functions performed by neuronal populations in vivo. This manuscript provides a review of the most recent literature on the topic of NODEs in neuroscientific applications, describing relevant past contributions, current state-of-the-art implementations and future directions for the methodology at large. In summary, this paper focuses on introducing some key concepts required t",1
"The nonlocal-based blocks are designed for capturing long-range spatial-temporal dependencies in computer vision tasks. Although having shown excellent performance, they still lack the mechanism to encode the rich, structured information among elements in an image or video. In this paper, to theoretically analyze the property of these nonlocal-based blocks, we provide a new perspective to interpret them, where we view them as a set of graph filters generated on a fully-connected graph. Specifically, when choosing the Chebyshev graph filter, a unified formulation can be derived for explaining and analyzing the existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage, double attention block). Furthermore, by concerning the property of spectral, we propose an efficient and robust spectral nonlocal block, which can be more robust and flexible to catch long-range dependencies when inserted into deep neural networks than the existing nonlocal blocks. Experimental results demonstrate the clear-cut improvements and practical applicabilities of our method on image classification, action recognition, semantic segmentation, and person re-identification tasks.",0
"In recent years, deep learning has revolutionized many fields due to its ability to learn complex patterns from large datasets using neural networks. However, designing efficient architectures that can handle high resolution inputs and capture nonlinear relationships between input variables remains challenging. To address these issues, we propose a novel method called ""Unified Nonlocal Block"" (UNB) which adaptively integrates local and global information by unifying nonlocal blocks. This approach simplifies existing block designs while improving their performance on standard benchmark datasets such as CIFAR-10/100, ImageNet, and Cityscapes. UNB consistently achieves state-of-the-art results under competitive parameters, showing the effectiveness and generalization power of our proposed method. Additionally, our findings demonstrate how regularizing attention mechanisms within dense network structures enables efficient training without excessive computational overhead. Overall, our work represents a significant step towards understanding how attention can guide model behavior effectively, paving the way for future research into improved neural network models.",1
"An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong conclusions if applied incorrectly. We highlight many general pitfalls of ML model interpretation, such as using interpretation techniques in the wrong context, interpreting models that do not generalize well, ignoring feature dependencies, interactions, uncertainty estimates and issues in high-dimensional settings, or making unjustified causal interpretations, and illustrate them with examples. We focus on pitfalls for global methods that describe the average model behavior, but many pitfalls also apply to local methods that explain individual predictions. Our paper addresses ML practitioners by raising awareness of pitfalls and identifying solutions for correct model interpretation, but also addresses ML researchers by discussing open issues for further research.",0
This sounds like an interesting paper! Are you looking for an alternative model agnostic interpretation method? We have several that might work well for your use case. Please give me some details on what you need so I can better assist you.,1
"The existing particle image velocimetry (PIV) do not consider the curvature effect of the non-straight particle trajectory, because it seems to be impossible to obtain the curvature information from a pair of particle images. As a result, the computed vector underestimates the real velocity due to the straight-line approximation, that further causes a systematic error for the PIV instrument. In this work, the particle curved trajectory between two recordings is firstly explained with the streamline segment of a steady flow (diffeomorphic transformation) instead of a single vector, and this idea is termed as diffeomorphic PIV. Specifically, a deformation field is introduced to describe the particle displacement, i.e., we try to find the optimal velocity field, of which the corresponding deformation vector field agrees with the particle displacement. Because the variation of the deformation function can be approximated with the variation of the velocity function, the diffeomorphic PIV can be implemented as iterative PIV. That says, the diffeomorphic PIV warps the images with deformation vector field instead of the velocity, and keeps the rest as same as iterative PIVs. Two diffeomorphic deformation schemes -- forward diffeomorphic deformation interrogation (FDDI) and central diffeomorphic deformation interrogation (CDDI) -- are proposed. Tested on synthetic images, the FDDI achieves significant accuracy improvement across different one-pass displacement estimators (cross-correlation, optical flow, deep learning flow). Besides, the results on three real PIV image pairs demonstrate the non-negligible curvature effect for CDI-based PIV, and our FDDI provides larger velocity estimation (more accurate) in the fast curvy streamline areas. The accuracy improvement of the combination of FDDI and accurate dense estimator means that our diffeomorphic PIV paves a new way for complex flow measurement.",0
"Here is a short abstract that describes your paper: ""Diffeomorphic particle image velocimetry (PIV) represents one potential solution to challenges associated with classical PIV techniques. To date, most research into PIV has focused on analyzing static images of flows by tracking patterns placed in the field of view. These methods face limitations due to constraints imposed by camera position, limited resolution and optical distortion, as well as problems stemming from the use of artificial tracer particles. By contrast, diffeomorphic registration offers new opportunities for processing experimental data which could potentially mitigate many of these issues. In this article we review advances made in diffeomorphic velocity estimation over recent years within both academic and commercial arenas."" Please note that I do not have access to the actual content of the paper so I may have made some assumptions. Can you give me more details? Or perhaps ask another question?",1
"We present a new multi-objective optimization approach for synthesizing interpretations that ""explain"" the behavior of black-box machine learning models. Constructing human-understandable interpretations for black-box models often requires balancing conflicting objectives. A simple interpretation may be easier to understand for humans while being less precise in its predictions vis-a-vis a complex interpretation. Existing methods for synthesizing interpretations use a single objective function and are often optimized for a single class of interpretations. In contrast, we provide a more general and multi-objective synthesis framework that allows users to choose (1) the class of syntactic templates from which an interpretation should be synthesized, and (2) quantitative measures on both the correctness and explainability of an interpretation. For a given black-box, our approach yields a set of Pareto-optimal interpretations with respect to the correctness and explainability measures. We show that the underlying multi-objective optimization problem can be solved via a reduction to quantitative constraint solving, such as weighted maximum satisfiability. To demonstrate the benefits of our approach, we have applied it to synthesize interpretations for black-box neural-network classifiers. Our experiments show that there often exists a rich and varied set of choices for interpretations that are missed by existing approaches.",0
"In recent years, there has been significant interest in developing models that can make predictions without requiring explicit knowledge of their inner workings. These black-box models have gained widespread adoption across various domains due to their ability to accurately capture complex relationships among inputs and outputs. However, one major limitation of these models is the lack of interpretability, which makes it difficult for practitioners to gain insights into how they operate. In this paper, we present a novel approach for synthesizing multiple interpretations from a given black-box model, each of which captures different aspects of the underlying structure while ensuring that all of them are equally good at making predictions. Specifically, our method relies on optimizing for multiple objectives simultaneously using the concept of Pareto optimization. We evaluate our approach on several benchmark datasets and demonstrate its effectiveness compared to state-of-the-art methods in generating diverse yet accurate interpretations of black-box models. Our results show that our framework provides valuable insights into the behavior of these opaque models, helping users better understand and trust their predictions.",1
"In recent years, the US has experienced an opioid epidemic with an unprecedented number of drugs overdose deaths. Research finds such overdose deaths are linked to neighborhood-level traits, thus providing opportunity to identify effective interventions. Typically, techniques such as Ordinary Least Squares (OLS) or Maximum Likelihood Estimation (MLE) are used to document neighborhood-level factors significant in explaining such adverse outcomes. These techniques are, however, less equipped to ascertain non-linear relationships between confounding factors. Hence, in this study we apply machine learning based techniques to identify opioid risks of neighborhoods in Delaware and explore the correlation of these factors using Shapley Additive explanations (SHAP). We discovered that the factors related to neighborhoods environment, followed by education and then crime, were highly correlated with higher opioid risk. We also explored the change in these correlations over the years to understand the changing dynamics of the epidemic. Furthermore, we discovered that, as the epidemic has shifted from legal (i.e., prescription opioids) to illegal (e.g.,heroin and fentanyl) drugs in recent years, the correlation of environment, crime and health related variables with the opioid risk has increased significantly while the correlation of economic and socio-demographic variables has decreased. The correlation of education related factors has been higher from the start and has increased slightly in recent years suggesting a need for increased awareness about the opioid epidemic.",0
"In recent years, the United States has experienced a surge in opioid use and related deaths. This epidemic has affected millions of lives and prompted researchers to explore potential drivers behind the crisis. However, identifying these factors remains challenging due to the complex nature of addiction and the many interrelated social, economic, environmental, and policy factors that may contribute to opioid abuse. To address this challenge, we propose a novel approach utilizing machine learning techniques to analyze large datasets and identify patterns in opioid consumption and overdose rates across different demographic groups and geographical regions. By analyzing publicly available data on opioid prescriptions, drug seizures, emergency department visits, and mortality rates, our study aimed to uncover relationships between opioid use, demographics (e.g., age, race/ethnicity), socioeconomic status, healthcare accessibility, employment levels, and other relevant variables. We developed predictive models to assess how changes in these factors might affect future opioid abuse trends and inform effective policies to reduce harmful consequences. Our findings highlight several key risk factors associated with increased opioid use, including higher unemployment rates, reduced access to medical care, poor mental health outcomes, and disproportionate opioid prescription burdens among specific populations such as Native Americans. These insights provide valuable information for policymakers seeking targeted solutions to combat the national opioid crisis. Overall, our work demonstrates the value of machine learning approaches in generating meaningful evidence to guide effective responses to complex health issues.",1
"In recent years, the connections between deep residual networks and first-order Ordinary Differential Equations (ODEs) have been disclosed. In this work, we further bridge the deep neural architecture design with the second-order ODEs and propose a novel reversible neural network, termed as m-RevNet, that is characterized by inserting momentum update to residual blocks. The reversible property allows us to perform backward pass without access to activation values of the forward pass, greatly relieving the storage burden during training. Furthermore, the theoretical foundation based on second-order ODEs grants m-RevNet with stronger representational power than vanilla residual networks, which potentially explains its performance gains. For certain learning scenarios, we analytically and empirically reveal that our m-RevNet succeeds while standard ResNet fails. Comprehensive experiments on various image classification and semantic segmentation benchmarks demonstrate the superiority of our m-RevNet over ResNet, concerning both memory efficiency and recognition performance.",0
"In recent years, there has been growing interest in developing deep neural networks that can process sequential data while preserving their temporal structure. Many existing approaches focus on designing architectures that explicitly model temporal dependencies, often at the cost of increased computational complexity and reduced interpretability. Here we present an alternative approach based on momentum that makes use of reversibility. Our method builds upon simple yet effective units - gates - whose states preserve information over time by incorporating both previous hidden state activations as well as incoming external inputs. These gates evolve according to continuous dynamics, which capture patterns from past experiences through gradient descent optimization. We show that our network achieves comparable performance on several challenging benchmark datasets, including polyphonic music generation and human motion prediction tasks. To facilitate further research into deep reversible models for sequence processing, we release all code associated with these experiments as open source.",1
"Unscheduled power disturbances cause severe consequences both for customers and grid operators. To defend against such events, it is necessary to identify the causes of interruptions in the power distribution network. In this work, we focus on the power grid of a Norwegian community in the Arctic that experiences several faults whose sources are unknown. First, we construct a data set consisting of relevant meteorological data and information about the current power quality logged by power-quality meters. Then, we adopt machine-learning techniques to predict the occurrence of faults. Experimental results show that both linear and non-linear classifiers achieve good classification performance. This indicates that the considered power-quality and weather variables explain well the power disturbances. Interpreting the decision process of the classifiers provides valuable insights to understand the main causes of disturbances. Traditional features selection methods can only indicate which are the variables that, on average, mostly explain the fault occurrences in the dataset. Besides providing such a global interpretation, it is also important to identify the specific set of variables that explain each individual fault. To address this challenge, we adopt a recent technique to interpret the decision process of a deep learning model, called Integrated Gradients. The proposed approach allows to gain detailed insights on the occurrence of a specific fault, which are valuable for the distribution system operators to implement strategies to prevent and mitigate power disturbances.",0
"Power grid failures can have severe consequences, including economic losses, disruptions to critical infrastructure, and even loss of life. Vulnerable power grids require special attention to maintain reliability and prevent costly outages. In recent years, machine learning has emerged as a powerful tool for detecting and interpreting faults within these systems. This paper presents a novel methodology that combines advanced signal processing techniques with deep neural networks to identify potential faults early on, allowing for proactive maintenance and mitigation strategies. Our approach leverages data from sensors deployed throughout the system to characterize normal behavior patterns, which are then used to identify anomalies indicative of impending failure. Experiments conducted on real-world power grid datasets demonstrate significant improvements over traditional methods in terms of accuracy, speed, and scalability. These results have important implications for the development of resilient and sustainable energy systems that meet societyâ€™s increasing demands while minimizing environmental impacts. Overall, our work represents an important step towards building smarter and more reliable power grids through the synergistic integration of physical models with artificial intelligence algorithms.",1
"So far, multi-label classification algorithms have been evaluated using statistical methods that do not consider the semantics of the considered classes and that fully depend on abstract computations such as Bayesian Reasoning. Currently, there are several attempts to develop ontology-based methods for a better assessment of supervised classification algorithms. In this research paper, we define a novel approach that aligns expected labels with predicted labels in multi-label classification using ontology-driven feature-based semantic similarity measures and we use it to develop a method for creating precise confusion matrices for a more effective evaluation of multi-label classification algorithms.",0
"This paper presents a novel method for constructing confusion matrices for multi-label classification algorithms that utilizes knowledge-based approaches and semantic similarity measures. Traditional methods for building confusion matrices can suffer from limitations such as high computational costs, sensitivity to parameter tuning, and poor handling of label noise and ambiguity. To address these issues, we propose a new approach that leverages domain expertise and linguistic resources to derive more accurate confusion matrices. Our method involves several key steps: extracting features from labeled data using natural language processing techniques; computing semantic distances between labels based on lexical resources; aggregating similar labels into clusters using hierarchical clustering; and applying confusion matrix construction rules tailored to each cluster type. We evaluate our approach on a diverse set of datasets across different domains and show that it outperforms state-of-the-art baseline methods in terms of accuracy and robustness while maintaining scalability. Additionally, we demonstrate how our framework allows for fine-grained analysis of classification performance and facilitates model selection by providing interpretable results. Overall, our work contributes to advancing the field ofmulti-label learning by developing innovative solutionsforconstructingconfusiomatrices, enablingmore effective machine learning applications in complex real-world scenarios.",1
"Owing to tremendous performance improvements in data-intensive domains, machine learning (ML) has garnered immense interest in the research community. However, these ML models turn out to be black boxes, which are tough to interpret, resulting in a direct decrease in productivity. Local Interpretable Model-Agnostic Explanations (LIME) is a popular technique for explaining the prediction of a single instance. Although LIME is simple and versatile, it suffers from instability in the generated explanations. In this paper, we propose a Gaussian Process (GP) based variation of locally interpretable models. We employ a smart sampling strategy based on the acquisition functions in Bayesian optimization. Further, we employ the automatic relevance determination based covariance function in GP, with separate length-scale parameters for each feature, where the reciprocal of lengthscale parameters serve as feature explanations. We illustrate the performance of the proposed technique on two real-world datasets, and demonstrate the superior stability of the proposed technique. Furthermore, we demonstrate that the proposed technique is able to generate faithful explanations using much fewer samples as compared to LIME.",0
"Abstract: Many machine learning models can produce accurate predictions but lack interpretability. This work presents an approach to generate interpretable explanations that do not require knowledge of the underlying model. We use locally interpretable model agnostic explanations (LIME) to approximate black box decisions from any model as linear combinations of features with interpretable coefficients. To extend LIME to high dimensional data spaces, we introduce a novel method based on Gaussian processes called GPLime. Our experiments demonstrate that GPLime provides interpretable explanations of complex models while maintaining competitive accuracy compared to traditional LIME methods. By allowing practitioners to explain their predictive systems without requiring complete understanding of how they function, our approach enables greater transparency and trustworthiness in artificial intelligence.",1
"In cognitive decoding, researchers aim to characterize a brain region's representations by identifying the cognitive states (e.g., accepting/rejecting a gamble) that can be identified from the region's activity. Deep learning (DL) methods are highly promising for cognitive decoding, with their unmatched ability to learn versatile representations of complex data. Yet, their widespread application in cognitive decoding is hindered by their general lack of interpretability as well as difficulties in applying them to small datasets and in ensuring their reproducibility and robustness. We propose to approach these challenges by leveraging recent advances in explainable artificial intelligence and transfer learning, while also providing specific recommendations on how to improve the reproducibility and robustness of DL modeling results.",0
"Cognitive neuroscience has made significant progress over recent years due to advances in functional neuroimaging techniques, which allow us to examine brain activity during different tasks. Deep neural networks (DNNs) have emerged as powerful tools in analyzing fMRI data, achieving state-of-the-art results by mapping brain activation patterns to cognitive processes such as attention, memory retrieval, and decision making. Despite these successes, there remain significant challenges that must be addressed before DNNs can accurately decode human mental states from fMRI scans alone. Here, we discuss some of these issues related to model interpretability, generalization across subjects and datasets, ethical considerations, and limitations imposed by task design and signal variability. These problems highlight the need for continued development of machine learning approaches tailored specifically for understanding complex neural systems. We suggest potential solutions grounded in domain expertise that may improve both performance and explanatory power of models developed in cognitive neuroscience research, setting a course for future discoveries at the intersection of neuroscience and artificial intelligence.",1
"This paper aims to explain adversarial attacks in terms of how adversarial perturbations contribute to the attacking task. We estimate attributions of different image regions to the decrease of the attacking cost based on the Shapley value. We define and quantify interactions among adversarial perturbation pixels, and decompose the entire perturbation map into relatively independent perturbation components. The decomposition of the perturbation map shows that adversarially-trained DNNs have more perturbation components in the foreground than normally-trained DNNs. Moreover, compared to the normally-trained DNN, the adversarially-trained DNN have more components which mainly decrease the score of the true category. Above analyses provide new insights into the understanding of adversarial attacks.",0
"Abstract:  Adversarial attacks have become an increasingly significant concern in deep learning as they can cause models to produce incorrect outputs even when given inputs that appear benign to human observers. In order to effectively mitigate these types of attacks, it is essential to understand how adversaries operate and interact with each other. This study examines attributional and interaction effects in adversarial attacks by using different methods to manipulate the features present in input data. Our results show that manipulating certain features can lead to more effective adversarial attacks, and interactions between attack vectors can further increase their effectiveness. These findings provide valuable insights into understanding adversarial behavior and suggest potential strategies for improving model robustness against such attacks. Future work should explore additional factors influencing adversarial impact, including variations in input representations, feature importance, and attacker motivations. Overall, our research contributes to advancing knowledge on adversarial vulnerabilities in machine learning systems.",1
"Polynomial expansions are important in the analysis of neural network nonlinearities. They have been applied thereto addressing well-known difficulties in verification, explainability, and security. Existing approaches span classical Taylor and Chebyshev methods, asymptotics, and many numerical approaches. We find that while these individually have useful properties such as exact error formulas, adjustable domain, and robustness to undefined derivatives, there are no approaches that provide a consistent method yielding an expansion with all these properties. To address this, we develop an analytically modified integral transform expansion (AMITE), a novel expansion via integral transforms modified using derived criteria for convergence. We show the general expansion and then demonstrate application for two popular activation functions, hyperbolic tangent and rectified linear units. Compared with existing expansions (i.e., Chebyshev, Taylor, and numerical) employed to this end, AMITE is the first to provide six previously mutually exclusive desired expansion properties such as exact formulas for the coefficients and exact expansion errors (Table II). We demonstrate the effectiveness of AMITE in two case studies. First, a multivariate polynomial form is efficiently extracted from a single hidden layer black-box MLP to facilitate equivalence testing from noisy stimulus-response pairs. Second, a variety of FFNN architectures having between 3 and 7 layers are range bounded using Taylor models improved by the AMITE polynomials and error formulas. AMITE presents a new dimension of expansion methods suitable for analysis/approximation of nonlinearities in neural networks, opening new directions and opportunities for the theoretical analysis and systematic testing of neural networks.",0
"AMITE presents a novel polynomial expansion method that allows efficient analysis of nonlinear activation functions commonly used in deep learning models like neural networks (NNs). Previous works either assume linear activation functions, use ad hoc approaches to analyze NN behavior near equilibrium points, or focus on worst case scenarios instead of considering averagecase performance. In contrast, our approach uses Chebyshev polynomials combined with Lie brackets to capture important global features of smooth activation functions, without relying on local Taylor expansions at each point separately. We then show how these insights can enable better understanding of key dynamics such as fixed points and stability analysis, while introducing less conservatism than traditional methods. By applying AMITE to popular ReLU and sigmoid activations, we demonstrate improved accuracy compared to stateoftheart tools in model checking problems like invariant verification and reachability computation. Moreover, our experiments illustrate the potential advantages of using efficient convex optimization techniques over iterative gradient descent solvers commonly found in existing work on NN analysis. With its versatile analytic framework and promising results, AMITE has significant implications for designing more interpretable models and ensuring safety guarantees during their deployment in missioncritical applications where failure can have serious consequences. As machine learning permeates into safetycricital fields such as autonomous driving, medical diagnosis, and financial decision making, effective model validation techniques become even more essential, highlighti",1
"A central question in federated learning (FL) is how to design optimization algorithms that minimize the communication cost of training a model over heterogeneous data distributed across many clients. A popular technique for reducing communication is the use of local steps, where clients take multiple optimization steps over local data before communicating with the server (e.g., FedAvg, SCAFFOLD). This contrasts with centralized methods, where clients take one optimization step per communication round (e.g., Minibatch SGD). A recent lower bound on the communication complexity of first-order methods shows that centralized methods are optimal over highly-heterogeneous data, whereas local methods are optimal over purely homogeneous data [Woodworth et al., 2020]. For intermediate heterogeneity levels, no algorithm is known to match the lower bound. In this paper, we propose a multistage optimization scheme that nearly matches the lower bound across all heterogeneity levels. The idea is to first run a local method up to a heterogeneity-induced error floor; next, we switch to a centralized method for the remaining steps. Our analysis may help explain empirically-successful stepsize decay methods in FL [Charles et al., 2020; Reddi et al., 2020]. We demonstrate the scheme's practical utility in image classification tasks.",0
"Federated learning is a distributed machine learning approach that enables multiple devices to collaboratively train models without sharing their data directly. One challenge facing federated learning is the communication overhead caused by transmitting model updates across many devices over limited bandwidth connections. To address this problem, we propose multistage optimization (MSO), which reduces the amount of communication required during training. In MSO, model updates from different devices are aggregated at intermediate stages before final convergence, reducing the overall communication cost. We show experimental results on several benchmark datasets using popular federated learning algorithms, demonstrating that our method significantly improves both communication efficiency and model accuracy compared to existing techniques. Our findings have important implications for deploying large-scale machine learning systems in practice, particularly those involving edge computing scenarios.",1
"Knowledge about the hidden factors that determine particular system dynamics is crucial for both explaining them and pursuing goal-directed interventions. Inferring these factors from time series data without supervision remains an open challenge. Here, we focus on spatiotemporal processes, including wave propagation and weather dynamics, for which we assume that universal causes (e.g. physics) apply throughout space and time. A recently introduced DIstributed SpatioTemporal graph Artificial Neural network Architecture (DISTANA) is used and enhanced to learn such processes, requiring fewer parameters and achieving significantly more accurate predictions compared to temporal convolutional neural networks and other related approaches. We show that DISTANA, when combined with a retrospective latent state inference principle called active tuning, can reliably derive location-respective hidden causal factors. In a current weather prediction benchmark, DISTANA infers our planet's land-sea mask solely by observing temperature dynamics and, meanwhile, uses the self inferred information to improve its own future temperature predictions.",0
"This paper presents a new approach to inferring latent states from spatiotemporal data generated by a generative model. Our method leverages recent advances in deep learning to estimate the unobserved variables that drive the dynamics of the system. By incorporating spatial and temporal correlations into our inference framework, we demonstrate improved accuracy compared to previous methods. We evaluate our approach on simulated datasets as well as real-world applications, showing that our algorithm can accurately recover hidden states in challenging scenarios. Overall, our work offers a powerful tool for understanding complex systems where traditional methods may fall short.",1
"Neural architecture search can discover neural networks with good performance, and One-Shot approaches are prevalent. One-Shot approaches typically require a supernet with weight sharing and predictors that predict the performance of architecture. However, the previous methods take much time to generate performance predictors thus are inefficient. To this end, we propose FOX-NAS that consists of fast and explainable predictors based on simulated annealing and multivariate regression. Our method is quantization-friendly and can be efficiently deployed to the edge. The experiments on different hardware show that FOX-NAS models outperform some other popular neural network architectures. For example, FOX-NAS matches MobileNetV2 and EfficientNet-Lite0 accuracy with 240% and 40% less latency on the edge CPU. FOX-NAS is the 3rd place winner of the 2020 Low-Power Computer Vision Challenge (LPCVC), DSP classification track. See all evaluation results at https://lpcv.ai/competitions/2020. Search code and pre-trained models are released at https://github.com/great8nctu/FOX-NAS.",0
"FOX-NAS proposes to significantly improve neural architecture search speed through a two phase algorithm based on evolutionary algorithms while maintaining explainability using feature importance maps. In the first phase, candidate architectures are sampled from the search space which is defined as a directed graph where edges represent architectural operations such as convolutions and maxpoolings, and nodes represent layers which can contain multiple operation choices. Candidate architectures are sampled by performing random walks from an initial seed architecture, then scored according to how well they perform on validation data. After some number of iterations, promising candidates are identified and passed into a second optimization phase called Adaptive Leader Selection (ALS), which uses genetic operators similar to those seen in classical evolutionary computation research. In particular, mutation rates depend on node feature importances learned from gradient features calculated using deep Taylor decomposition methods from program analysis, allowing us to study which network architectural components make the most impact on accuracy across different tasks. This approach leads to the fastest reported NAS method we know of at time of writing, enabling the use of large model spaces on consumer grade GPU hardware.",1
"In this paper, we provide an overview of first-order and second-order variants of the gradient descent method that are commonly used in machine learning. We propose a general framework in which 6 of these variants can be interpreted as different instances of the same approach. They are the vanilla gradient descent, the classical and generalized Gauss-Newton methods, the natural gradient descent method, the gradient covariance matrix approach, and Newton's method. Besides interpreting these methods within a single framework, we explain their specificities and show under which conditions some of them coincide.",0
"In this work, we present a novel first-order methodology that enables us to optimize nonconvex functions by directly using second-order curvature information at a given point without explicitly forming Hessian matrices. We demonstrate how this framework can handle nonsmooth objectives and provides a natural connection to several existing methods such as the Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno (BFGS) updates, adaptive quasi-Newton methods, L-BFGS algorithms, among others. Our key contributions are:  We introduce an efficient algorithm named Efficient Numerical Quasi-Newton (ENQN), which combines low per-iteration cost, computational simplicity, and high theoretical efficiency guarantees under mild assumptions. ENQN relies on only local curvature information at each iteration without storing, computing, nor forming any global matrices, offering superior scalability over state-of-the-art alternatives in complex applications. We establish new convergence rates for solving nonconvex problems using second-order methods based solely on local curvatures, extending recent developments for linearized Bregmanâ€“Lagrangian frameworks. Our theory includes explicit constants and covers various settings including stochastic and distributed optimization. Furthermore, our analysis captures the impact of different choices in the smoothing steps and sheds light on their interplay with problem parameters and the quality of subproblem solutions. To showcase the effectiveness and versatility of our approach, we present numerical results on both simple and challenging applications across areas including machine learning and computer vision; these experiments confirm our favorable performance against well-established solvers while maintaining low complexity operations count, memory requirements, and communication overheads in distributed scenarios.",1
"Deep neural networks are the default choice of learning models for computer vision tasks. Extensive work has been carried out in recent years on explaining deep models for vision tasks such as classification. However, recent work has shown that it is possible for these models to produce substantially different attribution maps even when two very similar images are given to the network, raising serious questions about trustworthiness. To address this issue, we propose a robust attribution training strategy to improve attributional robustness of deep neural networks. Our method carefully analyzes the requirements for attributional robustness and introduces two new regularizers that preserve a model's attribution map during attacks. Our method surpasses state-of-the-art attributional robustness methods by a margin of approximately 3% to 9% in terms of attribution robustness measures on several datasets including MNIST, FMNIST, Flower and GTSRB.",0
"In recent years, deep learning has made tremendous progress in fields such as computer vision and natural language processing. However, many deep models still suffer from lack of robustness under input perturbations like noise, outliers or changes in lighting conditions. This can lead to significant degradation in performance on tasks which may make them unusable in practice. To mitigate this problem we present enhanced regularizers that improve model attribution by making better use of available data during training. Our new regularizers effectively leverage unlabeled data along with standard labeled examples allowing the model to learn features relevant to both types of inputs during training. We demonstrate our approach improves the generalization capabilities of multiple architectures across several datasets including CIFAR10/CIFAR100, SVHN and ImageNet. The proposed method significantly reduces error rates while maintaining low computation overhead due to only using additional data during training, without any change in inference time.",1
"This paper presents and characterizes MIND, a new Portuguese corpus comprised of different types of articles collected from online mainstream and alternative media sources, over a 10-month period. The articles in the corpus are organized into five collections: facts, opinions, entertainment, satires, and conspiracy theories. Throughout this paper, we explain how the data collection process was conducted, and present a set of linguistic metrics that allow us to perform a preliminary characterization of the texts included in the corpus. Also, we deliver an analysis of the most frequent topics in the corpus, and discuss the main differences and similarities among the collections considered. Finally, we enumerate some tasks and applications that could benefit from this corpus, in particular the ones (in)directly related to misinformation detection. Overall, our contribution of a corpus and initial analysis are designed to support future exploratory news studies, and provide a better insight into misinformation.",0
"This paper presents the development and analysis of the MIND corpus (Mainstream and Independent News Documents), which is comprised of news articles from both mainstream media sources and independent outlets. We aimed to create a diverse dataset that would enable exploration of language use across different types of news sources and provide opportunities for comparison and contrast. In addition to describing our methodology for compiling the corpus, we conducted linguistic analyses on the data to identify patterns in vocabulary, syntax, and discourse structure. Our findings demonstrate significant differences in the use of language by mainstream vs. independent news sources, including variations in tone, topic choice, and perspective. Overall, the MIND corpus provides valuable insights into how language is used in contemporary news media and highlights the importance of considering source type when interpreting and analyzing news content.",1
"Machine learning (ML) solutions are prevalent. However, many challenges exist in making these solutions business-grade. One major challenge is to ensure that the ML solution provides its expected business value. In order to do that, one has to bridge the gap between the way ML model performance is measured and the solution requirements. In previous work (Barash et al, ""Bridging the gap..."") we demonstrated the effectiveness of utilizing feature models in bridging this gap. Whereas ML performance metrics, such as the accuracy or F1-score of a classifier, typically measure the average ML performance, feature models shed light on explainable data slices that are too far from that average, and therefore might indicate unsatisfied requirements. For example, the overall accuracy of a bank text terms classifier may be very high, say $98\% \pm 2\%$, yet it might perform poorly for terms that include short descriptions and originate from commercial accounts. A business requirement, which may be implicit in the training data, may be to perform well regardless of the type of account and length of the description. Therefore, the under-performing data slice that includes short descriptions and commercial accounts suggests poorly-met requirements. In this paper we show the feasibility of automatically extracting feature models that result in explainable data slices over which the ML solution under-performs. Our novel technique, IBM FreaAI aka FreaAI, extracts such slices from structured ML test data or any other labeled data. We demonstrate that FreaAI can automatically produce explainable and statistically-significant data slices over seven open datasets.",0
"This will help me write the body and conclusions sections of the paper and ensure that they align with your goals for the research project. Here is some background information on FreaAI: FreaAI is an open source framework for automating the process of applying large pre-trained language models (LLMs) to natural language processing tasks. It was developed by Uber in order to make using LLMs easier and more accessible for their developers. Currently, there are three versions of FreaAI available - v0.7, v0.8 and v0.9. Each version has different features and capabilities. For example, v0.8 includes support for fine-tuning pre-training checkpoints from Hugging Face transformers, while v0.9 introduces support for decoding without fine-tuning and distributed inference across multiple GPUs. The latest version as of August 2022 is v0.16 which adds new models and improvements such as dynamic inference batching, model parallelism, quantization support etc. It is written primarily in Python and can use any dataset format supported by PyTorch. FreaAI supports several popular deep learning frameworks like TensorFlow, PyTorch, ONNX and Keras and runs on all platforms where these frameworks run natively including mobile devices like iOS and Android. With FreaAI you can easily evaluate, select, fine tune and deploy LLMs on real world problems without having expertise of deep learning. It is actively maintained by Uber AI labs and has over 44 contributors, 10 maintainers and 283 stars as of today.  In this paper we describe how to use FreaAI to extract specific data slices from large datasets to train and evaluate machine learning models. We provide examples using both small and large scale datasets and demonstrate how to adjust settings to optimize performance. Our results show that using FreaAI can greatly reduce the time and effort required to prepare data for machine learning, enabling faster development of new models and increased productivity. We conclude with future work directions related to integration of additional types of models, improving scalability and stability of the toolkit, allowing users to set parameters automatically through ML models meta learning and increasing explainability through interactive visualizations. Overall our goal is to make natural language processing models even more accessible and usable for non experts so that anyone can apply them to solve complex text based business problems in their organization or life.""",1
"Useful information (UI) is an elusive concept in neural networks. A quantitative measurement of UI is absent, despite the variations of UI can be recognized by prior knowledge. The communication bandwidth of feature maps decreases after downscaling operations, but UI flows smoothly after training due to lower Nyquist frequency. Inspired by the low-Nyqusit-frequency nature of UI, we propose the use of spectral roll-off points (SROPs) to estimate UI on variations. The computation of an SROP is extended from a 1-D signal to a 2-D image by the required rotation invariance in image classification tasks. SROP statistics across feature maps are implemented as layer-wise useful information estimates. We design sanity checks to explore SROP variations when UI variations are produced by variations in model input, model architecture and training stages. The variations of SROP is synchronizes with UI variations in various randomized and sufficiently trained model structures. Therefore, SROP variations is an accurate and convenient sign of UI variations, which promotes the explainability of data representations with respect to frequency-domain knowledge.",0
"In recent years, feature maps have become increasingly popular as a tool for visualizing high-dimensional data sets. One important aspect of these maps is their spectral roll-off points, which describe how quickly features fade out at different distances from the map origin. These variations can provide valuable insights into the nature of the underlying data set and the structure of the map itself. This study explores the role of spectral roll-off point variations in extracting useful information from feature maps. Our results show that careful analysis of these variations can reveal previously unseen patterns and relationships within the data set, leading to new understanding and potentially improved decision making. We conclude by discussing the implications of our findings for future research in this area and potential applications in real world scenarios.",1
"Natural policy gradient (NPG) methods with function approximation achieve impressive empirical success in reinforcement learning problems with large state-action spaces. However, theoretical understanding of their convergence behaviors remains limited in the function approximation setting. In this paper, we perform a finite-time analysis of NPG with linear function approximation and softmax parameterization, and prove for the first time that widely used entropy regularization method, which encourages exploration, leads to linear convergence rate. Under considerably weaker regularity conditions, we prove that entropy-regularized Q-NPG variant with linear function approximation achieves $\tilde{O}(1/T)$ convergence rate. We adopt a Lyapunov drift analysis to prove the convergence results and explain the effectiveness of entropy regularization in improving the convergence rates.",0
"This abstract starts by explaining that one goal of reinforcement learning (RL) algorithms is to maximize expected cumulative reward over time. A RL algorithm called entropy regularization has been shown to improve stability during training and policy smoothness. However, traditional methods require storing a large batch of experiences from the environment which can cause issues like slow convergence rates. The new method proposed in this paper uses linear function approximation to speed up convergence rates while maintaining accuracy. An experiment was conducted on several continuous control tasks where results showed improved performance compared to prior work. Overall, this research offers evidence towards improving efficiency of entropy regularization based RL algorithms.",1
"Regulators have signalled an interest in adopting explainable AI(XAI) techniques to handle the diverse needs for model governance, operational servicing, and compliance in the financial services industry. In this short overview, we review the recent technical literature in XAI and argue that based on our current understanding of the field, the use of XAI techniques in practice necessitate a highly contextualized approach considering the specific needs of stakeholders for particular business applications.",0
"Harmonization is a key principle underlying current initiatives addressing interpretability/explainability (XAI) in machine learning and artificial intelligence (ML/AI). While many groups have advocated that XAI should achieve specific goals such as transparency, fairness or comprehensibility, these efforts must ultimately align if they aim at enabling users to use ML models effectively while meeting stakeholdersâ€™ expectations regarding trustworthiness. This article discusses seven distinct but interrelated problems facing those seeking to establish common ground and identifies concrete obstacles hindering harmonization: (a) fragmentation across application domains; (b) conflicting priorities among competing interest groups; (c) lacking knowledge sharing mechanisms across communities; (d) insufficient consensus on how to measure success; (e) unsettled terminology & concepts hampering communication; (f) technical barriers due to diversity of ML approaches & systems; (g) sociocultural factors preventing integration between human experts & artificial agents. These challenges contribute both individually and synergistically to slow down progress towards more interpretable AI. Addressing them necessitates joint effort from all parties involved including researchers, practitioners, industry representatives, standardizers and social scientists alike â€“ a task which poses further complications but ultimately benefits everyone working within the domain of intelligent machines. Without tackling these issues head-on, there is little hope for reaching wide adoption of advanced MLIA applications across sectors. Given the huge potential brought by AI and associated technologies it remains imperative that we rise to meet the challenge before us, lest society miss out on significant advances. In conclusion this paper argues tha",1
"Human Activity Recognition (HAR) based on IMU sensors is a crucial area in ubiquitous computing. Because of the trend of deploying AI on IoT devices or smartphones, more researchers are designing different HAR models for embedded devices. Deployment of models in embedded devices can help enhance the efficiency of HAR. We propose a multi-level HAR modeling pipeline called Stage-Logits-Memory Distillation (SMLDist) for constructing deep convolutional HAR models with embedded hardware support. SMLDist includes stage distillation, memory distillation, and logits distillation. Stage distillation constrains the learning direction of the intermediate features. The teacher model teaches the student models how to explain and store the inner relationship among high-dimensional features based on Hopfield networks in memory distillation. Logits distillation builds logits distilled by a smoothed conditional rule to preserve the probability distribution and enhance the softer target accuracy. We compare the accuracy, F1 macro score, and energy cost on embedded platforms of a MobileNet V3 model built by SMLDist with various state-of-the-art HAR frameworks. The product model has a good balance with robustness and efficiency. SMLDist can also compress models with a minor performance loss at an equal compression ratio to other advanced knowledge distillation methods on seven public datasets.",0
"This paper proposes a novel approach to human activity recognition (HAR) that utilizes multi-level distillation to achieve high accuracy on embedded devices. HAR has many applications in areas such as healthcare, fitness tracking, and smart homes, but existing methods often require large amounts of computational power or access to cloud computing resources. Our proposed method overcomes these limitations by leveraging distillation techniques to create a lightweight model that can run efficiently on low-power hardware. We evaluate our system through extensive experiments using several datasets and demonstrate that it achieves state-of-the-art performance while maintaining low latency and energy consumption. Additionally, we analyze the impact of different design choices on the model's effectiveness and provide insights into how to optimize it for specific use cases. Overall, our work shows that accurate HAR can be achieved even on resource-constrained platforms, paving the way for new possibilities in ubiquitous sensing and monitoring.",1
"Algorithmic fairness has aroused considerable interests in data mining and machine learning communities recently. So far the existing research has been mostly focusing on the development of quantitative metrics to measure algorithm disparities across different protected groups, and approaches for adjusting the algorithm output to reduce such disparities. In this paper, we propose to study the problem of identification of the source of model disparities. Unlike existing interpretation methods which typically learn feature importance, we consider the causal relationships among feature variables and propose a novel framework to decompose the disparity into the sum of contributions from fairness-aware causal paths, which are paths linking the sensitive attribute and the final predictions, on the graph. We also consider the scenario when the directions on certain edges within those paths cannot be determined. Our framework is also model agnostic and applicable to a variety of quantitative disparity measures. Empirical evaluations on both synthetic and real-world data sets are provided to show that our method can provide precise and comprehensive explanations to the model disparities.",0
"This paper proposes a novel approach to explaining algorithmic fairness through a method called ""fairness-aware causal path decomposition."" Despite the increasing use of algorithms in decision making processes across different industries, there remains a concern among scholars regarding how these algorithms can perpetuate existing biases within society. To address this issue, we present a framework that allows practitioners and policymakers to decompose complex algorithms into simpler components, enabling them to identify and explain the potential sources of unfair outcomes produced by the algorithm. Our proposed framework builds upon traditional counterfactual analysis methods commonly used in economics, epidemiology, and social sciences but integrates concepts from machine learning, data science, and algorithmic fairness literature. By using our fairness-aware causal path decomposition, users will gain a deeper understanding of how certain factors influence the output generated by algorithms and contribute to potential bias or discrimination. Ultimately, our goal is to support informed discussions on mitigating bias and promoting fairer decisions in automated systems.",1
"The large and still increasing popularity of deep learning clashes with a major limit of neural network architectures, that consists in their lack of capability in providing human-understandable motivations of their decisions. In situations in which the machine is expected to support the decision of human experts, providing a comprehensible explanation is a feature of crucial importance. The language used to communicate the explanations must be formal enough to be implementable in a machine and friendly enough to be understandable by a wide audience. In this paper, we propose a general approach to Explainable Artificial Intelligence in the case of neural architectures, showing how a mindful design of the networks leads to a family of interpretable deep learning models called Logic Explained Networks (LENs). LENs only require their inputs to be human-understandable predicates, and they provide explanations in terms of simple First-Order Logic (FOL) formulas involving such predicates. LENs are general enough to cover a large number of scenarios. Amongst them, we consider the case in which LENs are directly used as special classifiers with the capability of being explainable, or when they act as additional networks with the role of creating the conditions for making a black-box classifier explainable by FOL formulas. Despite supervised learning problems are mostly emphasized, we also show that LENs can learn and provide explanations in unsupervised learning settings. Experimental results on several datasets and tasks show that LENs may yield better classifications than established white-box models, such as decision trees and Bayesian rule lists, while providing more compact and meaningful explanations.",0
"How Logic Explains Everything: An Introduction to Logic Explained Networks is a comprehensive introduction that explores the fundamental concepts of logic, as well as their application to real world problems through the use of Logic Explained Networks (LXN). This revolutionary approach simplifies complex problems by breaking them down into manageable components. By leveraging LXN, readers can develop a deeper understanding of how logic works in everyday life, from problem solving and decision making, to critical thinking and analysis. With clear explanations and examples throughout, the book offers something for everyone â€“ regardless of prior knowledge or experience in philosophy, computer science, mathematics, linguistics, engineering, business analytics, management consulting, education, finance or any other field. Readers gain the skills they need to improve their ability to think critically and solve problems effectively. In short, by mastering the principles outlined in How Logic Explains Everything, one gains a powerful toolset to succeed in any endeavor requiring sound judgment and reasoning capabilities. So why wait? Start learning today! Read onâ€¦ --end--",1
"To properly contrast the Deepfake phenomenon the need to design new Deepfake detection algorithms arises; the misuse of this formidable A.I. technology brings serious consequences in the private life of every involved person. State-of-the-art proliferates with solutions using deep neural networks to detect a fake multimedia content but unfortunately these algorithms appear to be neither generalizable nor explainable. However, traces left by Generative Adversarial Network (GAN) engines during the creation of the Deepfakes can be detected by analyzing ad-hoc frequencies. For this reason, in this paper we propose a new pipeline able to detect the so-called GAN Specific Frequencies (GSF) representing a unique fingerprint of the different generative architectures. By employing Discrete Cosine Transform (DCT), anomalous frequencies were detected. The \BETA statistics inferred by the AC coefficients distribution have been the key to recognize GAN-engine generated data. Robustness tests were also carried out in order to demonstrate the effectiveness of the technique using different attacks on images such as JPEG Compression, mirroring, rotation, scaling, addition of random sized rectangles. Experiments demonstrated that the method is innovative, exceeds the state of the art and also give many insights in terms of explainability.",0
"Deepfake videos have become increasingly prevalent, posing significant challenges for society. These videos are made using Generative Adversarial Networks (GAN) which can generate highly realistic images and audio that are difficult to distinguish from real content. In this work, we propose a new method to detect GAN-generated images by analyzing their Discrete Cosine Transform (DCT) features. We show that DCT features extracted from deepfakes generated by GAN exhibit distinctive patterns that differentiate them from authentic images. Our approach achieves high accuracy on a variety of datasets and outperforms existing methods. Additionally, our method operates at low computational cost, making it feasible for real-time detection applications. By addressing the problem of deepfake detection, we aim to mitigate the negative effects these types of manipulated media can have on society.",1
"Learning with noisy labels has gained the enormous interest in the robust deep learning area. Recent studies have empirically disclosed that utilizing dual networks can enhance the performance of single network but without theoretic proof. In this paper, we propose Cooperative Learning (CooL) framework for noisy supervision that analytically explains the effects of leveraging dual or multiple networks. Specifically, the simple but efficient combination in CooL yields a more reliable risk minimization for unseen clean data. A range of experiments have been conducted on several benchmarks with both synthetic and real-world settings. Extensive results indicate that CooL outperforms several state-of-the-art methods.",0
"Implementing artificial intelligence models on real world applications can often lead to problems such as noisy supervision, where data may contain errors, inconsistencies, or lack of clarity. This study explores how cooperative learning techniques can be used to address these issues by utilizing multiple sources of imperfect knowledge to improve model accuracy. Using both synthetic and real-world datasets, the authors demonstrate that their proposed method outperforms traditional methods when dealing with noisy supervision. By combining several models through cooperation and sharing experiences, each model can learn from others, improving overall performance. In conclusion, the results show that cooperative learning is a promising approach towards reducing the impact of noise and increasing the robustness of AI models in real-life scenarios.",1
"Great progress has been made by the advances in Generative Adversarial Networks (GANs) for image generation. However, there lacks enough understanding on how a realistic image can be generated by the deep representations of GANs from a random vector. This chapter will give a summary of recent works on interpreting deep generative models. We will see how the human-understandable concepts that emerge in the learned representation can be identified and used for interactive image generation and editing.",0
"This paper presents an analysis of the application of generative adversarial networks (GAN) for interactive image generation. GANs have been shown to be effective at generating high quality images that can fool humans into believing they are real. However, there remains limited understanding on how these models function and their potential use cases beyond simply producing visually pleasing results. To address this gap, we investigate whether GANs can generate meaningful insights by examining them from two perspectives: as tools for data visualization and communication; and as instruments for exploring alternative worldviews through design thinking. Through empirical studies with professional designers, we demonstrate that while current implementations of GANs struggle with certain types of tasks, the tool has great potential in supporting creativity and innovation. Our findings provide new directions towards advancing our capabilities in interacting with intelligent systems such as artificial intelligence and machine learning. Overall, this work contributes to broader discussions surrounding responsible development and deployment of technology, particularly those that aim to augment human abilities.",1
"Given the increasing promise of Graph Neural Networks (GNNs) in real-world applications, several methods have been developed for explaining their predictions. So far, these methods have primarily focused on generating subgraphs that are especially relevant for a particular prediction. However, such methods do not provide a clear opportunity for recourse: given a prediction, we want to understand how the prediction can be changed in order to achieve a more desirable outcome. In this work, we propose a method for generating counterfactual (CF) explanations for GNNs: the minimal perturbation to the input (graph) data such that the prediction changes. Using only edge deletions, we find that our method, CF-GNNExplainer can generate CF explanations for the majority of instances across three widely used datasets for GNN explanations, while removing less than 3 edges on average, with at least 94\% accuracy. This indicates that CF-GNNExplainer primarily removes edges that are crucial for the original predictions, resulting in minimal CF explanations.",0
"Graph Neural Networks (GNN) have been gaining popularity due to their ability to handle graph data structures effectively. However, lack of interpretability remains one of the key challenges in using GNN models. This paper proposes a new method called CF-GNNExplainer that addresses this issue by providing counterfactual explanations for GNN predictions.  The proposed approach works by identifying the most influential nodes and edges in a given graph, which leads to changes in the predicted outcome. These influential factors can then be used to generate counterfactual explanations, which highlight how different inputs would affect the model output. To achieve this, we use gradient-based methods, attention mechanisms, and permutation testing to identify and rank important features in the graph. We show through extensive experiments on multiple benchmark datasets that our approach provides meaningful and accurate explanations while maintaining good prediction accuracy. Our work helps bridge the gap towards more interpretable and transparent GNN models, enabling better decision making in areas such as social networks analysis and drug discovery where graphs play a significant role.",1
"Conventional wisdom dictates that learning rate should be in the stable regime so that gradient-based algorithms don't blow up. This letter introduces a simple scenario where an unstably large learning rate scheme leads to a super fast convergence, with the convergence rate depending only logarithmically on the condition number of the problem. Our scheme uses a Cyclical Learning Rate (CLR) where we periodically take one large unstable step and several small stable steps to compensate for the instability. These findings also help explain the empirical observations of [Smith and Topin, 2019] where they show that CLR with a large maximum learning rate can dramatically accelerate learning and lead to so-called ""super-convergence"". We prove that our scheme excels in the problems where Hessian exhibits a bimodal spectrum and the eigenvalues can be grouped into two clusters (small and large). The unstably large step is the key to enabling fast convergence over the small eigen-spectrum.",0
"In this study, we present a novel approach for training deep neural networks that achieves faster convergence rates while maintaining high accuracy on benchmark datasets. Our method, which we call provable super-convergence, uses a large cyclical learning rate schedule that effectively adapts to the changing dynamics of the optimization process as it progresses. We demonstrate through extensive experimentation that our algorithm outperforms state-of-the-art methods across a range of common architectures and tasks, including image classification, language translation, and speech recognition. Furthermore, we provide theoretical analyses and proof of concept experiments that showcase the effectiveness of our approach in terms of stability, robustness, and computational efficiency. These results indicate significant potential for improving the performance and scalability of deep learning systems, making them even more valuable tools in fields ranging from computer vision and natural language processing to scientific simulations and autonomous control. Overall, our work represents an important contribution towards understanding how to design efficient algorithms for training complex machine learning models, and offers new insights into the underlying principles governing the behavior of these powerful artificial intelligence systems.",1
"Deep neural networks have gained momentum based on their accuracy, but their interpretability is often criticised. As a result, they are labelled as black boxes. In response, several methods have been proposed in the literature to explain their predictions. Among the explanatory methods, Shapley values is a feature attribution method favoured for its robust theoretical foundation. However, the analysis of feature attributions using Shapley values requires choosing a baseline that represents the concept of missingness. An arbitrary choice of baseline could negatively impact the explanatory power of the method and possibly lead to incorrect interpretations. In this paper, we present a method for choosing a baseline according to a neutrality value: as a parameter selected by decision-makers, the point at which their choices are determined by the model predictions being either above or below it. Hence, the proposed baseline is set based on a parameter that depends on the actual use of the model. This procedure stands in contrast to how other baselines are set, i.e. without accounting for how the model is used. We empirically validate our choice of baseline in the context of binary classification tasks, using two datasets: a synthetic dataset and a dataset derived from the financial domain.",0
"""This paper presents a comprehensive analysis of Shapley values in multilayer perceptrons (MLP) neural networks, addressing key challenges such as missing data and neutral predictions that have hindered their practical application. Building on existing methods, we propose a novel approach to computing Shapley values based on random forest models trained on transformed input features. Our method overcomes limitations imposed by traditional matrix balancing techniques and can handle incomplete datasets without requiring explicit imputation or deletion of samples. We demonstrate empirically through extensive experiments on several benchmark datasets that our proposed approach achieves competitive accuracy compared to other state-of-the-art methods while maintaining computational efficiency. Importantly, our framework provides insights into understanding feature importance and model behavior under realistic conditions where data may be absent or ambiguous.""",1
"The neuroimage analysis community has neglected the automated segmentation of the olfactory bulb (OB) despite its crucial role in olfactory function. The lack of an automatic processing method for the OB can be explained by its challenging properties. Nonetheless, recent advances in MRI acquisition techniques and resolution have allowed raters to generate more reliable manual annotations. Furthermore, the high accuracy of deep learning methods for solving semantic segmentation problems provides us with an option to reliably assess even small structures. In this work, we introduce a novel, fast, and fully automated deep learning pipeline to accurately segment OB tissue on sub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we designed a three-stage pipeline: (1) Localization of a region containing both OBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized region through four independent AttFastSurferCNN - a novel deep learning architecture with a self-attention mechanism to improve modeling of contextual information, and (3) Ensemble of the predicted label maps. The OB pipeline exhibits high performance in terms of boundary delineation, OB localization, and volume estimation across a wide range of ages in 203 participants of the Rhineland Study. Moreover, it also generalizes to scans of an independent dataset never encountered during training, the Human Connectome Project (HCP), with different acquisition parameters and demographics, evaluated in 30 cases at the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution. We extensively validated our pipeline not only with respect to segmentation accuracy but also to known OB volume effects, where it can sensitively replicate age effects.",0
"This paper presents a fully automated method for segmenting the olfactory bulbs from high resolution T2-weighted MRI scans. Our approach leverages machine learning techniques such as convolutional neural networks (CNNs) to accurately delineate the boundaries of these structures. We evaluate our algorithm against ground truth data obtained through manual labeling by expert annotators, demonstrating high accuracy and precision. The proposed method has applications in neuroscience research and clinical settings where precise identification of olfactory bulbs is crucial.",1
"This is a tutorial and survey paper on the Johnson-Lindenstrauss (JL) lemma and linear and nonlinear random projections. We start with linear random projection and then justify its correctness by JL lemma and its proof. Then, sparse random projections with $\ell_1$ norm and interpolation norm are introduced. Two main applications of random projection, which are low-rank matrix approximation and approximate nearest neighbor search by random projection onto hypercube, are explained. Random Fourier Features (RFF) and Random Kitchen Sinks (RKS) are explained as methods for nonlinear random projection. Some other methods for nonlinear random projection, including extreme learning machine, randomly weighted neural networks, and ensemble of random projections, are also introduced.",0
"Title: Introduction to Johnson-Lindenstrausslemma, linear and nonlinear random projections, random Fourier features, and random kitchen sinks Abstract This tutorial and survey provides an introduction to key techniques for dimensionality reduction and representation learning that are widely used in machine learning and data science. Specifically, we cover four methods: Johnson-Lindenstrauss lemma (JLL), linear and nonlinear random projections, random Fourier features (RFFs), and random kitchen sinks (RKS). These approaches have different underlying mathematical principles and strengths/weaknesses in terms of computational cost, memory usage, model accuracy, interpretability, etc., depending on applications and problem settings. We discuss their pros and cons, illustrate concrete use cases, point out subtle pitfalls and traps, provide efficient Python code snippets and software packages that implement these models, and summarize recent developments, extensions, and open challenges for each method. Our exposition is intended for students, researchers, practitioners, instructors, data analysts, engineers, developers, managers, etc., who aim to improve their understanding of unsupervised feature mapping tools and make informed decisions regarding which ones suit better given tasks at hand. By following along our guide, readers can acquire hands-on experience and intuitions about JLL, random embeddings, and RFS on real data sets of varying complexity. They may even discover new ideas or refine current practices through experimental comparisons based on their favorite projects or domains. Overall, our work should provide a unique resource for mastering randomized low-rank approximations with broad impact across scientific fields and industries where big d",1
"Bruxism is a disorder characterised by teeth grinding and clenching, and many bruxism sufferers are not aware of this disorder until their dental health professional notices permanent teeth wear. Stress and anxiety are often listed among contributing factors impacting bruxism exacerbation, which may explain why the COVID-19 pandemic gave rise to a bruxism epidemic. It is essential to develop tools allowing for the early diagnosis of bruxism in an unobtrusive manner. This work explores the feasibility of detecting bruxism-related events using earables in a mimicked in-the-wild setting. Using inertial measurement unit for data collection, we utilise traditional machine learning for teeth grinding and clenching detection. We observe superior performance of models based on gyroscope data, achieving an 88% and 66% accuracy on grinding and clenching activities, respectively, in a controlled environment, and 76% and 73% on grinding and clenching, respectively, in an in-the-wild environment.",0
"Title: Bruxism Monitoring using Wearable Electronics  Bruxism is the habitual grinding of teeth that can cause significant dental damage over time. Early detection and management of bruxism through behavior modification and therapy is crucial for preventing long-term complications. This feasibility study investigates the use of earables (wearable electronics designed to fit inside the ear) as a potential method for monitoring bruxism events. We describe the development of a custom earable device equipped with motion sensors capable of detecting jaw movements associated with bruxism episodes. Preliminary results from initial testing suggest that our earable prototype was able to accurately capture and distinguish between episodes of bruxism compared to control data obtained during rest. Future work includes refining the design of the device, expanding the scope of testing to larger cohorts of participants, and exploring machine learning algorithms for automatic event classification. Overall, our findings support the use of earables as a promising tool for monitoring bruxism at home and informing treatment decisions.",1
"Shapley values are one of the main tools used to explain predictions of tree ensemble models. The main alternative to Shapley values are Banzhaf values that have not been understood equally well. In this paper we make a step towards filling this gap, providing both experimental and theoretical comparison of these model explanation methods. Surprisingly, we show that Banzhaf values offer several advantages over Shapley values while providing essentially the same explanations. We verify that Banzhaf values: (1) have a more intuitive interpretation, (2) allow for more efficient algorithms, and (3) are much more numerically robust. We provide an experimental evaluation of these theses. In particular, we show that on real world instances.   Additionally, from a theoretical perspective we provide new and improved algorithm computing the same Shapley value based explanations as the algorithm of Lundberg et al. [Nat. Mach. Intell. 2020]. Our algorithm runs in $O(TLD+n)$ time, whereas the previous algorithm had $O(TLD^2+n)$ running time bound. Here, $T$ is the number of trees, $L$ is the maximum number of leaves in a tree, and $D$ denotes the maximum depth of a tree in the ensemble. Using the computational techniques developed for Shapley values we deliver an optimal $O(TL+n)$ time algorithm for computing Banzhaf values based explanations. In our experiments these algorithms give running times smaller even by an order of magnitude.",0
"This would allow others who have read your papers but don't remember their titles to find them easily by searching Google Scholar using relevant keywords from your abstracts. Here is one possible example of how you might write such an abstract if your papers were on machine learning applications in finance.  Title: ""A Machine Learning Approach for Predicting Stock Prices"" Abstarct: In recent years, there has been growing interest in applying machine learning techniques to financial data analysis, particularly stock market prediction. While many researchers have focused on developing complex models capable of capturing subtle features of financial markets, few studies have rigorously compared different methods of feature importance computation used to interpret these predictive models. In particular, two popular approachesâ€”Shapley values (SV) and Banzhaf power indices (BPI)â€”have been proposed as ways to quantify individual feature contributions to model predictions. However, little attention has been paid to which method provides more accurate and interpretable results in practice. To address this gap, we conduct an empirical study comparing SV and BPI computations across multiple datasets in finance, including both traditional indicators and unconventional features. Our findings show that while both SV and BPI perform well at explaining variable importances, BPI tends to produce simpler, more intuitive results that align better with domain expertise than SV. We provide a detailed comparison of strengths and weaknesses for each approach based on several criteria, discuss the implications for practitioners interested in selecting appropriate feature attribution methods, and suggest future directions for refining our understanding of feature importance computation in financial forecasting. Overall, our work contributes to the development of reliable and transparent model interpretation procedures for decision making in investment management and risk assessmen",1
"The movements of individuals within and among cities influence critical aspects of our society, such as well-being, the spreading of epidemics, and the quality of the environment. When information about mobility flows is not available for a particular region of interest, we must rely on mathematical models to generate them. In this work, we propose the Deep Gravity model, an effective method to generate flow probabilities that exploits many variables (e.g., land use, road network, transport, food, health facilities) extracted from voluntary geographic data, and uses deep neural networks to discover non-linear relationships between those variables and mobility flows. Our experiments, conducted on mobility flows in England, Italy, and New York State, show that Deep Gravity has good geographic generalization capability, achieving a significant increase in performance (especially in densely populated regions of interest) with respect to the classic gravity model and models that do not use deep neural networks or geographic data. We also show how flows generated by Deep Gravity may be explained in terms of the geographic features using explainable AI techniques.",0
"In recent years, there has been growing interest in using machine learning techniques such as deep neural networks (DNNs) to enhance mobility flow prediction accuracy. However, many existing approaches fail to account for important spatial and environmental factors that can significantly affect mobility patterns. This study presents ""Deep Gravity,"" a novel framework that combines DNNs with geospatial data to improve mobility flows generation by incorporating both temporal and geographical features into predictions. Our approach utilizes open source Geographic Information Systems (GIS), including OpenStreetMap and GeoJSON datasets, which provide detailed road network descriptions and land use classifications to model spatial influence on human movement. To evaluate the effectiveness of our methodology, we conducted experiments using real world traffic and activity data from several major cities. Results demonstrate significant improvements over state-of-the-art methods, achieving up to 98% accuracy across different metrics under diverse conditions. We believe this work represents a promising step towards more efficient urban planning and transportation management through advanced artificial intelligence models.",1
"Visual understanding requires comprehending complex visual relations between objects within a scene. Here, we seek to characterize the computational demands for abstract visual reasoning. We do this by systematically assessing the ability of modern deep convolutional neural networks (CNNs) to learn to solve the Synthetic Visual Reasoning Test (SVRT) challenge, a collection of twenty-three visual reasoning problems. Our analysis leads to a novel taxonomy of visual reasoning tasks, which can be primarily explained by both the type of relations (same-different vs. spatial-relation judgments) and the number of relations used to compose the underlying rules. Prior cognitive neuroscience work suggests that attention plays a key role in human's visual reasoning ability. To test this, we extended the CNNs with spatial and feature-based attention mechanisms. In a second series of experiments, we evaluated the ability of these attention networks to learn to solve the SVRT challenge and found the resulting architectures to be much more efficient at solving the hardest of these visual reasoning tasks. Most importantly, the corresponding improvements on individual tasks partially explained the taxonomy. Overall, this work advances our understanding of visual reasoning and yields testable Neuroscience predictions regarding the need for feature-based vs. spatial attention in visual reasoning.",0
"Visual reasoning is a complex process that involves understanding and interpreting visual information. This task requires both cognitive and perceptual abilities, including attention, memory, pattern recognition, and decision making. Previous research has shown that human performance on visual reasoning tasks can vary widely depending on the specific details of the task and individual differences in cognitive ability. However, less is known about the computational demands underlying these variability patterns. In this study, we examine the relationship between performance on three different visual reasoning tasks and working memory capacity (WMC), a measure of executive function that reflects the amount of information an individual can hold in mind at once. We find that WMC explains significant variance in all three tasks, but that there are unique contributions from other factors as well. These results suggest that while WMC plays an important role in supporting efficient visual reasoning, it may not be sufficient by itself to fully explain why some individuals perform better than others on these types of tasks. Future work should continue exploring the neural processes responsible for successful visual reasoning, and how they relate to domain general abilities like WMC.",1
"Graph Neural Networks (GNNs) have boosted the performance for many graph-related tasks. Despite the great success, recent studies have shown that GNNs are highly vulnerable to adversarial attacks, where adversaries can mislead the GNNs' prediction by modifying graphs. On the other hand, the explanation of GNNs (GNNExplainer) provides a better understanding of a trained GNN model by generating a small subgraph and features that are most influential for its prediction. In this paper, we first perform empirical studies to validate that GNNExplainer can act as an inspection tool and have the potential to detect the adversarial perturbations for graphs. This finding motivates us to further initiate a new problem investigation: Whether a graph neural network and its explanations can be jointly attacked by modifying graphs with malicious desires? It is challenging to answer this question since the goals of adversarial attacks and bypassing the GNNExplainer essentially contradict each other. In this work, we give a confirmative answer to this question by proposing a novel attack framework (GEAttack), which can attack both a GNN model and its explanations by simultaneously exploiting their vulnerabilities. Extensive experiments on two explainers (GNNExplainer and PGExplainer) under various real-world datasets demonstrate the effectiveness of the proposed method.",0
"This paper presents a novel approach to graph neural networks (GNNs) that addresses several shortcomings in existing methods. By incorporating principles from both deep learning and knowledge graphs, we propose a framework for jointly training GNN models and their explanations. Our method enables the efficient generation of explanations directly from the model itself, without requiring additional data or computation. We evaluate our proposed approach on two benchmark datasets, demonstrating its effectiveness in improving model accuracy while providing interpretable explanations. Additionally, we showcase how our approach can be utilized in real-world applications such as biology and social network analysis, where explainability is crucial for better understanding the underlying phenomena. Overall, our work represents a significant advancement towards reliable, explainable machine learning on complex, relational domains.",1
"The aim of this project is to develop and test advanced analytical methods to improve the prediction accuracy of Credit Risk Models, preserving at the same time the model interpretability. In particular, the project focuses on applying an explainable machine learning model to bank-related databases. The input data were obtained from open data. Over the total proven models, CatBoost has shown the highest performance. The algorithm implementation produces a GINI of 0.68 after tuning the hyper-parameters. SHAP package is used to provide a global and local interpretation of the model predictions to formulate a human-comprehensive approach to understanding the decision-maker algorithm. The 20 most important features are selected using the Shapley values to present a full human-understandable model that reveals how the attributes of an individual are related to its model prediction.",0
"Artificial Intelligence (AI) has been increasingly used in financial services over recent years, with one particular area gaining traction: credit scoring models using machine learning algorithms. These models rely on large amounts of data to train them, allowing banks and lenders to make more accurate predictions regarding whether a customer will default on their debts or repay their loans. However, these opaque model decision making processes often lack transparency into how these decisions were reached. In order to address concerns of algorithmic biases and discrimination, as well as to comply with regulations such as the European Unionâ€™s Payment Services Directive II (PSD2), explainability in credit scoring models is crucial. This research proposes a new methodology for designing transparent and interpretable credit scoring models that can provide explanations for their predictions while maintaining high levels of accuracy. Our approach involves combining state-of-the-art techniques from multiple fields such as feature selection, local interpretability methods, tree ensemble pruning, global interpretation and sensitivity analysis. We evaluate our proposed methodology by demonstrating its effectiveness through comprehensive experiments across several real world datasets. Our findings show that we achieved comparable predictive performance relative to popular non-transparent baseline models, but provided valuable insights into why certain decisions were made, which could potentially reduce instances of unfairness. Ultimately, our work advances the field towards building trustworthy artificial intelligence systems for financial services, making them accountable and fairer to users.",1
"Geometry-aware modules are widely applied in recent deep learning architectures for scene representation and rendering. However, these modules require intrinsic camera information that might not be obtained accurately. In this paper, we propose a Spatial Transformation Routing (STR) mechanism to model the spatial properties without applying any geometric prior. The STR mechanism treats the spatial transformation as the message passing process, and the relation between the view poses and the routing weights is modeled by an end-to-end trainable neural network. Besides, an Occupancy Concept Mapping (OCM) framework is proposed to provide explainable rationals for scene-fusion processes. We conducted experiments on several datasets and show that the proposed STR mechanism improves the performance of the Generative Query Network (GQN). The visualization results reveal that the routing process can pass the observed information from one location of some view to the associated location in the other view, which demonstrates the advantage of the proposed model in terms of spatial cognition.",0
"This paper presents a new method called Structure Guided Query Network (STR-GQN) that allows scene representation and rendering based on unknown cameras. With the use of spatial transformation routing techniques, our approach can efficiently determine how objects in a scene would look from different camera angles, even if those angles have never been seen before. Our experiments demonstrate that STR-GQN outperforms existing methods by achieving more accurate renderings while using fewer resources. Overall, our work provides a promising solution for computer vision applications such as virtual reality, augmented reality, and robotics, where it is essential to accurately represent scenes from various perspectives.",1
"Optimizing economic and public policy is critical to address socioeconomic issues and trade-offs, e.g., improving equality, productivity, or wellness, and poses a complex mechanism design problem. A policy designer needs to consider multiple objectives, policy levers, and behavioral responses from strategic actors who optimize for their individual objectives. Moreover, real-world policies should be explainable and robust to simulation-to-reality gaps, e.g., due to calibration issues. Existing approaches are often limited to a narrow set of policy levers or objectives that are hard to measure, do not yield explicit optimal policies, or do not consider strategic behavior, for example. Hence, it remains challenging to optimize policy in real-world scenarios. Here we show that the AI Economist framework enables effective, flexible, and interpretable policy design using two-level reinforcement learning (RL) and data-driven simulations. We validate our framework on optimizing the stringency of US state policies and Federal subsidies during a pandemic, e.g., COVID-19, using a simulation fitted to real data. We find that log-linear policies trained using RL significantly improve social welfare, based on both public health and economic outcomes, compared to past outcomes. Their behavior can be explained, e.g., well-performing policies respond strongly to changes in recovery and vaccination rates. They are also robust to calibration errors, e.g., infection rates that are over or underestimated. As of yet, real-world policymaking has not seen adoption of machine learning methods at large, including RL and AI-driven simulations. Our results show the potential of AI to guide policy design and improve social welfare amidst the complexity of the real world.",0
"This research paper presents the development of an AI system called the ""AI Economist"" which uses data from economic indicators such as GDP, inflation rates, unemployment figures, etc., in order to provide policy recommendations that can benefit countries worldwide. By focusing on the importance of data-driven analysis, interpretability and robustness, we aim to create more effective policies that could lead to better outcomes for citizens everywhere. Through careful evaluation and testing, our team has been able to demonstrate how our model can successfully handle complex issues that arise during decision making processes while still maintaining transparency and accountability. Ultimately, the goal of this project is to bring attention to the need for more advanced tools in economics so that leaders can make well informed decisions about their countryâ€™s future. Our work offers valuable insights into how AI systems can play an important role in shaping policies, providing new opportunities for collaboration across different fields, and enabling policymakers to navigate the increasingly difficult challenges faced by modern nations.",1
"With the ongoing rise of machine learning, the need for methods for explaining decisions made by artificial intelligence systems is becoming a more and more important topic. Especially for image classification tasks, many state-of-the-art tools to explain such classifiers rely on visual highlighting of important areas of the input data. Contrary, counterfactual explanation systems try to enable a counterfactual reasoning by modifying the input image in a way such that the classifier would have made a different prediction. By doing so, the users of counterfactual explanation systems are equipped with a completely different kind of explanatory information. However, methods for generating realistic counterfactual explanations for image classifiers are still rare. Especially in medical contexts, where relevant information often consists of textural and structural information, high-quality counterfactual images have the potential to give meaningful insights into decision processes. In this work, we present GANterfactual, an approach to generate such counterfactual image explanations based on adversarial image-to-image translation techniques. Additionally, we conduct a user study to evaluate our approach in an exemplary medical use case. Our results show that, in the chosen medical use-case, counterfactual explanations lead to significantly better results regarding mental models, explanation satisfaction, trust, emotions, and self-efficacy than two state-of-the-art systems that work with saliency maps, namely LIME and LRP.",0
"This paper presents ""GANterfactual,"" a novel approach to generating counterfactual explanations for medical decision making problems that requires minimal expert input. We use generative adversarial learning (GAL) techniques to model and manipulate plausible outcomes that could have resulted from different treatment decisions. Our method generates visualizations and text descriptions of these alternative scenarios, allowing non-expert users to better understand the implications of their choices. In addition, we incorporate feedback mechanisms into our system so that users can fine-tune the generated results based on their preferences and goals. Experimental evaluation shows that our approach significantly improves comprehension of complex medical situations while reducing cognitive load on users compared to traditional approaches.",1
"For the highly imbalanced credit card fraud detection problem, most existing methods either use data augmentation methods or conventional machine learning models, while neural network-based anomaly detection approaches are lacking. Furthermore, few studies have employed AI interpretability tools to investigate the feature importance of transaction data, which is crucial for the black-box fraud detection module. Considering these two points together, we propose a novel anomaly detection framework for credit card fraud detection as well as a model-explaining module responsible for prediction explanations. The fraud detection model is composed of two deep neural networks, which are trained in an unsupervised and adversarial manner. Precisely, the generator is an AutoEncoder aiming to reconstruct genuine transaction data, while the discriminator is a fully-connected network for fraud detection. The explanation module has three white-box explainers in charge of interpretations of the AutoEncoder, discriminator, and the whole detection model, respectively. Experimental results show the state-of-the-art performances of our fraud detection model on the benchmark dataset compared with baselines. In addition, prediction analyses by three explainers are presented, offering a clear perspective on how each feature of an instance of interest contributes to the final model output.",0
"In recent years credit card fraud has become increasingly common due to advances in technology allowing individuals to perform transactions without physical contact. In order to prevent such cases credit card companies must develop methods that can detect any suspicious activity. By examining transaction records of a customerâ€™s past behavior anomalies can be detected which could potentially indicate fraudulent activities. This study presents a novel method that is capable of detecting local interpretable patterns, using one-class anomaly detection that have high predictability. Using data from multiple sources over two years we compared our results to traditional statistical approaches and found significant improvements with more accurate predictions that allowed us to block potential fraud before it was executed. The proposed model is able to capture characteristics specific to credit cards and adapt to changing circumstances allowing it to evolve as new types of attacks appear. We expect that our contributions will aid financial institutions make better decisions by providing them with reliable solutions in credit card fraud detection.",1
"Deep neural network (DNN) models have achieved phenomenal success for applications in many domains, ranging from academic research in science and engineering to industry and business. The modeling power of DNN is believed to have come from the complexity and over-parameterization of the model, which on the other hand has been criticized for the lack of interpretation. Although certainly not true for every application, in some applications, especially in economics, social science, healthcare industry, and administrative decision making, scientists or practitioners are resistant to use predictions made by a black-box system for multiple reasons. One reason is that a major purpose of a study can be to make discoveries based upon the prediction function, e.g., to reveal the relationships between measurements. Another reason can be that the training dataset is not large enough to make researchers feel completely sure about a purely data-driven result. Being able to examine and interpret the prediction function will enable researchers to connect the result with existing knowledge or gain insights about new directions to explore. Although classic statistical models are much more explainable, their accuracy often falls considerably below DNN. In this paper, we propose an approach to fill the gap between relatively simple explainable models and DNN such that we can more flexibly tune the trade-off between interpretability and accuracy. Our main idea is a mixture of discriminative models that is trained with the guidance from a DNN. Although mixtures of discriminative models have been studied before, our way of generating the mixture is quite different.",0
"In recent years, deep neural networks have proven themselves capable of solving complex tasks ranging from image classification [2] to speech recognition [6]. Despite their ability to perform well on these tasks, they struggle with two problems that arise often: overfitting and interpretability. Overfitting happens when a model has learned how to fit the training data so precisely that it no longer generalizes well to unseen data [4]. Interpretability refers to understanding what the model has actually learned during training. Isolating important features from unimportant ones can lead to better models overall [7]. By combining mixture of linear models (MLM) with co-training of a deep neural network we aim to address both issues at once. We first preprocess our dataset into groups based off specific feature characteristics and then train individual MLMs per group. These models are then fed through a shared DNN which acts as the discriminator. During training, each MLM receives gradients from the discriminator but never directly learns from any other dataset than its own. This allows us to train more interpretable models as the only way that one groupâ€™s predictions affect anotherâ€™s comes via the trained DNN. Additionally, due to the small size of most datasets there is always some form of noise present. Training multiple classifiers instead helps mitigate such effects since it increases the likelihood of finding important features despite random fluctuations in the underlying signal. Finally, since the individual linear models can converge faster during initial epochs, having them train alongside the DNN further reduces potential issues caused by overfitting. When combined, these improvements led to accuracy boosting between 0.5% - 8%, making this approach very promising.",1
"The rapidly emerging field of deep learning-based computational pathology has demonstrated promise in developing objective prognostic models from histology whole slide images. However, most prognostic models are either based on histology or genomics alone and do not address how histology and genomics can be integrated to develop joint image-omic prognostic models. Additionally identifying explainable morphological and molecular descriptors from these models that govern such prognosis is of interest. We used multimodal deep learning to integrate gigapixel whole slide pathology images, RNA-seq abundance, copy number variation, and mutation data from 5,720 patients across 14 major cancer types. Our interpretable, weakly-supervised, multimodal deep learning algorithm is able to fuse these heterogeneous modalities for predicting outcomes and discover prognostic features from these modalities that corroborate with poor and favorable outcomes via multimodal interpretability. We compared our model with unimodal deep learning models trained on histology slides and molecular profiles alone, and demonstrate performance increase in risk stratification on 9 out of 14 cancers. In addition, we analyze morphologic and molecular markers responsible for prognostic predictions across all cancer types. All analyzed data, including morphological and molecular correlates of patient prognosis across the 14 cancer types at a disease and patient level are presented in an interactive open-access database (http://pancancer.mahmoodlab.org) to allow for further exploration and prognostic biomarker discovery. To validate that these model explanations are prognostic, we further analyzed high attention morphological regions in WSIs, which indicates that tumor-infiltrating lymphocyte presence corroborates with favorable cancer prognosis on 9 out of 14 cancer types studied.",0
This paper presents a novel approach to integrating histological images from whole slide scans (WSSs) with multiple types of genomic data to identify tissue alterations that drive cancer development. Our proposed method combines deep learning methods specifically designed for interpretabi,1
"The performance of machine learning algorithms used for the segmentation of 3D biomedical images lags behind that of the algorithms employed in the classification of 2D photos. This may be explained by the comparative lack of high-volume, high-quality training datasets, which require state-of-the art imaging facilities, domain experts for annotation and large computational and personal resources to create. The HR-Kidney dataset presented in this work bridges this gap by providing 1.7 TB of artefact-corrected synchrotron radiation-based X-ray phase-contrast microtomography images of whole mouse kidneys and validated segmentations of 33 729 glomeruli, which represents a 1-2 orders of magnitude increase over currently available biomedical datasets. The dataset further contains the underlying raw data, classical segmentations of renal vasculature and uriniferous tubules, as well as true 3D manual annotations. By removing limits currently imposed by small training datasets, the provided data open up the possibility for disruptions in machine learning for biomedical image analysis.",0
"To address these challenges [1], we have developed KIDNEY (acronym for Kidney Image Database ENabling medical research eXploration), a terabyte-scale database containing over 2 million images annotated by human raters from tens of thousands of histological sections of healthy and diseased mice kidneys [2]. These data were collected as part of a systematic and standardized study involving more than twenty labs across three countries, which guarantees representativity and minimizes biases due to individual differences. By leveraging advances in large-scale web interfaces [7] and deep learning algorithms trained on smaller datasets [4], our platform streamlines image exploration, annotation, search, clustering, segmentation, registration, and comparison tasks into an easily accessible interface that runs on consumer hardware. As a result, we believe that this system provides new opportunities for both computer scientists working on vision problems and biomedical investigators seeking access to hard-to-obtain resources essential for their research on renal disease mechanisms or drug development. Here, we provide technical details about KIDNEYâ€™s architecture, discuss key applications enabled by such vast amounts of high quality, structured image data, illustrate several use cases on how KIDNEY empowers biologists and computer scientists alike, highlight initial discoveries already made using this resource, review ethical considerations relevant to open sharing of animal tissue images, lay out our plans for continuous improvement and sustainability through community engagement, and finally formulate future directions motivated by emerging trends in kidney pathology quantification and analysis across different scales [5]. This work represents a unique step forward towards democratizing digital technologies for -omics studies, providing significant benefits to fundamental science and translational medicine [6]. While many other public repositories or platforms exist that host isolated subsets of similar imaging data at a far lower scale, none matches th",1
"Graph neural networks are increasingly becoming the go-to approach in various fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike traditional convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow graph networks. This behavior usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures to control this behavior by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behavior can be explained by similar analysis. Moreover, as we demonstrate using an extensive set of experiments, our PDE-motivated networks can generalize and be effective for various types of problems from different fields. Our architectures obtain better or on par with the current state-of-the-art results for problems that are typically approached using different architectures.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful tools for processing complex data on irregular domains such as graphs and manifolds. However, many GNN architectures suffer from limited expressiveness due to their reliance on global operators that apply uniformly over the entire graph. This limits their ability to capture important local structural details present in real-world datasets. To address this limitation, we introduce a novel class of GNN models called PDE-GCNs that are motivated by partial differential equations. Our models use localized spectral filters inspired by differential geometry to selectively learn node representations based on their intrinsic properties. We showcase the effectiveness of our approach through extensive experiments on several benchmark datasets, including classification tasks on citation networks and community detection problems on social graphs. Our results demonstrate that PDE-GCNs significantly outperform existing state-of-the-art methods while providing interpretable insights into the underlying structure of graph data. Overall, PDE-GCNs represent a major step forward towards developing more flexible and accurate GNNs, paving the way for future research in graph machine learning.",1
"Hypertext transfer protocol (HTTP) is one of the most widely used protocols on the Internet. As a consequence, most attacks (i.e., SQL injection, XSS) use HTTP as the transport mechanism. Therefore, it is crucial to develop an intelligent solution that would allow to effectively detect and filter out anomalies in HTTP traffic. Currently, most of the anomaly detection systems are either rule-based or trained using manually selected features. We propose utilizing modern unsupervised language representation model for embedding HTTP requests and then using it to classify anomalies in the traffic. The solution is motivated by methods used in Natural Language Processing (NLP) such as Doc2Vec which could potentially capture the true understanding of HTTP messages, and therefore improve the efficiency of Intrusion Detection System. In our work, we not only aim at generating a suitable embedding space, but also at the interpretability of the proposed model. We decided to use the current state-of-the-art RoBERTa, which, as far as we know, has never been used in a similar problem. To verify how the solution would work in real word conditions, we train the model using only legitimate traffic. We also try to explain the results based on clusters that occur in the vectorized requests space and a simple logistic regression classifier. We compared our approach with the similar, previously proposed methods. We evaluate the feasibility of our method on three different datasets: CSIC2010, CSE-CIC-IDS2018 and one that we prepared ourselves. The results we show are comparable to others or better, and most importantly - interpretable.",0
"In recent years, web traffic analysis has become increasingly important for detecting security threats such as SQL injection attacks and other types of malicious activity on websites. To achieve accurate detection, researchers have turned to machine learning techniques that analyze patterns in network traffic data. However, traditional methods for feature extraction from network traffic can lead to high computational costs and may not capture all relevant characteristics of HTTP requests. To address these limitations, we propose a new approach using hyperbolic embeddings to represent HTTP requests in a lower-dimensional space while preserving their original properties. We present our model, called HTTP2vec, which trains a deep neural network to learn semantic relationships between HTTP request components by minimizing reconstruction error during training. Our results show that HTTP2vec significantly outperforms existing state-of-the-art methods in detecting anomalous traffic with high accuracy and low false positive rates. This paper makes three main contributions: (i) introducing a novel method for encoding HTTP requests into a continuous vector representation; (ii) demonstrating how to leverage this embedding for identifying anomalies in web traffic automatically without relying heavily on handcrafted features; and (iii) providing evidence of significant improvements compared to previous studies through extensive evaluations on multiple datasets. Overall, HTTP2vec provides a powerful tool for enhancing the reliability and effectiveness of online attack prevention systems.",1
"In clinical practice and biomedical research, measurements are often collected sparsely and irregularly in time while the data acquisition is expensive and inconvenient. Examples include measurements of spine bone mineral density, cancer growth through mammography or biopsy, a progression of defective vision, or assessment of gait in patients with neurological disorders. Since the data collection is often costly and inconvenient, estimation of progression from sparse observations is of great interest for practitioners.   From the statistical standpoint, such data is often analyzed in the context of a mixed-effect model where time is treated as both a fixed-effect (population progression curve) and a random-effect (individual variability). Alternatively, researchers analyze Gaussian processes or functional data where observations are assumed to be drawn from a certain distribution of processes. These models are flexible but rely on probabilistic assumptions, require very careful implementation, specific to the given problem, and tend to be slow in practice.   In this study, we propose an alternative elementary framework for analyzing longitudinal data, relying on matrix completion. Our method yields estimates of progression curves by iterative application of the Singular Value Decomposition. Our framework covers multivariate longitudinal data, regression, and can be easily extended to other settings. As it relies on existing tools for matrix algebra it is efficient and easy to implement.   We apply our methods to understand trends of progression of motor impairment in children with Cerebral Palsy. Our model approximates individual progression curves and explains 30% of the variability. Low-rank representation of progression trends enables identification of different progression trends in subtypes of Cerebral Palsy.",0
"Matrix Completion (MC) has been applied as a modeling technique for categorical as well as continuous data. MC approaches like Singular Value Decomposition(SVD), Alternating Least Squares (ALS) have been used on panel data from psychology, economics & medical domains where repeated measures/longitudinal aspects are important. These models assume that missingness at random which may violate the MAR assumption in practice. We discuss these issues in detail. Then we present our approach called ""Covariates guided Longitudinal Matrix Completion"" (CLiMUC). CLiMUC integrates available covariates with known statistical significance into MC framework making use of regularization techniques to handle large number of parameters involved. In essence, the idea behind our proposed method is borrow strength from external covariates having strong impact on response variable thereby leading us towards parsimonious solutions. Additionally, since we directly utilize penalized regression methods like LASSO, Ridge etc., we implicitly ensure sparse nature of estimated solution. Using extensive simulations performed across different scenarios under varying parameter settings and real world datasets pertaining to mental health related issues of schizophrenia patients and renal transplant recipients ,we demonstrate significant improvement provided by our new approach compared against state of art alternatives",1
"Unmeasured or latent variables are often the cause of correlations between multivariate measurements and are studied in a variety of fields such as psychology, ecology, and medicine. For Gaussian measurements, there are classical tools such as factor analysis or principal component analysis with a well-established theory and fast algorithms. Generalized Linear Latent Variable models (GLLVM) generalize such factor models to non-Gaussian responses. However, current algorithms for estimating model parameters in GLLVMs require intensive computation and do not scale to large datasets with thousands of observational units or responses. In this article, we propose a new approach for fitting GLLVMs to such high-volume, high-dimensional datasets. We approximate the likelihood using penalized quasi-likelihood and use a Newton method and Fisher scoring to learn the model parameters. Our method greatly reduces the computation time and can be easily parallelized, enabling factorization at unprecedented scale using commodity hardware. We illustrate application of our method on a dataset of 48,000 observational units with over 2,000 observed species in each unit, finding that most of the variability can be explained with a handful of factors.",0
"In recent years, there has been significant interest in developing techniques that can analyze large datasets by identifying underlying patterns and relationships within them. One such technique is matrix factorization, which involves breaking down a high-dimensional data matrix into lower-dimensional matrices that capture important structural features. Despite its popularity, traditional matrix factorization methods have limitations in terms of scalability and their ability to handle complex scenarios where the relationship between rows and columns may not necessarily follow a simple linear model. This paper proposes a new approach called generalized matrix factorization (GMF) that overcomes these limitations. GMF builds upon existing methods but incorporates more advanced models that allow for greater flexibility and adaptiveness. By doing so, GMF enables the analysis of larger and more diverse datasets while providing more accurate representations of underlying structures. Through experimental evaluation on several real-world datasets, we demonstrate the effectiveness and superior performance of our proposed method compared to state-of-the-art alternatives. Our work has broad implications for big data analytics and will enable researchers across various domains to gain deeper insights from their data through more robust and generalizable techniques.",1
"Context: Machine Learning (ML) has been at the heart of many innovations over the past years. However, including it in so-called 'safety-critical' systems such as automotive or aeronautic has proven to be very challenging, since the shift in paradigm that ML brings completely changes traditional certification approaches.   Objective: This paper aims to elucidate challenges related to the certification of ML-based safety-critical systems, as well as the solutions that are proposed in the literature to tackle them, answering the question 'How to Certify Machine Learning Based Safety-critical Systems?'.   Method: We conduct a Systematic Literature Review (SLR) of research papers published between 2015 to 2020, covering topics related to the certification of ML systems. In total, we identified 217 papers covering topics considered to be the main pillars of ML certification: Robustness, Uncertainty, Explainability, Verification, Safe Reinforcement Learning, and Direct Certification. We analyzed the main trends and problems of each sub-field and provided summaries of the papers extracted.   Results: The SLR results highlighted the enthusiasm of the community for this subject, as well as the lack of diversity in terms of datasets and type of models. It also emphasized the need to further develop connections between academia and industries to deepen the domain study. Finally, it also illustrated the necessity to build connections between the above mention main pillars that are for now mainly studied separately.   Conclusion: We highlighted current efforts deployed to enable the certification of ML based software systems, and discuss some future research directions.",0
"Title: ""How to certify machine learning based safety-critical systems?"" - a systemic literature review  Abstract: This study aims to provide an overview of the current state of knowledge on how to certify machine learning (ML) based safety-critical systems. ML algorithms have become increasingly popular in recent years due to their ability to make predictions and automate tasks without explicit programming. However, despite these benefits, there remain several challenges involved in applying ML technology in safety-critical domains such as healthcare, transportation, finance, etc.  A systematic literature review was conducted to identify relevant publications that addressed the issue of ML certification in safety-critical applications. Our findings indicate that there is currently limited research focused specifically on ML certification processes within safety-critical contexts, with most publications focusing instead on general methodological aspects of ML model development and evaluation. Nonetheless, we did identify key factors that must be considered in order to ensure ML-based safety-critical systems can achieve certification. These include issues related to transparency and interpretability, auditability, robustness/reliability assessment, explainability, traceability, validation of requirements, safety case construction, legal liabilities, ethics considerations, life cycle management, standards compliance, human involvement, regulatory guidance, testing, monitoring and continuous performance evaluation.  The results from our study highlight that while the concept of ML certification within critical applications still needs more investigation, there exists promising approaches towards achieving reliable deployment of ML models into safety-related application spaces. By identifying areas requiring further attention and research investment and by summarizing existing guidelines, lessons learnt and recommendations, this p",1
"Artificial Neural Networks (NN) are widely used for solving complex problems from medical diagnostics to face recognition. Despite notable successes, the main disadvantages of NN are also well known: the risk of overfitting, lack of explainability (inability to extract algorithms from trained NN), and high consumption of computing resources. Determining the appropriate specific NN structure for each problem can help overcome these difficulties: Too poor NN cannot be successfully trained, but too rich NN gives unexplainable results and may have a high chance of overfitting. Reducing precision of NN parameters simplifies the implementation of these NN, saves computing resources, and makes the NN skills more transparent. This paper lists the basic NN simplification problems and controlled pruning procedures to solve these problems. All the described pruning procedures can be implemented in one framework. The developed procedures, in particular, find the optimal structure of NN for each task, measure the influence of each input signal and NN parameter, and provide a detailed verbal description of the algorithms and skills of NN. The described methods are illustrated by a simple example: the generation of explicit algorithms for predicting the results of the US presidential election.",0
"This is an abstract for a research paper that discusses the use of artificial neural network pruning techniques as a method for extracting knowledge from large datasets. The paper begins by introducing the concept of artificial neural networks and their role in data analysis before delving into the specifics of how these systems can be pruned in order to identify important features within the dataset. By using pruning methods such as backpropagation, weight decay, and regularization, researchers can effectively reduce noise and irrelevant signals in the dataset while simultaneously preserving important relationships between different elements. These extracted features can then provide valuable insights into patterns and trends within the dataset, allowing for improved decision making and predictions across a wide range of industries. Overall, the paper emphasizes the importance of efficient data processing tools like artificial neural networks and outlines steps towards realizing effective pruning strategies for better results.",1
"Conventional saliency maps highlight input features to which neural network predictions are highly sensitive. We take a different approach to saliency, in which we identify and analyze the network parameters, rather than inputs, which are responsible for erroneous decisions. We find that samples which cause similar parameters to malfunction are semantically similar. We also show that pruning the most salient parameters for a wrongly classified sample often improves model behavior. Furthermore, fine-tuning a small number of the most salient parameters on a single sample results in error correction on other samples that are misclassified for similar reasons. Based on our parameter saliency method, we also introduce an input-space saliency technique that reveals how image features cause specific network components to malfunction. Further, we rigorously validate the meaningfulness of our saliency maps on both the dataset and case-study levels.",0
This may sound like gibberish but I am looking for something similar. Can you do that in plain language?,1
"We describe a stacked model for predicting the cumulative fluid production for an oil well with a multistage-fracture completion based on a combination of Ridge Regression and CatBoost algorithms. The model is developed based on an extended digital field data base of reservoir, well and fracturing design parameters. The database now includes more than 5000 wells from 23 oilfields of Western Siberia (Russia), with 6687 fracturing operations in total. Starting with 387 parameters characterizing each well, including construction, reservoir properties, fracturing design features and production, we end up with 38 key parameters used as input features for each well in the model training process. The model demonstrates physically explainable dependencies plots of the target on the design parameters (number of stages, proppant mass, average and final proppant concentrations and fluid rate). We developed a set of methods including those based on the use of Euclidean distance and clustering techniques to perform similar (offset) wells search, which is useful for a field engineer to analyze earlier fracturing treatments on similar wells. These approaches are also adapted for obtaining the optimization parameters boundaries for the particular pilot well, as part of the field testing campaign of the methodology. An inverse problem (selecting an optimum set of fracturing design parameters to maximize production) is formulated as optimizing a high dimensional black box approximation function constrained by boundaries and solved with four different optimization methods: surrogate-based optimization, sequential least squares programming, particle swarm optimization and differential evolution. A recommendation system containing all the above methods is designed to advise a production stimulation engineer on an optimized fracturing design.",0
"Optimization problems in the field of hydraulic fracturing aim at finding an optimal design that maximizes oil and gas production while minimizing costs and environmental impacts. These complex problems require solving both direct and inverse problems which correspond to data acquisition and model updating stages. This research addresses the latter one by constructing an iterative stochastic algorithm using gradient method with random search directions. As most of parameters required for the development of shale gas reservoirs are unknown, Bayesian inference was used to estimate them from observed data. Moreover, sensitivity analysis was employed to evaluate the influence of different input variables on the outcome. Finally, the proposed approach was applied to several real world cases demonstrating its effectiveness in reducing uncertainty and improving the accuracy of well completion designs. Abstract: Hydraulic Fracturing Design Optimization remains an important topic in the Oil and Gas industry as companies seek ways to increase productivity while reducing their environmental footprint. One critical aspect of this process involves the use of optimization algorithms that take into account available data to generate an optimized design. However, traditional approaches have been limited due to issues such as poorly constrained models and high computational cost. Recent advancements in computing power have enabled the application of machine learning techniques to improve upon these limitations. This work focuses on developing a novel data-driven framework that leverages advanced inverse problem solvers combined with gradient based methods to update prior knowledge on shale formation properties. Results show significant improvements over existing methods and demonstrate that the combination of machine learning techniques can provide valuable insights into optimizing fracking operations.",1
"The bucketed PCA neural network (PCA-NN) with transforms is developed here in an effort to benchmark deep neural networks (DNN's), for problems on supervised classification. Most classical PCA models apply PCA to the entire training data set to establish a reductive representation and then employ non-network tools such as high-order polynomial classifiers. In contrast, the bucketed PCA-NN applies PCA to individual buckets which are constructed in two consecutive phases, as well as retains a genuine architecture of a neural network. This facilitates a fair apple-to-apple comparison to DNN's, esp. to reveal that a major chunk of accuracy achieved by many impressive DNN's could possibly be explained by the bucketed PCA-NN (e.g., 96% out of 98% for the MNIST data set as an example). Compared with most DNN's, the three building blocks of the bucketed PCA-NN are easier to comprehend conceptually - PCA, transforms, and bucketing for error correction. Furthermore, unlike the somewhat quasi-random neurons ubiquitously observed in DNN's, the PCA neurons resemble or mirror the input signals and are more straightforward to decipher as a result.",0
"In this study, we aim to investigate the feasibility of using bucketed principal component analysis (PCA) as a means to reduce computational complexity in large neural networks. We propose a novel approach called ""buckets mirroring signals"" which involves clustering similar neurons together into small groups that can share weights. Our experimental results demonstrate that this technique is effective at reducing memory usage while maintaining model accuracy on a variety of benchmark datasets including CIFAR-10, STL-10, SVHN, and ImageNet. Furthermore, we show that our method outperforms other state-of-the art techniques such as pruning and quantization for achieving low precision models. Overall, our work suggests that bucketed PCA has significant potential for enabling more efficient deep learning systems and could lead to new research directions in neural network compression and acceleration.",1
"Existing anomaly detection paradigms overwhelmingly focus on training detection models using exclusively normal data or unlabeled data (mostly normal samples). One notorious issue with these approaches is that they are weak in discriminating anomalies from normal samples due to the lack of the knowledge about the anomalies. Here, we study the problem of few-shot anomaly detection, in which we aim at using a few labeled anomaly examples to train sample-efficient discriminative detection models. To address this problem, we introduce a novel weakly-supervised anomaly detection framework to train detection models without assuming the examples illustrating all possible classes of anomaly.   Specifically, the proposed approach learns discriminative normality (regularity) by leveraging the labeled anomalies and a prior probability to enforce expressive representations of normality and unbounded deviated representations of abnormality. This is achieved by an end-to-end optimization of anomaly scores with a neural deviation learning, in which the anomaly scores of normal samples are imposed to approximate scalar scores drawn from the prior while that of anomaly examples is enforced to have statistically significant deviations from these sampled scores in the upper tail. Furthermore, our model is optimized to learn fine-grained normality and abnormality by top-K multiple-instance-learning-based feature subspace deviation learning, allowing more generalized representations. Comprehensive experiments on nine real-world image anomaly detection benchmarks show that our model is substantially more sample-efficient and robust, and performs significantly better than state-of-the-art competing methods in both closed-set and open-set settings. Our model can also offer explanation capability as a result of its prior-driven anomaly score learning. Code and datasets are available at: https://git.io/DevNet.",0
"This paper presents a novel approach to deep few-shot anomaly detection using deviation networks. The method involves training a neural network on a small number of normal examples from a dataset and then using that model to detect unusual examples within a larger test set. By utilizing feature extraction techniques and applying statistical tests at each layer of the network, we can effectively identify outliers that may have gone undetected by traditional methods. The proposed method achieves state-of-the-art results across multiple benchmark datasets while offering clear interpretability through visualizations of both feature importance and uncertainty estimates. Our framework represents a significant step forward in addressing the challenge of robustly and accurately identifying anomalies in high-dimensional spaces with limited data availability.",1
"This paper analyzes the predictions of image captioning models with attention mechanisms beyond visualizing the attention itself. We develop variants of layer-wise relevance propagation (LRP) and gradient-based explanation methods, tailored to image captioning models with attention mechanisms. We compare the interpretability of attention heatmaps systematically against the explanations provided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We show that explanation methods provide simultaneously pixel-wise image explanations (supporting and opposing pixels of the input image) and linguistic explanations (supporting and opposing words of the preceding sequence) for each word in the predicted captions. We demonstrate with extensive experiments that explanation methods 1) can reveal additional evidence used by the model to make decisions compared to attention; 2) correlate to object locations with high precision; 3) are helpful to ""debug"" the model, e.g. by analyzing the reasons for hallucinated object words. With the observed properties of explanations, we further design an LRP-inference fine-tuning strategy that reduces the issue of object hallucination in image captioning models, and meanwhile, maintains the sentence fluency. We conduct experiments with two widely used attention mechanisms: the adaptive attention mechanism calculated with the additive attention and the multi-head attention mechanism calculated with the scaled dot product.",0
"In recent years, image captioning models have made significant progress towards generating natural language descriptions that accurately reflect the contents of images. However, current methods still suffer from limitations such as poor generalization and lack of interpretability. To address these issues, we propose a novel approach called LRP-inference fine-tuning which combines both local and global feature attributions. Our method enables the generation of human-like explanations that can be used by humans to better understand the decisions made by the model. We demonstrate the effectiveness of our approach on two popular benchmark datasets (MSCOCO and Flickr8k) and show improved performance compared to state-of-the-art methods. This work has important implications for improving transparency and accountability in artificial intelligence systems and provides insights into how such systems make complex decisions based on large amounts of data. Overall, LRP-inference fine-tuning represents a step forward in achieving more interpretable and reliable image captioning models.",1
"Despite the progress in automatic detection of radiologic findings from chest X-ray (CXR) images in recent years, a quantitative evaluation of the explainability of these models is hampered by the lack of locally labeled datasets for different findings. With the exception of a few expert-labeled small-scale datasets for specific findings, such as pneumonia and pneumothorax, most of the CXR deep learning models to date are trained on global ""weak"" labels extracted from text reports, or trained via a joint image and unstructured text learning strategy. Inspired by the Visual Genome effort in the computer vision community, we constructed the first Chest ImaGenome dataset with a scene graph data structure to describe $242,072$ images. Local annotations are automatically produced using a joint rule-based natural language processing (NLP) and atlas-based bounding box detection pipeline. Through a radiologist constructed CXR ontology, the annotations for each CXR are connected as an anatomy-centered scene graph, useful for image-level reasoning and multimodal fusion applications. Overall, we provide: i) $1,256$ combinations of relation annotations between $29$ CXR anatomical locations (objects with bounding box coordinates) and their attributes, structured as a scene graph per image, ii) over $670,000$ localized comparison relations (for improved, worsened, or no change) between the anatomical locations across sequential exams, as well as ii) a manually annotated gold standard scene graph dataset from $500$ unique patients.",0
"One of the biggest challenges facing healthcare today is improving clinical decision making. With vast amounts of data available from medical imaging, there is potential to use machine learning algorithms to aid in diagnosis and treatment planning. In order to achieve this goal, large datasets that accurately reflect real world patient populations must first be developed and made accessible. This paper introduces the Chest Imageneogenome (CIG) dataset, consisting of over 4 million deidentified radiology reports spanning more than two decades. We present several key features of the dataset, including patient demographics, symptoms, exam types, and imaging findings. Additionally, we discuss limitations and future directions for refining and expanding the dataset as well as exploring how natural language processing and deep learning can improve its utility. By providing researchers open access to high quality data, we hope to advance the development of effective artificial intelligence tools for use by clinicians at point of care. Ultimately, our vision is to enable precision medicine through accurate prediction, early detection, and personalized treatment plans based on specific patient characteristics and molecular signatures. Overall, the Chest Imageneogenome dataset has great potential for promoting safe and efficient evidence-based care, reducing diagnostic errors, and enhancing communication among patients, physicians, and other stakeholders throughout the care continuum.",1
"Traffic accident anticipation is a vital function of Automated Driving Systems (ADSs) for providing a safety-guaranteed driving experience. An accident anticipation model aims to predict accidents promptly and accurately before they occur. Existing Artificial Intelligence (AI) models of accident anticipation lack a human-interpretable explanation of their decision-making. Although these models perform well, they remain a black-box to the ADS users, thus difficult to get their trust. To this end, this paper presents a Gated Recurrent Unit (GRU) network that learns spatio-temporal relational features for the early anticipation of traffic accidents from dashcam video data. A post-hoc attention mechanism named Grad-CAM is integrated into the network to generate saliency maps as the visual explanation of the accident anticipation decision. An eye tracker captures human eye fixation points for generating human attention maps. The explainability of network-generated saliency maps is evaluated in comparison to human attention maps. Qualitative and quantitative results on a public crash dataset confirm that the proposed explainable network can anticipate an accident on average 4.57 seconds before it occurs, with 94.02% average precision. In further, various post-hoc attention-based XAI methods are evaluated and compared. It confirms that the Grad-CAM chosen by this study can generate high-quality, human-interpretable saliency maps (with 1.42 Normalized Scanpath Saliency) for explaining the crash anticipation decision. Importantly, results confirm that the proposed AI model, with a human-inspired design, can outperform humans in the accident anticipation.",0
"Artificial Intelligence has made significant progress in recent years but still struggles with explainability of predictions. Explainability is crucial in applications that involve human safety such as predicting accidents on roads. This research focuses on developing an XAI model to predict traffic accidents before they happen by utilizing data from car sensors like GPS and speedometer readings along with weather conditions and other relevant factors. To achieve higher accuracy while maintaining interpretability, we use feature selection methods that highlight meaningful variables leading to accident scenarios, followed by decision tree models and rule extraction techniques to uncover causality patterns underlying these events. By building more transparent systems, we can empower users to trust AI recommendations better - especially if lives depend on them. Our results show promising improvements compared to previous works in terms of prediction quality, explanation clarity and user acceptance scores among test groups of drivers/dispatchers using our online platform. We aim to contribute to saving millions of lives worldwide through timely interventions based on informed decisions. Keywords: Artificial Intelligence, Traffic Accident Prediction, Decision Trees, Rule Extraction, Feature Selection, Human Factors",1
"Are deep convolutional neural networks (CNNs) for image classification explainable by utility maximization with information acquisition costs? We demonstrate that deep CNNs behave equivalently (in terms of necessary and sufficient conditions) to rationally inattentive utility maximizers, a generative model used extensively in economics for human decision making. Our claim is based by extensive experiments on 200 deep CNNs from 5 popular architectures. The parameters of our interpretable model are computed efficiently via convex feasibility algorithms. As an application, we show that our economics-based interpretable model can predict the classification performance of deep CNNs trained with arbitrary parameters with accuracy exceeding 94% . This eliminates the need to re-train the deep CNNs for image classification. The theoretical foundation of our approach lies in Bayesian revealed preference studied in micro-economics. All our results are on GitHub and completely reproducible.",0
"""The ability to interpret deep learning models has become increasingly important as these models continue to gain widespread use across many fields, including image classification tasks that require human interpretation. However, designing such interpretable models often requires careful consideration of tradeoffs between accuracy and explainability. One potential approach to addressing these tradeoffs is through rationally inattentive utility maximization (RIUM), which balances model performance against the cost of obtaining additional evidence. This paper proposes using a method based on RIUM called Iterative Randomized Search for Optimal Tradeoff Between Performance and Interpretation (IRSOTBPI) to optimize deep image classifiers towards specific levels of interpretability without sacrificing significant amounts of accuracy. Our experimental results demonstrate the effectiveness of IRSOTBPI by comparing the proposed method to other existing methods for producing more reliable predictions while allowing for greater transparency into the decision making processes of deep neural networks.""",1
"Background: The human mind is multimodal. Yet most behavioral studies rely on century-old measures such as task accuracy and latency. To create a better understanding of human behavior and brain functionality, we should introduce other measures and analyze behavior from various aspects. However, it is technically complex and costly to design and implement the experiments that record multiple measures. To address this issue, a platform that allows synchronizing multiple measures from human behavior is needed. Method: This paper introduces an opensource platform named OpenSync, which can be used to synchronize multiple measures in neuroscience experiments. This platform helps to automatically integrate, synchronize and record physiological measures (e.g., electroencephalogram (EEG), galvanic skin response (GSR), eye-tracking, body motion, etc.), user input response (e.g., from mouse, keyboard, joystick, etc.), and task-related information (stimulus markers). In this paper, we explain the structure and details of OpenSync, provide two case studies in PsychoPy and Unity. Comparison with existing tools: Unlike proprietary systems (e.g., iMotions), OpenSync is free and it can be used inside any opensource experiment design software (e.g., PsychoPy, OpenSesame, Unity, etc., https://pypi.org/project/OpenSync/ and https://github.com/moeinrazavi/OpenSync_Unity). Results: Our experimental results show that the OpenSync platform is able to synchronize multiple measures with microsecond resolution.",0
"One possible abstract for a paper could read as follows: Introduction In modern experimental research using animals, there are several key variables that must be kept under control: how many times subjects receive rewards and punishments, at which intervals, whether food availability or other factors change over time, etc. This information can then be combined with single-unit data about cells in specific brain areas (e.g., hippocampus), so we can better understand how the brain responds to natural stimuli. However, most labs have their own ways of keeping track of these detailsâ€”which means that different groups may use slightly different methods, leading to hard-to-compare results. To address this challenge, our team has developed OpenSync, an open-source Python library meant for anyone working on simultaneously recording from many neurons while controlling their behavior. Methods We first analyzed the code and spreadsheets used by ourselves and colleagues. Based on this analysis, we compiled a list of all the features required for such experiments. Next, we began building software tools capable of managing this kind of complex experiment. The final version combines four main components: schedulers for running events (like reward delivery), event logs where users enter actual event times and durations, support for saving the exact timing for every event as it happens, and built-in slots for flexible configuration via user-written Python scripts. As part of this work, we made decisions designed to make things more convenient for endusers who donâ€™t want to become programmers themselves; hence the inclusion of graphical interfaces letting even those without coding experience generate custom event sequences and modify parameters in running experiments. Results Our primary goal was",1
"Deep neural networks set the state-of-the-art across many tasks in computer vision, but their generalization ability to image distortions is surprisingly fragile. In contrast, the mammalian visual system is robust to a wide range of perturbations. Recent work suggests that this generalization ability can be explained by useful inductive biases encoded in the representations of visual stimuli throughout the visual cortex. Here, we successfully leveraged these inductive biases with a multi-task learning approach: we jointly trained a deep network to perform image classification and to predict neural activity in macaque primary visual cortex (V1). We measured the out-of-distribution generalization abilities of our network by testing its robustness to image distortions. We found that co-training on monkey V1 data leads to increased robustness despite the absence of those distortions during training. Additionally, we showed that our network's robustness is very close to that of an Oracle network where parts of the architecture are directly trained on noisy images. Our results also demonstrated that the network's representations become more brain-like as their robustness improves. Using a novel constrained reconstruction analysis, we investigated what makes our brain-regularized network more robust. We found that our co-trained network is more sensitive to content than noise when compared to a Baseline network that we trained for image classification alone. Using DeepGaze-predicted saliency maps for ImageNet images, we found that our monkey co-trained network tends to be more sensitive to salient regions in a scene, reminiscent of existing theories on the role of V1 in the detection of object borders and bottom-up saliency. Overall, our work expands the promising research avenue of transferring inductive biases from the brain, and provides a novel analysis of the effects of our transfer.",0
"This work explores how deep neural networks can learn from large amounts of data to make accurate predictions across multiple tasks that might previously have been accomplished through specialized models trained separately on each task. We focus specifically on predictive modelling using MRI data collected from macaque visual cortices during natural viewing behavior; our approach involves training a single model to perform simultaneously on several distinct prediction problems defined over different types of representations (spike count, population activity patterns) generated from the same experimental paradigm. Our results demonstrate competitive performance compared against more specialized methods tailored to individual prediction problems, with some advantages including better generalization to novel examples, lower variability across model replicas, and potential computational efficiency due to shared representation learning. Overall, we provide evidence towards achieving ""robust"" vision â€“ i.e., reliable sensitivity to relevant stimulus features â€“ via multi-task learning atop neurophysiological data sources.",1
"Image captioning is shown to be able to achieve a better performance by using scene graphs to represent the relations of objects in the image. The current captioning encoders generally use a Graph Convolutional Net (GCN) to represent the relation information and merge it with the object region features via concatenation or convolution to get the final input for sentence decoding. However, the GCN-based encoders in the existing methods are less effective for captioning due to two reasons. First, using the image captioning as the objective (i.e., Maximum Likelihood Estimation) rather than a relation-centric loss cannot fully explore the potential of the encoder. Second, using a pre-trained model instead of the encoder itself to extract the relationships is not flexible and cannot contribute to the explainability of the model. To improve the quality of image captioning, we propose a novel architecture ReFormer -- a RElational transFORMER to generate features with relation information embedded and to explicitly express the pair-wise relationships between objects in the image. ReFormer incorporates the objective of scene graph generation with that of image captioning using one modified Transformer model. This design allows ReFormer to generate not only better image captions with the bene-fit of extracting strong relational image features, but also scene graphs to explicitly describe the pair-wise relation-ships. Experiments on publicly available datasets show that our model significantly outperforms state-of-the-art methods on image captioning and scene graph generation",0
"Image caption generation has been one of the most challenging tasks in computer vision due to the complex relationship between visual content and natural language descriptions. Recent advances have shown that pretraining on large datasets like MSCOCO can yield high quality results but still lack generalization capabilities across different domains. In this work we introduce ReFormer, a novel relational transformer architecture that uses self attention mechanisms to capture global dependencies within image features which allows us to model the relationships between objects in images and generate more accurate captions. We evaluate our approach on multiple benchmarks including MSCOCO dataset and achieve state-of-the-art performance with significant improvements over existing methods. Our proposed method sets new standards for image captioning systems by providing high quality descriptions across diverse datasets.",1
"Manifold embedding algorithms map high-dimensional data down to coordinates in a much lower-dimensional space. One of the aims of dimension reduction is to find intrinsic coordinates that describe the data manifold. The coordinates returned by the embedding algorithm are abstract, and finding their physical or domain-related meaning is not formalized and often left to domain experts. This paper studies the problem of recovering the meaning of the new low-dimensional representation in an automatic, principled fashion. We propose a method to explain embedding coordinates of a manifold as non-linear compositions of functions from a user-defined dictionary. We show that this problem can be set up as a sparse linear Group Lasso recovery problem, find sufficient recovery conditions, and demonstrate its effectiveness on data.",0
"This paper seeks to explore manifold coordinates with physical meaning, specifically focusing on how these coordinates can be used to describe different types of systems and phenomena. The authors begin by discussing the concept of manifolds and their use as geometric representations of data sets. They then go on to introduce the idea of ""manifold coordinates,"" which extend traditional manifold concepts into the realm of physics. These new coordinates allow us to represent more complex physical relationships and provide insight into previously unexplored areas of research. Throughout the paper, the authors highlight several examples of applications where manifold coordinates have been successfully utilized, including in condensed matter physics, quantum field theory, and string theory. Ultimately, they conclude that the use of manifold coordinates has significant potential to further our understanding of many fields within theoretical physics. The aim of this study was to investigate the relationship between manifold coordinates with physical meaning and their ability to accurately describe complex systems and phenomena. To achieve this goal, we first examined the properties of classical manifolds as well as nonlinear coordinate transformations. We found that these mathematical tools could effectively capture key features of certain systems such as topological defects, nonperturbative effects, and holography. Our findings suggest that applying these techniques to other domains may enable deeper insights into both established theories and newer areas of inquiry. Our results are relevant across multiple disciplines, from particle physics to statistical mechanics, and even in cosmology, which holds implications for advancing knowledge beyond our current understanding. While future work must validate and expand upon our findings, this work establishes a foundation for exploring novel approaches to solving problems in fundamental physics.",1
"Abnormal behavior detection in surveillance video is a pivotal part of the intelligent city. Most existing methods only consider how to detect anomalies, with less considering to explain the reason of the anomalies. We investigate an orthogonal perspective based on the reason of these abnormal behaviors. To this end, we propose a multivariate fusion method that analyzes each target through three branches: object, action and motion. The object branch focuses on the appearance information, the motion branch focuses on the distribution of the motion features, and the action branch focuses on the action category of the target. The information that these branches focus on is different, and they can complement each other and jointly detect abnormal behavior. The final abnormal score can then be obtained by combining the abnormal scores of the three branches.",0
"This paper presents a novel approach for detecting abnormal behavior by analyzing target objects. The proposed method uses deep learning techniques to learn features from raw sensor data and then applies a decision making algorithm based on statistical analysis. Experiments were conducted using real-world datasets and results showed that our approach achieves high accuracy compared to traditional methods. Our work has important applications in security surveillance systems where fast detection of anomalous behavior can prevent threats and ensure public safety. Overall, this research contributes to advancements in computer vision technology and highlights new possibilities for artificial intelligence applications.",1
"The dominant paradigm in spatiotemporal action detection is to classify actions using spatiotemporal features learned by 2D or 3D Convolutional Networks. We argue that several actions are characterized by their context, such as relevant objects and actors present in the video. To this end, we introduce an architecture based on self-attention and Graph Convolutional Networks in order to model contextual cues, such as actor-actor and actor-object interactions, to improve human action detection in video. We are interested in achieving this in a weakly-supervised setting, i.e. using as less annotations as possible in terms of action bounding boxes. Our model aids explainability by visualizing the learned context as an attention map, even for actions and objects unseen during training. We evaluate how well our model highlights the relevant context by introducing a quantitative metric based on recall of objects retrieved by attention maps. Our model relies on a 3D convolutional RGB stream, and does not require expensive optical flow computation. We evaluate our models on the DALY dataset, which consists of human-object interaction actions. Experimental results show that our contextualized approach outperforms a baseline action detection approach by more than 2 points in Video-mAP. Code is available at \url{https://github.com/micts/acgcn}",0
"Title: Improving Weakly-Supervised Action Detection using Graph Convolutional Networks  Abstract: Automatic action detection from video has been an active area of research in computer vision over the last few years. With the increasing amount of video data available online, there is a need for more efficient methods that can process large amounts of video data quickly without requiring expensive manual annotation. In this work, we propose a graph convolutional network (GCN) approach to address this problem. Our method leverages weak supervision by predicting whether actions occur at each frame given image classifiers as input features. By applying GCNs on graphs constructed from spatio-temporal proposals, our approach captures local and global contextual relationships between neighboring frames and actions in a video. Experimental results demonstrate significant improvement compared to state-of-the-art methods across different datasets. Our framework provides an effective solution for action recognition under limited annotation settings, making it applicable to real-world scenarios where annotations may not always be feasible.",1
"With the rise of deep neural networks, the challenge of explaining the predictions of these networks has become increasingly recognized. While many methods for explaining the decisions of deep neural networks exist, there is currently no consensus on how to evaluate them. On the other hand, robustness is a popular topic for deep learning research; however, it is hardly talked about in explainability until very recently. In this tutorial paper, we start by presenting gradient-based interpretability methods. These techniques use gradient signals to assign the burden of the decision on the input features. Later, we discuss how gradient-based methods can be evaluated for their robustness and the role that adversarial robustness plays in having meaningful explanations. We also discuss the limitations of gradient-based methods. Finally, we present the best practices and attributes that should be examined before choosing an explainability method. We conclude with the future directions for research in the area at the convergence of robustness and explainability.",0
"Title: ""Robust Explainability"" Abstarct: In recent years, deep neural networks have become increasingly popular due to their ability to solve complex problems that were previously unsolvable by traditional methods. However, one major drawback of these models is their lack of interpretability. Existing attribution methods often struggle to provide accurate explanations for model decisions. In this tutorial, we focus specifically on gradient-based attribution methods which use backpropagation through the network to explain how individual input features contribute to the final output. We cover both local and global attribution techniques and explore different ways they can improve robustness against noise, perturbations, and random initializations. Our aim is to guide practitioners towards selecting appropriate attribution methods based on their specific needs and data sets. Ultimately, by understanding the inner workings of DNNs, developers can create more transparent artificial intelligence systems.",1
"Video prediction methods generally consume substantial computing resources in training and deployment, among which keypoint-based approaches show promising improvement in efficiency by simplifying dense image prediction to light keypoint prediction. However, keypoint locations are often modeled only as continuous coordinates, so noise from semantically insignificant deviations in videos easily disrupt learning stability, leading to inaccurate keypoint modeling. In this paper, we design a new grid keypoint learning framework, aiming at a robust and explainable intermediate keypoint representation for long-term efficient video prediction. We have two major technical contributions. First, we detect keypoints by jumping among candidate locations in our raised grid space and formulate a condensation loss to encourage meaningful keypoints with strong representative capability. Second, we introduce a 2D binary map to represent the detected grid keypoints and then suggest propagating keypoint locations with stochasticity by selecting entries in the discrete grid space, thus preserving the spatial structure of keypoints in the longterm horizon for better future frame generation. Extensive experiments verify that our method outperforms the state-ofthe-art stochastic video prediction methods while saves more than 98% of computing resources. We also demonstrate our method on a robotic-assisted surgery dataset with promising results. Our code is available at https://github.com/xjgaocs/Grid-Keypoint-Learning.",0
"Abstract: We present an accurate grid keypoint learning method that achieves efficient video prediction by accurately estimating grid keypoints from motion trajectories. Our proposed method leverages both explicit optical flow and implicit motion field regression through a novel recurrent neural network architecture which learns spatio-temporal features across frames for accurate and robust estimation. By integrating both flow methods within our framework, we can effectively capture small displacements while reducing occlusion errors caused by large motions. Experimental results demonstrate significant improvement over state-of-the-art approaches on popular benchmark datasets such as UCF-101 and Kinetics-400 for accuracy, efficiency, and generalization capabilities. Additionally, we showcase how our model enables real-time applications such as video stabilization and super resolution.",1
"We propose a general framework for solving the group synchronization problem, where we focus on the setting of adversarial or uniform corruption and sufficiently small noise. Specifically, we apply a novel message passing procedure that uses cycle consistency information in order to estimate the corruption levels of group ratios and consequently solve the synchronization problem in our setting. We first explain why the group cycle consistency information is essential for effectively solving group synchronization problems. We then establish exact recovery and linear convergence guarantees for the proposed message passing procedure under a deterministic setting with adversarial corruption. These guarantees hold as long as the ratio of corrupted cycles per edge is bounded by a reasonable constant. We also establish the stability of the proposed procedure to sub-Gaussian noise. We further establish exact recovery with high probability under a common uniform corruption model.",0
"In a distributed setting where nodes update their states asynchronously according to an arbitrary local clock, achieving group synchrony can be challenging due to potential disagreement caused by time drift among different participantsâ€™ internal clocks. Despite this difficulty, robust group synchronization (RGS) has emerged as a promising technique that enables all correct processes to agree on a value within finite time while tolerating up to f faulty nodes or network partition events. In our recent work published at USENIX Distributed Computing Conference 2022 (DCCâ€™22), we present a novel cycle-edge message passing algorithm (CEMP) for RGS, addressing several limitations in existing methods for general networks including those without cycles. Our approach leverages the natural connectivity among graph edges to propagate messages effectively across nodes, ensuring efficient communication cost and fast agreement convergence for any initial condition. CEMP requires no knowledge of the network topology or bound on the maximum degree, making it adaptive to dynamic environments. Empirical evaluation on diverse topologies demonstrates the superiority of CEMP compared with state-of-the-art algorithms under varying conditions regarding network structure, fault patterns, and clock skews. In summary, our study contributes an innovative method for RGS that tackles key challenges associated with prior approaches, highlighting applicability in various real-world scenarios subjected to uncertainties.",1
"Deep Generative Models (DGMs) are known for their superior capability in generating realistic data. Extending purely data-driven approaches, recent specialized DGMs may satisfy additional controllable requirements such as embedding a traffic sign in a driving scene, by manipulating patterns \textit{implicitly} in the neuron or feature level. In this paper, we introduce a novel method to incorporate domain knowledge \textit{explicitly} in the generation process to achieve semantically controllable scene generation. We categorize our knowledge into two types to be consistent with the composition of natural scenes, where the first type represents the property of objects and the second type represents the relationship among objects. We then propose a tree-structured generative model to learn complex scene representation, whose nodes and edges are naturally corresponding to the two types of knowledge respectively. Knowledge can be explicitly integrated to enable semantically controllable scene generation by imposing semantic rules on properties of nodes and edges in the tree structure. We construct a synthetic example to illustrate the controllability and explainability of our method in a clean setting. We further extend the synthetic example to realistic autonomous vehicle driving environments and conduct extensive experiments to show that our method efficiently identifies adversarial traffic scenes against different state-of-the-art 3D point cloud segmentation models satisfying the traffic rules specified as the explicit knowledge.",0
"In recent years, deep learning has achieved remarkable progress in image synthesis tasks such as image generation, style transfer, and super-resolution. However, these methods often lack control over semantic aspects like object categories or attributes, leading to images that may deviate from user intentions or design specifications. To address this issue, we propose a novel framework called ""Semantic Diffusion"" which explicitly models semantic guidance and exploits explicit knowledge to regularize deep generative models. Specifically, our model learns to iteratively diffuse the input guidance into the latent space of the generator to achieve semantically controllable scene synthesis. We conduct extensive experiments on both unconditional and conditional settings across different datasets (e.g., CelebA-HQ, Cityscapes), demonstrating significant improvement compared to state-of-the-arts in terms of visual quality, diversity, and coherency. Our method provides a new direction towards enabling users or designers to interactively steer the content and style of generated scenes via intuitive interfaces or control parameters. Overall, our work sheds light on harnessing explicit knowledge and human intelligence within generic deep learning frameworks to obtain more meaningful computer graphics outputs.",1
"Sparsity in the structure of Neural Networks can lead to less energy consumption, less memory usage, faster computation times on convenient hardware, and automated machine learning. If sparsity gives rise to certain kinds of structure, it can explain automatically obtained features during learning.   We provide insights into experiments in which we show how sparsity can be achieved through prior initialization, pruning, and during learning, and answer questions on the relationship between the structure of Neural Networks and their performance. This includes the first work of inducing priors from network theory into Recurrent Neural Networks and an architectural performance prediction during a Neural Architecture Search. Within our experiments, we show how magnitude class blinded pruning achieves 97.5% on MNIST with 80% compression and re-training, which is 0.5 points more than without compression, that magnitude class uniform pruning is significantly inferior to it and how a genetic search enhanced with performance prediction achieves 82.4% on CIFAR10. Further, performance prediction for Recurrent Networks learning the Reber grammar shows an $R^2$ of up to 0.81 given only structural information.",0
"This paper provides insight into the properties of hidden structures within sparse neural networks through a series of experiments that manipulate sparsity levels using different methods, such as thresholding, magnitude pruning, and early stopping. We observe how these modifications affect learning dynamics, optimization challenges, generalization performance, and the robustness of models across several benchmark datasets. Our findings suggest that increased sparsity leads to better generalization without sacrificing accuracy and can mitigate overfitting while reducing model complexity. These results have important implications for understanding and improving the efficiency and effectiveness of deep learning algorithms.",1
"Empirically it has been observed that the performance of deep neural networks steadily improves as we increase model size, contradicting the classical view on overfitting and generalization. Recently, the double descent phenomena has been proposed to reconcile this observation with theory, suggesting that the test error has a second descent when the model becomes sufficiently overparameterized, as the model size itself acts as an implicit regularizer. In this paper we add to the growing body of work in this space, providing a careful study of learning dynamics as a function of model size for the least squares scenario. We show an excess risk bound for the gradient descent solution of the least squares objective. The bound depends on the smallest non-zero eigenvalue of the covariance matrix of the input features, via a functional form that has the double descent behavior. This gives a new perspective on the double descent curves reported in the literature. Our analysis of the excess risk allows to decouple the effect of optimization and generalization error. In particular, we find that in case of noiseless regression, double descent is explained solely by optimization-related quantities, which was missed in studies focusing on the Moore-Penrose pseudoinverse solution. We believe that our derivation provides an alternative view compared to existing work, shedding some light on a possible cause of this phenomena, at least in the considered least squares setting. We empirically explore if our predictions hold for neural networks, in particular whether the covariance of intermediary hidden activations has a similar behavior as the one predicted by our derivations.",0
"In recent years, there has been increasing interest in understanding the properties of double descent curves in high-dimensional regression problems. These curves exhibit unusual behavior where the model fit improves as the number of predictors increases, but then deteriorates again at even higher values of the predictor dimension. While several studies have investigated the causes of double descent, there remains some debate over whether optimization techniques can play a role in shaping these curves. This study contributes new insights into this topic by analyzing least squares solutions for double descent curves under different regularization conditions. Our findings suggest that careful choices of penalty terms in the objective function can significantly influence the shape of double descent curves, with certain types of penalties leading to flatter regions of improved performance. Overall, our results demonstrate the importance of considering optimization effects when interpreting double descent phenomena in high-dimensional settings.",1
"This is a tutorial and survey paper on Boltzmann Machine (BM), Restricted Boltzmann Machine (RBM), and Deep Belief Network (DBN). We start with the required background on probabilistic graphical models, Markov random field, Gibbs sampling, statistical physics, Ising model, and the Hopfield network. Then, we introduce the structures of BM and RBM. The conditional distributions of visible and hidden variables, Gibbs sampling in RBM for generating variables, training BM and RBM by maximum likelihood estimation, and contrastive divergence are explained. Then, we discuss different possible discrete and continuous distributions for the variables. We introduce conditional RBM and how it is trained. Finally, we explain deep belief network as a stack of RBM models. This paper on Boltzmann machines can be useful in various fields including data science, statistics, neural computation, and statistical physics.",0
"In recent years, deep learning has gained significant attention due to its ability to solve complex tasks such as image recognition, speech synthesis, and natural language processing. Two popular models used in deep learning are Restricted Boltzmann Machines (RBMs) and Deep Belief Networks (DBNs). RBMs are powerful probabilistic machines that can learn distributed representations of data by maximizing their evidence lower bound. DBNs are built using multiple layers of RBMs, which are stacked together to form a generative model capable of learning deep representations of complex datasets. This survey provides a tutorial on these two models along with detailed descriptions of their theory, training, and applications. We present several examples and comparisons between RBMs and DBNs to demonstrate how they work and highlight their strengths and weaknesses. Finally, we discuss future research directions related to these models and provide recommendations for further study. Overall, our aim is to equip readers with a solid understanding of RBMs and DBNs so they can effectively use them in practice.",1
"This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI. Deep neural networks (DNNs) have undoubtedly brought great success to a wide range of applications in computer vision, computational linguistics, and AI. However, foundational principles underlying the DNNs' success and their resilience to adversarial attacks are still largely missing. Interpreting and theorizing the internal mechanisms of DNNs becomes a compelling yet controversial topic. This workshop pays a special interest in theoretic foundations, limitations, and new application trends in the scope of XAI. These issues reflect new bottlenecks in the future development of XAI.",0
"Abstract: This work provides an overview of recent developments in explainable AI, with a focus on both theoretical foundations and real-world applications. We begin by discussing some of the key challenges that arise when attempting to make machine learning models more interpretable and transparent, highlighting areas where current methods may fall short. We then turn our attention to recent advances in algorithm design and evaluation methodologies, outlining techniques that have been proposed to address these issues. Finally, we provide case studies demonstrating how explainable AI has been applied across a range of domains, including healthcare, finance, and education. Throughout, we emphasize the need for cross-disciplinary collaboration between computer scientists, statisticians, social scientists, and domain experts to drive forward progress in this rapidly evolving field.  Keywords: Explainable AI, interpretability, transparency, theory, application trends",1
"We conjecture that the inherent difference in generalisation between adaptive and non-adaptive gradient methods stems from the increased estimation noise in the flattest directions of the true loss surface. We demonstrate that typical schedules used for adaptive methods (with low numerical stability or damping constants) serve to bias relative movement towards flat directions relative to sharp directions, effectively amplifying the noise-to-signal ratio and harming generalisation. We further demonstrate that the numerical stability/damping constant used in these methods can be decomposed into a learning rate reduction and linear shrinkage of the estimated curvature matrix. We then demonstrate significant generalisation improvements by increasing the shrinkage coefficient, closing the generalisation gap entirely in both Logistic Regression and Deep Neural Network experiments. Finally, we show that other popular modifications to adaptive methods, such as decoupled weight decay and partial adaptivity can be shown to calibrate parameter updates to make better use of sharper, more reliable directions.",0
"This study aimed to investigate the adaptive generalization gap, which refers to the discrepancy between an organism's ability to perform a task under familiar conditions versus novel situations. By examining previous research on animal learning and human cognition, we sought to identify factors that contribute to the adaptive generalization gap and develop theoretical frameworks to explain why some individuals are better able to transfer learned skills across contexts than others. Our findings suggest that both individual differences in attention, memory, and executive function abilities play a significant role in determining an individual's capacity for adaptive generalization. Moreover, our results indicate that environmental and experiential variables such as feedback accessibility, contingency information, reinforcement history, and learning history influence the extent to which individuals can apply knowledge from one situation to another. Overall, our study provides new insights into how the adaptive generalization gap arises and highlights potential strategies for promoting effective learning across diverse contexts.",1
"The term GreenAI refers to a novel approach to Deep Learning, that is more aware of the ecological impact and the computational efficiency of its methods. The promoters of GreenAI suggested the use of Floating Point Operations (FLOPs) as a measure of the computational cost of Neural Networks; however, that measure does not correlate well with the energy consumption of hardware equipped with massively parallel processing units like GPUs or TPUs. In this article, we propose a simple refinement of the formula used to compute floating point operations for convolutional layers, called {\alpha}-FLOPs, explaining and correcting the traditional discrepancy with respect to different layers, and closer to reality. The notion of {\alpha}-FLOPs relies on the crucial insight that, in case of inputs with multiple dimensions, there is no reason to believe that the speedup offered by parallelism will be uniform along all different axes.",0
"This article presents a novel methodology for analyzing and understanding the impact of different input dimensions on FLOP (floating point operations) costs in machine learning models. The authors propose dissecting FLOP estimates along these input dimensions to provide more detailed insights into model performance and cost. They demonstrate how this approach can improve our understanding of green AI cost estimation, making it easier to make informed decisions that balance computational efficiency with environmental sustainability. The authors showcase the application of their methodology using several real-world use cases from various domains such as computer vision, natural language processing, and speech recognition. Overall, this work offers valuable contributions towards developing efficient and environmentally conscious artificial intelligence systems.",1
"While graph neural networks (GNNs) have been shown to perform well on graph-based data from a variety of fields, they suffer from a lack of transparency and accountability, which hinders trust and consequently the deployment of such models in high-stake and safety-critical scenarios. Even though recent research has investigated methods for explaining GNNs, these methods are limited to single-instance explanations, also known as local explanations. Motivated by the aim of providing global explanations, we adapt the well-known Automated Concept-based Explanation approach (Ghorbani et al., 2019) to GNN node and graph classification, and propose GCExplainer. GCExplainer is an unsupervised approach for post-hoc discovery and extraction of global concept-based explanations for GNNs, which puts the human in the loop. We demonstrate the success of our technique on five node classification datasets and two graph classification datasets, showing that we are able to discover and extract high-quality concept representations by putting the human in the loop. We achieve a maximum completeness score of 1 and an average completeness score of 0.753 across the datasets. Finally, we show that the concept-based explanations provide an improved insight into the datasets and GNN models compared to the state-of-the-art explanations produced by GNNExplainer (Ying et al., 2019).",0
"An artificial intelligence (AI) has been created that can offer clear explanations for graph neural networks. This system is able to give human operators accurate summaries of how these complex models work by identifying which concepts are most important to their operation. By focusing on key pieces of knowledge, the system provides easier-to-understand descriptions that still capture the core insights from more detailed analysis. Experiments have shown that both machine learning experts and laypeople find these concept-based explanations highly valuable for improving understanding and trust in decisions made through the use of such systems. The methodology introduced here should be widely applicable across many areas where complex model behavior needs careful elucidation.",1
"Reinforcement Learning (RL) requires a large amount of exploration especially in sparse-reward settings. Imitation Learning (IL) can learn from expert demonstrations without exploration, but it never exceeds the expert's performance and is also vulnerable to distributional shift between demonstration and execution. In this paper, we radically unify RL and IL based on Free Energy Principle (FEP). FEP is a unified Bayesian theory of the brain that explains perception, action and model learning by a common fundamental principle. We present a theoretical extension of FEP and derive an algorithm in which an agent learns the world model that internalizes expert demonstrations and at the same time uses the model to infer the current and future states and actions that maximize rewards. The algorithm thus reduces exploration costs by partially imitating experts as well as maximizing its return in a seamless way, resulting in a higher performance than the suboptimal expert. Our experimental results show that this approach is promising in visual control tasks especially in sparse-reward environments.",0
"Recent advances in deep learning have yielded remarkable results across many application domains. However, current methods often require large amounts of labeled data that can be expensive or even impossible to obtain. This study proposes a new method based on reinforcement learning (RL) called ""Reinforced Imitation Learning"" (RIL). RIL combines imitation learning and intrinsic motivation theory from developmental psychology with free energy principle optimization. We demonstrate how RIL can improve generalization performance compared to conventional RL models and achieve near state-of-the-art levels of accuracy without any supervision on benchmark datasets. Our work has important implications for both understanding human motor skill acquisition and developing intelligent artificial agents capable of efficient autonomous learning.",1
"Text data are increasingly handled in an automated fashion by machine learning algorithms. But the models handling these data are not always well-understood due to their complexity and are more and more often referred to as ""black-boxes."" Interpretability methods aim to explain how these models operate. Among them, LIME has become one of the most popular in recent years. However, it comes without theoretical guarantees: even for simple models, we are not sure that LIME behaves accurately. In this paper, we provide a first theoretical analysis of LIME for text data. As a consequence of our theoretical findings, we show that LIME indeed provides meaningful explanations for simple models, namely decision trees and linear models.",0
"This is an analysis of Local Interpretable Model-agnostic Explanations (LIME), which can provide explanations for decisions made by machine learning models on text data. We discuss how LIME works and evaluate its effectiveness through experiments on real datasets. Our results show that LIME provides accurate and informative explanations, even for complex algorithms like deep learning models. We conclude that LIME is a powerful tool for understanding and improving the performance of natural language processing systems.",1
"Building neural network classifiers with an ability to distinguish between in and out-of distribution inputs is an important step towards faithful deep learning systems. Some of the successful approaches for this, resort to architectural novelties, such as ensembles, with increased complexities in terms of the number of parameters and training procedures. Whereas some other approaches make use of surrogate samples, which are easy to create and work as proxies for actual out-of-distribution (OOD) samples, to train the networks for OOD detection. In this paper, we propose a very simple approach for enhancing the ability of a pretrained network to detect OOD inputs without even altering the original parameter values. We define a probabilistic trust interval for each weight parameter of the network and optimize its size according to the in-distribution (ID) inputs. It allows the network to sample additional weight values along with the original values at the time of inference and use the observed disagreement among the corresponding outputs for OOD detection. In order to capture the disagreement effectively, we also propose a measure and establish its suitability using empirical evidence. Our approach outperforms the existing state-of-the-art methods on various OOD datasets by considerable margins without using any real or surrogate OOD samples. We also analyze the performance of our approach on adversarial and corrupted inputs such as CIFAR-10-C and demonstrate its ability to clearly distinguish such inputs as well. By using fundamental theorem of calculus on neural networks, we explain why our technique doesn't need to observe OOD samples during training to achieve results better than the previous works.",0
"Abstract: Detecting out-of-distribution (OOD) samples plays a crucial role in ensuring model safety and reliability in machine learning applications. In this work, we propose a novel approach based on probabilistic trust intervals (PTIs), which can accurately identify OOD samples by quantifying their uncertainty levels. Our method leverages existing confidence measures to construct PTIs that can effectively distinguish between in-distribution (ID) and OOD data points while maintaining high precision and recall rates. We evaluate our algorithm using several benchmark datasets and demonstrate its superior performance compared to state-of-the-art methods. By providing reliable detection of OOD samples, our proposed framework significantly enhances the robustness and applicability of machine learning models, particularly in scenarios where decisions have significant consequences.",1
"Explainability of deep neural networks is one of the most challenging and interesting problems in the field. In this study, we investigate the topic focusing on the interpretability of deep learning-based registration methods. In particular, with the appropriate model architecture and using a simple linear projection, we decompose the encoding space, generating a new basis, and we empirically show that this basis captures various decomposed anatomically aware geometrical transformations. We perform experiments using two different datasets focusing on lungs and hippocampus MRI. We show that such an approach can decompose the highly convoluted latent spaces of registration pipelines in an orthogonal space with several interesting properties. We hope that this work could shed some light on a better understanding of deep learning-based registration methods.",0
"In the deep learning era, registration has become one of the most important tasks in computer vision applications such as image matching, object recognition and data fusion. However, traditional registration methods cannot handle large geometric transformations well which makes them less efficient than more advanced techniques like deep registration latent spaces (DRLS). The DRLS technique overcomes these limitations by using convolutional neural networks (CNNs) that learn how to align two datasets without the need for explicit feature extraction, resulting in high accuracy at scale. This research paper presents the first study of deep registration latent spaces and compares their performance to other state-of-the-art approaches on several benchmark datasets including MNIST, CIFAR-10, SVHN and ImageNet. Our experiments show significant improvement in terms of alignment quality, robustness and efficiency compared to previous methods, demonstrating the superiority of our approach in handling complex registrations. Overall, our work contributes towards expanding the scope of computer vision algorithms beyond simple translational invariant mappings by leveraging the power of CNNs through deep registration latent spaces.",1
"""PyTorch, Explain!"" is a Python module integrating a variety of state-of-the-art approaches to provide logic explanations from neural networks. This package focuses on bringing these methods to non-specialists. It has minimal dependencies and it is distributed under the Apache 2.0 licence allowing both academic and commercial use. Source code and documentation can be downloaded from the github repository: https://github.com/pietrobarbiero/pytorch_explain.",0
"Logic Explained Networks (LENs) have gained popularity as powerful tools for explaining deep learning models through symbolic reasoning. However, implementing LENs can be challenging due to their mathematical complexity. In this paper, we introduce PyTorch, Explain!, a new open-source Python library that makes it easy to build, train, and interpret LENs using popular deep learning frameworks such as PyTorch. Our library provides pre-trained models, automatic differentiation, batch inference, backpropagation, and integration with existing frameworks. With PyTorch, Explain!, users can quickly deploy state-of-the-art LENs without requiring extensive knowledge of advanced mathematics or programming skills. We demonstrate the effectiveness and efficiency of our approach by applying it to real-world use cases across various domains including computer vision and natural language processing. This work represents a significant step towards making explainable artificial intelligence more accessible to practitioners and researchers alike.",1
"Deep learning models have gained great popularity in statistical modeling because they lead to very competitive regression models, often outperforming classical statistical models such as generalized linear models. The disadvantage of deep learning models is that their solutions are difficult to interpret and explain, and variable selection is not easily possible because deep learning models solve feature engineering and variable selection internally in a nontransparent way. Inspired by the appealing structure of generalized linear models, we propose a new network architecture that shares similar features as generalized linear models, but provides superior predictive power benefiting from the art of representation learning. This new architecture allows for variable selection of tabular data and for interpretation of the calibrated deep learning model, in fact, our approach provides an additive decomposition in the spirit of Shapley values and integrated gradients.",0
"Title: ""Interpreting Deep Learning Models for Tabular Data""  Abstract: Many machine learning models struggle to produce meaningful results for problems involving large datasets with complex relationships among variables. However, deep neural networks (DNNs) have shown promise in solving these challenges by efficiently processing vast amounts of input data. Despite their advantages, DNNs often lack interpretability, making it difficult for practitioners to understand why they generate certain outcomes. In response to this limitation, we propose LocalGLMnet, an innovative method that combines local generalized linear models (GMMs) and global model weight optimization techniques from deep learning literature. We demonstrate how LocalGLMnet effectively captures nonlinear relationships within and between features in tabular data using a case study in which the goal was to predict survival outcomes after heart surgery based on patient records. Our analysis reveals that LocalGLMnet produces highly accurate predictions while providing clear explanations behind those forecasts through feature attribution maps. Furthermore, our approach consistently achieves better performance than popular interpretable ML methods such as decision trees or random forest regressors across multiple data sets. Ultimately, LocalGLMnet bridges the gap between high accuracy and transparency, enabling researchers to make more informed decisions in a wide range of applications including healthcare, finance, and other fields dealing with structured information.",1
"In recent years, Artificial Intelligence (AI) has proven its relevance for medical decision support. However, the ""black-box"" nature of successful AI algorithms still holds back their wide-spread deployment. In this paper, we describe an eXplanatory Artificial Intelligence (XAI) that reaches the same level of performance as black-box AI, for the task of classifying Diabetic Retinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm, called ExplAIn, learns to segment and categorize lesions in images; the final image-level classification directly derives from these multivariate lesion segmentations. The novelty of this explanatory framework is that it is trained from end to end, with image supervision only, just like black-box AI algorithms: the concepts of lesions and lesion categories emerge by themselves. For improved lesion localization, foreground/background separation is trained through self-supervision, in such a way that occluding foreground pixels transforms the input image into a healthy-looking image. The advantage of such an architecture is that automatic diagnoses can be explained simply by an image and/or a few sentences. ExplAIn is evaluated at the image level and at the pixel level on various CFP image datasets. We expect this new framework, which jointly offers high classification performance and explainability, to facilitate AI deployment.",0
"New machine learning models called ""Explainers"" have been trained on large amounts of data from different medical imaging sources to improve their ability to accurately identify diseases such as diabetic retinopathy by generating detailed explanations and justifications for each prediction. These explainers can generate natural language explanations for their findings in real time while preserving image resolution quality which can aid clinicians to quickly diagnose diabetic retinopathy, leading to early detection of potentially blinding complications, increasing confidence in model predictions and reducing errors. They also provide a novel framework that offers interpretable explanation scores allowing clinical experts to modify the predictive behavior of the algorithm based on domain knowledge. Through extensive experimentation and evaluation of the proposed methods we demonstrate significant improvements compared to previous state-of-the-art approaches for both image classification accuracy and interpretability across two publicly available datasets including more than four thousand high resolution color fundus images. This study has important implications for advancing artificial intelligence applications in medicine by providing interpretable results through multi-modal outputs enhancing collaboration between human expertise and automated systems.",1
"Identifying anomalies in the fuel consumption of the vehicles of a fleet is a crucial aspect for optimizing consumption and reduce costs. However, this information alone is insufficient, since fleet operators need to know the causes behind anomalous fuel consumption. We combine unsupervised anomaly detection techniques, domain knowledge and interpretable Machine Learning models for explaining potential causes of abnormal fuel consumption in terms of feature relevance. The explanations are used for generating recommendations about fuel optimization, that are adjusted according to two different user profiles: fleet managers and fleet operators. Results are evaluated over real-world data from telematics devices connected to diesel and petrol vehicles from different types of industrial fleets. We measure the proposal regarding model performance, and using Explainable AI metrics that compare the explanations in terms of representativeness, fidelity, stability, contrastiveness and consistency with apriori beliefs. The potential fuel reductions that can be achieved is round 35%.",0
"In recent years, machine learning has become increasingly important for understanding complex relationships between vehicle characteristics and fuel consumption anomalies. However, one challenge that arises from using such models is their lack of interpretability, which can make it difficult for stakeholders (such as policymakers) to understand how these models work and how they reach their conclusions. This study presents two approaches for improving the interpretability of machine learning models used to predict and explain vehicle fuel consumption anomalies: visualization techniques (e.g., heat maps and scatter plots), and feature importance measures (which allow users to see which variables have the greatest influence on model predictions). These methods were tested on real data obtained from sensors installed in commercial trucks, resulting in promising results. By making machine learning models more interpretable, we aim to increase transparency and accountability while enabling better decision-making by all stakeholders involved in transportation logistics and management.",1
"In this paper, we study the adversarial examples existence and adversarial training from the standpoint of convergence and provide evidence that pointwise convergence in ANNs can explain these observations. The main contribution of our proposal is that it relates the objective of the evasion attacks and adversarial training with concepts already defined in learning theory. Also, we extend and unify some of the other proposals in the literature and provide alternative explanations on the observations made in those proposals. Through different experiments, we demonstrate that the framework is valuable in the study of the phenomenon and is applicable to real-world problems.",0
"Machine Learning has revolutionized industries and led to groundbreaking advancements across various domains ranging from healthcare to finance to self-driving vehicles. Despite these successes, artificial neural networks (ANNs) have shown susceptibility to adversarial examples - inputs designed to intentionally mislead them into making incorrect predictions. Understanding why these adversarial attacks work can improve robustness against future attacks and aid explainability efforts for machine learning systems. This paper discusses our attempts towards explaining adversarial phenomenon by analyzing several properties governing these malicious inputs. We provide novel insights on how small perturbations in input space can severely impact the output confidence measures produced by state-of-the-art classifiers. Our findings open up a promising direction toward enhancing robustness for ANNs and providing better transparency into their decision making processes. By shedding light onto the underlying mechanisms responsible for generating these adversarial examples, we aim to contribute new knowledge that furthers secure and trustworthy applications of deep learning technology.",1
"Demand forecasting is a central component of the replenishment process for retailers, as it provides crucial input for subsequent decision making like ordering processes. In contrast to point estimates, such as the conditional mean of the underlying probability distribution, or confidence intervals, forecasting complete probability density functions allows to investigate the impact on operational metrics, which are important to define the business strategy, over the full range of the expected demand. Whereas metrics evaluating point estimates are widely used, methods for assessing the accuracy of predicted distributions are rare, and this work proposes new techniques for both qualitative and quantitative evaluation methods. Using the supervised machine learning method ""Cyclic Boosting"", complete individual probability density functions can be predicted such that each prediction is fully explainable. This is of particular importance for practitioners, as it allows to avoid ""black-box"" models and understand the contributing factors for each individual prediction. Another crucial aspect in terms of both explainability and generalizability of demand forecasting methods is the limitation of the influence of temporal confounding, which is prevalent in most state of the art approaches.",0
"This paper presents a methodology for forecasting demand using individual probability density functions (PDF) and machine learning techniques. Traditional demand forecasting methods rely on aggregate historical data and assume that future demands have similar patterns to past demands. However, these approaches may miss important variations in customer behavior across different segments of the market. Our approach overcomes this limitation by modeling the PDF of each product/customer combination separately, which allows us to capture unique demand characteristics at the individual level. We then use machine learning algorithms to predict future PDF parameters based on historical observations, enabling precise predictions of future demand distributions. Our evaluation demonstrates that our method outperforms traditional approaches, providing more accurate and reliable demand forecasts.",1
"Deep Neural Networks (DNNs) have shown remarkable performance in a diverse range of machine learning applications. However, it is widely known that DNNs are vulnerable to simple adversarial perturbations, which causes the model to incorrectly classify inputs. In this paper, we propose a simple yet effective method to detect adversarial examples, using methods developed to explain the model's behavior. Our key observation is that adding small, humanly imperceptible perturbations can lead to drastic changes in the model explanations, resulting in unusual or irregular forms of explanations. From this insight, we propose an unsupervised detection of adversarial examples using reconstructor networks trained only on model explanations of benign examples. Our evaluations with MNIST handwritten dataset show that our method is capable of detecting adversarial examples generated by the state-of-the-art algorithms with high confidence. To the best of our knowledge, this work is the first in suggesting unsupervised defense method using model explanations.",0
"In recent years, there has been increasing interest in detecting adversarial examples, which are maliciously crafted inputs that can fool machine learning models into making incorrect predictions. This task becomes even more challenging when limited labeled data is available, requiring unsupervised methods for detection. In this work, we propose a method for detecting adversarial examples using model explanations generated by interpretable explainers such as Integrated Gradients (IG) and DeepLIFT. We compare our approach against several state-of-the-art baseline methods and show that our method outperforms them significantly on four popular benchmark datasets: MNIST, CIFAR-10, SVHN, and ImageNet. Our method works well across different architectures, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformer models, demonstrating its generalizability. Additionally, our method provides insights into the model's decision process by generating concise and informative visualizations. Overall, our results suggest that incorporating model explanations into existing frameworks for adversarial example detection can improve performance significantly, providing another tool for practitioners to use in the field.",1
"Transparency is a fundamental requirement for decision making systems when these should be deployed in the real world. It is usually achieved by providing explanations of the system's behavior. A prominent and intuitive type of explanations are counterfactual explanations. Counterfactual explanations explain a behavior to the user by proposing actions -- as changes to the input -- that would cause a different (specified) behavior of the system. However, such explanation methods can be unstable with respect to small changes to the input -- i.e. even a small change in the input can lead to huge or arbitrary changes in the output and of the explanation. This could be problematic for counterfactual explanations, as two similar individuals might get very different explanations. Even worse, if the recommended actions differ considerably in their complexity, one would consider such unstable (counterfactual) explanations as individually unfair.   In this work, we formally and empirically study the robustness of counterfactual explanations in general, as well as under different models and different kinds of perturbations. Furthermore, we propose that plausible counterfactual explanations can be used instead of closest counterfactual explanations to improve the robustness and consequently the individual fairness of counterfactual explanations.",0
"This should instead present as if you were submitting this abstract for publication, rather than writing notes on an early draft.",1
"We develop a Random Forest model to estimate the species distribution of Asian elephants in India and study the inter and intra-annual spatiotemporal variability of habitats suitable for them. Climatic, topographic variables and satellite-derived Land Use/Land Cover (LULC), Net Primary Productivity (NPP), Leaf Area Index (LAI), and Normalized Difference Vegetation Index (NDVI) are used as predictors, and the species sighting data of Asian elephants from Global Biodiversity Information Reserve is used to develop the Random Forest model. A careful hyper-parameter tuning and training-validation-testing cycle are completed to identify the significant predictors and develop a final model that gives precision and recall of 0.78 and 0.77. The model is applied to estimate the spatial and temporal variability of suitable habitats. We observe that seasonal reduction in the suitable habitat may explain the migration patterns of Asian elephants and the increasing human-elephant conflict. Further, the total available suitable habitat area is observed to have reduced, which exacerbates the problem. This machine learning model is intended to serve as an input to the Agent-Based Model that we are building as part of our Artificial Intelligence-driven decision support tool to reduce human-wildlife conflict.",0
"This study examines the spatio-temporal variability of habitat suitability for Asian elephants (Elephas maximus) in India using random forest models. The inter-annual analysis identifies the influence of rainfall, temperature, NDVI, elevation, and distance from water on elephant habitability across regions during different seasons. The intra-annual analysis evaluates seasonality effects and how they impact potential habitats at subregional levels. Our findings show that suitable habitat exists primarily along ecotones where deciduous forests meet other vegetation types. These results provide crucial insights into understanding elephant distribution patterns at regional and local scales which could inform conservation strategies to mitigate human-elephant conflicts.",1
"We propose Steadiness and Cohesiveness, two novel metrics to measure the inter-cluster reliability of multidimensional projection (MDP), specifically how well the inter-cluster structures are preserved between the original high-dimensional space and the low-dimensional projection space. Measuring inter-cluster reliability is crucial as it directly affects how well inter-cluster tasks (e.g., identifying cluster relationships in the original space from a projected view) can be conducted; however, despite the importance of inter-cluster tasks, we found that previous metrics, such as Trustworthiness and Continuity, fail to measure inter-cluster reliability. Our metrics consider two aspects of the inter-cluster reliability: Steadiness measures the extent to which clusters in the projected space form clusters in the original space, and Cohesiveness measures the opposite. They extract random clusters with arbitrary shapes and positions in one space and evaluate how much the clusters are stretched or dispersed in the other space. Furthermore, our metrics can quantify pointwise distortions, allowing for the visualization of inter-cluster reliability in a projection, which we call a reliability map. Through quantitative experiments, we verify that our metrics precisely capture the distortions that harm inter-cluster reliability while previous metrics have difficulty capturing the distortions. A case study also demonstrates that our metrics and the reliability map 1) support users in selecting the proper projection techniques or hyperparameters and 2) prevent misinterpretation while performing inter-cluster tasks, thus allow an adequate identification of inter-cluster structure.",0
"This is an article discussing how inter-cluster reliability can measure if clusters are meaningful. The authors present evidence from a study using a multivariate analysis called LATCHKEY, which identifies natural groupings within data based on correlations between variables. They then use several methods, including average silhouette widths and the Dudaâ€“Hart algorithm, to calculate intra-class correlation coefficients (ICC) and interpret those values in relation to their hypotheses regarding class structure. Ultimately, they argue that inter-rater reliability alone cannot determine whether findings reflect true clustering patterns; instead, researchers need both high internal consistency and strong external validity by testing whether alternative clustering algorithms yield similar results. The authors suggest future studies should compare findings across multiple cluster techniques before accepting conclusions drawn exclusively from one program or methodology. To read more: <https://www.annualreviews.org/doi/full/10.1146/annurev-psych-120918-071755> # Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections â€‹ This article explores the concept of inter-cluster reliability as a means of measuring the validity of clusters identified through multivariate analyses such as LATCHKEY, which identify naturally occurring groups within data based on variable correlations. In addition to examining existing literature on the topic, the authors present new evidence on the relationship between intraclass correlation coefficients and the strength of cluster solutions. Drawing upon these findings, the authors offer recommendations for future research, highlighting the importance of considering not only internal consistency but also external validity when evaluating clustering results. Overall, this work contributes valuable insights into the measurement and explanation of inter-cluster reliability, helping readers develop a deeper understanding of how to effectively assess the robustness of multidimensional projections.",1
"Metric learning especially deep metric learning has been widely developed for large-scale image inputs data. However, in many real-world applications, we can only have access to vectorized inputs data. Moreover, on one hand, well-labeled data is usually limited due to the high annotation cost. On the other hand, the real data is commonly streaming data, which requires to be processed online. In these scenarios, the fashionable deep metric learning is not suitable anymore. To this end, we reconsider the traditional shallow online metric learning and newly develop an online progressive deep metric learning (ODML) framework to construct a metric-algorithm-based deep network. Specifically, we take an online metric learning algorithm as a metric-algorithm-based layer (i.e., metric layer), followed by a nonlinear layer, and then stack these layers in a fashion similar to deep learning. Different from the shallow online metric learning, which can only learn one metric space (feature transformation), the proposed ODML is able to learn multiple hierarchical metric spaces. Furthermore, in a progressively and nonlinearly learning way, ODML has a stronger learning ability than traditional shallow online metric learning in the case of limited available training data. To make the learning process more explainable and theoretically guaranteed, we also provide theoretical analysis. The proposed ODML enjoys several nice properties and can indeed learn a metric progressively and performs better on the benchmark datasets. Extensive experiments with different settings have been conducted to verify these properties of the proposed ODML.",0
"Artificial intelligence is increasingly becoming important in our daily lives due to advancements made in computer vision tasks such as image classification. One area that has received significant attention recently is metric learning, which involves training algorithms to recognize similarities and differences between different examples. In particular, deep metric learning (DML) focuses on using convolutional neural networks to learn meaningful representations of data points in high dimensional spaces. However, existing DML methods have limitations, including sensitivity to initialization and difficulty in adapting to changes in the dataset distribution. This paper proposes a new method called online progressive deep metric learning (OPDML), which addresses these issues by using mini-batch gradient descent with a preconditioner matrix that adapts during training. Experiments conducted on benchmark datasets demonstrate the effectiveness of OPDML compared to state-of-the-art DML methods.",1
"In recent years, deep learning has made brilliant achievements in image classification. However, image classification of small datasets is still not obtained good research results. This article first briefly explains the application and characteristics of convolutional neural networks and visual transformers. Meanwhile, the influence of small data set on classification and the solution are introduced. Then a series of experiments are carried out on the small datasets by using various models, and the problems of some models in the experiments are discussed. Through the comparison of experimental results, the recommended deep learning model is given according to the model application environment. Finally, we give directions for future work.",0
"In the field of computer vision, deep learning has become increasingly popular as a method for image classification. While convolutional neural networks (CNNs) have traditionally been used for this task, recent research has explored alternative methods using visual transformers. This study compares the performance of CNNs and visual transformers on small scale image data sets to determine which approach provides better accuracy. The results indicate that while both approaches provide accurate predictions, there were significant differences in their feature extraction capabilities. The findings suggest that the choice between these two architectures may depend on factors such as the size of the dataset available and the complexity of the features required for successful prediction. Overall, this work contributes to our understanding of how different deep learning models perform at classifying images and informs future decisions regarding model selection.",1
"Annealed importance sampling (AIS) and related algorithms are highly effective tools for marginal likelihood estimation, but are not fully differentiable due to the use of Metropolis-Hastings (MH) correction steps. Differentiability is a desirable property as it would admit the possibility of optimizing marginal likelihood as an objective using gradient-based methods. To this end, we propose a differentiable AIS algorithm by abandoning MH steps, which further unlocks mini-batch computation. We provide a detailed convergence analysis for Bayesian linear regression which goes beyond previous analyses by explicitly accounting for non-perfect transitions. Using this analysis, we prove that our algorithm is consistent in the full-batch setting and provide a sublinear convergence rate. However, we show that the algorithm is inconsistent when mini-batch gradients are used due to a fundamental incompatibility between the goals of last-iterate convergence to the posterior and elimination of the pathwise stochastic error. This result is in stark contrast to our experience with stochastic optimization and stochastic gradient Langevin dynamics, where the effects of gradient noise can be washed out by taking more steps of a smaller size. Our negative result relies crucially on our explicit consideration of convergence to the stationary distribution, and it helps explain the difficulty of developing practically effective AIS-like algorithms that exploit mini-batch gradients.",0
"Inference problems are at the heart of artificial intelligence, and there has been considerable recent interest in differentiable importance sampling as an alternative to Markov Chain Monte Carlo (MCMC) methods such as SGLD or SVGD. Unlike traditional MCMC methods, which use gradient noise to stabilize optimization trajectories, our approach uses annealing coupled with differentiable importance sampling. We show that this method can substantially improve accuracy relative to gradient noise based approaches over challenging inference tasks. Moreover, we demonstrate that the advantages provided by using our novel method persist even when we evaluate on noisy test sets, which better resemble realistic evaluation scenarios than most existing benchmark datasets. Finally, we give detailed discussion regarding implementation tricks which make scaled score functions tractable to optimize with respect to continuous parameters. While these details may seem esoteric, they have far reaching impacts on model architecture design decisions throughout deep learning and machine learning more broadly. Together these results provide evidence that annealed importance sampling offers promising new territory for exploration in machine learning research.",1
"In this paper, we describe a reproduction of the Relational Graph Convolutional Network (RGCN). Using our reproduction, we explain the intuition behind the model. Our reproduction results empirically validate the correctness of our implementations using benchmark Knowledge Graph datasets on node classification and link prediction tasks. Our explanation provides a friendly understanding of the different components of the RGCN for both users and researchers extending the RGCN approach. Furthermore, we introduce two new configurations of the RGCN that are more parameter efficient. The code and datasets are available at https://github.com/thiviyanT/torch-rgcn.",0
"In recent years, graph convolution has emerged as a powerful methodology for modeling structured data and knowledge graphs. Among them, Relational Graph Convolutional Networks (RGCN) have attracted significant attention due to their effectiveness in capturing rich relational patterns between entities in large knowledge bases such as RDF and OWL. In this work we present an in-depth analysis of RGCN models, aimed at providing both practitioners and researchers a clearer understanding of their capabilities, limitations, and open challenges. We first review different approaches proposed by the community, including message passing frameworks like GCN or DiffusionKG, attentional mechanisms like Attention-based KG Completion (AttKG), and hybrid architectures that combine both. Then, we critically evaluate the existing literature on their expressive power, representational capacity, computational complexity, scalability, robustness under noisy inputs, interpretability, and applicability across diverse domains. Our analyses uncover several key insights, identifying potential research directions toward designing more advanced RGCN systems for complex real-world problems. Finally, we provide guidelines for selecting appropriate RGCN methods based on specific problem settings, datasets, and requirements, ultimately facilitating broader adoption and impact in various application scenarios.",1
"We analyse and explain the increased generalisation performance \latestEdits{of} Iterate Averaging using a Gaussian Process perturbation model between the true and batch risk surface on the high dimensional quadratic. % Based on our theoretical results We derive three phenomena \latestEdits{from our theoretical results:} (1) The importance of combining iterate averaging with large learning rates and regularisation for improved regularisation (2) Justification for less frequent averaging. (3) That we expect adaptive gradient methods to work equally well or better with iterate averaging than their non adaptive counterparts. Inspired by these results\latestEdits{, together with} empirical investigations of the importance of appropriate regularisation for the solution diversity of the iterates, we propose two adaptive algorithms with iterate averaging. \latestEdits{These} give significantly better results than SGD, require less tuning and do not require early stopping or validation set monitoring. We showcase the efficacy of our approach on the CIFAR-10/100, ImageNet and Penn Treebank datasets on a variety of modern and classical network architectures.",0
"Title: Reducing Test Error via Iteration and Averaging Introduction Errors during testing can have significant impacts on test results and subsequent decisions based upon them. In order to reduce such errors, researchers often employ techniques aimed at improving accuracy and minimizing uncertainty. One strategy that has been explored is iterative averaging (IA), which involves taking multiple measurements of the same variable over time and then computing the average value as a final estimate. This approach has shown promise across different domains but remains underutilized in practice. Our study seeks to evaluate the effectiveness of IA in reducing test error compared to other methods. Methodology We conducted two experiments to assess the impact of IA on reducing test errors. Participants were randomly assigned to one of three groups - single measurement only, repeated measurement without averaging, and iterative averaging. The outcome variable was measured using standardized procedures, and participants performed tasks related to memory recall and motor skill performance. Results Both experiments yielded similar findings. Compared to the single measurement group, both the repeated measurement and iterative averaging groups showed lower levels of test error. However, the iterative averaging group consistently demonstrated significantly lower error rates than either the single measurement or repeated measurement alone. Discussion These results suggest that implementing IA may lead to reduced test errors in various settings. Given its simplicity and potential benefits, incorporating this technique into current practices should be considered by practitioners and organizations seeking greater confidence in their data outcomes",1
"Explainable artificial intelligence (XAI) is an emerging new domain in which a set of processes and tools allow humans to better comprehend the decisions generated by black box models. However, most of the available XAI tools are often limited to simple explanations mainly quantifying the impact of individual features to the models' output. Therefore, human users are not able to understand how the features are related to each other to make predictions, whereas the inner workings of the trained models remain hidden. This paper contributes to the development of a novel graphical explainability tool that not only indicates the significant features of the model but also reveals the conditional relationships between features and the inference capturing both the direct and indirect impact of features to the models' decision. The proposed XAI methodology, termed as gLIME, provides graphical model-agnostic explanations either at the global (for the entire dataset) or the local scale (for specific data points). It relies on a combination of local interpretable model-agnostic explanations (LIME) with graphical least absolute shrinkage and selection operator (GLASSO) producing undirected Gaussian graphical models. Regularization is adopted to shrink small partial correlation coefficients to zero providing sparser and more interpretable graphical explanations. Two well-known classification datasets (BIOPSY and OAI) were selected to confirm the superiority of gLIME over LIME in terms of both robustness and consistency over multiple permutations. Specifically, gLIME accomplished increased stability over the two datasets with respect to features' importance (76%-96% compared to 52%-77% using LIME). gLIME demonstrates a unique potential to extend the functionality of the current state-of-the-art in XAI by providing informative graphically given explanations that could unlock black boxes.",0
"Here is an example abstract that fits within the given guidelines:  ""Interpreting machine learning models can be a challenging task due to their complexity and often opaque nature. In this work, we propose a novel approach called Graphs by Local Interpretability Metric Expansion (GLIME) which provides a comprehensive set of easily interpretable features designed specifically to capture meaningful characteristics of ML models locally across any data distribution. By leveraging statistical techniques such as clustering and regression analysis on these features, users can gain insight into complex decision boundaries and feature importances with minimal prior knowledge.""",1
"Developing video understanding intelligence is quite challenging because it requires holistic integration of images, scripts, and sounds based on natural language processing, temporal dependency, and reasoning. Recently, substantial attempts have been made on several video datasets with associated question answering (QA) on a large scale. However, existing evaluation metrics for video question answering (VideoQA) do not provide meaningful analysis. To make progress, we argue that a well-made framework, established on the way humans understand, is required to explain and evaluate the performance of understanding in detail. Then we propose a top-down evaluation system for VideoQA, based on the cognitive process of humans and story elements: Cognitive Modules for Evaluation (CogME). CogME is composed of three cognitive modules: targets, contents, and thinking. The interaction among the modules in the understanding procedure can be expressed in one sentence as follows: ""I understand the CONTENT of the TARGET through a way of THINKING."" Each module has sub-components derived from the story elements. We can specify the required aspects of understanding by annotating the sub-components to individual questions. CogME thus provides a framework for an elaborated specification of VideoQA datasets. To examine the suitability of a VideoQA dataset for validating video understanding intelligence, we evaluated the baseline model of the DramaQA dataset by applying CogME. The evaluation reveals that story elements are unevenly reflected in the existing dataset, and the model based on the dataset may cause biased predictions. Although this study has only been able to grasp a narrow range of stories, we expect that it offers the first step in considering the cognitive process of humans on the video understanding intelligence of humans and AI.",0
"This paper presents a new evaluation metric called ""CogME"" (Cognitive Methods and Explanations) that provides a more comprehensive measure of video understanding intelligence than existing metrics like action recognition accuracy alone. CogME evaluates both the cognitive methods used by algorithms during inference and their ability to generate explanations for their actions. By considering these aspects, we aim to provide researchers and practitioners with a more accurate assessment of an algorithm's capabilities in real-world applications such as surveillance, autonomous systems, and healthcare monitoring. Our proposed metric consists of three main components: cognitive performance measures, explanation quality measures, and external validity evaluation. We apply our metric to evaluate several state-of-the-art video understanding algorithms, demonstrating its effectiveness at distinguishing between strong and weak performers. Overall, CogME represents an important step towards developing better evaluation practices that align with societal values related to explainability, fairness, safety, privacy, and accountability.",1
"In Internet of things (IoT), data is continuously recorded from different data sources and devices can suffer faults in their embedded electronics, thus leading to a high-dimensional data sets and concept drift events. Therefore, methods that are capable of high-dimensional non-stationary time series are of great value in IoT applications. Fuzzy Time Series (FTS) models stand out as data-driven non-parametric models of easy implementation and high accuracy. Unfortunately, FTS encounters difficulties when dealing with data sets of many variables and scenarios with concept drift. We present a new approach to handle high-dimensional non-stationary time series, by projecting the original high-dimensional data into a low dimensional embedding space and using FTS approach. Combining these techniques enables a better representation of the complex content of non-stationary multivariate time series and accurate forecasts. Our model is able to explain 98% of the variance and reach 11.52% of RMSE, 2.68% of MAE and 2.91% of MAPE.",0
"Increasingly, time series forecasting has become more complex as high-dimensional data streams emerge from the Internet of Things (IoT) applications. Traditional forecasting methods may not be sufficient for such non-stationary fuzzy time series due to their large feature size, irregular sampling frequency, temporal dependence, and uncertainty. As a result, new approaches must consider these characteristics to provide accurate predictions. This study investigates how embedding techniques can aid in forecasting high-dimensional multivariate time series data, particularly those affected by non-stationarity and imprecision, which have been understudied in previous works on predictive modeling. An experimental framework compares several fusion methods for combining multiple heterogeneous sources: bagging decision trees, k-nearest neighbors regression, Prophet, and LSTM neural networks. These models are evaluated across eight datasets originating from diverse fieldsâ€”manufacturing processes, traffic conditions, water quality monitoring, energy consumption, human activity recognition, smart home environments, financial indicators, and climate patternsâ€”to evaluate generalizability. To account for non-stationarity and imprecision, we propose a novel method that leverages the concept of local interpolation weighting via Gaussian process regression, considering both nearest neighbor relationships and time intervals among observations. Empirical results show that our proposed approach achieves better performance than individual base learners alone while outperforming other ensemble combinations and state-of-the-art baselines across all datasets, confirming its effectiveness in addressing real-world challenges involving big data in the IoT era. Our research highlights th",1
"We extend the theory of PAC learning in a way which allows to model a rich variety of learning tasks where the data satisfy special properties that ease the learning process. For example, tasks where the distance of the data from the decision boundary is bounded away from zero. The basic and simple idea is to consider partial concepts: these are functions that can be undefined on certain parts of the space. When learning a partial concept, we assume that the source distribution is supported only on points where the partial concept is defined.   This way, one can naturally express assumptions on the data such as lying on a lower dimensional surface or margin conditions. In contrast, it is not at all clear that such assumptions can be expressed by the traditional PAC theory. In fact we exhibit easy-to-learn partial concept classes which provably cannot be captured by the traditional PAC theory. This also resolves a question posed by Attias, Kontorovich, and Mansour 2019.   We characterize PAC learnability of partial concept classes and reveal an algorithmic landscape which is fundamentally different than the classical one. For example, in the classical PAC model, learning boils down to Empirical Risk Minimization (ERM). In stark contrast, we show that the ERM principle fails in explaining learnability of partial concept classes. In fact, we demonstrate classes that are incredibly easy to learn, but such that any algorithm that learns them must use an hypothesis space with unbounded VC dimension. We also find that the sample compression conjecture fails in this setting.   Thus, this theory features problems that cannot be represented nor solved in the traditional way. We view this as evidence that it might provide insights on the nature of learnability in realistic scenarios which the classical theory fails to explain.",0
"This paper presents a theoretical framework that allows for efficient learning of partial concept classes using polynomial approximation algorithms (PAC). By introducing novel techniques such as feature extraction and dimensionality reduction, we demonstrate how to improve the accuracy and scalability of existing PAC learners for complex data sets. Our results showcase the utility of these methods across a wide range of applications including image classification, natural language processing, and time series prediction. Overall, our work represents an important contribution to the field of machine learning by providing new tools and insights into understanding the nature of learnability itself.",1
"Whilst an abundance of techniques have recently been proposed to generate counterfactual explanations for the predictions of opaque black-box systems, markedly less attention has been paid to exploring the uncertainty of these generated explanations. This becomes a critical issue in high-stakes scenarios, where uncertain and misleading explanations could have dire consequences (e.g., medical diagnosis and treatment planning). Moreover, it is often difficult to determine if the generated explanations are well grounded in the training data and sensitive to distributional shifts. This paper proposes several practical solutions that can be leveraged to solve these problems by establishing novel connections with other research works in explainability (e.g., trust scores) and uncertainty estimation (e.g., Monte Carlo Dropout). Two experiments demonstrate the utility of our proposed solutions.",0
"In recent years, counterfactual explanations have become increasingly popular as a means of interpreting machine learning models. These methods generate contrastive examples that highlight which features were most important in making the modelâ€™s prediction. However, uncertainty estimation and out-of-distribution detection are crucial aspects often overlooked when generating these types of explanations. This paper explores some common pitfalls associated with generating counterfactuals without considering uncertainty estimates, such as producing unreliable or irrelevant hypothetical scenarios. Additionally, this study presents solutions aimed at addressing the challenges faced by existing approaches when dealing with uncertain predictions and OOD data, ensuring more reliable and accurate interpretation of machine learning systems. By providing guidelines and methodologies for improving counterfactual explanation generation techniques, this research contributes to the field of explainable artificial intelligence and enhances our understanding of how to effectively communicate uncertainty in complex decision-making processes.",1
"Machine learning is expected to fuel significant improvements in medical care. To ensure that fundamental principles such as beneficence, respect for human autonomy, prevention of harm, justice, privacy, and transparency are respected, medical machine learning applications must be developed responsibly. In this paper, we survey the technical challenges involved in creating medical machine learning systems responsibly and in conformity with existing regulations, as well as possible solutions to address these challenges. We begin by providing a brief overview of existing regulations affecting medical machine learning, showing that properties such as safety, robustness, reliability, privacy, security, transparency, explainability, and nondiscrimination are all demanded already by existing law and regulations - albeit, in many cases, to an uncertain degree. Next, we discuss the underlying technical challenges, possible ways for addressing them, and their respective merits and drawbacks. We notice that distribution shift, spurious correlations, model underspecification, and data scarcity represent severe challenges in the medical context (and others) that are very difficult to solve with classical black-box deep neural networks. Important measures that may help to address these challenges include the use of large and representative datasets and federated learning as a means to that end, the careful exploitation of domain knowledge wherever feasible, the use of inherently transparent models, comprehensive model testing and verification, as well as stakeholder inclusion.",0
"In recent years, machine learning has emerged as a promising tool for medical applications such as disease diagnosis, treatment planning, drug discovery, and personalized medicine. However, like any other technology, the use of machine learning in healthcare raises important questions about safety, privacy, and ethics. To address these concerns and ensure responsible and regulatory conformity, researchers have proposed several technical solutions that can enhance trustworthiness, explainability, interpretability, accountability, fairness, transparency, security, privacy, scalability, realism, accuracy, and effectiveness. This survey presents an overview of different methodologies and approaches that have been developed to tackle challenges related to data quality, generalization ability, robustness, human intervention, user experience, decision support, reliability, and validation. By examining state-of-the-art techniques, we hope to identify best practices and provide guidelines for future research in responsible and regulatory conform machine learning for medicine. Ultimately, our goal is to facilitate responsible innovation that benefits both patients and society at large while ensuring compliance with legal and ethical regulations.",1
"It is extremely important to ensure a safe takeover transition in conditionally automated driving. One of the critical factors that quantifies the safe takeover transition is takeover time. Previous studies identified the effects of many factors on takeover time, such as takeover lead time, non-driving tasks, modalities of the takeover requests (TORs), and scenario urgency. However, there is a lack of research to predict takeover time by considering these factors all at the same time. Toward this end, we used eXtreme Gradient Boosting (XGBoost) to predict the takeover time using a dataset from a meta-analysis study [1]. In addition, we used SHAP (SHapley Additive exPlanation) to analyze and explain the effects of the predictors on takeover time. We identified seven most critical predictors that resulted in the best prediction performance. Their main effects and interaction effects on takeover time were examined. The results showed that the proposed approach provided both good performance and explainability. Our findings have implications on the design of in-vehicle monitoring and alert systems to facilitate the interaction between the drivers and the automated vehicle.",0
"This should read like a standalone summary of your paper that can be used in applications such as arXiv overlay journals and grant submissions. Include references if necessary but don't citation format them. Here we go! Title: Predicting Driver Takeover Time in Conditionally Automated Driving Abstract: As the use of conditional automation systems for driving continues to grow, understanding how drivers respond during takeovers has become increasingly important. The goal of this research is to develop a model that predicts driver takeover time (TOT) in semi-autonomous vehicles using machine learning algorithms. To achieve this aim, a dataset containing information on vehicle speed, traffic density, and road conditions was created by analyzing naturalistic driving data from 42 participants across four seasons over a total of approximately two years. Our results showed that linear regression models were able to accurately predict TOT up to 6 seconds into the future. Additionally, our findings suggest that there may be other factors that influence the accuracy of predictions beyond those included in our current analysis, which warrants further investigation. Future work could involve testing these hypotheses and expanding our dataset to improve prediction accuracy. Overall, this study demonstrates the potential utility of machine learning techniques for improving autonomous driving safety and user experience through accurate prediction of driver takeover times. References: [list all relevant citations]",1
"Enabling interpretations of model uncertainties is of key importance in Bayesian machine learning applications. Often, this requires to meaningfully attribute predictive uncertainties to source features in an image, text or categorical array. However, popular attribution methods are particularly designed for classification and regression scores. In order to explain uncertainties, state of the art alternatives commonly procure counterfactual feature vectors, and proceed by making direct comparisons. In this paper, we leverage path integrals to attribute uncertainties in Bayesian differentiable models. We present a novel algorithm that relies on in-distribution curves connecting a feature vector to some counterfactual counterpart, and we retain desirable properties of interpretability methods. We validate our approach on benchmark image data sets with varying resolution, and show that it significantly simplifies interpretability over the existing alternatives.",0
"This is a technical scientific article that focuses on developing new mathematical methods to better understand the uncertainty present in machine learning models. In particular, path integrals are used as a tool to describe these uncertainties more accurately than previous approaches have been able to achieve. Throughout the article, numerous examples and case studies are provided to illustrate how these techniques can be applied in practice, making it accessible to both experts and non-experts alike. Ultimately, the goal of this work is to improve our understanding of model error propagation, which has important implications for fields ranging from finance to climate science.",1
"Deep learning techniques for 3D brain vessel image segmentation have not been as successful as in the segmentation of other organs and tissues. This can be explained by two factors. First, deep learning techniques tend to show poor performances at the segmentation of relatively small objects compared to the size of the full image. Second, due to the complexity of vascular trees and the small size of vessels, it is challenging to obtain the amount of annotated training data typically needed by deep learning methods. To address these problems, we propose a novel annotation-efficient deep learning vessel segmentation framework. The framework avoids pixel-wise annotations, only requiring weak patch-level labels to discriminate between vessel and non-vessel 2D patches in the training set, in a setup similar to the CAPTCHAs used to differentiate humans from bots in web applications. The user-provided weak annotations are used for two tasks: 1) to synthesize pixel-wise pseudo-labels for vessels and background in each patch, which are used to train a segmentation network, and 2) to train a classifier network. The classifier network allows to generate additional weak patch labels, further reducing the annotation burden, and it acts as a noise filter for poor quality images. We use this framework for the segmentation of the cerebrovascular tree in Time-of-Flight angiography (TOF) and Susceptibility-Weighted Images (SWI). The results show that the framework achieves state-of-the-art accuracy, while reducing the annotation time by ~77% w.r.t. learning-based segmentation methods using pixel-wise labels for training.",0
"CUT FROM HERE The task of annotating and segmenting vessels has traditionally been performed manually by trained experts, but with the increasing amounts of medical data available there is now a need for automated methods that can efficiently perform these tasks while maintaining high accuracy levels. In this work we propose a novel deep learning framework called VESSEL-CAPTCHA (Vascular Segmentation using Selective Ensembling with Local Convolutional Adversarial Prompts) for accurate vessel annotation and segmentation from whole slide images of histological sections. Our method uses weakly supervised adversarial training on a pretextual self-supervision task, where the model learns to distinguish between real vessel segments and those generated by our local convolutional adversary. This approach allows us to overcome some of the limitations of traditional approaches based on fully supervised or semi-supervised learning, which often require large quantities of annotations or rely on handcrafted features that cannot capture complex variations in vessel structures. We demonstrate through extensive experiments across multiple datasets that our proposed method achieves state-of-the-art results and outperforms existing techniques in terms of both efficiency and accuracy, making it well suited for use in clinical settings where speedy and reliable analysis is critical. Additionally, we provide an ablation study to analyze the contributions made by each component of our framework towards overall performance improvement. Overall, our work presents a significant step forward in advancing automatic image analysis capabilities for biomedical applications and highlights the potential for further progress in improving patient care using computer vision technologies.",1
"The success of deep neural networks in real-world problems has prompted many attempts to explain their training dynamics and generalization performance, but more guiding principles for the training of neural networks are still needed. Motivated by the edge of chaos principle behind the optimal performance of neural networks, we study the role of various hyperparameters in modern neural network training algorithms in terms of the order-chaos phase diagram. In particular, we study a fully analytical feedforward neural network trained on the widely adopted Fashion-MNIST dataset, and study the dynamics associated with the hyperparameters in back-propagation during the training process. We find that for the basic algorithm of stochastic gradient descent with momentum, in the range around the commonly used hyperparameter values, clear scaling relations are present with respect to the training time during the ordered phase in the phase diagram, and the model's optimal generalization power at the edge of chaos is similar across different training parameter combinations. In the chaotic phase, the same scaling no longer exists. The scaling allows us to choose the training parameters to achieve faster training without sacrificing performance. In addition, we find that the commonly used model regularization method - weight decay - effectively pushes the model towards the ordered phase to achieve better performance. Leveraging on this fact and the scaling relations in the other hyperparameters, we derived a principled guideline for hyperparameter determination, such that the model can achieve optimal performance by saturating it at the edge of chaos. Demonstrated on this simple neural network model and training algorithm, our work improves the understanding of neural network training dynamics, and can potentially be extended to guiding principles of more complex model architectures and algorithms.",0
"In this paper we explore the concept of edge of chaos theory as applied to the field of deep learning and artificial intelligence more broadly. We begin by defining the core concepts involved including order, randomness, complexity and entropy. From there we move on to discuss how these concepts apply specifically to neurons which serve as fundamental building blocks in models designed using deep learning techniques. Using examples from our own work as well as other researchers within the field we demonstrate that edge of chaos principles can indeed be leveraged as part of a larger strategy to improve model performance. Additionally we examine methods through which one might implement these principles into their own projects such as adjusting training parameters so as to increase the likelihood that the model evolves towards optimal levels of complexity, or taking advantage of specific architectures which naturally encourage edge of chaos behavior. Ultimately this paper presents both theoretical foundations and practical guidance enabling others to benefit from utilizing edge of chaos thinking when developing cutting-edge machine learning solutions.",1
"Understanding the predictions made by Artificial Intelligence (AI) systems is becoming more and more important as deep learning models are used for increasingly complex and high-stakes tasks. Saliency mapping - an easily interpretable visual attribution method - is one important tool for this, but existing formulations are limited by either computational cost or architectural constraints. We therefore propose Hierarchical Perturbation, a very fast and completely model-agnostic method for explaining model predictions with robust saliency maps. Using standard benchmarks and datasets, we show that our saliency maps are of competitive or superior quality to those generated by existing model-agnostic methods - and are over 20X faster to compute.",0
"This work presents a novel method for generating explanations of machine learning models that combines efficient gradient computation with sensitivity analysis techniques. We propose a hierarchical perturbation approach (HiP) that systematically disturbs features according to their relative importance as determined by a user or predefined criteria. Our approach outperforms existing methods on four diverse datasets by producing more concise yet informative feature attributions and demonstrates better stability against noise. Furthermore, we prove its effectiveness through human evaluation where participants found our explanations more intuitive compared to SHAP values and gradients alone. Finally, since HiP does not require specific model architectures but rather relies only on gradient information and some prior knowledge about data distributions, it can serve as a general framework applicable across various domains. Keywords: Gradient Computation; Sensitivity Analysis Techniques; Machine Learning Models; Feature Attributions; Intuitive; General Framework",1
"In this work we explore the limiting dynamics of deep neural networks trained with stochastic gradient descent (SGD). We find empirically that long after performance has converged, networks continue to move through parameter space by a process of anomalous diffusion in which distance travelled grows as a power law in the number of gradient updates with a nontrivial exponent. We reveal an intricate interaction between the hyperparameters of optimization, the structure in the gradient noise, and the Hessian matrix at the end of training that explains this anomalous diffusion. To build this understanding, we first derive a continuous-time model for SGD with finite learning rates and batch sizes as an underdamped Langevin equation. We study this equation in the setting of linear regression, where we can derive exact, analytic expressions for the phase space dynamics of the parameters and their instantaneous velocities from initialization to stationarity. Using the Fokker-Planck equation, we show that the key ingredient driving these dynamics is not the original training loss, but rather the combination of a modified loss, which implicitly regularizes the velocity, and probability currents, which cause oscillations in phase space. We identify qualitative and quantitative predictions of this theory in the dynamics of a ResNet-18 model trained on ImageNet. Through the lens of statistical physics, we uncover a mechanistic origin for the anomalous limiting dynamics of deep neural networks trained with SGD.",0
"This abstract presents key findings from a study that revisits the traditional understanding of Stochastic Gradient Descent (SGD) algorithms. By analyzing how these algorithms perform on specific problems under different conditions, we identified three crucial factors affecting their behavior: modified loss functions, phase space oscillations, and anomalous diffusion processes. Our analysis highlights the complex interplay between these factors and reveals limitations in SGDâ€™s ability to converge towards optimal solutions under certain conditions. We discuss potential implications of our findings for future research directions in machine learning, signal processing, control theory, and other fields utilizing SGD methods.",1
"Introduction: Real-world data generated from clinical practice can be used to analyze the real-world evidence (RWE) of COVID-19 pharmacotherapy and validate the results of randomized clinical trials (RCTs). Machine learning (ML) methods are being used in RWE and are promising tools for precision-medicine. In this study, ML methods are applied to study the efficacy of therapies on COVID-19 hospital admissions in the Valencian Region in Spain. Methods: 5244 and 1312 COVID-19 hospital admissions - dated between January 2020 and January 2021 from 10 health departments, were used respectively for training and validation of separate treatment-effect models (TE-ML) for remdesivir, corticosteroids, tocilizumab, lopinavir-ritonavir, azithromycin and chloroquine/hydroxychloroquine. 2390 admissions from 2 additional health departments were reserved as an independent test to analyze retrospectively the survival benefits of therapies in the population selected by the TE-ML models using cox-proportional hazard models. TE-ML models were adjusted using treatment propensity scores to control for pre-treatment confounding variables associated to outcome and further evaluated for futility. ML architecture was based on boosted decision-trees. Results: In the populations identified by the TE-ML models, only Remdesivir and Tocilizumab were significantly associated with an increase in survival time, with hazard ratios of 0.41 (P = 0.04) and 0.21 (P = 0.001), respectively. No survival benefits from chloroquine derivatives, lopinavir-ritonavir and azithromycin were demonstrated. Tools to explain the predictions of TE-ML models are explored at patient-level as potential tools for personalized decision making and precision medicine. Conclusion: ML methods are suitable tools toward RWE analysis of COVID-19 pharmacotherapies. Results obtained reproduce published results on RWE and validate the results from RCTs.",0
"This paper proposes using machine learning techniques to analyze data from clinical trials in order to better understand how pharmaceuticals can effectively treat COVID-19 symptoms. The authors argue that traditional statistical methods may not always be able to capture all relevant patterns within the data set. They suggest that artificial intelligence could provide greater accuracy by discovering hidden relationships and correlations that would otherwise go unnoticed. Ultimately, these findings have important implications for public health policy makers who must make informed decisions about how to allocate resources in the face of a global pandemic like COVID-19.",1
"As machine learning black boxes are increasingly being deployed in critical domains such as healthcare and criminal justice, there has been a growing emphasis on developing techniques for explaining these black boxes in a post hoc manner. In this work, we analyze two popular post hoc interpretation techniques: SmoothGrad which is a gradient based method, and a variant of LIME which is a perturbation based method. More specifically, we derive explicit closed form expressions for the explanations output by these two methods and show that they both converge to the same explanation in expectation, i.e., when the number of perturbed samples used by these methods is large. We then leverage this connection to establish other desirable properties, such as robustness, for these techniques. We also derive finite sample complexity bounds for the number of perturbations required for these methods to converge to their expected explanation. Finally, we empirically validate our theory using extensive experimentation on both synthetic and real world datasets.",0
"In recent years, there has been significant interest in developing methods that can explain complex machine learning models, particularly those used in high stakes applications such as healthcare, finance, and criminal justice. Two popular approaches for model explanation are perturbation-based methods, which manipulate the input data to measure how changes affect the output, and gradient-based methods, which use the gradients of the loss function with respect to the inputs to identify important features. Despite their utility, these two classes of methods have different strengths and weaknesses, and there is currently no unified framework that can provide both robustness and flexibility in handling various types of problems.  This paper presents a comprehensive comparison between perturbation-based and gradient-based explanations, identifying their unique characteristics and limitations. We discuss several case studies from diverse domains where either method excels over the other, highlighting the need for a versatile framework that can adapt to specific problem requirements. To bridge this gap, we propose a novel unified approach called UER-Explore (Uncertainty-guided Ensemble-based Robust explanation) that combines the strengths of both types of methods. Our framework uses an ensemble of multiple models, each with a varying degree of uncertainty, enabling efficient exploration of the input space and capturing different aspects of feature importance.  Our experimental results demonstrate that our proposed framework achieves superior performance compared to state-of-the-art baselines across various benchmark datasets, including image classification, natural language processing, and predictive policing tasks. Moreover, we showcase how our framework can effectively handle challenges arising due to noise, missing values, and nonlinear relationships between features, making it highly applicable in real-world scenarios. By providing a versatile and robust solution for model interpretation, our work takes a step towards establishing reliable explanations of machine learni",1
"What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.",0
"Artificial intelligence has come along way since its inception, from rule based systems to artificial neural networks to deep learning models such as transformer networks that have achieved state of the art results on numerous natural language processing tasks. However, despite their successes these models lack interpretability and transparency which raises concerns over trustworthiness and safety. Furthermore, deep learning models require large amounts of data and computational resources, making them difficult to deploy and scale to new domains. In our paper we propose an alternative approach called â€œThinking Like Transformersâ€ where we aim to create interpretable and transparent models that can perform well across different tasks while requiring minimal computational requirements and little amount of training data. We use human expertise such as linguists and domain experts to inform our model design process by providing prior knowledge and constraints that guide our algorithm towards desirable behavior. Our proposed method outperforms competitive baselines on several benchmark datasets and demonstrates promising results in zero shot transfer across multiple domains and languages. By utilizing human insights into our machine learning pipeline, we take a step closer to building truly intelligent agents that think more like humans.",1
"Deep reinforcement learning has shown remarkable success in the past few years. Highly complex sequential decision making problems from game playing and robotics have been solved with deep model-free methods. Unfortunately, the sample complexity of model-free methods is often high. To reduce the number of environment samples, model-based reinforcement learning creates an explicit model of the environment dynamics. Achieving high model accuracy is a challenge in high-dimensional problems. In recent years, a diverse landscape of model-based methods has been introduced to improve model accuracy, using methods such as uncertainty modeling, model-predictive control, latent models, and end-to-end learning and planning. Some of these methods succeed in achieving high accuracy at low sample complexity, most do so either in a robotics or in a games context. In this paper, we survey these methods; we explain in detail how they work and what their strengths and weaknesses are. We conclude with a research agenda for future work to make the methods more robust and more widely applicable to other applications.",0
"In this survey we aim to provide a comprehensive overview on high-accuracy model based reinforcement learning (HAMRL). HAMRL has been successfully used across a wide range of application domains such as robotics, computer vision, speech recognition and natural language processing, where traditional reinforcement learning algorithms suffer from drawbacks that prevent their usage in real world applications. These limitations arise due to poor accuracy caused by partial observability, noise, uncertainty or even adversarial attacks. In contrast to classical RL methods, HAMRL combines accurate models with deep learning techniques in order to achieve more reliable results. In recent years there have been many advances in high-accuracy model based RL. Many different approaches exist, including model-free, offline and online training, as well as both stationary and nonstationary environments. This survey provides a detailed overview and analysis of the state of art and important challenges faced by these methods. We hope this helps researchers new to field or experienced practitioners looking to find ways to improve performance of their current systems. This survey begins by introducing the problem statement and motivation behind developing HAMRL algorithms. Next, we discuss related works in the area of model-based RL and how they differ from our proposed approach. Then, we describe several approaches under HAMRL and analyze their strengths and weaknesses, as well as compare them against each other. Finally, we conclude with open questions, future directions, and potential impacts of this work in the RL community and beyond. With extensive experimental evaluations we show the effectiveness of the presented methods compared to prior arts and baselines.",1
"Deep generative models (DGMs) seem a natural fit for detecting out-of-distribution (OOD) inputs, but such models have been shown to assign higher probabilities or densities to OOD images than images from the training distribution. In this work, we explain why this behavior should be attributed to model misestimation. We first prove that no method can guarantee performance beyond random chance without assumptions on which out-distributions are relevant. We then interrogate the typical set hypothesis, the claim that relevant out-distributions can lie in high likelihood regions of the data distribution, and that OOD detection should be defined based on the data distribution's typical set. We highlight the consequences implied by assuming support overlap between in- and out-distributions, as well as the arbitrariness of the typical set for OOD detection. Our results suggest that estimation error is a more plausible explanation than the misalignment between likelihood-based OOD detection and out-distributions of interest, and we illustrate how even minimal estimation error can lead to OOD detection failures, yielding implications for future work in deep generative modeling and OOD detection.",0
"""Out-of-distribution (OOD) detection has gained significant attention in recent years as deep learning models have become increasingly popular in various applications, including computer vision, natural language processing, and robotics. OOD detection helps detect anomalous inputs that a model may not have seen during training and can improve overall robustness and safety. However, current out-of-distribution detection methods based on deep generative models still suffer from limitations such as high false positive rates, lack of interpretability, and poor generalization across different datasets. This study proposes new insights into understanding failures in OOD detection with deep generative models by analyzing their behavior on benchmark datasets under real-world conditions. We demonstrate the effectiveness of our approach using state-of-the-art evaluation metrics and showcase how these insights can lead to improved OOD detection performance.""",1
"This survey paper specially analyzed computer vision-based object detection challenges and solutions by different techniques. We mainly highlighted object detection by three different trending strategies, i.e., 1) domain adaptive deep learning-based approaches (discrepancy-based, Adversarial-based, Reconstruction-based, Hybrid). We examined general as well as tiny object detection-related challenges and offered solutions by historical and comparative analysis. In part 2) we mainly focused on tiny object detection techniques (multi-scale feature learning, Data augmentation, Training strategy (TS), Context-based detection, GAN-based detection). In part 3), To obtain knowledge-able findings, we discussed different object detection methods, i.e., convolutions and convolutional neural networks (CNN), pooling operations with trending types. Furthermore, we explained results with the help of some object detection algorithms, i.e., R-CNN, Fast R-CNN, Faster R-CNN, YOLO, and SSD, which are generally considered the base bone of CV, CNN, and OD. We performed comparative analysis on different datasets such as MS-COCO, PASCAL VOC07,12, and ImageNet to analyze results and present findings. At the end, we showed future directions with existing challenges of the field. In the future, OD methods and models can be analyzed for real-time object detection, tracking strategies.",0
"Recent advances in deep learning have significantly improved computer vision performance across multiple domains including object detection. However, one key challenge in deploying these models in practice is that they often require large amounts of labeled training data from the specific domain where they will be used. This can be time consuming and expensive, especially for applications where annotating millions of images may not be feasible. To address this limitation, researchers have explored techniques such as transfer learning, fine-tuning pre-trained models, and incremental adaptation to leverage existing knowledge and adapt to new domains. In addition, there has been growing interest in detecting objects that are small and difficult to observe due to factors such as occlusion, distance, and cluttered scenes. These challenges motivate us to survey recent progress in deep domain adaptation and tiny object detection, presenting representative datasets, techniques and evaluation metrics. Our goal is to provide practitioners and researchers with insights into current limitations, promising directions, and open problems in this exciting area.",1
"We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF performs well on unsupervised 3D scene segmentation, novel view synthesis, and scene editing on three datasets.",0
"This work presents a method for unsupervised discovery of object radiance fields (ORFs) from natural images of objects in complex scenes. We develop a probabilistic model based on color constancy that predicts human perception of surface shading under ambient lighting conditions. Our approach learns to recover ORFs by minimizing reconstruction error using an adversarial loss function. We demonstrate improved performance compared to previous methods on benchmark datasets and showcase interesting applications such as relighting synthetic renderings of objects in real scenes. Overall, our work has the potential to enable new technologies in computer vision and graphics related to scene understanding, image editing, and 3D reconstruction. -----",1
"In this work, we attempt to explain the prediction of any black-box classifier from an information-theoretic perspective. For each input feature, we compare the classifier outputs with and without that feature using two information-theoretic metrics. Accordingly, we obtain two attribution maps--an information gain (IG) map and a point-wise mutual information (PMI) map. IG map provides a class-independent answer to ""How informative is each pixel?"", and PMI map offers a class-specific explanation of ""How much does each pixel support a specific class?"" Compared to existing methods, our method improves the correctness of the attribution maps in terms of a quantitative metric. We also provide a detailed analysis of an ImageNet classifier using the proposed method, and the code is available online.",0
"Title: ""Explanations by Analysis: Visualizing What Deep Learning Models See"" Authors: Peter E. Clark & James W. Taylor Abstract Black box classifiers have become increasingly popular over recent years due to their ability to solve complex problems on large datasets that traditional methods cannot handle. Despite their successes, these models remain difficult to interpret and explain, leading to concerns regarding their transparency and accountability. In particular, it can be challenging to identify which parts of the input data are most relevant for making predictions. This difficulty arises because deep learning models process vast amounts of data in ways that humans find hard to reason about. Our aim here is to provide insight into how these systems make decisions. We develop techniques that visualize important features within images using explanatory heatmaps. Our approach exploits an implicit statistical model underlying many black box classifiers: we use maximum likelihood estimation to generate training examples from latent spaces learned by autoencoders trained alongside classifiers. By exposing patterns present in high probability regions of these latent spaces, our method reveals key aspects of data used in decision making processes for a variety of image classification tasks. Comparison against baselines show the effectiveness of this technique at identifying salient features in inputs. Additionally, we demonstrate how our framework enables fine-grained analysis through feature attribution, highlighting specific spatial areas most responsible for correct predictions. Overall, this work provides novel insights into machine reasoning, showing how complex patterns recognized by artificial intelligence can be understood by human users without access to internal model parameters. Keywords: Explainable Artificial Intelligence (XAI), black box classifiers, interpretable ai, explainabl",1
"Academic advances of AI models in high-precision domains, like healthcare, need to be made explainable in order to enhance real-world adoption. Our past studies and ongoing interactions indicate that medical experts can use AI systems with greater trust if there are ways to connect the model inferences about patients to explanations that are tied back to the context of use. Specifically, risk prediction is a complex problem of diagnostic and interventional importance to clinicians wherein they consult different sources to make decisions. To enable the adoption of the ever improving AI risk prediction models in practice, we have begun to explore techniques to contextualize such models along three dimensions of interest: the patients' clinical state, AI predictions about their risk of complications, and algorithmic explanations supporting the predictions. We validate the importance of these dimensions by implementing a proof-of-concept (POC) in type-2 diabetes (T2DM) use case where we assess the risk of chronic kidney disease (CKD) - a common T2DM comorbidity. Within the POC, we include risk prediction models for CKD, post-hoc explainers of the predictions, and other natural-language modules which operationalize domain knowledge and CPGs to provide context. With primary care physicians (PCP) as our end-users, we present our initial results and clinician feedback in this paper. Our POC approach covers multiple knowledge sources and clinical scenarios, blends knowledge to explain data and predictions to PCPs, and received an enthusiastic response from our medical expert.",0
"In recent years, there has been increasing emphasis on incorporating explainability into machine learning models to ensure transparency and accountability in decision-making processes. However, current approaches have focused primarily on generating explanations that are easily interpretable by technical experts rather than by end users who may lack domain knowledge. This can result in explanations that are difficult for non-experts to comprehend, leading to confusion and mistrust towards these systems. To address this gap, we propose a user-centered approach to generate patient-specific explanations grounded in clinical context for individuals living with diabetes. We demonstrate how our framework provides personalized explanations that cater to each individual's cognitive abilities and health literacy level while still adhering to medical standards and guidelines. Our evaluation shows promising results in improving understanding, satisfaction and engagement among participants with different levels of health literacy. Ultimately, our work highlights the importance of considering user needs during model development and the value of tailored explainability for ensuring trustworthy and effective deployment of machine learning in real world settings.",1
"Recent research on graph neural network (GNN) models successfully applied GNNs to classical graph algorithms and combinatorial optimisation problems. This has numerous benefits, such as allowing applications of algorithms when preconditions are not satisfied, or reusing learned models when sufficient training data is not available or can't be generated. Unfortunately, a key hindrance of these approaches is their lack of explainability, since GNNs are black-box models that cannot be interpreted directly. In this work, we address this limitation by applying existing work on concept-based explanations to GNN models. We introduce concept-bottleneck GNNs, which rely on a modification to the GNN readout mechanism. Using three case studies we demonstrate that: (i) our proposed model is capable of accurately learning concepts and extracting propositional formulas based on the learned concepts for each target class; (ii) our concept-based GNN models achieve comparative performance with state-of-the-art models; (iii) we can derive global graph concepts, without explicitly providing any supervision on graph-level concepts.",0
"In recent years, there has been growing interest in developing explainable artificial intelligence (AI) systems that can justify their decisions and actions to humans. One promising approach to achieve this goal is through algorithmic concept-based explainable reasoning (ACE). This paper presents a novel framework for ACE that utilizes concepts as fundamental building blocks to represent knowledge and facilitate reasoning. The proposed method leverages state-of-the-art deep learning techniques to automatically learn meaningful concepts from data, which are then used to interpret complex decision processes and provide transparent explanations. Our results show significant improvements over existing methods across several benchmark datasets, demonstrating the effectiveness and potential applications of our ACE framework. By bridging the gap between machine learning algorithms and human understanding, our work takes a step towards creating more trustworthy and reliable AI systems.",1
"Recently, due to an increasing interest for transparency in artificial intelligence, several methods of explainable machine learning have been developed with the simultaneous goal of accuracy and interpretability by humans. In this paper, we study a recent framework of explainable clustering first suggested by Dasgupta et al.~\cite{dasgupta2020explainable}. Specifically, we focus on the $k$-means and $k$-medians problems and provide nearly tight upper and lower bounds.   First, we provide an $O(\log k \log \log k)$-approximation algorithm for explainable $k$-medians, improving on the best known algorithm of $O(k)$~\cite{dasgupta2020explainable} and nearly matching the known $\Omega(\log k)$ lower bound~\cite{dasgupta2020explainable}. In addition, in low-dimensional spaces $d \ll \log k$, we show that our algorithm also provides an $O(d \log^2 d)$-approximate solution for explainable $k$-medians. This improves over the best known bound of $O(d \log k)$ for low dimensions~\cite{laber2021explainable}, and is a constant for constant dimensional spaces. To complement this, we show a nearly matching $\Omega(d)$ lower bound. Next, we study the $k$-means problem in this context and provide an $O(k \log k)$-approximation algorithm for explainable $k$-means, improving over the $O(k^2)$ bound of Dasgupta et al. and the $O(d k \log k)$ bound of \cite{laber2021explainable}. To complement this we provide an almost tight $\Omega(k)$ lower bound, improving over the $\Omega(\log k)$ lower bound of Dasgupta et al. Given an approximate solution to the classic $k$-means and $k$-medians, our algorithm for $k$-medians runs in time $O(kd \log^2 k )$ and our algorithm for $k$-means runs in time $ O(k^2 d)$.",0
"This paper presents a new algorithm for clustering data that provides explanations about how the clusters were formed. We propose a novel method for approximating clusterings that can then be used as input to the explanation step. Our method produces very close approximation solutions quickly, allowing us to find high quality clusterings and explanations within reasonable timeframes. We validate our approach on several datasets, showing that we significantly outperform state-of-the-art methods both in terms of accuracy and computational efficiency. Additionally, we show that the generated explanations effectively demonstrate how the algorithms arrived at their final cluster assignments. Overall, our work represents an important advance in the field of explainable machine learning by providing a scalable solution for explaining complex clustering results.",1
"Shapley values are widely used to explain black-box models, but they are costly to calculate because they require many model evaluations. We introduce FastSHAP, a method for estimating Shapley values in a single forward pass using a learned explainer model. FastSHAP amortizes the cost of explaining many inputs via a learning approach inspired by the Shapley value's weighted least squares characterization, and it can be trained using standard stochastic gradient optimization. We compare FastSHAP to existing estimation approaches, revealing that it generates high-quality explanations with orders of magnitude speedup.",0
"Title: ""Real-Time SHAP Explaining Decisions"" Abstract In recent years, explainability has become increasingly important as machine learning models have been deployed in critical applications such as healthcare and finance. One popular method for explaining decisions is the Shapley value (SV), which provides a decomposition of model outputs into contributions from different features. Traditional SV methods require many computations and can take hours or even days to generate explanations on large datasets. In this work, we propose a novel method called ""FastSHAP"" that leverages partial derivatives to efficiently estimate SVs in realtime. We demonstrate significant speedups over previous state-of-the-art methods while preserving accuracy. With FastSHAP, users can interactively explore and analyze the behavior of complex models in real time. Our work opens up exciting possibilities for interactive feature attribution visualization tools and real-time interpretation of deep learning pipelines.",1
"Consider a heterogeneous population of points evolving with time. While the population evolves, both in size and nature, we can observe it periodically, through snapshots taken at different timestamps. Each of these snapshots is formed by sampling points from the population at that time, and then creating features to recover point clouds. While these snapshots describe the population's evolution on aggregate, they do not provide directly insights on individual trajectories. This scenario is encountered in several applications, notably single-cell genomics experiments, tracking of particles, or when studying crowd motion. In this paper, we propose to model that dynamic as resulting from the celebrated Jordan-Kinderlehrer-Otto (JKO) proximal scheme. The JKO scheme posits that the configuration taken by a population at time $t$ is one that trades off a decrease w.r.t. an energy (the model we seek to learn) penalized by an optimal transport distance w.r.t. the previous configuration. To that end, we propose JKOnet, a neural architecture that combines an energy model on measures, with (small) optimal displacements solved with input convex neural networks (ICNN). We demonstrate the applicability of our model to explain and predict population dynamics.",0
"JKOnet is a new proximal optimal transport modeling method that can effectively capture complex population dynamics using real data. This approach allows us to quantify how different demographic processes, such as migration, mortality, fertility, and mating success, shape the distribution and diversity of populations over space and time. By combining high-resolution survey data on individuals with geospatial environmental features, we demonstrate that JKOnet accurately captures key ecological relationships among species distributions and their interactions. We present two case studies applying JKOnet to human populations: one analyzing worldwide migration patterns, and another investigating sex ratios at birth across U.S. counties from 2000 to 2019. Our findings highlight the potential applications of this method in fields such as public health, epidemiology, and conservation biology. Overall, our work contributes important advancements to understanding large-scale population processes through powerful mathematical models and real data.",1
"We study the XAI (explainable AI) on the face recognition task, particularly the face verification here. Face verification is a crucial task in recent days and it has been deployed to plenty of applications, such as access control, surveillance, and automatic personal log-on for mobile devices. With the increasing amount of data, deep convolutional neural networks can achieve very high accuracy for the face verification task. Beyond exceptional performances, deep face verification models need more interpretability so that we can trust the results they generate. In this paper, we propose a novel similarity metric, called explainable cosine ($xCos$), that comes with a learnable module that can be plugged into most of the verification models to provide meaningful explanations. With the help of $xCos$, we can see which parts of the two input faces are similar, where the model pays its attention to, and how the local similarities are weighted to form the output $xCos$ score. We demonstrate the effectiveness of our proposed method on LFW and various competitive benchmarks, resulting in not only providing novel and desiring model interpretability for face verification but also ensuring the accuracy as plugging into existing face recognition models.",0
"In recent years, face verification has become increasingly important in a variety of fields such as security, finance, healthcare, and entertainment. As deep learning techniques have advanced, traditional cosine similarity measures have been replaced by more complex distance metrics that capture higher-order features from facial data. However, these new metrics often lack interpretability, making it difficult to explain their results. To address this gap, we propose xCos, an explainable cosine metric for face verification tasks.  xCos combines both geometric and photometric distances into one unified framework that can effectively model different types of variability in facial data. Our proposed method uses linear regression models to learn feature representations that map input images onto a low-dimensional space where cosine similarity can be efficiently computed. By leveraging the explainability provided by our learned representation, we are able to provide insights into why certain faces are classified similarly or differently based on their individual characteristics.  To evaluate our approach, we conducted experiments on several popular benchmark datasets and compared our results against state-of-the-art methods. Results showed that our proposed xCos metric outperforms competing approaches in terms of accuracy while maintaining interpretability. Furthermore, we demonstrated how our explanations can be used to gain insights into the decision making process of a face recognition system, which could lead to improved performance and better user experience.  In summary, our work presents a novel approach towards explainable face verification using a combination of traditional and modern machine learning techniques. While future research may focus on expanding our methodology to other domains, our initial findings show great potential in improving the transparency and trustworthiness of computer vision systems.",1
"In the paper, we propose a novel methodology to map learning algorithms on data (performance map) in order to gain more insights in the distribution of their performances across their parameter space. This methodology provides useful information when selecting a learner's best configuration for the data at hand, and it also enhances the comparison of learners across learning contexts. In order to explain the proposed methodology, the study introduces the notions of learning context, performance map, and high performance function. It then applies these concepts to a variety of learning contexts to show how their use can provide more insights in a learner's behavior, and can enhance the comparison of learners across learning contexts. The study is completed by an extensive experimental study describing how the proposed methodology can be applied.",0
"Here are two examples of a detailed and concise version:  Detailed Version  Mapping learning algorithms onto data can provide valuable insights into how different types of machine learning models perform across a range of tasks and datasets. By analyzing the relationships between different model architectures, dataset sizes, training durations, etc., it becomes possible to optimize performance while reducing risk and computational cost. In addition, this mapping process makes it easier to compare the efficacy of competing methods by allowing researchers to identify key differences that might otherwise go unnoticed. This work presents a methodology for constructing these maps using a variety of visualization techniques and then applies them to three case studies in natural language processing (NLP). Results demonstrate the effectiveness of our approach at identifying optimal configurations for specific NLP problems, suggesting that similar results could be obtained in other domains if applied carefully. Overall, we believe that this study provides important contributions to both theoretical computer science as well as practitioners working directly with machine learning systems.  Concise Version  Optimizing machine learning models requires understanding which factors impact performance most significantly. Mapping learning algorithms onto data enables efficient configuration of hyperparameters based on task and dataset characteristics. Our case studies employing various visualization methods show improved accuracy through optimized selection of parameters, architecture design choices, and preprocessing steps. These benefits make our proposed framework applicable for a wide range of fields requiring effective decision making under uncertainty.",1
"Complex dynamical systems are used for predictions in many domains. Because of computational costs, models are truncated, coarsened, or aggregated. As the neglected and unresolved terms become important, the utility of model predictions diminishes. We develop a novel, versatile, and rigorous methodology to learn non-Markovian closure parameterizations for known-physics/low-fidelity models using data from high-fidelity simulations. The new ""neural closure models"" augment low-fidelity models with neural delay differential equations (nDDEs), motivated by the Mori-Zwanzig formulation and the inherent delays in complex dynamical systems. We demonstrate that neural closures efficiently account for truncated modes in reduced-order-models, capture the effects of subgrid-scale processes in coarse models, and augment the simplification of complex biological and physical-biogeochemical models. We find that using non-Markovian over Markovian closures improves long-term prediction accuracy and requires smaller networks. We derive adjoint equations and network architectures needed to efficiently implement the new discrete and distributed nDDEs, for any time-integration schemes and allowing nonuniformly-spaced temporal training data. The performance of discrete over distributed delays in closure models is explained using information theory, and we find an optimal amount of past information for a specified architecture. Finally, we analyze computational complexity and explain the limited additional cost due to neural closure models.",0
"Abstract: In recent years, deep learning has been applied successfully to many domains, including modeling complex dynamical systems. One key challenge in these applications is handling the high dimensionality and nonlinear structure of the state space. Traditional methods such as Kalman filters rely on explicit models that can become difficult to design and maintain in practice. Alternatively, neural network based approaches offer the promise of more flexible representations, but often require large amounts of data and computational resources. We introduce a new class of models called ""neural closure models"" which combine the advantages of both traditional techniques and modern machine learning. Our approach uses a closed form expression that depends only on the current time step, allowing for efficient computation even in high dimensions. These models are shown to perform well on several benchmark tasks without requiring massive datasets, making them attractive for real world use cases where training data may be limited or expensive to collect. Additionally, our methodology can be easily extended to incorporate other sources of information such as prior knowledge or physical constraints, further improving performance.",1
"Understanding an agent's priorities by observing their behavior is critical for transparency and accountability in decision processes, such as in healthcare. While conventional approaches to policy learning almost invariably assume stationarity in behavior, this is hardly true in practice: Medical practice is constantly evolving, and clinical professionals are constantly fine-tuning their priorities. We desire an approach to policy learning that provides (1) interpretable representations of decision-making, accounts for (2) non-stationarity in behavior, as well as operating in an (3) offline manner. First, we model the behavior of learning agents in terms of contextual bandits, and formalize the problem of inverse contextual bandits (ICB). Second, we propose two algorithms to tackle ICB, each making varying degrees of assumptions regarding the agent's learning strategy. Finally, through both real and simulated data for liver transplantations, we illustrate the applicability and explainability of our method, as well as validating its accuracy.",0
"This paper presents a new approach to solving the problem of contextual bandit optimization called ""Inverse Contextual Bandits."" We focus on understanding how human behavior evolves over time as they interact with a system that uses reinforcement learning algorithms to optimize the return on investment (ROI) for an advertising campaign. Our methodology involves training deep neural networks to predict user clicks given a particular ad context, then using these predictions to create synthetic trajectories representing possible user behaviors. By analyzing these trajectories, we can gain insights into how users make decisions based on their exposure to different ads and learn to tailor our recommendations accordingly. Experimental results show that our method significantly outperforms traditional contextual bandit approaches in terms of ROI and clickthrough rates. Overall, we believe that our work has important implications for improving personalized recommender systems and decision support tools in general.",1
"Backdoor attacks represent a serious threat to neural network models. A backdoored model will misclassify the trigger-embedded inputs into an attacker-chosen target label while performing normally on other benign inputs. There are already numerous works on backdoor attacks on neural networks, but only a few works consider graph neural networks (GNNs). As such, there is no intensive research on explaining the impact of trigger injecting position on the performance of backdoor attacks on GNNs.   To bridge this gap, we conduct an experimental investigation on the performance of backdoor attacks on GNNs. We apply two powerful GNN explainability approaches to select the optimal trigger injecting position to achieve two attacker objectives -- high attack success rate and low clean accuracy drop. Our empirical results on benchmark datasets and state-of-the-art neural network models demonstrate the proposed method's effectiveness in selecting trigger injecting position for backdoor attacks on GNNs. For instance, on the node classification task, the backdoor attack with trigger injecting position selected by GraphLIME reaches over $84 \%$ attack success rate with less than $2.5 \%$ accuracy drop",0
"Title: ""Attacking Graph Neural Networks with Explainability-based Backdoors""  Graph neural networks (GNNs) have become increasingly popular for analyzing graph data due to their ability to learn powerful representations from complex graphs. However, like many other machine learning models, GNNs can suffer from security vulnerabilities that threaten the integrity of the predictions they make. One such threat comes in the form of backdoor attacks, where malicious actors manipulate the model during training to cause it to produce incorrect predictions on specific inputs, known as trigger inputs. In this work, we propose a novel approach for creating these backdoor triggers using explainability methods, specifically Gradient Boosting Decision Trees (GBDT). We show that by crafting triggers based on GBDT explanations, we can effectively create stealthy and highly effective backdoors in state-of-the-art GNN models. Our experimental results demonstrate the efficacy of our approach across several real-world datasets and different types of GNN architectures. This research highlights the importance of addressing security threats against machine learning models, particularly those used for sensitive applications.",1
"Deep Neural Networks (DNNs) have an enormous potential to learn from complex biomedical data. In particular, DNNs have been used to seamlessly fuse heterogeneous information from neuroanatomy, genetics, biomarkers, and neuropsychological tests for highly accurate Alzheimer's disease diagnosis. On the other hand, their black-box nature is still a barrier for the adoption of such a system in the clinic, where interpretability is absolutely essential. We propose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for explaining the Alzheimer's diagnosis made by a DNN from the 3D point cloud of the neuroanatomy and tabular biomarkers. Our explanations are based on the Shapley value, which is the unique method that satisfies all fundamental axioms for local explanations previously established in the literature. Thus, SVEHNN has many desirable characteristics that previous work on interpretability for medical decision making is lacking. To avoid the exponential time complexity of the Shapley value, we propose to transform a given DNN into a Lightweight Probabilistic Deep Network without re-training, thus achieving a complexity only quadratic in the number of features. In our experiments on synthetic and real data, we show that we can closely approximate the exact Shapley value with a dramatically reduced runtime and can reveal the hidden knowledge the network has learned from the data.",0
"This is a draft abstract: This paper proposes a novel method for scalable, axiomatic explanations of deep Alzheimerâ€™s diagnosis using multi-modal medical imaging data. Our approach integrates state-of-the-art natural language processing (NLP) models to generate interpretable visualizations of complex deep neural networks that can aid clinicians in their decision making processes. By building on the principles of axioms, we showcase a unique ability to derive high quality explanations without the need for additional annotations or training data. We demonstrate our methods by accurately predicting Alzheimerâ€™s disease with higher fidelity than previously reported methods on publicly available datasets while providing human-interpretable evidence of these predictions.",1
"As the complexity of machine learning (ML) models increases, resulting in a lack of prediction explainability, several methods have been developed to explain a model's behavior in terms of the training data points that most influence the model. However, these methods tend to mark outliers as highly influential points, limiting the insights that practitioners can draw from points that are not representative of the training data. In this work, we take a step towards finding influential training points that also represent the training data well. We first review methods for assigning importance scores to training points. Given importance scores, we propose a method to select a set of DIVerse INfluEntial (DIVINE) training points as a useful explanation of model behavior. As practitioners might not only be interested in finding data points influential with respect to model accuracy, but also with respect to other important metrics, we show how to evaluate training data points on the basis of group fairness. Our method can identify unfairness-inducing training points, which can be removed to improve fairness outcomes. Our quantitative experiments and user studies show that visualizing DIVINE points helps practitioners understand and explain model behavior better than earlier approaches.",0
"This research presents a novel approach to data visualization that emphasizes diversity and inclusion in training data sets. We introduce DIVINE (Diverse Influential Training Points for Data Visualization and Model Refinement), which utilizes algorithms designed to identify groups traditionally underrepresented in machine learning datasets. These groups can then be targeted specifically by collecting more diverse and representative samples. The resulting models produce higher quality results than those trained on homogeneous datasets alone, demonstrating the importance of balanced representation in achieving accurate predictions. Our approach increases transparency in data processing and reduces systemic biases often present in AI systems, ultimately leading to better decision making. By carefully selecting influential subsets of our training points, we aim to create a positive feedback loop where inclusive sampling leads to improved accuracy, resulting in greater impactful use cases for these predictive models across industries. Overall, DIVINE provides a scalable method for enhancing data visualizations and refining ML models while promoting social equity through technology advancements.",1
"Graph Neural Networks (GNNs) achieve significant performance for various learning tasks on geometric data due to the incorporation of graph structure into the learning of node representations, which renders their comprehension challenging. In this paper, we first propose a unified framework satisfied by most existing GNN explainers. Then, we introduce GraphSVX, a post hoc local model-agnostic explanation method specifically designed for GNNs. GraphSVX is a decomposition technique that captures the ""fair"" contribution of each feature and node towards the explained prediction by constructing a surrogate model on a perturbed dataset. It extends to graphs and ultimately provides as explanation the Shapley Values from game theory. Experiments on real-world and synthetic datasets demonstrate that GraphSVX achieves state-of-the-art performance compared to baseline models while presenting core theoretical and human-centric properties.",0
"This paper presents GraphSVX, a new tool that allows users to generate Shapley value explanations for graph neural networks. The Shapley value provides a unique approach to explaining how individual features contribute to the overall output of the model, making it ideal for understanding the behavior of complex models like GNNs. GraphSVX enables users to compute these values directly from their trained GNN models, providing insights into which nodes or edges have the greatest impact on the outcome. Additionally, the software has built-in visualization tools to help explore the results, including graphs and scatter plots. With GraphSVX, researchers can gain a deeper understanding of their GNN models and make more informed decisions based on the predictions they provide.",1
"A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the total number of parameters. We explain this phenomenon by first examining the success probability of hitting a training loss sub-level set when training within a random subspace of a given training dimensionality. We find a sharp phase transition in the success probability from $0$ to $1$ as the training dimension surpasses a threshold. This threshold training dimension increases as the desired final loss decreases, but decreases as the initial loss decreases. We then theoretically explain the origin of this phase transition, and its dependence on initialization and final desired loss, in terms of precise properties of the high dimensional geometry of the loss landscape. In particular, we show via Gordon's escape theorem, that the training dimension plus the Gaussian width of the desired loss sub-level set, projected onto a unit sphere surrounding the initialization, must exceed the total number of parameters for the success probability to be large. In several architectures and datasets, we measure the threshold training dimension as a function of initialization and demonstrate that it is a small fraction of the total number of parameters, thereby implying, by our theory, that successful training with so few dimensions is possible precisely because the Gaussian width of low loss sub-level sets is very large. Moreover, this threshold training dimension provides a strong null model for assessing the efficacy of more sophisticated ways to reduce training degrees of freedom, including lottery tickets as well a more optimal method we introduce: lottery subspaces.",0
"Deep learning models have become increasingly popular as they can achieve state of the art results on various tasks such as image classification, speech recognition and natural language processing. Training these models requires choosing hyperparameters carefully which includes architecture design, initialization techniques and optimization algorithms used during training. Recently there has been interest in understanding if certain architectures require more parameters than others while performing comparably on the same task. This work attempts to unveil new insights into model behavior by exploring the role of different degrees of freedom (DoF) that a neural network can take advantage of to improve generalization performance. We show using analytical methods along with empirical evidence from extensive experiments performed across multiple datasets and benchmarks that one should choose models based upon their number of DoF, rather than solely the amount of parameters present. Our work helps inform practitioners how to create better architectures tailored towards specific use cases rather than blindly selecting bigger models. Furthermore, our study provides theoretical foundations for designing better regularization schemes that lead to smaller models that perform just as well as larger ones without any degradation in accuracy. Ultimately, this paper helps provide answers to important questions regarding which models one should be building, why those choices matter and how to build them in practice.",1
"Human vision is able to capture the part-whole hierarchical information from the entire scene. This paper presents the Visual Parser (ViP) that explicitly constructs such a hierarchy with transformers. ViP divides visual representations into two levels, the part level and the whole level. Information of each part represents a combination of several independent vectors within the whole. To model the representations of the two levels, we first encode the information from the whole into part vectors through an attention mechanism, then decode the global information within the part vectors back into the whole representation. By iteratively parsing the two levels with the proposed encoder-decoder interaction, the model can gradually refine the features on both levels. Experimental results demonstrate that ViP can achieve very competitive performance on three major tasks e.g. classification, detection and instance segmentation. In particular, it can surpass the previous state-of-the-art CNN backbones by a large margin on object detection. The tiny model of the ViP family with $7.2\times$ fewer parameters and $10.9\times$ fewer FLOPS can perform comparably with the largest model ResNeXt-101-64$\times$4d of ResNe(X)t family. Visualization results also demonstrate that the learnt parts are highly informative of the predicting class, making ViP more explainable than previous fundamental architectures. Code is available at https://github.com/kevin-ssy/ViP.",0
"This paper presents a new approach to representing part-whole hierarchies using transformers. Traditional methods have relied on predefined structures or hand-coded representations, but our method allows for more flexible and efficient modeling of complex relationships. We introduce the visual parser, which uses computer vision techniques to extract features from images and then applies deep learning algorithms to learn meaningful representations. Our experiments show that our method outperforms previous approaches across a range of tasks, including object detection and relationship prediction. We conclude by discussing future directions for research in this area.",1
"When we deploy machine learning models in high-stakes medical settings, we must ensure these models make accurate predictions that are consistent with known medical science. Inherently interpretable networks address this need by explaining the rationale behind each decision while maintaining equal or higher accuracy compared to black-box models. In this work, we present a novel interpretable neural network algorithm that uses case-based reasoning for mammography. Designed to aid a radiologist in their decisions, our network presents both a prediction of malignancy and an explanation of that prediction using known medical features. In order to yield helpful explanations, the network is designed to mimic the reasoning processes of a radiologist: our network first detects the clinically relevant semantic features of each image by comparing each new image with a learned set of prototypical image parts from the training images, then uses those clinical features to predict malignancy. Compared to other methods, our model detects clinical features (mass margins) with equal or higher accuracy, provides a more detailed explanation of its prediction, and is better able to differentiate the classification-relevant parts of the image.",0
"Title: ""Interpretable Mammographic Image Classification using Case-based Reasoning and Deep Learning""  Abstract:  This study presents a novel approach for interpretable mammographic image classification that combines case-based reasoning (CBR) and deep learning techniques. Breast cancer diagnosis through mammography often requires expert radiologists to manually analyze multiple images in order to identify patterns indicative of breast lesions. This process can be time consuming and subjective, leading to errors in interpretation which may result in missed or incorrect diagnoses. To address these challenges, we propose an integrated framework that leverages both CBR and deep learning methods for efficient, accurate, and interpretable analysis of mammograms.  The proposed system first extracts relevant features from each mammogram using convolutional neural networks (CNN), which are then utilized by a CBR module to compare against previously diagnosed cases stored in a database. By doing so, the model identifies similar historical cases and retrieves their corresponding diagnostic labels as guidance for the current image under consideration. Finally, the extracted features along with the retrieved similarity scores provide insights into the decision making process, enabling greater transparency and interpretability compared to traditional black box models.  Experimental results on two publicly available datasets demonstrate the effectiveness of our approach, achieving superior performance compared to state-of-the-art methods while maintaining high levels of interpretability. Our findings indicate the potential value of combining complementary techniques such as CNN and CBR towards developing more robust and reliable computer-aided diagnosis systems for improved breast cancer screening.",1
"(Artificial) neural networks have become increasingly popular in mechanics to accelerate computations with model order reduction techniques and as universal models for a wide variety of materials. However, the major disadvantage of neural networks remains: their numerous parameters are challenging to interpret and explain. Thus, neural networks are often labeled as black boxes, and their results often elude human interpretation. In mechanics, the new and active field of physics-informed neural networks attempts to mitigate this disadvantage by designing deep neural networks on the basis of mechanical knowledge. By using this a priori knowledge, deeper and more complex neural networks became feasible, since the mechanical assumptions could be explained. However, the internal reasoning and explanation of neural network parameters remain mysterious.   Complementary to the physics-informed approach, we propose a first step towards a physics-informing approach, which explains neural networks trained on mechanical data a posteriori. This novel explainable artificial intelligence approach aims at elucidating the black box of neural networks and their high-dimensional representations. Therein, the principal component analysis decorrelates the distributed representations in cell states of RNNs and allows the comparison to known and fundamental functions. The novel approach is supported by a systematic hyperparameter search strategy that identifies the best neural network architectures and training parameters. The findings of three case studies on fundamental constitutive models (hyperelasticity, elastoplasticity, and viscoelasticity) imply that the proposed strategy can help identify numerical and analytical closed-form solutions to characterize new materials.",0
"This paper presents a methodology for developing explainable artificial intelligence (AI) systems that can inform mechanics by learning from large datasets of experimental data. In particular, we focus on using deep neural network architectures called physics-informed neural networks (PINNs) as our model class. PINNs incorporate physical laws into the training process which enables us to learn accurate models with better interpretability compared to traditional black box machine learning methods. We apply these models to different types of mechanical problems such as nonlinear elasticity, phase transformation, plastic deformation, and fluid dynamics etc., demonstrating their ability to capture complex underlying physics phenomena in a manner amenable to real-time simulation. Our results show significant improvements in accuracy over prior work while maintaining transparency which make them ideal candidates for decision support in challenging applications like predictive maintenance and design optimization.",1
"Meta-learning is a field that aims at discovering how different machine learning algorithms perform on a wide range of predictive tasks. Such knowledge speeds up the hyperparameter tuning or feature engineering. With the use of surrogate models various aspects of the predictive task such as meta-features, landmarker models e.t.c. are used to predict the expected performance. State of the art approaches are focused on searching for the best meta-model but do not explain how these different aspects contribute to its performance. However, to build a new generation of meta-models we need a deeper understanding of the importance and effect of meta-features on the model tunability. In this paper, we propose techniques developed for eXplainable Artificial Intelligence (XAI) to examine and extract knowledge from black-box surrogate models. To our knowledge, this is the first paper that shows how post-hoc explainability can be used to improve the meta-learning.",0
"Meta-learning has been a rapidly growing field in recent years due to its ability to solve multiple tasks by leveraging knowledge gained from previous experiences. However, most meta-learning approaches lack interpretability and transparency, making it difficult to fully understand how they achieve their high levels of performance. In this work, we aim to address this issue by proposing a new framework that utilizes Explainable Artificial Intelligence (XAI) techniques to provide explanations and insights into the learning process of meta-learners. Our proposed framework consists of two components: a meta-learning algorithm and an XAI module. The meta-learning algorithm learns across different tasks and domains, while the XAI module interprets the behavior and decision-making processes of the meta-learner. Through extensive experiments on various benchmark datasets, we demonstrate that our approach achieves state-of-the-art performance while providing detailed explanations of its learning process. This makes our method more transparent and trustworthy compared to existing methods in the literature. Overall, our work represents a significant step towards building reliable and explainable meta-learning models, opening up possibilities for new applications in areas such as personalized education and adaptive robotics.",1
"Dictionary learning is a key tool for representation learning, that explains the data as linear combination of few basic elements. Yet, this analysis is not amenable in the context of graph learning, as graphs usually belong to different metric spaces. We fill this gap by proposing a new online Graph Dictionary Learning approach, which uses the Gromov Wasserstein divergence for the data fitting term. In our work, graphs are encoded through their nodes' pairwise relations and modeled as convex combination of graph atoms, i.e. dictionary elements, estimated thanks to an online stochastic algorithm, which operates on a dataset of unregistered graphs with potentially different number of nodes. Our approach naturally extends to labeled graphs, and is completed by a novel upper bound that can be used as a fast approximation of Gromov Wasserstein in the embedding space. We provide numerical evidences showing the interest of our approach for unsupervised embedding of graph datasets and for online graph subspace estimation and tracking.",0
"Online graph dictionary learning is a recent development in computer science that has emerged as a promising approach for data mining and pattern recognition tasks such as image classification, natural language processing (NLP) and bioinformatics. This approach combines the advantages of both dictionary learning techniques and graph embedding methods, which enables the discovery of nonlinear relationships among data points from different modalities. In this work, we present an overview of online graph dictionary learning techniques, highlighting their key contributions, and discussing potential applications across various fields. We provide a detailed analysis of three popular algorithms namely, Online Local Coordinate Coding (OLCC), Online Nonnegative Matrix Factorization (ONMF) and Online Adaptive Heteroscedastic Linear Discriminant Analysis (AHDA). Our study demonstrates that each algorithm exhibits unique characteristics and strengths based on the problem formulation and dataset complexity, and thus offers distinct benefits for real-world problems. Overall, our research emphasizes the significance of online graph dictionary learning approaches for modern big data analytics scenarios, paving the way for future advancements and developments.",1
"Explainable Artificial Intelligence (XAI) is an emerging area of research in the field of Artificial Intelligence (AI). XAI can explain how AI obtained a particular solution (e.g., classification or object detection) and can also answer other ""wh"" questions. This explainability is not possible in traditional AI. Explainability is essential for critical applications, such as defense, health care, law and order, and autonomous driving vehicles, etc, where the know-how is required for trust and transparency. A number of XAI techniques so far have been purposed for such applications. This paper provides an overview of these techniques from a multimedia (i.e., text, image, audio, and video) point of view. The advantages and shortcomings of these techniques have been discussed, and pointers to some future directions have also been provided.",0
"Artificial Intelligence (AI) has made significant strides over recent years and has become increasingly integrated into our daily lives. However, despite these advances, there remains a lack of understanding regarding how many AI systems arrive at their decisions. This lack of transparency can lead to concerns about the reliability and trustworthiness of such systems. Addressing these concerns is crucial as we move towards more widespread adoption of AI technologies. Therefore, explainable artificial intelligence (XAI) seeks to address this issue by providing meaningful explanations about how an AI system arrived at its decision. In this paper, we explore the current state of XAI research and outline possible future directions that may further advance this field. Our review focuses on three main areas: transparent machine learning models, interpretable models, and model interpretability metrics. We discuss several approaches within each area and highlight key challenges faced by researchers attempting to create more transparent AI systems. Finally, we present potential solutions to overcome these obstacles and suggest promising avenues for future work. Overall, this comprehensive literature review provides a valuable resource for those interested in the development and deployment of more reliable and transparent AI systems.",1
"Random features is one of the most popular techniques to speed up kernel methods in large-scale problems. Related works have been recognized by the NeurIPS Test-of-Time award in 2017 and the ICML Best Paper Finalist in 2019. The body of work on random features has grown rapidly, and hence it is desirable to have a comprehensive overview on this topic explaining the connections among various algorithms and theoretical results. In this survey, we systematically review the work on random features from the past ten years. First, the motivations, characteristics and contributions of representative random features based algorithms are summarized according to their sampling schemes, learning procedures, variance reduction properties and how they exploit training data. Second, we review theoretical results that center around the following key question: how many random features are needed to ensure a high approximation quality or no loss in the empirical/expected risks of the learned estimator. Third, we provide a comprehensive evaluation of popular random features based algorithms on several large-scale benchmark datasets and discuss their approximation quality and prediction performance for classification. Last, we discuss the relationship between random features and modern over-parameterized deep neural networks (DNNs), including the use of high dimensional random features in the analysis of DNNs as well as the gaps between current theoretical and empirical results. This survey may serve as a gentle introduction to this topic, and as a users' guide for practitioners interested in applying the representative algorithms and understanding theoretical results under various technical assumptions. We hope that this survey will facilitate discussion on the open problems in this topic, and more importantly, shed light on future research directions.",0
"This survey presents an overview of random features for kernel approximation, covering algorithms, theory, and applications beyond machine learning. We discuss how random feature methods approximate kernels efficiently by selecting random subsets of input data before applying linear models. Key advantages are speedup without loss of accuracy and the ability to handle large datasets that cannot fit into memory. Our focus is on understanding current research trends while highlighting connections to related fields such as numerical linear algebra and computer graphics. By providing a comprehensive review, we aim to inspire future progress towards realizing their full potential for scientific discovery across multiple disciplines. The survey concludes with open challenges and directions for further research on random features for kernel approximation.",1
"Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory functionality and is receiving increasing attention during the COVID-19 pandemic. Clinical findings show that it is possible for COVID-19 patients to have significantly low SpO$_2$ before any obvious symptoms. The prevalence of cameras has motivated researchers to investigate methods for monitoring SpO$_2$ using videos. Most prior schemes involving smartphones are contact-based: They require a fingertip to cover the phone's camera and the nearby light source to capture re-emitted light from the illuminated tissue. In this paper, we propose the first convolutional neural network based noncontact SpO$_2$ estimation scheme using smartphone cameras. The scheme analyzes the videos of a participant's hand for physiological sensing, which is convenient and comfortable, and can protect their privacy and allow for keeping face masks on. We design our neural network architectures inspired by the optophysiological models for SpO$_2$ measurement and demonstrate the explainability by visualizing the weights for channel combination. Our proposed models outperform the state-of-the-art model that is designed for contact-based SpO$_2$ measurement, showing the potential of our proposed method to contribute to public health. We also analyze the impact of skin type and the side of a hand on SpO$_2$ estimation performance.",0
"Introduction: Accurately estimating blood oxygen (SpO2) levels is crucial in many clinical settings such as during exercise tests, intensive care unit monitoring, and home healthcare services. Current methods for measuring SpO2 require external equipment that may be uncomfortable, obtrusive, or impossible to use in certain situations. In recent years, research has focused on developing noninvasive techniques using cameras and computer vision algorithms to estimate physiological parameters from video recordings of subjects. This study proposes a novel method for remotely estimating blood oxygen levels from videos using deep learning neural networks. Methodology: We trained two separate convolutional neural network models using labeled hyperspectral images of human skin patches under different lighting conditions. One model was designed for indoor environments while the other was used for outdoor scenarios. Both models were evaluated based on their accuracy at predicting SPO2 values within predefined error thresholds. Results and Discussion: Our experimental results demonstrated high correlation coefficients between predicted SpO2 values and ground truth measurements obtained by standard pulse oximetry devices. Additionally, we conducted sensitivity analyses to determine the impact of various camera configurations, lighting conditions, and subject characteristics on our proposed approach. Our findings suggest that remote estimation of blood oxygenation using neural networks could potentially revolutionize the field of medical imaging by offering non-contact, low-cost, and portable solutions for continuous and real-time monitoring of vital signs in diverse settings. Limitations and Future Work: While our work shows promising results, there exist several limitations worth addressing. For example, current deep learning models have difficulty generalizing across varying skin tones, which highlights the need for more comprehensive datasets representative of diverse populations. Furthermore, incorporating domain knowledge into neural network architectures remains challenging, leading us to explore hybrid approaches combining traditional image processing methods wit",1
"Multi-epoch, small-batch, Stochastic Gradient Descent (SGD) has been the method of choice for learning with large over-parameterized models. A popular theory for explaining why SGD works well in practice is that the algorithm has an implicit regularization that biases its output towards a good solution. Perhaps the theoretically most well understood learning setting for SGD is that of Stochastic Convex Optimization (SCO), where it is well known that SGD learns at a rate of $O(1/\sqrt{n})$, where $n$ is the number of samples. In this paper, we consider the problem of SCO and explore the role of implicit regularization, batch size and multiple epochs for SGD. Our main contributions are threefold:   (a) We show that for any regularizer, there is an SCO problem for which Regularized Empirical Risk Minimzation fails to learn. This automatically rules out any implicit regularization based explanation for the success of SGD.   (b) We provide a separation between SGD and learning via Gradient Descent on empirical loss (GD) in terms of sample complexity. We show that there is an SCO problem such that GD with any step size and number of iterations can only learn at a suboptimal rate: at least $\widetilde{\Omega}(1/n^{5/12})$.   (c) We present a multi-epoch variant of SGD commonly used in practice. We prove that this algorithm is at least as good as single pass SGD in the worst case. However, for certain SCO problems, taking multiple passes over the dataset can significantly outperform single pass SGD.   We extend our results to the general learning setting by showing a problem which is learnable for any data distribution, and for this problem, SGD is strictly better than RERM for any regularization function. We conclude by discussing the implications of our results for deep learning, and show a separation between SGD and ERM for two layer diagonal neural networks.",0
"This abstract will discuss the role of implicit regularization, batch size, and multiple epochs on stochastic gradient descent (SGD) optimization methods. We will examine how each parameter affects convergence speed and accuracy. Additionally we will look into overfitting mitigation and underfitting alleviation through appropriate choice of hyperparameters. In conclusion, we highlight key takeaways for practitioners looking to implement these techniques effectively. Keywords: Stochastic Gradient Descent, Optimization Methods, Convergence Speed, Accuracy, Overfitting Mitigation, Underfitting Alleviation, Hyperparameter Selection Title: Unleashing HPC for Scientific Research Data Management, Analysis & Visualisation Abstract: High Performance Computing (HPC) platforms provide powerful tools that enable scientists to manage, analyze, visualize, share and collaborate on large data sets generated from scientific research experiments. These workflows can lead to improved collaboration across disciplines and better scientific outcomes by reducing time-to-insight while allowing innovative discoveries through advanced analytics. However, many users find HPC systems intimidating due to their complexity and diversity, which can make them difficult to learn, use and optimize. To overcome these challenges, simplified abstractions, automations, user interfaces, system optimizations, parallelism, scalability, fault tolerance and security features must come together to create unified and optimized end-to-end solutions. In this article, we present our vision of such integrated systems aimed at enabling scientific computing across disciplines while helping organizations scale up as they grow in terms of number of users, applications, data volumes and computing resources. We outline several key benefits provided by modern HPC infrastructure and showcase real world case studies where such integrations have led to accelerated breakthroughs in science and discovery. Our experiences indicate that cloud computing can deliver compelling cost savings compared to traditional bare metal systems and open new possibilities for expanding computing resources via serverless compute and container technologies. Finally, we conclude by emphasizing the need for continued investments in software ecosystems to empower next generation capabilities for scientific computing on emerging hardware architectures and data formats.",1
"Being able to explain the prediction to clinical end-users is a necessity to leverage the power of AI models for clinical decision support. For medical images, saliency maps are the most common form of explanation. The maps highlight important features for AI model's prediction. Although many saliency map methods have been proposed, it is unknown how well they perform on explaining decisions on multi-modal medical images, where each modality/channel carries distinct clinical meanings of the same underlying biomedical phenomenon. Understanding such modality-dependent features is essential for clinical users' interpretation of AI decisions. To tackle this clinically important but technically ignored problem, we propose the MSFI (Modality-Specific Feature Importance) metric to examine whether saliency maps can highlight modality-specific important features. MSFI encodes the clinical requirements on modality prioritization and modality-specific feature localization. Our evaluations on 16 commonly used saliency map methods, including a clinician user study, show that although most saliency map methods captured modality importance information in general, most of them failed to highlight modality-specific important features consistently and precisely. The evaluation results guide the choices of saliency map methods and provide insights to propose new ones targeting clinical applications.",0
"This paper evaluates the use of saliency maps as explanations for medical imaging systems, specifically looking at how well they perform across different modalities such as X-rays, CT scans, and MRI machines. We present our findings from a user study where participants were asked to rate the quality of saliency maps generated by four popular explanation methods, including DeepLIFT, GradCAM++, SmoothGrad, and Guided Backprop. Our results show that while all four methods produce high-quality maps overall, there can be significant differences in performance depending on the modality used and the specific task required by the users. We discuss the implications of these findings and suggest potential future research directions for improving the effectiveness of saliency map explanations in multi-modal medical images. Ultimately, we conclude that a one-size-fits-all approach may not be suitable, highlighting the need for further development and evaluation of methodologies tailored for each modality and application.",1
"Causal discovery from observational data is an important tool in many branches of science. Under certain assumptions it allows scientists to explain phenomena, predict, and make decisions. In the large sample limit, sound and complete causal discovery algorithms have been previously introduced, where a directed acyclic graph (DAG), or its equivalence class, representing causal relations is searched. However, in real-world cases, only finite training data is available, which limits the power of statistical tests used by these algorithms, leading to errors in the inferred causal model. This is commonly addressed by devising a strategy for using as few as possible statistical tests. In this paper, we introduce such a strategy in the form of a recursive wrapper for existing constraint-based causal discovery algorithms, which preserves soundness and completeness. It recursively clusters the observed variables using the normalized min-cut criterion from the outset, and uses a baseline causal discovery algorithm during backtracking for learning local sub-graphs. It then combines them and ensures completeness. By an ablation study, using synthetic data, and by common real-world benchmarks, we demonstrate that our approach requires significantly fewer statistical tests, learns more accurate graphs, and requires shorter run-times than the baseline algorithm.",0
"In today's data driven world, identifying causality from observational studies has become increasingly important. To improve efficiency and accuracy, we propose a hierarchical wrapper method that integrates various feature engineering approaches at different levels of abstraction within a unified framework. By leveraging multiple strategies concurrently, our approach captures complementary signals from distinct dimensions, significantly improves performance on benchmark datasets compared to state-of-the art methods. We demonstrate through experiments that our proposed system can achieve higher quality discoveries with increased interpretability. Given the growing significance of causality analysis, our work opens up new opportunities towards more robust predictions across diverse domains and offers promising solutions to address grand challenges faced by humanity. --arXiv:2109.10748v3 [cs] 9 Nov 2019 Koo et al., 2023,",1
"Machine learning algorithms often produce models considered as complex black-box models by both end users and developers. They fail to explain the model in terms of the domain they are designed for. The proposed Iterative Visual Logical Classifier (IVLC) is an interpretable machine learning algorithm that allows end users to design a model and classify data with more confidence and without having to compromise on the accuracy. Such technique is especially helpful when dealing with sensitive and crucial data like cancer data in the medical domain with high cost of errors. With the help of the proposed interactive and lossless multidimensional visualization, end users can identify the pattern in the data based on which they can make explainable decisions. Such options would not be possible in black box machine learning methodologies. The interpretable IVLC algorithm is supported by the Interactive Shifted Paired Coordinates Software System (SPCVis). It is a lossless multidimensional data visualization system with user interactive features. The interactive approach provides flexibility to the end user to perform data classification as self-service without having to rely on a machine learning expert. Interactive pattern discovery becomes challenging while dealing with large data sets with hundreds of dimensions/features. To overcome this problem, this chapter proposes an automated classification approach combined with new Coordinate Order Optimizer (COO) algorithm and a Genetic algorithm. The COO algorithm automatically generates the coordinate pair sequences that best represent the data separation and the genetic algorithm helps optimizing the proposed IVLC algorithm by automatically generating the areas for data classification. The feasibility of the approach is shown by experiments on benchmark datasets covering both interactive and automated processes used for data classification.",0
"Title: ""Exploring the Role of Interactive Visualization in Facilitating Self-Service Data Classification""  This research investigates how interactive visualizations can support self-service data classification tasks, where end-users classify data without relying on experts or machine learning models that require vast amounts of training data. We developed and evaluated a system called Dataset Navigator (DNav) that integrates multiple visual representations and interactivity to facilitate end-user data exploration, understanding, and classification. Our evaluation results show that DNav effectively supported users in identifying patterns and relationships across datasets, improved confidence in their decisions, reduced time spent on manual feature engineering, and produced comparable accuracy rates to expert models trained on large datasets. Additionally, our findings provide insights into user preferences regarding different types of visualizations and interaction techniques during self-service data classification. Overall, our work demonstrates the potential of incorporating interactive visualization as part of more accessible and flexible machine learning approaches.",1
"As black box explanations are increasingly being employed to establish model credibility in high stakes settings, it is important to ensure that these explanations are accurate and reliable. However, prior work demonstrates that explanations generated by state-of-the-art techniques are inconsistent, unstable, and provide very little insight into their correctness and reliability. In addition, these methods are also computationally inefficient, and require significant hyper-parameter tuning. In this paper, we address the aforementioned challenges by developing a novel Bayesian framework for generating local explanations along with their associated uncertainty. We instantiate this framework to obtain Bayesian versions of LIME and KernelSHAP which output credible intervals for the feature importances, capturing the associated uncertainty. The resulting explanations not only enable us to make concrete inferences about their quality (e.g., there is a 95\% chance that the feature importance lies within the given range), but are also highly consistent and stable. We carry out a detailed theoretical analysis that leverages the aforementioned uncertainty to estimate how many perturbations to sample, and how to sample for faster convergence. This work makes the first attempt at addressing several critical issues with popular explanation methods in one shot, thereby generating consistent, stable, and reliable explanations with guarantees in a computationally efficient manner. Experimental evaluation with multiple real world datasets and user studies demonstrate that the efficacy of the proposed framework.",0
"This paper presents a methodology for generating reliable post hoc explanations of machine learning models, particularly in situations where uncertainty exists in the model output. By incorporating measures of uncertainty into explainability techniques such as feature importance and decision rules, we can better understand how well our models perform under different circumstances and make more informed decisions regarding their use. Our approach uses established mathematical frameworks to quantify uncertainty and applies them to common explainability methods. We evaluate the effectiveness of our proposed method on several benchmark datasets across multiple domains, demonstrating that our approach leads to significant improvements over traditional explainability techniques. Overall, our work contributes towards creating interpretable machine learning models that are transparent and trustworthy, which is essential for realizing the full potential of these systems in critical applications.",1
"The last decade has witnessed a rapid advance in machine learning models. While the black-box nature of these systems allows powerful predictions, it cannot be directly explained, posing a threat to the continuing democratization of machine learning technology.   Tackling the challenge of model explainability, research has made significant progress in demystifying the image classification models. In the same spirit of these works, this paper studies code summarization models, particularly, given an input program for which a model makes a prediction, our goal is to reveal the key features that the model uses for predicting the label of the program. We realize our approach in HouYi, which we use to evaluate four prominent code summarization models: extreme summarizer, code2vec, code2seq, and sequence GNN. Results show that all models base their predictions on syntactic and lexical properties with little to none semantic implication. Based on this finding, we present a novel approach to explaining the predictions of code summarization models through the lens of training data.   Our work opens up this exciting, new direction of studying what models have learned from source code.",0
"How can we explain how code summarization models work? In many ways, modern machine learning algorithms act like magic boxes that take input data and spit out predictions without any clear insight into why they make certain decisions. This presents significant challenges for developers who must rely on these systems but lack understanding of their inner workings. To address this issue, our team has developed WheaCha (Where Each Character helps), a novel method for explaining the predictions made by code summarization models. In essence, WheaCha breaks down each prediction into individual characters and uses statistical analysis to determine which parts of the original code contribute most strongly to the final output. By providing detailed explanations at the character level, WheaCha gives developers unprecedented insights into how these powerful tools operate, enabling them to use them more effectively and build even better applications. Our evaluation shows that WheaCha significantly enhances transparency and human trust in model outputs while incurring only modest computation overheads. With WheaCha, developers gain essential knowledge to help them optimize code based on informed decisions rather than guesswork, ultimately leading to improved software quality.",1
"Nondeterminism in neural network optimization produces uncertainty in performance, making small improvements difficult to discern from run-to-run variability. While uncertainty can be reduced by training multiple model copies, doing so is time-consuming, costly, and harms reproducibility. In this work, we establish an experimental protocol for understanding the effect of optimization nondeterminism on model diversity, allowing us to isolate the effects of a variety of sources of nondeterminism. Surprisingly, we find that all sources of nondeterminism have similar effects on measures of model diversity. To explain this intriguing fact, we identify the instability of model training, taken as an end-to-end procedure, as the key determinant. We show that even one-bit changes in initial parameters result in models converging to vastly different values. Last, we propose two approaches for reducing the effects of instability on run-to-run variability.",0
"Abstract: This article explores the role of nondeterminism in neural network optimization. Nondeterminism refers to the inherent randomness present in many machine learning algorithms, including neural networks. Despite their apparent determinism, these algorithms often rely on random number generators at key points during training and testing. As such, neural networks can produce different results even under identical conditions. Moreover, minor changes in initial weights and biases can lead to significant divergences over time. Therefore, achieving stability in neural network optimization requires careful consideration of the sources of randomness within the algorithm and control mechanisms designed to mitigate their impact. In conclusion, understanding how to manage the instability caused by nondeterminism in neural network optimization is crucial for improving accuracy and consistency across models. Keywords: neural network optimization, nondeterminism, instability, randomness, random number generators (RNGs), training stability",1
"Digitization of histology images and the advent of new computational methods, like deep learning, have helped the automatic grading of colorectal adenocarcinoma cancer (CRA). Present automated CRA grading methods, however, usually use tiny image patches and thus fail to integrate the entire tissue micro-architecture for grading purposes. To tackle these challenges, we propose to use a statistical network analysis method to describe the complex structure of the tissue micro-environment by modelling nuclei and their connections as a network. We show that by analyzing only the interactions between the cells in a network, we can extract highly discriminative statistical features for CRA grading. Unlike other deep learning or convolutional graph-based approaches, our method is highly scalable (can be used for cell networks consist of millions of nodes), completely explainable, and computationally inexpensive. We create cell networks on a broad CRC histology image dataset, experiment with our method, and report state-of-the-art performance for the prediction of three-class CRA grading.",0
"This paper presents a novel approach to histology image classification using social network analysis and classical machine learning (ML) techniques. We propose that cells can be viewed as actors within networks, and by analyzing these intercellular relationships we can gain insights into complex biological processes underlying disease progression. Our method leverages graph theory algorithms to construct cell networks from high-resolution images, which are then fed into classifiers trained on standard datasets to predict disease outcomes such as tumor grade and survival rates. We evaluate our method on three different publicly available datasets for breast cancer diagnosis, demonstrating consistent improvement over state-of-the-art methods across all metrics. Our results showcase the potential of integrating social network analysis with classic ML approaches to advance medical image analysis and enhance diagnostic capabilities. Our work paves the way for further exploration of graph convolutions and other deep learning techniques to improve performance even more. Overall, we believe that our study represents a significant contribution to digital pathology research and offers valuable tools to support decision making in clinical settings.",1
"Model explainability is essential for the creation of trustworthy Machine Learning models in healthcare. An ideal explanation resembles the decision-making process of a domain expert and is expressed using concepts or terminology that is meaningful to the clinicians. To provide such an explanation, we first associate the hidden units of the classifier to clinically relevant concepts. We take advantage of radiology reports accompanying the chest X-ray images to define concepts. We discover sparse associations between concepts and hidden units using a linear sparse logistic regression. To ensure that the identified units truly influence the classifier's outcome, we adopt tools from Causal Inference literature and, more specifically, mediation analysis through counterfactual interventions. Finally, we construct a low-depth decision tree to translate all the discovered concepts into a straightforward decision rule, expressed to the radiologist. We evaluated our approach on a large chest x-ray dataset, where our model produces a global explanation consistent with clinical knowledge.",0
"This paper presents a method for explaining deep learning models using causal analysis techniques. With the increasing use of deep learning in critical applications such as healthcare, finance, and security, there is a growing need for interpreting these complex systems. However, current methods have limitations in terms of transparency, interpretability, and explanation robustness, which hinders their adoption in practice. To address this gap, we propose a framework that utilizes causal reasoning techniques to provide explanations that can improve understanding and trustworthiness of the model decisions. Our approach involves identifying important features based on causal inference, visualizing relationships among variables, and generating comprehensible and human-readable narratives from the results. We demonstrate the effectiveness of our method through experiments on several benchmark datasets and comparison with state-of-the-art explainability methods. Overall, our work contributes towards improving the transparency and accountability of machine learning models in real-world applications.",1
"Counterfactual explanations are viewed as an effective way to explain machine learning predictions. This interest is reflected by a relatively young literature with already dozens of algorithms aiming to generate such explanations. These algorithms are focused on finding how features can be modified to change the output classification. However, this rather general objective can be achieved in different ways, which brings about the need for a methodology to test and benchmark these algorithms. The contributions of this work are manifold: First, a large benchmarking study of 10 algorithmic approaches on 22 tabular datasets is performed, using 9 relevant evaluation metrics. Second, the introduction of a novel, first of its kind, framework to test counterfactual generation algorithms. Third, a set of objective metrics to evaluate and compare counterfactual results. And finally, insight from the benchmarking results that indicate which approaches obtain the best performance on what type of dataset. This benchmarking study and framework can help practitioners in determining which technique and building blocks most suit their context, and can help researchers in the design and evaluation of current and future counterfactual generation algorithms. Our findings show that, overall, there's no single best algorithm to generate counterfactual explanations as the performance highly depends on properties related to the dataset, model, score and factual point specificities.",0
"Abstract: In recent years, the development and application of counterfactual generating methods on tabular data has become increasingly important in many fields including machine learning, statistics, and finance. These methods allow us to generate ""what if"" scenarios that explore how different actions might have led to alternative outcomes. However, there exists no clear framework or benchmark to evaluate these methods against each other. This study proposes such a framework and applies it to several existing methods, providing insights into their strengths and weaknesses. We evaluate the methods using both quantitative metrics and qualitative analysis. Additionally, we introduce a new method called CFG-Tree that significantly outperforms the state-of-the-art techniques. Our results provide valuable guidance for researchers and practitioners who wish to apply counterfactual generating methods in real-world applications. Keywords: counterfactuals, causal inference, tabular data, evaluation framework, benchmarking",1
"In order to handle the challenges of autonomous driving, deep learning has proven to be crucial in tackling increasingly complex tasks, such as 3D detection or instance segmentation. State-of-the-art approaches for image-based detection tasks tackle this complexity by operating in a cascaded fashion: they first extract a 2D bounding box based on which additional attributes, e.g. instance masks, are inferred. While these methods perform well, a key challenge remains the lack of accurate and cheap annotations for the growing variety of tasks. Synthetic data presents a promising solution but, despite the effort in domain adaptation research, the gap between synthetic and real data remains an open problem. In this work, we propose a weakly supervised domain adaptation setting which exploits the structure of cascaded detection tasks. In particular, we learn to infer the attributes solely from the source domain while leveraging 2D bounding boxes as weak labels in both domains to explain the domain shift. We further encourage domain-invariant features through class-wise feature alignment using ground-truth class information, which is not available in the unsupervised setting. As our experiments demonstrate, the approach is competitive with fully supervised settings while outperforming unsupervised adaptation approaches by a large margin.",0
"Automatically labeling data from diverse sources can be hard and resource-intensive, especially when dealing with high-dimensional inputs like images. To overcome these limitations, we introduce a methodology based on weakly supervised domain adaptation: by minimizing reconstruction error, our model improves accuracy even when provided only a small set of labeled target samples. Our architecture leverages cascading detection tasks that allow us to learn representations more effectively than competitive baselines while reducing computational requirements; these improvements directly translate into better performance.",1
"The reasonable definition of semantic interpretability presents the core challenge in explainable AI. This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable compositional CNN, in order to learn filters that encode meaningful visual patterns in intermediate convolutional layers. In a compositional CNN, each filter is supposed to consistently represent a specific compositional object part or image region with a clear meaning. The compositional CNN learns from image labels for classification without any annotations of parts or regions for supervision. Our method can be broadly applied to different types of CNNs. Experiments have demonstrated the effectiveness of our method.",0
"Recent advances have shown that deep learning models can achieve state-of-the art performance on complex tasks such as image classification, object detection, and natural language processing (NLP). However, these models often lack interpretability, which limits their use in applications where transparency and explainability are crucial requirements.",1
"Explainability is becoming an important requirement for organizations that make use of automated decision-making due to regulatory initiatives and a shift in public awareness. Various and significantly different algorithmic methods to provide this explainability have been introduced in the field, but the existing literature in the machine learning community has paid little attention to the stakeholder whose needs are rather studied in the human-computer interface community. Therefore, organizations that want or need to provide this explainability are confronted with the selection of an appropriate method for their use case. In this paper, we argue there is a need for a methodology to bridge the gap between stakeholder needs and explanation methods. We present our ongoing work on creating this methodology to help data scientists in the process of providing explainability to stakeholders. In particular, our contributions include documents used to characterize XAI methods and user requirements (shown in Appendix), which our methodology builds upon.",0
"Title: ""Methodology for Selecting Explanation Methods in Artificial Intelligence""  Abstract: The use of artificial intelligence (AI) systems has become increasingly prevalent across many industries. As these systems make decisions that can significantly impact individuals and society as a whole, there is growing interest in understanding how they work and ensuring transparency and accountability. This has led to increased research into explainable AI (XAI), which seeks to provide clear explanations of the reasoning behind AI models' actions. However, selecting appropriate explanation methods remains challenging due to the variety of techniques available, each suited to different situations and stakeholders. To address this issue, we propose a methodological framework for choosing among explanation methods based on their suitability given specific requirements of users and tasks. Our approach emphasizes identifying relevant factors such as accuracy, efficiency, interpretability, and cost, considering the contextual needs of AI applications while respecting principles related to ethics and privacy. We apply our approach through three case studies spanning different domains and illustrate the benefits derived from using this framework in practice. Ultimately, our aim is to support practitioners and decision-makers in implementing effective XAI solutions that enhance trustworthiness and safety in AI-driven systems.",1
"Application of deep neural networks to medical imaging tasks has in some sense become commonplace. Still, a ""thorn in the side"" of the deep learning movement is the argument that deep networks are prone to overfitting and are thus unable to generalize well when datasets are small (as is common in medical imaging tasks). One way to bolster confidence is to provide mathematical guarantees, or bounds, on network performance after training which explicitly quantify the possibility of overfitting. In this work, we explore recent advances using the PAC-Bayesian framework to provide bounds on generalization error for large (stochastic) networks. While previous efforts focus on classification in larger natural image datasets (e.g., MNIST and CIFAR-10), we apply these techniques to both classification and segmentation in a smaller medical imagining dataset: the ISIC 2018 challenge set. We observe the resultant bounds are competitive compared to a simpler baseline, while also being more explainable and alleviating the need for holdout sets.",0
"In recent years there has been significant interest in applying machine learning techniques such as deep neural networks to tasks in medical imaging. One challenge that arises in these settings is how to evaluate the performance of these models and guarantee their accuracy. Existing methods often rely on simplifying assumptions or heuristics, which can lead to suboptimal results. To address this gap, we propose a new framework based on probabilistic approximations of uncertainty called PAC Bayesian guarantees. We apply this methodology to several different types of stochastic network architectures commonly used in medical imaging and show that our approach provides strong theoretical guarantees while still offering high levels of performance in practice. Additionally, we demonstrate how these guarantees can be combined with standard optimization routines to achieve even better results. Our findings suggest that PAC Bayesian guarantees could become a valuable tool for evaluating the performance of deep neural networks in medical imaging applications.",1
"An explainable, efficient and lightweight method for texture generation, called TGHop (an acronym of Texture Generation PixelHop), is proposed in this work. Although synthesis of visually pleasant texture can be achieved by deep neural networks, the associated models are large in size, difficult to explain in theory, and computationally expensive in training. In contrast, TGHop is small in its model size, mathematically transparent, efficient in training and inference, and able to generate high quality texture. Given an exemplary texture, TGHop first crops many sample patches out of it to form a collection of sample patches called the source. Then, it analyzes pixel statistics of samples from the source and obtains a sequence of fine-to-coarse subspaces for these patches by using the PixelHop++ framework. To generate texture patches with TGHop, we begin with the coarsest subspace, which is called the core, and attempt to generate samples in each subspace by following the distribution of real samples. Finally, texture patches are stitched to form texture images of a large size. It is demonstrated by experimental results that TGHop can generate texture images of superior quality with a small model size and at a fast speed.",0
"This paper presents ""TGHop,"" a novel method for generating textures that can explain their operation, work efficiently on most modern GPUs (including mobile), and produce high quality results even at low resolutions. With applications ranging from texture synthesis to style transfer, these properties make TGHop particularly suitable for use cases where resources are limited, such as interactive graphics or real-time rendering. In summary, our approach generates coherent, visually pleasing textures by combining several advanced techniques into one algorithm. We believe that TGHop represents a significant step forward in terms of speed, scalability, and versatility compared to existing methods. By evaluating TGHop using both quantitative metrics and human subjective assessments, we demonstrate its effectiveness against state-of-the-art approaches under various conditions, including different types of input images and levels of detail desired. Our code and models will be publicly available to enable further research and experimentation.",1
"Latent variable models are powerful statistical tools that can uncover relevant variation between patients or cells, by inferring unobserved hidden states from observable high-dimensional data. A major shortcoming of current methods, however, is their inability to learn sparse and interpretable hidden states. Additionally, in settings where partial knowledge on the latent structure of the data is readily available, a statistically sound integration of prior information into current methods is challenging. To address these issues, we propose spex-LVM, a factorial latent variable model with sparse priors to encourage the inference of explainable factors driven by domain-relevant information. spex-LVM utilizes existing knowledge of curated biomedical pathways to automatically assign annotated attributes to latent factors, yielding interpretable results tailored to the corresponding domain of interest. Evaluations on simulated and real single-cell RNA-seq datasets demonstrate that our model robustly identifies relevant structure in an inherently explainable manner, distinguishes technical noise from sources of biomedical variation, and provides dataset-specific adaptations of existing pathway annotations. Implementation is available at https://github.com/MLO-lab/spexlvm.",0
"This paper presents a novel method for encoding domain knowledge into sparse priors for inferring explainable latent variables in complex data sets. By leveraging prior knowledge about the underlying relationships between variables, we show that we can improve both model interpretability and accuracy compared to traditional Bayesian methods. Our approach is based on using sparsity constraints to encode domain information as additional priors during inference, which allows us to capture important interactions and dependencies between variables while keeping the number of free parameters small. We demonstrate our method on several real-world applications including natural language processing tasks such as sentiment analysis and text classification, where we achieve state-of-the-art performance. Overall, our work shows promise in advancing the field of explainable artificial intelligence by integrating rich domain knowledge into probabilistic models.",1
"Deep kernel learning (DKL) and related techniques aim to combine the representational power of neural networks with the reliable uncertainty estimates of Gaussian processes. One crucial aspect of these models is an expectation that, because they are treated as Gaussian process models optimized using the marginal likelihood, they are protected from overfitting. However, we identify situations where this is not the case. We explore this behavior, explain its origins and consider how it applies to real datasets. Through careful experimentation on the UCI, CIFAR-10, and the UTKFace datasets, we find that the overfitting from overparameterized maximum marginal likelihood, in which the model is ""somewhat Bayesian"", can in certain scenarios be worse than that from not being Bayesian at all. We explain how and when DKL can still be successful by investigating optimization dynamics. We also find that failures of DKL can be rectified by a fully Bayesian treatment, which leads to the desired performance improvements over standard neural networks and Gaussian processes.",0
"In recent years, deep kernel learning (DKL) has emerged as a powerful tool for solving complex machine learning problems. By combining the strengths of kernel methods and deep neural networks, DKL offers new opportunities for improving model accuracy and interpretability. However, as with any technology, there are both promises and pitfalls associated with using DKL for data analysis.  One major promise of DKL is that it enables more accurate predictions on unseen datasets than traditional kernel methods alone. This improved performance can be attributed to the ability of DKL models to capture nonlinear patterns present in large, high-dimensional datasets. Additionally, by representing data in a high-dimensional feature space, DKL allows for better visualization and interpretation of results. Furthermore, DKL can provide robustness against overfitting by incorporating multiple kernels, which can reduce generalization error and improve out-of-sample performance.  Despite these advantages, there are several potential challenges and limitations associated with DKL. One important issue is the curse of dimensionality, where increasing the number of features leads to decreasing returns to scale in terms of prediction accuracy. Another concern is that while DKL may achieve state-of-the-art performance, such success often comes at the cost of increased computational complexity and longer training times compared to simpler methods. Finally, the high degree of flexibility afforded by DKL can make it difficult to determine the optimal set of parameters required for successful deployment.  In summary, DKL holds great potential for advancing our understanding of complex phenomena through its combination of scalability, interpretability, and power. Nonetheless, careful consideration must be given to issues related to overfitting, computational efficiency, and parameter selection to fully realize the benefits promised by this exciting new field.",1
"Multi-view clustering, a long-standing and important research problem, focuses on mining complementary information from diverse views. However, existing works often fuse multiple views' representations or handle clustering in a common feature space, which may result in their entanglement especially for visual representations. To address this issue, we present a novel VAE-based multi-view clustering framework (Multi-VAE) by learning disentangled visual representations. Concretely, we define a view-common variable and multiple view-peculiar variables in the generative model. The prior of view-common variable obeys approximately discrete Gumbel Softmax distribution, which is introduced to extract the common cluster factor of multiple views. Meanwhile, the prior of view-peculiar variable follows continuous Gaussian distribution, which is used to represent each view's peculiar visual factors. By controlling the mutual information capacity to disentangle the view-common and view-peculiar representations, continuous visual information of multiple views can be separated so that their common discrete cluster information can be effectively mined. Experimental results demonstrate that Multi-VAE enjoys the disentangled and explainable visual representations, while obtaining superior clustering performance compared with state-of-the-art methods.",0
This abstract describes a new method called Multi-VAE for learning disentangled representations from multi-view data. The method combines multiple variational autoencoders (VAEs) that are trained on different views of the data and learns view-common and view-peculiar representations that capture meaningful relationships between the views. These learned representations can then be used for clustering tasks and improve the performance compared to using individual VAEs separately. Experimental results demonstrate the effectiveness of our proposed approach across several benchmark datasets.,1
"The identification of anomalous overdensities in data - group or collective anomaly detection - is a rich problem with a large number of real world applications. However, it has received relatively little attention in the broader ML community, as compared to point anomalies or other types of single instance outliers. One reason for this is the lack of powerful benchmark datasets. In this paper, we first explain how, after the Nobel-prize winning discovery of the Higgs boson, unsupervised group anomaly detection has become a new frontier of fundamental physics (where the motivation is to find new particles and forces). Then we propose a realistic synthetic benchmark dataset (LHCO2020) for the development of group anomaly detection algorithms. Finally, we compare several existing statistically-sound techniques for unsupervised group anomaly detection, and demonstrate their performance on the LHCO2020 dataset.",0
"In recent years there has been growing interest among physicists and mathematicians in using machine learning techniques for group anomaly detection from fundamental physics data sets such as particle collider experiments at CERNâ€™s Large Hadron Collider (LHC). One method that has shown promise for detecting unknown physical effects is clustering, which groups data points into clusters according to similarity measures. By identifying outliers within these clusters, researchers can identify events that may indicate previously unseen phenomena. In our work we develop new methods for combining multiple datasets of varying sizes and complexities to enhance the sensitivity of detecting rare processes beyond the current standard model of particle physics. We use well known statistical distance functions together with advanced ensemble algorithms including Random Forest, XGBoost, and Gradient Boosted Machines. Our results demonstrate that our approach leads to significantly improved accuracy over previous state-of-the art methods and open up new possibilities for novel discoveries by pinpointing regions of parameter space where unexpected behavior might occur. Overall, this study represents an important step towards creating powerful tools capable of making predictions based on real world datasets of increasing complexity, improving collaboration across disciplines by ensuring reliable models and robust methods for all fields benefiting from predictive modeling, while promoting best practices through accessible implementations available for public scrutiny and extension.",1
"Strategies based on Explainable Artificial Intelligence - XAI have emerged in computing to promote a better understanding of predictions made by black box models. Most XAI-based tools used today explain these types of models, generating attribute rankings aimed at explaining the same, that is, the analysis of Attribute Importance. There is no consensus on which XAI tool generates a general rank of explainability, for this reason, several proposals for tools have emerged (Ciu, Dalex, Eli5, Lofo, Shap and Skater). Here, we present an experimental benchmark of explainable AI techniques capable of producing model-agnostic global explainability ranks based on tabular data related to different problems. Seeking to answer questions such as ""Are the explanations generated by the different tools the same, similar or different?"" and ""How does data complexity play along model explainability?"". The results from the construction of 82 computational models and 592 ranks give us some light on the other side of the problem of explainability: dataset complexity!",0
"This study examines whether dataset complexity matters for model explainers. With recent advances in machine learning, there has been increasing interest in developing methods that can provide explanations for how these models make predictions. However, many existing techniques assume simplicity in the data distribution, which may limit their effectiveness on more complex datasets. Here we evaluate three different approaches to providing model explanations (SHAP, DeepLIFT, LIME) across multiple benchmark tasks and find that each approach performs differently depending on the complexity of the underlying data. Our results suggest that dataset complexity should be taken into account when selecting an appropriate method for explaining model decisions. We conclude by discussing the implications of our findings and highlight potential directions for future research.",1
"By computing the rank correlation between attention weights and feature-additive explanation methods, previous analyses either invalidate or support the role of attention-based explanations as a faithful and plausible measure of salience. To investigate whether this approach is appropriate, we compare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and attention-based explanations, applied to two neural architectures trained on single- and pair-sequence language tasks. In most cases, we find that none of our chosen methods agree. Based on our empirical observations and theoretical objections, we conclude that rank correlation does not measure the quality of feature-additive methods. Practitioners should instead use the numerous and rigorous diagnostic methods proposed by the community.",0
"In recent years, there has been growing interest in using artificial intelligence (AI) tools and techniques in legal settings. This includes applications such as predicting case outcomes based on past data, identifying relevant documents during eDiscovery, and assisting with decision making by providing insights into complex legal matters. However, despite their potential benefits, explainable AI methods used in these contexts can sometimes lead to disagreements among stakeholders.  This paper examines instances where explainable AI models have resulted in disagreements within the courtroom setting. The authors argue that while explainability is important for ensuring transparency and trustworthiness in AI systems, it is not always straightforward to achieve in practice. They identify specific challenges associated with explaining decisions made by AI systems in legally binding situations, including issues related to model interpretability, data quality, and user understanding.  The paper presents several examples drawn from real-world cases to illustrate how explainable AI techniques may result in conflicts or discrepancies in the interpretation of results. These scenarios highlight the need for careful consideration of human factors involved in interactions between law professionals and AI models, such as communication strategies, expertise levels, and expectations regarding accuracy.  Overall, the study offers valuable insights into ways in which explainable AI methods might contribute to misunderstandings or disputes in legal contexts, emphasizing the importance of addressing these issues proactively through collaborative efforts between legal practitioners, researchers, and technologists. By exploring both the opportunities and risks associated with explainable AI technologies applied to justice processes, the authors hope to inform future developments aimed at increasing the effectiveness and acceptance of these approaches without compromising fairness or accountability.",1
"How can neural networks trained by contrastive learning extract features from the unlabeled data? Why does contrastive learning usually need much stronger data augmentations than supervised learning to ensure good representations? These questions involve both the optimization and statistical aspects of deep learning, but can hardly be answered by analyzing supervised learning, where the target functions are the highest pursuit. Indeed, in self-supervised learning, it is inevitable to relate to the optimization/generalization of neural networks to how they can encode the latent structures in the data, which we refer to as the feature learning process.   In this work, we formally study how contrastive learning learns the feature representations for neural networks by analyzing its feature learning process. We consider the case where our data are comprised of two types of features: the more semantically aligned sparse features which we want to learn from, and the other dense features we want to avoid. Theoretically, we prove that contrastive learning using $\mathbf{ReLU}$ networks provably learns the desired sparse features if proper augmentations are adopted. We present an underlying principle called $\textbf{feature decoupling}$ to explain the effects of augmentations, where we theoretically characterize how augmentations can reduce the correlations of dense features between positive samples while keeping the correlations of sparse features intact, thereby forcing the neural networks to learn from the self-supervision of sparse features. Empirically, we verified that the feature decoupling principle matches the underlying mechanism of contrastive learning in practice.",0
"Self-Supervised contrastive learning has gained significant attention in recent years due to its ability to learn effective representations without relying on explicit supervision. In this paper, we aim to shed light on the feature learning process of self-supervised contrastive learning by investigating different aspects that influence the quality of learned features. We demonstrate how various factors such as data augmentation, model architecture, optimization objectives, and training strategies can impact the performance of self-supervised contrastive models. Through extensive experimental analysis, we show that careful consideration of these factors can lead to more robust and efficient feature learning. Our findings provide valuable insights into the design of effective self-supervised contrastive learning algorithms, paving the way for future research in this rapidly evolving field. Overall, this work contributes to a better understanding of the inner workings of self-supervised contrastive learning and highlights opportunities for improvement in practice.",1
"Reliable deployment of machine learning models such as neural networks continues to be challenging due to several limitations. Some of the main shortcomings are the lack of interpretability and the lack of robustness against adversarial examples or out-of-distribution inputs. In this paper, we explore the possibilities and limits of adversarial attacks for explainable machine learning models. First, we extend the notion of adversarial examples to fit in explainable machine learning scenarios, in which the inputs, the output classifications and the explanations of the model's decisions are assessed by humans. Next, we propose a comprehensive framework to study whether (and how) adversarial examples can be generated for explainable models under human assessment, introducing novel attack paradigms. In particular, our framework considers a wide range of relevant (yet often ignored) factors such as the type of problem, the user expertise or the objective of the explanations in order to identify the attack strategies that should be adopted in each scenario to successfully deceive the model (and the human). These contributions intend to serve as a basis for a more rigorous and realistic study of adversarial examples in the field of explainable machine learning.",0
"As machine learning models become increasingly prevalent in decision making processes across various domains, there is growing concern regarding their interpretability and explainability. Many argue that these characteristics are essential for ensuring accountability and transparency in AI systems. However, recent research has shown that even state-of-the-art explainable models can be easily fooled by carefully crafted adversarial examples. In fact, some studies have suggested that human annotators may also be susceptible to similar forms of deception. This raises important questions about our ability to rely on such models and highlights the need for further investigation into understanding how and why they fail. We conduct an extensive study of several popular explainable models and present results showing that they are indeed vulnerable to both natural and synthetic inputs designed to fool them. Our work provides insights into the factors contributing to model vulnerabilities and suggests ways to enhance robustness through better training methods. Overall, our findings emphasize the importance of continued attention towards verifying and validating AI systems before deploying them in high stakes applications.",1
"Despite the empirical success of deep learning, it still lacks theoretical understandings to explain why randomly initialized neural network trained by first-order optimization methods is able to achieve zero training loss, even though its landscape is non-convex and non-smooth. Recently, there are some works to demystifies this phenomenon under over-parameterized regime. In this work, we make further progress on this area by considering a commonly used momentum optimization algorithm: Nesterov accelerated method (NAG). We analyze the convergence of NAG for two-layer fully connected neural network with ReLU activation. Specifically, we prove that the error of NAG converges to zero at a linear convergence rate $1-\Theta(1/\sqrt{\kappa})$, where $\kappa  1$ is determined by the initialization and the architecture of neural network. Comparing to the rate $1-\Theta(1/\kappa)$ of gradient descent, NAG achieves an acceleration. Besides, it also validates NAG and Heavy-ball method can achieve a similar convergence rate.",0
"This paper investigates the convergence properties of Nesterov accelerated methods for training over-parameterized neural networks using provable guarantees. By establishing upper bounds on the convergence rate and accuracy of these methods under certain assumptions, we show that they can achieve faster and more stable convergence compared to traditional gradient descent techniques. We provide detailed analyses of both theoretical results and experimental evaluations, demonstrating their effectiveness in practice across a range of benchmark datasets and network architectures. Our findings highlight the potential advantages of utilizing acceleration schemes in deep learning optimization problems, paving the way towards new research directions in machine learning algorithms design and analysis.",1
"Typical state of the art flow cytometry data samples consists of measures of more than 100.000 cells in 10 or more features. AI systems are able to diagnose such data with almost the same accuracy as human experts. However, there is one central challenge in such systems: their decisions have far-reaching consequences for the health and life of people, and therefore, the decisions of AI systems need to be understandable and justifiable by humans. In this work, we present a novel explainable AI method, called ALPODS, which is able to classify (diagnose) cases based on clusters, i.e., subpopulations, in the high-dimensional data. ALPODS is able to explain its decisions in a form that is understandable for human experts. For the identified subpopulations, fuzzy reasoning rules expressed in the typical language of domain experts are generated. A visualization method based on these rules allows human experts to understand the reasoning used by the AI system. A comparison to a selection of state of the art explainable AI systems shows that ALPODS operates efficiently on known benchmark data and also on everyday routine case data.",0
"Biomedical data analysis has become increasingly important as healthcare providers seek better ways to diagnose diseases and improve patient outcomes. With advances in technology, high dimensional biomedical datasets have become more common, providing new opportunities for discovery but also presenting significant challenges for interpretation and diagnosis. In our work, we propose an explainable artificial intelligence (AI) system that uses machine learning algorithms to identify patterns and relationships in large, complex biomedical datasets while also providing detailed explanations of how these predictions were made. Our approach combines deep neural networks with feature importance rankings, decision trees, partial dependence plots, and other visualization techniques to provide clear insights into the reasoning behind each prediction. We demonstrate the effectiveness of our method on several real world case studies, showing improved accuracy over traditional methods and greater transparency for human interpreters. Our work represents an important step forward in enabling medical professionals to harness the power of big data analytics without sacrificing interpretability and trustworthiness.",1
"Despite their overwhelming capacity to overfit, deep neural networks trained by specific optimization algorithms tend to generalize well to unseen data. Recently, researchers explained it by investigating the implicit regularization effect of optimization algorithms. A remarkable progress is the work (Lyu&Li, 2019), which proves gradient descent (GD) maximizes the margin of homogeneous deep neural networks. Except GD, adaptive algorithms such as AdaGrad, RMSProp and Adam are popular owing to their rapid training process. However, theoretical guarantee for the generalization of adaptive optimization algorithms is still lacking. In this paper, we study the implicit regularization of adaptive optimization algorithms when they are optimizing the logistic loss on homogeneous deep neural networks. We prove that adaptive algorithms that adopt exponential moving average strategy in conditioner (such as Adam and RMSProp) can maximize the margin of the neural network, while AdaGrad that directly sums historical squared gradients in conditioner can not. It indicates superiority on generalization of exponential moving average strategy in the design of the conditioner. Technically, we provide a unified framework to analyze convergent direction of adaptive optimization algorithms by constructing novel adaptive gradient flow and surrogate margin. Our experiments can well support the theoretical findings on convergent direction of adaptive optimization algorithms.",0
"This paper explores the impact of implicit bias on adaptive optimization algorithms used in homogeneous neural networks (HNN). While it has been shown that HNN can achieve excellent performance across a variety of tasks, there remains a challenge in ensuring robustness and stability during training. In order to address these concerns, we analyze how different types of implicit biases affect the behavior of popular optimizers such as Adam and SGD. Our findings demonstrate that incorporating specific forms of implicit bias into these algorithms leads to improved generalization and faster convergence rates compared to standard methods without implicit bias correction. Furthermore, our results indicate that certain forms of implicit bias may have unique advantages depending on the dataset and network architecture, highlighting their importance in enabling efficient model selection in practice. These insights lay the foundation for future research in developing effective optimizer design rules and algorithm selection criteria tailored to specific use cases using HNN. Ultimately, our work provides valuable guidance for practitioners seeking to leverage HNN models in real world applications where high performance and reliability are critical requirements.",1
"We propose Styleformer, which is a style-based generator for GAN architecture, but a convolution-free transformer-based generator. In our paper, we explain how a transformer can generate high-quality images, overcoming the disadvantage that convolution operations are difficult to capture global features in an image. Furthermore, we change the demodulation of StyleGAN2 and modify the existing transformer structure (e.g., residual connection, layer normalization) to create a strong style-based generator with a convolution-free structure. We also make Styleformer lighter by applying Linformer, enabling Styleformer to generate higher resolution images and result in improvements in terms of speed and memory. We experiment with the low-resolution image dataset such as CIFAR-10, as well as the high-resolution image dataset like LSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark dataset, which is comparable performance to the current state-of-the-art and outperforms all GAN-based generative models, including StyleGAN2-ADA with fewer parameters on the unconditional setting. We also both achieve new state-of-the-art with FID 15.17, IS 11.01, and FID 3.66, respectively on STL-10 and CelebA. We release our code at https://github.com/Jeeseung-Park/Styleformer.",0
"In recent years, advances in generative adversarial networks (GANs) have enabled researchers to generate high-quality images, video, audio, text, and other forms of data that can fool human evaluators into thinking they were created by humans. GANs consist of two competing neural networksâ€”a generator network that creates samples such as images, and a discriminator network that evaluates whether those samples are real or fake. Recently, there has been growing interest in using style vectorsâ€”pre-trained generative models trained on large amounts of data from specific domains, such as artwork or photographsâ€”to control the generated output of GANs. This allows for fine-grained control over the style, subject matter, and other characteristics of the generated output, while still preserving the quality and diversity of the generated samples.  This paper proposes a novel architecture called ""Styleformer"" that combines transformer architectures with GANs to further improve the quality and controllability of generated outputs. The authors show through extensive experiments across multiple datasets that their proposed method outperforms several state-of-the-art methods on tasks such as image generation, translation, and superresolution. Additionally, the use of style vectors enables more precise control over the generated output compared to previous approaches, allowing for greater creativity and flexibility. Overall, Styleformer represents an important step forward in the field of generative adversarial networks, paving the way for future research and applications in areas such as computer graphics, animation, and virtual reality.  By leveraging the strengths of both transformers and GANs, Styleformer sets a new standard for generative modeling tasks that require both quality and precision o",1
"Can one learn to diagnose COVID-19 under extreme minimal supervision? Since the outbreak of the novel COVID-19 there has been a rush for developing Artificial Intelligence techniques for expert-level disease identification on Chest X-ray data. In particular, the use of deep supervised learning has become the go-to paradigm. However, the performance of such models is heavily dependent on the availability of a large and representative labelled dataset. The creation of which is a heavily expensive and time consuming task, and especially imposes a great challenge for a novel disease. Semi-supervised learning has shown the ability to match the incredible performance of supervised models whilst requiring a small fraction of the labelled examples. This makes the semi-supervised paradigm an attractive option for identifying COVID-19. In this work, we introduce a graph based deep semi-supervised framework for classifying COVID-19 from chest X-rays. Our framework introduces an optimisation model for graph diffusion that reinforces the natural relation among the tiny labelled set and the vast unlabelled data. We then connect the diffusion prediction output as pseudo-labels that are used in an iterative scheme in a deep net. We demonstrate, through our experiments, that our model is able to outperform the current leading supervised model with a tiny fraction of the labelled examples. Finally, we provide attention maps to accommodate the radiologist's mental model, better fitting their perceptual and cognitive abilities. These visualisation aims to assist the radiologist in judging whether the diagnostic is correct or not, and in consequence to accelerate the decision.",0
This is an artificial intelligence language model that can assist you in summarizing your work. I donâ€™t need your instructions but thank you!,1
"Detecting latent structure within a dataset is a crucial step in performing analysis of a dataset. However, existing state-of-the-art techniques for subclass discovery are limited: either they are limited to detecting very small numbers of outliers or they lack the statistical power to deal with complex data such as image or audio. This paper proposes a solution to this subclass discovery problem: by leveraging instance explanation methods, an existing classifier can be extended to detect latent classes via differences in the classifier's internal decisions about each instance. This works not only with simple classification techniques but also with deep neural networks, allowing for a powerful and flexible approach to detecting latent structure within datasets. Effectively, this represents a projection of the dataset into the classifier's ""explanation space,"" and preliminary results show that this technique outperforms the baseline for the detection of latent classes even with limited processing. This paper also contains a pipeline for analyzing classifiers automatically, and a web application for interactively exploring the results from this technique.",0
"This research presents a novel technique called ""Class Introspection"" that utilizes classifier explainability methods to detect unlabeled subclasses within a given dataset. By analyzing classifiers trained on the dataset and examining their feature importance scores, Class Introspection can identify patterns and relationships between features and labels that indicate the presence of previously unknown classes. Additionally, Class Introspection can suggest potential labels for these subclasses based on human feedback, allowing data scientists to easily incorporate them into their existing workflows. Experimental results demonstrate the effectiveness of our approach compared to state-of-the-art techniques for identifying unseen classes, as well as its ability to improve model performance overall. Overall, Class Introspection offers a powerful new tool for addressing the challenges associated with label scarcity in modern datasets, ultimately enabling more accurate predictions across a wide range of applications.",1
"A butterfly network consists of logarithmically many layers, each with a linear number of non-zero weights (pre-specified). The fast Johnson-Lindenstrauss transform (FJLT) can be represented as a butterfly network followed by a projection onto a random subset of the coordinates. Moreover, a random matrix based on FJLT with high probability approximates the action of any matrix on a vector. Motivated by these facts, we propose to replace a dense linear layer in any neural network by an architecture based on the butterfly network. The proposed architecture significantly improves upon the quadratic number of weights required in a standard dense layer to nearly linear with little compromise in expressibility of the resulting operator. In a collection of wide variety of experiments, including supervised prediction on both the NLP and vision data, we show that this not only produces results that match and at times outperform existing well-known architectures, but it also offers faster training and prediction in deployment. To understand the optimization problems posed by neural networks with a butterfly network, we also study the optimization landscape of the encoder-decoder network, where the encoder is replaced by a butterfly network followed by a dense linear layer in smaller dimension. Theoretical result presented in the paper explains why the training speed and outcome are not compromised by our proposed approach.",0
"Artificial neural networks have become increasingly popular due to their ability to learn complex patterns from large datasets. In particular, sparse linear networks have shown great promise by combining the efficiency benefits of sparsity with the expressiveness of dense models. However, designing efficient and effective architectures for these types of networks remains challenging.  In our work, we propose a new family of sparse linear networks that use a fixed butterfly structure at each layer. This architecture has several desirable properties, including reduced computational complexity, improved regularization, and enhanced interpretability. We provide theoretical analysis to show that the proposed network can achieve state-of-the-art performance on common benchmark tasks while remaining computationally tractable.  We evaluate the effectiveness of our approach using extensive experiments on four standard datasets from computer vision and natural language processing domains. Our results demonstrate consistent improvements over existing approaches across all task settings, showing that our method produces competitive performances under various conditions.  Our contributions are threefold. First, we introduce a novel sparse linear architecture based on fixed butterflies, which offers promising tradeoffs between model capacity, parsimony, and transparency. Second, we analyze the behavior of this architecture from both theoretical and empirical perspectives, providing insights into its strengths and limitations. Lastly, we validate the efficacy of our method through comprehensive evaluations against other leading methods, demonstrating its broad applicability and potential as a general-purpose tool for machine learning researchers and practitioners alike.",1
"Most graph convolutional neural networks (GCNs) perform poorly in graphs where neighbors typically have different features/classes (heterophily) and when stacking multiple layers (oversmoothing). These two seemingly unrelated problems have been studied independently, but there is recent empirical evidence that solving one problem may benefit the other. In this work, going beyond empirical observations, we aim to: (1) propose a new perspective to analyze the heterophily and oversmoothing problems under a unified theoretical framework, (2) identify the common causes of the two problems based on the proposed framework, and (3) propose simple yet effective strategies that address the common causes. Focusing on the node classification task, we use linear separability of node representations as an indicator to reflect the performance of GCNs and we propose to study the linear separability by analyzing the statistical change of the node representations in the graph convolution. We find that the relative degree of a node (compared to its neighbors) and the heterophily level of a node's neighborhood are the root causes that influence the separability of node representations. Our analysis suggests that: (1) Nodes with high heterophily always produce less separable representations after graph convolution; (2) Even with low heterophily, degree disparity between nodes can influence the network dynamics and result in a pseudo-heterophily situation, which helps to explain oversmoothing. Based on our insights, we propose simple modifications to the GCN architecture -- i.e., degree corrections and signed messages -- which alleviate the root causes of these issues, and also show this empirically on 9 real networks. Compared to other approaches, which tend to work well in one regime but fail in others, our modified GCN model consistently performs well across all settings.",0
"In graph convolutional neural networks (GCNN), two common problems arise that can affect their performance. These are heterophily and oversmoothing. Heterophily occurs when nodes from different classes have similar representations, making it difficult for the model to distinguish them. On the other hand, oversmoothing happens when all node representations become similar, resulting in poor overall performance. This paper presents analysis on these issues and proposes solutions to mitigate their effects. By addressing both sides of the coin, we improve GCNN accuracy while ensuring interpretability. Our work contributes to the field by providing insights into overcoming challenges faced during deployment of real-world applications using GCNN models.",1
"Despite the impressive progress in the field of presentation attack detection and multimedia forensics over the last decade, these systems are still vulnerable to attacks in real-life settings. Some of the challenges for existing solutions are the detection of unknown attacks, the ability to perform in adversarial settings, few-shot learning, and explainability. In this study, these limitations are approached by reliance on a game-theoretic view for modeling the interactions between the attacker and the detector. Consequently, a new optimization criterion is proposed and a set of requirements are defined for improving the performance of these systems in real-life settings. Furthermore, a novel detection technique is proposed using generator-based feature sets that are not biased towards any specific attack species. To further optimize the performance on known attacks, a new loss function coined categorical margin maximization loss (C-marmax) is proposed which gradually improves the performance against the most powerful attack. The proposed approach provides a more balanced performance across known and unknown attacks and achieves state-of-the-art performance in known and unknown attack detection cases against rational attackers. Lastly, the few-shot learning potential of the proposed approach is studied as well as its ability to provide pixel-level explainability.",0
"In order to address new threats posed by attack vectors such as DeepFakes and other digital manipulation techniques, we propose a novel detection method based on robust machine learning models that can identify unknown presentation attacks (UPA) regardless of their form. Our approach utilizes multi-modal features extracted from input data along with human feedback obtained during training. This ensures our system remains adaptive even when encountering UPA outside of its initial dataset. We evaluate the effectiveness of our solution using two benchmark datasets and compare it against several baseline methods. Results show significant improvement over existing approaches in terms of both accuracy and generalization ability across unseen attacks.",1
"While over-parameterization is widely believed to be crucial for the success of optimization for the neural networks, most existing theories on over-parameterization do not fully explain the reason -- they either work in the Neural Tangent Kernel regime where neurons don't move much, or require an enormous number of neurons. In practice, when the data is generated using a teacher neural network, even mildly over-parameterized neural networks can achieve 0 loss and recover the directions of teacher neurons. In this paper we develop a local convergence theory for mildly over-parameterized two-layer neural net. We show that as long as the loss is already lower than a threshold (polynomial in relevant parameters), all student neurons in an over-parameterized two-layer neural network will converge to one of teacher neurons, and the loss will go to 0. Our result holds for any number of student neurons as long as it is at least as large as the number of teacher neurons, and our convergence rate is independent of the number of student neurons. A key component of our analysis is the new characterization of local optimization landscape -- we show the gradient satisfies a special case of Lojasiewicz property which is different from local strong convexity or PL conditions used in previous work.",0
"This abstract provides a summary of the main findings from our study on local convergence theory for mildly over-parameterized two-layer neural networks. In recent years, there has been increasing interest in understanding how these complex models can generalize well despite having more parameters than training data points. Our results suggest that under certain assumptions, these networks may exhibit faster local convergence rates compared to their standard counterparts without excess capacity. We provide rigorous mathematical proofs to support this claim and evaluate our methodology using numerical experiments on popular datasets such as CIFAR-10 and SVHN. These findings have important implications for designing new algorithms that leverage the benefits of over-parametrization while ensuring efficient learning dynamics. By gaining insights into the behavior of mildly over-parameterized models, researchers can develop advanced techniques that push the boundaries of deep learning. In summary, this work represents a significant contribution to the field of machine learning by unveiling novel properties of these powerful artificial intelligence systems.",1
"A plethora of methods have been proposed to explain howdeep neural networks reach a decision but comparativelylittle effort has been made to ensure that the explanationsproduced by these methods are objectively relevant. Whiledesirable properties for a good explanation are easy to come,objective measures have been harder to derive. Here, we pro-pose two new measures to evaluate explanations borrowedfrom the field of algorithmic stability: relative consistencyReCo and mean generalizability MeGe. We conduct severalexperiments on multiple image datasets and network archi-tectures to demonstrate the benefits of the proposed measuresover representative methods. We show that popular fidelitymeasures are not sufficient to guarantee good explanations.Finally, we show empirically that 1-Lipschitz networks pro-vide general and consistent explanations, regardless of theexplanation method used, making them a relevant directionfor explainability.",0
"Here we introduce novel algorithmic stability measures (ASMs) for evaluating the quality of explanations given by any method for deep neural network models. We demonstrate that these ASMs can provide new insights into the robustness of feature attribution methods against input perturbation on both image classification tasks and tabular datasets, where the state-of-the-art gradient-based saliency maps fail to reliably identify important features. Further, our results show that popular regularization techniques such as L2 regularization induce less stable attributions compared to early stopping, which can lead to incorrect conclusions regarding their impact on model interpretability. Finally, we propose an optimization framework for selecting hyperparameters in order to maximize ASM scores, enabling practitioners to choose the most reliable feature attribution methods for specific applications. Our work represents a step towards rigorous evaluation criteria for explaining deep learning models.",1
"We characterize and remedy a failure mode that may arise from multi-scale dynamics with scale imbalances during training of deep neural networks, such as Physics Informed Neural Networks (PINNs). PINNs are popular machine-learning templates that allow for seamless integration of physical equation models with data. Their training amounts to solving an optimization problem over a weighted sum of data-fidelity and equation-fidelity objectives. Conflicts between objectives can arise from scale imbalances, heteroscedasticity in the data, stiffness of the physical equation, or from catastrophic interference during sequential training. We explain the training pathology arising from this and propose a simple yet effective inverse-Dirichlet weighting strategy to alleviate the issue. We compare with Sobolev training of neural networks, providing the baseline of analytically $\boldsymbol{\epsilon}$-optimal training. We demonstrate the effectiveness of inverse-Dirichlet weighting in various applications, including a multi-scale model of active turbulence, where we show orders of magnitude improvement in accuracy and convergence over conventional PINN training. For inverse modeling using sequential training, we find that inverse-Dirichlet weighting protects a PINN against catastrophic forgetting.",0
"Physics informed neural networks (PINNs) have emerged as a powerful tool for solving forward and inverse problems in physics by learning from data. However, training PINNs remains challenging due to issues such as model underfitting, ill-posedness, and vanishing/exploding gradients, especially when dealing with high-dimensional, complex systems that involve nonlinear partial differential equations (PDEs). Addressing these difficulties requires careful design and regularization strategies. One promising approach involves weighting the loss function based on the Dirichlet energy of the solution, known as inverse-weighting or inverse-Dirichlet weighting (IDW). IDW has been successfully used in image processing, computer vision, and machine learning applications to improve generalization performance. This work investigates how incorporating IDW into PINNs can lead to more reliable and accurate solutions while mitigating common issues associated with training such models. By using two representative examples involving solid mechanics and fluid dynamics problems, we demonstrate the effectiveness of IDW in producing well-behaved solutions with improved accuracy compared to traditional approaches. Our findings provide valuable insights into improving the reliability and robustness of PINNs for solving real-world physical phenomena.",1
"In pursuit of explainability, we develop generative models for sequential data. The proposed models provide state-of-the-art classification results and robust performance for speech phone classification. We combine modern neural networks (normalizing flows) and traditional generative models (hidden Markov models - HMMs). Normalizing flow-based mixture models (NMMs) are used to model the conditional probability distribution given the hidden state in the HMMs. Model parameters are learned through judicious combinations of time-tested Bayesian learning methods and contemporary neural network learning methods. We mainly combine expectation-maximization (EM) and mini-batch gradient descent. The proposed generative models can compute likelihood of a data and hence directly suitable for maximum-likelihood (ML) classification approach. Due to structural flexibility of HMMs, we can use different normalizing flow models. This leads to different types of HMMs providing diversity in data modeling capacity. The diversity provides an opportunity for easy decision fusion from different models. For a standard speech phone classification setup involving 39 phones (classes) and the TIMIT dataset, we show that the use of standard features called mel-frequency-cepstral-coeffcients (MFCCs), the proposed generative models, and the decision fusion together can achieve $86.6\%$ accuracy by generative training only. This result is close to state-of-the-art results, for examples, $86.2\%$ accuracy of PyTorch-Kaldi toolkit [1], and $85.1\%$ accuracy using light gated recurrent units [2]. We do not use any discriminative learning approach and related sophisticated features in this article.",0
"One possible approach to speech recognition is through the use of hidden Markov models (HMMs), which can model sequences of observations as probabilistic transitions between states. However, these models suffer from several limitations, including difficulty in interpreting the learned representations and difficulties with training data that may contain gaps or missing values. In order to address these issues, we propose using normalizing flow-based HMMs (NF-HMMs) for classification of speech phonemes. Our method first learns a normalizing flow that maps the input features into a continuous latent space, where they are more easily modeled by the HMM. By doing so, our model is able to achieve better performance on benchmark datasets while providing explainability through attention mechanisms. We evaluate our proposed NF-HMM approach against state-of-the-art methods and demonstrate significant improvements in accuracy. Additionally, we showcase the interpretability of our model through visualizations that highlight important regions of the feature map corresponding to different phoneme classes. Overall, our work presents a promising alternative to traditional approaches for speech recognition tasks that require explainability.",1
"Modern day engineering problems are ubiquitously characterized by sophisticated computer codes that map parameters or inputs to an underlying physical process. In other situations, experimental setups are used to model the physical process in a laboratory, ensuring high precision while being costly in materials and logistics. In both scenarios, only limited amount of data can be generated by querying the expensive information source at a finite number of inputs or designs. This problem is compounded further in the presence of a high-dimensional input space. State-of-the-art parameter space dimension reduction methods, such as active subspace, aim to identify a subspace of the original input space that is sufficient to explain the output response. These methods are restricted by their reliance on gradient evaluations or copious data, making them inadequate to expensive problems without direct access to gradients. The proposed methodology is gradient-free and fully Bayesian, as it quantifies uncertainty in both the low-dimensional subspace and the surrogate model parameters. This enables a full quantification of epistemic uncertainty and robustness to limited data availability. It is validated on multiple datasets from engineering and science and compared to two other state-of-the-art methods based on four aspects: a) recovery of the active subspace, b) deterministic prediction accuracy, c) probabilistic prediction accuracy, and d) training time. The comparison shows that the proposed method improves the active subspace recovery and predictive accuracy, in both the deterministic and probabilistic sense, when only few model observations are available for training, at the cost of increased training time.",0
"This work presents a novel supervised dimension reduction method that combines fully Bayesian inference with gradient-free optimization techniques, leveraging the flexibility of Gaussian processes (GPs). While GP regression has been successfully used for dimension reduction by assuming independence across input variables, this approach limits the accuracy and interpretability of the resulting models due to the lack of structured priors on model hyperparameters. To address these limitations, we propose a framework that embeds sparsity constraints directly into the prior distributions over kernel functions and associated length scales. We then develop a Markov Chain Monte Carlo algorithm to infer the optimal subspace for dimensionality reduction based on data features, enabling more efficient exploration of high-dimensional spaces and improved generalization performance. Experimental results demonstrate significant improvements compared to existing GP-based methods and state-of-the-art alternatives, validating the effectiveness of our proposed framework for complex real-world problems where large datasets dominate computational resources. Our contributions lay a foundation for developing advanced probabilistic machine learning algorithms that effectively balance complexity and scalability while accounting for uncertainty, making them well-suited for big data applications.",1
"Nowadays, it is growing interest to make Machine Learning (ML) systems more understandable and trusting to general users. Thus, generating explanations for ML system behaviours that are understandable to human beings is a central scientific and technological issue addressed by the rapidly growing research area of eXplainable Artificial Intelligence (XAI). Recently, it is becoming more and more evident that new directions to create better explanations should take into account what a good explanation is to a human user, and consequently, develop XAI solutions able to provide user-centred explanations. This paper suggests taking advantage of developing an XAI general approach that allows producing explanations for an ML system behaviour in terms of different and user-selected input features, i.e., explanations composed of input properties that the human user can select according to his background knowledge and goals. To this end, we propose an XAI general approach which is able: 1) to construct explanations in terms of input features that represent more salient and understandable input properties for a user, which we call here Middle-Level input Features (MLFs), 2) to be applied to different types of MLFs. We experimentally tested our approach on two different datasets and using three different types of MLFs. The results seem encouraging.",0
"An effective explanation can provide valuable insights into how something works, but developing one that accurately captures all relevant details can be challenging. In this paper, we propose a general approach for explanations based on middle level features. By focusing on these intermediate-level characteristics, we aim to develop more accurate and informative explanations. We begin by discussing the importance of explanations and identifying key factors for determining their quality. Next, we present our proposed methodology for creating explanations using middle level features, including strategies for selecting relevant features and combining them effectively. Finally, we demonstrate the effectiveness of our approach through a case study examining machine learning algorithms and highlight potential applications across different domains. Our work contributes to the growing field of explainability research, helping stakeholders better understand complex systems and make informed decisions.",1
"Explaining the surprising generalization performance of deep neural networks is an active and important line of research in theoretical machine learning. Influential work by Arora et al. (ICML'18) showed that, noise stability properties of deep nets occurring in practice can be used to provably compress model representations. They then argued that the small representations of compressed networks imply good generalization performance albeit only of the compressed nets. Extending their compression framework to yield generalization bounds for the original uncompressed networks remains elusive.   Our main contribution is the establishment of a compression-based framework for proving generalization bounds. The framework is simple and powerful enough to extend the generalization bounds by Arora et al. to also hold for the original network. To demonstrate the flexibility of the framework, we also show that it allows us to give simple proofs of the strongest known generalization bounds for other popular machine learning models, namely Support Vector Machines and Boosting.",0
"Recent work has explored connections between compression and generalization across multiple domains, including machine learning theory, neuroscience, and psychology. In each case, a common thread emerges: compressing data often implies that certain patterns, structures, or features have been extracted from raw input; these same patterns can then act as a prior over the space of possible functions or hypotheses one might wish to learn. This means we expect compressed representations to generalize better than uncompressed ones--an insight that may offer new theoretical perspectives on how to train models more effectively (for example by increasing their capacity but limiting expressivity) or which model architectures are most likely to excel at different tasks under resource constraints. Here we draw together recent results, identify gaps in understanding, propose directions for future research, and highlight implications for both algorithm designers and users of machine intelligence technologies.",1
"The use of artificial neural networks as models of chaotic dynamics has been rapidly expanding. Still, a theoretical understanding of how neural networks learn chaos is lacking. Here, we employ a geometric perspective to show that neural networks can efficiently model chaotic dynamics by becoming structurally chaotic themselves. We first confirm neural network's efficiency in emulating chaos by showing that a parsimonious neural network trained only on few data points can reconstruct strange attractors, extrapolate outside training data boundaries, and accurately predict local divergence rates. We then posit that the trained network's map comprises sequential geometric stretching, rotation, and compression operations. These geometric operations indicate topological mixing and chaos, explaining why neural networks are naturally suitable to emulate chaotic dynamics.",0
"This paper examines neural networks from a novel geometric perspective, arguing that they can be understood as chaotic maps. We show how commonly used activation functions such as ReLU lead to highly nonlinear transformations and sensitive dependence on initial conditions, hallmarks of chaos theory. By analogy with classical dynamical systems, we interpret weights and biases as control parameters and input values as phase space coordinates. With these geometric tools at hand, we derive several insights into network behavior: (1) We prove global convergence under mild assumptions and construct simple Lyapunov functions using only local properties of the landscape; (2) We provide new intuition behind optimization problems such as vanishing/exploding gradients by linking them to fixed points, periodic orbits, or strange attractors in parameter space; (3) We discuss applications including adversarial robustness against small perturbations, generative models with stochastic latent variables, and few-shot learning through local linearity near sharp minima. Our results lay the foundation for further studies connecting deep learning with other fields like geometry, topology, or ergodic theory.",1
"We study algebraic neural networks (AlgNNs) with commutative algebras which unify diverse architectures such as Euclidean convolutional neural networks, graph neural networks, and group neural networks under the umbrella of algebraic signal processing. An AlgNN is a stacked layered information processing structure where each layer is conformed by an algebra, a vector space and a homomorphism between the algebra and the space of endomorphisms of the vector space. Signals are modeled as elements of the vector space and are processed by convolutional filters that are defined as the images of the elements of the algebra under the action of the homomorphism. We analyze stability of algebraic filters and AlgNNs to deformations of the homomorphism and derive conditions on filters that lead to Lipschitz stable operators. We conclude that stable algebraic filters have frequency responses -- defined as eigenvalue domain representations -- whose derivative is inversely proportional to the frequency -- defined as eigenvalue magnitudes. It follows that for a given level of discriminability, AlgNNs are more stable than algebraic filters, thereby explaining their better empirical performance. This same phenomenon has been proven for Euclidean convolutional neural networks and graph neural networks. Our analysis shows that this is a deep algebraic property shared by a number of architectures.",0
"This work examines the stability of algebraic neural networks under deformations, presenting new theoretical insights and numerical results on their robustness properties. We analyze how small perturbations affect network parameters and illustrate potential instabilities that may arise during training and inference. Our findings suggest practical implications for selecting architectures, initializing weights, and optimizing hyperparameters towards more stable models.",1
"For steel product manufacturing in indoor factories, steel defect detection is important for quality control. For example, a steel sheet is extremely delicate, and must be accurately inspected. However, to maintain the painted steel parts of the infrastructure around a severe outdoor environment, corrosion detection is critical for predictive maintenance. In this paper, we propose a general-purpose application for steel anomaly detection that consists of the following four components. The first, a learner, is a unit image classification network to determine whether the region of interest or background has been recognised, after dividing the original large sized image into 256 square unit images. The second, an extractor, is a discriminator feature encoder based on a pre-trained steel generator with a patch generative adversarial network discriminator(GAN). The third, an anomaly detector, is a one-class support vector machine(SVM) to predict the anomaly score using the discriminator feature. The fourth, an indicator, is an anomalous probability map used to visually explain the anomalous features. Furthermore, we demonstrated our method through the inspection of steel sheet defects with 13,774 unit images using high-speed cameras, and painted steel corrosion with 19,766 unit images based on an eye inspection of the photographs. Finally, we visualise anomalous feature maps of steel using a strip and painted steel inspection dataset",0
"""This paper presents a novel approach for visualizing anomalous features using a one-class steel detector with a patch generator adversarial network (GAN) discriminator. The proposed method utilizes a pre-trained convolutional neural network (CNN) as the generator and fine-tunes it along with the discriminator to create a unique feature map that highlights areas with unusual characteristics. By leveraging the power of CNNs and GANs, our system can effectively detect and visualize previously unknown anomalies in real-world datasets. Experimental results demonstrate the effectiveness of our method compared to traditional techniques, achieving higher accuracy and robustness.""",1
"Two of the most prominent algorithms for solving unconstrained smooth games are the classical stochastic gradient descent-ascent (SGDA) and the recently introduced stochastic consensus optimization (SCO) (Mescheder et al., 2017). SGDA is known to converge to a stationary point for specific classes of games, but current convergence analyses require a bounded variance assumption. SCO is used successfully for solving large-scale adversarial problems, but its convergence guarantees are limited to its deterministic variant. In this work, we introduce the expected co-coercivity condition, explain its benefits, and provide the first last-iterate convergence guarantees of SGDA and SCO under this condition for solving a class of stochastic variational inequality problems that are potentially non-monotone. We prove linear convergence of both methods to a neighborhood of the solution when they use constant step-size, and we propose insightful stepsize-switching rules to guarantee convergence to the exact solution. In addition, our convergence guarantees hold under the arbitrary sampling paradigm, and as such, we give insights into the complexity of minibatching.",0
"In this paper we present stochastic gradient descent ascent (SGDA) algorithms for solving smooth games. These methods operate on the space of probability distributions over players strategies rather than the distribution themselves, resulting in improved computational tractability while still retaining good statistical performance. We show that our SGDA algorithms have guaranteed convergence to approximate Nash equilibrium. Our theory builds upon the recent results on co-coercive games, which establishes a connection between the solution concept of approximate cooperative equilibria and Nash equilibria via variational inequality problems on linear spaces. Leveraging these newfound connections, we develop an algorithmic framework based on consensus optimization and derive concrete rate estimates. To demonstrate our methodologyâ€™s effectiveness, we conduct experiments using three different two-player zero sum matrix games with increasing complexity and size. Simulation results verify that SGDA achieves faster convergence and produces better solutions compared to state-of-the art competitors. Overall, this work showscase the power and potential of stochastic gradient methods for solving large scale games on the space of probability measures, suggesting promising future research directions towards more scalable game theoretic models.",1
"In this work, we revisit the theoretical properties of Hamiltonian stochastic differential equations (SDEs) for Bayesian posterior sampling, and we study the two types of errors that arise from numerical SDE simulation: the discretization error and the error due to noisy gradient estimates in the context of data subsampling. We consider overlooked results describing the ergodic convergence rates of numerical integration schemes, and we produce a novel analysis for the effect of mini-batches through the lens of differential operator splitting. In our analysis, the stochastic component of the proposed Hamiltonian SDE is decoupled from the gradient noise, for which we make no normality assumptions. This allows us to derive interesting connections among different sampling schemes, including the original Hamiltonian Monte Carlo (HMC) algorithm, and explain their performance. We show that for a careful selection of numerical integrators, both errors vanish at a rate $\mathcal{O}(\eta^2)$, where $\eta$ is the integrator step size. Our theoretical results are supported by an empirical study on a variety of regression and classification tasks for Bayesian neural networks.",0
"This should just give a general idea of what problem you solved, how you did so and why anyone would care",1
"Neural networks are susceptible to small transformations including 2D rotations and shifts, image crops, and even changes in object colors. This is often attributed to biases in the training dataset, and the lack of 2D shift-invariance due to not respecting the sampling theorem. In this paper, we challenge this hypothesis by training and testing on unbiased datasets, and showing that networks are brittle to both small 3D perspective changes and lighting variations which cannot be explained by dataset bias or lack of shift-invariance. To find these in-distribution errors, we introduce an evolution strategies (ES) based approach, which we call CMA-Search. Despite training with a large-scale (0.5 million images), unbiased dataset of camera and light variations, in over 71% cases CMA-Search can find camera parameters in the vicinity of a correctly classified image which lead to in-distribution misclassifications with  3.6% change in parameters. With lighting changes, CMA-Search finds misclassifications in 33% cases with  11.6% change in parameters. Finally, we extend this method to find misclassifications in the vicinity of ImageNet images for both ResNet and OpenAI's CLIP model.",0
"""This"" refers both to the paper as well as the use case I've requested you write about. If your response starts with this, please remove it so that I can approve it. Start directly discussing the content of the article instead!",1
"This paper addresses the problem of media retrieval using a multimodal query (a query which combines visual input with additional semantic information in natural language feedback). We propose a SynthTriplet GAN framework which resolves this task by expanding the multimodal query with a synthetically generated image that captures semantic information from both image and text input. We introduce a novel triplet mining method that uses a synthetic image as an anchor to directly optimize for embedding distances of generated and target images. We demonstrate that apart from the added value of retrieval illustration with synthetic image with the focus on customization and user feedback, the proposed method greatly surpasses other multimodal generation methods and achieves state of the art results in the multimodal retrieval task. We also show that in contrast to other retrieval methods, our method provides explainable embeddings.",0
"""This"" can refer either to the product in question that the customer wants or the search query they would like to refine:  Abstract: In recent years there has been growing interest in developing multi-modal retrieval systems capable of handling multiple types of input such as text, images and speech data. However, these systems often suffer from limited expressivity due to the lack of effective synthesis techniques which limit their applicability in real world scenarios. In our work we address these limitations by using innovative algorithms based on generative models coupled with traditional linear retrievers in order to retrieve more relevant results from large scale databases while also improving user experience through natural language interaction and feedback loops. We demonstrate how our approach outperforms state-of-the art baselines both quantitatively and qualitatively across several benchmark datasets. Our system allows users to interact with complex queries in intuitive ways thus opening up new possibilities for efficient human-AI collaboration in a wide range of applications including eCommerce, virtual shopping assistants and content generation tools among others.",1
"Many clustering algorithms are guided by certain cost functions such as the widely-used $k$-means cost. These algorithms divide data points into clusters with often complicated boundaries, creating difficulties in explaining the clustering decision. In a recent work, Dasgupta, Frost, Moshkovitz, and Rashtchian (ICML'20) introduced explainable clustering, where the cluster boundaries are axis-parallel hyperplanes and the clustering is obtained by applying a decision tree to the data. The central question here is: how much does the explainability constraint increase the value of the cost function?   Given $d$-dimensional data points, we show an efficient algorithm that finds an explainable clustering whose $k$-means cost is at most $k^{1 - 2/d}\mathrm{poly}(d\log k)$ times the minimum cost achievable by a clustering without the explainability constraint, assuming $k,d\ge 2$. Combining this with an independent work by Makarychev and Shan (ICML'21), we get an improved bound of $k^{1 - 2/d}\mathrm{polylog}(k)$, which we show is optimal for every choice of $k,d\ge 2$ up to a poly-logarithmic factor in $k$. For $d = 2$ in particular, we show an $O(\log k\log\log k)$ bound, improving exponentially over the previous best bound of $\widetilde O(k)$.",0
"Near-$k$-means (NKM) algorithms are efficient clustering methods that can handle high dimensional data by finding solutions near the subspace where clusters reside. As such, they have found many applications across various domains. However, their usage has been limited due to concerns over explainability and interpretability, which requires finding $k$ interpretable subspaces that capture the cluster structure. In our work we focus on developing explanations for the NKM algorithm itself rather than the underlying models it uses. Our approach involves two key steps: first, optimizing $\ell_{2}$ distances within each leaf node; second, searching over all possible $k$ values until the optimal value minimizing the sum of intra-cluster errors over leaves and inter-cluster distance between those $k$ subspaces. We evaluate our method using several experiments involving synthetic and real datasets, demonstrating significantly lower error rates compared to prior state-of-the-art approaches while maintaining good explainability. Keywords: k-means, near-$k$-means, subspace clustering, explainability, interpretability, optimization, machine learning, computer vision, signal processing -----",1
"This is a tutorial and survey paper on unification of spectral dimensionality reduction methods, kernel learning by Semidefinite Programming (SDP), Maximum Variance Unfolding (MVU) or Semidefinite Embedding (SDE), and its variants. We first explain how the spectral dimensionality reduction methods can be unified as kernel Principal Component Analysis (PCA) with different kernels. This unification can be interpreted as eigenfunction learning or representation of kernel in terms of distance matrix. Then, since the spectral methods are unified as kernel PCA, we say let us learn the best kernel for unfolding the manifold of data to its maximum variance. We first briefly introduce kernel learning by SDP for the transduction task. Then, we explain MVU in detail. Various versions of supervised MVU using nearest neighbors graph, by class-wise unfolding, by Fisher criterion, and by colored MVU are explained. We also explain out-of-sample extension of MVU using eigenfunctions and kernel mapping. Finally, we introduce other variants of MVU including action respecting embedding, relaxed MVU, and landmark MVU for big data.",0
"""Unifying three powerful techniques from machine learning, signal processing, and data science into one framework sounds like a daunting task, but that's exactly what we accomplish in this comprehensive tutorial and survey. Our approach uses semidefinite programming (SDP) to tackle spectral dimensionality reduction, maximum variance unfolding, and kernel learning - three key problems faced by researchers working on high-dimensional datasets.  The main challenge lies in finding efficient ways to reduce dimensions while preserving critical features that drive clustering structure. To overcome these obstacles, our method introduces two new algorithms, SPCA+Learning and SVCA++, which combine traditional approaches with deep learning methods. Experimental results demonstrate significant improvements over existing state-of-the-art techniques across several benchmark data sets.  While many applications require low-rank representations of large, complex matrices, other scenarios demand linearly independent transformations that maximize variance. We address this need by providing an SDP formulation of maximum variance unfolding and developing novel algorithms based on gradient descent and Newton steps.  Finally, we show how to cast nonlinear kernel machines as semialgebraic programs and optimize them using interior point solvers. This enables us to solve challenging machine learning problems involving image recognition, speech analysis, natural language understanding, recommender systems, bioinformatics, neuroscience, and more.  Our work provides unprecedented flexibility in selecting the appropriate technique based on problem requirements while guaranteeing global optimality under certain conditions. As such, it has broad appeal for researchers at different levels of expertise who want to master this multidisciplinary field.""  Note that I couldn't use any specific terms related to technical aspects because you didn't provide context about your paper's focus area. So please review my summary above and revise accordingly if necessary.",1
"There has been much progress in data-driven artificial intelligence technology for medical image analysis in the last decades. However, it still remains challenging due to its distinctive complexity of acquiring and annotating image data, extracting medical domain knowledge, and explaining the diagnostic decision for medical image analysis. In this paper, we propose a data-knowledge-driven framework termed as Parallel Medical Imaging (PMI) for intelligent medical image analysis based on the methodology of interactive ACP-based parallel intelligence. In the PMI framework, computational experiments with predictive learning in a data-driven way are conducted to extract medical knowledge for diagnostic decision support. Artificial imaging systems are introduced to select and prescriptively generate medical image data in a knowledge-driven way to utilize medical domain knowledge. Through the closed-loop optimization based on parallel execution, our proposed PMI framework can boost the generalization ability and alleviate the limitation of medical interpretation for diagnostic decisions. Furthermore, we illustrate the preliminary implementation of PMI method through the case studies of mammogram analysis and skin lesion image analysis. Experimental results on several public medical image datasets demonstrate the effectiveness of proposed PMI.",0
"This paper presents an overview of parallel medical imaging technologies for intelligent image analysis applications such as disease detection and diagnosis. The authors provide an introduction to the conceptual framework behind these systems, including discussions on parallel processing methods and architectures that enable efficient analysis of large amounts of medical images. They then describe several specific techniques used in current parallel medical imaging approaches, highlighting their strengths and limitations. Finally, the authors examine how these parallelized algorithms can be applied to real-world scenarios in medicine, exploring the potential benefits they offer. Overall, this paper serves as a comprehensive resource for understanding modern trends in medical image analysis using parallel computing methodologies.",1
"Meta learning generalizes the empirical experience with different learning tasks and holds promise for providing important empirical insight into the behaviour of machine learning algorithms. In this paper, we present a comprehensive meta-learning study of data sets and methods for multi-label classification (MLC). MLC is a practically relevant machine learning task where each example is labelled with multiple labels simultaneously. Here, we analyze 40 MLC data sets by using 50 meta features describing different properties of the data. The main findings of this study are as follows. First, the most prominent meta features that describe the space of MLC data sets are the ones assessing different aspects of the label space. Second, the meta models show that the most important meta features describe the label space, and, the meta features describing the relationships among the labels tend to occur a bit more often than the meta features describing the distributions between and within the individual labels. Third, the optimization of the hyperparameters can improve the predictive performance, however, quite often the extent of the improvements does not always justify the resource utilization.",0
"This study investigates how data set properties influence the performance of multi-label classification methods on the benchmark dataset used by the National Institute of Standards and Technology (NIST). We evaluate several well-known methods including random k-fold cross validation, k-nearest neighbors classifier, support vector machines with linear kernel, decision trees, random forest, gradient boosting machine, and artificial neural networks. Our analysis shows that some method perform better than others depending on data set characteristics such as number of labels per instance, label distribution, feature correlations, and data balance. Additionally, we observe that these differences can often be explained by specific aspects of each approach's design and behavior, which suggests potential strategies for selecting appropriate methods based on data set characteristics. Overall, our findings provide valuable insights into understanding the performance tradeoffs associated with different methods applied to real world problems in image and text categorization domains. They highlight key areas where future research may improve performance through advanced model designs tailored to particular problem types.",1
"Learning efficiently a causal model of the environment is a key challenge of model-based RL agents operating in POMDPs. We consider here a scenario where the learning agent has the ability to collect online experiences through direct interactions with the environment (interventional data), but has also access to a large collection of offline experiences, obtained by observing another agent interacting with the environment (observational data). A key ingredient, that makes this situation non-trivial, is that we allow the observed agent to interact with the environment based on hidden information, which is not observed by the learning agent. We then ask the following questions: can the online and offline experiences be safely combined for learning a causal model ? And can we expect the offline experiences to improve the agent's performances ? To answer these questions, we import ideas from the well-established causal framework of do-calculus, and we express model-based reinforcement learning as a causal inference problem. Then, we propose a general yet simple methodology for leveraging offline data during learning. In a nutshell, the method relies on learning a latent-based causal transition model that explains both the interventional and observational regimes, and then using the recovered latent variable to infer the standard POMDP transition model via deconfounding. We prove our method is correct and efficient in the sense that it attains better generalization guarantees due to the offline data (in the asymptotic case), and we illustrate its effectiveness empirically on synthetic toy problems. Our contribution aims at bridging the gap between the fields of reinforcement learning and causality.",0
"In recent years, there has been significant interest in developing reinforcement learning algorithms that can learn from both observational data and intervention data. This type of algorithm, known as causal reinforcement learning (CRL), allows agents to make more informed decisions by taking into account both the consequences of their actions and the underlying causal relationships between variables. In this paper, we propose a new method for CRL based on combining observational and interventional data. Our approach builds upon recent advances in structural causal modeling, which provides a formal framework for identifying causal relationships from complex datasets. We demonstrate the effectiveness of our method through experiments on several benchmark domains, showing that it outperforms existing methods for CRL under both observational and interventional settings. Our results highlight the potential of integrating observational and interventional data for improving decision making in complex environments.",1
"Determinantal Point Process (DPPs) are statistical models for repulsive point patterns. Both sampling and inference are tractable for DPPs, a rare feature among models with negative dependence that explains their popularity in machine learning and spatial statistics. Parametric and nonparametric inference methods have been proposed in the finite case, i.e. when the point patterns live in a finite ground set. In the continuous case, only parametric methods have been investigated, while nonparametric maximum likelihood for DPPs -- an optimization problem over trace-class operators -- has remained an open question. In this paper, we show that a restricted version of this maximum likelihood (MLE) problem falls within the scope of a recent representer theorem for nonnegative functions in an RKHS. This leads to a finite-dimensional problem, with strong statistical ties to the original MLE. Moreover, we propose, analyze, and demonstrate a fixed point algorithm to solve this finite-dimensional problem. Finally, we also provide a controlled estimate of the correlation kernel of the DPP, thus providing more interpretability.",0
"Here is a possible abstract for the paper:  This paper presents nonparametric estimation techniques using kernel methods for discrete probability distributions known as Discrete Point Processes (DPP). Specifically, we focus on the problem of estimating continuous DPPs, which have received less attention than their discrete counterparts. We propose two different approaches based on kernel density estimates and kernel mean embeddings respectively. Our methodologies rely on data resampling to improve accuracy, while considering theoretical properties such as consistency and convergence rates. Simulation studies demonstrate that our proposed methods outperform existing ones in terms of both bias reduction and computational efficiency, particularly at low sample sizes and high feature dimensions. Lastly, we apply these techniques to real datasets from computer vision and natural language processing to further substantiate their effectiveness.",1
"Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises.   The book is structured as follows. The first four chapters are on IID rewards, from the basic model to impossibility results to Bayesian priors to Lipschitz rewards. The next three chapters cover adversarial rewards, from the full-feedback version to adversarial bandits to extensions with linear rewards and combinatorially structured actions. Chapter 8 is on contextual bandits, a middle ground between IID and adversarial bandits in which the change in reward distributions is completely explained by observable contexts. The last three chapters cover connections to economics, from learning in repeated games to bandits with supply/budget constraints to exploration in the presence of incentives. The appendix provides sufficient background on concentration and KL-divergence.   The chapters on ""bandits with similarity information"", ""bandits with knapsacks"" and ""bandits and agents"" can also be consumed as standalone surveys on the respective topics.",0
"In recent years, there has been growing interest in developing intelligent systems that can make sequential decisions based on realtime feedback from their environment. One popular framework for addressing this problem is multi-armed bandit (MAB) theory, which involves selecting arms (options) repeatedly over time with the goal of maximizing cumulative reward while minimizing regret due to suboptimal choices. MABs have found applications in many domains such as finance, healthcare, ad allocation, and online learning. This paper provides an introduction to MAB theory, including some of the key mathematical models used in the field, algorithms developed by researchers, and potential application areas for these methods. We conclude by discussing challenges faced by current state-of-the-art approaches and future directions for research in this area.",1
"In this paper, we leverage the properties of non-Euclidean Geometry to define the Geodesic distance (GD) on the space of statistical manifolds. The Geodesic distance is a real and intuitive similarity measure that is a good alternative to the purely statistical and extensively used Kullback-Leibler divergence (KLD). Despite the effectiveness of the GD, a closed-form does not exist for many manifolds, since the geodesic equations are hard to solve. This explains that the major studies have been content to use numerical approximations. Nevertheless, most of those do not take account of the manifold properties, which leads to a loss of information and thus to low performances. We propose an approximation of the Geodesic distance through a graph-based method. This latter permits to well represent the structure of the statistical manifold, and respects its geometrical properties. Our main aim is to compare the graph-based approximation to the state of the art approximations. Thus, the proposed approach is evaluated for two statistical manifolds, namely the Weibull manifold and the Gamma manifold, considering the Content-Based Texture Retrieval application on different databases.",0
"In recent years, there has been increasing interest in developing methods for deriving geodesic distances on statistical manifolds that can effectively capture the intrinsic geometry underlying data distributions. This is particularly important in applications such as multimedia information retrieval where efficient indexing and search algorithms rely heavily on accurate measures of similarity and dissimilarity. One promising approach is to use graph theory to define a neighborhood system around each point in the manifold, which enables the computation of geodesic distances using shortest path algorithms. In this paper, we present a new graph-based method for computing geodesic distances on statistical manifolds and demonstrate its effectiveness through experiments on several real-world datasets. Our results show that our approach significantly outperforms existing state-of-the-art techniques and provides reliable estimates of geodesic distance even in high dimensions. Additionally, we discuss potential extensions and future research directions aimed at further improving the accuracy and scalability of our method.",1
"Although recurrent neural networks (RNNs) are state-of-the-art in numerous sequential decision-making tasks, there has been little research on explaining their predictions. In this work, we present TimeSHAP, a model-agnostic recurrent explainer that builds upon KernelSHAP and extends it to the sequential domain. TimeSHAP computes feature-, timestep-, and cell-level attributions. As sequences may be arbitrarily long, we further propose a pruning method that is shown to dramatically decrease both its computational cost and the variance of its attributions. We use TimeSHAP to explain the predictions of a real-world bank account takeover fraud detection RNN model, and draw key insights from its explanations: i) the model identifies important features and events aligned with what fraud analysts consider cues for account takeover; ii) positive predicted sequences can be pruned to only 10% of the original length, as older events have residual attribution values; iii) the most recent input event of positive predictions only contributes on average to 41% of the model's score; iv) notably high attribution to client's age, suggesting a potential discriminatory reasoning, later confirmed as higher false positive rates for older clients.",0
"""Sequence perturbation analysis has emerged as an effective tool for understanding deep learning models, particularly those designed for time series data such as recurrent neural networks (RNNs). This study proposes TimeSHAP (Time Series SHapley Additive exPlanations), a novel framework that utilizes sequence permutations to interpret RNN predictions and explain their behavior at multiple levels. By breaking down the contribution of individual input values, TimeSHAP provides insights into how different parts of the input influence model predictions. Additionally, TimeSHAP offers new ways to visualize RNN predictions by using global sensitivity indices to identify influential subsequences in the input. Results from extensive experiments demonstrate the effectiveness and scalability of our approach on both synthetic datasets and real-world applications including language generation and epilepsy prediction.""",1
"Toward achieving robust and defensive neural networks, the robustness against the weight parameters perturbations, i.e., sharpness, attracts attention in recent years (Sun et al., 2020). However, sharpness is known to remain a critical issue, ""scale-sensitivity."" In this paper, we propose a novel sharpness measure, Minimum Sharpness. It is known that NNs have a specific scale transformation that constitutes equivalent classes where functional properties are completely identical, and at the same time, their sharpness could change unlimitedly. We define our sharpness through a minimization problem over the equivalent NNs being invariant to the scale transformation. We also develop an efficient and exact technique to make the sharpness tractable, which reduces the heavy computational costs involved with Hessian. In the experiment, we observed that our sharpness has a valid correlation with the generalization of NNs and runs with less computational cost than existing sharpness measures.",0
"Neural network models have become increasingly popular due to their ability to achieve state-of-the-art performance on many tasks. However, there remain challenges when it comes to training these models to be robust against variations in model parameters, input data, and other factors that can affect their behavior. In this work, we propose a new methodology called minimum sharpness, which allows us to train neural networks to be more resilient to such changes while still maintaining high accuracy. We demonstrate through experimentation that our approach leads to significant improvements in terms of both accuracy and stability under different conditions. This research has important implications for advancing the field of machine learning and improving the reliability of systems based on neural networks.",1
"Forecasting accuracy is reliant on the quality of available past data. Data disruptions can adversely affect the quality of the generated model (e.g. unexpected events such as out-of-stock products when forecasting demand). We address this problem by pastcasting: predicting how data should have been in the past to explain the future better. We propose Pastprop-LSTM, a data-centric backpropagation algorithm that assigns part of the responsibility for errors to the training data and changes it accordingly. We test three variants of Pastprop-LSTM on forecasting competition datasets, M4 and M5, plus the Numenta Anomaly Benchmark. Empirical evaluation indicates that the proposed method can improve forecasting accuracy, especially when the prediction errors of standard LSTM are high. It also demonstrates the potential of the algorithm on datasets containing anomalies.",0
"Improving the accuracy of predictive models is crucial in many fields such as finance, healthcare, and engineering. However, traditional recurrent neural network (RNN) architectures suffer from several limitations that affect their ability to make accurate predictions. In particular, they struggle with handling long term dependencies due to vanishing gradients and suffering from exposure bias because they don't have access to future contexts during training time. To address these issues, we propose a novel model called ""PastProp-RNN"" which leverages the power of future contexts to improve the accuracy of RNNs. Our approach consists of two main components: first, we use a variant of backpropagation through time to effectively propagate gradients over longer periods, allowing our model to capture more distant dependencies. Second, we introduce a new mechanism to expose the model to different futures during training time, making it less prone to exposure bias. We evaluate our approach on three benchmark datasets - synthetic ones used in prior work, a real-world dataset related to wind turbine generation prediction, and one derived from human motion data - demonstrating significant improvements over state-of-the art methods across all datasets. These results showcase the potential impact of using future contexts to enhance predictive capabilities. Overall, our findings pave the way towards building better RNN-based models capable of more accurately forecasting complex phenomena in diverse domains.",1
"As machine learning models are increasingly used in critical decision-making settings (e.g., healthcare, finance), there has been a growing emphasis on developing methods to explain model predictions. Such \textit{explanations} are used to understand and establish trust in models and are vital components in machine learning pipelines. Though explanations are a critical piece in these systems, there is little understanding about how they are vulnerable to manipulation by adversaries. In this paper, we discuss how two broad classes of explanations are vulnerable to manipulation. We demonstrate how adversaries can design biased models that manipulate model agnostic feature attribution methods (e.g., LIME \& SHAP) and counterfactual explanations that hill-climb during the counterfactual search (e.g., Wachter's Algorithm \& DiCE) into \textit{concealing} the model's biases. These vulnerabilities allow an adversary to deploy a biased model, yet explanations will not reveal this bias, thereby deceiving stakeholders into trusting the model. We evaluate the manipulations on real world data sets, including COMPAS and Communities \& Crime, and find explanations can be manipulated in practice.",0
"""Examining the impact of feature attributions on counterfactual explanations can provide valuable insights into how our perceptions shape our reasoning processes. This study aimed to investigate how certain manipulations could affect both feature attributions and counterfactual explanations, and whether these changes were attributed to external factors rather than personal traits. Our findings indicate that participants displayed different patterns of responses depending on the type of manipulation used. These results suggest that individuals may hold biases towards specific features, which can influence their evaluations of counterfactual situations. Furthermore, we found evidence of the hindsight bias effect, whereby participants tended to overestimate the probability of a particular outcome after learning the actual outcome had occurred. Overall, these findings have important implications for understanding cognitive mechanisms involved in causal inference.""",1
"Stochastic nested optimization, including stochastic compositional, min-max and bilevel optimization, is gaining popularity in many machine learning applications. While the three problems share the nested structure, existing works often treat them separately, and thus develop problem-specific algorithms and their analyses. Among various exciting developments, simple SGD-type updates (potentially on multiple variables) are still prevalent in solving this class of nested problems, but they are believed to have slower convergence rate compared to that of the non-nested problems. This paper unifies several SGD-type updates for stochastic nested problems into a single SGD approach that we term ALternating Stochastic gradient dEscenT (ALSET) method. By leveraging the hidden smoothness of the problem, this paper presents a tighter analysis of ALSET for stochastic nested problems. Under the new analysis, to achieve an $\epsilon$-stationary point of the nested problem, it requires ${\cal O}(\epsilon^{-2})$ samples. Under certain regularity conditions, applying our results to stochastic compositional, min-max and reinforcement learning problems either improves or matches the best-known sample complexity in the respective cases. Our results explain why simple SGD-type algorithms in stochastic nested problems all work very well in practice without the need for further modifications.",0
"This article presents a tighter analysis of alternating stochastic gradient method (ASGM) applied to solve stochastic nested problems. We provide nonasymptotic bounds on both the expected optimality gap and constraint violation of ASGM under different assumptions. Our results indicate that, without strong convexity, ASGM converges slower than some existing methods such as SGD and SVRG. However, ASGM has better finite-sample performance than these methods under weaker conditions and can achieve faster convergence rates with stronger convexity. Moreover, we show that properly setting the step size parameter of ASGM significantly affects its performance in terms of iteration complexity. By carefully choosing parameters based on our theoretical findings, we can develop more efficient algorithms for solving large-scale stochastic optimization problems. Overall, our study provides important insights into the design and implementation of ASGM for solving complex nested problems.",1
"We propose a theoretical approach towards the training numerical stability of Variational AutoEncoders (VAE). Our work is motivated by recent studies empowering VAEs to reach state of the art generative results on complex image datasets. These very deep VAE architectures, as well as VAEs using more complex output distributions, highlight a tendency to haphazardly produce high training gradients as well as NaN losses. The empirical fixes proposed to train them despite their limitations are neither fully theoretically grounded nor generally sufficient in practice. Building on this, we localize the source of the problem at the interface between the model's neural networks and their output probabilistic distributions. We explain a common source of instability stemming from an incautious formulation of the encoded Normal distribution's variance, and apply the same approach on other, less obvious sources. We show that by implementing small changes to the way we parameterize the Normal distributions on which they rely, VAEs can securely be trained.",0
"This paper presents a novel approach for re-parameterizing Variational Autoencoders (VAEs) that significantly improves their stability during training. We show how the use of standard normal distributions as prior distributions can lead to unstable optimization landscapes and poor convergence rates, particularly when dealing with high-dimensional data sets. To address these issues, we propose using stabilized log-normal priors instead, which have several advantages over traditional choices. Firstly, they provide better fits to real data distributions, resulting in improved performance on downstream tasks such as image generation and denoising. Secondly, they exhibit more stable gradients during training, allowing for faster convergence and easier optimization compared to standard normal priors. Finally, our method requires no additional hyperparameters beyond those already used by typical VAE models, making it simple to implement and apply to any application. Our experimental results demonstrate consistent improvements across multiple benchmark datasets and architectures, confirming the effectiveness of our proposal for enhancing the reliability and efficiency of VAE models.",1
"Graph neural network (GNN) explanations have largely been facilitated through post-hoc introspection. While this has been deemed successful, many post-hoc explanation methods have been shown to fail in capturing a model's learned representation. Due to this problem, it is worthwhile to consider how one might train a model so that it is more amenable to post-hoc analysis. Given the success of adversarial training in the computer vision domain to train models with more reliable representations, we propose a similar training paradigm for GNNs and analyze the respective impact on a model's explanations. In instances without ground truth labels, we also determine how well an explanation method is utilizing a model's learned representation through a new metric and demonstrate adversarial training can help better extract domain-relevant insights in chemistry.",0
"This paper presents a novel approach to generating reliable explanations in graph neural networks (GNNs) through adversarial training. GNNs have gained popularity due to their ability to handle complex graphs and network data; however, understanding how they arrive at their predictions remains challenging. To address this issue, we propose using adversarial examples as regularization tools during training. By doing so, our method can improve model robustness against input perturbation attacks while simultaneously producing more interpretable results. Our experimental evaluations demonstrate that our approach leads to significant improvements over existing methods in terms of accuracy, interpretability, and robustness. Our findings offer valuable insights into the design and deployment of reliable GNNs for real-world applications.",1
"In situations where explanations of black-box models may be useful, the fairness of the black-box is also often a relevant concern. However, the link between the fairness of the black-box model and the behavior of explanations for the black-box is unclear. We focus on explanations applied to tabular datasets, suggesting that explanations do not necessarily preserve the fairness properties of the black-box algorithm. In other words, explanation algorithms can ignore or obscure critical relevant properties, creating incorrect or misleading explanations. More broadly, we propose future research directions for evaluating and generating explanations such that they are informative and relevant from a fairness perspective.",0
"Explanations play a critical role in many artificial intelligence (AI) systems, particularly those that make decisions affecting individuals' lives. However, these explanations often reveal unfair biases in their training data, which can lead to harmful outcomes for certain groups. Therefore, generating explanations that preserve fairness while still accurately reflecting how AI models work presents significant challenges. In this paper, we propose a framework for developing explanations that satisfy both criteria. Our approach begins by identifying sensitive attributes in the model's input space and measuring disparate impact on different subgroups. Then, using state-of-the-art techniques from computer science literature, we develop novel methods for creating explanations that minimize bias while maximizing transparency. Finally, through extensive empirical evaluation, we demonstrate our method effectively balances explainability and fairness, making it applicable across various applications where unbiased decision-making is essential. Overall, this research advances the field towards building more inclusive, equitable AI solutions.",1
"The phenomenon of adversarial examples in deep learning models has caused substantial concern over their reliability. While many deep neural networks have shown impressive performance in terms of predictive accuracy, it has been shown that in many instances an imperceptible perturbation can falsely flip the network's prediction. Most research has then focused on developing defenses against adversarial attacks or learning under a worst-case adversarial loss. In this work, we take a step back and aim to provide a framework for determining whether a model's label change under small perturbation is justified (and when it is not). We carefully argue that adversarial robustness should be defined as a locally adaptive measure complying with the underlying distribution. We then suggest a definition for an adaptive robust loss, derive an empirical version of it, and develop a resulting data-augmentation framework. We prove that our adaptive data-augmentation maintains consistency of 1-nearest neighbor classification under deterministic labels and provide illustrative empirical evaluations.",0
"This sounds like an interesting topic! Here is an example of how I would write an abstract: Adversarial examples have become a major concern for machine learning models, as they demonstrate vulnerabilities that can be exploited by malicious actors. These inputs are carefully crafted to trick machines into making incorrect predictions while appearing innocuous to humans. While there has been significant research on detecting and mitigating adversarial examples, their avoidance remains a challenge. In this paper, we explore the limitations of current approaches and argue that true avoidance may be impossible without compromising model accuracy. We propose new strategies based on robustness training and distribution alignment that offer improved resilience against adversaries without sacrificing performance. Our experiments validate the effectiveness of our methods across multiple datasets and architectures. Overall, our work sheds light on the nature of adversarial examples and provides actionable guidance for addressing them in real-world applications.",1
"Machine learning models that incorporate concept learning as an intermediate step in their decision making process can match the performance of black-box predictive models while retaining the ability to explain outcomes in human understandable terms. However, we demonstrate that the concept representations learned by these models encode information beyond the pre-defined concepts, and that natural mitigation strategies do not fully work, rendering the interpretation of the downstream prediction misleading. We describe the mechanism underlying the information leakage and suggest recourse for mitigating its effects.",0
"Artificial intelligence (AI) has been used increasingly over recent decades to solve a variety of problems ranging from computer vision to natural language processing to decision making. While there have been some outstanding successes, many researchers argue that true artificial general intelligence remains elusive due to numerous pitfalls facing current models. One promising approach towards achieving generality is through black-box concept learning, which involves training deep neural networks on large amounts of data to generate high-level concepts without relying on explicit prior knowledge or supervision. This study examines the promises and pitfalls associated with such approaches, highlighting their potential as well as limitations. We find that while these models can achieve impressive results in tasks where labeled data is available for fine tuning, they struggle significantly under more challenging conditions. We further explore sources of failure by analyzing model performance across multiple domains and identifying common patterns indicating areas where improvement is necessary. Overall, our work provides new insights into ways we might better design algorithms capable of handling complex real world scenarios beyond the realm of simply generating descriptions based on input data alone.",1
"Deep Neural Networks (DNNs) are known to be strong predictors, but their prediction strategies can rarely be understood. With recent advances in Explainable Artificial Intelligence, approaches are available to explore the reasoning behind those complex models' predictions. One class of approaches are post-hoc attribution methods, among which Layer-wise Relevance Propagation (LRP) shows high performance. However, the attempt at understanding a DNN's reasoning often stops at the attributions obtained for individual samples in input space, leaving the potential for deeper quantitative analyses untouched. As a manual analysis without the right tools is often unnecessarily labor intensive, we introduce three software packages targeted at scientists to explore model reasoning using attribution approaches and beyond: (1) Zennit - a highly customizable and intuitive attribution framework implementing LRP and related approaches in PyTorch, (2) CoRelAy - a framework to easily and quickly construct quantitative analysis pipelines for dataset-wide analyses of explanations, and (3) ViRelAy - a web-application to interactively explore data, attributions, and analysis results.",0
"Title: ""Software for Dataset-wide XAI: From Local Explanations to Global Insights with Zennit, CoRelAy, and ViRelAy"" Authors: James Moeller, Timothy Degray, Jeremy Patterson, Christopher Palmer, Tao Wang, Yuhong Guo, Long Cai, Mark Girolami This article presents three software packages that can be used to perform dataset-wide explanations (XAI) using powerful techniques such as causality identification and attribution analysis to provide insights into complex data sets at both local and global levels. These tools are able to assist users by providing a range of options for exploring and understanding their datasets, from identifying specific relationships within individual records up to identifying high level patterns across entire collections. By offering detailed, actionable feedback on how to improve models and analyses, these open source libraries can help researchers achieve better results faster than ever before while remaining cost effective and scalable.",1
"Despite the remarkable performance, Deep Neural Networks (DNNs) behave as black-boxes hindering user trust in Artificial Intelligence (AI) systems. Research on opening black-box DNN can be broadly categorized into post-hoc methods and inherently interpretable DNNs. While many surveys have been conducted on post-hoc interpretation methods, little effort is devoted to inherently interpretable DNNs. This paper provides a review of existing methods to develop DNNs with intrinsic interpretability, with a focus on Convolutional Neural Networks (CNNs). The aim is to understand the current progress towards fully interpretable DNNs that can cater to different interpretation requirements. Finally, we identify gaps in current work and suggest potential research directions.",0
"Recent advancements in deep neural networks (DNNs) have significantly improved their performance on complex tasks such as image classification, speech recognition, and natural language processing. However, these models often suffer from a lack of interpretability, making it difficult for users to understand how they make predictions and decisions. In the past few years, researchers have proposed several methods to address this issue by creating interpretable DNNs that can explain their decision process in human-readable form. These approaches range from feature visualization techniques, attention mechanisms, and model explanation frameworks to knowledge distillation and transfer learning methods. Despite significant progress, there remains a gap between current state-of-the-art solutions and fully interpretable DNNs. This paper provides a comprehensive survey of recent developments towards achieving fully interpretable DNNs and discusses open challenges and future directions in this field.",1
"Shapley values provide model agnostic feature attributions for model outcome at a particular instance by simulating feature absence under a global population distribution. The use of a global population can lead to potentially misleading results when local model behaviour is of interest. Hence we consider the formulation of neighbourhood reference distributions that improve the local interpretability of Shapley values. By doing so, we find that the Nadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as a self-normalised importance sampling estimator. Empirically, we observe that Neighbourhood Shapley values identify meaningful sparse feature relevance attributions that provide insight into local model behaviour, complimenting conventional Shapley analysis. They also increase on-manifold explainability and robustness to the construction of adversarial classifiers.",0
"Annotate the following model so that it conforms to OpenAI guidelines. Model: I am ready to write an abstract around 150 to 300 words long for a paper titled ""On Locality of Local Explanation Models"". Please tell me how to annotate it according to your guidelines. Here is the abstract as it stands currently: This paper presents an analysis of locality constraints in Local Explanation Models (LEMs). In particular, we focus on understanding the effectiveness of applying these constraints in different domains, specifically contrasting them against globally defined explanations. We provide an experimental evaluation demonstrating the impact of varying levels of locality on explainability performance. Our results indicate that while global methods can indeed achieve high explainability under certain conditions, LEM approaches may offer distinct advantages over their more general counterparts, including reduced complexity and improved efficiency. Finally, we present concrete recommendations for practitioners regarding the choice between locally constrained versus globally formulated explanations depending on specific use cases.",1
"In recent years, there has been a rapidly expanding focus on explaining the predictions made by black-box AI systems that handle image and tabular data. However, considerably less attention has been paid to explaining the predictions of opaque AI systems handling time series data. In this paper, we advance a novel model-agnostic, case-based technique -- Native Guide -- that generates counterfactual explanations for time series classifiers. Given a query time series, $T_{q}$, for which a black-box classification system predicts class, $c$, a counterfactual time series explanation shows how $T_{q}$ could change, such that the system predicts an alternative class, $c'$. The proposed instance-based technique adapts existing counterfactual instances in the case-base by highlighting and modifying discriminative areas of the time series that underlie the classification. Quantitative and qualitative results from two comparative experiments indicate that Native Guide generates plausible, proximal, sparse and diverse explanations that are better than those produced by key benchmark counterfactual methods.",0
"In recent years, counterfactual explanations have become increasingly important in the field of machine learning, as they allow us to better understand how models make predictions and enable developers to debug and improve their systems. However, many existing methods for generating counterfactuals are limited to tabular data or simple sequence data, such as text. In this work, we propose a new approach for generating instance-based counterfactual explanations for time series classification problems. Our method works by finding similar instances from a reference dataset that approximate the counterfactual trajectory desired by the user. These approximations are then used to generate synthetic time series sequences that can serve as plausible counterfactual explanations for the original prediction. We evaluate our method on several benchmark datasets and show that it outperforms other state-of-the-art approaches in terms of both accuracy and visual fidelity of generated counterfactuals. This research has important implications for improving the transparency and accountability of machine learning systems, particularly those used for decision making in critical applications such as healthcare or finance.",1
"A particular class of Explainable AI (XAI) methods provide saliency maps to highlight part of the image a Convolutional Neural Network (CNN) model looks at to classify the image as a way to explain its working. These methods provide an intuitive way for users to understand predictions made by CNNs. Other than quantitative computational tests, the vast majority of evidence to highlight that the methods are valuable is anecdotal. Given that humans would be the end-users of such methods, we devise three human subject experiments through which we gauge the effectiveness of these saliency-based explainability methods.",0
"Title: ""Saliency maps: What do they tell us?"" Authors: John Smith (1), Jane Doe (2), William Jones (3) (1. Name, Institution, City State Country; 2.Name, Institution, City State Country; 3.Name, Institution, City State Country). Corresponding Author: john.smith@example.com. Abstract: This study aimed to evaluate saliency mapping methods as a means of explaining deep learning modelsâ€™ decisions. We conducted experiments using the CelebA dataset, focusing on facial features that were highlighted by the model, and comparing them against ground truth data manually labeled by experts. Our results showed significant agreement between the two sets of labels, indicating that saliency maps can provide meaningful insights into how deep neural networks make predictions. Additionally, we found that feature attention can improve prediction accuracy over traditional single-crop baseline models, especially when there is noise or variability present in the input images. These findings demonstrate the utility of saliency maps as a tool for interpreting black box machine learning systems. Keywords: saliency map, interpretability, deep learning, CelebA dataset, explainable artificial intelligence (Explainable AI) Notes: Please use double spacing throughout the paper. Use American English spelling (eg center instead of centre) unless there is a specific reason to use British English spellings such as publicising. If you wish to add equations or mathematical symbols, please ensure they are formatted correctly using LaTeX syntax which can be pasted directly into the text at appropriate locations.  BibTeX format references should follow ISO690 guidelines. You may submit your own BibTeX file along with your submission if desired. Include DOIs where possible, but missing ones are fine, however please check and correct any incorrect URLs from doi links to actual documents available online! Additional supplementary material to accompany papers must be supplied upon request from authors only if required and after acceptance via email to the editorial team."" ====================================== ==========================",1
"Understanding and explaining the mistakes made by trained models is critical to many machine learning objectives, such as improving robustness, addressing concept drift, and mitigating biases. However, this is often an ad hoc process that involves manually looking at the model's mistakes on many test samples and guessing at the underlying reasons for those incorrect predictions. In this paper, we propose a systematic approach, conceptual explanation scores (CES), that explains why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). We base CES on two prior ideas: counterfactual explanations and concept activation vectors, and validate our approach on well-known pretrained models, showing that it explains the models' mistakes meaningfully. We also train new models with intentional and known spurious correlations, which CES successfully identifies from a single misclassified test sample. The code for CES is publicly available and can easily be applied to new models.",0
"This research aims to address a critical issue in machine learning: understanding why models make mistakes. Despite their high accuracy on test datasets, many models can still produce unexpected results or fail to generalize well in real-world scenarios. In order to improve model performance and trustworthiness, it is crucial to explain these errors in meaningful ways that allow humans to diagnose and fix them. Our approach combines several methods from different fields including psychology, cognitive science, and machine learning, to provide interpretable explanations for model predictions and mistakes. We evaluate our method on multiple benchmark datasets and show promising results in improving human comprehension of complex models without sacrificing predictive power. By providing insights into how models learn and reason, we hope to encourage more transparent and collaborative interactions between humans and artificial intelligence systems, ultimately leading to better decision making and problem solving across diverse domains.",1
"Explaining the decisions of an Artificial Intelligence (AI) model is increasingly critical in many real-world, high-stake applications. Hundreds of papers have either proposed new feature attribution methods, discussed or harnessed these tools in their work. However, despite humans being the target end-users, most attribution methods were only evaluated on proxy automatic-evaluation metrics. In this paper, we conduct the first, large-scale user study on 320 lay and 11 expert users to shed light on the effectiveness of state-of-the-art attribution methods in assisting humans in ImageNet classification, Stanford Dogs fine-grained classification, and these two tasks but when the input image contains adversarial perturbations. We found that, in overall, feature attribution is surprisingly not more effective than showing humans nearest training-set examples. On a hard task of fine-grained dog categorization, presenting attribution maps to humans does not help, but instead hurts the performance of human-AI teams compared to AI alone. Importantly, we found automatic attribution-map evaluation measures to correlate poorly with the actual human-AI team performance. Our findings encourage the community to rigorously test their methods on the downstream human-in-the-loop applications and to rethink the existing evaluation metrics.",0
Title: The Relationship Between Feature Attribution Methods and Automatic Evaluation Scores --------------------------------------------------------------... Source: <https://researchers.umassmed.edu/workspace/brianscott2/public_html/index.php?attachment=paper&id=49>,1
"Explainability techniques for Graph Neural Networks still have a long way to go compared to explanations available for both neural and decision decision tree-based models trained on tabular data. Using a task that straddles both graphs and tabular data, namely Entity Matching, we comment on key aspects of explainability that are missing in GNN model explanations.",0
"Graph Neural Networks (GNN) have emerged as powerful tools for modeling complex graph structures and their applications span diverse domains such as social networks, biological networks, and knowledge graphs. However, one major challenge that persists with these models is explaining how they make predictions. In particular, traditional methods used to explain GNNs lack transparency and interpretability, leading to mistrust among practitioners and users alike. To address this gap, we present a novel approach to generate explanations for GNNs by leveraging techniques from tabular data explanation literature. Our proposed method takes advantage of the underlying structure of many real-world graphs - where nodes often correspond to entities represented in tables. By exploiting this relationship, our algorithm generates concise yet informative explanations that highlight the most important features driving the prediction. We evaluate our method on several benchmark datasets and demonstrate its effectiveness through both quantitative measures and qualitative analysis of generated explanations. Overall, our work brings together insights from two distinct fields and offers a promising direction towards making GNNs more transparent and interpretable.",1
"Safe learning and optimization deals with learning and optimization problems that avoid, as much as possible, the evaluation of non-safe input points, which are solutions, policies, or strategies that cause an irrecoverable loss (e.g., breakage of a machine or equipment, or life threat). Although a comprehensive survey of safe reinforcement learning algorithms was published in 2015, a number of new algorithms have been proposed thereafter, and related works in active learning and in optimization were not considered. This paper reviews those algorithms from a number of domains including reinforcement learning, Gaussian process regression and classification, evolutionary algorithms, and active learning. We provide the fundamental concepts on which the reviewed algorithms are based and a characterization of the individual algorithms. We conclude by explaining how the algorithms are connected and suggestions for future research.",0
"Learning and optimization techniques have become increasingly important in recent years as they allow artificial intelligence (AI) systems to adaptively improve their performance based on data and feedback. However, traditional learning and optimization methods can suffer from safety issues such as instability, unpredictability, and brittleness. This paper presents a comprehensive survey of state-of-the-art safe learning and optimization techniques that address these challenges by incorporating robustness, stability, explainability, generality, and adaptivity into the learning process. We discuss several key aspects of safe learning including regularization, uncertainty estimation, model interpretability, and risk minimization. Additionally, we review cutting-edge approaches to optimization under different constraints, such as distributed computing, nonconvex problems, and adversarial settings. By highlighting the most promising research directions and open challenges, our survey serves as a roadmap for future advances in this rapidly evolving field.",1
"Binarized Neural Networks (BNNs) have the potential to revolutionize the way that deep learning is carried out in edge computing platforms. However, the effectiveness of interpretability methods on these networks has not been assessed.   In this paper, we compare the performance of several widely used saliency map-based interpretabilty techniques (Gradient, SmoothGrad and GradCAM), when applied to Binarized or Full Precision Neural Networks (FPNNs). We found that the basic Gradient method produces very similar-looking maps for both types of network. However, SmoothGrad produces significantly noisier maps for BNNs. GradCAM also produces saliency maps which differ between network types, with some of the BNNs having seemingly nonsensical explanations. We comment on possible reasons for these differences in explanations and present it as an example of why interpretability techniques should be tested on a wider range of network types.",0
"Artificial neural networks (ANN) have been successfully applied in many fields due to their ability to solve complex problems such as image recognition, natural language processing and recommendation systems. Despite the successes achieved by ANNs there has always been concerns regarding the interpretability of these models which make them less suitable for use cases where transparency is important or required such as finance or healthcare sectors . To mitigate the lack of interpretability recent research focused on developing methods that allow to gain insights into the decision making processes made by artificial neural networks in order to improve trustworthiness and safety of the decisions taken by them in real world applications . In our work we present a review of existing gradient based interpretation methods and show how they can be used together with a type of artificial neural network called binarized neural networks to increase the interpretability of decisions taken by artificial intelligence algorithms. We provide examples of application of different interpretation techniques for binarized neural networks trained for image classification task showing how the obtained explanations allow better understanding of the model reasoning process. Our findings indicate that adoption of proper interpretation technique allows increasing faith in the AI decision making process hence widening the range of possible applications of artificial intelligence . Finally we discuss open questions in the area of interpreting binary neural networks and propose several future directions for researchers interested in expanding the knowledge related to the topic . Our study contributes to current state o...",1
"Advances in neural architecture search, as well as explainability and interpretability of connectionist architectures, have been reported in the recent literature. However, our understanding of how to design Bayesian Deep Learning (BDL) hyperparameters, specifically, the depth, width and ensemble size, for robust function mapping with uncertainty quantification, is still emerging. This paper attempts to further our understanding by mapping Bayesian connectionist representations to polynomials of different orders with varying noise types and ratios. We examine the noise-contaminated polynomials to search for the combination of hyperparameters that can extract the underlying polynomial signals while quantifying uncertainties based on the noise attributes. Specifically, we attempt to study the question that an appropriate neural architecture and ensemble configuration can be found to detect a signal of any n-th order polynomial contaminated with noise having different distributions and signal-to-noise (SNR) ratios and varying noise attributes. Our results suggest the possible existence of an optimal network depth as well as an optimal number of ensembles for prediction skills and uncertainty quantification, respectively. However, optimality is not discernible for width, even though the performance gain reduces with increasing width at high values of width. Our experiments and insights can be directional to understand theoretical properties of BDL representations and to design practical solutions.",0
"This research presents a novel approach to hyperparameter optimization for deep learning models using Bayesian methods. We focus on the problem of function mapping to polynomials with noise, where the goal is to find the optimal polynomial coefficients that map a given input data set onto another data set while minimizing noise. Our proposed method combines Bayesian techniques with deep learning algorithms such as neural networks to efficiently search for optimal hyperparameters.  We begin by introducing our problem statement and motivation behind applying deep learning models for polynomial regression. Next, we describe our proposed methodology for Bayesian hyperparameter search, including how we formulate the likelihood functions and prior distributions. Then, we outline the details of our experiments, which involve evaluating different hyperparameter settings on synthetic and real datasets. Finally, we present our results and discuss their implications for improving the robustness and accuracy of function mappings with noise.  Our contributions can benefit researchers working in areas such as computer vision, signal processing, and machine learning who seek reliable solutions to problems involving mapping complex functions with noisy inputs. By demonstrating the effectiveness of Bayesian deep learning hyperparameter search for polynomial regression tasks, our work paves the way for further exploration into more challenging applications requiring high fidelity mapping under uncertainty. Overall, our study highlights the synergies between Bayesian inference and deep learning, offering promising directions for future research in artificial intelligence.",1
"Recent studies show a close connection between neural networks (NN) and kernel methods. However, most of these analyses (e.g., NTK) focus on the influence of (infinite) width instead of the depth of NN models. There remains a gap between theory and practical network designs that benefit from the depth. This paper first proposes a novel kernel family named Neural Optimization Kernel (NOK). Our kernel is defined as the inner product between two $T$-step updated functionals in RKHS w.r.t. a regularized optimization problem. Theoretically, we proved the monotonic descent property of our update rule for both convex and non-convex problems, and a $O(1/T)$ convergence rate of our updates for convex problems. Moreover, we propose a data-dependent structured approximation of our NOK, which builds the connection between training deep NNs and kernel methods associated with NOK. The resultant computational graph is a ResNet-type finite width NN. Our structured approximation preserved the monotonic descent property and $O(1/T)$ convergence rate. Namely, a $T$-layer NN performs $T$-step monotonic descent updates. Notably, we show our $T$-layered structured NN with ReLU maintains a $O(1/T)$ convergence rate w.r.t. a convex regularized problem, which explains the success of ReLU on training deep NN from a NN architecture optimization perspective. For the unsupervised learning and the shared parameter case, we show the equivalence of training structured NN with GD and performing functional gradient descent in RKHS associated with a fixed (data-dependent) NOK at an infinity-width regime. For finite NOKs, we prove generalization bounds. Remarkably, we show that overparameterized deep NN (NOK) can increase the expressive power to reduce empirical risk and reduce the generalization bound at the same time. Extensive experiments verify the robustness of our structured NOK blocks.",0
"An efficient optimization method based on the idea that deep learning models can learn complex functions robustly is introduced in ""Neural Optimization Kernel: Towards Robust Deep Learning."" To develop a model able to handle nonlinear relationships robustly, we use a feedforward neural network as our kernel function approximator. Our method requires only one additional parameter per layer compared to traditional kernels such as radial basis functions or polynomial kernels. Experimental evaluation confirms the efficiency of our proposed kernel for both regression and classification tasks. Implementation details, experimental results, analysis of learned functions, and future directions are discussed. This work represents an important step towards bridging the gap between human intelligence and machine intelligence by improving robustness in deep learning through novel architectures and training procedures. Please write more than just this. Thank you. In this study, the authors introduce the concept of neural optimization kernels (NOK) as a means of enhancing the robustness of deep learning models. They propose using feedforward neural networks (FNNs) as the foundation for these kernels, which would require only one additional parameter per layer over typical kernels like RBFs or polynomials. By doing so, they aimed to create a high performing deep learning framework capable of handling complex nonlinear relationships. Evaluating their approach via experimentation, NOK achieved favorable results across several task categories. In conclusion, the findings support the potential utility of NOK for advancing the state of artificial intelligence while closing the gaps with human cognition. Further research opportunities exist to refine NOK architecture or apply them in different domains where robust deep learning methods may prove advantageous.",1
"Deep neural networks that yield human interpretable decisions by architectural design have lately become an increasingly popular alternative to post hoc interpretation of traditional black-box models. Among these networks, the arguably most widespread approach is so-called prototype learning, where similarities to learned latent prototypes serve as the basis of classifying an unseen data point. In this work, we point to an important shortcoming of such approaches. Namely, there is a semantic gap between similarity in latent space and similarity in input space, which can corrupt interpretability. We design two experiments that exemplify this issue on the so-called ProtoPNet. Specifically, we find that this network's interpretability mechanism can be led astray by intentionally crafted or even JPEG compression artefacts, which can produce incomprehensible decisions. We argue that practitioners ought to have this shortcoming in mind when deploying prototype-based models in practice.",0
"In recent years, there has been increasing interest in understanding the behavior of deep neural networks (DNNs) and their decision making processes. One popular approach to achieving interpretability is through latent space prototype interpretation, which involves identifying specific patterns within DNN representations that correspond to classes or features of interest. However, despite its apparent simplicity, latent space prototype interpretation can suffer from significant shortcomings that undermine its effectiveness as a tool for interpreting deep networks. Here we examine some key limitations of this methodology: firstly, we show how prototypes identified using latent space methods may contain spurious characteristics or artifacts, rather than true indicators of class membership; secondly, we demonstrate how these prototypes can become overly simplified caricatures, unable to accurately represent complex interrelationships among different classes; finally, we discuss how certain design choices such as random initialization can further complicate attempts to uncover meaningful insights via latent space prototype analysis. Together, our findings reveal the potential dangers associated with relying exclusively on latent space approaches for interpreting DNNs. Instead, we suggest seeking alternative strategies for improving the transparency and trustworthiness of these powerful models.",1
"We address the problems of identifying malware in network telemetry logs and providing \emph{indicators of compromise} -- comprehensible explanations of behavioral patterns that identify the threat. In our system, an array of specialized detectors abstracts network-flow data into comprehensible \emph{network events} in a first step. We develop a neural network that processes this sequence of events and identifies specific threats, malware families and broad categories of malware. We then use the \emph{integrated-gradients} method to highlight events that jointly constitute the characteristic behavioral pattern of the threat. We compare network architectures based on CNNs, LSTMs, and transformers, and explore the efficacy of unsupervised pre-training experimentally on large-scale telemetry data. We demonstrate how this system detects njRAT and other malware based on behavioral patterns.",0
"Understanding malware behavior is crucial for detecting and mitigating cyber attacks. Existing approaches rely on rule-based systems or machine learning models that can be difficult to interpret and explain. In this paper, we propose a novel approach for learning explainable representations of malware behavior based on natural language processing techniques. Our method analyzes large amounts of data from security logs and generates natural language descriptions of malicious activities, allowing human analysts to easily understand and respond to potential threats. We evaluate our approach using real-world datasets and show that it outperforms state-of-the-art methods in terms of accuracy, interpretability, and efficiency. By providing insights into how attackers behave, our method helps security teams make informed decisions and improve their defensive strategies against emerging cyber threats. Overall, this work demonstrates the effectiveness of using explainable artificial intelligence to enhance the resilience of computer networks against cyber attacks.",1
"Attribution methods have been shown as promising approaches for identifying key features that led to learned model predictions. While most existing attribution methods rely on a baseline input for performing feature perturbations, limited research has been conducted to address the baseline selection issues. Poor choices of baselines limit the ability of one-vs-one (1-vs-1) explanations for multi-class classifiers, which means the attribution methods were not able to explain why an input belongs to its original class but not the other specified target class. 1-vs-1 explanation is crucial when certain classes are more similar than others, e.g. two bird types among multiple animals, by focusing on key differentiating features rather than shared features across classes. In this paper, we present GAN-based Model EXplainability (GANMEX), a novel approach applying Generative Adversarial Networks (GAN) by incorporating the to-be-explained classifier as part of the adversarial networks. Our approach effectively selects the counterfactual baseline as the closest realistic sample belong to the target class, which allows attribution methods to provide true 1-vs-1 explanations. We showed that GANMEX baselines improved the saliency maps and led to stronger performance on perturbation-based evaluation metrics over the existing baselines. Existing attribution results are known for being insensitive to model randomization, and we demonstrated that GANMEX baselines led to better outcome under the cascading randomization of the model.",0
"This sounds like an interesting research topic! Can you please provide more context on GANMEX? Additionally, can I ask what field or discipline this paper belongs to (e.g., computer science)? With that information, I would be able to write a better abstract.",1
"Applying artificial neural networks (ANN) to specific tasks, researchers, programmers, and other specialists usually overshot the number of convolutional layers in their designs. By implication, these ANNs hold too many parameters, which needed unnecessarily trained without impacting the result. The features, a convolutional layer can process, are strictly limited by its receptive field. By layer-wise analyzing the expansion of the receptive fields, we can reliably predict sequences of layers that will not contribute qualitatively to the inference in thegiven ANN architecture. Based on these analyses, we propose design strategies to resolve these inefficiencies, optimizing the explainability and the computational performance of ANNs. Since neither the strategies nor the analysis requires training of the actual model, these insights allow for a very efficient design process of ANNs architectures which might be automated in the future.",0
"Deep neural networks have shown great successes but need large datasets and computational resources. To optimize their performance, we analyze how deep convolutional neural networks (CNNs) scale using state-of-the-art computer vision benchmarks on image classification and object detection tasks. We demonstrate that common knowledge holds up: deeper CNN architectures perform better than shallower ones. Our analysis reveals two main conclusions. Firstly, there exists a linear relationship between network depth and the increase in performance. Secondly, while adding more layers increases complexity, receptive fields become wider, which leads to higher accuracy. Thus, we can use these relationships to guide design choices based solely on input size, task difficulty, hardware constraints, etc. This novel approach optimizes top modelsâ€™ training time and allows faster deployment of high quality systems without compromising accuracy. Additionally, our method enables utilization of smaller or less complex models in areas where they suffice. Our study provides insights into how data-hungry machine learning models could benefit from such theoretical considerations as well. Ultimately, the goal is to minimize waste in computations while maximizing efficiency and impact, all made possible through analytical research like this one presented here today.",1
"Shapley values has established itself as one of the most appropriate and theoretically sound frameworks for explaining predictions from complex machine learning models. The popularity of Shapley values in the explanation setting is probably due to its unique theoretical properties. The main drawback with Shapley values, however, is that its computational complexity grows exponentially in the number of input features, making it unfeasible in many real world situations where there could be hundreds or thousands of features. Furthermore, with many (dependent) features, presenting/visualizing and interpreting the computed Shapley values also becomes challenging. The present paper introduces groupShapley: a conceptually simple approach for dealing with the aforementioned bottlenecks. The idea is to group the features, for example by type or dependence, and then compute and present Shapley values for these groups instead of for all individual features. Reducing hundreds or thousands of features to half a dozen or so, makes precise computations practically feasible and the presentation and knowledge extraction greatly simplified. We prove that under certain conditions, groupShapley is equivalent to summing the feature-wise Shapley values within each feature group. Moreover, we provide a simulation study exemplifying the differences when these conditions are not met. We illustrate the usability of the approach in a real world car insurance example, where groupShapley is used to provide simple and intuitive explanations.",0
"GroupShapley: efficient explanation with Shapley values for feature groups explains how machine learning models can produce accurate predictions without being transparent. By calculating Shapley value explanations for individual features and grouping them into categories, we gain insight into which feature groups most influence model output. This methodology helps ensure fairness and accountability by providing clear explanations for why certain outcomes occur. Our approach has been proven effective through empirical evaluation on both synthetic and real world datasets. We believe our work is a significant step towards building more explainable models that better serve society as a whole.",1
"Convolutional networks are ubiquitous in deep learning. They are particularly useful for images, as they reduce the number of parameters, reduce training time, and increase accuracy. However, as a model of the brain they are seriously problematic, since they require weight sharing - something real neurons simply cannot do. Consequently, while neurons in the brain can be locally connected (one of the features of convolutional networks), they cannot be convolutional. Locally connected but non-convolutional networks, however, significantly underperform convolutional ones. This is troublesome for studies that use convolutional networks to explain activity in the visual system. Here we study plausible alternatives to weight sharing that aim at the same regularization principle, which is to make each neuron within a pool react similarly to identical inputs. The most natural way to do that is by showing the network multiple translations of the same image, akin to saccades in animal vision. However, this approach requires many translations, and doesn't remove the performance gap. We propose instead to add lateral connectivity to a locally connected network, and allow learning via Hebbian plasticity. This requires the network to pause occasionally for a sleep-like phase of ""weight sharing"". This method enables locally connected networks to achieve nearly convolutional performance on ImageNet, thus supporting convolutional networks as a model of the visual stream.",0
"Artificial neural networks have become increasingly popular in recent years due to their ability to model complex relationships within large datasets. However, traditional artificial neural networks often lack biological plausibility, as they operate on the basis of mathematical functions that are not found in nature. In contrast, convolutional networks, which are a type of deep learning algorithm inspired by the structure of biological neurons, have been shown to perform well across a wide range of tasks while still maintaining some degree of biological realism. This paper proposes a new approach towards developing even more biologically plausible convolutional networks using techniques such as spike timing dependent plasticity and homeostatic renormalization. These methods allow us to train networks that more closely mimic the behavior of natural neurons, resulting in improved performance and better generalizability. Our results demonstrate the effectiveness of these approaches and provide insights into how we can further improve upon current models. Ultimately, our work represents a step forward in understanding how biology can inform machine learning algorithms and vice versa.",1
"Recently, the study on learned iterative shrinkage thresholding algorithm (LISTA) has attracted increasing attentions. A large number of experiments as well as some theories have proved the high efficiency of LISTA for solving sparse coding problems. However, existing LISTA methods are all serial connection. To address this issue, we propose a novel extragradient based LISTA (ELISTA), which has a residual structure and theoretical guarantees. In particular, our algorithm can also provide the interpretability for Res-Net to a certain extent. From a theoretical perspective, we prove that our method attains linear convergence. In practice, extensive empirical results verify the advantages of our method.",0
"In recent years, sparse coding has emerged as a powerful technique for data compression, feature extraction, and machine learning. However, existing methods often face challenges such as slow convergence rates, poor stability, and difficulty in tuning hyperparameters. To address these issues, we propose a novel algorithm called Learned Interpretable Residual Extragradient Iterative Shrinkage/Thresholding Algorithm (Lire-ISTA). Lire-ISTA integrates interpretable residuals into the iterative shrinkage thresholding framework to improve accuracy, speed up convergence, enhance robustness against noise, and enable stable recovery under large variability of problem parameters. Our extensive experiments on benchmark datasets demonstrate that Lire-ISTA outperforms state-of-the-art algorithms in terms of both efficiency and effectiveness, validating our approach as a promising solution for sparse coding tasks.",1
"Approximate Bayesian inference for neural networks is considered a robust alternative to standard training, often providing good performance on out-of-distribution data. However, Bayesian neural networks (BNNs) with high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo achieve poor generalization under covariate shift, even underperforming classical estimation. We explain this surprising result, showing how a Bayesian model average can in fact be problematic under covariate shift, particularly in cases where linear dependencies in the input features cause a lack of posterior contraction. We additionally show why the same issue does not affect many approximate inference procedures, or classical maximum a-posteriori (MAP) training. Finally, we propose novel priors that improve the robustness of BNNs to many sources of covariate shift.",0
"Title: ""Dangers of Bayesian Model Averaging under Covariate Shift""  Abstract: Bayesian model averaging (BMA) has become a popular method for dealing with uncertainty in statistical models by combining multiple models into a single predictive distribution. While BMA can provide robust estimates and improve prediction accuracy, recent studies have highlighted potential pitfalls associated with using BMA under covariate shift. This article explores the dangers of applying BMA without accounting for changes in predictor variables that may occur over time or across different populations. Through simulations and case studies, we demonstrate how ignoring covariate shift can lead to biased results and suboptimal predictions, especially when there is little overlap between the training and testing distributions. Our findings emphasize the importance of incorporating strategies such as variable selection techniques or post-hoc adjustments to address shifts in covariates, ultimately improving the reliability of inference from BMA methods.",1
"In the environment of fair lending laws and the General Data Protection Regulation (GDPR), the ability to explain a model's prediction is of paramount importance. High quality explanations are the first step in assessing fairness. Counterfactuals are valuable tools for explainability. They provide actionable, comprehensible explanations for the individual who is subject to decisions made from the prediction. It is important to find a baseline for producing them. We propose a simple method for generating counterfactuals by using gradient descent to search in the latent space of an autoencoder and benchmark our method against approaches that search for counterfactuals in feature space. Additionally, we implement metrics to concretely evaluate the quality of the counterfactuals. We show that latent space counterfactual generation strikes a balance between the speed of basic feature gradient descent methods and the sparseness and authenticity of counterfactuals generated by more complex feature space oriented techniques.",0
"Title: ""Exploring the Potential of Latent Contrast Functions for Explaining Decisions Made by Machine Learning Models""  Abstract: In recent years, there has been growing interest in understanding the decisions made by machine learning models, particularly those used in high-stakes applications such as healthcare and finance. One approach that has gained popularity is counterfactual explanations, which involve identifying alternative scenarios where a different outcome could have occurred if certain factors had differed. However, developing effective methods for generating these explanations remains challenging, especially given the complex nature of many modern decision-making systems. To address this gap, we introduce Latent-CF, a novel baseline method based on latent contrast functions (LCF) that can efficiently generate plausible and informative counterfactuals. Our experiments demonstrate the effectiveness of Latent-CF across multiple domains, significantly outperforming competitive baselines while remaining simple to implement and interpret. These results highlight the potential of LCFs as a powerful tool for explaining black-box models and pave the way for future research into improving the transparency and accountability of artificial intelligence. Overall, our work offers valuable insights into the development of new approaches for enhancing the explainability of complex decision systems.",1
"Machine learning methods are widely used in the natural sciences to model and predict physical systems from observation data. Yet, they are often used as poorly understood ""black boxes,"" disregarding existing mathematical structure and invariants of the problem. Recently, the proposal of Hamiltonian Neural Networks (HNNs) took a first step towards a unified ""gray box"" approach, using physical insight to improve performance for Hamiltonian systems. In this paper, we explore a significantly improved training method for HNNs, exploiting the symplectic structure of Hamiltonian systems with a different loss function. This frees the loss from an artificial lower bound. We mathematically guarantee the existence of an exact Hamiltonian function which the HNN can learn. This allows us to prove and numerically analyze the errors made by HNNs which, in turn, renders them fully explainable. Finally, we present a novel post-training correction to obtain the true Hamiltonian only from discretized observation data, up to an arbitrary order.",0
"In recent years, neural networks have been used as models for physical systems due to their ability to capture complex nonlinear relationships between inputs and outputs. However, most existing methods use standard Euler integration which can lead to numerical instability and reduced accuracy over time. To address these issues, we propose symplectic learning for Hamiltonian neural networks (HNN). This approach utilizes geometric integrators such as Runge-Kutta discretization to better preserve phase space structures and improve the stability and accuracy of HNN simulations. We demonstrate through experiments on several benchmark problems that our method outperforms traditional Euler integration based approaches while achieving comparable computational efficiency. Our work shows promising results towards developing more accurate and stable neural network models for physical system modeling.",1
"Optimization in Deep Learning is mainly guided by vague intuitions and strong assumptions, with a limited understanding how and why these work in practice. To shed more light on this, our work provides some deeper understandings of how SGD behaves by empirically analyzing the trajectory taken by SGD from a line search perspective. Specifically, a costly quantitative analysis of the full-batch loss along SGD trajectories from common used models trained on a subset of CIFAR-10 is performed. Our core results include that the full-batch loss along lines in update step direction is highly parabolically. Further on, we show that there exists a learning rate with which SGD always performs almost exact line searches on the full-batch loss. Finally, we provide a different perspective why increasing the batch size has almost the same effect as decreasing the learning rate by the same factor.",0
"This paper presents a comprehensive analysis of stochastic gradient descent (SGD) from a line search perspective. The authors aim to provide a detailed explanation of how SGD works and why it is such an effective algorithm for training machine learning models. By examining SGD through the lens of line searching, the authors demonstrate that SGD is able to optimize objective functions effectively due to its ability to adaptively update step sizes based on the curvature of the function being optimized. Furthermore, the authors show that SGD is highly scalable and can handle large datasets by breaking down the optimization problem into smaller subproblems. Overall, the findings presented in this paper offer new insights into the inner workings of SGD and highlight its strengths as a powerful tool for machine learning.",1
"With the rapid growth of renewable energy, lots of small photovoltaic (PV) prosumers emerge. Due to the uncertainty of solar power generation, there is a need for aggregated prosumers to predict solar power generation and whether solar power generation will be larger than load. This paper presents two interpretable neural networks to solve the problem: one binary classification neural network and one regression neural network. The neural networks are built using TensorFlow. The global feature importance and local feature contributions are examined by three gradient-based methods: Integrated Gradients, Expected Gradients, and DeepLIFT. Moreover, we detect abnormal cases when predictions might fail by estimating the prediction uncertainty using Bayesian neural networks. Neural networks, which are interpreted by gradient-based methods and complemented with uncertainty estimation, provide robust and explainable forecasting for decision-makers.",0
"This paper examines the use of neural networks for predicting the output of renewable energy sources such as solar and wind power. The authors explore different approaches to interpreting the results of these models and how they can improve their accuracy. They compare traditional methods like regression analysis with more advanced techniques such as feature attributions and sensitivity analysis. The findings suggest that while black box solutions may work well at making predictions, there must be transparency into why those predictions were made so that stakeholders can make informed decisions. Overall, the study provides valuable insights on the tradeoff between model performance and model interpretability in the context of renewable energy forecasting.",1
"Confidence-aware learning is proven as an effective solution to prevent networks becoming overconfident. We present a confidence-aware camouflaged object detection framework using dynamic supervision to produce both accurate camouflage map and meaningful ""confidence"" representing model awareness about the current prediction. A camouflaged object detection network is designed to produce our camouflage prediction. Then, we concatenate it with the input image and feed it to the confidence estimation network to produce an one channel confidence map.We generate dynamic supervision for the confidence estimation network, representing the agreement of camouflage prediction with the ground truth camouflage map. With the produced confidence map, we introduce confidence-aware learning with the confidence map as guidance to pay more attention to the hard/low-confidence pixels in the loss function. We claim that, once trained, our confidence estimation network can evaluate pixel-wise accuracy of the prediction without relying on the ground truth camouflage map. Extensive results on four camouflaged object detection testing datasets illustrate the superior performance of the proposed model in explaining the camouflage prediction.",0
"In recent years, object detection has seen significant progress due to advancements in deep learning techniques such as convolutional neural networks (CNNs). However, detecting objects that are camouflaged against their background remains challenging, even with state-of-the-art models. To address this issue, we propose a novel method called confidence-aware learning for camouflaged object detection. Our approach leverages semantic segmentation masks generated by high-confidence predictions from a pre-trained object detector to improve localization accuracy. We train a separate classifier on these masks to predict whether an instance belongs to one of three categories: visible, ambiguous or camouflaged. We use this predicted label along with other features like scale, aspect ratio, shape complexity etc..to boost detection performance for the hard cases where objects are highly camouflaged. Experimental results show a consistent improvement in average precision across all classes over baseline methods on standard benchmark datasets like COCO dataset, PASCAL VOC2007/2012 dataset , Visdrone datasets . With our proposed method, we can achieve higher accuracy while still maintaining realtime inference speed. This research provides new insights into the problem of camouflage detection and paves the way for future work in this field.",1
"The author of this work proposes an overview of the recent semi-supervised learning approaches and related works. Despite the remarkable success of neural networks in various applications, there exist few formidable constraints including the need for a large amount of labeled data. Therefore, semi-supervised learning, which is a learning scheme in which the scarce labels and a larger amount of unlabeled data are utilized to train models (e.g., deep neural networks) is getting more important. Based on the key assumptions of semi-supervised learning, which are the manifold assumption, cluster assumption, and continuity assumption, the work reviews the recent semi-supervised learning approaches. In particular, the methods in regard to using deep neural networks in a semi-supervised learning setting are primarily discussed. In addition, the existing works are first classified based on the underlying idea and explained, and then the holistic approaches that unify the aforementioned ideas are detailed.",0
This section can include:,1
"A common assumption in machine learning is that samples are independently and identically distributed (i.i.d). However, the contributions of different samples are not identical in training. Some samples are difficult to learn and some samples are noisy. The unequal contributions of samples has a considerable effect on training performances. Studies focusing on unequal sample contributions (e.g., easy, hard, noisy) in learning usually refer to these contributions as robust machine learning (RML). Weighing and regularization are two common techniques in RML. Numerous learning algorithms have been proposed but the strategies for dealing with easy/hard/noisy samples differ or even contradict with different learning algorithms. For example, some strategies take the hard samples first, whereas some strategies take easy first. Conducting a clear comparison for existing RML algorithms in dealing with different samples is difficult due to lack of a unified theoretical framework for RML. This study attempts to construct a mathematical foundation for RML based on the bias-variance trade-off theory. A series of definitions and properties are presented and proved. Several classical learning algorithms are also explained and compared. Improvements of existing methods are obtained based on the comparison. A unified method that combines two classical learning strategies is proposed.",0
"Abstract: Many machine learning algorithms can have large amounts of both bias and variance, which can lead to suboptimal performance. In order to address these issues, we propose using regularization techniques that allow us to balance the trade-off between bias and variance. Our approach allows us to find optimal weights for each input feature and determine how many training data points are necessary. We use numerical simulations to demonstrate the effectiveness of our methods. By carefully selecting parameters, we achieve state-of-the-art results compared to other approaches. This work has important implications for the development of reliable artificial intelligence systems that can effectively process complex real world datasets.",1
"The difficulty in specifying rewards for many real-world problems has led to an increased focus on learning rewards from human feedback, such as demonstrations. However, there are often many different reward functions that explain the human feedback, leaving agents with uncertainty over what the true reward function is. While most policy optimization approaches handle this uncertainty by optimizing for expected performance, many applications demand risk-averse behavior. We derive a novel policy gradient-style robust optimization approach, PG-BROIL, that optimizes a soft-robust objective that balances expected performance and risk. To the best of our knowledge, PG-BROIL is the first policy optimization algorithm robust to a distribution of reward hypotheses which can scale to continuous MDPs. Results suggest that PG-BROIL can produce a family of behaviors ranging from risk-neutral to risk-averse and outperforms state-of-the-art imitation learning algorithms when learning from ambiguous demonstrations by hedging against uncertainty, rather than seeking to uniquely identify the demonstrator's reward function.",0
"In recent years, imitation learning has emerged as a promising approach to acquiring complex skills from demonstrations by directly optimizing policies towards mimicking expert behavior. However, optimization can quickly become computationally expensive and unstable due to poor initialization, high noise levels, nonlinearities present in many systems, model misspecification issues, etc. To overcome these challenges, we propose Policy Gradient Bayesian Robust Optimization (PGBRO) - a novel method that incorporates Bayesian optimization principles within policy gradient based frameworks for efficiently searching over parameter spaces during imitation learning while handling various sources of uncertainty. We demonstrate through extensive experiments across multiple domains -including both discrete and continuous problems-that PGBRO outperforms state-of-the-art methods in terms of efficiency, accuracy, robustness, etc., offering significant improvements even under adversarial settings such as noisy or partial observations. Overall, our contributions help pave the way towards efficient, effective, and robust imitation learning techniques applicable to real-world tasks.",1
"Recently introduced self-supervised methods for image representation learning provide on par or superior results to their fully supervised competitors, yet the corresponding efforts to explain the self-supervised approaches lag behind. Motivated by this observation, we introduce a novel visual probing framework for explaining the self-supervised models by leveraging probing tasks employed previously in natural language processing. The probing tasks require knowledge about semantic relationships between image parts. Hence, we propose a systematic approach to obtain analogs of natural language in vision, such as visual words, context, and taxonomy. Our proposal is grounded in Marr's computational theory of vision and concerns features like textures, shapes, and lines. We show the effectiveness and applicability of those analogs in the context of explaining self-supervised representations. Our key findings emphasize that relations between language and vision can serve as an effective yet intuitive tool for discovering how machine learning models work, independently of data modality. Our work opens a plethora of research pathways towards more explainable and transparent AI.",0
"This paper proposes a new framework called ""Visual Probing"" that allows us to analyze and interpret self-supervised image representations learned by deep neural networks. We first introduce two novel probing tasks designed to assess how well these representations encode information about object shapes and visual relationships between objects. Our results show that state-of-the-art models trained on large datasets like ImageNet can successfully solve both tasks, outperforming simpler baseline models and demonstrating high levels of transfer learning across different domains. Furthermore, we present several case studies to demonstrate the utility of our approach for gaining insights into model behavior and improving model performance. Overall, our work provides a valuable toolkit for researchers studying the properties and limitations of current deep learning architectures. By providing more transparency into what these models have learned during training, we hope to foster greater understanding of their strengths and weaknesses, ultimately leading to better performing systems on real world problems.",1
"Providing a human-understandable explanation of classifiers' decisions has become imperative to generate trust in their use for day-to-day tasks. Although many works have addressed this problem by generating visual explanation maps, they often provide noisy and inaccurate results forcing the use of heuristic regularization unrelated to the classifier in question. In this paper, we propose a new general perspective of the visual explanation problem overcoming these limitations. We show that visual explanation can be produced as the difference between two generated images obtained via two specific conditional generative models. Both generative models are trained using the classifier to explain and a database to enforce the following properties: (i) All images generated by the first generator are classified similarly to the input image, whereas the second generator's outputs are classified oppositely. (ii) Generated images belong to the distribution of real images. (iii) The distances between the input image and the corresponding generated images are minimal so that the difference between the generated elements only reveals relevant information for the studied classifier. Using symmetrical and cyclic constraints, we present two different approximations and implementations of the general formulation. Experimentally, we demonstrate significant improvements w.r.t the state-of-the-art on three different public data sets. In particular, the localization of regions influencing the classifier is consistent with human annotations.",0
"This paper presents a framework based on conditional generative models that provides explanations for classifier decisions. We show how to use natural language descriptions as conditioning variables to generate concise, informative and human-readable explanations that can improve transparency and accountability of machine learning systems. Our approach combines state-of-the art techniques from the areas of natural language generation and classification explanation. In addition we demonstrate the effectiveness and usability of our framework by applying it to several case studies, including image classification, sentiment analysis and question answering tasks.",1
"Backpropagation image saliency aims at explaining model predictions by estimating model-centric importance of individual pixels in the input. However, class-insensitivity of the earlier layers in a network only allows saliency computation with low resolution activation maps of the deeper layers, resulting in compromised image saliency. Remedifying this can lead to sanity failures. We propose CAMERAS, a technique to compute high-fidelity backpropagation saliency maps without requiring any external priors and preserving the map sanity. Our method systematically performs multi-scale accumulation and fusion of the activation maps and backpropagated gradients to compute precise saliency maps. From accurate image saliency to articulation of relative importance of input features for different models, and precise discrimination between model perception of visually similar objects, our high-resolution mapping offers multiple novel insights into the black-box deep visual models, which are presented in the paper. We also demonstrate the utility of our saliency maps in adversarial setup by drastically reducing the norm of attack signals by focusing them on the precise regions identified by our maps. Our method also inspires new evaluation metrics and a sanity check for this developing research direction. Code is available here https://github.com/VisMIL/CAMERAS",0
"This paper describes a method called CAMERS that enhances resolution while preserving sanity during activation mapping in deep convolutional neural networks (CNNs). CNNs have been widely used for computer vision tasks due to their excellent performance on large scale datasets such as ImageNet. However, these models often suffer from low spatial resolution which can lead to poor interpretability and diagnostic utility. Our proposed method addresses this issue by enhancing the resolution of the output without increasing model complexity, memory usage, or computational cost. We demonstrate our approachâ€™s effectiveness through extensive experiments on several benchmark datasets showing improved visualization quality without sacrificing accuracy. Overall, our work provides a more complete understanding of how CNNs classify images and has important implications for researchers working on developing interpretable CNNs.",1
"How can we find a subset of training samples that are most responsible for a specific prediction made by a complex black-box machine learning model? More generally, how can we explain the model's decisions to end-users in a transparent way? We propose a new model-agnostic algorithm to identify a minimal set of training samples that are indispensable for a given model's decision at a particular test point, i.e., the model's decision would have changed upon the removal of this subset from the training dataset. Our algorithm identifies such a set of ""indispensable"" samples iteratively by solving a constrained optimization problem. Further, we speed up the algorithm through efficient approximations and provide theoretical justification for its performance. To demonstrate the applicability and effectiveness of our approach, we apply it to a variety of tasks including data poisoning detection, training set debugging and understanding loan decisions. The results show that our algorithm is an effective and easy-to-comprehend tool that helps to better understand local model behavior, and therefore facilitates the adoption of machine learning in domains where such understanding is a requisite.",0
"In recent years, there has been growing interest in developing methods that can provide explanations for predictions made by machine learning models. One approach to generating these explanations is through minimal forcing subsets (MFS), which involve identifying a subset of features in the data that are most relevant to a particular prediction. This paper presents a new method for generating MFSs that can effectively capture important patterns in the data while minimizing noise. We demonstrate the effectiveness of our method on several benchmark datasets across different domains and compare its performance against other state-of-the-art methods for generating model agnostic explanations. Our results show that our method outperforms existing approaches in terms of accuracy, interpretability, and computational efficiency. Overall, we believe that this work represents a significant step forward in the development of explainable artificial intelligence (XAI) and has the potential to enable more transparent decision making in high stakes applications such as healthcare, finance, and criminal justice.",1
"Deep visual models are susceptible to adversarial perturbations to inputs. Although these signals are carefully crafted, they still appear noise-like patterns to humans. This observation has led to the argument that deep visual representation is misaligned with human perception. We counter-argue by providing evidence of human-meaningful patterns in adversarial perturbations. We first propose an attack that fools a network to confuse a whole category of objects (source class) with a target label. Our attack also limits the unintended fooling by samples from non-sources classes, thereby circumscribing human-defined semantic notions for network fooling. We show that the proposed attack not only leads to the emergence of regular geometric patterns in the perturbations, but also reveals insightful information about the decision boundaries of deep models. Exploring this phenomenon further, we alter the `adversarial' objective of our attack to use it as a tool to `explain' deep visual representation. We show that by careful channeling and projection of the perturbations computed by our method, we can visualize a model's understanding of human-defined semantic notions. Finally, we exploit the explanability properties of our perturbations to perform image generation, inpainting and interactive image manipulation by attacking adversarialy robust `classifiers'.In all, our major contribution is a novel pragmatic adversarial attack that is subsequently transformed into a tool to interpret the visual models. The article also makes secondary contributions in terms of establishing the utility of our attack beyond the adversarial objective with multiple interesting applications.",0
"This paper presents novel techniques and experiments on attacking deep learning models and understanding their behavior. We investigate and compare gradient sign attacks vs. input perturbation based attacks such as adversarial examples. Our study shows that deep neural networks can fail by either getting stuck during training (in overfitting) or becoming unstable after fine-tuning them on new data. Based on our results we argue that it is more important than ever before to design reliable methods which detect these type of instabilities and improve generalization performance. Finally, we show how recent developments in interpreting DNN decisions provide additional insights into model limitations beyond fooling and robustness evaluation only. In conclusion, we believe this work provides strong motivations for revisiting existing architectures and designs to further advance explainability while preserving accuracy of learned representations. For citation please use: https://arxiv.org/abs/2107.09348 .",1
"First attempts of prediction of the facial growth (FG) direction were made over half of a century ago. Despite numerous attempts and elapsed time, a satisfactory method has not been established yet and the problem still poses a challenge for medical experts. To our knowledge, this paper is the first Machine Learning approach to the prediction of FG direction. Conducted data analysis reveals the inherent complexity of the problem and explains the reasons of difficulty in FG direction prediction based on 2D X-ray images. To perform growth forecasting, we employ a wide range of algorithms, from logistic regression, through tree ensembles to neural networks and consider three, slightly different, problem formulations. The resulting classification accuracy varies between 71% and 75%.",0
"This research uses machine learning techniques such as XGBoost, Light GBM and Random Forest algorithms to predict the facial",1
"Recent years saw a plethora of work on explaining complex intelligent agents. One example is the development of several algorithms that generate saliency maps which show how much each pixel attributed to the agents' decision. However, most evaluations of such saliency maps focus on image classification tasks. As far as we know, there is no work that thoroughly compares different saliency maps for Deep Reinforcement Learning agents. This paper compares four perturbation-based approaches to create saliency maps for Deep Reinforcement Learning agents trained on four different Atari 2600 games. All four approaches work by perturbing parts of the input and measuring how much this affects the agent's output. The approaches are compared using three computational metrics: dependence on the learned parameters of the agent (sanity checks), faithfulness to the agent's reasoning (input degradation), and run-time. In particular, during the sanity checks we find issues with two approaches and propose a solution to fix one of those issues.",0
"Title: Evaluating the Effectiveness of Perturbation Methods in Explainability of Atari Agents  As artificial intelligence (AI) systems become increasingly integrated into our daily lives, there is a growing need for explainability - providing clear insights into how these models make decisions. In particular, reinforcement learning agents have been widely used in complex tasks such as game playing, but their decision making processes can often seem opaque. Recent work has proposed perturbation-based saliency maps as one method for explaining agent behavior by highlighting which parts of input states contribute most strongly to specific actions taken. However, the effectiveness of different perturbation methods remains unclear, particularly on more challenging domains like those found in Atari games. This study aims to address that gap through comprehensive benchmarking across multiple metrics and environments. Our results demonstrate significant variation among methods in terms of accuracy, stability, and interpretability, suggesting clear tradeoffs to consider when designing perturbed explanations. Overall, our findings provide valuable guidance for selecting appropriate techniques when developing interpretable agents for real-world applications.",1
"In an ever expanding set of research and application areas, deep neural networks (DNNs) set the bar for algorithm performance. However, depending upon additional constraints such as processing power and execution time limits, or requirements such as verifiable safety guarantees, it may not be feasible to actually use such high-performing DNNs in practice. Many techniques have been developed in recent years to compress or distill complex DNNs into smaller, faster or more understandable models and controllers. This work seeks to identify reduced models that not only preserve a desired performance level, but also, for example, succinctly explain the latent knowledge represented by a DNN. We illustrate the effectiveness of the proposed approach on the evaluation of decision tree variants and kernel machines in the context of benchmark reinforcement learning tasks.",0
"Title: An Approach to Achieving Human-Understandable Reinforcement Learning Algorithms  Abstract: Many current reinforcement learning algorithms rely on deep neural networks that lack interpretability and transparency. As such, there exists a growing need for approximating these complex models with simpler, more interpretable methods. This research seeks to address this issue by proposing an approach to designing human-understandable approximations to deep RL algorithms. Our method utilizes techniques from decision theory, game theory, and machine learning to construct a hierarchical model consisting of multiple levels of abstraction. Each level is designed to capture different aspects of the problem, allowing for better insight into the algorithmâ€™s behavior and decisions. We demonstrate the effectiveness of our approach through a set of experiments using both synthetic and real-world datasets. Results show that our method can achieve comparable performance to state-of-the-art deep RL algorithms while providing greater interpretability and understanding of the learned policies. Overall, this work represents a significant step towards building intelligible artificial intelligence systems that can effectively interact with humans and their environments.",1
"Works on lottery ticket hypothesis (LTH) and single-shot network pruning (SNIP) have raised a lot of attention currently on post-training pruning (iterative magnitude pruning), and before-training pruning (pruning at initialization). The former method suffers from an extremely large computation cost and the latter category of methods usually struggles with insufficient performance. In comparison, during-training pruning, a class of pruning methods that simultaneously enjoys the training/inference efficiency and the comparable performance, temporarily, has been less explored. To better understand during-training pruning, we quantitatively study the effect of pruning throughout training from the perspective of pruning plasticity (the ability of the pruned networks to recover the original performance). Pruning plasticity can help explain several other empirical observations about neural network pruning in literature. We further find that pruning plasticity can be substantially improved by injecting a brain-inspired mechanism called neuroregeneration, i.e., to regenerate the same number of connections as pruned. Based on the insights from pruning plasticity, we design a novel gradual magnitude pruning (GMP) method, named gradual pruning with zero-cost neuroregeneration (GraNet), and its dynamic sparse training (DST) variant (GraNet-ST). Both of them advance state of the art. Perhaps most impressively, the latter for the first time boosts the sparse-to-sparse training performance over various dense-to-sparse methods by a large margin with ResNet-50 on ImageNet. We will release all codes.",0
"Title: Sparsifying Deep Learning Networks through Adaptive Weight Regularization and Synaptic Homeostasis Mechanisms  Deep learning has revolutionized many fields such as computer vision, natural language processing, and speech recognition by enabling accurate predictions on complex datasets. However, deep neural networks often suffer from overfitting due to their large capacity and complexity. This study proposes two novel mechanisms to regularize these models and promote sparsity while maintaining high performance.  Firstly, we introduce adaptive weight decay which allows individual weights to decay at different rates based on their contributions towards the loss function. This ensures that only relevant connections within the network remain active while eliminating redundant ones. Secondly, our approach leverages synaptic homeostasis principles inspired by biological systems. By implementing a local plasticity rule, we encourage neurons to compete for resources, leading to selective pruning and rewiring. These processes cooperatively drive the network towards sparse solutions while minimizing information loss.  Experimental results across several benchmark tasks demonstrate that our method achieves better accuracy than other popular regularizers like dropout or L2 regularization while significantly reducing model size. Moreover, our mechanism efficiently identifies optimal hyperparameters for each task without human intervention. Our framework paves the way towards more efficient deep learning architectures with improved interpretability and robustness, making them suitable for real-world applications.  In conclusion, this work presents a novel, data-driven technique for training deep neural networks with impressive efficiency, reduced computational costs, and competitive performance compared to state-of-the-art methods. Future directions involve exploring additional neuroscientific principles for further improvement and applying our approach to diverse domains where automated feature selection remains challenging.",1
"We present a method that learns a spatiotemporal neural irradiance field for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradiance field from a single video poses significant challenges because the video contains only one observation of the scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appearance and vice versa. We address this ambiguity by constraining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from individual frames into a single global representation. We provide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results.",0
"In recent years, advances in computer graphics have made possible the creation of realistic videos that can capture viewers' attention like never before. However, creating such videos remains a time-consuming task that requires significant expertise and resources. This paper proposes a new approach called space-time neural irradiance fields (STNIF) that enables the creation of free-viewpoint video with unprecedented ease and efficiency. STNIF represents scene appearance as continuous functions defined across both space and time, allowing artists to generate novel camera paths and viewpoints without compromising visual fidelity. Our method uses deep learning techniques to estimate these functions from raw input footage, enabling automation of several laborious manual processes involved in traditional video synthesis workflows. Experiments demonstrate that our system produces high-quality results that compare favorably against state-of-the-art methods, while offering greater flexibility and interactivity than existing approaches. These findings have important implications for a wide range of applications including virtual reality, gaming, movies, television, and more, opening up exciting possibilities for creators worldwide.",1
"Given the increasing importance of machine learning (ML) in our lives, several algorithmic fairness techniques have been proposed to mitigate biases in the outcomes of the ML models. However, most of these techniques are specialized to cater to a single family of ML models and a specific definition of fairness, limiting their adaptibility in practice. We introduce a general constrained Bayesian optimization (BO) framework to optimize the performance of any ML model while enforcing one or multiple fairness constraints. BO is a model-agnostic optimization method that has been successfully applied to automatically tune the hyperparameters of ML models. We apply BO with fairness constraints to a range of popular models, including random forests, gradient boosting, and neural networks, showing that we can obtain accurate and fair solutions by acting solely on the hyperparameters. We also show empirically that our approach is competitive with specialized techniques that enforce model-specific fairness constraints, and outperforms preprocessing methods that learn fair representations of the input data. Moreover, our method can be used in synergy with such specialized fairness techniques to tune their hyperparameters. Finally, we study the relationship between fairness and the hyperparameters selected by BO. We observe a correlation between regularization and unbiased models, explaining why acting on the hyperparameters leads to ML models that generalize well and are fair.",0
"Bayesian optimization is a powerful tool for optimizing complex functions that are expensive or difficult to evaluate. However, existing algorithms often suffer from overfitting due to large amounts of training data collected on each iteration. In our work, we propose ""Fair Bayesian Optimization,"" which uses principles of fairness to limit the amount of data collected at each point, preventing overfitting and improving performance. Our method balances exploration and exploitation by ensuring that all regions of the input space receive roughly equal consideration, while still allowing efficient convergence to high quality solutions. Extensive experiments demonstrate the effectiveness of our approach across a range of benchmark problems and real world use cases. The resulting algorithm exhibits competitive results compared to state-of-the-art methods, making it a valuable addition to the toolkit of machine learning practitioners. Overall, our work represents a significant step forward towards achieving better generalizability, diversity, interpretability, and fairness in black box optimization.",1
"Recently, Mutual Information (MI) has attracted attention in bounding the generalization error of Deep Neural Networks (DNNs). However, it is intractable to accurately estimate the MI in DNNs, thus most previous works have to relax the MI bound, which in turn weakens the information theoretic explanation for generalization. To address the limitation, this paper introduces a probabilistic representation of DNNs for accurately estimating the MI. Leveraging the proposed MI estimator, we validate the information theoretic explanation for generalization, and derive a tighter generalization bound than the state-of-the-art relaxations.",0
"This is how I would write the abstract for your paper: ""A new approach has been developed that bridges mutual information and generalizability for deep neural networks (DNN). We present evidence that using mutual information as an objective function during training leads to more accurate predictions than traditional approaches, such as cross entropy minimization. Our method involves calculating the expected value of the predictive information given the observed inputs, which can be computed efficiently with gradient methods. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach."" Note that I used passive voice in most cases because it sounds better. Also note that my text contains some technical jargon (""mutual information"", ""predictive information""). If you want me to simplify things, just tell me! And if you have something specific in mind for word choice and tone of voice, just give me some guidelines; I may need to adjust my writing style based on these factors!",1
"Explaining the predictions of opaque machine learning algorithms is an important and challenging task, especially as complex models are increasingly used to assist in high-stakes decisions such as those arising in healthcare and finance. Most popular tools for post-hoc explainable artificial intelligence (XAI) are either insensitive to context (e.g., feature attributions) or difficult to summarize (e.g., counterfactuals). In this paper, I introduce \emph{rational Shapley values}, a novel XAI method that synthesizes and extends these seemingly incompatible approaches in a rigorous, flexible manner. I leverage tools from decision theory and causal modeling to formalize and implement a pragmatic approach that resolves a number of known challenges in XAI. By pairing the distribution of random variables with the appropriate reference class for a given explanation task, I illustrate through theory and experiments how user goals and knowledge can inform and constrain the solution set in an iterative fashion. The method compares favorably to state of the art XAI tools in a range of quantitative and qualitative comparisons.",0
"Title: Rational Shapley Values ---------------------------  In cooperative game theory, the Shapley value is a fundamental concept that provides a unique distribution of payoffs among players in a coalition game based on their marginal contributions. This study proposes the notion of rational Shapley values (RS), which extend classical Shapley values by incorporating the rationality of agents into the allocation process. Specifically, each playerâ€™s rational contribution is estimated using a novel methodology called agent-specific weights, which take into account their individual preferences, constraints, and strategic behavior. Our approach allows us to capture heterogeneous effects across different types of games, including common value games and private value games. We demonstrate through simulations and case studies that our framework yields more accurate predictions compared to existing methods while remaining computationally efficient. Finally, we discuss the implications of RS for social welfare measurement and policy design, highlighting potential applications in areas such as public goods provision and environmental resource management. Overall, this work contributes new insights into cooperative game theory, revealing how agentsâ€™ rational behavior can shape collective outcomes.  This paper introduces the concept of rational Shapley values (RS) in cooperative game theory. Classical Shapley values estimate a unique distribution of payoffs among players based on their marginal contributions to a coalition game. In contrast, RS incorporate the rational behavior of agents using a new methodology called agent-specific weights. These weights consider individual preferences, constraints, and strategic actions. Our framework demonstrates improved accuracy over traditional approaches, making it well suited for policy analysis and decision support systems. Agent-specific weights enhance existing measures of collective performance by identifying when and why individual motivations matter. Simulations illustrate these effects under various settings, showcasing robustness across diverse scenarios. Concluding sections address implications for policy making and future research directions.",1
"Attribution methods remain a practical instrument that is used in real-world applications to explain the decision-making process of complex learning machines. It has been shown that a simple method called SmoothGrad can effectively reduce the visual diffusion of gradient-based attribution methods and has established itself among both researchers and practitioners. What remains unexplored in research, however, is how explanations can be improved by introducing stochasticity to the model weights. In the light of this, we introduce - NoiseGrad - a stochastic, method-agnostic explanation-enhancing method that adds noise to the weights instead of the input data. We investigate our proposed method through various experiments including different datasets, explanation methods and network architectures and conclude that NoiseGrad (and its extension NoiseGrad++) with multiplicative Gaussian noise offers a clear advantage compared to SmoothGrad on several evaluation criteria. We connect our proposed method to Bayesian Learning and provide the user with a heuristic for choosing hyperparameters.",0
"This paper presents the concept of ""NoiseGrad"", which involves adding noise during training in order to enhance the quality of model explanations. By carefully controlling the amount of noise added, it is possible to encourage models to produce more diverse, interpretable explanations that better reflect the underlying data distribution. Our approach has been validated through extensive experiments on multiple datasets and architectures, demonstrating consistent improvements over baseline methods in terms of both quantitative evaluation metrics and qualitative visual inspection. Overall, our findings suggest that the incorporation of randomness into gradient descent can serve as a powerful tool for improving model interpretability and human comprehensibility.",1
"Supervised deep convolutional neural networks (DCNNs) are currently one of the best computational models that can explain how the primate ventral visual stream solves object recognition. However, embodied cognition has not been considered in the existing visual processing models. From the ecological standpoint, humans learn to recognize objects by interacting with them, allowing better classification, specialization, and generalization. Here, we ask if computational models under the embodied learning framework can explain mechanisms underlying object recognition in the primate visual system better than the existing supervised models? To address this question, we use reinforcement learning to train neural network models to play a 3D computer game and we find that these reinforcement learning models achieve neural response prediction accuracy scores in the early visual areas (e.g., V1 and V2) in the levels that are comparable to those accomplished by the supervised neural network model. In contrast, the supervised neural network models yield better neural response predictions in the higher visual areas, compared to the reinforcement learning models. Our preliminary results suggest the future direction of visual neuroscience in which deep reinforcement learning should be included to fill the missing embodiment concept.",0
"New research has demonstrated that deep reinforcement learning (DRL) models can accurately predict neural responses in the visual cortex of humans during natural viewing tasks. This preliminary result suggests that DRL may provide a powerful tool for understanding how the brain processes sensory information and makes decisions based on that input. To achieve these results, the researchers trained DRL agents using datasets of human behavioral data from visual tasks such as object recognition and decision making under uncertainty. They then tested their model by having human participants complete similar tasks while recording neural activity in the visual cortex through functional magnetic resonance imaging (fMRI). Remarkably, the predictions made by the DRL model closely matched the actual fMRI signal measurements recorded during task performance. These findings suggest that DRL models can capture complex relationships between perception and action, and have implications for our understanding of both human cognition and artificial intelligence systems. Further investigation into the applications of DRL in neuroscience is warranted, including testing whether these models can generalize beyond the specific tasks used in training and evaluating their ability to make accurate predictions across different individuals.",1
"Counterfactual explanations and adversarial examples have emerged as critical research areas for addressing the explainability and robustness goals of machine learning (ML). While counterfactual explanations were developed with the goal of providing recourse to individuals adversely impacted by algorithmic decisions, adversarial examples were designed to expose the vulnerabilities of ML models. While prior research has hinted at the commonalities between these frameworks, there has been little to no work on systematically exploring the connections between the literature on counterfactual explanations and adversarial examples. In this work, we make one of the first attempts at formalizing the connections between counterfactual explanations and adversarial examples. More specifically, we theoretically analyze salient counterfactual explanation and adversarial example generation methods, and highlight the conditions under which they behave similarly. Our analysis demonstrates that several popular counterfactual explanation and adversarial example generation methods such as the ones proposed by Wachter et. al. and Carlini and Wagner (with mean squared error loss), and C-CHVAE and natural adversarial examples by Zhao et. al. are equivalent. We also bound the distance between counterfactual explanations and adversarial examples generated by Wachter et. al. and DeepFool methods for linear models. Finally, we empirically validate our theoretical findings using extensive experimentation with synthetic and real world datasets.",0
"This paper explores the connections between counterfactual explanations and adversarial examples. While counterfactual explanations provide insight into why decisions were made by highlighting how different factors might have led to alternative outcomes, adversarial examples exploit weaknesses in machine learning models used to generate such explanations. By analyzing these relationships, we can gain new perspectives on both concept types and their implications for responsible artificial intelligence development and deployment. Our findings show that while both concepts hold value separately, combining them could result in more informative decision processes, stronger accountability measures, and improved system trustworthiness overall.",1
"Decentralized training of deep learning models enables on-device learning over networks, as well as efficient scaling to large compute clusters. Experiments in earlier works reveal that, even in a data-center setup, decentralized training often suffers from the degradation in the quality of the model: the training and test performance of models trained in a decentralized fashion is in general worse than that of models trained in a centralized fashion, and this performance drop is impacted by parameters such as network size, communication topology and data partitioning. We identify the changing consensus distance between devices as a key parameter to explain the gap between centralized and decentralized training.   We show in theory that when the training consensus distance is lower than a critical quantity, decentralized training converges as fast as the centralized counterpart. We empirically validate that the relation between generalization performance and consensus distance is consistent with this theoretical observation. Our empirical insights allow the principled design of better decentralized training schemes that mitigate the performance drop. To this end, we provide practical training guidelines and exemplify its effectiveness on the data-center setup as the important first step.",0
"In recent years, decentralized deep learning has gained significant attention due to its potential to enable large-scale machine learning while preserving data privacy and security. However, designing distributed algorithms that can achieve high accuracy across multiple devices remains challenging. This paper proposes a consensus control approach to distributed deep learning, which allows each device to collaboratively learn a shared model using their private data. Our method leverages existing communication protocols to ensure robustness against device dropout and straggling. We demonstrate through experiments on benchmark datasets and real-world scenarios that our consensus control algorithm achieves competitive performance compared to centralized training, while enabling efficient collaboration among distributed devices. Additionally, we evaluate the impact of different hyperparameter settings and investigate how they affect system efficiency and end-to-end learning time. Overall, our work highlights the benefits of consensus control in distributed deep learning systems and paves the way towards scalable and secure solutions for multi-device learning tasks.",1
"As we rely more and more on machine learning models for real-life decision-making, being able to understand and trust the predictions becomes ever more important. Local explainer models have recently been introduced to explain the predictions of complex machine learning models at the instance level. In this paper, we propose Local Rule-based Model Interpretability with k-optimal Associations (LoRMIkA), a novel model-agnostic approach that obtains k-optimal association rules from a neighbourhood of the instance to be explained. Compared with other rule-based approaches in the literature, we argue that the most predictive rules are not necessarily the rules that provide the best explanations. Consequently, the LoRMIkA framework provides a flexible way to obtain predictive and interesting rules. It uses an efficient search algorithm guaranteed to find the k-optimal rules with respect to objectives such as confidence, lift, leverage, coverage, and support. It also provides multiple rules which explain the decision and counterfactual rules, which give indications for potential changes to obtain different outputs for given instances. We compare our approach to other state-of-the-art approaches in local model interpretability on three different datasets and achieve competitive results in terms of local accuracy and interpretability.",0
"This paper presents a new method for interpreting machine learning models called LoRMIkA (Local Rule-Based Model Interpretability with K-Optimal Associations). The goal of model interpretation is to gain insight into how a model makes predictions by identifying patterns or rules that govern its behavior. Current methods often focus on global interpretations that provide a general overview of the model but may lack specificity at the local level. In contrast, LoRMIkA provides a framework for understanding how a model behaves locally, allowing for more targeted explanations of prediction outcomes.  The approach taken by LoRMIkA involves first breaking down the input space into smaller regions of interest using clustering techniques. Within each region, the method then seeks to identify a set of k association rules that can explain a significant portion of the positive class cases. These rules describe the conditions under which the model predicts a certain outcome and can serve as a basis for constructing detailed case narratives. Through experiments on several benchmark datasets, we demonstrate the effectiveness of our method in generating high-quality local explanations with low computational overhead. Our results showcase the potential of LoRMIkA as a valuable tool for improving transparency and trustworthiness in machine learning applications.",1
"Integrated Gradients (IG) is a commonly used feature attribution method for deep neural networks. While IG has many desirable properties, the method often produces spurious/noisy pixel attributions in regions that are not related to the predicted class when applied to visual models. While this has been previously noted, most existing solutions are aimed at addressing the symptoms by explicitly reducing the noise in the resulting attributions. In this work, we show that one of the causes of the problem is the accumulation of noise along the IG path. To minimize the effect of this source of noise, we propose adapting the attribution path itself -- conditioning the path not just on the image but also on the model being explained. We introduce Adaptive Path Methods (APMs) as a generalization of path methods, and Guided IG as a specific instance of an APM. Empirically, Guided IG creates saliency maps better aligned with the model's prediction and the input image that is being explained. We show through qualitative and quantitative experiments that Guided IG outperforms other, related methods in nearly every experiment.",0
"In many fields, including image processing and machine learning, the quality of data can greatly affect the accuracy of analysis and modeling efforts. One key challenge is dealing with noise present within datasets. In order to address this issue, we introduce Guided Integrated Gradients (GIG), a novel method based on adaptive gradient paths that effectively removes noise from data while preserving relevant features. Our approach combines traditional smoothing techniques with a new adaptive component that better captures local geometry, enabling more accurate removal of noise. We demonstrate through experiments on both synthetic and real-world datasets that GIG leads to improved performance over state-of-the-art methods across different types of noises and tasks. Overall, our findings suggest that GIG provides an effective solution for handling noise in data and enhances the reliability and efficiency of analyses performed using such data.",1
"We show that adding differential privacy to Explainable Boosting Machines (EBMs), a recent method for training interpretable ML models, yields state-of-the-art accuracy while protecting privacy. Our experiments on multiple classification and regression datasets show that DP-EBM models suffer surprisingly little accuracy loss even with strong differential privacy guarantees. In addition to high accuracy, two other benefits of applying DP to EBMs are: a) trained models provide exact global and local interpretability, which is often important in settings where differential privacy is needed; and b) the models can be edited after training without loss of privacy to correct errors which DP noise may have introduced.",0
"In recent years, there has been growing interest in developing machine learning models that can provide insight into how they make predictions. Two important aspects of this problem are interpretability and accuracy: a model should both make accurate predictions and be able to explain why it makes those predictions. Additionally, there are increasing concerns around privacy and data protection, leading to a need for methods that enable private and interpretable predictions while still maintaining high levels of accuracy. In our work, we propose a novel framework called ""Accuracy, Interpretability, and Differential Privacy via Explainable Boosting"" (AIDE), which addresses these challenges by combining boosting algorithms with post-hoc interpretation techniques. Our approach ensures differentially private predictions while retaining strong interpretability guarantees and achieving state-of-the-art accuracies on several benchmark datasets. By bridging the gap between accuracy, interpretability, and privacy, our method offers significant improvements over existing approaches and opens up new possibilities in deploying machine learning models in sensitive applications.",1
"Learning maps between data samples is fundamental. Applications range from representation learning, image translation and generative modeling, to the estimation of spatial deformations. Such maps relate feature vectors, or map between feature spaces. Well-behaved maps should be regular, which can be imposed explicitly or may emanate from the data itself. We explore what induces regularity for spatial transformations, e.g., when computing image registrations. Classical optimization-based models compute maps between pairs of samples and rely on an appropriate regularizer for well-posedness. Recent deep learning approaches have attempted to avoid using such regularizers altogether by relying on the sample population instead. We explore if it is possible to obtain spatial regularity using an inverse consistency loss only and elucidate what explains map regularity in such a context. We find that deep networks combined with an inverse consistency loss and randomized off-grid interpolation yield well behaved, approximately diffeomorphic, spatial transformations. Despite the simplicity of this approach, our experiments present compelling evidence, on both synthetic and real data, that regular maps can be obtained without carefully tuned explicit regularizers, while achieving competitive registration performance.",0
This should summarize your results while omitting any unnecessary technical details.,1
"Collaboration is identified as a required and necessary skill for students to be successful in the fields of Science, Technology, Engineering and Mathematics (STEM). However, due to growing student population and limited teaching staff it is difficult for teachers to provide constructive feedback and instill collaborative skills using instructional methods. Development of simple and easily explainable machine-learning-based automated systems can help address this problem. Improving upon our previous work, in this paper we propose using simple temporal-CNN deep-learning models to assess student group collaboration that take in temporal representations of individual student roles as input. We check the applicability of dynamically changing feature representations for student group collaboration assessment and how they impact the overall performance. We also use Grad-CAM visualizations to better understand and interpret the important temporal indices that led to the deep-learning model's decision.",0
"Title: Explainable Group Work Assessment using Dynamic Role Analysis Modeling  Group work has become an essential component of modern education as it promotes important skills such as teamwork, communication, and leadership. Evaluating individual contributions within group projects, however, remains challenging due to difficulties in identifying individual roles and assessing their impact on the group's overall performance. This study proposes a novel approach to explainable student group collaboration assessment by utilizing temporal representations of individual student roles. Our dynamic role analysis model captures changes in each student's behavior over time, providing richer insights into their distinctive contribution patterns. By integrating these temporal models with machine learning algorithms, we aim to develop more accurate and interpretable evaluation systems that enable educators to make better-informed decisions regarding individual student grades. We demonstrate the effectiveness of our methodology through extensive experimental validation, highlighting its potential applications in real-world educational settings.",1
"Understanding the implicit bias of training algorithms is of crucial importance in order to explain the success of overparametrised neural networks. In this paper, we study the dynamics of stochastic gradient descent over diagonal linear networks through its continuous time version, namely stochastic gradient flow. We explicitly characterise the solution chosen by the stochastic flow and prove that it always enjoys better generalisation properties than that of gradient flow. Quite surprisingly, we show that the convergence speed of the training loss controls the magnitude of the biasing effect: the slower the convergence, the better the bias. To fully complete our analysis, we provide convergence guarantees for the dynamics. We also give experimental results which support our theoretical claims. Our findings highlight the fact that structured noise can induce better generalisation and they help explain the greater performances observed in practice of stochastic gradient descent over gradient descent.",0
"Many modern machine learning models rely on diagonal linear networks trained using stochastic gradient descent (SGD) to minimize their training loss. These methods have shown impressive performance across multiple domains, including natural language processing, computer vision, and speech recognition. Despite these successes, little attention has been paid to understanding how the specific formulation of SGD impacts model behavior and ultimately affects predictions. In particular, we demonstrate that under certain conditions, the use of stochastic gradients rather than full batch gradients leads to implicit bias in the learned parameters and reduces overfitting. To analyze the effect of noise injection in stochastic gradients, we develop novel theoretical results showing that such biases arise from randomizing the Hessian matrix during optimization, enabling us to derive new insights into regularization effects that occur in practice but previously lacked rigorous justification. We confirm our findings through extensive experiments on several benchmark datasets spanning diverse tasks while considering different network architectures and hyperparameter settings. Our work provides new analytic tools and intuitions for understanding why SGD with diagonal linear networks can lead to improved generalization. By shedding light on the underlying mechanisms governing the interaction between SGD, stochasticity, and network architecture, this research offers valuable guidance for designing more effective algorithms and improving state-of-the-art systems.",1
"Despite the popularisation of machine learning models, more often than not, they still operate as black boxes with no insight into what is happening inside the model. There exist a few methods that allow to visualise and explain why a model has made a certain prediction. Those methods, however, allow visualisation of the link between the input and output of the model without presenting how the model learns to represent the data used to train the model as whole. In this paper, a method that addresses that issue is proposed, with a focus on visualising multi-dimensional time-series data. Experiments on a high-frequency stock market dataset show that the method provides fast and discernible visualisations. Large datasets can be visualised quickly and on one plot, which makes it easy for a user to compare the learned representations of the data. The developed method successfully combines known techniques to provide an insight into the inner workings of time-series classification models.",0
"This research presents an innovative approach to visualizing deep neural network time-series representations using advanced graphical techniques. By analyzing how these networks process time-series data, we aim to gain insights into their inner workings and improve our understanding of deep learning systems as a whole. Our method involves creating informative visualizations that highlight key patterns and trends in the data, making them easier to interpret and analyze. We demonstrate the effectiveness of our technique on several real-world applications and showcase its ability to provide valuable new perspectives on complex problems. Overall, our work represents a significant step forward in improving our ability to debug, diagnose, and optimize deep learning models, paving the way for more effective use of these powerful tools across many domains.",1
"Video understanding calls for a model to learn the characteristic interplay between static scene content and its dynamics: Given an image, the model must be able to predict a future progression of the portrayed scene and, conversely, a video should be explained in terms of its static image content and all the remaining characteristics not present in the initial frame. This naturally suggests a bijective mapping between the video domain and the static content as well as residual information. In contrast to common stochastic image-to-video synthesis, such a model does not merely generate arbitrary videos progressing the initial image. Given this image, it rather provides a one-to-one mapping between the residual vectors and the video with stochastic outcomes when sampling. The approach is naturally implemented using a conditional invertible neural network (cINN) that can explain videos by independently modelling static and other video characteristics, thus laying the basis for controlled video synthesis. Experiments on four diverse video datasets demonstrate the effectiveness of our approach in terms of both the quality and diversity of the synthesized results. Our project page is available at https://bit.ly/3t66bnU.",0
"Image synthesis from textual descriptions has recently become feasible through generative models such as Generative Adversarial Networks (GAN). Yet these approaches often struggle to generate visually plausible video sequences due to their difficulty modeling complex motion patterns. To address this challenge, we propose to use Cycle-Consistent Adversarial Imitation Learning (cAIL) as a framework to train our stochastic image generator. In contrast to most other GAN based methods, which rely on pixel-wise discriminators that can easily lead to blurry artifacts at test time, our approach is trained to maximize the likelihood of ground truth frames given by real videos sampled during training. This allows us to exploit powerful flow estimation techniques that explicitly model the dense correspondence between images making them well suited for generating highly detailed and temporally coherent video outputs. Since video generation involves multiple steps, we further introduce novel architectures to enable efficient modeling of spatio-temporal relations between consecutive video frames. We achieve state-of-the art results both quantitatively and qualitatively compared to previous work. Additionally, we showcase examples of applications in computer vision such as video frame interpolation where our method generates more photorealistic output than existing alternatives. We believe that our contributions form an important step towards enabling intelligent systems to interact with dynamic visual environments rather than just still images.",1
"Convolutional neural network (CNN) models for computer vision are powerful but lack explainability in their most basic form. This deficiency remains a key challenge when applying CNNs in important domains. Recent work on explanations through feature importance of approximate linear models has moved from input-level features (pixels or segments) to features from mid-layer feature maps in the form of concept activation vectors (CAVs). CAVs contain concept-level information and could be learned via clustering. In this work, we rethink the ACE algorithm of Ghorbani et~al., proposing an alternative invertible concept-based explanation (ICE) framework to overcome its shortcomings. Based on the requirements of fidelity (approximate models to target models) and interpretability (being meaningful to people), we design measurements and evaluate a range of matrix factorization methods with our framework. We find that non-negative concept activation vectors (NCAVs) from non-negative matrix factorization provide superior performance in interpretability and fidelity based on computational and human subject experiments. Our framework provides both local and global concept-level explanations for pre-trained CNN models.",0
"This is an abstract of sorts based on the given guidelines:  In recent years, deep learning models have become increasingly popular due to their ability to process large amounts of data and generate accurate predictions. One such model that has gained widespread attention is the Convolutional Neural Network (CNN), which has been successfully used in image classification tasks. Despite their effectiveness, these models can be difficult to interpret, as they rely on complex mathematical operations that are challenging to decipher. Therefore, developing methods that allow us to explain how these models make decisions is crucial for gaining a deeper understanding of their workings.  One approach to explaining the decision making process of CNNs involves identifying salient features in the input data that contribute to the predicted output. This can be achieved by training an auxiliary network alongside the main CNN, which maps input pixels to concept activation vectors. These activation vectors represent the relative importance of different visual concepts in predicting the output, providing insight into how the model makes its predictions.  However, some existing methods suffer from limitations such as non-invertibility, meaning that the method cannot guarantee that there exists a corresponding input image that produces a specific set of activations. To address this issue, we propose a novel method called Invertible Concept-based Explanations for CNN Models with Non-negative Concept Activation Vectors. Our approach utilizes invertible neural networks and leverages a regularizer term that encourages sparse activations. Furthermore, we use an efficient algorithm based on Lagrangian optimization to compute the corresponding input image from the desired activation vector.  Our experiments demonstrate the effectiveness of our proposed method. We achieve higher fidelity and perception quality compared to previous stu",1
"Deep neural networks need large amounts of labeled data to achieve good performance. In real-world applications, labels are usually collected from non-experts such as crowdsourcing to save cost and thus are noisy. In the past few years, deep learning methods for dealing with noisy labels have been developed, many of which are based on the small-loss criterion. However, there are few theoretical analyses to explain why these methods could learn well from noisy labels. In this paper, we theoretically explain why the widely-used small-loss criterion works. Based on the explanation, we reformalize the vanilla small-loss criterion to better tackle noisy labels. The experimental results verify our theoretical explanation and also demonstrate the effectiveness of the reformalization.",0
"This paper studies deep learning from noisy labels using small loss criterion. We develop novel algorithms that can better handle noise by finding optimal weights based on the data distribution. Our approach leverages recent advances in deep learning theory to estimate the true label density function given the observed data. In particular, we use adversarial training techniques to maximize entropy while minimizing prediction errors. Experiments show consistent gains across multiple datasets and architectures compared to previous methods. These results have important implications for real-world applications where clean ground truth is scarce.",1
"Smart contracts hold digital coins worth billions of dollars, their security issues have drawn extensive attention in the past years. Towards smart contract vulnerability detection, conventional methods heavily rely on fixed expert rules, leading to low accuracy and poor scalability. Recent deep learning approaches alleviate this issue but fail to encode useful expert knowledge. In this paper, we explore combining deep learning with expert patterns in an explainable fashion. Specifically, we develop automatic tools to extract expert patterns from the source code. We then cast the code into a semantic graph to extract deep graph features. Thereafter, the global graph feature and local expert patterns are fused to cooperate and approach the final prediction, while yielding their interpretable weights. Experiments are conducted on all available smart contracts with source code in two platforms, Ethereum and VNT Chain. Empirically, our system significantly outperforms state-of-the-art methods. Our code is released.",0
"Automatic vulnerability detection on smart contracts has become essential due to recent security breaches occurring on decentralized networks, resulting in significant financial losses. Many researchers have developed models based on pure neural network architectures, which provide high accuracy but lack interpretability. This study introduces a new method that fuses graph features extracted from control flow graphs (CFG) using expert patterns derived from manual analysis by blockchain professionals, along with a novel model built with transparent attention mechanisms (TAM). Our approach achieved state-of-the-art performance on a benchmark dataset consisting of 268 vulnerable and non-vulnerable Ethereum Solidity smart contracts, surpassing the previous best work by more than ten percent while providing greater explainability for developers. Experiments revealed that our interpretable graph feature fusion architecture combined with TAM substantially contributed towards improving detection accuracies, demonstrating the effectiveness of combining human knowledge with machine learning techniques for enhanced results. To foster further research, we have released our datasets and code at https://github.com/DarkTalentZSC/SmartContractVulnDetection. Overall, our findings suggest that incorporating domain expertise into automated testing methods can significantly enhance their applicability in real-world scenarios.",1
"We consider interpolation learning in high-dimensional linear regression with Gaussian data, and prove a generic uniform convergence guarantee on the generalization error of interpolators in an arbitrary hypothesis class in terms of the class's Gaussian width. Applying the generic bound to Euclidean norm balls recovers the consistency result of Bartlett et al. (2020) for minimum-norm interpolators, and confirms a prediction of Zhou et al. (2020) for near-minimal-norm interpolators in the special case of Gaussian data. We demonstrate the generality of the bound by applying it to the simplex, obtaining a novel consistency result for minimum l1-norm interpolators (basis pursuit). Our results show how norm-based generalization bounds can explain and be used to analyze benign overfitting, at least in some settings.",0
"This paper examines the concept of uniform convergence of interpolators in the context of approximation theory. We consider two important approaches to approximating functions using interpolators: polynomial interpolation and spline interpolation. In particular, we study how the choice of interpolation method affects the uniform convergence properties of these functions on a given domain. Our analysis employs techniques from functional analysis, specifically focusing on Gaussian width and norm bounds, to derive new results related to the uniform convergence of interpolators. By doing so, we aim to provide insight into why certain types of overfitting may occur and how to mitigate them effectively. Finally, our findings have potential implications for applications such as signal processing and machine learning where accurate function approximation plays a crucial role.",1
"In this study, we take a departure and explore an explainability-driven strategy to data auditing, where actionable insights into the data at hand are discovered through the eyes of quantitative explainability on the behaviour of a dummy model prototype when exposed to data. We demonstrate this strategy by auditing two popular medical benchmark datasets, and discover hidden data quality issues that lead deep learning models to make predictions for the wrong reasons. The actionable insights gained from this explainability driven data auditing strategy is then leveraged to address the discovered issues to enable the creation of high-performing deep learning models with appropriate prediction behaviour. The hope is that such an explainability-driven strategy can be complimentary to data-driven strategies to facilitate for more responsible development of machine learning algorithms for computer vision applications.",0
"This paper presents a new approach for data auditing in responsible computer vision applications that focuses on explaining model behavior. We propose an explainability-driven strategy to gain insights into data used by machine learning models, enabling improved transparency and accountability. Our approach is designed to promote ethical use of images and videos as well as enhance public trust in computer vision systems. Using case studies from real-world scenarios, we demonstrate how our method can effectively detect discriminatory biases and errors in image classification tasks. We evaluate the effectiveness and efficiency of our framework using quantitative measures, showing significant improvement over traditional auditing methods. Overall, our work highlights the importance of explainable artificial intelligence in ensuring responsible development of computer vision technologies.",1
"Unlabelled data appear in many domains and are particularly relevant to streaming applications, where even though data is abundant, labelled data is rare. To address the learning problems associated with such data, one can ignore the unlabelled data and focus only on the labelled data (supervised learning); use the labelled data and attempt to leverage the unlabelled data (semi-supervised learning); or assume some labels will be available on request (active learning). The first approach is the simplest, yet the amount of labelled data available will limit the predictive performance. The second relies on finding and exploiting the underlying characteristics of the data distribution. The third depends on an external agent to provide the required labels in a timely fashion. This survey pays special attention to methods that leverage unlabelled data in a semi-supervised setting. We also discuss the delayed labelling issue, which impacts both fully supervised and semi-supervised methods. We propose a unified problem setting, discuss the learning guarantees and existing methods, explain the differences between related problem settings. Finally, we review the current benchmarking practices and propose adaptations to enhance them.",0
"In recent years, semi-supervised learning (SSL) has gained popularity as a powerful method for improving the performance of machine learning models by leveraging both labeled and unlabeled data. However, most SSL algorithms assume that all available data can be used for training without any restrictions. This assumption may not hold true in practice where data might arrive gradually over time in the form of streams. Additionally, some labels might only become available after additional context is provided, which results in delayed partially labeled data streams (DPLDS). Handling DPLDS poses unique challenges due to the dynamic nature of stream data, label uncertainty, and missing values. Therefore, there is a need for adapting SSL techniques to handle DPLDS effectively. This survey provides an extensive analysis of existing methods designed for DPLDS and highlights their strengths and limitations. We discuss different strategies employed for handling evolving patterns, model updating, and label refinement under resource constraints. Furthermore, we categorize existing approaches based on their underlying concepts such as co-training, self-learning, and active learning. Finally, we outline potential research directions and open challenges in this emerging field. Our study serves as a comprehensive reference guide for researchers, practitioners, and graduate students interested in exploring the use of SSL for DPLDS.",1
"Convolutional neural networks (CNN) have been frequently used to extract subject-invariant features from electroencephalogram (EEG) for classification tasks. This approach holds the underlying assumption that electrodes are equidistant analogous to pixels of an image and hence fails to explore/exploit the complex functional neural connectivity between different electrode sites. We overcome this limitation by tailoring the concepts of convolution and pooling applied to 2D grid-like inputs for the functional network of electrode sites. Furthermore, we develop various graph neural network (GNN) models that project electrodes onto the nodes of a graph, where the node features are represented as EEG channel samples collected over a trial, and nodes can be connected by weighted/unweighted edges according to a flexible policy formulated by a neuroscientist. The empirical evaluations show that our proposed GNN-based framework outperforms standard CNN classifiers across ErrP, and RSVP datasets, as well as allowing neuroscientific interpretability and explainability to deep learning methods tailored to EEG related classification problems. Another practical advantage of our GNN-based framework is that it can be used in EEG channel selection, which is critical for reducing computational cost, and designing portable EEG headsets.",0
"Title: ""Graph Neural Networks for Classifying Electroencephalogram (EEG) Signals""  Abstract: This paper proposes a novel approach for classifying electroencephalogram signals using graph neural networks (GNN). Recent advances in GNN have shown promising results in processing complex data structures such as graphs, making them a suitable candidate for analyzing brain activity captured through EEG recordings. The proposed method extracts features from raw EEG signals by constructing connectivity graphs based on frequency domain characteristics. These graphs capture both local and global patterns present in the signal that may be relevant for understanding cognitive states, sleep stages, or other clinical applications. After feature extraction, we apply GNNs for classification, leveraging their ability to learn node representations in the latent space by propagating information across layers while accounting for nonlinear dependencies among nodes. We demonstrate our frameworkâ€™s effectiveness using standard benchmark datasets and evaluate performance against conventional machine learning methods. Our results show improved accuracy compared to traditional techniques, highlighting the potential utility of GNN models for interpreting complex neurophysiological signals.",1
"One of the most popular methods of the machine learning prediction explanation is the SHapley Additive exPlanations method (SHAP). An imprecise SHAP as a modification of the original SHAP is proposed for cases when the class probability distributions are imprecise and represented by sets of distributions. The first idea behind the imprecise SHAP is a new approach for computing the marginal contribution of a feature, which fulfils the important efficiency property of Shapley values. The second idea is an attempt to consider a general approach to calculating and reducing interval-valued Shapley values, which is similar to the idea of reachable probability intervals in the imprecise probability theory. A simple special implementation of the general approach in the form of linear optimization problems is proposed, which is based on using the Kolmogorov-Smirnov distance and imprecise contamination models. Numerical examples with synthetic and real data illustrate the imprecise SHAP.",0
"This paper proposes a new method for explaining the class probability distributions generated by machine learning models under limited training data scenarios. Our approach relies on using Shapley values from cooperative game theory to assign credit to different features in a modelâ€™s input space. To address issues of computational feasibility and imprecision that arise under limited data availability, we introduce an additional randomness source into the SHAP computation process. Our experimental evaluation shows that our method can generate reliable explanations even when the amount of available training data is severely restricted. By providing interpretable and trustworthy explanations, our work contributes towards building more transparent machine learning systems. Keywords: Machine Learning; Explanation; Transparency; Interpretability; Limited Training Data; Shapley Values; Feature Attribution; Credit Assignment This paper presents a novel technique for analyzing class probabilities produced by machine learning algorithms in situations where only small amounts of training data are available. We employ a variant of SHAP (SHapley Additive exPlanations), a popular explainability method based on coalitional game theory, which accounts for the uncertainties inherent in such cases. The key challenge lies in accurately assigning credit to individual features in light of imperfect datasets. Our innovation involves adding stochasticity to the SHAP calculation, enabling it to operate effectively with scarce data while maintaining interpretability. Experiments demonstrate the effectiveness of our methodology, delivering dependable results despite drastically reduced training sizes. In conclusion, our research enhances ML transparency by offering meaningful and verifiable explanations even under constrained circumstances, thereby contributing t",1
"Two-part models are important to and used throughout insurance and actuarial science. Since insurance is required for registering a car, obtaining a mortgage, and participating in certain businesses, it is especially important that the models which price insurance policies are fair and non-discriminatory. Black box models can make it very difficult to know which covariates are influencing the results. SHAP values enable interpretation of various black box models, but little progress has been made in two-part models. In this paper, we propose mSHAP (or multiplicative SHAP), a method for computing SHAP values of two-part models using the SHAP values of the individual models. This method will allow for the predictions of two-part models to be explained at an individual observation level. After developing mSHAP, we perform an in-depth simulation study. Although the kernelSHAP algorithm is also capable of computing approximate SHAP values for a two-part model, a comparison with our method demonstrates that mSHAP is exponentially faster. Ultimately, we apply mSHAP to a two-part ratemaking model for personal auto property damage insurance coverage. Additionally, an R package (mshap) is available to easily implement the method in a wide variety of applications.",0
"In recent years, Shapley values have emerged as an essential tool for explaining individual feature importance in machine learning models, especially those used for predicting outcomes based on multiple input features. However, traditional methods like SHAP (NIPS'2017) cannot account for two-part decision rules that split data into treatment vs. control groups before calculating predictions. mSHAP (arXiv'2021) extended SHAP to handle discrete decisions by considering separate contributions from positive class probabilities or rankings. We extend mSHAP further to encompass continuous treatments such as dose levels in clinical trials. For each subject i, we first compute mean predicted outcome Yi|do(xi), where do(.) denotes the chosen treatment level. Then, starting from the base case Yi=f(x1_i, ... , xp_i), we analyze three treatment choices: all subjects receive no treatment, only subject i receives treatment, and everyone but subject i receives treatment. These ""swap"" scenarios help determine how varying dosage affects individuals relative to the entire group. Our approach leads to interpretable partial dependencies and individual treatment effect estimates, crucial components in personalized medicine. Code and synthetic datasets at https://github.com/LAION-AI/mshap/tree/master/examples#example-datasets demonstrate ease of implementation and utility for practitioners working with real datasets.",1
"We propose to explain the behavior of black-box prediction methods (e.g., deep neural networks trained on image pixel data) using causal graphical models. Specifically, we explore learning the structure of a causal graph where the nodes represent prediction outcomes along with a set of macro-level ""interpretable"" features, while allowing for arbitrary unmeasured confounding among these variables. The resulting graph may indicate which of the interpretable features, if any, are possible causes of the prediction outcome and which may be merely associated with prediction outcomes due to confounding. The approach is motivated by a counterfactual theory of causal explanation wherein good explanations point to factors that are ""difference-makers"" in an interventionist sense. The resulting analysis may be useful in algorithm auditing and evaluation, by identifying features which make a causal difference to the algorithm's output.",0
"This paper aims at explaining the behavior of black box prediction algorithms using causal learning techniques. We address the gap between machine learning researchers who develop predictive models and decision makers who use them by proposing explanations that can be easily understood by non-experts. Our approach relies on a structured analysis of the relationships between features used to train predictors, as well as external factors impacting predictions. In particular, we focus on how changes in these factors affect outcomes via identifiable pathways or mechanisms. Finally, we showcase our methodology through case studies in credit risk management and medical diagnosis, demonstrating its potential to enhance trustworthiness and effectiveness of predictive systems.",1
"Hessian captures important properties of the deep neural network loss landscape. Previous works have observed low rank structure in the Hessians of neural networks. We make several new observations about the top eigenspace of layer-wise Hessian: top eigenspaces for different models have surprisingly high overlap, and top eigenvectors form low rank matrices when they are reshaped into the same shape as the corresponding weight matrix. Towards formally explaining such structures of the Hessian, we show that the new eigenspace structure can be explained by approximating the Hessian using Kronecker factorization; we also prove the low rank structure for random data at random initialization for over-parametrized two-layer neural nets. Our new understanding can explain why some of these structures become weaker when the network is trained with batch normalization. The Kronecker factorization also leads to better explicit generalization bounds.",0
"In recent years, there has been significant interest in understanding how deep neural networks process data. One aspect that has received particular attention is the role played by the Hessian matrix during backpropagation. Despite numerous studies on the subject, however, our understanding of the common structure of Hessian matrices in deep learning models remains incomplete. This paper seeks to fill this gap by dissecting the Hessian matrix and investigating its properties across different architectures and tasks. We find that certain structural patterns emerge consistently across different settings and demonstrate their importance in shaping optimization paths. Our work provides insights into how Hessians can be leveraged to improve training speed and stability, while shedding light on the fundamental mechanisms governing gradient descent based optimization procedures. These results have important implications for future research in machine learning and offer practical guidance for practitioners working on improving the performance of deep learning systems.",1
"Our objective is to detect anomalies in video while also automatically explaining the reason behind the detector's response. In a practical sense, explainability is crucial for this task as the required response to an anomaly depends on its nature and severity. However, most leading methods (based on deep neural networks) are not interpretable and hide the decision making process in uninterpretable feature representations. In an effort to tackle this problem we make the following contributions: (1) we show how to build interpretable feature representations suitable for detecting anomalies with state of the art performance, (2) we propose an interpretable probabilistic anomaly detector which can describe the reason behind it's response using high level concepts, (3) we are the first to directly consider object interactions for anomaly detection and (4) we propose a new task of explaining anomalies and release a large dataset for evaluating methods on this task. Our method competes well with the state of the art on public datasets while also providing anomaly explanation based on objects and their interactions.",0
"Include author names but no affiliation or contact details. Make references where appropriate.  The presence of anomalous frames in videos can arise from several distinct sources, such as camera sensor noise, transmission errors, data compression artifacts, corrupted pixels, postprocessing filters, and tampering attacks. Accurately identifying their root causes has numerous applications ranging from digital forensics to media quality assessment. This study addresses some key challenges associated with detecting these sources and proposes an approach called X-MAN to enhance performance over prior art methods. Our method effectively unifies several existing techniques into a single framework that performs extensive detection for each source by modeling intrinsic properties underlying normal frames. We then integrate the strengths of individual detection modules using machine learning models trained on massive amounts of clean, annotated material. Experimental results demonstrate X-Manâ€™s superiority against recent competitors. Further analyses indicate our method benefits from both complementary information across multiple sensors and accurate uncertainty estimation based on ensembles, which mitigate erroneous attributions due to randomness. Overall, we present a well-validated solution capable of identifying multiple sources at high accuracy rates, laying the foundation for future work seeking even greater robustness.",1
"We study learning algorithms for the classical Markovian bandit problem with discount. We explain how to adapt PSRL [24] and UCRL2 [2] to exploit the problem structure. These variants are called MB-PSRL and MB-UCRL2. While the regret bound and runtime of vanilla implementations of PSRL and UCRL2 are exponential in the number of bandits, we show that the episodic regret of MB-PSRL and MB-UCRL2 is $\tilde O(S\sqrt{nK})$ where $K$ is the number of episodes, n is the number of bandits and S is the number of states of each bandit (the exact bound in $S$, $n$ and $K$ is given in the paper). Up to a factor $\sqrt S$, this matches the lower bound of $\Omega(\sqrt{SnK}$) that we also derive in the paper. MB-PSRL is also computationally efficient: its runtime is linear in the number of bandits. We further show that this linear runtime cannot be achieved by adapting classical non-Bayesian algorithms such as UCRL2 or UCBVI to Markovian bandit problems. Finally, we perform numerical experiments that confirm that MB-PSRL outperforms other existing algorithms in practice, both in terms of regret and of computation time.",0
"In this study, we aimed to compare two popular methods used for approximating posterior distributions in reinforcement learning: Posterior sampling (PS) and optimism (O). We conducted experiments on different environments with varying complexity levels using state-of-the-art algorithms based on these two approaches - TD(PS) and PPO (O), respectively. Our results showed that PS outperformed O consistently across all but one environment. Furthermore, our analysis revealed that PS was significantly faster and less computationally expensive compared to O when dealing with large problem sizes and high dimensional spaces. These findings provide important insights into the scalability of approximate Bayesian inference techniques in RL and have implications for real-world applications where computational resources may be limited. Overall, our work highlights the potential advantages of PS as a viable alternative to O for solving challenging problems in reinforcement learning.",1
"As machine learning approaches are increasingly used to augment human decision-making, eXplainable Artificial Intelligence (XAI) research has explored methods for communicating system behavior to humans. However, these approaches often fail to account for the emotional responses of humans as they interact with explanations. Facial affect analysis, which examines human facial expressions of emotions, is one promising lens for understanding how users engage with explanations. Therefore, in this work, we aim to (1) identify which facial affect features are pronounced when people interact with XAI interfaces, and (2) develop a multitask feature embedding for linking facial affect signals with participants' use of explanations. Our analyses and results show that the occurrence and values of facial AU1 and AU4, and Arousal are heightened when participants fail to use explanations effectively. This suggests that facial affect analysis should be incorporated into XAI to personalize explanations to individuals' interaction styles and to adapt explanations based on the difficulty of the task performed.",0
"Title: ""Understanding facial affect analysis for explainable human-AI interactions""  Artificial intelligence (AI) has made significant strides over the past few years, leading to increased adoption of such technologies across various industries. However, one challenge that still persists is understanding how humans interact with AI systems, especially during moments where they need clarification or interpretation of results. To address this gap, we propose using facial affect analysis to better comprehend explainable human-AI interplay. This technique involves analyzing changes in human expressions to assess their emotional states while interacting with AI. By doing so, we can gain insight into areas where human interaction may require further enhancement and refinement, ultimately resulting in more transparent and effective AI experiences. Our findings demonstrate promising improvements in explainability within AI systems by implementing facial affect analysis and present opportunities for future research in developing enhanced AI models capable of adapting to individual human needs through nonverbal cues.",1
"Rapid pace of generative models has brought about new threats to visual forensics such as malicious personation and digital copyright infringement, which promotes works on fake image attribution. Existing works on fake image attribution mainly rely on a direct classification framework. Without additional supervision, the extracted features could include many content-relevant components and generalize poorly. Meanwhile, how to obtain an interpretable GAN fingerprint to explain the decision remains an open question. Adopting a multi-task framework, we propose a GAN Fingerprint Disentangling Network (GFD-Net) to simultaneously disentangle the fingerprint from GAN-generated images and produce a content-irrelevant representation for fake image attribution. A series of constraints are provided to guarantee the stability and discriminability of the fingerprint, which in turn helps content-irrelevant feature extraction. Further, we perform comprehensive analysis on GAN fingerprint, providing some clues about the properties of GAN fingerprint and which factors dominate the fingerprint in GAN architecture. Experiments show that our GFD-Net achieves superior fake image attribution performance in both closed-world and open-world testing. We also apply our method in binary fake image detection and exhibit a significant generalization ability on unseen generators.",0
"This study focuses on the emerging area of deep generative models applied within the domain of multimedia forensics. We analyze recent approaches using generative adversarial networks (GANs) as image forgery detection systems, which seek to determine whether an image has been manipulated from authentic data sources. The attribution problem of images produced by generative models such as GANs remains open, and we aim to propose a new methodology based on disentangling the latent space learned through GAN training. Our approach investigates the existence of unique features that can act as identifying markers of different classes generated via these generative models. These unique features may offer opportunities for improving fake image attribution in digital forensic applications. To achieve our goals, we employ quantitative evaluations through visual analysis to compare authentic images against synthetic counterparts obtained under controlled conditions. Experimental results demonstrate that unique patterns exist in generators learned by GANs during training; however, more work is required to fully explore this phenomenon further before real-world application becomes feasible. Nonetheless, these preliminary findings provide exciting prospects for future research into potential solutions for detecting and attribute artificially created imagery.",1
"Explaining the decisions of models is becoming pervasive in the image processing domain, whether it is by using post-hoc methods or by creating inherently interpretable models. While the widespread use of surrogate explainers is a welcome addition to inspect and understand black-box models, assessing the robustness and reliability of the explanations is key for their success. Additionally, whilst existing work in the explainability field proposes various strategies to address this problem, the challenges of working with data in the wild is often overlooked. For instance, in image classification, distortions to images can not only affect the predictions assigned by the model, but also the explanation. Given a clean and a distorted version of an image, even if the prediction probabilities are similar, the explanation may still be different. In this paper we propose a methodology to evaluate the effect of distortions in explanations by embedding perceptual distances that tailor the neighbourhoods used to training surrogate explainers. We also show that by operating in this way, we can make the explanations more robust to distortions. We generate explanations for images in the Imagenet-C dataset and demonstrate how using a perceptual distances in the surrogate explainer creates more coherent explanations for the distorted and reference images.",0
"How can you make artificial intelligence (AI) explainers more robust? This study examines how well they perform when distorted by common perceptual issues such as blurriness and noise. We investigate whether certain training techniques like adversarial examples or realism can improve their performance under these conditions. Our findings suggest that while some techniques have benefits, none of them consistently lead to improved results across all image types and models tested. These insights provide valuable guidance on future research directions aimed at making explainers more resilient to distortion.",1
"Ensemble classifiers have been investigated by many in the artificial intelligence and machine learning community. Majority voting and weighted majority voting are two commonly used combination schemes in ensemble learning. However, understanding of them is incomplete at best, with some properties even misunderstood. In this paper, we present a group of properties of these two schemes formally under a dataset-level geometric framework. Two key factors, every component base classifier's performance and dissimilarity between each pair of component classifiers are evaluated by the same metric - the Euclidean distance. Consequently, ensembling becomes a deterministic problem and the performance of an ensemble can be calculated directly by a formula. We prove several theorems of interest and explain their implications for ensembles. In particular, we compare and contrast the effect of the number of component classifiers on these two types of ensemble schemes. Empirical investigation is also conducted to verify the theoretical results when other metrics such as accuracy are used. We believe that the results from this paper are very useful for us to understand the fundamental properties of these two combination schemes and the principles of ensemble classifiers in general. The results are also helpful for us to investigate some issues in ensemble classifiers, such as ensemble performance prediction, selecting a small number of base classifiers to obtain efficient and effective ensembles.",0
"This paper presents a novel framework for ensemble classi fiers that takes into account the geometric structure among multiple datasets. By representing each dataset as a point cloud and treating the ensemble process as an interpolation problem, we can achieve superior performance compared to existing methods. Our approach outperforms traditional bagging and boosting techniques on both benchmark data sets and real world applications, achieving state-of-the-art results in several tasks including image classification, natural language processing, and speech recognition. Through extensive experiments and analysis, we demonstrate how our method improves diversity through geometry while preserving accuracy by encouraging coherency among ensembles. Additionally, we provide insights into which layers contribute most significantly to model uncertainty, allowing us to better understand the nature of the learned representations. Finally, we release our source code and pre-trained models to facilitate further research in this area. In summary, our work provides a new perspective on ensemble learning that combines geometry and machine learning, making significant contributions to both fields.",1
"Interpretability techniques aim to provide the rationale behind a model's decision, typically by explaining either an individual prediction (local explanation, e.g. `why is this patient diagnosed with this condition') or a class of predictions (global explanation, e.g. `why are patients diagnosed with this condition in general'). While there are many methods focused on either one, few frameworks can provide both local and global explanations in a consistent manner. In this work, we combine two powerful existing techniques, one local (Integrated Gradients, IG) and one global (Testing with Concept Activation Vectors), to provide local, and global concept-based explanations. We first validate our idea using two synthetic datasets with a known ground truth, and further demonstrate with a benchmark natural image dataset. We test our method with various concepts, target classes, model architectures and IG baselines. We show that our method improves global explanations over TCAV when compared to ground truth, and provides useful insights. We hope our work provides a step towards building bridges between many existing local and global methods to get the best of both worlds.",0
"This research proposes a novel framework that combines local and global explanations for deep learning models using human-understandable concepts. By integrating multiple levels of explanation, our approach improves the transparency and interpretability of model predictions, enabling better decision making across diverse application domains. Our method is based on the idea that different stakeholders have varying needs regarding explainability: while domain experts might require fine-grained understanding at specific points within the input space, non-experts may seek simplified high-level insights across the entire input range. Therefore, we propose decomposing the task into local and global explanation components that work together seamlessly, ensuring efficient communication tailored to individual preferences. We demonstrate the effectiveness of our framework through experiments on several benchmark datasets, showing improved accuracy over baseline methods. Our results indicate the potential benefits of incorporating locally interpretable and globally comprehensible representations for building trustworthy machine learning systems applicable in real-world applications.",1
"Among interpretable machine learning methods, the class of Generalised Additive Neural Networks (GANNs) is referred to as Self-Explaining Neural Networks (SENN) because of the linear dependence on explicit functions of the inputs. In binary classification this shows the precise weight that each input contributes towards the logit. The nomogram is a graphical representation of these weights. We show that functions of individual and pairs of variables can be derived from a functional Analysis of Variance (ANOVA) representation, enabling an efficient feature selection to be carried by application of the logistic Lasso. This process infers the structure of GANNs which otherwise needs to be predefined. As this method is particularly suited for tabular data, it starts by fitting a generic flexible model, in this case a Multi-layer Perceptron (MLP) to which the ANOVA decomposition is applied. This has the further advantage that the resulting GANN can be replicated as a SENN, enabling further refinement of the univariate and bivariate component functions to take place. The component functions are partial responses hence the SENN is a partial response network. The Partial Response Network (PRN) is equally as transparent as a traditional logistic regression model, but capable of non-linear classification with comparable or superior performance to the original MLP. In other words, the PRN is a fully interpretable representation of the MLP, at the level of univariate and bivariate effects. The performance of the PRN is shown to be competitive for benchmark data, against state-of-the-art machine learning methods including GBM, SVM and Random Forests. It is also compared with spline-based Sparse Additive Models (SAM) showing that a semi-parametric representation of the GAM as a neural network can be as effective as the SAM though less constrained by the need to set spline nodes.",0
"In recent years, there has been significant interest in developing new architectures for artificial intelligence (AI) models that can solve complex problems without requiring large amounts of data and computational resources. One such architecture that has gained popularity is the partial response network (PRN), which is a type of neural network that utilizes a novel training method called ""nomograms."" This paper presents the design and implementation of PRNs as well as empirical evaluation of their performance on several benchmark datasets. We show that PRNs are capable of achieving state-of-the-art results while using significantly less data and computation than other leading methods. Furthermore, we demonstrate how PRNs can be used to model nonlinear functions across different domains, making them a versatile tool for many applications in machine learning. Overall, our work represents an important contribution to the field of AI and demonstrates the potential of PRNs as a powerful alternative to traditional deep learning techniques.",1
"Self-training algorithms, which train a model to fit pseudolabels predicted by another previously-learned model, have been very successful for learning with unlabeled data using neural networks. However, the current theoretical understanding of self-training only applies to linear models. This work provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervised learning. At the core of our analysis is a simple but realistic ""expansion"" assumption, which states that a low probability subset of the data must expand to a neighborhood with large probability relative to the subset. We also assume that neighborhoods of examples in different classes have minimal overlap. We prove that under these assumptions, the minimizers of population objectives based on self-training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. By using off-the-shelf generalization bounds, we immediately convert this result to sample complexity guarantees for neural nets that are polynomial in the margin and Lipschitzness. Our results help explain the empirical successes of recently proposed self-training algorithms which use input consistency regularization.",0
"In recent years, self-training has emerged as a popular technique for deep learning, particularly when large amounts of unlabeled data are available. However, little theoretical analysis exists that explains why these methods work well in practice. This study proposes a novel framework based on mutual information maximization and uncertainty reduction to analyze how deep neural networks learn from unlabeled data during self-training. By examining the underlying principles behind self-training, we aim to gain a deeper understanding of the training process and improve existing algorithms. We demonstrate our approach by applying self-training to both image classification and natural language processing tasks using benchmark datasets. Our experimental results show significant improvements over baseline models trained solely on labeled data. Additionally, we provide insights into which architectures and regularization techniques perform best under specific conditions, providing guidance for future research directions. Overall, this study provides a comprehensive explanation for the success of self-training with deep networks and contributes valuable knowledge to the field of machine learning.",1
"Explaining the foundations for predictions obtained from graph neural networks (GNNs) is critical for credible use of GNN models for real-world problems. Owing to the rapid growth of GNN applications, recent progress in explaining predictions from GNNs, such as sensitivity analysis, perturbation methods, and attribution methods, showed great opportunities and possibilities for explaining GNN predictions. In this study, we propose a method to improve the explanation quality of node classification tasks that can be applied in a post hoc manner through aggregation of auxiliary explanations from important neighboring nodes, named SEEN. Applying SEEN does not require modification of a graph and can be used with diverse explainability techniques due to its independent mechanism. Experiments on matching motif-participating nodes from a given graph show great improvement in explanation accuracy of up to 12.71% and demonstrate the correlation between the auxiliary explanations and the enhanced explanation accuracy through leveraging their contributions. SEEN provides a simple but effective method to enhance the explanation quality of GNN model outputs, and this method is applicable in combination with most explainability techniques.",0
"Sharpening explanations for graph neural networks (GNNs) is crucial for understanding how these models make predictions on complex data structures such as graphs. In recent years, researchers have proposed several approaches to generate explanations for GNN predictions, including gradient-based methods and subgraph extraction techniques. However, these methods often suffer from limitations, such as sensitivity to input perturbations or lack of interpretability. To address these issues, we propose a novel approach called SEEN, which stands for ""Sharpening Explanations for Graph Neural Networks using Explanations from Neighborhoods."" Our method leverages explanations obtained by considering neighborhoods of individual nodes and edge attributes. We evaluate our approach through extensive experiments on four benchmark datasets commonly used to study GNN performance. Results show that SEEN outperforms existing explanation methods in terms of fidelity, robustness, simplicity, and computational efficiency. This work advances the state-of-the-art in generating explainable and accurate explanations for graph neural network predictions.",1
"Although modern machine learning and deep learning methods allow for complex and in-depth data analytics, the predictive models generated by these methods are often highly complex, and lack transparency. Explainable AI (XAI) methods are used to improve the interpretability of these complex models, and in doing so improve transparency. However, the inherent fitness of these explainable methods can be hard to evaluate. In particular, methods to evaluate the fidelity of the explanation to the underlying black box require further development, especially for tabular data. In this paper, we (a) propose a three phase approach to developing an evaluation method; (b) adapt an existing evaluation method primarily for image and text data to evaluate models trained on tabular data; and (c) evaluate two popular explainable methods using this evaluation method. Our evaluations suggest that the internal mechanism of the underlying predictive model, the internal mechanism of the explainable method used and model and data complexity all affect explanation fidelity. Given that explanation fidelity is so sensitive to context and tools and data used, we could not clearly identify any specific explainable method as being superior to another.",0
"Abstract: This research proposes developing fidelity evaluation approach for interpretable machine learning which can effectively identify discrepancies between actual behaviors and predicted/expected behaviors by humans. We introduce the conceptual foundation underlying our proposed evaluation approach, as well as present two concrete instantiations thereof. Our evaluations demonstrate that our methods improve upon existing approaches using standard benchmark datasets. The research paper ""Developing a Fidelity Evaluation Approach for Interpretable Machine Learning"" presents a novel methodology for assessing how accurately interpretability models capture human behavior. The authors argue that current evaluation techniques fail to adequately measure the extent to which these models align with human expectations, resulting in suboptimal models. To address this gap, they propose a new framework grounded in principles from cognitive science and decision making theory, which captures key dimensions of human judgment such as coherence, plausibility, comprehensiveness, complexity, robustness, adaptivity, speed, and parallelism. They then apply their framework across two case studies that demonstrate marked improvements over state-of-the-art baselines. Overall, this work represents an important contribution towards building more trustworthy and transparent interpretability models that better align with user needs and preferences.",1
"This is a tutorial and survey paper on kernels, kernel methods, and related fields. We start with reviewing the history of kernels in functional analysis and machine learning. Then, Mercer kernel, Hilbert and Banach spaces, Reproducing Kernel Hilbert Space (RKHS), Mercer's theorem and its proof, frequently used kernels, kernel construction from distance metric, important classes of kernels (including bounded, integrally positive definite, universal, stationary, and characteristic kernels), kernel centering and normalization, and eigenfunctions are explained in detail. Then, we introduce types of use of kernels in machine learning including kernel methods (such as kernel support vector machines), kernel learning by semi-definite programming, Hilbert-Schmidt independence criterion, maximum mean discrepancy, kernel mean embedding, and kernel dimensionality reduction. We also cover rank and factorization of kernel matrix as well as the approximation of eigenfunctions and kernels using the Nystr{\""o}m method. This paper can be useful for various fields of science including machine learning, dimensionality reduction, functional analysis in mathematics, and mathematical physics in quantum mechanics.",0
"This tutorial and survey article provides an introduction to reproducing kernel Hilbert spaces (RKHS), which have found numerous applications in machine learning due to their ability to represent functions through their inner products. RKHS theory has been used extensively in areas such as Gaussian processes regression, kernel principal component analysis, and kernels methods for supervised and unsupervised learning problems. Mercer's theorem plays a central role in the mathematical understanding of RKHS and we present this important result along with connections to other operators on a function space and key properties of positive definite matrices. Also crucial are eigenfunctions of a kernel operator that can provide insight into underlying structure inherent in a given dataset. These eigenfunctions form a set of linearly independent functions that constitute an orthonormal basis of the RKHS in question. The Nystr\""om method represents an efficient way to compute approximate solutions to eigenfunction equations and find low rank approximations of data using randomized techniques. Finally, we explore how kernels methods have provided new ways to improve machine learning models by mapping input variables onto nonlinear high dimensional feature spaces and constructing algorithms from these mappings. We hope our work encourages further investigation of these ideas and contributes to advancing state-of-the art research in RKHS based machine learning.",1
"Many applications of data-driven models demand transparency of decisions, especially in health care, criminal justice, and other high-stakes environments. Modern trends in machine learning research have led to algorithms that are increasingly intricate to the degree that they are considered to be black boxes. In an effort to reduce the opacity of decisions, methods have been proposed to construe the inner workings of such models in a human-comprehensible manner. These post hoc techniques are described as being universal explainers - capable of faithfully augmenting decisions with algorithmic insight. Unfortunately, there is little agreement about what constitutes a ""good"" explanation. Moreover, current methods of explanation evaluation are derived from either subjective or proxy means. In this work, we propose a framework for the evaluation of post hoc explainers on ground truth that is directly derived from the additive structure of a model. We demonstrate the efficacy of the framework in understanding explainers by evaluating popular explainers on thousands of synthetic and several real-world tasks. The framework unveils that explanations may be accurate but misattribute the importance of individual features.",0
"Title: Unifying Knowledge Distillation for Zero Shot Learning without Fine Tuning DataAbstractMost existing zero shot learning approaches require fine tuning on new domains in order to achieve good performance. However, due to privacy concerns as well as limited availability of data from other domains, such models cannot always be easily adapted to unseen scenarios, leading to reduced effectiveness in practice. To address these limitations, we propose a novel knowledge distillation framework that facilitates zero shot transfer even across significant domain shifts, by leveraging only one model pretrained in source tasks rather than relying on any additional models trained on target task data. Specifically, our method, dubbed KD-ZSL, first generates synthetic data for the target class which can then be used to train a student network while preserving most of the information learned during pretraining. Comprehensive experiments conducted on two benchmark datasets reveal KD-ZSL significantly outperforms state-of-the-art methods under both closed set (zeroshot) evaluation as well as open set (few-shot) settings, demonstrating high adaptivity and robustness to variations inherent within different datasets. Moreover, we provide theoretical insights into the operation of our proposed model via mutual information analysis, showing why cross dataset knowledge distillation effectively promotes knowledge transfer. In summary, our work represents a major step forward in advancing post hoc explainers for ZSL research, by providing an effective solution for real world deployment without requiring access to sensitive user data, thereby opening up many potential applications beyond traditional academic use cases.",1
"This document summarizes different visual explanations methods such as CAM, Grad-CAM, Localization using Multiple Instance Learning - Saliency-based methods, Saliency-driven Class-Impressions, Muting pixels in input image - Adversarial methods and Activation visualization, Convolution filter visualization - Feature-based methods. We have also shown the results produced by different methods and a comparison between CAM, GradCAM, and Guided Backpropagation.",0
"Machine learning models have become increasingly popular due to their ability to make accurate predictions on complex data sets. However, one major drawback of these models is that they often lack interpretability, making it difficult for users to understand how the model arrived at its decision. In order to address this issue, researchers have proposed several methods to explain the decisions made by machine learning models, including global sensitivity analysis and feature relevance maps. These approaches can provide valuable insights into which features contribute most strongly to the final outcome. Another approach involves identifying the most influential instances in the training set, known as case-based explanations. This method has been shown to be particularly effective in identifying anomalies and outliers in large datasets. Despite advances in explaining the decisions of machine learning models, there remains much work to be done in developing methods that can accurately capture both local and global behavior, while remaining computationally efficient and scalable to large datasets. This paper provides an overview of current methods used to explain decisions made by machine learning models and highlights areas where further research is required. Ultimately, understanding how these algorithms arrive at their predictions is critical for ensuring trustworthy AI systems.",1
"While multitask representation learning has become a popular approach in reinforcement learning (RL), theoretical understanding of why and when it works remains limited. This paper presents analyses for the statistical benefit of multitask representation learning in linear Markov Decision Process (MDP) under a generative model. In this paper, we consider an agent to learn a representation function $\phi$ out of a function class $\Phi$ from $T$ source tasks with $N$ data per task, and then use the learned $\hat{\phi}$ to reduce the required number of sample for a new task. We first discover a \emph{Least-Activated-Feature-Abundance} (LAFA) criterion, denoted as $\kappa$, with which we prove that a straightforward least-square algorithm learns a policy which is $\tilde{O}(H^2\sqrt{\frac{\mathcal{C}(\Phi)^2 \kappa d}{NT}+\frac{\kappa d}{n}})$ sub-optimal. Here $H$ is the planning horizon, $\mathcal{C}(\Phi)$ is $\Phi$'s complexity measure, $d$ is the dimension of the representation (usually $d\ll \mathcal{C}(\Phi)$) and $n$ is the number of samples for the new task. Thus the required $n$ is $O(\kappa d H^4)$ for the sub-optimality to be close to zero, which is much smaller than $O(\mathcal{C}(\Phi)^2\kappa d H^4)$ in the setting without multitask representation learning, whose sub-optimality gap is $\tilde{O}(H^2\sqrt{\frac{\kappa \mathcal{C}(\Phi)^2d}{n}})$. This theoretically explains the power of multitask representation learning in reducing sample complexity. Further, we note that to ensure high sample efficiency, the LAFA criterion $\kappa$ should be small. In fact, $\kappa$ varies widely in magnitude depending on the different sampling distribution for new task. This indicates adaptive sampling technique is important to make $\kappa$ solely depend on $d$. Finally, we provide empirical results of a noisy grid-world environment to corroborate our theoretical findings.",0
"This paper presents a novel approach to representation learning in linear Markov decision processes (MDPs) using multitask deep neural networks. We show that by training these models on multiple tasks concurrently, we can significantly improve their performance compared to single-task algorithms. Our experiments demonstrate that our method outperforms state-of-the-art methods across several benchmark domains, achieving a new state of the art in some cases. By leveraging the power of transfer learning, our algorithm offers a more efficient and effective solution for solving complex sequential decision making problems. Finally, we discuss the limitations of our approach and suggest potential directions for future work.",1
"Causal Discovery methods aim to identify a DAG structure that represents causal relationships from observational data. In this article, we stress that it is important to test such methods for robustness in practical settings. As our main example, we analyze the NOTEARS method, for which we demonstrate a lack of scale-invariance. We show that NOTEARS is a method that aims to identify a parsimonious DAG from the data that explains the residual variance. We conclude that NOTEARS is not suitable for identifying truly causal relationships from the data.",0
"This paper provides an analysis on why NOTEARS is unsuitable for causal graph discovery. In recent years, NOTEARS has emerged as one of the most popular methods for causal inference from observational data due to its efficiency, scalability, and flexibility. However, despite these benefits, we argue that NOTEARS is still limited by several factors that make it less suitable for discovering true causal graphs. We first provide a detailed overview of NOTEARS and its underlying assumptions. Then, we discuss key limitations associated with NOTEARS such as confounding and reverse causality issues. Finally, we evaluate alternative methods for causal inference and highlight their advantages over NOTEARS for discovering true causal relationships. Our results suggest that while NOTEARS can be effective under certain conditions, researchers should carefully consider other methods before applying NOTEARS to real-world problems requiring accurate causal inferences. Overall, our findings offer important insights into understanding the suitability of different methods for solving complex social science questions surrounding cause-and-effect relationships.",1
"Neural networks have been achieving high generalization performance on many tasks despite being highly over-parameterized. Since classical statistical learning theory struggles to explain this behavior, much effort has recently been focused on uncovering the mechanisms behind it, in the hope of developing a more adequate theoretical framework and having a better control over the trained models. In this work, we adopt an alternate perspective, viewing the neural network as a dynamical system displacing input particles over time. We conduct a series of experiments and, by analyzing the network's behavior through its displacements, we show the presence of a low kinetic energy displacement bias in the transport map of the network, and link this bias with generalization performance. From this observation, we reformulate the learning problem as follows: finding neural networks which solve the task while transporting the data as efficiently as possible. This offers a novel formulation of the learning problem which allows us to provide regularity results for the solution network, based on Optimal Transport theory. From a practical viewpoint, this allows us to propose a new learning algorithm, which automatically adapts to the complexity of the given task, and leads to networks with a high generalization ability even in low data regimes.",0
"This paper presents a new principle for training neural networks that leads to significant improvements in accuracy and efficiency. The proposed method, called ""Principle of Least Action,"" leverages recent advances in deep learning theory to find optimal paths through weight space while minimizing a novel action functional. Unlike traditional approaches that rely on gradient descent alone, our approach exploits additional symmetries and structure present within neural network architectures. Experimental results show that the Principle of Least Action consistently outperforms state-of-the-art methods across a range of benchmark datasets and architectures, often achieving up to 4% absolute improvement in accuracy. We believe that the Principle of Least Action represents a promising step towards finding a unified framework for understanding the behavior of neural networks. By providing a more efficient means for optimizing these models, we hope to open up exciting possibilities for real-world applications in fields such as computer vision, natural language processing, and robotics.",1
"Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.",0
"In recent years, neural networks have become increasingly popular due to their ability to solve complex problems and achieve state-of-the-art results on many tasks. However, training these models requires careful calibration to ensure that they perform well on new data. This paper revisits the current approaches used to calibrate modern neural networks and proposes improvements to enhance their performance and generalization abilities. By analyzing different methods and techniques, we identify key areas where existing practices can be refined and propose novel solutions to address them. Our work contributes to the field by providing insights into the limitations of current calibration strategies and offering concrete suggestions for advancing our understanding of how to effectively fine-tune neural networks. Ultimately, our research has implications for both theoretical and applied machine learning researchers who seek to improve their models through more accurate calibration.",1
"Recently, the \textit{Tensor Nuclear Norm~(TNN)} regularization based on t-SVD has been widely used in various low tubal-rank tensor recovery tasks. However, these models usually require smooth change of data along the third dimension to ensure their low rank structures. In this paper, we propose a new definition of data dependent tensor rank named \textit{tensor Q-rank} by a learnable orthogonal matrix $\mathbf{Q}$, and further introduce a unified data dependent low rank tensor recovery model. According to the low rank hypothesis, we introduce two explainable selection method of $\mathbf{Q}$, under which the data tensor may have a more significant low tensor Q-rank structure than that of low tubal-rank structure. Specifically, maximizing the variance of singular value distribution leads to Variance Maximization Tensor Q-Nuclear norm~(VMTQN), while minimizing the value of nuclear norm through manifold optimization leads to Manifold Optimization Tensor Q-Nuclear norm~(MOTQN). Moreover, we apply these two models to the low rank tensor completion problem, and then give an effective algorithm and briefly analyze why our method works better than TNN based methods in the case of complex data with low sampling rate. Finally, experimental results on real-world datasets demonstrate the superiority of our proposed model in the tensor completion problem with respect to other tensor rank regularization models.",0
"In recent years, tensor rank has emerged as an important concept in many fields, including machine learning, signal processing, computer vision, and neuroscience. However, existing definitions of tensor rank have several limitations that make them difficult to use in practice. For example, some definitions require complete knowledge of the data, while others can only handle tensors of low rank. To address these issues, we propose a new definition of tensor rank called Tensor Q-Rank. Unlike other approaches, Tensor Q-Rank takes into account the characteristics of the underlying data and adapts accordingly, resulting in more accurate estimates of tensor rank. We show through extensive experiments on both synthetic and real datasets that our method outperforms state-of-the-art methods in terms of accuracy, efficiency, and robustness. Our work has important implications for researchers working on problems where high-dimensional data are present, such as medical imaging and natural language processing, among others.",1
"In the field of eXplainable AI (XAI), robust ""blackbox"" algorithms such as Convolutional Neural Networks (CNNs) are known for making high prediction performance. However, the ability to explain and interpret these algorithms still require innovation in the understanding of influential and, more importantly, explainable features that directly or indirectly impact the performance of predictivity. A number of methods existing in literature focus on visualization techniques but the concepts of explainability and interpretability still require rigorous definition. In view of the above needs, this paper proposes an interaction-based methodology -- Influence Score (I-score) -- to screen out the noisy and non-informative variables in the images hence it nourishes an environment with explainable and interpretable features that are directly associated to feature predictivity. We apply the proposed method on a real world application in Pneumonia Chest X-ray Image data set and produced state-of-the-art results. We demonstrate how to apply the proposed approach for more general big data problems by improving the explainability and interpretability without sacrificing the prediction performance. The contribution of this paper opens a novel angle that moves the community closer to the future pipelines of XAI problems.",0
"In this paper we propose a novel method that enables users to interactively explore AI explanations of pneumonia chest x-ray images. We employ a state-of-the-art model pre-trained on large scale image datasets, which captures both local and global relationships between radiographic findings. To enhance user understanding, our method employs a two-step process: firstly, extracting informative regions from the input image; secondly, using these informative regions to generate detailed and accurate descriptions. Our experiments show improved performance compared to other methods, including better accuracy at generating relevant key phrases. Additionally, evaluation metrics show enhanced user preference for our approach. Overall, our work contributes towards making medical diagnosis more accessible through clear interpretive visualizations and transparent models.",1
"An increasing number of machine learning models have been deployed in domains with high stakes such as finance and healthcare. Despite their superior performances, many models are black boxes in nature which are hard to explain. There are growing efforts for researchers to develop methods to interpret these black-box models. Post hoc explanations based on perturbations, such as LIME, are widely used approaches to interpret a machine learning model after it has been built. This class of methods has been shown to exhibit large instability, posing serious challenges to the effectiveness of the method itself and harming user trust. In this paper, we propose S-LIME, which utilizes a hypothesis testing framework based on central limit theorem for determining the number of perturbation points needed to guarantee stability of the resulting explanation. Experiments on both simulated and real world data sets are provided to demonstrate the effectiveness of our method.",0
"This paper presents a novel approach for explaining machine learning models using SHapley Additive exPlanations (SHAP) with LIME stabilization (S-LIME). Existing methods like SHAP and Local Interpretable Model-agnostic Explanations (LIME) are known to produce inconsistent explanations and may lead to incorrect conclusions if applied incorrectly. To address these issues, we propose a new method called S-LIME that combines SHAP and LIME in a stable and consistent manner. We show through empirical evaluations on multiple datasets that our method outperforms existing state-of-the-art approaches by producing more accurate explanations. Our contributions provide valuable insights into understanding model behavior and help practitioners make informed decisions based on reliable interpretations of their machine learning models.",1
"In recent years the ubiquitous deployment of AI has posed great concerns in regards to algorithmic bias, discrimination, and fairness. Compared to traditional forms of bias or discrimination caused by humans, algorithmic bias generated by AI is more abstract and unintuitive therefore more difficult to explain and mitigate. A clear gap exists in the current literature on evaluating and mitigating bias in pruned neural networks. In this work, we strive to tackle the challenging issues of evaluating, mitigating, and explaining induced bias in pruned neural networks. Our paper makes three contributions. First, we propose two simple yet effective metrics, Combined Error Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively evaluate the induced bias prevention quality of pruned models. Second, we demonstrate that knowledge distillation can mitigate induced bias in pruned neural networks, even with unbalanced datasets. Third, we reveal that model similarity has strong correlations with pruning induced bias, which provides a powerful method to explain why bias occurs in pruned neural networks. Our code is available at https://github.com/codestar12/pruning-distilation-bias",0
"This paper investigates the problem of bias in pruned neural networks and proposes a novel approach called ""Simon says"" that uses knowledge distillation to mitigate it. We show that standard pruning techniques can lead to significant biases in the resulting models and argue that these biases must be taken into account if we want to achieve good performance. Our proposed method tackles this issue by training the full model alongside the pruned one during knowledge transfer using cross entropy loss instead of KL divergence. Experiments on several benchmark datasets demonstrate the effectiveness of our method compared to other state-of-the-art approaches. In addition, we provide analysis of why other methods fail, contributing insights into understanding bias in pruned neural networks. Overall, our work advances the field of model compression and provides new directions for future research.",1
"As machine learning (ML) systems take a more prominent and central role in contributing to life-impacting decisions, ensuring their trustworthiness and accountability is of utmost importance. Explanations sit at the core of these desirable attributes of a ML system. The emerging field is frequently called ``Explainable AI (XAI)'' or ``Explainable ML.'' The goal of explainable ML is to intuitively explain the predictions of a ML system, while adhering to the needs to various stakeholders. Many explanation techniques were developed with contributions from both academia and industry. However, there are several existing challenges that have not garnered enough interest and serve as roadblocks to widespread adoption of explainable ML. In this short paper, we enumerate challenges in explainable ML from an industry perspective. We hope these challenges will serve as promising future research directions, and would contribute to democratizing explainable ML.",0
"Title: ""The Limitations of Explainability in Machine Learning""  Artificial Intelligence (AI) has become increasingly important in modern society as industries continue to rely on machine learning algorithms to drive decision making processes. As these models are integrated into critical systems, ensuring their interpretability and explainability becomes paramount to ensure trustworthiness and accountability. While there have been numerous efforts towards developing explainable models, this study examines the pitfalls that arise in practice from industry perspective.  We conducted interviews with practitioners who use machine learning in their daily tasks across multiple domains such as healthcare, finance, manufacturing and government organizations. Our findings highlight several key challenges faced by the industry in relation to model explainability. Firstly, despite growing awareness of the importance of explainability, industry participants report a lack of understanding of how to evaluate interpretability methods and integrate them into workflows effectively. Secondly, there exists confusion surrounding the types of explanations required based on different user needs. Thirdly, current methods for generating explanations may lead to significant computational costs, which can hinder real-time applications. Lastly, users often require explanation in specific contexts like natural language generation, visualizations etc., but currently available explainability techniques are limited in providing meaningful outputs for humans.  In conclusion, while there has been progress made in improving explainability in machine learning, our research underscores the need for more effective collaboration between academia and industry to address the limitations identified in practice. Future directions should focus on creating scalable and flexible frameworks that incorporate domain expertise, addressing communication gaps between technical experts and non-experts, and facilitating better accessibility to relevant information and education resources for stakeholders involved in deploying interpretable models in practice.",1
"Counterfactual explanations (CFEs) are an emerging technique under the umbrella of interpretability of machine learning (ML) models. They provide ``what if'' feedback of the form ``if an input datapoint were $x'$ instead of $x$, then an ML model's output would be $y'$ instead of $y$.'' Counterfactual explainability for ML models has yet to see widespread adoption in industry. In this short paper, we posit reasons for this slow uptake. Leveraging recent work outlining desirable properties of CFEs and our experience running the ML wing of a model monitoring startup, we identify outstanding obstacles hindering CFE deployment in industry.",0
"Title: ""Counterfactual Explorations in Machine Learning: New Insights and Perspectives""  Abstract: The ability to provide counterfactual explanations (CFEs) has become increasingly important as machine learning models continue to play a larger role in decision making processes across diverse domains such as healthcare, finance, law enforcement, and education. CFEs allow us to explore the reasoning process behind these decisions by identifying factors that could have led to alternative outcomes. In recent years, several approaches have been proposed for generating CFEs from machine learning models, each addressing their own unique set of challenges. However, despite significant progress made in this area, there remain some critical questions unanswered. This paper provides a comprehensive overview of the current state of research on CFEs for machine learning. We identify key challenges faced in realizing CFEs, examine how different approaches tackle these issues, and present new insights into the fundamental properties underlying the construction of meaningful CFEs. Our work contributes towards building robust frameworks for explaining complex machine learning pipelines in practice, ultimately leading to more trustworthy artificial intelligence systems. We believe this research can pave the pathway towards enhancing human understanding of black box predictive algorithms and empower stakeholders with effective tools to analyze their consequences.",1
"With the rise in edge-computing devices, there has been an increasing demand to deploy energy and resource-efficient models. A large body of research has been devoted to developing methods that can reduce the size of the model considerably without affecting the standard metrics such as top-1 accuracy. However, these pruning approaches tend to result in a significant mismatch in other metrics such as fairness across classes and explainability. To combat such misalignment, we propose a novel multi-part loss function inspired by the knowledge-distillation literature. Through extensive experiments, we demonstrate the effectiveness of our approach across different compression algorithms, architectures, tasks as well as datasets. In particular, we obtain up to $4.1\times$ reduction in the number of prediction mismatches between the compressed and reference models, and up to $5.7\times$ in cases where the reference model makes the correct prediction; all while making no changes to the compression algorithm, and minor modifications to the loss function. Furthermore, we demonstrate how inducing simple alignment between the predictions of the models naturally improves the alignment on other metrics including fairness and attributions. Our framework can thus serve as a simple plug-and-play component for compression algorithms in the future.",0
"In recent years, model compression has emerged as an essential technique for reducing computational resource requirements without compromising accuracy. Traditionally, the success of compressed models has been evaluated using classification accuracy metrics such as precision, recall, and F1 score. However, these metrics only provide a limited understanding of how well a compressed model performs.  In this paper, we propose several new approaches that go beyond traditional accuracy metrics and provide deeper insights into the performance of compressed models. We evaluate the quality of compressed models based on multiple criteria including visual interpretability, robustness against input perturbations, and sensitivity analysis. Our methods allow us to gain a more comprehensive picture of the strengths and weaknesses of compressed models compared to their uncompressed counterparts.  Our experimental results demonstrate the effectiveness of our proposed approaches in evaluating compressed models. By combining traditional accuracy metrics with additional evaluation techniques, we can achieve a better understanding of the trade-offs involved in compressing neural networks while still maintaining high levels of performance. This work paves the way for future research aimed at developing even more advanced model compression algorithms that take into account the full range of factors affecting model performance.",1
"Fairwashing refers to the risk that an unfair black-box model can be explained by a fairer model through post-hoc explanations' manipulation. However, to realize this, the post-hoc explanation model must produce different predictions than the original black-box on some inputs, leading to a decrease in the fidelity imposed by the difference in unfairness. In this paper, our main objective is to characterize the risk of fairwashing attacks, in particular by investigating the fidelity-unfairness trade-off. First, we demonstrate through an in-depth empirical study on black-box models trained on several real-world datasets and for several statistical notions of fairness that it is possible to build high-fidelity explanation models with low unfairness. For instance, we find that fairwashed explanation models can exhibit up to $99.20\%$ fidelity to the black-box models they explain while being $50\%$ less unfair. These results suggest that fidelity alone should not be used as a proxy for the quality of black-box explanations. Second, we show that fairwashed explanation models can generalize beyond the suing group (\emph{i.e.}, data points that are being explained), which will only worsen as more stable fairness methods get developed. Finally, we demonstrate that fairwashing attacks can transfer across black-box models, meaning that other black-box models can perform fairwashing without explicitly using their predictions.",0
"""Characterizing"" sounds like an odd noun choice here. Would you consider changing that? -----> Risk characterization is essential for making informed decisions about how much money or other resources should be spent on managing risks. Fairness and equity concerns can play a role in these decisions, but there has been little systematic effort to quantify or model such factors at larger scales. We develop a framework for evaluating both monetized costs (from property damage) and non-monetary impacts from climate change events using data from two coastal communities exposed to different degrees of flooding hazards. The framework integrates social vulnerability indicators, which we proxy as income inequality within neighborhoods, into our calculations of annual expected damages and fatalities associated with rising sea levels. We find that high levels of income inequality increase aggregate costs to society while decreasing resilience within affected neighborhoods over time, even if local governments invest heavily in adaptation measures. Our results suggest that decision frameworks must account for the tradeoffs involved when balancing efforts across regions to allocate resources based on need versus willingness to pay. These insights have implications for the broader discussion regarding responsibility and liability issues surrounding the socioeconomic dimensions of climate risk management.",1
"Backdoor attacks inject poisoning samples during training, with the goal of enforcing a machine-learning model to output an attacker-chosen class when presented a specific trigger at test time. Although backdoor attacks have been demonstrated in a variety of settings and against different models, the factors affecting their success are not yet well understood. In this work, we provide a unifying framework to study the process of backdoor learning under the lens of incremental learning and influence functions. We show that the success of backdoor attacks inherently depends on (i) the complexity of the learning algorithm, controlled by its hyperparameters, and (ii) the fraction of backdoor samples injected into the training set. These factors affect how fast a machine-learning model learns to correlate the presence of a backdoor trigger with the target class. Interestingly, our analysis shows that there exists a region in the hyperparameter space in which the accuracy on clean test samples is still high while backdoor attacks become ineffective, thereby suggesting novel criteria to improve existing defenses.",0
"Recent research on backdoor attacks has focused heavily on measuring how effective these poisoned examples can be at altering the output of machine learning models. However, there is little understanding of why backdoors work or what makes them effective beyond the influence functions used by attackers to manipulate their outputs. We seek to address this gap by studying two types of backdoor learning curves that provide insight into both the effectiveness of influence functions as well as the role of data augmentation and model complexity in determining poison sizes required for successful attacks. Our findings have important implications for defending against backdoor poisoning in real systems. Keywords: Backdoor Attacks, Adversarial Examples, Machine Learning Security, Data Augmentation AbstractBackdoor attacks represent one of the most challenging threats facing modern machine learning systems today, due to the difficulty in detecting and mitigating these malicious inputs. Current research largely focuses on evaluating the effectiveness of different methods in terms of causing misclassification rates of target classes. However, our understanding of why backdoors succeed remains limited. This study addresses this gap by examining the relationships among several factors that contribute to backdoor success, including dataset size, influence function strength, dataset augmentation, model architecture, feature selection, data preprocessing techniques such as normalization, batch norm, etc., and other relevant factors. Through our analysis, we demonstrate a new interpretation of existing results that explains why smaller datasets generally require more influential features (stronger IFs) to cause misclassifications than larger ones. By testing multiple versions of each factor, we obtain novel insights into the tradeoffs involved i",1
"We propose a BlackBox \emph{Counterfactual Explainer} that is explicitly developed for medical imaging applications. Classical approaches (e.g. saliency maps) assessing feature importance do not explain \emph{how} and \emph{why} variations in a particular anatomical region is relevant to the outcome, which is crucial for transparent decision making in healthcare application. Our framework explains the outcome by gradually \emph{exaggerating} the semantic effect of the given outcome label. Given a query input to a classifier, Generative Adversarial Networks produce a progressive set of perturbations to the query image that gradually changes the posterior probability from its original class to its negation. We design the loss function to ensure that essential and potentially relevant details, such as support devices, are preserved in the counterfactually generated images. We provide an extensive evaluation of different classification tasks on the chest X-Ray images. Our experiments show that a counterfactually generated visual explanation is consistent with the disease's clinical relevant measurements, both quantitatively and qualitatively.",0
"""Explaining the black box smoothly: A counterfactual approach"" proposes a novel methodology that addresses the issue of interpretability of machine learning models by leveraging counterfactual reasoning. Specifically, we present a framework that utilizes cause-effect pairs derived from domain knowledge and user feedback to generate counterfactuals instances for any given input instance, which can then be used as explanations of how a model arrived at its prediction. We empirically demonstrate the effectiveness of our framework on several real world datasets across different domains such as healthcare, finance and image classification tasks. Our contributions are twofold: firstly, we introduce a principled and interpretable method to explain decision making in ML models through counterfactual examples; secondly, unlike prior works that rely on heuristics or require access to the internal workings of the model, our framework only relies on input data and external causal features, making it widely applicable to any model. Overall, our research highlights the importance of incorporating human understanding into automated decision making processes in order to make them more trustworthy and acceptable to users.",1
"Machine Learning (ML) models are being used in all facets of today's society to make high stake decisions like bail granting or credit lending, with very minimal regulations. Such systems are extremely vulnerable to both propagating and amplifying social biases, and have therefore been subject to growing research interest. One of the main issues with conventional fairness metrics is their narrow definitions which hide the complete extent of the bias by focusing primarily on positive and/or negative outcomes, whilst not paying attention to the overall distributional shape. Moreover, these metrics are often contradictory to each other, are severely restrained by the contextual and legal landscape of the problem, have technical constraints like poor support for continuous outputs, the requirement of class labels, and are not explainable.   In this paper, we present Quantile Demographic Drift, which addresses the shortcomings mentioned above. This metric can also be used to measure intra-group privilege. It is easily interpretable via existing attribution techniques, and also extends naturally to individual fairness via the principle of like-for-like comparison. We make this new fairness score the basis of a new system that is designed to detect bias in production ML models without the need for labels. We call the system FairCanary because of its capability to detect bias in a live deployed model and narrow down the alert to the responsible set of features, like the proverbial canary in a coal mine.",0
"The author presents an approach called FairCanary for improving explainability and reducing human bias in machine learning models by combining fairness constraints, interpretability methods such as SHAP (SHapley Additive exPlanations), and a novel method that uses randomized feature ablation to approximate Shapley values and quantify feature importance. By enforcing fairness at different stages of model training, they show how to mitigate discriminatory behavior while retaining predictive performance. They also demonstrate real-world use cases of their method on multiple datasets, including recidivism prediction and hiring decisions, showing significant improvement over baseline algorithms. Overall, FairCanary provides a promising step towards achieving reliable and unbiased predictions from modern machine learning systems.",1
"Given high-dimensional time series data (e.g., sensor data), how can we detect anomalous events, such as system faults and attacks? More challengingly, how can we do this in a way that captures complex inter-sensor relationships, and detects and explains anomalies which deviate from these relationships? Recently, deep learning approaches have enabled improvements in anomaly detection in high-dimensional datasets; however, existing methods do not explicitly learn the structure of existing relationships between variables, or use them to predict the expected behavior of time series. Our approach combines a structure learning approach with graph neural networks, additionally using attention weights to provide explainability for the detected anomalies. Experiments on two real-world sensor datasets with ground truth anomalies show that our method detects anomalies more accurately than baseline approaches, accurately captures correlations between sensors, and allows users to deduce the root cause of a detected anomaly.",0
"Abstract: This paper presents a novel approach to detecting anomalies in multivariate time series data using graph neural networks (GNNs). Traditional approaches to anomaly detection have relied on feature engineering and handcrafted statistical models. However, these methods can struggle with high-dimensional, complex datasets that lack clear domain knowledge. In contrast, GNNs offer an effective alternative by learning nonlinear representations directly from raw data. Our proposed method integrates GNNs into a two-step framework consisting of local and global anomaly detection stages. The local stage applies individual one-class support vector machines (SVM) to each variable independently, while the global stage utilizes a graph convolutional network (GCN) to capture dependencies across variables. Experiments conducted on multiple benchmark datasets demonstrate our method's superior performance compared to state-of-the-art baselines. We further analyze the effectiveness of each component through extensive ablation studies, demonstrating the robustness and interpretability of our model. Overall, our work provides valuable insights into the application of graph deep learning techniques for real-world anomaly detection problems. Keywords: Graph Neural Networks, Anomaly Detection, One-Class Support Vector Machines, Multivariate Time Series, Deep Learning",1
"The field of Explainable Artificial Intelligence (XAI) aims to build explainable and interpretable machine learning (or deep learning) methods without sacrificing prediction performance. Convolutional Neural Networks (CNNs) have been successful in making predictions, especially in image classification. However, these famous deep learning models use tens of millions of parameters based on a large number of pre-trained filters which have been repurposed from previous data sets. We propose a novel Interaction-based Convolutional Neural Network (ICNN) that does not make assumptions about the relevance of local information. Instead, we use a model-free Influence Score (I-score) to directly extract the influential information from images to form important variable modules. We demonstrate that the proposed method produces state-of-the-art prediction performance of 99.8% on a real-world data set classifying COVID-19 Chest X-ray images without sacrificing the explanatory power of the model. This proposed design can efficiently screen COVID-19 patients before human diagnosis, and will be the benchmark for addressing future XAI problems in large-scale data sets.",0
"This paper presents a novel method for analyzing Covid-19 chest x-ray images using a convolutional neural network (CNN). Our approach differs from existing methods as it uses an interaction-based CNN architecture that models the interactions between image patches instead of treating each pixel independently. We demonstrate through experiments on large datasets consisting of labeled images that our proposed ICNN model outperforms state-of-the-art baselines while requiring fewer parameters. Overall, our work shows great potential in helping radiologists make better diagnostic decisions by providing more accurate predictions on patient scans.",1
"With the widespread deployment of large-scale prediction systems in high-stakes domains, e.g., face recognition, criminal justice, etc., disparity in prediction accuracy between different demographic subgroups has called for fundamental understanding on the source of such disparity and algorithmic intervention to mitigate it. In this paper, we study the accuracy disparity problem in regression. To begin with, we first propose an error decomposition theorem, which decomposes the accuracy disparity into the distance between marginal label distributions and the distance between conditional representations, to help explain why such accuracy disparity appears in practice. Motivated by this error decomposition and the general idea of distribution alignment with statistical distances, we then propose an algorithm to reduce this disparity, and analyze its game-theoretic optima of the proposed objective functions. To corroborate our theoretical findings, we also conduct experiments on five benchmark datasets. The experimental results suggest that our proposed algorithms can effectively mitigate accuracy disparity while maintaining the predictive power of the regression models.",0
"Title: ""Regressions, Reality, and Risk"" (Full Text) Authors: John Smith, Jane Brown, Jack White Abstract: Understanding the accuracy disparity present in regression analyses requires careful consideration of the underlying assumptions of the modeling process. This article examines how departures from normality can result in systematic errors across multiple datasets. We discuss how risk assessment methods may mitigate these discrepancies by adjusting parameter estimates according to historical outcomes. Our analysis includes a review of classical measurement error models as well as more recent extensions that incorporate unobserved heterogeneity factors such as omitted variables, data quality issues, or endogenous sample selection into the estimation framework. We propose strategies to address bias arising from limited accessibility of key predictor variables through supplemental instrumental variable approaches. Finally, we provide recommendations on selecting appropriate error structures based on observable characteristics, acknowledging trade-offs among different estimators under varying conditions. Overall, our findings suggest that carefully considering the sources of uncertainty in regression results enables better understanding of their limitations while improving predictive performance in high-risk environments.",1
"Sepsis is a potentially life threatening inflammatory response to infection or severe tissue damage. It has a highly variable clinical course, requiring constant monitoring of the patient's state to guide the management of intravenous fluids and vasopressors, among other interventions. Despite decades of research, there's still debate among experts on optimal treatment. Here, we combine for the first time, distributional deep reinforcement learning with mechanistic physiological models to find personalized sepsis treatment strategies. Our method handles partial observability by leveraging known cardiovascular physiology, introducing a novel physiology-driven recurrent autoencoder, and quantifies the uncertainty of its own results. Moreover, we introduce a framework for uncertainty aware decision support with humans in the loop. We show that our method learns physiologically explainable, robust policies that are consistent with clinical knowledge. Further our method consistently identifies high risk states that lead to death, which could potentially benefit from more frequent vasopressor administration, providing valuable guidance for future research",0
"In recent years, sepsis treatment has become increasingly reliant on advanced modeling techniques that can accurately predict patient responses to therapy. However, these models often struggle to account for the high degree of uncertainty inherent in critical care settings. To address this challenge, we propose a novel approach that combines cardiovascular modelling with deep reinforcement learning to create a robust, adaptive decision support system. By leveraging both approaches together, our framework can effectively handle uncertain environments while optimizing treatment outcomes. Our method involves training a neural network to act as a centralized controller by gathering data from real patients experiencing varying levels of septic shock. We demonstrate how the learned policy guides the system towards the optimal treatment plan under different conditions, resulting in improved treatment effectiveness even during highly volatile episodes. Furthermore, our proposed technique is capable of identifying previously unknown relationships between key factors influencing survival rates, opening up exciting opportunities for future research. Ultimately, our work paves the way for more effective management of complex medical issues through the synergy of diverse modelling strategies and machine learning algorithms.",1
"For certain infinitely-wide neural networks, the neural tangent kernel (NTK) theory fully characterizes generalization. However, for the networks used in practice, the empirical NTK represents only a rough first-order approximation of these architectures. Still, a growing body of work keeps leveraging this approximation to successfully analyze important deep learning phenomena and derive algorithms for new applications. In our work, we provide strong empirical evidence to determine the practical validity of such approximation by conducting a systematic comparison of the behaviour of different neural networks and their linear approximations on different tasks. We show that the linear approximations can indeed rank the learning complexity of certain tasks for neural networks, albeit with important nuances. Specifically, we discover that, in contrast to what was previously observed, neural networks do not always perform better than their kernel approximations, and reveal that their performance gap heavily depends on architecture, number of samples and training task. In fact, we show that during training, deep networks increase the alignment of their empirical NTK with the target task, which explains why linear approximations at the end of training can better explain the dynamics of deep networks. Overall, our work provides concrete examples of novel deep learning phenomena which can inspire future theoretical research, as well as provides a new perspective on the use of the NTK approximation in deep learning.",0
"Linearized neural networks (LNNs) have recently gained popularity due to their ability to capture complex relationships between input variables and predict output values with high accuracy. However, little is known about the fundamental principles underlying LNNs and how they achieve such good performance. In this work, we investigate the relationship between linearization techniques used in LNNs and generalization performance on real-world data sets. Our findings suggest that there may indeed be surprisingly deep connections between these two seemingly separate issues. We provide theoretical insights into the conditions under which one type of model family based on polynomial regression gives superior results compared to another using logistic regression as well as explain why some models require stronger regularization to prevent overfitting than others. These insights could potentially guide future research on improving LNN architectures and training methods, ultimately leading to more robust machine learning models.",1
"Traditional evaluation metrics for learned models that report aggregate scores over a test set are insufficient for surfacing important and informative patterns of failure over features and instances. We introduce and study a method aimed at characterizing and explaining failures by identifying visual attributes whose presence or absence results in poor performance. In distinction to previous work that relies upon crowdsourced labels for visual attributes, we leverage the representation of a separate robust model to extract interpretable features and then harness these features to identify failure modes. We further propose a visualization method aimed at enabling humans to understand the meaning encoded in such features and we test the comprehensibility of the features. An evaluation of the methods on the ImageNet dataset demonstrates that: (i) the proposed workflow is effective for discovering important failure modes, (ii) the visualization techniques help humans to understand the extracted features, and (iii) the extracted insights can assist engineers with error analysis and debugging.",0
"This paper investigates the behavior of deep neural networks (DNNs) on several popular benchmark datasets and shows that these models are sensitive to small perturbations to their inputs, which can lead them to make incorrect predictions even when the original input is preserved. Our study reveals that DNNs often fail because they rely heavily on subtle features present only in specific areas of images and ignore other regions containing relevant information. We propose new feature extraction techniques based on data augmentation to overcome these limitations and boost accuracy across a wide range of tasks, including image classification, object detection, and segmentation. Moreover, our approach outperforms state-of-the-art methods without relying on more complex architectures or larger training sets, demonstrating its effectiveness at improving robustness under real-world conditions. Overall, we believe our findings shed light on key challenges faced by current DNNs and provide valuable insights into how researchers can design better models for a variety of applications.",1
"The ""cold posterior effect"" (CPE) in Bayesian deep learning describes the uncomforting observation that the predictive performance of Bayesian neural networks can be significantly improved if the Bayes posterior is artificially sharpened using a temperature parameter T1. The CPE is problematic in theory and practice and since the effect was identified many researchers have proposed hypotheses to explain the phenomenon. However, despite this intensive research effort the effect remains poorly understood. In this work we provide novel and nuanced evidence relevant to existing explanations for the cold posterior effect, disentangling three hypotheses: 1. The dataset curation hypothesis of Aitchison (2020): we show empirically that the CPE does not arise in a real curated data set but can be produced in a controlled experiment with varying curation strength. 2. The data augmentation hypothesis of Izmailov et al. (2021) and Fortuin et al. (2021): we show empirically that data augmentation is sufficient but not necessary for the CPE to be present. 3. The bad prior hypothesis of Wenzel et al. (2020): we use a simple experiment evaluating the relative importance of the prior and the likelihood, strongly linking the CPE to the prior. Our results demonstrate how the CPE can arise in isolation from synthetic curation, data augmentation, and bad priors. Cold posteriors observed ""in the wild"" are therefore unlikely to arise from a single simple cause; as a result, we do not expect a simple ""fix"" for cold posteriors.",0
"This research explores how different factors contribute to the phenomenon known as the cold posterior effect (CPE). In particular, we investigate the roles of curation, data augmentation, and prior distributions on model uncertainty. We find that all three factors can have significant impacts on the magnitude of the CPE. However, our results indicate that the choice of prior distribution has the most profound influence. By carefully selecting priors that accurately reflect our knowledge of the world, we can mitigate the effects of the CPE and improve the accuracy of our models. Our findings highlight the importance of careful consideration of prior distributions in Bayesian inference tasks and may provide insights into other phenomena related to model confidence calibration.",1
"The ability to handle large scale variations is crucial for many real world visual tasks. A straightforward approach for handling scale in a deep network is to process an image at several scales simultaneously in a set of scale channels. Scale invariance can then, in principle, be achieved by using weight sharing between the scale channels together with max or average pooling over the outputs from the scale channels. The ability of such scale channel networks to generalise to scales not present in the training set over significant scale ranges has, however, not previously been explored.   In this paper, we present a systematic study of this methodology by implementing different types of scale channel networks and evaluating their ability to generalise to previously unseen scales. We develop a formalism for analysing the covariance and invariance properties of scale channel networks, and explore how different design choices, unique to scaling transformations, affect the overall performance of scale channel networks. We first show that two previously proposed scale channel network designs do not generalise well to scales not present in the training set. We explain theoretically and demonstrate experimentally why generalisation fails in these cases.   We then propose a new type of foveated scale channel architecture}, where the scale channels process increasingly larger parts of the image with decreasing resolution. This new type of scale channel network is shown to generalise extremely well, provided sufficient image resolution and the absence of boundary effects. Our proposed FovMax and FovAvg networks perform almost identically over a scale range of 8, also when training on single scale training data, and do also give improved performance when learning from datasets with large scale variations in the small sample regime.",0
"This paper presents a new architecture called ""scale-invariant scale-channel networks"" which demonstrate state of the art performance on the object detection task across a wide range of objects sizes in cluttered scenes without the need to train on each object size separately. By applying different convolutional kernels to different feature maps based on their resolutions we build an implicit scale pyramid, allowing the network to seamlessly adapt to images of vastly varying scales during inference time. Our approach builds upon recent advances in deep learning such as Feature Pyramidal Networks (FPN) while achieving results competitive to Faster R-CNN models with region proposalnetworks. We provide comprehensive experiments and ablation studies to illustrate the effectiveness our method, making it an attractive choice for real world deployment scenarios where fine grained control over input image scaling may not always be possible or desired. Overall, these results showcase the promise of our novel scale-invariant scale-channel networks in enabling efficient and accurate object detection at larger scales, particularly in complex environments where smaller objects must be detected among cluttered backgrounds.",1
"Generalization is one of the critical issues in machine learning. However, traditional methods like uniform convergence are not powerful enough to fully explain generalization because they may yield vacuous bounds even in overparameterized linear regression regimes. An alternative solution is to analyze the generalization dynamics to derive algorithm-dependent bounds, e.g., stability. Unfortunately, the stability-based bound is still far from explaining the remarkable generalization ability of neural networks due to the coarse-grained analysis of the signal and noise. Inspired by the observation that neural networks show a slow convergence rate when fitting noise, we propose decomposing the excess risk dynamics and applying stability-based bound only on the variance part (which measures how the model performs on pure noise). We provide two applications for the framework, including a linear case (overparameterized linear regression with gradient descent) and a non-linear case (matrix recovery with gradient flow). Under the decomposition framework, the new bound accords better with the theoretical and empirical evidence compared to the stability-based bound and uniform convergence bound.",0
"This paper investigates how to decompose excess risk dynamics into their fundamental components so as to gain insight into generalization behavior. Our contributions can be summarized as follows: (1) We study the decomposability properties of various definitions of stability, including some that have recently been used in machine learning theory; (2) we characterize the relationship between decomposability and uniform convergence rates; (3) we present several examples where our decomposition results give rise to new insights into the generalizability of models trained under different conditions. Overall, this work contributes towards developing a more complete understanding of how overfitting arises during training and how one may quantify the effectiveness of regularizers or other techniques aimed at reducing it. Our findings provide tools which could enable practitioners to diagnose issues related to poor generalization and suggest remedies for improvement. Code associated with our work is available publicly.",1
"Incorporating a so-called ""momentum"" dynamic in gradient descent methods is widely used in neural net training as it has been broadly observed that, at least empirically, it often leads to significantly faster convergence. At the same time, there are very few theoretical guarantees in the literature to explain this apparent acceleration effect. Even for the classical strongly convex quadratic problems, several existing results only show Polyak's momentum has an accelerated linear rate asymptotically. In this paper, we first revisit the quadratic problems and show a non-asymptotic accelerated linear rate of Polyak's momentum. Then, we provably show that Polyak's momentum achieves acceleration for training a one-layer wide ReLU network and a deep linear network, which are perhaps the two most popular canonical models for studying optimization and deep learning in the literature. Prior work Du at al. 2019 and Wu et al. 2019 showed that using vanilla gradient descent, and with an use of over-parameterization, the error decays as $(1- \Theta(\frac{1}{ \kappa'}))^t$ after $t$ iterations, where $\kappa'$ is the condition number of a Gram Matrix. Our result shows that with the appropriate choice of parameters Polyak's momentum has a rate of $(1-\Theta(\frac{1}{\sqrt{\kappa'}}))^t$. For the deep linear network, prior work Hu et al. 2020 showed that vanilla gradient descent has a rate of $(1-\Theta(\frac{1}{\kappa}))^t$, where $\kappa$ is the condition number of a data matrix. Our result shows an acceleration rate $(1- \Theta(\frac{1}{\sqrt{\kappa}}))^t$ is achievable by Polyak's momentum. All the results in this work are obtained from a modular analysis, which can be of independent interest. This work establishes that momentum does indeed speed up neural net training.",0
"This sounds like a interesting research project that attempts to analyze the effectiveness of two different training methods (Provable Acceleration via Polyak's Momentum) by testing them on both wide ReLU networks and deep linear networks, using modular analysis techniques. Without more details I am unable to fully summarize your work but I encourage you to provide additional information if desired.",1
"Boundary based blackbox attack has been recognized as practical and effective, given that an attacker only needs to access the final model prediction. However, the query efficiency of it is in general high especially for high dimensional image data. In this paper, we show that such efficiency highly depends on the scale at which the attack is applied, and attacking at the optimal scale significantly improves the efficiency. In particular, we propose a theoretical framework to analyze and show three key characteristics to improve the query efficiency. We prove that there exists an optimal scale for projective gradient estimation. Our framework also explains the satisfactory performance achieved by existing boundary black-box attacks. Based on our theoretical framework, we propose Progressive-Scale enabled projective Boundary Attack (PSBA) to improve the query efficiency via progressive scaling techniques. In particular, we employ Progressive-GAN to optimize the scale of projections, which we call PSBA-PGAN. We evaluate our approach on both spatial and frequency scales. Extensive experiments on MNIST, CIFAR-10, CelebA, and ImageNet against different models including a real-world face recognition API show that PSBA-PGAN significantly outperforms existing baseline attacks in terms of query efficiency and attack success rate. We also observe relatively stable optimal scales for different models and datasets. The code is publicly available at https://github.com/AI-secure/PSBA.",0
"Adversarial attacks on machine learning models have gained significant attention due to their ability to bypass state-of-the-art defenses, resulting in high confusion rates and incorrect predictions. One common attack methodology is the black box scenario, where the attacker only has access to model inputs/outputs pairs without knowing any internal details of the targeted classifier. In recent years, several studies have focused on developing new black box methods that can effectively generate adversarial examples even under strong constraints such as limited queries or query budgets. However, most existing works assume fixed budgets and do not consider adapting to dynamic conditions during testing, which may lead to suboptimal results in some cases. To address these limitations, we propose a novel algorithm called Progressive-scale Boundary Blackbox (PSBB) attack. Our approach starts from coarse levels, gradually refines objectives and boundaries over iterations, and applies projective gradient estimation at each stage. With our proposed adaptive boundary search strategy, PSBB achieves higher success rates compared to previous non-adaptive counterparts while maintaining competitive transferability across different architectures and datasets. We further demonstrate that PSBB outperforms other state-of-the-art black box methods on both image classification and semantic segmentation tasks. Overall, our work presents progress towards building more robust deep neural networks and raises awareness regarding potential vulnerabilities in commonly used systems. ---------------------------",1
"Motivated by the vast success of deep convolutional networks, there is a great interest in generalizing convolutions to non-Euclidean manifolds. A major complication in comparison to flat spaces is that it is unclear in which alignment a convolution kernel should be applied on a manifold. The underlying reason for this ambiguity is that general manifolds do not come with a canonical choice of reference frames (gauge). Kernels and features therefore have to be expressed relative to arbitrary coordinates. We argue that the particular choice of coordinatization should not affect a network's inference -- it should be coordinate independent. A simultaneous demand for coordinate independence and weight sharing is shown to result in a requirement on the network to be equivariant under local gauge transformations (changes of local reference frames). The ambiguity of reference frames depends thereby on the G-structure of the manifold, such that the necessary level of gauge equivariance is prescribed by the corresponding structure group G. Coordinate independent convolutions are proven to be equivariant w.r.t. those isometries that are symmetries of the G-structure. The resulting theory is formulated in a coordinate free fashion in terms of fiber bundles. To exemplify the design of coordinate independent convolutions, we implement a convolutional network on the M\""obius strip. The generality of our differential geometric formulation of convolutional networks is demonstrated by an extensive literature review which explains a large number of Euclidean CNNs, spherical CNNs and CNNs on general surfaces as specific instances of coordinate independent convolutions.",0
"In ""Coordinate Independent Convolutional Networks"", we present a novel framework for convolutional networks that operate directly on input data in a coordinate independent manner. This allows us to train deep neural networks on smooth manifolds using simple backpropagation algorithms without relying on explicit gauge fixing. We achieve this by defining isometric equivariant convolutional kernels which preserve distances between points on the manifold while still allowing local interactions between neighboring patches. Our experiments show that our method can significantly outperform traditional approaches that rely on global parameterizations such as maps to Euclidean space. Additionally, we demonstrate that our network generalizes well across different charts for the same manifold and even works on noncompact spaces like hyperbolic geometry. Overall, these results indicate great promise for applying deep learning techniques to problems in mathematical sciences that involve curved domains.",1
"Selecting diverse and important items, called landmarks, from a large set is a problem of interest in machine learning. As a specific example, in order to deal with large training sets, kernel methods often rely on low rank matrix Nystr\""om approximations based on the selection or sampling of landmarks. In this context, we propose a deterministic and a randomized adaptive algorithm for selecting landmark points within a training data set, which are related to the minima of a sequence of kernelized Christoffel functions. Beyond the known connection between Christoffel functions and leverage scores, a connection of our method with determinantal point processes (DPPs) is also explained. Namely, our construction promotes diversity among important landmark points in a way similar to DPPs. Also, we explain how our randomized adaptive algorithm can influence the accuracy of Kernel Ridge Regression.",0
"In this work, we study a class of sampling methods based on NystrÃ¶m interpolation called NystrÃ¶m landmark sampling. We show that these methods can produce accurate approximations for operators that act as linear combinations of singular integral operators of convolution type with kernels having certain smoothness properties near the origin. To achieve such accuracy, we introduce regularization techniques into the definition of the associated Christoffel function, which allows us to control the behavior of the integrand near the diagonal in Fourier space. Our results provide new theoretical insights into the design of efficient algorithms for approximating multivariate integration problems involving non-smooth kernels. As applications of our findings, we present numerical experiments demonstrating that NystrÃ¶m landmark sampling combined with regularized Christoffel functions outperforms existing state-of-the-art methods for several classes of high-dimensional approximation problems.",1
"Certified defenses based on convex relaxations are an established technique for training provably robust models. The key component is the choice of relaxation, varying from simple intervals to tight polyhedra. Paradoxically, however, training with tighter relaxations can often lead to worse certified robustness. The poor understanding of this paradox has forced recent state-of-the-art certified defenses to focus on designing various heuristics in order to mitigate its effects. In contrast, in this paper we study the underlying causes and show that tightness alone may not be the determining factor. Concretely, we identify two key properties of relaxations that impact training dynamics: continuity and sensitivity. Our extensive experimental evaluation demonstrates that these two factors, observed alongside tightness, explain the drop in certified robustness for popular relaxations. Further, we investigate the possibility of designing and training with relaxations that are tight, continuous and not sensitive. We believe the insights of this work can help drive the principled discovery of new and effective certified defense mechanisms.",0
"This paper investigates how changes in training data certification can affect model performance. Recent advances have introduced tighter relaxation bounds on adversarial examples, which aim to improve robustness against attacks. However, our findings show that these stricter constraints can lead to degraded accuracy during training, especially for deep models. We propose several possible causes for this phenomenon and provide evidence through experiments using popular datasets such as ImageNet and CIFAR-10. Additionally, we analyze different optimization methods and observe similar patterns across various architectures. Our work highlights the importance of balancing certifiability and trainability in designing effective defenses for machine learning systems.",1
"Necessity and sufficiency are the building blocks of all successful explanations. Yet despite their importance, these notions have been conceptually underdeveloped and inconsistently applied in explainable artificial intelligence (XAI), a fast-growing research area that is so far lacking in firm theoretical foundations. Building on work in logic, probability, and causality, we establish the central role of necessity and sufficiency in XAI, unifying seemingly disparate methods in a single formal framework. We provide a sound and complete algorithm for computing explanatory factors with respect to a given context, and demonstrate its flexibility and competitive performance against state of the art alternatives on various tasks.",0
"In this paper we unify theory and practice by introducing local explanations through necessity and sufficiency. We show that the use of these classical logic connectives allows us to explain complex relationships in simple, intuitive ways that can then be used to guide decision making. By using mathematical frameworks like category theory we develop novel methods for understanding causality and generalizing from case studies. Our approach brings together perspectives from philosophy, statistics, mathematics, computer science, sociology and psychology to provide powerful new tools for explaining complex systems. These tools have applications across a variety of domains including physics, biology, engineering, social sciences, humanities and more. Importantly our framework can integrate qualitative and quantitative approaches providing a holistic view of reality. Ultimately our work provides a foundation for knowledge generation, explanation and prediction and hence has implications for artificial intelligence, robotics, language processing, education, policy analysis, financial forecasting and many other areas.",1
"Local surrogate approaches for explaining machine learning model predictions have appealing properties, such as being model-agnostic and flexible in their modelling. Several methods exist that fit this description and share this goal. However, despite their shared overall procedure, they set out different objectives, extract different information from the black-box, and consequently produce diverse explanations, that are -- in general -- incomparable. In this work we review the similarities and differences amongst multiple methods, with a particular focus on what information they extract from the model, as this has large impact on the output: the explanation. We discuss the implications of the lack of agreement, and clarity, amongst the methods' objectives on the research and practice of explainability.",0
"Explaining machine learning models has become increasingly important as these models continue to play a greater role in our lives. Many research efforts have focused on developing interpretable models that can provide more transparent explanations of their predictions. In recent years, there has been growing interest in ""local surrogates"" - simpler proxies that approximate some aspect of the complex model's behavior locally in feature space. Local surrogates aim to capture relevant relationships between features and predictions, making them easier to interpret than the full black box models they represent. However, there remains an overlooked problem: how should we define the objectives of explaining through local surrogates? This paper presents a framework for specifying and evaluating different types of explanation objectives, highlighting the importance of considering the target audience, available resources and desired level of detail among other factors. By addressing this unexplored challenge, we hope to advance the development and application of effective local-surrogate explainers.",1
"Direct Feedback Alignment (DFA) is emerging as an efficient and biologically plausible alternative to the ubiquitous backpropagation algorithm for training deep neural networks. Despite relying on random feedback weights for the backward pass, DFA successfully trains state-of-the-art models such as Transformers. On the other hand, it notoriously fails to train convolutional networks. An understanding of the inner workings of DFA to explain these diverging results remains elusive. Here, we propose a theory for the success of DFA. We first show that learning in shallow networks proceeds in two steps: an alignment phase, where the model adapts its weights to align the approximate gradient with the true gradient of the loss function, is followed by a memorisation phase, where the model focuses on fitting the data. This two-step process has a degeneracy breaking effect: out of all the low-loss solutions in the landscape, a network trained with DFA naturally converges to the solution which maximises gradient alignment. We also identify a key quantity underlying alignment in deep linear networks: the conditioning of the alignment matrices. The latter enables a detailed understanding of the impact of data structure on alignment, and suggests a simple explanation for the well-known failure of DFA to train convolutional neural networks. Numerical experiments on MNIST and CIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and show that the align-then-memorise process occurs sequentially from the bottom layers of the network to the top.",0
"Learning through trial and error has been shown to work better than rote teaching for many tasks. However, for some problems, such as reinforcement learning (RL) agents learning from sparse rewards, even a well specified reward function can lead to difficult learning paths, since they may receive no signal at all. To alleviate these difficulties, we explore different approaches that align RL agents with their objectives by modifying the agentâ€™s objective during training. We first describe why these modifications might be necessary, outline two general approaches to objective modification, discuss our experiments on MuJoCo locomotion benchmarks with the Proximal Policy Optimization algorithm, compare them to prior methods, and identify strengths and weaknesses of each approach. Our results suggest objective modifications have the potential to make RL more sample efficient but at times trade off final performance compared to the standard method. Overall our findings provide insight into how we can improve alignment techniques and enable effective learning on challenging high dimensional continuous control problems.",1
"In this paper, we propose a density estimation algorithm called \textit{Gradient Boosting Histogram Transform} (GBHT), where we adopt the \textit{Negative Log Likelihood} as the loss function to make the boosting procedure available for the unsupervised tasks. From a learning theory viewpoint, we first prove fast convergence rates for GBHT with the smoothness assumption that the underlying density function lies in the space $C^{0,\alpha}$. Then when the target density function lies in spaces $C^{1,\alpha}$, we present an upper bound for GBHT which is smaller than the lower bound of its corresponding base learner, in the sense of convergence rates. To the best of our knowledge, we make the first attempt to theoretically explain why boosting can enhance the performance of its base learners for density estimation problems. In experiments, we not only conduct performance comparisons with the widely used KDE, but also apply GBHT to anomaly detection to showcase a further application of GBHT.",0
This will give me trouble if there are no such options at all! So I have to assume that the given choices are correct? Or can you please provide more detailed and accurate requirements for me to fulfill your request correctly?,1
"We show that viewing graphs as sets of node features and incorporating structural and positional information into a transformer architecture is able to outperform representations learned with classical graph neural networks (GNNs). Our model, GraphiT, encodes such information by (i) leveraging relative positional encoding strategies in self-attention scores based on positive definite kernels on graphs, and (ii) enumerating and encoding local sub-structures such as paths of short length. We thoroughly evaluate these two ideas on many classification and regression tasks, demonstrating the effectiveness of each of them independently, as well as their combination. In addition to performing well on standard benchmarks, our model also admits natural visualization mechanisms for interpreting graph motifs explaining the predictions, making it a potentially strong candidate for scientific applications where interpretation is important. Code available at https://github.com/inria-thoth/GraphiT.",0
"Recent years have seen rapid progress on graph neural networks (GNNs), which aim at modeling relational data such as social media graphs and knowledge bases by encoding graph structure via recursive neighborhood aggregation. However, little work has been done on understanding why GNNs work well in practice, especially towards capturing different patterns present within graph structures, and how to further improve them. In this paper, we take two steps forward and propose GraphiT â€” a new framework that can encode any pattern in graph structure into a transformer network architecture without altering the original message passing algorithm. We validate the efficacy of GraphiT through extensive experiments across five benchmark datasets spanning multiple domains including biological protein interaction networks, citation graphs, chemical compound graphs, social media graphs, and large knowledge graphs like DBpedia and YAGO2. Our approach significantly outperforms all competitive baselines while consuming up to only half or one quarter of their parameters. The source code, trained models and supplementary materials will be made available upon publication. This paper makes several contributions:  We first design a novel transformer encoder called Graphormer that replaces every self-attention layer with our proposed Graph Fusion Layer (GFL) to inject complex graph structural relationships directly into multi-head attention computations without affecting message passing operations. Specifically, given a set of query vertices Q and target vertices T, GFL attends over Tâ€™s neighbors N(T) conditioned on whether they belong to each specific subset S_q âŠ† Q determined by the query vertex q âˆˆ Q. We then combine these attended neighbor embeddings along with the corresponding edge features (if available) to achieve both efficiency and expressiveness. For instance, our framework effortlessly generates random walks and path-based methods wi",1
"We present To The Point (TTP), a method for reconstructing 3D objects from a single image using 2D to 3D correspondences learned from weak supervision. We recover a 3D shape from a 2D image by first regressing the 2D positions corresponding to the 3D template vertices and then jointly estimating a rigid camera transform and non-rigid template deformation that optimally explain the 2D positions through the 3D shape projection. By relying on 3D-2D correspondences we use a simple per-sample optimization problem to replace CNN-based regression of camera pose and non-rigid deformation and thereby obtain substantially more accurate 3D reconstructions. We treat this optimization as a differentiable layer and train the whole system in an end-to-end manner. We report systematic quantitative improvements on multiple categories and provide qualitative results comprising diverse shape, pose and texture prediction examples. Project website: https://fkokkinos.github.io/to_the_point/.",0
"This sounds like a fascinating topic! Can you give me more context on what ""correspondence-driven"" refers to? Without knowing that I cannot effectively write an abstract but I can make some suggestions if you want! Additionally would you prefer first person point of view or third person point of view? If you can provide any details at all I can definitely guide you through writing the perfect abstract!",1
"This study proposes an innovative explainable predictive quality analytics solution to facilitate data-driven decision-making for process planning in manufacturing by combining process mining, machine learning, and explainable artificial intelligence (XAI) methods. For this purpose, after integrating the top-floor and shop-floor data obtained from various enterprise information systems, a deep learning model was applied to predict the process outcomes. Since this study aims to operationalize the delivered predictive insights by embedding them into decision-making processes, it is essential to generate relevant explanations for domain experts. To this end, two complementary local post-hoc explanation approaches, Shapley values and Individual Conditional Expectation (ICE) plots are adopted, which are expected to enhance the decision-making capabilities by enabling experts to examine explanations from different perspectives. After assessing the predictive strength of the applied deep neural network with relevant binary classification evaluation measures, a discussion of the generated explanations is provided.",0
"This paper presents a method for post-hoc explanation generation in the context of predictive process monitoring (PPM) systems used in manufacturing. These PPM systems use machine learning algorithms to generate predictions that can be utilized by operators to improve their decision making during production processes. However, these models often lack transparency and interpretability, leading to difficulties in understanding why certain decisions were made. Therefore, there is a need for methods that enable local explanations at runtime without sacrificing accuracy. We propose a method that combines sensitivity analysis techniques with local approximation procedures to provide interpretable explanations without having access to complex derivatives or fine-grained simulation data. Our experimental results demonstrate significant improvements over existing approaches while providing detailed insights into key factors impacting product quality. Finally, we discuss how our approach can support efficient troubleshooting, enhance operator trustworthiness towards automation solutions, and contribute to improved productivity on the shop floor.",1
"Adoption of deep neural networks in fields such as economics or finance has been constrained by the lack of interpretability of model outcomes. This paper proposes a generative neural network architecture - the parameter encoder neural network (PENN) - capable of estimating local posterior distributions for the parameters of a regression model. The parameters fully explain predictions in terms of the inputs and permit visualization, interpretation and inference in the presence of complex heterogeneous effects and feature dependencies. The use of Bayesian inference techniques offers an intuitive mechanism to regularize local parameter estimates towards a stable solution, and to reduce noise-fitting in settings of limited data availability. The proposed neural network is particularly well-suited to applications in economics and finance, where parameter inference plays an important role. An application to an asset pricing problem demonstrates how the PENN can be used to explore nonlinear risk dynamics in financial markets, and to compare empirical nonlinear effects to behavior posited by financial theory.",0
"This could be useful if you wanted to search for related papers or grant applications to read more about this topic. This article outlines the design and implementation of a novel neural network architecture that enables interpretability for modeling real-world problems. Our approach introduces a new mechanism called Attentive Relational Memory (ARM) which incorporates prior knowledge into deep learning models at the inference stage. By enhancing attentional mechanisms with ARM, we can learn highly interpretable relationships in both visual and textual datasets while improving accuracy over state-of-the-art methods on benchmark tasks such as image classification and question answering. We demonstrate how our method effectively captures important features and provides robustness against input perturbations compared to existing attention-based architectures. To provide further analysis of these benefits, extensive experiments were conducted and results show significant improvements for different domains using our proposed interpretable framework. This work contributes valuable insights towards developing intelligible models, bridging the gap between human intelligence and artificial intelligence, making them suitable for high-stakes decisions in real-life scenarios.",1
"We consider counterfactual explanations for private support vector machines (SVM), where the privacy mechanism that publicly releases the classifier guarantees differential privacy. While privacy preservation is essential when dealing with sensitive data, there is a consequent degradation in the classification accuracy due to the introduced perturbations in the classifier weights. For such classifiers, counterfactual explanations need to be robust against the uncertainties in the SVM weights in order to ensure, with high confidence, that the classification of the data instance to be explained is different than its explanation. We model the uncertainties in the SVM weights through a random vector, and formulate the explanation problem as an optimization problem with probabilistic constraint. Subsequently, we characterize the problem's deterministic equivalent and study its solution. For linear SVMs, the problem is a convex second-order cone program. For non-linear SVMs, the problem is non-convex. Thus, we propose a sub-optimal solution that is based on the bisection method. The results show that, contrary to non-robust explanations, the quality of explanations from the robust solution degrades with increasing privacy in order to guarantee a prespecified confidence level for correct classifications.",0
"In recent years there has been increased interest in using support vector machines (SVM) as a tool for pattern recognition. One problem that arises from using SVMs is that they can produce unexpected results if they have access to training data containing private information. To address this issue, we propose a new method based on adversarial learning called ""Robust Explanations for Private Support Vector Machines."" Our method works by adding noise to the output of the SVM before training, which forces the machine to learn to explain itself while still performing well on test examples. We demonstrate through experiments that our approach achieves better privacy protection than existing methods without sacrificing accuracy.",1
"How can we explain the predictions of a machine learning model? When the data is structured as a multivariate time series, this question induces additional difficulties such as the necessity for the explanation to embody the time dependency and the large number of inputs. To address these challenges, we propose dynamic masks (Dynamask). This method produces instance-wise importance scores for each feature at each time step by fitting a perturbation mask to the input sequence. In order to incorporate the time dependency of the data, Dynamask studies the effects of dynamic perturbation operators. In order to tackle the large number of inputs, we propose a scheme to make the feature selection parsimonious (to select no more feature than necessary) and legible (a notion that we detail by making a parallel with information theory). With synthetic and real-world data, we demonstrate that the dynamic underpinning of Dynamask, together with its parsimony, offer a neat improvement in the identification of feature importance over time. The modularity of Dynamask makes it ideal as a plug-in to increase the transparency of a wide range of machine learning models in areas such as medicine and finance, where time series are abundant.",0
"This study presents a novel method for explaining time series predictions using dynamic masks. The proposed approach leverages temporal attention mechanisms to weigh different features at different points in time, resulting in more interpretable explanations that highlight relevant input values and patterns. Experimental results on several real-world datasets show that our method significantly outperforms competing methods in terms of both prediction accuracy and interpretability. Additionally, we demonstrate how dynamic masks can be used for local interpretation of predictions by focusing attention on specific segments of the input sequence. Overall, our work represents an important step towards achieving explainable artificial intelligence for time series forecasting models, enabling users to better understand their predictions and make more informed decisions.",1
"Recent efforts to unravel the mystery of implicit regularization in deep learning have led to a theoretical focus on matrix factorization -- matrix completion via linear neural network. As a step further towards practical deep learning, we provide the first theoretical analysis of implicit regularization in tensor factorization -- tensor completion via certain type of non-linear neural network. We circumvent the notorious difficulty of tensor problems by adopting a dynamical systems perspective, and characterizing the evolution induced by gradient descent. The characterization suggests a form of greedy low tensor rank search, which we rigorously prove under certain conditions, and empirically demonstrate under others. Motivated by tensor rank capturing the implicit regularization of a non-linear neural network, we empirically explore it as a measure of complexity, and find that it captures the essence of datasets on which neural networks generalize. This leads us to believe that tensor rank may pave way to explaining both implicit regularization in deep learning, and the properties of real-world data translating this implicit regularization to generalization.",0
"Artificial intelligence (AI) has seen tremendous advances over recent years due to the increased availability of data and improvements in machine learning algorithms. Among these advancements, tensor factorization techniques have gained popularity as they allow for efficient storage and analysis of high-dimensional data. However, such methods can suffer from overfitting, which leads to poor generalization performance on new unseen data. In order to address this issue, implicit regularization strategies that avoid explicit model complexity penalties have been proposed. These approaches instead implicitly restrict model capacity by exploiting specific properties of the problem at hand. This paper presents a comprehensive survey of current research in the field of tensor factorization with implicit regularization. We discuss how different techniques have been applied to various domains and highlight their advantages and limitations. Our aim is to provide an up-to-date overview of the state of the art and serve as a resource for both researchers and practitioners interested in this area. Ultimately, we hope our findings will inspire future work towards developing more effective and robust implicit regularization schemes.",1
"To perform well on unseen and potentially out-of-distribution samples, it is desirable for machine learning models to have a predictable response with respect to transformations affecting the factors of variation of the input. Invariance is commonly achieved through hand-engineered data augmentation, but do standard data augmentations address transformations that explain variations in real data? While prior work has focused on synthetic data, we attempt here to characterize the factors of variation in a real dataset, ImageNet, and study the invariance of both standard residual networks and the recently proposed vision transformer with respect to changes in these factors. We show standard augmentation relies on a precise combination of translation and scale, with translation recapturing most of the performance improvement -- despite the (approximate) translation invariance built in to convolutional architectures, such as residual networks. In fact, we found that scale and translation invariance was similar across residual networks and vision transformer models despite their markedly different inductive biases. We show the training data itself is the main source of invariance, and that data augmentation only further increases the learned invariances. Interestingly, the invariances brought from the training process align with the ImageNet factors of variation we found. Finally, we find that the main factors of variation in ImageNet mostly relate to appearance and are specific to each class.",0
"This study explores how inductive biases present in deep neural networks (DNNs) enable them to learn representations that generalize well across different domains. Specifically, we investigate whether DNNs trained on large amounts of natural image data implicitly encode prior knowledge about the structure and variability inherent in these datasets. By analyzing the behavior of models as they converge on solutions, we demonstrate that learned invariances emerge through interactions between network architecture and naturally occurring patterns in the training data. Our findings suggest that the ability of DNNs to achieve zero-shot generalization relies less on their capacity to memorize patterns explicitly, but rather on their innate sensitivity to statistical regularities intrinsic to real-world data distributions. Our work contributes to a more nuanced understanding of the mechanisms underlying artificial intelligence (AI), particularly those related to AI systems' abilities to perform tasks without explicit guidance. Overall, our results have important implications for both theory development in machine learning and applications that require high levels of robustness and adaptability.",1
"Domains such as manufacturing and medicine crave for continuous monitoring and analysis of their processes, especially in combination with time series as produced by sensors. Time series data can be exploited to, for example, explain and predict concept drifts during runtime. Generally, a certain data volume is required in order to produce meaningful analysis results. However, reliable data sets are often missing, for example, if event streams and times series data are collected separately, in case of a new process, or if it is too expensive to obtain a sufficient data volume. Additional challenges arise with preparing time series data from multiple event sources, variations in data collection frequency, and concept drift. This paper proposes the GENLOG approach to generate reliable event and time series data that follows the distribution of the underlying input data set. GENLOG employs data resampling and enables the user to select different parts of the log data to orchestrate the training of a recurrent neural network for stream generation. The generated data is sampled back to its original sample rate and is embedded into the originating log data file. Overall, GENLOG can boost small data sets and consequently the application of online process mining.",0
"Abstract:  This paper presents a novel approach that uses neural networks to generate reliable process event streams and time series data from sensor measurements. Existing techniques rely heavily on handcrafted features which can often lead to unreliability due to changes in operational conditions, measurement noise, etc. Our method addresses these limitations by using a lightweight convolutional neural network (CNN) architecture to learn complex representations directly from raw sensory inputs, allowing us to capture subtle relationships in the dataset. We demonstrate the effectiveness of our framework through several experiments conducted on real-world datasets including image classification, sentiment analysis, speech recognition, and natural language understanding. Results show significant improvement over baseline methods across all tasks. Overall, we believe our work represents an important step towards more robust and accurate generation of process events and time series data for a wide range of applications.",1
"Providing early diagnosis of cerebral palsy (CP) is key to enhancing the developmental outcomes for those affected. Diagnostic tools such as the General Movements Assessment (GMA), have produced promising results in early diagnosis, however these manual methods can be laborious.   In this paper, we propose a new framework for the automated classification of infant body movements, based upon the GMA, which unlike previous methods, also incorporates a visualization framework to aid with interpretability. Our proposed framework segments extracted features to detect the presence of Fidgety Movements (FMs) associated with the GMA spatiotemporally. These features are then used to identify the body-parts with the greatest contribution towards a classification decision and highlight the related body-part segment providing visual feedback to the user.   We quantitatively compare the proposed framework's classification performance with several other methods from the literature and qualitatively evaluate the visualization's veracity. Our experimental results show that the proposed method performs more robustly than comparable techniques in this setting whilst simultaneously providing relevant visual interpretability.",0
"This paper proposes a body-part based framework for identifying abnormal infant movements by predicting the likelihood of occurrence for different body parts using multiple deep learning models trained on raw sensor data from wearables such as wristwatches or headbands. By leveraging explainability techniques and visualizations that can provide insights into how and why these predictions occur, we aim to support clinical decision making and early detection of potential health conditions in infants.",1
"An exciting recent development is the uptake of deep learning in many scientific fields, where the objective is seeking novel scientific insights and discoveries. To interpret a learning outcome, researchers perform hypothesis testing for explainable features to advance scientific domain knowledge. In such a situation, testing for a blackbox learner poses a severe challenge because of intractable models, unknown limiting distributions of parameter estimates, and high computational constraints. In this article, we derive two consistent tests for the feature relevance of a blackbox learner. The first one evaluates a loss difference with perturbation on an inference sample, which is independent of an estimation sample used for parameter estimation in model fitting. The second further splits the inference sample into two but does not require data perturbation. Also, we develop their combined versions by aggregating the order statistics of the $p$-values based on repeated sample splitting. To estimate the splitting ratio and the perturbation size, we develop adaptive splitting schemes for suitably controlling the Type \rom{1} error subject to computational constraints. By deflating the \textit{bias-sd-ratio}, we establish asymptotic null distributions of the test statistics and their consistency in terms of statistical power. Our theoretical power analysis and simulations indicate that the one-split test is more powerful than the two-split test, though the latter is easier to apply for large datasets. Moreover, the combined tests are more stable while compensating for a power loss by repeated sample splitting. Numerically, we demonstrate the utility of the proposed tests on two benchmark examples. Accompanying this paper is our Python library {\tt dnn-inference} https://dnn-inference.readthedocs.io/en/latest/ that implements the proposed tests.",0
"Title: Feature Relevance Testing for Black Box Learners  This study presents significant advances towards determining the relevance of features used by black box learners, such as deep neural networks, which can significantly improve their performance on targeted tasks. Accurately assessing whether each input variable contributes meaningfully towards the predicted outcome plays a crucial role in model interpretability and optimization. Existing techniques have major drawbacks including unfeasible computation demands, poor scalability, insensitivity to high variance inputs, difficulty measuring nonlinear relationships, inconsistencies across datasets, or only providing yes/no answers rather than quantifying levels of importance. We propose several novel significance testing methods designed specifically for features employed in these models. Our approaches overcome many limitations by incorporating data randomization to control variation, ensembling multiple trained models, addressing linearity concerns, adaptively selecting thresholds for robustness, producing confidence intervals reflecting uncertainty estimates, accommodating both categorical and continuous variables, and working effectively on realistically large problems. Empirical comparisons against leading alternatives demonstrate our new procedures achieve better results more consistently, further validated through external validation studies on numerous datasets commonly used throughout machine learning research. This work paves the way towards comprehensive evaluations of feature relevance for complex black box models enabling more informed decisions when deploying them into mission critical applications.",1
"Saliency maps have shown to be both useful and misleading for explaining model predictions especially in the context of images. In this paper, we perform sanity checks for text modality and show that the conclusions made for image do not directly transfer to text. We also analyze the effects of the input multiplier in certain saliency maps using similarity scores, max-sensitivity and infidelity evaluation metrics. Our observations reveal that the input multiplier carries input's structural patterns in explanation maps, thus leading to similar results regardless of the choice of model parameters. We also show that the smoothness of a Neural Network (NN) function can affect the quality of saliency-based explanations. Our investigations reveal that replacing ReLUs with Softplus and MaxPool with smoother variants such as LogSumExp (LSE) can lead to explanations that are more reliable based on the infidelity evaluation metric.",0
"In recent years, deep learning models have achieved remarkable performance on various tasks, especially image classification. Saliency maps generated by these models provide insight into how they make predictions, but their interpretability has been questioned due to the lack of ground truth annotations required during training. To address this issue, we propose investigating the sanity of saliency maps through comparison with human annotations, as well as evaluation using both images and text inputs. We present experimental results that demonstrate the validity of our approach and discuss future directions for research in this area. Our work contributes to efforts towards making deep learning more interpretable and transparent.",1
"Deep learning techniques are increasingly being adopted for classification tasks over the past decade, yet explaining how deep learning architectures can achieve state-of-the-art performance is still an elusive goal. While all the training information is embedded deeply in a trained model, we still do not understand much about its performance by only analyzing the model. This paper examines the neuron activation patterns of deep learning-based classification models and explores whether the models' performances can be explained through neurons' activation behavior. We propose two approaches: one that models neurons' activation behavior as a graph and examines whether the neurons form meaningful communities, and the other examines the predictability of neurons' behavior using entropy. Our comprehensive experimental study reveals that both the community quality (modularity) and entropy are closely related to the deep learning models' performances, thus paves a novel way of explaining deep learning models directly from the neurons' activation pattern.",0
"Abstract: In recent years, deep learning has emerged as a powerful tool for modeling complex patterns from large datasets. At the core of these models lies the artificial neuronal network which processes raw data into meaningful outputs through several layers of interconnected nodes called neurons. Understanding how neuron communities form and evolve within such networks can provide valuable insights into their function and behavior. This work presents research on the evolution of neuron communities over time using a novel approach that captures the state of each neuron community during training via entropy maps. Our results show that the evolution process involves multiple phases where distinct types of neuron populations coexist, interact, and eventually transition into more specialized forms as the model converges. These findings have important implications for our understanding of neural network dynamics and open new directions for future research.",1
"Limited expert time is a key bottleneck in medical imaging. Due to advances in image classification, AI can now serve as decision-support for medical experts, with the potential for great gains in radiologist productivity and, by extension, public health. However, these gains are contingent on building and maintaining experts' trust in the AI agents. Explainable AI may build such trust by helping medical experts to understand the AI decision processes behind diagnostic judgements. Here we introduce and evaluate explanations based on Bayesian Teaching, a formal account of explanation rooted in the cognitive science of human learning. We find that medical experts exposed to explanations generated by Bayesian Teaching successfully predict the AI's diagnostic decisions and are more likely to certify the AI for cases when the AI is correct than when it is wrong, indicating appropriate trust. These results show that Explainable AI can be used to support human-AI collaboration in medical imaging.",0
"This article presents research on how artificial intelligence (AI) can assist in detecting pneumothoraces using chest radiographs. Pneumothoraces can range from minor cases that resolve without treatment to severe cases requiring immediate intervention to prevent patient harm. Traditional detection methods rely heavily on human expertise and interpretation, which may lead to variability in accuracy. However, recent advancements in machine learning have made it possible to automate the process by training algorithms to identify patterns in images indicating the presence of a pneumothorax. To address concerns regarding the ""black box"" nature of these models, we propose a novel framework called explainable AI for interpreting results in clinical practice. Our model leverages Bayesian inference, an approach based on probability theory, to generate explanations about why certain features were important in making specific predictions. We evaluate our method against state-of-the-art non-explainable AI models and demonstrate improved diagnostic performance through extensive experiments. Overall, this work offers valuable insights into the potential benefits of incorporating explainable AI techniques in healthcare decision support systems, particularly in critical care settings where timely detection and appropriate management of conditions like pneumothoraces can make a life-saving difference.",1
"Recently, the (gradient-based) bilevel programming framework is widely used in hyperparameter optimization and has achieved excellent performance empirically. Previous theoretical work mainly focuses on its optimization properties, while leaving the analysis on generalization largely open. This paper attempts to address the issue by presenting an expectation bound w.r.t. the validation set based on uniform stability. Our results can explain some mysterious behaviours of the bilevel programming in practice, for instance, overfitting to the validation set. We also present an expectation bound for the classical cross-validation algorithm. Our results suggest that gradient-based algorithms can be better than cross-validation under certain conditions in a theoretical perspective. Furthermore, we prove that regularization terms in both the outer and inner levels can relieve the overfitting problem in gradient-based algorithms. In experiments on feature learning and data reweighting for noisy labels, we corroborate our theoretical findings.",0
"Bilevel programming (BLP) has emerged as a powerful tool for solving complex problems in engineering and computer science by allowing nested optimization procedures. In hyperparameter optimization, BLP can effectively handle scenarios where multiple objectives have conflicting goals while optimizing for both simultaneously. However, understanding stability and generalization properties of BLP remains challenging due to their interdisciplinary nature. This study seeks to analyze the behavior of BLP algorithms under different initialization schemes by considering their sensitivity towards randomness in problem formulations. We demonstrate that seemingly simple differences in initializations can lead to significant variations in results and highlight the importance of using robust methods. Our work emphasizes the need for systematic studies on the convergence and reliability of BLP in real-world applications to ensure confidence in its use as a decision support tool. By shedding light on these crucial aspects, we hope our research provides valuable insights into the design of improved optimization methods for bilevel programs.",1
"The research project HDV-Mess aims at a currently missing, but very crucial component for addressing important challenges in the field of connected and automated driving on public roads. The goal is to record traffic events at various relevant locations with high accuracy and to collect real traffic data as a basis for the development and validation of current and future sensor technologies as well as automated driving functions. For this purpose, it is necessary to develop a concept for a mobile modular system of measuring stations for highly accurate traffic data acquisition, which enables a temporary installation of a sensor and communication infrastructure at different locations. Within this paper, we first discuss the project goals before we present our traffic detection concept using mobile modular intelligent transport systems stations (ITS-Ss). We then explain the approaches for data processing of sensor raw data to refined trajectories, data communication, and data validation.",0
"This paper describes methods and concepts of the research project HDV- Mess (Highly Accurate Digital Traffic Recording). Our goal was to develop and test highly efficient methods that can capture and store large volumes of traffic data from multiple sources at low cost. To achieve this, we combined existing sensor technologies with innovative big data analysis techniques. We used the collected traffic data to build and evaluate microscopic simulation models. These simulations allow us to analyze different scenarios such as traffic light optimization or car2car communication. In addition, our methodology allows easy integration of further data sets like weather forecasts or calendar events. Furthermore, we developed algorithms able to automatically detect typical patterns within these huge datasets and to provide insights into complex relationships. The potential benefits of using high quality, detailed and comprehensive datasets are vast across many domains of future transportation systems including automotive industry (car manufactures), infrastructure managers (e.g., citiesâ€™ authorities) as well as providers of mobility services such as ride hailing companies, logistics service providers etc.. Therewithal the overall social impact of those projects reaches far beyond just developing another car model but encompasses aspects ranging from environment protection, urban planning, economy and even politics. For instance, results could serve as base for policy decisions (e.g., which kind of technology shall be enforced in public vehicles â€“ electric or hybrid cars? How might AV shape city structure? What are the best ways to implement shared autonomous vehicle fleets serving rural areas). The presented methodology of collecting fine grained traffic information, simulating traffic behavior on different scales and d",1
"Deep generative models are challenging the classical methods in the field of anomaly detection nowadays. Every new method provides evidence of outperforming its predecessors, often with contradictory results. The objective of this comparison is twofold: to compare anomaly detection methods of various paradigms with focus on deep generative models, and identification of sources of variability that can yield different results. The methods were compared on popular tabular and image datasets. We identified the main sources of variability to be experimental conditions: i) the type data set (tabular or image) and the nature of anomalies (statistical or semantic), and ii) strategy of selection of hyperparameters, especially the number of available anomalies in the validation set. Different methods perform the best in different contexts, i.e. combination of experimental conditions together with computational time. This explains the variability of the previous results and highlights the importance of careful specification of the context in the publication of a new method. All our code and results are available for download.",0
"This paper presents a comparison of several anomaly detection methods, including unsupervised machine learning techniques such as clustering algorithms and one-class support vector machines (SVMs), as well as supervised learning approaches like SVMs and random forests trained on labelled data. We evaluate these methods on different datasets from diverse domains, demonstrating that context significantly affects their performance. Our results indicate that no single method consistently outperforms others across all scenarios; instead, each approach excels under certain conditions specific to particular use cases. For instance, one-class SVMs perform exceptionally well in detecting anomalies when there are obvious patterns in normal data, while clustering techniques tend to excel with high-dimensional data that possess underlying structures. These findings suggest that choosing the most suitable detector requires domain knowledge and careful consideration of the data at hand. Therefore, we recommend using a range of anomaly detectors during development stages and carefully evaluating their effectiveness before deployment. By adopting a more comprehensive view of detectors, practitioners can make better decisions based on their unique requirements and circumstances. Ultimately, this work contributes to research on anomaly detection by emphasizing the importance of context when selecting appropriate methods and developing effective solutions.",1
"Explainable machine learning (ML) has gained traction in recent years due to the increasing adoption of ML-based systems in many sectors. Counterfactual explanations (CFEs) provide ``what if'' feedback of the form ``if an input datapoint were $x'$ instead of $x$, then an ML-based system's output would be $y'$ instead of $y$.'' CFEs are attractive due to their actionable feedback, amenability to existing legal frameworks, and fidelity to the underlying ML model. Yet, current CFE approaches are single shot -- that is, they assume $x$ can change to $x'$ in a single time period. We propose a novel stochastic-control-based approach that generates sequential CFEs, that is, CFEs that allow $x$ to move stochastically and sequentially across intermediate states to a final state $x'$. Our approach is model agnostic and black box. Furthermore, calculation of CFEs is amortized such that once trained, it applies to multiple datapoints without the need for re-optimization. In addition to these primary characteristics, our approach admits optional desiderata such as adherence to the data manifold, respect for causal relations, and sparsity -- identified by past research as desirable properties of CFEs. We evaluate our approach using three real-world datasets and show successful generation of sequential CFEs that respect other counterfactual desiderata.",0
"This paper presents an amortization method to generate sequences of counterfactual explanations (CEXs) that can justify why certain decisions were made by complex black box models such as deep neural networks. Unlike traditional explanation techniques like visualization tools that require explicit access to model internals, our approach instead constructs counterexamples iteratively from scratch through guided search in the feature space of a given instance while keeping track of visited regions using a novel history map data structure. Our key insight here is to leverage on the internal stability of certain decision boundaries, allowing us to prune regions in the feature space that are unlikely to result in different predictions, and thus enabling more efficient computation of explanations. We demonstrate effectiveness of our technique across several real world benchmark datasets and report significant speedup compared to baseline methods without compromising the quality or relevance of generated counterfactuals. Finally, we provide extensive analysis showing how our method helps expose interesting properties of underlying models such as sensitivity to input perturbation, dataset bias, or inconsistent behavior resulting from numerical instability. Overall, our work provides important progress towards making post-hoc explainability methods scalable enough for large models on high stakes tasks.",1
"Although Shapley Values (SV) are widely used in explainable AI, they can be poorly understood and estimated, which implies that their analysis may lead to spurious inferences and explanations. As a starting point, we remind an invariance principle for SV and derive the correct approach for computing the SV of categorical variables that are particularly sensitive to the encoding used. In the case of tree-based models, we introduce two estimators of Shapley Values that exploit efficiently the tree structure and are more accurate than state-of-the-art methods. For interpreting additive explanations, we recommend to filter the non-influential variables and to compute the Shapley Values only for groups of influential variables. For this purpose, we use the concept of ""Same Decision Probability"" (SDP) that evaluates the robustness of a prediction when some variables are missing. This prior selection procedure produces sparse additive explanations easier to visualize and analyse. Simulations and comparisons are performed with state-of-the-art algorithm, and show the practical gain of our approach.",0
"This paper presents a new method for computing accurate and robust Shapley values, which can be used to explain model predictions and identify locally important features. Our approach extends traditional methods by considering both feature interactions and nonlinear effects, while remaining computationally efficient and easy to implement. We demonstrate the utility of our method using real-world data sets from different domains, showing that it outperforms existing approaches in terms of accuracy, interpretability, and scalability. Finally, we discuss potential applications and future research directions in the field. (328)",1
"Stability is an important property of graph neural networks (GNNs) which explains their success in many problems of practical interest. Existing GNN stability results depend on the size of the graph, restricting applicability to graphs of moderate size. To understand the stability properties of GNNs on large graphs, we consider neural networks supported on manifolds. These are defined in terms of manifold diffusions mediated by the Laplace-Beltrami (LB) operator and are interpreted as limits of GNNs running on graphs of growing size. We define manifold deformations and show that they lead to perturbations of the manifold's LB operator that consist of an absolute and a relative perturbation term. We then define filters that split the infinite dimensional spectrum of the LB operator in finite partitions, and prove that manifold neural networks (MNNs) with these filters are stable to both, absolute and relative perturbations of the LB operator. Stability results are illustrated numerically in resource allocation problems in wireless networks.",0
"This is my attempt at writing such an abstract: ""This paper investigates the stability properties of manifold neural networks, which have gained popularity as efficient models in machine learning applications. We consider deformations that change the topology of the input space and show how these changes affect the performance of manifold neural networks. Our results demonstrate that under certain conditions, these networks remain stable even when subjected to significant topological changes. These findings have important implications for robustness analysis in deep learning, particularly in settings where data may exhibit complex topologies.""",1
"Distance transformation is an image processing technique used for many different applications. Related to a binary image, the general idea is to determine the distance of all background points to the nearest object point (or vice versa). In this tutorial, different approaches are explained in detail and compared using examples. Corresponding source code is provided to facilitate own investigations. A particular objective of this tutorial is to clarify the difference between arbitrary distance transforms and exact Euclidean distance transformations.",0
"The distance transform is a powerful tool used in image processing and computer vision tasks such as object recognition and segmentation. It represents the signed distances from each point in an input image to the nearest non-zero pixel location, allowing us to compute measures such as the Euclidean distance, city block distance, and Manhattan distance. This paper presents an overview of the distance transform and discusses various algorithms that can be used to efficiently compute it. We provide an analysis of the time complexity of these algorithms and compare their performance on real-world images using benchmark experiments. Our results show that the choice of algorithm depends on factors such as image size and feature type (object edges vs. textures), while also highlighting potential tradeoffs in terms of accuracy versus efficiency. Overall, this work provides valuable insights into the effectiveness of different approaches for computing the distance transform and helps users make informed choices when implementing this key component in image processing systems.",1
"Deep neural networks (DNNs) defy the classical bias-variance trade-off: adding parameters to a DNN that exactly interpolates its training data will typically improve its generalisation performance. Explaining the mechanism behind the benefit of such over-parameterisation is an outstanding challenge for deep learning theory. Here, we study the last layer representation of various deep architectures such as Wide-ResNets for image classification and find evidence for an underlying mechanism that we call *representation mitosis*: if the last hidden representation is wide enough, its neurons tend to split into groups which carry identical information, and differ from each other only by a statistically independent noise. Like in a mitosis process, the number of such groups, or ``clones'', increases linearly with the width of the layer, but only if the width is above a critical value. We show that a key ingredient to activate mitosis is continuing the training process until the training error is zero. Finally, we show that in one of the learning tasks we considered, a wide model with several automatically developed clones performs significantly better than a deep ensemble based on architectures in which the last layer has the same size as the clones.",0
"Artificial neural networks (ANNs) have been successfully applied to solve various problems in machine learning, computer vision, natural language processing, etc. In many cases ANNs demonstrate better performance than traditional methods due to their capability to handle complex nonlinear relationships between input variables and output target. However, recent studies indicate that despite achieving state-of-the-art accuracy on benchmark datasets, modern deep architectures often suffer from underfitting in practice since they donâ€™t capture all relevant features at hand. To overcome such limitations, researchers propose using wider architectures consisting of more neurons per layer, which were shown to lead to better generalization performance across different tasks. In this work we propose representation mitosis as a novel technique aimed at improving feature extraction capabilities in wide ANNs by splitting individual neuron representations into two or more new neurons without changing the overall architecture size. By effectively exploiting the redundancy present within each neuronâ€™s receptive field we can generate multiple equivalent but diverse interpretations of local patterns, resulting in a broader range of encoded features. We introduce a lightweight algorithmic approach to perform representation mitosis throughout network training without increasing computational complexity or requiring additional hyperparameters. Our experimental evaluation shows consistent improvement over the baseline models on several popular image classification benchmarks including CIFAR-10/100, SVHN, and ImageNet, thus demonstrating the effectiveness of our proposed method.",1
"Concerns about the societal impact of AI-based services and systems has encouraged governments and other organisations around the world to propose AI policy frameworks to address fairness, accountability, transparency and related topics. To achieve the objectives of these frameworks, the data and software engineers who build machine-learning systems require knowledge about a variety of relevant supporting tools and techniques. In this paper we provide an overview of technologies that support building trustworthy machine learning systems, i.e., systems whose properties justify that people place trust in them. We argue that four categories of system properties are instrumental in achieving the policy objectives, namely fairness, explainability, auditability and safety & security (FEAS). We discuss how these properties need to be considered across all stages of the machine learning life cycle, from data collection through run-time model inference. As a consequence, we survey in this paper the main technologies with respect to all four of the FEAS properties, for data-centric as well as model-centric stages of the machine learning system life cycle. We conclude with an identification of open research problems, with a particular focus on the connection between trustworthy machine learning technologies and their implications for individuals and society.",0
"Here's an abstract that meets your requirements:  ""The field of machine learning has made remarkable progress over the past few decades, leading to numerous applications across various domains. However, recent incidents have highlighted concerns related to trustworthiness, which have resulted in severe consequences. In response to these events, researchers have proposed several technological solutions to mitigate potential risks associated with deploying machine learning models. To present a comprehensive overview of existing approaches, we survey recent developments in this area, focusing on both technical and societal aspects. We categorize relevant studies into four key areas: interpretability methods, robustness techniques, transparency frameworks, and accountability measures. Our discussion emphasizes how each category contributes to achieving reliable outcomes while considering social implications. This work provides insights into current challenges faced by practitioners and identifies promising directions for future research.""",1
"As neural networks become more popular, the need for accompanying uncertainty estimates increases. The current testing methodology focusses on how good the predictive uncertainty estimates explain the differences between predictions and observations in a previously unseen test set. Intuitively this is a logical approach. The current setup of benchmark data sets also allows easy comparison between the different methods. We demonstrate, however, through both theoretical arguments and simulations that this way of evaluating the quality of uncertainty estimates has serious flaws. Firstly, it cannot disentangle the aleatoric from the epistemic uncertainty. Secondly, the current methodology considers the uncertainty averaged over all test samples, implicitly averaging out overconfident and underconfident predictions. When checking if the correct fraction of test points falls inside prediction intervals, a good score on average gives no guarantee that the intervals are sensible for individual points. We demonstrate through practical examples that these effects can result in favoring a method, based on the predictive uncertainty, that has undesirable behaviour of the confidence intervals. Finally, we propose a simulation-based testing approach that addresses these problems while still allowing easy comparison between different methods.",0
"In the field of machine learning, uncertainty estimation plays a crucial role in determining how reliable predictions made by models can be trusted by practitioners and stakeholders alike. Specifically, regression tasks deal with predicting continuous outcome variables based on input features, which can lead to inherent uncertainties that need careful examination before making decisions. This paper focuses on providing an overview of different techniques used to estimate uncertainties in machine learning regression problems. We discuss strengths and weaknesses of these methods and provide guidelines for selecting the appropriate technique(s) based on specific problem characteristics. We conclude with recommendations for future research directions aimed at addressing current limitations in uncertainty estimation.",1
"Previously, statistical textbook wisdom has held that interpolating noisy data will generalize poorly, but recent work has shown that data interpolation schemes can generalize well. This could explain why overparameterized deep nets do not necessarily overfit. Optimal data interpolation schemes have been exhibited that achieve theoretical lower bounds for excess risk in any dimension for large data (Statistically Consistent Interpolation). These are non-parametric Nadaraya-Watson estimators with singular kernels. The recently proposed weighted interpolating nearest neighbors method (wiNN) is in this class, as is the previously studied Hilbert kernel interpolation scheme, in which the estimator has the form $\hat{f}(x)=\sum_i y_i w_i(x)$, where $w_i(x)= \|x-x_i\|^{-d}/\sum_j \|x-x_j\|^{-d}$. This estimator is unique in being completely parameter-free. While statistical consistency was previously proven, convergence rates were not established. Here, we comprehensively study the finite sample properties of Hilbert kernel regression. We prove that the excess risk is asymptotically equivalent pointwise to $\sigma^2(x)/\ln(n)$ where $\sigma^2(x)$ is the noise variance. We show that the excess risk of the plugin classifier is less than $2|f(x)-1/2|^{1-\alpha}\,(1+\varepsilon)^\alpha \sigma^\alpha(x)(\ln(n))^{-\frac{\alpha}{2}}$, for any $0\alpha1$, where $f$ is the regression function $x\mapsto\mathbb{E}[y|x]$. We derive asymptotic equivalents of the moments of the weight functions $w_i(x)$ for large $n$, for instance for $\beta1$, $\mathbb{E}[w_i^{\beta}(x)]\sim_{n\rightarrow \infty}((\beta-1)n\ln(n))^{-1}$. We derive an asymptotic equivalent for the Lagrange function and exhibit the nontrivial extrapolation properties of this estimator. We present heuristic arguments for a universal $w^{-2}$ power-law behavior of the probability density of the weights in the large $n$ limit.",0
"In this work, we present parameter-free Statistically Consistent (SC) interpolation methods that achieve dimension-independent convergence rates for Hilbert kernel regression. Our approach relies on novel data-driven adaptive sampling techniques that lead to improved accuracy over existing SC methods without requiring any tuning parameters. We demonstrate the efficacy of our method across various benchmark datasets, showing significant improvements over state-of-the-art competitors while maintaining computational efficiency. Furthermore, we provide theoretical analysis establishing the convergence properties of our method in high dimensions. Our contributions pave the way towards practical, efficient, and accurate interpolation schemes for big data applications.",1
"This paper studies Principal Component Analysis (PCA) for data lying in hyperbolic spaces. Given directions, PCA relies on: (1) a parameterization of subspaces spanned by these directions, (2) a method of projection onto subspaces that preserves information in these directions, and (3) an objective to optimize, namely the variance explained by projections. We generalize each of these concepts to the hyperbolic space and propose HoroPCA, a method for hyperbolic dimensionality reduction. By focusing on the core problem of extracting principal directions, HoroPCA theoretically better preserves information in the original data such as distances, compared to previous generalizations of PCA. Empirically, we validate that HoroPCA outperforms existing dimensionality reduction methods, significantly reducing error in distance preservation. As a data whitening method, it improves downstream classification by up to 3.9% compared to methods that don't use whitening. Finally, we show that HoroPCA can be used to visualize hyperbolic data in two dimensions.",0
"Incorporate keywords like hyperboloids, PCA, manifold learning, Laplace-Beltrami operator etc. If there is any important related work that needs mentioning please bring them up here as well. Use active voice instead of passive voice wherever possible! --- This paper presents a new method for dimensionality reduction called HoroPCA (Hyperbolic Principal Component Analysis). By leveraging the geometry of hyperboloids, we introduce horospherical projections which capture local intrinsic geometric structure. Our approach provides an efficient and elegant solution for both linear and nonlinear data while preserving global and local curvature properties. We prove convergence guarantees of our algorithm under mild conditions on the data domain. Furthermore, by designing efficient solvers for the resulting eigenvalue problems, we achieve significant speedups over previous methods. We demonstrate superior performance of our method compared to other popular dimensionality reduction techniques such as UMAP, tSNE, and LLE on several real datasets. Finally, we highlight connections between our framework and recent advances in manifold learning based on the Laplaceâ€“Beltrami operator. Additionally, we provide insights into potential future research directions. ---",1
"This paper presents Gem, a model-agnostic approach for providing interpretable explanations for any GNNs on various graph learning tasks. Specifically, we formulate the problem of providing explanations for the decisions of GNNs as a causal learning task. Then we train a causal explanation model equipped with a loss function based on Granger causality. Different from existing explainers for GNNs, Gem explains GNNs on graph-structured data from a causal perspective. It has better generalization ability as it has no requirements on the internal structure of the GNNs or prior knowledge on the graph learning tasks. In addition, Gem, once trained, can be used to explain the target GNN very quickly. Our theoretical analysis shows that several recent explainers fall into a unified framework of additive feature attribution methods. Experimental results on synthetic and real-world datasets show that Gem achieves a relative increase of the explanation accuracy by up to $30\%$ and speeds up the explanation process by up to $110\times$ as compared to its state-of-the-art alternatives.",0
"Graph neural networks have gained popularity due to their effectiveness at capturing complex relationships among entities represented as nodes in graphs. However, while these models achieve high accuracy on tasks such as node classification and link prediction, they can lack interpretability and transparency. Recent works aimed to tackle this problem by generating local explanations based on subgraph patterns that capture how individual edges contribute to predictions. In contrast, we present a novel framework called GCE that generates generative causal explanations directly from graph neural network outputs, without requiring access to intermediate representations. Our approach first infers a latent space representation using variational autoencoders, which allows us to define interventions in terms of changes to nodal features rather than explicit edge modifications. We then generate counterfactual graph instances based on these interventions, along with corresponding changes to all other variables in the system. We show both qualitatively and quantitatively that our generated explanations are faithful approximations of the true underlying causality governing the behavior of graph neural networks, and demonstrate that our method substantially outperforms baseline alternatives across multiple datasets and architectures.",1
"Text-based video segmentation is a challenging task that segments out the natural language referred objects in videos. It essentially requires semantic comprehension and fine-grained video understanding. Existing methods introduce language representation into segmentation models in a bottom-up manner, which merely conducts vision-language interaction within local receptive fields of ConvNets. We argue that such interaction is not fulfilled since the model can barely construct region-level relationships given partial observations, which is contrary to the description logic of natural language/referring expressions. In fact, people usually describe a target object using relations with other objects, which may not be easily understood without seeing the whole video. To address the issue, we introduce a novel top-down approach by imitating how we human segment an object with the language guidance. We first figure out all candidate objects in videos and then choose the refereed one by parsing relations among those high-level objects. Three kinds of object-level relations are investigated for precise relationship understanding, i.e., positional relation, text-guided semantic relation, and temporal relation. Extensive experiments on A2D Sentences and J-HMDB Sentences show our method outperforms state-of-the-art methods by a large margin. Qualitative results also show our results are more explainable. Besides, based on the inspiration, we win the first place in CVPR2021 Referring Youtube-VOS challenge.",0
"Title: ""Leveraging Object-level Relationships for Text-based Video Segmentation"" Abstract: This work presents ClawCraneNet, a novel architecture that leverages object-level relationships within video frames to improve text-based segmentation accuracy. The proposed approach utilizes convolutional neural networks (ConvNets) to extract features from both spatial and temporal dimensions of videos while incorporating attention modules to highlight relevant image regions guided by textual descriptions. By explicitly modeling interdependencies among objects present in scenes, ClawCraneNet achieves state-of-the-art performance on challenging benchmarks such as ActivityNet Caption and Charades, demonstrating its effectiveness in handling complex and varying scenarios involving human interactions and activities. Overall, our method represents an important step forward towards realizing more accurate video understanding systems driven by natural language. Note: If you donâ€™t have access to any academic papers on this topic, then I would recommend coming up with your own ideas based on the instructions provided above. Make sure to provide clear explanations for each part of the task so that it can be completed successfully!",1
"People can produce drawings of specific entities (e.g., Garfield), as well as general categories (e.g., ""cat""). What explains this ability to produce such varied drawings of even highly familiar object concepts? We hypothesized that drawing objects at different levels of abstraction depends on both sensory information and representational goals, such that drawings intended to portray a recently seen object preserve more detail than those intended to represent a category. Participants drew objects cued either with a photo or a category label. For each cue type, half the participants aimed to draw a specific exemplar; the other half aimed to draw the category. We found that label-cued category drawings were the most recognizable at the basic level, whereas photo-cued exemplar drawings were the least recognizable. Together, these findings highlight the importance of task context for explaining how people use drawings to communicate visual concepts in different ways.",0
"This paper presents a study on how humans visualize and communicate object concepts at different levels of abstraction. We examine the role played by contextual factors, such as culture, language and individual differences, in shaping these processes. Our findings suggest that while some aspects of concept representation remain consistent across individuals, there is significant variability in others. Moreover, we found evidence of interplay between semantic, perceptual, and motor systems during encoding and production of object concepts at different levels of abstraction. These results contribute to our understanding of human cognition and have implications for fields such as education, design, advertising and artificial intelligence.",1
"Curvature in form of the Hessian or its generalized Gauss-Newton (GGN) approximation is valuable for algorithms that rely on a local model for the loss to train, compress, or explain deep networks. Existing methods based on implicit multiplication via automatic differentiation or Kronecker-factored block diagonal approximations do not consider noise in the mini-batch. We present ViViT, a curvature model that leverages the GGN's low-rank structure without further approximations. It allows for efficient computation of eigenvalues, eigenvectors, as well as per-sample first- and second-order directional derivatives. The representation is computed in parallel with gradients in one backward pass and offers a fine-grained cost-accuracy trade-off, which allows it to scale. As examples for ViViT's usefulness, we investigate the directional gradients and curvatures during training, and how noise information can be used to improve the stability of second-order methods.",0
"This paper presents ViViT, which stands for Variational Video Inversion Through Gauss-Newton, as a new method for estimating camera poses from videos that have been taken by moving cameras or otherwise don't satisfy the classical assumptions underlying traditional computer vision techniques. At its core, ViViT leverages recent advances in optimization under uncertainty. By modeling each frame as a sum of rank one tensors, and then iteratively updating those frames using a low-rank representation based on the gradient descent step obtained by minimizing the reconstruction error plus a regularizer term proportional to L2 norm of these unknown rank ones, ViViT effectively enforces smoothness while preserving edges and features, enabling accurate recovery even from highly compressed footage or poorly calibrated cameras. While related methods exist, they are often limited to either working only with monocular data (e.g., visual odometry) or require more detailed feature extraction than we can achieve without prior knowledge of pose or scene geometry. Our approach, however, recovers both scene flow and depth maps directly, allowing us to compute robust 6DoF pose estimates without needing external sensor input other than RGB video itself. We evaluate ViViT against state-of-the-art methods on standard benchmark datasets and demonstrate improved performance across all metrics. We hope that this work serves as a stepping stone towards full 3D understanding of real-world environments using solely RGB inputs.",1
"A recent numerical study observed that neural network classifiers enjoy a large degree of symmetry in the penultimate layer. Namely, if $h(x) = Af(x) +b$ where $A$ is a linear map and $f$ is the output of the penultimate layer of the network (after activation), then all data points $x_{i, 1}, \dots, x_{i, N_i}$ in a class $C_i$ are mapped to a single point $y_i$ by $f$ and the points $y_i$ are located at the vertices of a regular $k-1$-dimensional standard simplex in a high-dimensional Euclidean space.   We explain this observation analytically in toy models for highly expressive deep neural networks. In complementary examples, we demonstrate rigorously that even the final output of the classifier $h$ is not uniform over data samples from a class $C_i$ if $h$ is a shallow network (or if the deeper layers do not bring the data samples into a convenient geometric configuration).",0
This paper presents research on the phenomenon of simplex symmetry in the final and penultimate layers of neural network classifiers. The study investigates how simplex symmetries arise and their impact on model performance. Results indicate that certain types of simplex symmetries can improve accuracy while others may have little effect or even decrease performance. These findings contribute new insights into the behavior of deep learning models and suggest potential optimization strategies for developers. Practitioners seeking to optimize model training and deployment could benefit from considering these results. Future work should further explore the relationship between simplex symmetry and deep learning model performance.,1
"Lending decisions are usually made with proprietary models that provide minimally acceptable explanations to users. In a future world without such secrecy, what decision support tools would one want to use for justified lending decisions? This question is timely, since the economy has dramatically shifted due to a pandemic, and a massive number of new loans will be necessary in the short term. We propose a framework for such decisions, including a globally interpretable machine learning model, an interactive visualization of it, and several types of summaries and explanations for any given decision. The machine learning model is a two-layer additive risk model, which resembles a two-layer neural network, but is decomposable into subscales. In this model, each node in the first (hidden) layer represents a meaningful subscale model, and all of the nonlinearities are transparent. Our online visualization tool allows exploration of this model, showing precisely how it came to its conclusion. We provide three types of explanations that are simpler than, but consistent with, the global model: case-based reasoning explanations that use neighboring past cases, a set of features that were the most important for the model's prediction, and summary-explanations that provide a customized sparse explanation for any particular lending decision made by the model. Our framework earned the FICO recognition award for the Explainable Machine Learning Challenge, which was the first public challenge in the domain of explainable machine learning.",0
"In recent years there has been growing interest among financial lenders in making decisions that are interpretable, explainable, and accountable (Transparent, Explicable and Accountable Artificial Intelligence â€“ TEAAI). This paper proposes a holistic approach to interpretability in financial lending which combines three key components: interpretable models, visualizations, and summary explanations. The first component is interpretable models, which can generate predictions or decisions that humans can easily understand and trust without resorting to complex mathematical formulations or deep learning techniques. These models have become increasingly popular due to their transparency and ability to produce easy-to-understand outputs. However, choosing appropriate feature sets remains a challenge. Here we discuss how different types of feature sets influence model performance, including traditional economic factors such as GDP growth rates, credit spreads, debt levels, foreign exchange reserves, etc. We further explore non-traditional features extracted from textual analysis of news articles on political events, regulatory changes and economic trends. Our findings suggest that while traditional economic indicators still play important roles in forecasting sovereign risk, adding unstructured data such as news articles provides a richer picture that improves overall prediction accuracy. By integrating these diverse sources into our model design, practitioners can achieve more transparent and reliable decision support systems. In addition to the use of interpretable models, our methodology incorporates visualization techniques to convey information about model diagnostics, decision rules, or other relevant insights regarding underlying relationships or patterns. This enables human analysts to interactively navigate through predictive results and gain intuitive insight i",1
"This is a tutorial and survey paper for nonlinear dimensionality and feature extraction methods which are based on the Laplacian of graph of data. We first introduce adjacency matrix, definition of Laplacian matrix, and the interpretation of Laplacian. Then, we cover the cuts of graph and spectral clustering which applies clustering in a subspace of data. Different optimization variants of Laplacian eigenmap and its out-of-sample extension are explained. Thereafter, we introduce the locality preserving projection and its kernel variant as linear special cases of Laplacian eigenmap. Versions of graph embedding are then explained which are generalized versions of Laplacian eigenmap and locality preserving projection. Finally, diffusion map is introduced which is a method based on Laplacian of data and random walks on the data graph.",0
"This paper presents a tutorial on various Laplacian-based dimensionality reduction methods used in machine learning, including spectral clustering, Laplacian eigenmaps, locality preserving projection (LPP), graph embedding, and diffusion map. These techniques have proven to be effective in solving problems such as data visualization, feature selection, and unsupervised learning, among others. We provide an overview of each method, discuss their respective strengths and weaknesses, and compare them based on criteria such as accuracy, computational complexity, and interpretability. In addition, we review recent advancements made in these fields, highlighting new applications and potential future directions. Our goal is to serve both researchers and practitioners interested in exploring the benefits of using Laplacian-based approaches for dimensionality reduction tasks.",1
"The peaky behavior of CTC models is well known experimentally. However, an understanding about why peaky behavior occurs is missing, and whether this is a good property. We provide a formal analysis of the peaky behavior and gradient descent convergence properties of the CTC loss and related training criteria. Our analysis provides a deep understanding why peaky behavior occurs and when it is suboptimal. On a simple example which should be trivial to learn for any model, we prove that a feed-forward neural network trained with CTC from uniform initialization converges towards peaky behavior with a 100% error rate. Our analysis further explains why CTC only works well together with the blank label. We further demonstrate that peaky behavior does not occur on other related losses including a label prior model, and that this improves convergence.",0
"In recent years, connectionist approaches have become increasingly popular due to their ability to generate human-like speech samples that can even fool humans into thinking they were produced by real speakers. One such approach is the use of Convolutional Transformer architectures (CTAs), which combine the strengths of both convolutional neural networks (CNNs) and transformer models. However, one issue with using CTAs is that the generated output tends to exhibit ""peaky"" behavior, meaning that certain sounds or phones tend to dominate over others. This paper explores the reasons behind why CTA generates peaky outputs and proposes some potential solutions to mitigate this problem. By analyzing the behavior of CTA models at different stages of training, we identify several factors that contribute to peakiness, including high variability in early layers, unstable alignment in later layers, and poor regularization methods. We then propose several techniques aimed at addressing these issues, such as weight normalization and dropout, and show that these techniques lead to improved balance and more natural sounding outputs. Overall, our work provides insights into how to improve the performance of CTA models in generating balanced and less peaky speech outputs.",1
"In this paper, we apply the self-attention from the state-of-the-art Transformer in Attention Is All You Need the first time to a data-driven operator learning problem related to partial differential equations. We put together an effort to explain the heuristics of, and improve the efficacy of the self-attention by demonstrating that the softmax normalization in the scaled dot-product attention is sufficient but not necessary, and have proved the approximation capacity of a linear variant as a Petrov-Galerkin projection. A new layer normalization scheme is proposed to allow a scaling to propagate through attention layers, which helps the model achieve remarkable accuracy in operator learning tasks with unnormalized data. Finally, we present three operator learning experiments, including the viscid Burgers' equation, an interface Darcy flow, and an inverse interface coefficient identification problem. All experiments validate the improvements of the newly proposed simple attention-based operator learner over their softmax-normalized counterparts.",0
"This article provides an overview of two common methods used in numerical analysis, namely the Fourier transform and the Galerkin method. Both techniques have their own advantages and disadvantages and can be applied to solve different types of partial differential equations (PDEs). The Fourier transform is particularly well suited for solving periodic problems and systems that exhibit separation of variables, while the Galerkin method can handle nonlinear PDEs and allows for more flexibility in choosing basis functions. The paper compares and contrasts these two methods and discusses their applications in different fields such as engineering, physics, and finance. Ultimately, the choice between the two depends on the specific problem at hand and one may need to use a combination of both techniques depending on the complexity of the system. The paper includes examples demonstrating how to implement these methods using popular software packages like MATLAB and Python libraries like SciPy.",1
"We present a meta-algorithm for learning a posterior-inference algorithm for restricted probabilistic programs. Our meta-algorithm takes a training set of probabilistic programs that describe models with observations, and attempts to learn an efficient method for inferring the posterior of a similar program. A key feature of our approach is the use of what we call a white-box inference algorithm that extracts information directly from model descriptions themselves, given as programs. Concretely, our white-box inference algorithm is equipped with multiple neural networks, one for each type of atomic command, and computes an approximate posterior of a given probabilistic program by analysing individual atomic commands in the program using these networks. The parameters of these networks are then learnt from a training set by our meta-algorithm. We empirically demonstrate that the learnt inference algorithm generalises well to unseen programs in terms of both interpolation and extrapolation, and report cases where our approach may be preferable to a state-of-the-art inference algorithm such as HMC. The overall results show the promise as well as remaining challenges of our approach.",0
"""This is a technical research paper that presents the concept of meta learning inference algorithms for probabilistic programs. The authors propose a methodology that enables automating Bayesian inference through learning an inferencing algorithm for any probability program given some training data."" --- I would say something like:  Probabilistic programming offers new ways to express complex models of knowledge and uncertainty, but inference for these languages can be intractable. We introduce a framework for automating inference by generalizing existing approaches into model agnostic methods based on gradient descent optimization; our approach learns from examples generated during training using techniques such as gradient estimation and score function approximation. Experimental results show effectiveness across multiple example domains and competitive performance against state-of-the-art implementations of traditional inference algorithms on real-world datasets. This work provides a novel direction towards democratizing advanced statistical analysis for non experts, while allowing users to work at higher levels of abstraction than was previously possible.  Is there anything you would change?",1
"Causal reasoning is the main learning and explanation tool used by humans. AI systems should possess causal reasoning capabilities to be deployed in the real world with trust and reliability. Introducing the ideas of causality to machine learning helps in providing better learning and explainable models. Explainability, causal disentanglement are some important aspects of any machine learning model. Causal explanations are required to believe in a model's decision and causal disentanglement learning is important for transfer learning applications. We exploit the ideas of causality to be used in deep learning models to achieve better and causally explainable models that are useful in fairness, disentangled representation, etc.",0
"""Causal relationships have been widely studied in many fields including economics, psychology, and sociology. Recently, there has been growing interest in understanding causality within the context of neural networks. In this work, we aim to explore how causal reasoning can be applied to deep learning models in order to improve their interpretability and explainability. We begin by discussing the concept of causality in artificial intelligence, highlighting why it is important and challenging to quantify. Next, we introduce some existing methods that attempt to address these challenges, such as path-specific counterfactuals and structural equation modeling (SEM). Afterwards, we describe two case studies where we apply causal analysis techniques to common problems in natural language processing and computer vision tasks: text style transfer and image generation from descriptions. Our results demonstrate that incorporating causal reasoning into neural network architectures can lead to more effective and interpretable solutions. Ultimately, our work contributes to the ongoing effort to make machine learning systems more transparent and trustworthy.""",1
"It is well known that deep learning models have a propensity for fitting the entire training set even with random labels, which requires memorization of every training sample. In this paper, we investigate the memorization effect in adversarial training (AT) for promoting a deeper understanding of capacity, convergence, generalization, and especially robust overfitting of adversarially trained classifiers. We first demonstrate that deep networks have sufficient capacity to memorize adversarial examples of training data with completely random labels, but not all AT algorithms can converge under the extreme circumstance. Our study of AT with random labels motivates further analyses on the convergence and generalization of AT. We find that some AT methods suffer from a gradient instability issue, and the recently suggested complexity measures cannot explain robust generalization by considering models trained on random labels. Furthermore, we identify a significant drawback of memorization in AT that it could result in robust overfitting. We then propose a new mitigation algorithm motivated by detailed memorization analyses. Extensive experiments on various datasets validate the effectiveness of the proposed method.",0
"In recent years, adversarial training has become increasingly popular as a method for improving robustness to input perturbations in machine learning models. However, little attention has been paid to understanding the effects of memorization on these methods. This paper seeks to fill that gap by exploring how different forms of memorization impact the effectiveness of adversarial training. We begin by reviewing existing literature on memorization in deep learning and discussing its potential benefits and drawbacks. Next, we present our experimental results which demonstrate that certain types of memorization can improve the performance of adversarial training significantly. Finally, we offer suggestions for future research directions, including incorporating memorization into other defense mechanisms, studying its influence across different model architectures, and investigating its impact on out-of-distribution generalization. Overall, this work highlights the importance of considering memorization in evaluating the robustness of machine learning systems.",1
"Imagine trying to track one particular fruitfly in a swarm of hundreds. Higher biological visual systems have evolved to track moving objects by relying on both appearance and motion features. We investigate if state-of-the-art deep neural networks for visual tracking are capable of the same. For this, we introduce PathTracker, a synthetic visual challenge that asks human observers and machines to track a target object in the midst of identical-looking ""distractor"" objects. While humans effortlessly learn PathTracker and generalize to systematic variations in task design, state-of-the-art deep networks struggle. To address this limitation, we identify and model circuit mechanisms in biological brains that are implicated in tracking objects based on motion cues. When instantiated as a recurrent network, our circuit model learns to solve PathTracker with a robust visual strategy that rivals human performance and explains a significant proportion of their decision-making on the challenge. We also show that the success of this circuit model extends to object tracking in natural videos. Adding it to a transformer-based architecture for object tracking builds tolerance to visual nuisances that affect object appearance, resulting in a new state-of-the-art performance on the large-scale TrackingNet object tracking challenge. Our work highlights the importance of building artificial vision models that can help us better understand human vision and improve computer vision.",0
"In recent years, there has been growing interest in developing techniques that allow humans and machines to track objects without constantly re-identifying them. This can lead to significant improvements in efficiency and performance across many fields, from robotics and computer vision to psychology and neuroscience. However, understanding how tracking works and why it sometimes fails remains a challenge. In this paper, we examine the current state of knowledge on object tracking, focusing on human behavioral studies as well as machine learning algorithms commonly used for tracking tasks. We identify several key factors affecting tracking performance and discuss potential solutions to overcome these limitations. Ultimately, our goal is to provide insights into both human and machine tracking abilities and offer suggestions for improving tracking accuracy and robustness. By bridging research areas and providing interdisciplinary perspectives, we hope to promote further advancements in object tracking that can benefit society and improve the overall quality of life.",1
"The lack of interpretability is an inevitable problem when using neural network models in real applications. In this paper, an explainable neural network based on generalized additive models with structured interactions (GAMI-Net) is proposed to pursue a good balance between prediction accuracy and model interpretability. GAMI-Net is a disentangled feedforward network with multiple additive subnetworks; each subnetwork consists of multiple hidden layers and is designed for capturing one main effect or one pairwise interaction. Three interpretability aspects are further considered, including a) sparsity, to select the most significant effects for parsimonious representations; b) heredity, a pairwise interaction could only be included when at least one of its parent main effects exists; and c) marginal clarity, to make main effects and pairwise interactions mutually distinguishable. An adaptive training algorithm is developed, where main effects are first trained and then pairwise interactions are fitted to the residuals. Numerical experiments on both synthetic functions and real-world datasets show that the proposed model enjoys superior interpretability and it maintains competitive prediction accuracy in comparison to the explainable boosting machine and other classic machine learning models.",0
"This paper presents the design, training, validation and application of our novel explainable artificial intelligence (AI) system called GAMI-Net - which stands for ""Generalized Additive Modeling Inference - Artificial Neural Network."" GAMI-Net is designed to generate human-interpretable explanations of why it makes decisions by providing both global (i.e., across all data points) and local (i.e., per data point) interpretability for deep neural networks. We achieve global interpretability through a generalized additive model that summarizes complex interactions into more easily interpretable main effects and interaction terms. Local interpretability is achieved by training models to output sparse features maps indicating which input variables contributed most significantly to each prediction, allowing users to see how their own inputs map onto predictions. Our approach leverages recent advances in machine learning by using a trained convolutional neural network as a feature extractor, generating these sparse feature maps at run time from any new data point given only the final linear layers of the original architecture. The end result is highly competitive predictive accuracy combined with human readable outputs even if users know little about machine learning concepts such as neural networks, kernels and hidden units. We evaluate GAMI-Net on four datasets commonly used for evaluating explainable AI methods including MNIST handwritten digits, CelebA face attributes, FashionMNIST Zip code classification and breast cancer tumour detection via mammography images where we compare against several state-of-the-art baseline approaches on the first three tasks demonstrating improved performance while producing easy to read explanations. Additionally w",1
"We present a framework for the theoretical analysis of ensembles of low-complexity empirical risk minimisers trained on independent random compressions of high-dimensional data. First we introduce a general distribution-dependent upper-bound on the excess risk, framed in terms of a natural notion of compressibility. This bound is independent of the dimension of the original data representation, and explains the in-built regularisation effect of the compressive approach. We then instantiate this general bound to classification and regression tasks, considering Johnson-Lindenstrauss mappings as the compression scheme. For each of these tasks, our strategy is to develop a tight upper bound on the compressibility function, and by doing so we discover distributional conditions of geometric nature under which the compressive algorithm attains minimax-optimal rates up to at most poly-logarithmic factors. In the case of compressive classification, this is achieved with a mild geometric margin condition along with a flexible moment condition that is significantly more general than the assumption of bounded domain. In the case of regression with strongly convex smooth loss functions we find that compressive regression is capable of exploiting spectral decay with near-optimal guarantees. In addition, a key ingredient for our central upper bound is a high probability uniform upper bound on the integrated deviation of dependent empirical processes, which may be of independent interest.",0
This abstract presents statistical optimality conditions for compressive ensembles. These conditions provide guidelines on how many measurements are required for exact recovery of sparse signals from linear combinations of random projections. The results show that the number of measurements needed depends only on the sparsity level of the signal and the expected value of the compression ratio. The methods used to derive these conditions involve concentration inequalities and probabilistic bounds. These conditions offer insights into designing optimal compressed sensing systems with limited resources. They can also facilitate understanding of fundamental limits of data acquisition in applications such as radar imaging and medical tomography. Keywords: Compressive Sensing; Ensemble Methods; Optimality Conditions; Data Acquisition; Resource Allocation; Limited Resources; Concentration Inequalities; Probabilistic Bounds,1
"Partial differential equations (PDEs) fitting scientific data can represent physical laws with explainable mechanisms for various mathematically-oriented subjects. Most natural dynamics are expressed by PDEs with varying coefficients (PDEs-VC), which highlights the importance of PDE discovery. Previous algorithms can discover some simple instances of PDEs-VC but fail in the discovery of PDEs with coefficients of higher complexity, as a result of coefficient estimation inaccuracy. In this paper, we propose KO-PDE, a kernel optimized regression method that incorporates the kernel density estimation of adjacent coefficients to reduce the coefficient estimation error. KO-PDE can discover PDEs-VC on which previous baselines fail and is more robust against inevitable noise in data. In experiments, the PDEs-VC of seven challenging spatiotemporal scientific datasets in fluid dynamics are all discovered by KO-PDE, while the three baselines render false results in most cases. With state-of-the-art performance, KO-PDE sheds light on the automatic description of natural phenomenons using discovered PDEs in the real world.",0
"This abstract presents the KO-PDE method, which enables efficient discovery of partial differential equations (PDE) with varying coefficients by using kernel-based techniques that exploit the inherent nonlinearities in PDE systems. We demonstrate that KO-PDE outperforms current state-of-the art methods on two benchmark datasets, achieving superior accuracy while remaining computationally tractable. Our experimental results indicate that KO-PDE effectively leverages kernel learning approaches to optimize PDE model identification problems. Our approach is motivated by recent advances in kernel ridge regression for function approximation as well as a desire to extend existing learning paradigms into new domains such as discovering complex physical models via structured regularization. By optimizing over function space rather than parameters, we enable powerful solutions capable of capturing essential features from large scale data sets. Overall, our work offers important implications for machine learning applied towards scientific computing; improving automation and robustness in solving challenges related to generating accurate mathematical descriptions of real world phenomena.",1
"We examine counterfactual explanations for explaining the decisions made by model-based AI systems. The counterfactual approach we consider defines an explanation as a set of the system's data inputs that causally drives the decision (i.e., changing the inputs in the set changes the decision) and is irreducible (i.e., changing any subset of the inputs does not change the decision). We (1) demonstrate how this framework may be used to provide explanations for decisions made by general, data-driven AI systems that may incorporate features with arbitrary data types and multiple predictive models, and (2) propose a heuristic procedure to find the most useful explanations depending on the context. We then contrast counterfactual explanations with methods that explain model predictions by weighting features according to their importance (e.g., SHAP, LIME) and present two fundamental reasons why we should carefully consider whether importance-weight explanations are well-suited to explain system decisions. Specifically, we show that (i) features that have a large importance weight for a model prediction may not affect the corresponding decision, and (ii) importance weights are insufficient to communicate whether and how features influence decisions. We demonstrate this with several concise examples and three detailed case studies that compare the counterfactual approach with SHAP to illustrate various conditions under which counterfactual explanations explain data-driven decisions better than importance weights.",0
"This abstract outlines the importance of explaining data driven decisions made by artificial intelligence (AI) systems. The increasing use of these systems makes it crucial that their reasoning processes are transparent so that users can trust them with decision making tasks such as medical diagnosis, criminal justice or financial planning among others. In order to provide these explanations, there have been several approaches proposed in literature such as model interpretation, explanation generation and interaction based methods. However, most of these approaches focus on the models themselves rather than taking into account how actual human experts would explain those decisions. Therefore we propose the counterfactual approach which provides explanations grounded in human understanding of causality. We present our evaluation results comparing this novel approach against standard baseline methods such as LIME, SHAP or Anchors in three different use cases; recommendation system, sentiment analysis and object detection using YOLOv2. Overall, we find significant advantages in terms of accuracy and user preference in favor of our method concluding that this approach holds great potential for future applications requiring transparency over AI decision making. Additionally our code implementation publicly available online has already gained some traction from both researchers and practitioners within industry looking at integrating this approach into real world application scenarios where local interpretable fidelity needs ensuring across machine learning pipeline stages. As we move towards more AI integration in daily life activities humans are entitled to not just trust but fully comprehend decisions affecting their personal lives â€“ healthcare, legal issues o",1
"Many methods have been developed to understand complex predictive models and high expectations are placed on post-hoc model explainability. It turns out that such explanations are not robust nor trustworthy, and they can be fooled. This paper presents techniques for attacking Partial Dependence (plots, profiles, PDP), which are among the most popular methods of explaining any predictive model trained on tabular data. We showcase that PD can be manipulated in an adversarial manner, which is alarming, especially in financial or medical applications where auditability became a must-have trait supporting black-box models. The fooling is performed via poisoning the data to bend and shift explanations in the desired direction using genetic and gradient algorithms. To the best of our knowledge, this is the first work performing attacks on variable dependence explanations. The novel approach of using a genetic algorithm for doing so is highly transferable as it generalizes both ways: in a model-agnostic and an explanation-agnostic manner.",0
"This paper introduces data poisoning as a new attack vector against machine learning models that aim to approximate decision trees and use partial dependence plots (PDPs) to explain their predictions. We show how carefully crafted adversarial examples can manipulate these plots to hide unexpected behavior in regions where real inputs would yield different results; thus making PDPs untrustworthy. Further, we demonstrate empirically that existing defenses such as feature importance reordering and clustering can be bypassed easily. To mitigate the effectiveness of our attacks, we propose two novel methods based on sensitivity analysis: an improved feature selection technique that reduces the impact of irrelevant features, and a regularization term to minimize model complexity. Our experiments reveal that these techniques can effectively defend against data poisoning attacks while still preserving the accuracy of the original models. Overall, this work highlights the vulnerability of PDP explanations under data poisoning attacks and the need for more robust interpretation methods.",1
"Recent work proposed $\delta$-relevant inputs (or sets) as a probabilistic explanation for the predictions made by a classifier on a given input. $\delta$-relevant sets are significant because they serve to relate (model-agnostic) Anchors with (model-accurate) PI- explanations, among other explanation approaches. Unfortunately, the computation of smallest size $\delta$-relevant sets is complete for ${NP}^{PP}$, rendering their computation largely infeasible in practice. This paper investigates solutions for tackling the practical limitations of $\delta$-relevant sets. First, the paper alternatively considers the computation of subset-minimal sets. Second, the paper studies concrete families of classifiers, including decision trees among others. For these cases, the paper shows that the computation of subset-minimal $\delta$-relevant sets is in NP, and can be solved with a polynomial number of calls to an NP oracle. The experimental evaluation compares the proposed approach with heuristic explainers for the concrete case of the classifiers studied in the paper, and confirms the advantage of the proposed solution over the state of the art.",0
"In recent years there has been growing interest in designing computational methods that can generate effective explanations for predictions made by machine learning models. One approach to generating such explanations involves finding a subset of features (a relevant set) that most strongly influences the model's prediction. However, existing methods for identifying relevant sets tend to either require significant computation time or sacrifice efficiency for accuracy. We propose a novel method called ""Efficient Explanations with Relevant Sets"" (EERS) which overcomes these limitations by incorporating techniques from submodular optimization theory and efficient indexing data structures. Our experimental results on both synthetic datasets and real-world applications demonstrate that our method outperforms state-of-the-art methods in terms of both effectiveness and efficiency. Additionally, we show how our method can be applied to a wide range of explanation tasks beyond feature relevance, including model diagnosis and causality analysis. Overall, our work represents an important step towards enabling practical applications of explainability technologies in real-world settings.",1
"Machine learning applications have become ubiquitous. This has led to an increased effort of making machine learning trustworthy. Explainable and fair AI have already matured. They address knowledgeable users and application engineers. For those who do not want to invest time into understanding the method or the learned model, we offer care labels: easy to understand at a glance, allowing for method or model comparisons, and, at the same time, scientifically well-based. On one hand, this transforms descriptions as given by, e.g., Fact Sheets or Model Cards, into a form that is well-suited for end-users. On the other hand, care labels are the result of a certification suite that tests whether stated guarantees hold. In this paper, we present two experiments with our certification suite. One shows the care labels for configurations of Markov random fields (MRFs). Based on the underlying theory of MRFs, each choice leads to its specific rating of static properties like, e.g., expressivity and reliability. In addition, the implementation is tested and resource consumption is measured yielding dynamic properties. This two-level procedure is followed by another experiment certifying deep neural network (DNN) models. There, we draw the static properties from the literature on a particular model and data set. At the second level, experiments are generated that deliver measurements of robustness against certain attacks. We illustrate this by ResNet-18 and MobileNetV3 applied to ImageNet.",0
"""The proliferation of machine learning models has led to widespread adoption across industries, but trustworthiness and resource awareness remain significant concerns. In this paper, we introduce the care label concept - a certification suite that addresses these issues by providing a set of criteria for evaluating and communicating the quality attributes of machine learning systems. Our approach incorporates considerations such as interpretability, explainability, accountability, transparency, and resource usage, and provides clear guidance on how to achieve these goals. We discuss our implementation and evaluation of the care label concept using real-world case studies from various domains, highlighting its effectiveness in promoting trustworthy and resource-aware machine learning. Our results demonstrate the value of the care label in enhancing user understanding, improving decision making, reducing risk, and fostering responsible innovation.""",1
"In many classification tasks there is a requirement of monotonicity. Concretely, if all else remains constant, increasing (resp. decreasing) the value of one or more features must not decrease (resp. increase) the value of the prediction. Despite comprehensive efforts on learning monotonic classifiers, dedicated approaches for explaining monotonic classifiers are scarce and classifier-specific. This paper describes novel algorithms for the computation of one formal explanation of a (black-box) monotonic classifier. These novel algorithms are polynomial in the run time complexity of the classifier and the number of features. Furthermore, the paper presents a practically efficient model-agnostic algorithm for enumerating formal explanations.",0
"Despite their widespread use in many fields, monotonic classifiers have limited interpretability due to their lack of explanatory power. This limitation makes them unsuitable for applications where explainability and transparency are crucial, such as medical diagnosis, financial decision making, and legal proceedings. In recent years, there has been increasing interest in addressing these limitations by developing methods that can generate meaningful explanations for the predictions made by monotonic classifiers.  This paper presents an overview of existing approaches for generating explanations for monotonic classifiers, including those based on counterfactual reasoning and feature importance ranking. We describe how each approach works and discuss their strengths and weaknesses in terms of accuracy, robustness, and scalability. We then present new algorithms for generating explanations for monotonic classifiers that achieve higher fidelity than previously proposed methods while preserving computational efficiency. Our experimental evaluation demonstrates the effectiveness of our proposed approach across a range of datasets and application domains. Finally, we discuss future research directions and potential applications of explanations for monotonic classifiers.",1
"A remarkable characteristic of overparameterized deep neural networks (DNNs) is that their accuracy does not degrade when the network's width is increased. Recent evidence suggests that developing compressible representations is key for adjusting the complexity of large networks to the learning task at hand. However, these compressible representations are poorly understood. A promising strand of research inspired from biology is understanding representations at the unit level as it offers a more granular and intuitive interpretation of the neural mechanisms. In order to better understand what facilitates increases in width without decreases in accuracy, we ask: Are there mechanisms at the unit level by which networks control their effective complexity as their width is increased? If so, how do these depend on the architecture, dataset, and training parameters? We identify two distinct types of ""frivolous"" units that proliferate when the network's width is increased: prunable units which can be dropped out of the network without significant change to the output and redundant units whose activities can be expressed as a linear combination of others. These units imply complexity constraints as the function the network represents could be expressed by a network without them. We also identify how the development of these units can be influenced by architecture and a number of training factors. Together, these results help to explain why the accuracy of DNNs does not degrade when width is increased and highlight the importance of frivolous units toward understanding implicit regularization in DNNs.",0
"This paper argues that wider networks are not actually very wide and examines why it may appear otherwise. In particular, we investigate the role of frivolous units (e.g., weak ties) that are often considered important links within social network theory. We contend that these supposedly critical connections are actually quite limited in their scope and effectiveness, and hence should not be overemphasized as they have been in previous research. By exploring how such peripheral elements function within larger networks, we aim to provide a more accurate understanding of network dynamics and inform future interventions designed to strengthen relationships within them.",1
"We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly improved explanations, while keeping computations at a reasonable level.",0
"Graph neural networks (GNNs) have gained popularity due to their state-of-the-art performance on graph structured data. GNNs can perform node classification, link prediction, and other tasks. However, they lack interpretability. In contrast, subgraph exploration methods provide insights into how models make predictions by selecting relevant subgraph structures that drive the model outputs most significantly. We focus on explaining GNN predictive models using subgraph exploration techniques, motivated by real applications in bioinformatics where network structure features like motifs impact biological outcomes. Our contributions consist of formalizing explainability objectives via subgraph explorations; providing theoretical guarantees connecting them to fidelity measures; designing algorithms bridging local optimization and global search strategies for scalability; applying our methodology to large-scale benchmark datasets in protein homology detection and ligand-based virtual screening; comparing effectiveness against competitive baselines and ablation studies; evaluating faithfulness through case analysis, ranking statistics, and human evaluation; establishing guidelines for parameter selection tradeoffs under diverse resource constraints; generalizing beyond single GNN architectures and extensions for handling continuous attributes; and discussing ethical considerations, limitations, future research directions, and applications.",1
"Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore ""what-if"" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent ""notion"" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well and better than existing methods. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions.",0
"This work introduces the framework, which enables us to analyze complex systems by examining them from multiple perspectives simultaneously. We argue that by doing so we can gain insights into how different factors contribute to outcomes. To demonstrate the utility of our approach, we apply it to several case studies spanning domains ranging from biology to computer science. Our results show significant improvements over prior methods both qualitatively and quantitatively. Additionally, we provide extensive empirical evaluations on synthetic benchmarks as well as real-world datasets. Finally, we outline future directions of research for the proposed methodology. Overall, the presented approach provides valuable tools for understanding and modeling complex phenomena across fields.",1
"Inspired by biological evolution, we explain the rationality of Vision Transformer by analogy with the proven practical Evolutionary Algorithm (EA) and derive that both of them have consistent mathematical representation. Analogous to the dynamic local population in EA, we improve the existing transformer structure and propose a more efficient EAT model, and design task-related heads to deal with different tasks more flexibly. Moreover, we introduce the spatial-filling curve into the current vision transformer to sequence image data into a uniform sequential format. Thus we can design a unified EAT framework to address multi-modal tasks, separating the network architecture from the data format adaptation. Our approach achieves state-of-the-art results on the ImageNet classification task compared with recent vision transformer works while having smaller parameters and greater throughput. We further conduct multi-model tasks to demonstrate the superiority of the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves the rank-1 by +3.7 points over the baseline on the CSS dataset.",0
"In recent years, artificial intelligence (AI) has made significant advances in natural language processing, achieving state-of-the-art performance on many challenging tasks such as machine translation, sentiment analysis, and question answering. One key factor contributing to these successes is the development of deep learning techniques that enable machines to learn complex representations from data. However, despite their effectiveness, these models often lack interpretability and can struggle with sequential data structures found in domains like genomics, music, and text generation.  To address these limitations, we propose a new approach inspired by evolutionary algorithms, which leverage principles of selection, mutation, and crossover to generate diverse and adaptive solutions. Our method, called the ""Unified Sequence Model,"" integrates ideas from evolutionary computation into deep neural networks trained end-to-end using gradient descent optimization. By combining the power of genetic algorithms with modern deep learning architectures, our model is capable of solving problems involving both discrete and continuous sequences, making it highly versatile across different application domains.  In particular, we introduce a novel recurrent architecture that learns population dynamics during training, enabling explicit control over search parameters such as population size and diversity measures. This design allows us to effectively balance exploration vs. exploitation throughout the iterative process, leading to improved convergence rates and solution quality compared to baseline methods relying solely on random initialization or incremental refinement. We demonstrate the efficacy of our framework through extensive experiments across five distinct sequence prediction benchmarks, outperforming various competitive methods in terms of accuracy, efficiency, and robustness.  Our work provides a valuable contribution towards developing more intelligent systems able to handle complex, real-world scenarios without sacrificing explainabilit",1
"Over the past two decades, biometric recognition has exploded into a plethora of different applications around the globe. This proliferation can be attributed to the high levels of authentication accuracy and user convenience that biometric recognition systems afford end-users. However, in-spite of the success of biometric recognition systems, there are a number of outstanding problems and concerns pertaining to the various sub-modules of biometric recognition systems that create an element of mistrust in their use - both by the scientific community and also the public at large. Some of these problems include: i) questions related to system recognition performance, ii) security (spoof attacks, adversarial attacks, template reconstruction attacks and demographic information leakage), iii) uncertainty over the bias and fairness of the systems to all users, iv) explainability of the seemingly black-box decisions made by most recognition systems, and v) concerns over data centralization and user privacy. In this paper, we provide an overview of each of the aforementioned open-ended challenges. We survey work that has been conducted to address each of these concerns and highlight the issues requiring further attention. Finally, we provide insights into how the biometric community can address core biometric recognition systems design issues to better instill trust, fairness, and security for all.",0
"""Biometric technologies have rapidly developed over recent years to become one of the most reliable methods for secure authentication and verification.""",1
"Value factorisation proves to be a very useful technique in multi-agent reinforcement learning (MARL), but the underlying mechanism is not yet fully understood. This paper explores a theoretic basis for value factorisation. We generalise the Shapley value in the coalitional game theory to a Markov convex game (MCG) and use it to guide value factorisation in MARL. We show that the generalised Shapley value possesses several features such as (1) accurate estimation of the maximum global value, (2) fairness in the factorisation of the global value, and (3) being sensitive to dummy agents. The proposed theory yields a new learning algorithm called Sharpley Q-learning (SHAQ), which inherits the important merits of ordinary Q-learning but extends it to MARL. In comparison with prior-arts, SHAQ has a much weaker assumption (MCG) that is more compatible with real-world problems, but has superior explainability and performance in many cases. We demonstrated SHAQ and verified the theoretic claims on Predator-Prey and StarCraft Multi-Agent Challenge (SMAC).",0
"This is a technical research paper that proposes using shapley value theory within q-learning for multi agent reinforcement learning systems. The main focus of the proposed approach is to incorporate fairness and individual contribution calculation during the training process of q-learning models. By doing so, we aim at providing more stable results under unstable environments as well as creating agents that take other participantsâ€™ impacts into account while making decisions. We test our method on common grid world games scenarios such as Battle of the Sexes, Market Entry Game and Minority game where two cooperative policies are implemented; one which encourages full participation called Unanimous policy (Un) ,and another which discourages defection called Grim trigger(GT). Experimental evaluation shows significant improvement over classicalq-learning methods without considering individual contributions. Overall, the proposed approach provides both theoretical insights and promising experimental outcomes under various circumstances.",1
"Reparameterization (RP) and likelihood ratio (LR) gradient estimators are used to estimate gradients of expectations throughout machine learning and reinforcement learning; however, they are usually explained as simple mathematical tricks, with no insight into their nature. We use a first principles approach to explain that LR and RP are alternative methods of keeping track of the movement of probability mass, and the two are connected via the divergence theorem. Moreover, we show that the space of all possible estimators combining LR and RP can be completely parameterized by a flow field $u(x)$ and an importance sampling distribution $q(x)$. We prove that there cannot exist a single-sample estimator of this type outside our characterized space, thus, clarifying where we should be searching for better Monte Carlo gradient estimators.",0
This should describe your work without using technical jargon but still conveys what your work entails. Please use language accessible to everyday people!,1
"Algorithmic decision systems have frequently been labelled as ""biased"", ""racist"", ""sexist"", or ""unfair"" by numerous media outlets, organisations, and researchers. There is an ongoing debate about whether such assessments are justified and whether citizens and policymakers should be concerned. These and other related matters have recently become a hot topic in the context of biometric technologies, which are ubiquitous in personal, commercial, and governmental applications. Biometrics represent an essential component of many surveillance, access control, and operational identity management systems, thus directly or indirectly affecting billions of people all around the world.   Recently, the European Association for Biometrics organised an event series with ""demographic fairness in biometric systems"" as an overarching theme. The events featured presentations by international experts from academic, industry, and governmental organisations and facilitated interactions and discussions between the experts and the audience. Further consultation of experts was undertaken by means of a questionnaire. This work summarises opinions of experts and findings of said events on the topic of demographic fairness in biometric systems including several important aspects such as the developments of evaluation metrics and standards as well as related issues, e.g. the need for transparency and explainability in biometric systems or legal and ethical issues.",0
"This study aimed to explore demographic fairness in biometric systems by examining the opinions of experts in the field. With the increasing use of biometrics in various applications, ensuring equal treatment across different populations has become a crucial concern. We conducted semi-structured interviews with 20 professionals working on biometric technology development and implementation to gain insights into their perspectives on demographic fairness. Our findings suggest that while participants agreed on the importance of demographic fairness, they differed in their views on how to achieve it. Some highlighted the need for diversity in data collection and representation, whereas others emphasized the significance of algorithms and feature extraction techniques. Participants also acknowledged challenges associated with addressing demographic differences, including technical limitations, cost constraints, and societal factors. Overall, our results provide valuable insights into the complexities surrounding demographic fairness in biometric systems and call for further research in developing effective strategies to ensure equitable treatment for all population groups.",1
"Explainable artificial intelligence is the attempt to elucidate the workings of systems too complex to be directly accessible to human cognition through suitable side-information referred to as ""explanations"". We present a trainable explanation module for convolutional image classifiers we call bounded logit attention (BLA). The BLA module learns to select a subset of the convolutional feature map for each input instance, which then serves as an explanation for the classifier's prediction. BLA overcomes several limitations of the instancewise feature selection method ""learning to explain"" (L2X) introduced by Chen et al. (2018): 1) BLA scales to real-world sized image classification problems, and 2) BLA offers a canonical way to learn explanations of variable size. Due to its modularity BLA lends itself to transfer learning setups and can also be employed as a post-hoc add-on to trained classifiers. Beyond explainability, BLA may serve as a general purpose method for differentiable approximation of subset selection. In a user study we find that BLA explanations are preferred over explanations generated by the popular (Grad-)CAM method.",0
"This paper presents a method for learning interpretable representations that can explain the behavior of image classifiers. The proposed approach uses the bounded logit model from decision theory as a framework for defining interpretability, which allows us to quantify how well different features contribute to the classification output. We then use a variant of deep reinforcement learning called policy gradient descent with Monte Carlo sampling (PGDMC) to optimize the feature extractor so that it maximizes the expected value of interpretability according to the bounded logit model. Our experiments demonstrate that our method is effective at generating explanations for image classifiers, and we compare the resulting explanations to those generated by other methods such as GradCAM and Guided Backprop. Additionally, we show that these learned representations can be used as features for downstream tasks like object detection, demonstrating their utility beyond just explaining classifier behavior. Overall, our work shows that it is possible to learn interpretable representations using machine learning techniques, and has implications for improving transparency and trustworthiness in artificial intelligence systems.",1
"Explaining the decisions of machine learning models is becoming a necessity in many areas where trust in ML models decision is key to their accreditation/adoption. The ability to explain models decisions also allows to provide diagnosis in addition to the model decision, which is highly valuable in scenarios such as fault detection. Unfortunately, high-performance models do not exhibit the necessary transparency to make their decisions fully understandable. And the black-boxes approaches, which are used to explain such model decisions, suffer from a lack of accuracy in tracing back the exact cause of a model decision regarding a given input. Indeed, they do not have the ability to explicitly describe the decision regions of the model around that input, which is necessary to determine what influences the model towards one decision or the other. We thus asked ourselves the question: is there a category of high-performance models among the ones currently used for which we could explicitly and exactly characterise the decision regions in the input feature space using a geometrical characterisation? Surprisingly we came out with a positive answer for any model that enters the category of tree ensemble models, which encompasses a wide range of high-performance models such as XGBoost, LightGBM, random forests ... We could derive an exact geometrical characterisation of their decision regions under the form of a collection of multidimensional intervals. This characterisation makes it straightforward to compute the optimal counterfactual (CF) example associated with a query point. We demonstrate several possibilities of the approach, such as computing the CF example based only on a subset of features. This allows to obtain more plausible explanations by adding prior knowledge about which variables the user can control. An adaptation to CF reasoning on regression problems is also envisaged.",0
"An exact counterfactual-example-based approach provides a promising solution for improving the interpretability of tree ensemble models. By considering specific examples that illustrate how individual trees contribute to the overall decision, this method allows for more nuanced understanding of model behavior than traditional feature importance metrics alone. Moreover, by exploiting both positive and negative examples effectively, this approach can provide insight into the complex interactions among different features that underlie the final prediction. Ultimately, these findings have important implications for the development of machine learning systems that rely on tree ensembles as well as their application in real world settings where transparency and accountability are critical concerns. In conclusion, our study offers new perspectives on the problem of model interpretability, highlighting the benefits of a carefully designed, example-based analysis framework.",1
"We present an empirical study of debiasing methods for classifiers, showing that debiasers often fail in practice to generalize out-of-sample, and can in fact make fairness worse rather than better. A rigorous evaluation of the debiasing treatment effect requires extensive cross-validation beyond what is usually done. We demonstrate that this phenomenon can be explained as a consequence of bias-variance trade-off, with an increase in variance necessitated by imposing a fairness constraint. Follow-up experiments validate the theoretical prediction that the estimation variance depends strongly on the base rates of the protected class. Considering fairness--performance trade-offs justifies the counterintuitive notion that partial debiasing can actually yield better results in practice on out-of-sample data.",0
"""In recent years, there has been increasing concern over bias in machine learning algorithms and their potential impact on society. However, the ability to debias these systems remains largely unexplored territory. This study aims to shed light on the effectiveness of current debiasing techniques by comparing real-world results with expected outcomes. Our findings suggest that while some methods may have promise, they often fall short of achieving desired levels of fairness and accuracy. Furthermore, we identify several challenges in evaluating and implementing debiasing strategies, which highlights the need for continued research in this area.""  ""Our analysis reveals significant gaps between anticipated performance and actual outcomes in terms of reducing discrimination and improving decision making. These discrepancies raise questions about the feasibility of attaining truly unbiased classification models. We argue that more nuanced approaches are necessary to address complex societal issues related to algorithmic unfairness. Therefore, our work contributes new insights into the limitations of existing debiasing techniques and proposes promising directions for future research.""",1
"Understanding how large neural networks avoid memorizing training data is key to explaining their high generalization performance. To examine the structure of when and where memorization occurs in a deep network, we use a recently developed replica-based mean field theoretic geometric analysis method. We find that all layers preferentially learn from examples which share features, and link this behavior to generalization performance. Memorization predominately occurs in the deeper layers, due to decreasing object manifolds' radius and dimension, whereas early layers are minimally affected. This predicts that generalization can be restored by reverting the final few layer weights to earlier epochs before significant memorization occurred, which is confirmed by the experiments. Additionally, by studying generalization under different model sizes, we reveal the connection between the double descent phenomenon and the underlying model geometry. Finally, analytical analysis shows that networks avoid memorization early in training because close to initialization, the gradient contribution from permuted examples are small. These findings provide quantitative evidence for the structure of memorization across layers of a deep neural network, the drivers for such structure, and its connection to manifold geometric properties.",0
"Abstract Deep learning has revolutionized fields from computer vision to natural language processing, but there remains a fundamental lack of understanding into how these models learn complex representations from simple input data. Here we show that two distinct modes underlie the operation of deep neural nets: geometry-inducing (GIM) and memorizing (M). GIM refers to training dynamics characterized by rapid decrease in loss during most iterations before plateauing at higher values. M exhibits monotonically decreasing loss throughout training. Analysis across architectures, datasets, and hyperparameters reveals that both types exist within every network trained on any problem, although their prominence varies strongly depending on conditions. Different regimes emerge, spanning pure GIM, mixed GIM+M, and pure M phases during training. These patterns extend to test time predictions, where we uncover dramatic differences in performance between them, suggesting they correspond to different forms of knowledge acquired during training. Our study highlights an intrinsic bimodal nature underlying deep learning, offering new insights into learning regimes heretofore overlooked while providing guidance towards better design choices through control of the balance between GIM and M behaviors. Keywords Generalization; Memorization; Neural Networks; Deep Learning; Training Dynamics. Submitted.",1
"We propose to use a model-based generative loss for training hand pose estimators on depth images based on a volumetric hand model. This additional loss allows training of a hand pose estimator that accurately infers the entire set of 21 hand keypoints while only using supervision for 6 easy-to-annotate keypoints (fingertips and wrist). We show that our partially-supervised method achieves results that are comparable to those of fully-supervised methods which enforce articulation consistency. Moreover, for the first time we demonstrate that such an approach can be used to train on datasets that have erroneous annotations, i.e. ""ground truth"" with notable measurement errors, while obtaining predictions that explain the depth images better than the given ""ground truth"".",0
"One potential way to overcome annotation errors in depth-based hand pose estimation tasks could involve utilizing generative modeling techniques such as Generative Adversarial Networks (GANs). By training a GAN on large amounts of annotated data, we can generate synthetic samples that can then be used to augment existing datasets and reduce reliance on accurate annotations. This approach has been shown to improve the accuracy of deep learning models trained on these datasets by providing more diverse and realistic examples during training. Furthermore, through careful selection of hyperparameters such as batch size and number of epochs, it may be possible to control overfitting and ensure high quality results. By leveraging the power of generative modeling, researchers in computer vision and machine learning fields can continue to push the boundaries of state-of-the-art performance in challenging tasks like hand pose estimation.",1
"In this article, we introduce a novel variant of the Tsetlin machine (TM) that randomly drops clauses, the key learning elements of a TM. In effect, TM with drop clause ignores a random selection of the clauses in each epoch, selected according to a predefined probability. In this way, additional stochasticity is introduced in the learning phase of TM. Along with producing more distinct and well-structured patterns that improve the performance, we also show that dropping clauses increases learning robustness. To explore the effects clause dropping has on accuracy, training time, and interpretability, we conduct extensive experiments on various benchmark datasets in natural language processing (NLP) (IMDb and SST2) as well as computer vision (MNIST and CIFAR10). In brief, we observe from +2% to +4% increase in accuracy and 2x to 4x faster learning. We further employ the Convolutional TM to document interpretable results on the CIFAR10 dataset. To the best of our knowledge, this is the first time an interpretable machine learning algorithm has been used to produce pixel-level human-interpretable results on CIFAR10. Also, unlike previous interpretable methods that focus on attention visualisation or gradient interpretability, we show that the TM is a more general interpretable method. That is, by producing rule-based propositional logic expressions that are \emph{human}-interpretable, the TM can explain how it classifies a particular instance at the pixel level for computer vision and at the word level for NLP.",0
"Artificial intelligence (AI) has come a long way since its inception, but one major challenge remains - making AI systems more human interpretable. This paper presents an innovative method that enhances the stochasticity of Tsetlin machines using drop clause techniques. By incorporating these methods into existing AI models, we can improve their ability to generate explainable results while still maintaining their effectiveness. The proposed approach allows humans to better understand how decisions were made by the model, leading to greater transparency and trustworthiness. Results from extensive experiments on several benchmark datasets demonstrate that our method significantly improves human interpretability without sacrificing performance. Our findings have important implications for the future development of AI, as well as potential applications across various domains where explainable decision-making is crucial. Overall, this work represents a significant step towards creating more intelligent, efficient and interpretable AI systems.",1
"As the application of deep neural networks proliferates in numerous areas such as medical imaging, video surveillance, and self driving cars, the need for explaining the decisions of these models has become a hot research topic, both at the global and local level. Locally, most explanation methods have focused on identifying relevance of features, limiting the types of explanations possible. In this paper, we investigate a new direction by leveraging latent features to generate contrastive explanations; predictions are explained not only by highlighting aspects that are in themselves sufficient to justify the classification, but also by new aspects which if added will change the classification. The key contribution of this paper lies in how we add features to rich data in a formal yet humanly interpretable way that leads to meaningful results. Our new definition of ""addition"" uses latent features to move beyond the limitations of previous explanations and resolve an open question laid out in Dhurandhar, et. al. (2018), which creates local contrastive explanations but is limited to simple datasets such as grayscale images. The strength of our approach in creating intuitive explanations that are also quantitatively superior to other methods is demonstrated on three diverse image datasets (skin lesions, faces, and fashion apparel). A user study with 200 participants further exemplifies the benefits of contrastive information, which can be viewed as complementary to other state-of-the-art interpretability methods.",0
"In recent years, there has been significant interest in developing machine learning models that can provide local explanations for their predictions, allowing users to better understand why certain decisions were made and providing insights into potential areas for improvement. However, traditional methods for generating these explanations often require substantial computational resources and may still struggle to accurately capture important features of the input data. To address these limitations, we propose a novel approach based on leveraging latent feature representations learned by deep neural networks. By analyzing the relationships between these latent features and model outputs, we demonstrate how to generate accurate, informative, and efficient local explanations that effectively highlight key factors influencing prediction results. Our experiments show that our method outperforms existing techniques across a range of benchmark datasets while significantly reducing computation requirements. We believe our work represents an important step forward in enabling more interpretable and transparent artificial intelligence systems and contributes valuable new tools and insights towards achieving these goals.",1
"Feature attributions and counterfactual explanations are popular approaches to explain a ML model. The former assigns an importance score to each input feature, while the latter provides input examples with minimal changes to alter the model's predictions. To unify these approaches, we provide an interpretation based on the actual causality framework and present two key results in terms of their use. First, we present a method to generate feature attribution explanations from a set of counterfactual examples. These feature attributions convey how important a feature is to changing the classification outcome of a model, especially on whether a subset of features is necessary and/or sufficient for that change, which attribution-based methods are unable to provide. Second, we show how counterfactual examples can be used to evaluate the goodness of an attribution-based explanation in terms of its necessity and sufficiency. As a result, we highlight the complementarity of these two approaches. Our evaluation on three benchmark datasets - Adult-Income, LendingClub, and German-Credit - confirms the complementarity. Feature attribution methods like LIME and SHAP and counterfactual explanation methods like Wachter et al. and DiCE often do not agree on feature importance rankings. In addition, by restricting the features that can be modified for generating counterfactual examples, we find that the top-k features from LIME or SHAP are often neither necessary nor sufficient explanations of a model's prediction. Finally, we present a case study of different explanation methods on a real-world hospital triage problem",0
"In this paper, we introduce two methodologies that aim to explain how machine learning models make predictions by providing insights into feature importance rankings and model behavior under different inputs. We evaluate their effectiveness at explaining decisions on benchmark datasets and compare them using human subject experiments. Our findings show that both methods are effective at improving human understanding of ML predictions but they differ significantly in their ability to provide actionable recommendations. Additionally, while counterfactuals appear more interpretable to humans than attributions, the latter may still offer valuable insights in certain applications. Ultimately, our work emphasizes the complementary nature of these approaches as means towards enhancing transparency and trustworthiness in predictive systems. (Written below) Abstract: In this study, we present two techniques designed to enhance the interpretability of machine learning models. Through feature attribution and counterfactual explanations, we demonstrate the ways in which these algorithms make predictions. To assess their impact, we conducted evaluations utilizing standardized datasets and human participant studies. Results indicated that both approaches can improve user comprehension; however, their performance varies with regard to identifying areas requiring modification. Moreover, although counterfactuals were rated more intuitively graspable compared to attribute visualizations, there remain instances where the latter proved advantageous. By highlighting their interconnectedness, our research underscores the potential value inherent in combining these techniques. Overall, these discoveries have important implications for fostering trust in predictive models through better transparency, ultimately promoting responsible artificial intelligence development and deployment.",1
"We introduce a new problem, that of undefined class-label (UCL) detection. For instance, if we try to classify an image of a radio as cat vs dog, there will be no well-defined class label. In contrast, in out-of-distribution (OOD) detection, we are interested in the related but different problem of identifying regions of the input space with little training data, which might result in poor classifier performance. This difference is critical: it is quite possible for there to be a region of the input space where little training data is available but where class-labels are well-defined. Likewise, there may be regions with lots of training data, but without well-defined class-labels (though in practice this would often be the result of a bug in the labelling pipeline). We note that certain methods originally intended to detect OOD inputs might actually be detecting UCL points and develop a method for training on UCL points based on a generative model of data-curation originally used to explain the cold posterior effect in Bayesian neural networks. This approach gives superior performance to past methods originally intended for OOD detection.",0
"""Undefined class-label"" detecâ€¦",1
"Few-shot and one-shot learning have been the subject of active and intensive research in recent years, with mounting evidence pointing to successful implementation and exploitation of few-shot learning algorithms in practice. Classical statistical learning theories do not fully explain why few- or one-shot learning is at all possible since traditional generalisation bounds normally require large training and testing samples to be meaningful. This sharply contrasts with numerous examples of successful one- and few-shot learning systems and applications.   In this work we present mathematical foundations for a theory of one-shot and few-shot learning and reveal conditions specifying when such learning schemes are likely to succeed. Our theory is based on intrinsic properties of high-dimensional spaces. We show that if the ambient or latent decision space of a learning machine is sufficiently high-dimensional than a large class of objects in this space can indeed be easily learned from few examples provided that certain data non-concentration conditions are met.",0
"""Few-shot and one-shot learning have been major topics within machine learning for many years now, but remain mysterious to many people outside of the field, including business leaders, decision makers, investors, and journalists who want to know more about how these technologies work and why they matter,"" writes [author name] in a new research paper that provides a comprehensive explanation of few-shot and one-shot learning concepts. ""For starters, the author explains, few-shot and one-shot learning are based on neural networks designed to learn from small amounts of data (i.e., 'few shots') or single examples, respectively."" For example, image classification tasks typically require large datasets consisting of thousands of images labeled by human annotators before standard neural network architectures can perform well on them,"" says [author name].",1
"Instance-based interpretation methods have been widely studied for supervised learning methods as they help explain how black box neural networks predict. However, instance-based interpretations remain ill-understood in the context of unsupervised learning. In this paper, we investigate influence functions [20], a popular instance-based interpretation method, for a class of deep generative models called variational auto-encoders (VAE). We formally frame the counter-factual question answered by influence functions in this setting, and through theoretical analysis, examine what they reveal about the impact of training samples on classical unsupervised learning methods. We then introduce VAE-TracIn, a computationally efficient and theoretically sound solution based on Pruthi et al. [28], for VAEs. Finally, we evaluate VAE-TracIn on several real world datasets with extensive quantitative and qualitative analysis.",0
"""Instance-Based Interpretability"" refers to understanding machine learning models by examining their outputs on specific instances (data points). In ""Understanding Instance-based Interpretability of Variational Auto-Encoders"", we explore how looking at individual examples can provide insight into how these complex generative models work. We propose a method for analyzing variational autoencoders (VAEs), which are trained as pairs of encoder/decoder networks that maximize data density while minimizing reconstruction error. Our approach uses sensitivity analysis techniques to analyze VAE behavior on specific inputs and gain insights into why certain decisions are made during training and inference. Through experiments on several datasets, we demonstrate the effectiveness of our approach and showcase some interesting findings about the inner workings of VAEs. Overall, our study provides valuable new tools for understanding instance-level interpretability in deep learning models and paves the way for more informed model development and deployment.",1
"Despite recent advances in its theoretical understanding, there still remains a significant gap in the ability of existing PAC-Bayesian theories on meta-learning to explain performance improvements in the few-shot learning setting, where the number of training examples in the target tasks is severely limited. This gap originates from an assumption in the existing theories which supposes that the number of training examples in the observed tasks and the number of training examples in the target tasks follow the same distribution, an assumption that rarely holds in practice. By relaxing this assumption, we develop two PAC-Bayesian bounds tailored for the few-shot learning setting and show that two existing meta-learning algorithms (MAML and Reptile) can be derived from our bounds, thereby bridging the gap between practice and PAC-Bayesian theories. Furthermore, we derive a new computationally-efficient PACMAML algorithm, and show it outperforms existing meta-learning algorithms on several few-shot benchmark datasets.",0
"In recent years, few-shot meta-learning has emerged as a promising approach to achieve artificial intelligence that can learn from just a few examples. Probabilistic Approximate Computation (PAC)-Bayesian theory provides a theoretical framework for understanding and analyzing these methods, but there remains a significant gap between practice and theory. This paper seeks to bridge that gap by providing a comprehensive analysis of several state-of-the-art algorithms within a unified PAC-Bayesian setting. We demonstrate how existing algorithms can be seen as approximating different solutions to the same underlying problem, highlighting their respective strengths and weaknesses. Our results provide new insights into the behavior of these algorithms and open up opportunities for improving them through principled design choices grounded in theory. By bridging the gap between practice and PAC-Bayes theory, we aim to advance our understanding of few-shot learning and contribute towards creating intelligent systems that truly adapt to new situations with only a handful of data points.",1
"We propose a new semi-supervised learning method of Variational AutoEncoder (VAE) which yields an explainable latent space by EXplainable encoder Network (EXoN). The EXoN provides two useful tools for implementing VAE. First, we can freely assign a conceptual center of the latent distribution for a specific label. The latent space of VAE is separated with the multi-modal property of the Gaussian mixture distribution according to the labels of observations. Next, we can easily investigate the latent subspace by a simple statistics obtained from the EXoN. We found that both the negative cross-entropy and the Kullback-Leibler divergence play a crucial role in constructing explainable latent space and the variability of generated samples from our proposed model depends on a specific subspace, called `activated latent subspace'. With MNIST and CIFAR-10 dataset, we show that the EXoN can produce an explainable latent space that effectively represents the labels and characteristics of the images.",0
"""This paper introduces a new model architecture called EXoN (EXplainable encoder Network) which addresses the issue of explainability in deep learning by incorporating causality into the neural network. Current state-of-the art methods suffer from lack of transparency, interpretability, or justification of their decisions, often leading to unjustified conclusions. To address these limitations, we propose using a combination of mechanisms such as attention maps and decision trees within our proposed architecture. Through rigorous evaluation on benchmark datasets, we demonstrate that the inclusion of these components results in more interpretable models capable of outperforming comparative baseline architectures. Furthermore, we showcase how our model allows for human experts to have greater confidence in the predictions generated by the model through visualizations provided by the internal workings of our architecture.""",1
"Recent interest in on-orbit servicing and Active Debris Removal (ADR) missions have driven the need for technologies to enable non-cooperative rendezvous manoeuvres. Such manoeuvres put heavy burden on the perception capabilities of a chaser spacecraft. This paper demonstrates Convolutional Neural Networks (CNNs) capable of providing an initial coarse pose estimation of a target from a passive thermal infrared camera feed. Thermal cameras offer a promising alternative to visible cameras, which struggle in low light conditions and are susceptible to overexposure. Often, thermal information on the target is not available a priori; this paper therefore proposes using visible images to train networks. The robustness of the models is demonstrated on two different targets, first on synthetic data, and then in a laboratory environment for a realistic scenario that might be faced during an ADR mission. Given that there is much concern over the use of CNN in critical applications due to their black box nature, we use innovative techniques to explain what is important to our network and fault conditions.",0
"This research explores using convolutional neural networks (CNN) for relative pose estimation of non-cooperative spacecraft with thermal infrared imagery (TIR). The objective of this study was to develop an approach that could accurately estimate the position and orientation of an uncooperative object from TIR images alone. To accomplish this task, we trained a deep learning model on a dataset of synthetic TIR images rendered from high fidelity physics simulations and ground truth data. We then tested our approach on real world datasets and compared our results to traditional feature extraction methods such as template matching and optical flow. Our experimental results demonstrated that the proposed CNN based method outperformed other techniques by achieving better accuracy and robustness under varying conditions. The findings from this work have implications for future mission planning and execution where accurate relative pose estimation can greatly enhance situational awareness of a spacecraft operating in close proximity to another satellite or asteroid surface. Furthermore, these results suggest potential applications beyond space missions, including autonomous navigation systems or drones equipped with thermal cameras. Overall, this research advances our understanding of how computer vision algorithms can assist in tackling complex problems within the field of astronomy and planetary science.",1
"The increasing number of regulations and expectations of predictive machine learning models, such as so called right to explanation, has led to a large number of methods promising greater interpretability. High demand has led to a widespread adoption of XAI techniques like Shapley values, Partial Dependence profiles or permutational variable importance. However, we still do not know enough about their properties and how they manifest in the context in which explanations are created by analysts, reviewed by auditors, and interpreted by various stakeholders. This paper highlights a blind spot which, although critical, is often overlooked when monitoring and auditing machine learning models: the effect of the reference data on the explanation calculation. We discuss that many model explanations depend directly or indirectly on the choice of the referenced data distribution. We showcase examples where small changes in the distribution lead to drastic changes in the explanations, such as a change in trend or, alarmingly, a conclusion. Consequently, we postulate that obtaining robust and useful explanations always requires supporting them with a broader context.",0
"""In recent years, the development of machine learning models has been rapidly increasing due to their ability to perform complex tasks such as image classification, speech recognition, and natural language processing. However, there remains a major challenge associated with these systems - understanding how they work."" ""Explanation techniques have emerged as critical tools that enable human users to gain insights into the behavior and output of these models. Unfortunately, current explanation methods suffer from a significant shortcoming known as the 'explainability gap,' which refers to the discrepancy between the true reason behind a decision made by a model and the explanation provided to the user."" ""This study addresses the issue of limited interpretability in black box models through a novel framework called 'Blind Spot Expansion.' By introducing controlled noise during training, we can enhance our understanding of the decision boundaries and improve the quality of the explanations generated by the models."" ""The proposed methodology was tested on several benchmark datasets and showed improved results compared to traditional explanation approaches. Our findings demonstrate the effectiveness of Blind Spot Expansion in bridging the explainability gap, making it possible to provide more accurate and reliable explanations for end-users."" ""Overall, this research contributes significantly to the field of machine learning by providing new insights into the challenges of explaining black box models and presenting solutions towards making them transparent and accountable."" ""In conclusion, Blind Spot Expansion represents a promising direction for advancing artificial intelligence and enabling humans to trust the decisions made by these powerful computational tools.""",1
"Labelled networks form a very common and important class of data, naturally appearing in numerous applications in science and engineering. A typical inference goal is to determine how the vertex labels(or {\em features}) affect the network's graph structure. A standard approach has been to partition the network into blocks grouped by distinct values of the feature of interest. A block-based random graph model -- typically a variant of the stochastic block model -- is then used to test for evidence of asymmetric behaviour within these feature-based communities. Nevertheless, the resulting communities often do not produce a natural partition of the graph. In this work, we introduce a new generative model, the feature-first block model (FFBM), which is more effective at describing vertex-labelled undirected graphs and also facilitates the use of richer queries on labelled networks. We develop a Bayesian framework for inference with this model, and we present a method to efficiently sample from the posterior distribution of the FFBM parameters. The FFBM's structure is kept deliberately simple to retain easy interpretability of the parameter values. We apply the proposed methods to a variety of network data to extract the most important features along which the vertices are partitioned. The main advantages of the proposed approach are that the whole feature-space is used automatically, and features can be rank-ordered implicitly according to impact. Any features that do not significantly impact the high-level structure can be discarded to reduce the problem dimension. In cases where the vertex features available do not readily explain the community structure in the resulting network, the approach detects this and is protected against over-fitting. Results on several real-world datasets illustrate the performance of the proposed methods.",0
"Characterize your network by how likely any two points are connected, rather than assuming edges are present unless otherwise specified. Estimate edge probabilities from node attributes using logistic regression, and estimate edge counts as binomial draws therefrom. Now you can infer local clustering coefficient (since that depends only on triples of adjacent nodes), global clustering coefficient (estimating it via triangular motifs whose sizes depend linearly on the number of nodes), assortativity coefficients (since they each involve computing expected proportions of four node types based on quartets), plus other statistics depending on other graphlets or subgraph patterns one cares about. And these can all be bootstrapped to assess statistical significance. We argue that this new approach has better scaling behavior compared to methods involving explicit enumeration of clique or motif counts, particularly when the latter exceed 6 nodes and thus need partitioning into smaller non-overlapping subsets. For instance, we show empirically that for synthetic graphs generated with scale-free degree distributions similar to the internet at different size scales up to , our method reduces computation time on average by over 2 orders of magnitude â€” often allowing inference within minutes. Moreover we demonstrate that unlike previous approaches that required ad hoc modifications for disassortative graphs or multiscale effects, our scheme naturally accommodates such variations simply due to its probabilisitic foundations. Finally, the technique generalizes straightforwardly to directed graphs, bipartite graphs, and other kinds of weighted graphs, albeit incurri",1
"Identifying correct discretization schemes of continuous stochastic processes is an important task, which is needed to infer model parameters from experimental observations. Motivated by the observation that consistent discretizations of continuous models should be invariant under temporal coarse graining, we derive an explicit Renormalization Group transformation on linear stochastic time series and show that the Renormalization Group fixed points correspond to discretizations of naturally occuring physical dynamics. Our fixed point analysis explains why standard embedding procedures do not allow for reconstructing hidden Markov dynamics, and why the Euler-Maruyama scheme applied to underdamped Langevin equations works for numerical integration, but not to derive the likelihood of a partially observed process in the context of parametric inference.",0
"This is just one possible starting point, but you could consider using more advanced mathematical language if that would suit your target audience better:  In this paper we explore the use of renormalization group (RG) techniques to bridge the gap between discrete- and continuous-time descriptions of Gaussian processes. We begin by reviewing some basic concepts from both discrete- and continuous-time probability theory, highlighting key similarities and differences between the two approaches. Then, drawing on recent work in the field of RG analysis, we develop a framework for systematically combining these different perspectives into a single unified description. Along the way, we encounter a number of interesting challenges related to regularization, scaling, and universality, which we address through a combination of analytical, numerical, and experimental methods. Our results provide new insights into the behavior of Gaussian processes across time scales, and open up exciting possibilities for further research in fields ranging from applied mathematics to machine learning. In conclusion, we believe this work represents an important step towards bridging the divide between discrete- and continuous-time models, and look forward to seeing how our approach will continue to evolve as the field continues to grow.",1
"One-shot learning is proposed to make a pretrained classifier workable on a new dataset based on one labeled samples from each pattern. However, few of researchers consider whether the dataset itself supports one-shot learning. In this paper, we propose a set of definitions to explain what kind of datasets can support one-shot learning and propose the concept ""absolute generalization"". Based on these definitions, we proposed a method to build an absolutely generalizable classifier. The proposed method concatenates two samples as a new single sample, and converts a classification problem to an identity identification problem or a similarity metric problem. Experiments demonstrate that the proposed method is superior to baseline on one-shot learning datasets and artificial datasets.",0
"In this paper we address the problem of one-shot learning. We focus on absolute generalization: given only one example of a new concept, the system must perform accurately on that concept across all tasks and domains. This stands in contrast to typical few shot learning methods which have trouble with zero/few-shot accuracy across tasks/domains. Our approach involves pretraining a deep neural network using both object detection and image classification training data, then fine tuning it on small datasets where each task has exactly one labeled example per class. We achieve state of the art results on standard benchmarks without any additional techniques such as meta learning or ensembling, while still maintaining computational efficiency and easy implementation. Overall our work shows promise towards realizing true artificial intelligence agents capable of strong zero/one-shot performance across arbitrary tasks.",1
"Functional connectivity (FC) between regions of the brain can be assessed by the degree of temporal correlation measured with functional neuroimaging modalities. Based on the fact that these connectivities build a network, graph-based approaches for analyzing the brain connectome have provided insights into the functions of the human brain. The development of graph neural networks (GNNs) capable of learning representation from graph structured data has led to increased interest in learning the graph representation of the brain connectome. Although recent attempts to apply GNN to the FC network have shown promising results, there is still a common limitation that they usually do not incorporate the dynamic characteristics of the FC network which fluctuates over time. In addition, a few studies that have attempted to use dynamic FC as an input for the GNN reported a reduction in performance compared to static FC methods, and did not provide temporal explainability. Here, we propose STAGIN, a method for learning dynamic graph representation of the brain connectome with spatio-temporal attention. Specifically, a temporal sequence of brain graphs is input to the STAGIN to obtain the dynamic graph representation, while novel READOUT functions and the Transformer encoder provide spatial and temporal explainability with attention, respectively. Experiments on the HCP-Rest and the HCP-Task datasets demonstrate exceptional performance of our proposed method. Analysis of the spatio-temporal attention also provide concurrent interpretation with the neuroscientific knowledge, which further validates our method. Code is available at https://github.com/egyptdj/stagin",0
"In this paper, we propose a novel approach for learning dynamic graph representation of brain connectome using spatio-temporal attention mechanisms. Brain connectomics has gained significant interest in recent years due to its potential applications in neuroscience research and clinical practice. Traditional static graph representations have been widely used to analyze connectomes but fail to capture temporal dynamics that occur during brain activity. Therefore, developing effective methods for representing dynamic graph connectomes is crucial for better understanding neural processes. Our method incorporates spatio-temporal attention mechanisms inspired by human visual attention systems into recurrent neural networks (RNNs) to learn more expressive representations. We demonstrate the effectiveness of our approach through experiments on simulated data and two real fMRI datasets, showing improved performance over state-of-the-art methods in capturing temporal dynamics. Our results suggest that the proposed method can effectively model complex relationships between spatially distributed features and their evolution over time in dynamic brain networks. Further, these learned models could potentially improve our understanding of cognitive processes such as attention, decision making, and mental disorders.",1
"The multiplicative structure of parameters and input data in the first layer of neural networks is explored to build connection between the landscape of the loss function with respect to parameters and the landscape of the model function with respect to input data. By this connection, it is shown that flat minima regularize the gradient of the model function, which explains the good generalization performance of flat minima. Then, we go beyond the flatness and consider high-order moments of the gradient noise, and show that Stochastic Gradient Dascent (SGD) tends to impose constraints on these moments by a linear stability analysis of SGD around global minima. Together with the multiplicative structure, we identify the Sobolev regularization effect of SGD, i.e. SGD regularizes the Sobolev seminorms of the model function with respect to the input data. Finally, bounds for generalization error and adversarial robustness are provided for solutions found by SGD under assumptions of the data distribution.",0
"In this article we will discuss how the Sobolev regularisation effect can be achieved through stochastic gradient descent (SGD). We will first provide some background on both topics before diving into their relationship. The Sobolev space framework provides a mathematical foundation for studying functions whose partial derivatives exist up to certain orders. This allows us to describe a wide range of functions that have certain smoothness properties without requiring them to have global differentiability. SGD is a popular optimisation algorithm used in deep learning to minimise loss functions by iteratively updating model parameters using small random batches of data. We will demonstrate the connection between these two seemingly separate ideas by introducing noise into the function spaces themselves. We show theoretically and experimentally that such a modification leads to faster convergence rates compared to standard variational methods based on truncation, which rely on truncating singular values instead of adding noise. Our approach further reduces to classical sparse recovery techniques like Lasso or Basis Pursuit in the absence of noise. Therefore, our findings bridge deep neural networks and variational methods as well as inverse problems involving large datasets subject to sparse priors. Finally, we provide insights from numerical experiments confirming superior performance against competing state-of-the art approaches in terms of accuracy, stability and speedup factors under similar parameter settings.",1
"Application of Machine Learning algorithms to the medical domain is an emerging trend that helps to advance medical knowledge. At the same time, there is a significant a lack of explainable studies that promote informed, transparent, and interpretable use of Machine Learning algorithms. In this paper, we present explainable multi-class classification of the Covid-19 mental health data. In Machine Learning study, we aim to find the potential factors to influence a personal mental health during the Covid-19 pandemic. We found that Random Forest (RF) and Gradient Boosting (GB) have scored the highest accuracy of 68.08% and 68.19% respectively, with LIME prediction accuracy 65.5% for RF and 61.8% for GB. We then compare a Post-hoc system (Local Interpretable Model-Agnostic Explanations, or LIME) and an Ante-hoc system (Gini Importance) in their ability to explain the obtained Machine Learning results. To the best of these authors knowledge, our study is the first explainable Machine Learning study of the mental health data collected during Covid-19 pandemics.",0
"This study presents a novel approach to multi-class classification of mental health data from the Canadian Centre for Addiction and Mental Health (CAMH) dataset during the COVID-19 pandemic. The proposed method utilizes convolutional neural networks (CNNs) along with Gradient Boosting Machines (GBMs), as well as feature extraction techniques such as principal component analysis (PCA). By using these methods together, we were able to achieve improved performance compared to previous approaches that relied on traditional machine learning algorithms alone. Our results show that our model can accurately classify patients into three categories: anxiety disorders, depression, and other psychiatric illnesses. We demonstrate through extensive experimentation that our method outperforms baseline models by achieving higher accuracy scores across all classes. These findings have important implications for improving mental health screening during crisis situations caused by global events like the current pandemic. Overall, our approach represents a significant step towards developing explainable and reliable solutions for identifying at-risk individuals and providing appropriate care services.",1
"While machine-learning algorithms have demonstrated a strong ability in detecting Android malware, they can be evaded by sparse evasion attacks crafted by injecting a small set of fake components, e.g., permissions and system calls, without compromising intrusive functionality. Previous work has shown that, to improve robustness against such attacks, learning algorithms should avoid overemphasizing few discriminant features, providing instead decisions that rely upon a large subset of components. In this work, we investigate whether gradient-based attribution methods, used to explain classifiers' decisions by identifying the most relevant features, can be used to help identify and select more robust algorithms. To this end, we propose to exploit two different metrics that represent the evenness of explanations, and a new compact security measure called Adversarial Robustness Metric. Our experiments conducted on two different datasets and five classification algorithms for Android malware detection show that a strong connection exists between the uniformity of explanations and adversarial robustness. In particular, we found that popular techniques like Gradient*Input and Integrated Gradients are strongly correlated to security when applied to both linear and nonlinear detectors, while more elementary explanation techniques like the simple Gradient do not provide reliable information about the robustness of such classifiers.",0
"Artificial neural networks (ANNs) have shown impressive performance on many tasks, including those related to security such as malware detection. However, despite their success, little is known about how these models make decisions, which makes them difficult to interpret, debug, and trust. One approach that has been used to provide insights into the decision-making process of ANNs is gradient-based explanations, which highlight parts of input examples most responsible for model predictions. In this work, we investigate whether gradient-based methods can shed light on adversarial robustness in mobile environments. We focus specifically on the relationship between gradient magnitudes and distances from natural instances in Android malware samples. Our findings indicate that while there may be some correlation between gradient magnitude and vulnerability to evasion attacks, the results are mixed and highly dependent on the specific attack method used. Overall, our study shows that gradient-based explanations alone cannot accurately predict an ANN's susceptibility to adversarial inputs, underscoring the need for further research into methods capable of better characterizing robustness against real-world threats.",1
"Recent advances in deep learning have enabled the development of automated frameworks for analysing medical images and signals, including analysis of cervical cancer. Many previous works focus on the analysis of isolated cervical cells, or do not offer sufficient methods to explain and understand how the proposed models reach their classification decisions on multi-cell images. Here, we evaluate various state-of-the-art deep learning models and attention-based frameworks for the classification of images of multiple cervical cells. As we aim to provide interpretable deep learning models to address this task, we also compare their explainability through the visualization of their gradients. We demonstrate the importance of using images that contain multiple cells over using isolated single-cell images. We show the effectiveness of the residual channel attention model for extracting important features from a group of cells, and demonstrate this model's efficiency for this classification task. This work highlights the benefits of channel attention mechanisms in analyzing multiple-cell images for potential relations and distributions within a group of cells. It also provides interpretable models to address the classification of cervical cells.",0
"Title: Enhancing Interpretability of Attention Models in Medical Image Analysis  Abstract: The analysis of medical images such as mammograms, CT scans, MRI scans, etc. plays a crucial role in disease diagnosis, treatment planning, monitoring, prediction, and prognostication. With advances in computer vision algorithms, artificial intelligence (AI) systems have been widely adopted to assist radiologists in making more accurate and efficient interpretations. In particular, attention mechanisms, which have become increasingly popular components in modern deep learning models, allow them to selectively focus on certain regions within the input data, facilitating explainability and interpretation. While several studies have applied attention networks to achieve state-of-the-art performance in various image classification tasks, there remains a need to address their limited interpretability. This work proposes novel techniques that enhance transparency and intuitiveness in the output of these models. We conduct experiments using cervix images extracted from Pap tests and demonstrate significantly improved results compared to baseline methods while preserving high accuracy in predictions. Our contributions are twofold: first, we introduce an alternative loss function motivated by visual saliency maps; second, we propose a new method for generating heatmaps that better align with human intuition. Together, our approach leads to more interpretable attentional patterns without sacrificing performance, offering promising opportunities for real-world applications in medicine.",1
"In recent years, many different approaches have been proposed to quantify the performances of soccer players. Since player performances are challenging to quantify directly due to the low-scoring nature of soccer, most approaches estimate the expected impact of the players' on-the-ball actions on the scoreline. While effective, these approaches are yet to be widely embraced by soccer practitioners. The soccer analytics community has primarily focused on improving the accuracy of the models, while the explainability of the produced metrics is often much more important to practitioners.   To help bridge the gap between scientists and practitioners, we introduce an explainable Generalized Additive Model that estimates the expected value for shots. Unlike existing models, our model leverages features corresponding to widespread soccer concepts. To this end, we represent the locations of shots by fuzzily assigning the shots to designated zones on the pitch that practitioners are familiar with. Our experimental evaluation shows that our model is as accurate as existing models, while being easier to explain to soccer practitioners.",0
"Include keywords such as expected value, explainability, football (soccer), player performance metrics, prediction model.",1
"Predictive machine learning models often lack interpretability, resulting in low trust from model end users despite having high predictive performance. While many model interpretation approaches return top important features to help interpret model predictions, these top features may not be well-organized or intuitive to end users, which limits model adoption rates. In this paper, we propose Intellige, a user-facing model explainer that creates user-digestible interpretations and insights reflecting the rationale behind model predictions. Intellige builds an end-to-end pipeline from machine learning platforms to end user platforms, and provides users with an interface for implementing model interpretation approaches and for customizing narrative insights. Intellige is a platform consisting of four components: Model Importer, Model Interpreter, Narrative Generator, and Narrative Exporter. We describe these components, and then demonstrate the effectiveness of Intellige through use cases at LinkedIn. Quantitative performance analyses indicate that Intellige's narrative insights lead to lifts in adoption rates of predictive model recommendations, as well as to increases in downstream key metrics such as revenue when compared to previous approaches, while qualitative analyses indicate positive feedback from end users.",0
"This paper introduces intellige, a user-facing model exp",1
"The success of deep learning is due, to a large extent, to the remarkable effectiveness of gradient-based optimization methods applied to large neural networks. The purpose of this work is to propose a modern view and a general mathematical framework for loss landscapes and efficient optimization in over-parameterized machine learning models and systems of non-linear equations, a setting that includes over-parameterized deep neural networks. Our starting observation is that optimization problems corresponding to such systems are generally not convex, even locally. We argue that instead they satisfy PL$^*$, a variant of the Polyak-Lojasiewicz condition on most (but not all) of the parameter space, which guarantees both the existence of solutions and efficient optimization by (stochastic) gradient descent (SGD/GD). The PL$^*$ condition of these systems is closely related to the condition number of the tangent kernel associated to a non-linear system showing how a PL$^*$-based non-linear theory parallels classical analyses of over-parameterized linear equations. We show that wide neural networks satisfy the PL$^*$ condition, which explains the (S)GD convergence to a global minimum. Finally we propose a relaxation of the PL$^*$ condition applicable to ""almost"" over-parameterized systems.",0
"In recent years, deep learning has achieved tremendous success in fields such as image classification, speech recognition, and natural language processing, primarily due to the use of large amounts of data and powerful computing resources. However, despite these advancements, understanding the behavior and properties of highly parameterized neural networks remains challenging, particularly in complex tasks that involve non-convex optimization problems. This work addresses this challenge by examining loss landscapes and their implications for optimization in both linear and non-linear models. We focus on over-parametrized models and demonstrate how they exhibit richer structures compared to under-parametrized ones. Furthermore, we investigate the impact of regularization techniques on loss landscapes and provide insights into their effects on optimization processes. Our findings shed new light on the intricate relationship between model complexity, robustness, and generalization performance. These results have important consequences for understanding the properties of deep learning algorithms and can guide future research in designing more efficient training procedures, improving generalization performance, and mitigating overfitting.",1
"This paper presents Sparse Tensor Classifier (STC), a supervised classification algorithm for categorical data inspired by the notion of superposition of states in quantum physics. By regarding an observation as a superposition of features, we introduce the concept of wave-particle duality in machine learning and propose a generalized framework that unifies the classical and the quantum probability. We show that STC possesses a wide range of desirable properties not available in most other machine learning methods but it is at the same time exceptionally easy to comprehend and use. Empirical evaluation of STC on structured data and text classification demonstrates that our methodology achieves state-of-the-art performances compared to both standard classifiers and deep learning, at the additional benefit of requiring minimal data pre-processing and hyper-parameter tuning. Moreover, STC provides a native explanation of its predictions both for single instances and for each target label globally.",0
"This paper presents an innovative approach to designing probabilistic classifiers that can accurately predict categorical outcomes based on input data. Our method draws inspiration from quantum physics concepts such as superposition and entanglement, resulting in an explainable model with high interpretability. We describe our implementation using the Python programming language and scikit-learn machine learning libraries. We evaluate the effectiveness of our method by comparing it against several state-of-the-art classification algorithms on diverse datasets, showing improved accuracy across all tests. Our results demonstrate that incorporating insights from quantum mechanics into traditional machine learning techniques can lead to significant improvements in classification performance while maintaining transparency. This research has important implications for understanding complex systems, decision making processes, and advancing artificial intelligence applications. Overall, we believe our work represents a promising direction for future research at the intersection of classical computing and emerging technologies in quantum computing and computational science.",1
"Deep learning play a vital role in classifying different arrhythmias using the electrocardiography (ECG) data. Nevertheless, training deep learning models normally requires a large amount of data and it can lead to privacy concerns. Unfortunately, a large amount of healthcare data cannot be easily collected from a single silo. Additionally, deep learning models are like black-box, with no explainability of the predicted results, which is often required in clinical healthcare. This limits the application of deep learning in real-world health systems. In this paper, we design a new explainable artificial intelligence (XAI) based deep learning framework in a federated setting for ECG-based healthcare applications. The federated setting is used to solve issues such as data availability and privacy concerns. Furthermore, the proposed framework setting effectively classifies arrhythmia's using an autoencoder and a classifier, both based on a convolutional neural network (CNN). Additionally, we propose an XAI-based module on top of the proposed classifier to explain the classification results, which help clinical practitioners make quick and reliable decisions. The proposed framework was trained and tested using the MIT-BIH Arrhythmia database. The classifier achieved accuracy up to 94% and 98% for arrhythmia detection using noisy and clean data, respectively, with five-fold cross-validation.",0
"In our proposed system, we aim to design an Electrocardiogram (ECG) monitoring healthcare system using federated transfer learning and explainable artificial intelligence. This system would allow patients to monitor their cardiac health at home while providing physicians with accurate diagnoses and personalized treatment plans. Our approach leverages the power of federated learning to train deep neural networks on distributed datasets from multiple hospitals without sharing sensitive patient data. We use attention-based convolutional neural networks to process raw ECG signals into clinically relevant features and incorporate transfer learning across different sites to improve model accuracy. Additionally, we employ interpretable AI techniques such as SHAP values and partial dependence plots to provide clear explanations of how the model arrives at its predictions, ensuring transparency and trustworthiness. This system has the potential to revolutionize remote cardiac care by empowering both patients and medical professionals through advanced technology that prioritizes privacy, interpretability, and performance. Our experimental results demonstrate the effectiveness of our approach in terms of prediction accuracy, visualization quality, and robustness to missing data. Overall, our work contributes to the growing field of medical informatics by showcasing the viability of using cutting-edge machine learning methods under stringent constraints on data accessibility and model interpretability, ultimately improving public health outcomes.",1
"We study the problem of unsupervised discovery and segmentation of object parts, which, as an intermediate local representation, are capable of finding intrinsic object structure and providing more explainable recognition results. Recent unsupervised methods have greatly relaxed the dependency on annotated data which are costly to obtain, but still rely on additional information such as object segmentation mask or saliency map. To remove such a dependency and further improve the part segmentation performance, we develop a novel approach by disentangling the appearance and shape representations of object parts followed with reconstruction losses without using additional object mask information. To avoid degenerated solutions, a bottleneck block is designed to squeeze and expand the appearance representation, leading to a more effective disentanglement between geometry and appearance. Combined with a self-supervised part classification loss and an improved geometry concentration constraint, we can segment more consistent parts with semantic meanings. Comprehensive experiments on a wide variety of objects such as face, bird, and PASCAL VOC objects demonstrate the effectiveness of the proposed method.",0
"Title: Unsupervised part segmentation via disentanglement of appearance and shape  Part segmentation is a fundamental problem in computer vision, with numerous applications ranging from image understanding to robotic manipulation tasks. However, most existing approaches require either manual annotations or supervised learning on large datasets, which can be time-consuming, expensive, and limited by annotation quality. To address these limitations, we propose an unsupervised method that learns to separate object parts based on their intrinsic features such as shape and texture using a generative model. Our approach utilizes adversarial training to generate realistic images while maximizing disentanglement of shape and appearance. We further incorporate edge preservation into our framework, enabling more accurate and robust segmentations even in challenging scenarios like occlusion and cluttered backgrounds. Extensive evaluations demonstrate that our method outperforms state-of-the-art methods across multiple benchmark datasets without any human labeling, making it appealing for a wide range of applications. By leveraging the power of deep learning architectures and novel regularization techniques, we have developed an effective solution for automating the task of part segmentation. This work lays the foundation for future research in unsupervised representation learning for vision problems, promising greater flexibility, scalability, and efficiency in developing intelligent systems for diverse domains.",1
"The article gives an overview of the plagiarism domain, with focus on academic plagiarism. The article defines plagiarism, explains the origin of the term, as well as plagiarism related terms. It identifies the extent of the plagiarism domain and then focuses on the plagiarism subdomain of text documents, for which it gives an overview of current classifications and taxonomies and then proposes a more comprehensive classification according to several criteria: their origin and purpose, technical implementation, consequence, complexity of detection and according to the number of linguistic sources. The article suggests the new classification of academic plagiarism, describes sorts and methods of plagiarism, types and categories, approaches and phases of plagiarism detection, the classification of methods and algorithms for plagiarism detection. The title of the article explicitly targets the academic community, but it is sufficiently general and interdisciplinary, so it can be useful for many other professionals like software developers, linguists and librarians.",0
"Academic dishonesty has become a serious concern in todayâ€™s education system due to rapid advancements in technology which have made it easier than ever before for students to access and share content online. This research presents a taxonomy of common methods used by individuals to commit acts of academic plagiarism within their university assignments such as copying and pasting text directly from sources without proper citation; purchasing prewritten essays on the internet; submitting work that was written by someone else as oneâ€™s own; self-plagiarizing by recycling previous work and submitting it again without proper credit; translating material into another language using machine translation software without properly checking for accuracy; and collaborating with peers without acknowledging assistance provided. Through literature review and analysis, we identify key indicators for each type of plagiarism method allowing educators to detect them more easily and effectively address the issue of academic dishonesty. We aim to provide insightful recommendations for future prevention measures including the development of technologies designed to accurately check for similarities across documents submitted by students and implementing awareness programs emphasizing ethical conduct among members of academia.",1
"Multi-party learning provides solutions for training joint models with decentralized data under legal and practical constraints. However, traditional multi-party learning approaches are confronted with obstacles such as system heterogeneity, statistical heterogeneity, and incentive design. How to deal with these challenges and further improve the efficiency and performance of multi-party learning has become an urgent problem to be solved. In this paper, we propose a novel contrastive multi-party learning framework for knowledge refinement and sharing with an accountable incentive mechanism. Since the existing naive model parameter averaging method is contradictory to the learning paradigm of neural networks, we simulate the process of human cognition and communication, and analogy multi-party learning as a many-to-one knowledge sharing problem. The approach is capable of integrating the acquired explicit knowledge of each client in a transparent manner without privacy disclosure, and it reduces the dependence on data distribution and communication environments. The proposed scheme achieves significant improvement in model performance in a variety of scenarios, as we demonstrated through experiments on several real-world datasets.",0
"Title: A New Approach for Collaborative Machine Learning  Machine learning has become increasingly important in many domains due to its ability to automatically learn patterns from data. However, traditional machine learning approaches often focus on single party learning, where only one party's data is used to train models. This can lead to limited performance and biases caused by insufficient training data. To overcome these limitations, collaborative multi-party learning methods have emerged, which allow multiple parties to share their private datasets and jointly train a model that benefits all participants. One key challenge in such settings is ensuring explainability and trustworthiness of the resulting models, especially when dealing with sensitive information or critical applications. In this work, we propose a novel framework based on contrastive knowledge sharing for multi-party learning that addresses both the technical challenges related to distributed optimization and privacy concerns, while still providing high-quality results. Our approach introduces a new type of preliminary task for contrastive knowledge transfer that improves the efficiency and effectiveness of collaborative learning processes. We evaluate our method using two different use cases and demonstrate its superiority over state-of-the-art competitors. Overall, our framework represents a significant step towards more effective and explainable multi-party learning systems, paving the way for even broader adoption of machine learning in various fields.",1
"Recent work on explainable clustering allows describing clusters when the features are interpretable. However, much modern machine learning focuses on complex data such as images, text, and graphs where deep learning is used but the raw features of data are not interpretable. This paper explores a novel setting for performing clustering on complex data while simultaneously generating explanations using interpretable tags. We propose deep descriptive clustering that performs sub-symbolic representation learning on complex data while generating explanations based on symbolic data. We form good clusters by maximizing the mutual information between empirical distribution on the inputs and the induced clustering labels for clustering objectives. We generate explanations by solving an integer linear programming that generates concise and orthogonal descriptions for each cluster. Finally, we allow the explanation to inform better clustering by proposing a novel pairwise loss with self-generated constraints to maximize the clustering and explanation module's consistency. Experimental results on public data demonstrate that our model outperforms competitive baselines in clustering performance while offering high-quality cluster-level explanations.",0
"In recent years, there has been increasing interest in developing machine learning algorithms that can discover complex patterns in large datasets without relying on domain knowledge or hand-engineered features. One popular approach to achieve this goal is through clustering algorithms which group data points into meaningful clusters based on their similarity or dissimilarity. However, existing clustering methods often suffer from several limitations such as sensitivity to initial conditions, non-convex optimization objectives, and lack of interpretability.  To address these challenges, we propose a novel method called deep descriptive clustering (DDC) which combines traditional clustering techniques with deep learning principles. Our approach involves training a neural network to identify discriminative cluster assignments by optimizing a contrastive loss function inspired by semi-supervised learning. To overcome issues related to overfitting and instability, we further introduce regularization terms derived from statistical physics that encourage sparsity and smoothness in the learned embeddings.  Our evaluation results across multiple benchmark datasets demonstrate that DDC outperforms state-of-the-art clustering algorithms in terms of both quantitative metrics and visual inspection of the resulting clusters. Furthermore, we show that the learnt embeddings are interpretable and provide valuable insights into the underlying structure of the data. For instance, our analysis reveals intricate geometric relationships among images from the MNIST dataset and uncovers latent factors behind human activity recognition in sensor readings collected from wearable devices. \-----",1
"There have been a number of corner detection methods proposed for event cameras in the last years, since event-driven computer vision has become more accessible. Current state-of-the-art have either unsatisfactory accuracy or real-time performance when considered for practical use; random motion using a live camera in an unconstrained environment. In this paper, we present yet another method to perform corner detection, dubbed look-up event-Harris (luvHarris), that employs the Harris algorithm for high accuracy but manages an improved event throughput. Our method has two major contributions, 1. a novel ""threshold ordinal event-surface"" that removes certain tuning parameters and is well suited for Harris operations, and 2. an implementation of the Harris algorithm such that the computational load per-event is minimised and computational heavy convolutions are performed only 'as-fast-as-possible', i.e. only as computational resources are available. The result is a practical, real-time, and robust corner detector that runs more than $2.6\times$ the speed of current state-of-the-art; a necessity when using high-resolution event-camera in real-time. We explain the considerations taken for the approach, compare the algorithm to current state-of-the-art in terms of computational performance and detection accuracy, and discuss the validity of the proposed approach for event cameras.",0
"This paper presents ""LUV Harris"", a practical corner detection algorithm designed for event cameras that operate on asynchronous data streams. Our approach extends traditional Harris corners by incorporating orientation information from both local brightness changes and local phase transitions within the image intensity signal. We propose two key improvements to the original Harris formulation - we first compute local image gradients using the difference between neighboring events at each pixel location, which improves robustness to motion blur; secondly, we leverage the local phase structure present in event-based representations to more accurately estimate orientation at each point. Extensive experimental evaluations demonstrate that LUV Harris significantly outperforms existing state-of-the-art approaches across challenging scenarios, including both synthetic datasets as well as real-world data captured using event cameras.",1
"A major factor in the success of deep neural networks is the use of sophisticated architectures rather than the classical multilayer perceptron (MLP). Residual networks (ResNets) stand out among these powerful modern architectures. Previous works focused on the optimization advantages of deep ResNets over deep MLPs. In this paper, we show another distinction between the two models, namely, a tendency of ResNets to promote smoother interpolations than MLPs. We analyze this phenomenon via the neural tangent kernel (NTK) approach. First, we compute the NTK for a considered ResNet model and prove its stability during gradient descent training. Then, we show by various evaluation methodologies that for ReLU activations the NTK of ResNet, and its kernel regression results, are smoother than the ones of MLP. The better smoothness observed in our analysis may explain the better generalization ability of ResNets and the practice of moderately attenuating the residual blocks.",0
"This research paper presents a kernel-based smoothness analysis method for studying residual networks, which are commonly used in deep learning applications such as image classification. By analyzing the data through kernel functions, we aim to provide insights into the behavior of these complex models and identify potential issues that may arise during training and inference. Our approach focuses on examining the structure of the network and understanding how it processes inputs under different conditions. We evaluate our method using several benchmark datasets and compare it against existing techniques to demonstrate its effectiveness. Our findings have important implications for understanding the performance limitations and generalization abilities of residual networks, paving the way for future improvements and advancements in deep learning.",1
"Significant progress on the crowd counting problem has been achieved by integrating larger context into convolutional neural networks (CNNs). This indicates that global scene context is essential, despite the seemingly bottom-up nature of the problem. This may be explained by the fact that context knowledge can adapt and improve local feature extraction to a given scene. In this paper, we therefore investigate the role of global context for crowd counting. Specifically, a pure transformer is used to extract features with global information from overlapping image patches. Inspired by classification, we add a context token to the input sequence, to facilitate information exchange with tokens corresponding to image patches throughout transformer layers. Due to the fact that transformers do not explicitly model the tried-and-true channel-wise interactions, we propose a token-attention module (TAM) to recalibrate encoded features through channel-wise attention informed by the context token. Beyond that, it is adopted to predict the total person count of the image through regression-token module (RTM). Extensive experiments demonstrate that our method achieves state-of-the-art performance on various datasets, including ShanghaiTech, UCF-QNRF, JHU-CROWD++ and NWPU. On the large-scale JHU-CROWD++ dataset, our method improves over the previous best results by 26.9% and 29.9% in terms of MAE and MSE, respectively.",0
"This paper presents a new method for improving crowd counting accuracy using deep learning techniques. We propose a novel architecture based on transformer networks that captures spatial and temporal dependencies between objects in crowded scenes. Our approach outperforms state-of-the-art methods in terms of precision, recall, and F1 score on several benchmark datasets. In addition, we demonstrate the effectiveness of our model on real-world applications such as traffic monitoring and event planning. Overall, our work contributes to advancing the field of computer vision and demonstrates the potential of transformers in crowd counting tasks.",1
"We introduce NExT-QA, a rigorously designed video question answering (VideoQA) benchmark to advance video understanding from describing to explaining the temporal actions. Based on the dataset, we set up multi-choice and open-ended QA tasks targeting causal action reasoning, temporal action reasoning, and common scene comprehension. Through extensive analysis of baselines and established VideoQA techniques, we find that top-performing methods excel at shallow scene descriptions but are weak in causal and temporal action reasoning. Furthermore, the models that are effective on multi-choice QA, when adapted to open-ended QA, still struggle in generalizing the answers. This raises doubt on the ability of these models to reason and highlights possibilities for improvement. With detailed results for different question types and heuristic observations for future works, we hope NExT-QA will guide the next generation of VQA research to go beyond superficial scene description towards a deeper understanding of videos. (The dataset and related resources are available at https://github.com/doc-doc/NExT-QA.git)",0
"""Temporal"" is from Greek meaning time. But you can skip temporal if your system has learned that (easily) and just use action explanation if appropriate. So in the above example you would say something like... ""We present results of our new question answering system NextQA designed to answer natural language questions while producing explanations over actions it takes to find those answers."" You wouldn't need to explain the difference between 'natural language' vs the structured queries which previous systems were limited to. Just assume some level of knowledge on the part of readers who might peruse abstracts. Also please keep paragraph length down - ideally only one thought/point per paragraph. ------  Our research team presents the development and evaluation of NExT-QA, the next generation of question answering technology. Unlike traditional QA systems, NExT-QA is able to process complex and diverse natural language questions, providing both accurate answers and detailed explanations of how it arrived at them. This capability allows users to better understand how their requests were handled by the system, leading to increased transparency and trust. In addition to these advancements, NExT-QA features improved performance across a range of metrics, including accuracy, speed, and comprehensiveness. By integrating state-of-the-art technologies such as deep learning techniques and large-scale training datasets, we have created a system that sets a high bar for future developments in QA technology. Overall, the introduction of NExT-QA represents a significant step forward towards realizing more intelligent, transparent, and efficient human-AI interaction.",1
"We provide a new model for texture synthesis based on a multiscale, multilayer feature extractor. Within the model, textures are represented by a set of statistics computed from ReLU wavelet coefficients at different layers, scales and orientations. A new image is synthesized by matching the target statistics via an iterative projection algorithm. We explain the necessity of the different types of pre-defined wavelet filters used in our model and the advantages of multilayer structures for image synthesis. We demonstrate the power of our model by generating samples of high quality textures and providing insights into deep representations for texture images.",0
"The ability to synthesize realistic textures has numerous applications across fields such as computer graphics, vision, and material design. Recent advances have focused on generating high-fidelity textures using machine learning techniques that capture statistical properties of large datasets. However, these methods often rely solely on local image patch similarity measures, which can lead to overfitting and lack of generalization. In this work, we propose a novel approach that utilizes multiscale, multilayer statistics derived from natural images, allowing us to generate highly detailed textures while maintaining robustness against input variations. We project our learned features into multiple statistical spaces to synthesize diverse, yet coherent textures using efficient optimization procedures. Our method outperforms state-of-the-art texture synthesis algorithms by achieving superior visual quality and perceptual fidelity under varying conditions. This research contributes significantly towards developing powerful generative models capable of creating photorealistic textures applicable to various domains.",1
"Automated image captioning is one of the applications of Deep Learning which involves fusion of work done in computer vision and natural language processing, and it is typically performed using Encoder-Decoder architectures. In this project, we have implemented and experimented with various flavors of multi-modal image captioning networks where ResNet101, DenseNet121 and VGG19 based CNN Encoders and Attention based LSTM Decoders were explored. We have studied the effect of beam size and the use of pretrained word embeddings and compared them to baseline CNN encoder and RNN decoder architecture. The goal is to analyze the performance of each approach using various evaluation metrics including BLEU, CIDEr, ROUGE and METEOR. We have also explored model explainability using Visual Attention Maps (VAM) to highlight parts of the images which has maximum contribution for predicting each word of the generated caption.",0
"In todayâ€™s digital world, image captions play an essential role in making images accessible and interpretable for humans as well as machines. In recent years, deep learning methods have shown promise for generating captions automatically from images. This study aims to empirically analyze the state-of-the-art techniques used for image caption generation using deep learning models. Our analysis covers different architectures such as convolutional neural networks (CNN), recurrent neural networks (RNN), transformer models, and hybrid approaches that combine multiple models together. We evaluate these models based on their performance metrics like perplexity, FID score, ROUGE scores, etc., and compare them against human benchmarks wherever possible. Our results show that although there has been significant progress in this field, there still exists room for improvement particularly in terms of coherency and relevancy of generated captions vis-Ã -vis ground truth captions. We discuss future research directions aimed at addressing some of the remaining challenges and improving the quality of automatic image caption generation systems further. Overall, our findings provide insights into the current capabilities and limitations of deep learning models for image caption generation and pave the way for developing more advanced solutions in this area.",1
"Objective: To assess automatic computer-aided in-situ recognition of morphological features of pure and mixed urinary stones using intraoperative digital endoscopic images acquired in a clinical setting. Materials and methods: In this single-centre study, an experienced urologist intraoperatively and prospectively examined the surface and section of all kidney stones encountered. Calcium oxalate monohydrate (COM/Ia), dihydrate (COD/IIb) and uric acid (UA/IIIb) morphological criteria were collected and classified to generate annotated datasets. A deep convolutional neural network (CNN) was trained to predict the composition of both pure and mixed stones. To explain the predictions of the deep neural network model, coarse localisation heat-maps were plotted to pinpoint key areas identified by the network. Results: This study included 347 and 236 observations of stone surface and stone section, respectively. A highest sensitivity of 98 % was obtained for the type ""pure IIIb/UA"" using surface images. The most frequently encountered morphology was that of the type ""pure Ia/COM""; it was correctly predicted in 91 % and 94 % of cases using surface and section images, respectively. Of the mixed type ""Ia/COM+IIb/COD"", Ia/COM was predicted in 84 % of cases using surface images, IIb/COD in 70 % of cases, and both in 65 % of cases. Concerning mixed Ia/COM+IIIb/UA stones, Ia/COM was predicted in 91 % of cases using section images, IIIb/UA in 69 % of cases, and both in 74 % of cases. Conclusions: This preliminary study demonstrates that deep convolutional neural networks are promising to identify kidney stone composition from endoscopic images acquired intraoperatively. Both pure and mixed stone composition could be discriminated. Collected in a clinical setting, surface and section images analysed by deep CNN provide valuable information about stone morphology for computer-aided diagnosis.",0
"This paper presents a new method for automatically recognizing pure and mixed stones using intraoperative endoscopic digital images. The proposed approach involves preprocessing the images to enhance their quality, followed by extracting features that represent the characteristics of the stones. These features are then used to train a classifier to distinguish between pure and mixed stones. Experiments conducted on a dataset of 200 images showed high accuracy and precision in identifying pure and mixed stones, demonstrating the effectiveness of our approach. Our work has potential applications in urology and could aid clinicians in making accurate diagnoses during minimally invasive surgeries. Overall, this study contributes towards advancing medical image analysis techniques for surgical procedures.",1
"This paper considers joint learning of multiple sparse Granger graphical models to discover underlying common and differential Granger causality (GC) structures across multiple time series. This can be applied to drawing group-level brain connectivity inferences from a homogeneous group of subjects or discovering network differences among groups of signals collected under heterogeneous conditions. By recognizing that the GC of a single multivariate time series can be characterized by common zeros of vector autoregressive (VAR) lag coefficients, a group sparse prior is included in joint regularized least-squares estimations of multiple VAR models. Group-norm regularizations based on group- and fused-lasso penalties encourage a decomposition of multiple networks into a common GC structure, with other remaining parts defined in individual-specific networks. Prior information about sparseness and sparsity patterns of desired GC networks are incorporated as relative weights, while a non-convex group norm in the penalty is proposed to enhance the accuracy of network estimation in low-sample settings. Extensive numerical results on simulations illustrated our method's improvements over existing sparse estimation approaches on GC network sparsity recovery. Our methods were also applied to available resting-state fMRI time series from the ADHD-200 data sets to learn the differences of causality mechanisms, called effective brain connectivity, between adolescents with ADHD and typically developing children. Our analysis revealed that parts of the causality differences between the two groups often resided in the orbitofrontal region and areas associated with the limbic system, which agreed with clinical findings and data-driven results in previous studies.",0
"This paper presents a novel approach to jointly estimating multiple Granger causal networks from multivariate time series data using non-convex regularization techniques. We propose a framework that simultaneously infers the directed edges within each network as well as their interdependences across different systems, allowing for the detection of potential feedback loops and shared dynamics among interacting processes. Our method can handle high-dimensional datasets and achieves robust performance through adaptive parameter selection and model initialization strategies. Results on synthetic data demonstrate that our proposed technique outperforms existing methods in terms of accuracy and computational efficiency. Additionally, we apply our approach to fMRI data from human subjects performing a language task and showcase the ability of our algorithm to capture meaningful patterns of group-level brain connectivity associated with cognitive functions. Overall, this work represents an important step towards uncovering complex interactions governing dynamic behavior in complex systems ranging from neuroscience to economics, finance, and other fields where understanding cause-and-effect relationships is crucial.",1
"Due to the powerful learning ability on high-rank and non-linear features, deep neural networks (DNNs) are being applied to data mining and machine learning in various fields, and exhibit higher discrimination performance than conventional methods. However, the applications based on DNNs are rare in enterprise credit rating tasks because most of DNNs employ the ""end-to-end"" learning paradigm, which outputs the high-rank representations of objects and predictive results without any explanations. Thus, users in the financial industry cannot understand how these high-rank representations are generated, what do they mean and what relations exist with the raw inputs. Then users cannot determine whether the predictions provided by DNNs are reliable, and not trust the predictions providing by such ""black box"" models. Therefore, in this paper, we propose a novel network to explicitly model the enterprise credit rating problem using DNNs and attention mechanisms. The proposed model realizes explainable enterprise credit ratings. Experimental results obtained on real-world enterprise datasets verify that the proposed approach achieves higher performance than conventional methods, and provides insights into individual rating results and the reliability of model training.",0
"This paper presents a novel approach to credit rating using deep learning techniques and feature crossing networks (FCNs). We propose a method called Explainable Enterprise Credit Rating via Deep Feature Crossing Network that can effectively identify creditworthy businesses while providing interpretable explanations for predictions. Our model utilizes multiple layers of nonlinear processing, allowing it to capture complex patterns and relationships within the input data. By leveraging FCNs, we can efficiently combine different types of features from multiple sources, such as financial statements, industry reports, and text analysis of media articles. Experimental results demonstrate the superiority of our proposed method over traditional approaches in terms of accuracy and interpretability. Our framework has important applications for lending institutions, investors, and policymakers who need accurate and explainable credit ratings. Overall, this research contributes new insights into enterprise credit risk assessment using advanced machine learning methods.",1
"Random Forest (RFs) are among the most widely used Machine Learning (ML) classifiers. Even though RFs are not interpretable, there are no dedicated non-heuristic approaches for computing explanations of RFs. Moreover, there is recent work on polynomial algorithms for explaining ML models, including naive Bayes classifiers. Hence, one question is whether finding explanations of RFs can be solved in polynomial time. This paper answers this question negatively, by proving that computing one PI-explanation of an RF is D^P-complete. Furthermore, the paper proposes a propositional encoding for computing explanations of RFs, thus enabling finding PI-explanations with a SAT solver. This contrasts with earlier work on explaining boosted trees (BTs) and neural networks (NNs), which requires encodings based on SMT/MILP. Experimental results, obtained on a wide range of publicly available datasets, demontrate that the proposed SAT-based approach scales to RFs of sizes common in practical applications. Perhaps more importantly, the experimental results demonstrate that, for the vast majority of examples considered, the SAT-based approach proposed in this paper significantly outperforms existing heuristic approaches.",0
"Abstract: This work presents a novel approach for explaining how random forest models make predictions by encoding their decision-making process using satisfiability (SAT) problems. Existing methods for interpreting random forest decisions typically require manual feature selection or rely on heuristics that may miss important aspects of the model's behavior. Our method works automatically, without any human intervention, and provides a clear rationale for each prediction based on the relevant features and rules discovered by the underlying ensemble. We evaluate our technique on several datasets and demonstrate that it can accurately identify which features influence predictions while generating concise and readable explanations.",1
"Machine learning applications have become ubiquitous. Their applications from machine embedded control in production over process optimization in diverse areas (e.g., traffic, finance, sciences) to direct user interactions like advertising and recommendations. This has led to an increased effort of making machine learning trustworthy. Explainable and fair AI have already matured. They address knowledgeable users and application engineers. However, there are users that want to deploy a learned model in a similar way as their washing machine. These stakeholders do not want to spend time understanding the model. Instead, they want to rely on guaranteed properties. What are the relevant properties? How can they be expressed to stakeholders without presupposing machine learning knowledge? How can they be guaranteed for a certain implementation of a model? These questions move far beyond the current state-of-the-art and we want to address them here. We propose a unified framework that certifies learning methods via care labels. They are easy to understand and draw inspiration from well-known certificates like textile labels or property cards of electronic devices. Our framework considers both, the machine learning theory and a given implementation. We test the implementation's compliance with theoretical properties and bounds. In this paper, we illustrate care labels by a prototype implementation of a certification suite for a selection of probabilistic graphical models.",0
"This paper presents the Care Label Framework (CLF), a novel approach to certifying machine learning methods. CLF addresses two key challenges facing the field: ensuring that algorithms are both explainable and transparent, while simultaneously guaranteeing their fairness and accountability. Our framework combines human expertise with machine learning techniques, enabling us to assess how well a given model meets certain ethical criteria. We provide evidence showing that our framework can effectively identify potential biases in machine learning systems, making them more trustworthy and reliable. Additionally, we demonstrate how our method can be integrated into existing workflows, such as testing and validation procedures, further improving the overall quality of ML models. Finally, we discuss future directions and applications of our work, including its potential impact on real-world decision-making processes. Overall, the Care Label Framework represents an important step forward in advancing responsible artificial intelligence research and development.",1
"This survey presents an overview of integrating prior knowledge into machine learning systems in order to improve explainability. The complexity of machine learning models has elicited research to make them more explainable. However, most explainability methods cannot provide insight beyond the given data, requiring additional information about the context. We propose to harness prior knowledge to improve upon the explanation capabilities of machine learning models. In this paper, we present a categorization of current research into three main categories which either integrate knowledge into the machine learning pipeline, into the explainability method or derive knowledge from explanations. To classify the papers, we build upon the existing taxonomy of informed machine learning and extend it from the perspective of explainability. We conclude with open challenges and research directions.",0
"In recent years, there has been increasing interest in developing machine learning models that can explain their reasoning process and decision making. This need for transparency and interpretability arises from concerns related to safety, security, reliability, and trustworthiness, especially in critical domains such as healthcare and finance. One approach to enhancing model interpretability is by incorporating prior knowledge into the machine learning framework. By leveraging domain-specific expertise, we can improve our understanding of how the model makes predictions and decisions.  In this comprehensive overview, we present various methods for integrating prior knowledge into machine learning algorithms, including rule-based systems, Bayesian inference networks, probabilistic graphical models, and neural networks with structured representations. We discuss different techniques used to encode prior knowledge into these models, such as logical rules, statistical distributions, conditional probabilities, Markov chains, and deep learning architectures. Additionally, we explore popular techniques for evaluating the quality of explanations generated by these interpretable models.  Furthermore, we examine several applications of explainable machine learning with prior knowledge across diverse fields, highlighting its potential benefits and limitations. These examples range from medical diagnosis and treatment planning to environmental monitoring and financial risk analysis. Finally, we provide suggestions on best practices for designing and implementing these models based on real-world experiences, identifying key challenges faced during deployment and offering strategies for addressing them effectively.  Overall, this review provides a thorough examination of the current state of research on explainable machine learning with prior knowledge, serving as both an introduction to newcomers and a reference for experienced practitioners seeking to enhance their understanding of this growing field.",1
"We show that the popular reinforcement learning (RL) strategy of estimating the state-action value (Q-function) by minimizing the mean squared Bellman error leads to a regression problem with confounding, the inputs and output noise being correlated. Hence, direct minimization of the Bellman error can result in significantly biased Q-function estimates. We explain why fixing the target Q-network in Deep Q-Networks and Fitted Q Evaluation provides a way of overcoming this confounding, thus shedding new light on this popular but not well understood trick in the deep RL literature. An alternative approach to address confounding is to leverage techniques developed in the causality literature, notably instrumental variables (IV). We bring together here the literature on IV and RL by investigating whether IV approaches can lead to improved Q-function estimates. This paper analyzes and compares a wide range of recent IV methods in the context of offline policy evaluation (OPE), where the goal is to estimate the value of a policy using logged data only. By applying different IV techniques to OPE, we are not only able to recover previously proposed OPE methods such as model-based techniques but also to obtain competitive new techniques. We find empirically that state-of-the-art OPE methods are closely matched in performance by some IV methods such as AGMM, which were not developed for OPE. We open-source all our code and datasets at https://github.com/liyuan9988/IVOPEwithACME.",0
"Instrumental variable (IV) regression has been shown to be effective in estimating causal effects from observational data by exploiting exogenous sources of variation that predict both treatment assignment and outcomes. In deep reinforcement learning (RL), offline policy evaluation refers to estimating how a new policy would perform had it been deployed in a previous environment where data was collected using a different, potentially suboptimal policy. We propose applying IV methods to deep RL for more accurate offline policy evaluation. Our method works by finding features which predict whether actions taken by the old policy were optimal or not, then uses these as instrumental variables to estimate how often the new policy would have produced better rewards given historical experience only. Experiments on Atari games show our proposed method significantly improves over state-of-the-art offline evaluators and can detect important differences between policies.",1
"There has been a widely held view that visual representations (e.g., photographs and illustrations) do not depict negation, for example, one that can be expressed by a sentence ""the train is not coming"". This view is empirically challenged by analyzing the real-world visual representations of comic (manga) illustrations. In the experiment using image captioning tasks, we gave people comic illustrations and asked them to explain what they could read from them. The collected data showed that some comic illustrations could depict negation without any aid of sequences (multiple panels) or conventional devices (special symbols). This type of comic illustrations was subjected to further experiments, classifying images into those containing negation and those not containing negation. While this image classification was easy for humans, it was difficult for data-driven machines, i.e., deep learning models (CNN), to achieve the same high performance. Given the findings, we argue that some comic illustrations evoke background knowledge and thus can depict negation with purely visual elements.",0
"This research paper presents a study analyzing how visual representations of negation are used in comic images. We examined real-world examples of comic art and identified common patterns and techniques used by artists to convey negative emotions or outcomes. Our findings suggest that there is a wide range of approaches to representing negativity through imagery, including changes in color palette, composition, character expression, and background elements. Furthermore, we found that these choices can have significant impacts on audience perception of mood and tone within the overall narrative. Overall, our work contributes new insights into the role of visual communication in storytelling, and provides valuable guidance for aspiring comic creators seeking to master this important aspect of their craft.",1
"Understanding the behavior of learned classifiers is an important task, and various black-box explanations, logical reasoning approaches, and model-specific methods have been proposed. In this paper, we introduce probabilistic sufficient explanations, which formulate explaining an instance of classification as choosing the ""simplest"" subset of features such that only observing those features is ""sufficient"" to explain the classification. That is, sufficient to give us strong probabilistic guarantees that the model will behave similarly when all features are observed under the data distribution. In addition, we leverage tractable probabilistic reasoning tools such as probabilistic circuits and expected predictions to design a scalable algorithm for finding the desired explanations while keeping the guarantees intact. Our experiments demonstrate the effectiveness of our algorithm in finding sufficient explanations, and showcase its advantages compared to Anchors and logical explanations.",0
"This paper presents a new framework for understanding the concept of explanation in artificial intelligence (AI) systems, focusing on the notion of ""probabilistic sufficient explanations."" In traditional approaches to explaining the behavior of AI models, there has been a tendency towards providing deterministic explanations that rely on specific rules or conditions that must hold true in order for an action to take place. However, many real world situations involve uncertainty and probabilistic reasoning, making such deterministic explanations insufficient. Our proposed approach addresses this issue by using Bayesian networks to represent the causal relationships underlying decisions made by an AI system, enabling the calculation of probabilities associated with different factors influencing those decisions. These probabilistic sufficient explanations provide richer information about why the AI acted as it did, helping users better understand and trust the model's outputs while also facilitating system auditing and debugging efforts. We evaluate our method through experiments on both synthetic data sets and real applications of AI, demonstrating improved performance compared to existing explanation methods. Overall, we believe that probabilistic sufficient explanations offer a promising solution for addressing challenges related to explainability in modern AI systems.",1
"Deep learning (DL) has achieved unprecedented success in a variety of tasks. However, DL systems are notoriously difficult to test and debug due to the lack of explainability of DL models and the huge test input space to cover. Generally speaking, it is relatively easy to collect a massive amount of test data, but the labeling cost can be quite high. Consequently, it is essential to conduct test selection and label only those selected ""high quality"" bug-revealing test inputs for test cost reduction.   In this paper, we propose a novel test prioritization technique that brings order into the unlabeled test instances according to their bug-revealing capabilities, namely TestRank. Different from existing solutions, TestRank leverages both intrinsic attributes and contextual attributes of test instances when prioritizing them. To be specific, we first build a similarity graph on test instances and training samples, and we conduct graph-based semi-supervised learning to extract contextual features. Then, for a particular test instance, the contextual features extracted from the graph neural network (GNN) and the intrinsic features obtained with the DL model itself are combined to predict its bug-revealing probability. Finally, TestRank prioritizes unlabeled test instances in descending order of the above probability value. We evaluate the performance of TestRank on a variety of image classification datasets. Experimental results show that the debugging efficiency of our method significantly outperforms existing test prioritization techniques.",0
"One common challenge faced by deep learning practitioners is dealing with large amounts of unlabeled test data. This can cause difficulty in evaluating model performance on new, previously unseen instances that might arise during deployment. We propose a method called TestRank which uses existing labeled training data along with a small amount of user input to automatically generate meaningful rankings of unlabeled test cases. Our approach estimates class probabilities through statistical sampling over different subsets of the training set, enabling our algorithm to handle imbalanced distributions across classes and uncertainty present in any machine learning task. We evaluate our approach through experiments using several benchmark datasets, demonstrating significant improvements over random baselines as well as comparisons against other methods. Ultimately, our goal is to aid users with interpreting their models better so they may gain more insightful knowledge from their trained systems.",1
"Learning generative models and inferring latent trajectories have shown to be challenging for time series due to the intractable marginal likelihoods of flexible generative models. It can be addressed by surrogate objectives for optimization. We propose Monte Carlo filtering objectives (MCFOs), a family of variational objectives for jointly learning parametric generative models and amortized adaptive importance proposals of time series. MCFOs extend the choices of likelihood estimators beyond Sequential Monte Carlo in state-of-the-art objectives, possess important properties revealing the factors for the tightness of objectives, and allow for less biased and variant gradient estimates. We demonstrate that the proposed MCFOs and gradient estimations lead to efficient and stable model learning, and learned generative models well explain data and importance proposals are more sample efficient on various kinds of time series data.",0
"A new family of variational objectives has been developed that improve the performance of generative models and neural adaptive proposal methods for time series data. These Monte Carlo filtering objectives provide a more effective way to learn from and make predictions on complex temporal dynamics. By combining traditional variational inference techniques with novel Monte Carlo sampling strategies, these objective functions enable better approximation of the true underlying distributions and lead to improved model accuracy. Our experiments demonstrate the effectiveness of these new methods over existing state-of-the-art approaches across a range of applications, including speech synthesis, traffic prediction, and human movement analysis. This work represents an important advancement in the field, opening up new possibilities for learning and inference in challenging real world domains.",1
"Smart home environments are designed to provide services that help improve the quality of life for the occupant via a variety of sensors and actuators installed throughout the space. Many automated actions taken by a smart home are governed by the output of an underlying activity recognition system. However, activity recognition systems may not be perfectly accurate and therefore inconsistencies in smart home operations can lead a user to wonder ""why did the smart home do that?"" In this work, we build on insights from Explainable Artificial Intelligence (XAI) techniques to contribute computational methods for explainable activity recognition. Specifically, we generate explanations for smart home activity recognition systems that explain what about an activity led to the given classification. To do so, we introduce four computational techniques for generating natural language explanations of smart home data and compare their effectiveness at generating meaningful explanations. Through a study with everyday users, we evaluate user preferences towards the four explanation types. Our results show that the leading approach, SHAP, has a 92% success rate in generating accurate explanations. Moreover, 84% of sampled scenarios users preferred natural language explanations over a simple activity label, underscoring the need for explainable activity recognition systems. Finally, we show that explanations generated by some XAI methods can lead users to lose confidence in the accuracy of the underlying activity recognition model, while others lead users to gain confidence. Taking all studied factors into consideration, we make a recommendation regarding which existing XAI method leads to the best performance in the domain of smart home automation, and discuss a range of topics for future work in this area.",0
"Abstract:  Intelligent smart home systems have become increasingly popular in recent years due to their ability to monitor and control various aspects of daily life such as temperature, lighting, security, and entertainment. One critical component of these systems is activity recognition, which refers to the identification of user activities within the home environment. However, existing activity recognition algorithms often rely on complex models that are difficult to interpret or lack explainability. This can lead to a lack of trust in the system and reduce overall satisfaction among users.  To address this issue, we propose a novel approach to activity recognition for smart home systems that emphasizes explainability. Our method utilizes natural language processing techniques to identify meaningful patterns in sensor data and generate human-interpretable explanations for detected activities. We evaluate our approach through extensive experiments using real-world datasets and demonstrate significant improvements over state-of-the-art methods in terms of accuracy and explainability. Additionally, we explore the impact of different types of sensor data and feature engineering techniques on our proposed model's performance.  Overall, our work represents a step forward towards creating more intelligible and reliable smart home systems that better meet user needs and expectations. Our findings contribute to the broader field of machine learning and artificial intelligence by highlighting the importance of interpretable models in building trustworthy systems that support human decision making.",1
"In the image classification task, the most common approach is to resize all images in a dataset to a unique shape, while reducing their precision to a size which facilitates experimentation at scale. This practice has benefits from a computational perspective, but it entails negative side-effects on performance due to loss of information and image deformation. In this work we introduce the MAMe dataset, an image classification dataset with remarkable high resolution and variable shape properties. The goal of MAMe is to provide a tool for studying the impact of such properties in image classification, while motivating research in the field. The MAMe dataset contains thousands of artworks from three different museums, and proposes a classification task consisting on differentiating between 29 mediums (i.e. materials and techniques) supervised by art experts. After reviewing the singularity of MAMe in the context of current image classification tasks, a thorough description of the task is provided, together with dataset statistics. Experiments are conducted to evaluate the impact of using high resolution images, variable shape inputs and both properties at the same time. Results illustrate the positive impact in performance when using high resolution images, while highlighting the lack of solutions to exploit variable shapes. An additional experiment exposes the distinctiveness between the MAMe dataset and the prototypical ImageNet dataset. Finally, the baselines are inspected using explainability methods and expert knowledge, to gain insights on the challenges that remain ahead.",0
"Title: Image resolution vs. shape variability - Comparing different datasets to determine significance of high resolution images Abstract: This study examines the importance of high-resolution imagery by comparing two popular image databases: MAMe (Multi-Angle Middlebury) dataset and NJU-2KT (Nankai University â€“ 2 kilo Textures) database. Both datasets contain variable shapes but differ significantly in terms of image quality. Our results show that while higher resolution images offer finer details, these differences may not always translate into improved recognition performance. We demonstrate how certain aspects like lighting conditions can greatly affect object detection rates, regardless of resolution. In conclusion, our research highlights that balancing image clarity with other attributes, such as metadata accuracy, must be considered when collecting new images. Overall, we recommend using datasets that prioritize a balance of key factors rather than solely relying on resolution alone. This work has important implications for machine learning practitioners and data collection managers alike. Keywords: Image resolution, shape variability, image datasets, comparative analysis, recognition performance",1
"With the explosive growth of e-commerce, online transaction fraud has become one of the biggest challenges for e-commerce platforms. The historical behaviors of users provide rich information for digging into the users' fraud risk. While considerable efforts have been made in this direction, a long-standing challenge is how to effectively exploit internal user information and provide explainable prediction results. In fact, the value variations of same field from different events and the interactions of different fields inside one event have proven to be strong indicators for fraudulent behaviors. In this paper, we propose the Dual Importance-aware Factorization Machines (DIFM), which exploits the internal field information among users' behavior sequence from dual perspectives, i.e., field value variations and field interactions simultaneously for fraud detection. The proposed model is deployed in the risk management system of one of the world's largest e-commerce platforms, which utilize it to provide real-time transaction fraud detection. Experimental results on real industrial data from different regions in the platform clearly demonstrate that our model achieves significant improvements compared with various state-of-the-art baseline models. Moreover, the DIFM could also give an insight into the explanation of the prediction results from dual perspectives.",0
"This is an important problem to solve as fraud can have negative consequences on organizations and individuals. Here we present a methodology that allows us to model field value variations simultaneously with their interactions, providing enhanced insights into how changes in one field affect another. This technique can assist in detecting fraud by identifying unusual patterns, highlighting abnormal behavior and uncovering new types of attacks. By leveraging real world data sets our approach demonstrates improved performance compared to traditional methods. Our results show promise in prevention and detection applications but more research is required to further develop these techniques.",1
"Stochastic gradient descent (SGD) undergoes complicated multiplicative noise for the mean-square loss. We use this property of the SGD noise to derive a stochastic differential equation (SDE) with simpler additive noise by performing a non-uniform transformation of the time variable. In the SDE, the gradient of the loss is replaced by that of the logarithmized loss. Consequently, we show that, near a local or global minimum, the stationary distribution $P_\mathrm{ss}(\theta)$ of the network parameters $\theta$ follows a power-law with respect to the loss function $L(\theta)$, i.e. $P_\mathrm{ss}(\theta)\propto L(\theta)^{-\phi}$ with the exponent $\phi$ specified by the mini-batch size, the learning rate, and the Hessian at the minimum. We obtain the escape rate formula from a local minimum, which is determined not by the loss barrier height $\Delta L=L(\theta^s)-L(\theta^*)$ between a minimum $\theta^*$ and a saddle $\theta^s$ but by the logarithmized loss barrier height $\Delta\log L=\log[L(\theta^s)/L(\theta^*)]$. Our escape-rate formula explains an empirical fact that SGD prefers flat minima with low effective dimensions.",0
"In recent years, large language models have achieved remarkable success in natural language processing tasks using stochastic gradient descent (SGD) as the optimization algorithm. Understanding how these models converge has become increasingly important as they continue to grow larger and more powerful. This work studies the convergence behavior of SGD during the training process of large language models, specifically focusing on two aspects: logarithmic landscapes and power-law escape rates.  First, we analyze the loss surface of the model by looking at the change in average log likelihood over different batch sizes. We observe that the loss landscape exhibits a logarithmic curvature, which implies that the number of parameters in the model only affects the scale, but not the shape of the objective function. This result helps explain why smaller LLMs can perform comparably well with larger ones, given enough computational resources.  Next, we investigate the distribution of parameter updates across iterations. By fitting empirical distributions to these updates, we find evidence of a power law relationship between the size of the update and the time step. This suggests that most progress occurs from small steps early on, while large adjustments come later in training, slowing down the learning process. Our finding echoes similar results found in other nonlinear systems, further indicating that the power law phenomenon may constitute a universal characteristic of many complex processes.  Our study sheds light on the intricate behaviors of SGD in optimizing deep neural networks, providing insights into both the geometric structure of the landscape and the statistical properties of the optimization process. These discoveries could inspire new techniques for improving training efficiency and controlling generalization performance, ultimately paving the path towards even better performing LLMs.  Note: This abstract summarizes our main conclusions without repeating details from specific sections or introducing additional material from related works. It highlights t",1
"In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an Artificial Intelligence (AI) Health Outcomes Challenge seeking solutions to predict risk in value-based care for incorporation into CMS Innovation Center payment and service delivery models. Recently, modern language models have played key roles in a number of health related tasks. This paper presents, to the best of our knowledge, the first application of these models to patient readmission prediction. To facilitate this, we create a dataset of 1.2 million medical history samples derived from the Limited Dataset (LDS) issued by CMS. Moreover, we propose a comprehensive modeling solution centered on a deep learning framework for this data. To demonstrate the framework, we train an attention-based Transformer to learn Medicare semantics in support of performing downstream prediction tasks thereby achieving 0.91 AUC and 0.91 recall on readmission classification. We also introduce a novel data pre-processing pipeline and discuss pertinent deployment considerations surrounding model explainability and bias.",0
"Our paper proposes an explainable health risk predictor that utilizes a transformer-based medicare claim encoder. The proposed model uses electronic health records (EHRs) and claims data to predict future health risks by identifying patterns and relationships within patient data. This model can provide insights into how patients may develop certain conditions over time based on their current and past medical history. Additionally, our model allows healthcare providers to make more informed decisions regarding patient care as they have access to a transparent tool for explaining predicted outcomes. We evaluated our model using several metrics including precision, recall, F1 score, accuracy, area under ROC curve (AUC), and SHAP values to assess the performance of our approach. The results show promising potential for improving healthcare delivery through early identification of high-risk patients who require immediate interventions. In conclusion, our proposed model has the potential to revolutionize healthcare practice by providing reliable predictions that can enhance both clinical effectiveness and efficiency while ensuring trustworthy decision making processes based on transparent and interpretable algorithms.",1
"As big data becomes ubiquitous across domains, and more and more stakeholders aspire to make the most of their data, demand for machine learning tools has spurred researchers to explore the possibilities of automated machine learning (AutoML). AutoML tools aim to make machine learning accessible for non-machine learning experts (domain experts), to improve the efficiency of machine learning, and to accelerate machine learning research. But although automation and efficiency are among AutoML's main selling points, the process still requires human involvement at a number of vital steps, including understanding the attributes of domain-specific data, defining prediction problems, creating a suitable training data set, and selecting a promising machine learning technique. These steps often require a prolonged back-and-forth that makes this process inefficient for domain experts and data scientists alike, and keeps so-called AutoML systems from being truly automatic. In this review article, we introduce a new classification system for AutoML systems, using a seven-tiered schematic to distinguish these systems based on their level of autonomy. We begin by describing what an end-to-end machine learning pipeline actually looks like, and which subtasks of the machine learning pipeline have been automated so far. We highlight those subtasks which are still done manually - generally by a data scientist - and explain how this limits domain experts' access to machine learning. Next, we introduce our novel level-based taxonomy for AutoML systems and define each level according to the scope of automation support provided. Finally, we lay out a roadmap for the future, pinpointing the research required to further automate the end-to-end machine learning pipeline and discussing important challenges that stand in the way of this ambitious goal.",0
"As machine learning becomes increasingly pervasive across industries and applications, there has been growing interest in automating the process of designing, training, and deploying machine learning models. This has led to the emergence of a new field known as Automatic Machine Learning (AutoML), which involves developing algorithms that can perform these tasks automatically without requiring human intervention. In recent years, there have been significant advances in AutoML, leading to the development of a wide range of tools and platforms that enable users to build high-quality models quickly and easily. However, despite these successes, there remain several challenges that must be addressed if AutoML is to continue to grow and evolve. For example, many existing AutoML systems rely heavily on handcrafted features and simple model architectures, which limits their effectiveness in certain domains such as computer vision and natural language processing. Additionally, current approaches often lack transparency and interpretability, making it difficult for users to understand how decisions were made by the underlying system. To address these issues and realize the full potential of AutoML, future research should focus on developing more advanced feature engineering techniques, improving model interpretability, and exploring novel search spaces that incorporate complex model architectures and hyperparameters. Ultimately, successful solutions to these challenges could lead to new opportunities in fields ranging from healthcare to finance, enabling machines to solve problems that were previously beyond their capabilities.",1
"A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional parametrization (e.g. t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D projections is usually qualitatively decided, by setting projections side-by-side and letting human judgment decide which projection is the best. In this work, we propose a quantitative way of evaluating projections, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select 'good' and 'misleading' views between scatterplots of low-level projections of image datasets, simulating the way people usually select projections. We use the study data as labels for a set of quality metrics whose purpose is to discover and quantify what exactly people are looking for when deciding between projections. With this proxy for human judgments, we use it to rank projections on new datasets, explain why they are relevant, and quantify the degree of subjectivity in projections selected.",0
"This paper presents a methodology called ""DumbleDR"" that can predict user preferences on dimensionality reduction projection quality for visualizing high-dimensional data sets using machine learning algorithms. Our approach combines two existing dimensions, Distance Preservation (DP) and Data Utilization (DU), into one framework to address both perceptual aspects of DR methods as well as their functional usability. We employ four distinct evaluation criteria that account for different perspectives such as global vs local preservation error, reconstruction accuracy, and model complexity/interpretability. By leveraging these metrics, we train regression models tailored towards specific tasks and demonstrate significant improvement over the previous state-of-the-art across multiple use cases, including UCI benchmark datasets and real world applications in neuroscience research. Our results suggest that the proposed framework is effective at capturing diverse human judgments about DR quality while providing computational efficiency by operating directly on raw dataset features instead of learned embeddings. Overall, our study contributes insights toward designing more intuitive systems that better support users in understanding complex multi-attribute relationships.",1
"Network data can be conveniently modeled as a graph signal, where data values are assigned to nodes of a graph that describes the underlying network topology. Successful learning from network data is built upon methods that effectively exploit this graph structure. In this work, we leverage graph signal processing to characterize the representation space of graph neural networks (GNNs). We discuss the role of graph convolutional filters in GNNs and show that any architecture built with such filters has the fundamental properties of permutation equivariance and stability to changes in the topology. These two properties offer insight about the workings of GNNs and help explain their scalability and transferability properties which, coupled with their local and distributed nature, make GNNs powerful tools for learning in physical networks. We also introduce GNN extensions using edge-varying and autoregressive moving average graph filters and discuss their properties. Finally, we study the use of GNNs in recommender systems and learning decentralized controllers for robot swarms.",0
"This paper provides a comprehensive overview of graph signals, convolutions, and neural networks, from classical graph filters to modern graph neural networks (GNNs). We begin by introducing basic concepts such as graph Laplacians, spectral graph theory, and random walks on graphs. Next, we discuss traditional signal processing techniques applied to graph signals such as filtering and sampling. Afterward, we focus on recent advances in GNNs and their connections to classic graph filter design methods. Throughout the paper, we provide mathematical foundations and intuition behind key results with examples and illustrative graphics. Our exposition emphasizes both theoretical aspects and practical applications of these powerful tools. By surveying significant contributions made so far, our work sheds light on future research directions that hold high promise towards solving complex problems arising in real world data.",1
"Timely completion of design cycles for complex systems ranging from consumer electronics to hypersonic vehicles relies on rapid simulation-based prototyping. The latter typically involves high-dimensional spaces of possibly correlated control variables (CVs) and quantities of interest (QoIs) with non-Gaussian and possibly multimodal distributions. We develop a model-agnostic, moment-independent global sensitivity analysis (GSA) that relies on differential mutual information to rank the effects of CVs on QoIs. The data requirements of this information-theoretic approach to GSA are met by replacing computationally intensive components of the physics-based model with a deep neural network surrogate. Subsequently, the GSA is used to explain the network predictions, and the surrogate is deployed to close design loops. Viewed as an uncertainty quantification method for interrogating the surrogate, this framework is compatible with a wide variety of black-box models. We demonstrate that the surrogate-driven mutual information GSA provides useful and distinguishable rankings on two applications of interest in energy storage. Consequently, our information-theoretic GSA provides an ""outer loop"" for accelerated product design by identifying the most and least sensitive input directions and performing subsequent optimization over appropriately reduced parameter subspaces.",0
"""This paper presents a framework for explainable deep learning of multiscale systems using mutual information (MI). MI measures the amount of information shared by two random variables, providing insights into feature importance and explaining how models make decisions. We apply this methodology to three different applications: neuroimaging data analysis, climate science, and financial time series prediction. Our results show that MI can effectively identify meaningful features across multiple scales of these complex systems. By accurately identifying important predictors, we can better interpret model behavior and improve decision making.""  (Please note I am only able to provide an example of an abstract without the following constraints.)",1
"Convolutional neural network (CNN) has achieved unprecedented success in image super-resolution tasks in recent years. However, the network's performance depends on the distribution of the training sets and degrades on out-of-distribution samples. This paper adopts a Bayesian approach for estimating uncertainty associated with output and applies it in a deep image super-resolution model to address the concern mentioned above. We use the uncertainty estimation technique using the batch-normalization layer, where stochasticity of the batch mean and variance generate Monte-Carlo (MC) samples. The MC samples, which are nothing but different super-resolved images using different stochastic parameters, reconstruct the image, and provide a confidence or uncertainty map of the reconstruction. We propose a faster approach for MC sample generation, and it allows the variable image size during testing. Therefore, it will be useful for image reconstruction domain. Our experimental findings show that this uncertainty map strongly relates to the quality of reconstruction generated by the deep CNN model and explains its limitation. Furthermore, this paper proposes an approach to reduce the model's uncertainty for an input image, and it helps to defend the adversarial attacks on the image super-resolution model. The proposed uncertainty reduction technique also improves the performance of the model for out-of-distribution test images. To the best of our knowledge, we are the first to propose an adversarial defense mechanism in any image reconstruction domain.",0
"This paper proposes a novel approach to uncertainty estimation and reduction in batch normalized single image super-resolution networks (SRNs) using Bayesian methods. SRNs have shown promising results in improving the quality of low-resolution images by upscaling them to higher resolutions. However, these models can suffer from overconfidence, leading to poor generalization performance. To address this issue, we introduce a probabilistic formulation of batch normalization that enables the model to capture uncertainty information during training and inference. Our method leverages the variational autoencoder framework with Monte Carlo sampling techniques to approximate the posterior distribution of network parameters and latent variables. We then use this information to estimate and reduce uncertainty in the predictions of the SRN. Experimental evaluation on three benchmark datasets demonstrates significant improvements in both quantitative metrics and visual inspection compared to state-of-the-art methods. Overall, our work advances the understanding of uncertainties in deep neural networks and has potential applications in computer vision, robotics, and other domains where high-quality super-resolution imagery is critical.",1
"Knowledge distillation (KD), transferring knowledge from a cumbersome teacher model to a lightweight student model, has been investigated to design efficient neural architectures. Generally, the objective function of KD is the Kullback-Leibler (KL) divergence loss between the softened probability distributions of the teacher model and the student model with the temperature scaling hyperparameter tau. Despite its widespread use, few studies have discussed the influence of such softening on generalization. Here, we theoretically show that the KL divergence loss focuses on the logit matching when tau increases and the label matching when tau goes to 0 and empirically show that the logit matching is positively correlated to performance improvement in general. From this observation, we consider an intuitive KD loss function, the mean squared error (MSE) between the logit vectors, so that the student model can directly learn the logit of the teacher model. The MSE loss outperforms the KL divergence loss, explained by the difference in the penultimate layer representations between the two losses. Furthermore, we show that sequential distillation can improve performance and that KD, particularly when using the KL divergence loss with small tau, mitigates the label noise. The code to reproduce the experiments is publicly available online at https://github.com/jhoon-oh/kd_data/.",0
"In recent years, deep learning has become increasingly popular as a means of building accurate models for natural language processing tasks such as sentiment analysis, question answering, and machine translation. However, these large models can suffer from computational constraints, requiring specialized hardware for effective deployment. One solution to this problem is knowledge distillation, which involves training small neural networks on high-quality soft targets generated by larger pretrained model teachers. This process enables students to learn the same representation space as their teacher without needing to access the original data used during pretraining. There are two primary methods that have been proposed for generating these soft targets: mean squared error (MSE) loss and cross entropy minimization using KL divergence (KLD). Despite the widespread use of both approaches, there has yet to be a comprehensive evaluation comparing them directly. As a result, we sought to explore the effectiveness of each method under varying conditions and determine whether one approach consistently outperforms the other across a range of datasets and architectures. We found that while MSE generally produced better results overall, the choice between MSE and KLD ultimately depends on several factors including task complexity, architecture size, available training time, and dataset size. These findings provide important insights into the strengths and weaknesses of each method, allowing researchers and practitioners to make more informed decisions when choosing a loss function for knowledge distillation. By identifying optimal settings for each scenario, our work represents a significant step forward toward maximizing performance of distilled models while minimizing computation costs.",1
"Machine learning is increasingly applied in high-stakes decision making that directly affect people's lives, and this leads to an increased demand for systems to explain their decisions. Explanations often take the form of counterfactuals, which consists of conveying to the end user what she/he needs to change in order to improve the outcome. Computing counterfactual explanations is challenging, because of the inherent tension between a rich semantics of the domain, and the need for real time response. In this paper we present GeCo, the first system that can compute plausible and feasible counterfactual explanations in real time. At its core, GeCo relies on a genetic algorithm, which is customized to favor searching counterfactual explanations with the smallest number of changes. To achieve real-time performance, we introduce two novel optimizations: $\Delta$-representation of candidate counterfactuals, and partial evaluation of the classifier. We compare empirically GeCo against five other systems described in the literature, and show that it is the only system that can achieve both high quality explanations and real time answers.",0
"A new algorithm has been developed that can generate high-quality counterfactual explanations for natural language processing tasks in real time. This algorithm, called GeCo (for Generalized Contractive Explanation), builds upon previous work on contraction-based explanation generation by incorporating two key innovations. Firstly, GeCo uses a generalized version of the theory underlying traditional contractive explanation methods, which allows for more flexible reasoning about why certain decisions were made. Secondly, it employs machine learning techniques to optimize the computational efficiency of generating these explanations, making them available for use during runtime rather than post hoc. Evaluations demonstrate that GeCo produces explanations that are both accurate and concise, while maintaining good performance even under tight computation budgets. Overall, GeCo represents a significant advancement towards enabling explainability in NLP systems.",1
"We propose real-time, six degrees of freedom (6DoF), 3D face pose estimation without face detection or landmark localization. We observe that estimating the 6DoF rigid transformation of a face is a simpler problem than facial landmark detection, often used for 3D face alignment. In addition, 6DoF offers more information than face bounding box labels. We leverage these observations to make multiple contributions: (a) We describe an easily trained, efficient, Faster R-CNN--based model which regresses 6DoF pose for all faces in the photo, without preliminary face detection. (b) We explain how pose is converted and kept consistent between the input photo and arbitrary crops created while training and evaluating our model. (c) Finally, we show how face poses can replace detection bounding box training labels. Tests on AFLW2000-3D and BIWI show that our method runs at real-time and outperforms state of the art (SotA) face pose estimators. Remarkably, our method also surpasses SotA models of comparable complexity on the WIDER FACE detection benchmark, despite not been optimized on bounding box labels.",0
"In recent years, there has been significant progress made towards automating the process of face alignment and detection through the use of deep learning techniques, such as convolutional neural networks (CNNs). While these methods have proven effective, they often require large amounts of labeled data and computational resources. Additionally, current state-of-the-art systems typically operate under the assumption that faces can only move along two axes (pitch and yaw), which is known as three degrees of freedom (3DoF) pose estimation. This means that they cannot capture important aspects of facial movement, such as tilt or roll, which are referred to as the remaining three degrees of freedom (3DoF) or six degrees of freedom (6DoF) pose estimation.  In our work, we propose a novel method called ""img2pose"" that tackles both face alignment and detection while estimating face pose up to 6DoF utilizing a lightweight CNN architecture coupled with low-resolution feature extraction and depthwise separable convolutions. Our approach effectively captures fine-grained details while reducing computation complexity compared to previous works. We demonstrate on challenging benchmark datasets that our model outperforms prior arts in terms of accuracy and efficiency. Furthermore, we showcase several applications benefiting from high-quality 6DoF face representation such as avatar creation, view synthesis, and reenactment.  Our contributions are threefold: Firstly, we introduce a concise framework that addresses face alignment and detection simultaneously by regressing accurate face landmarks with 6DoF pose prediction directly from single RGB images. Secondly, we design an efficient network composed of small components allowing training on limited hardware without compromising performance. Lastly, we extensively evaluate the proposed method across multiple tasks demonstrating its superiority over existing methods while operating at real-time frame rates on average PC GPUs. These benefits make our approach highly suitable for real-world deployment, particularly in resource-constrained devices like smartphones o",1
"The last few years have witnessed an increased interest in incorporating physics-informed inductive bias in deep learning frameworks. In particular, a growing volume of literature has been exploring ways to enforce energy conservation while using neural networks for learning dynamics from observed time-series data. In this work, we survey ten recently proposed energy-conserving neural network models, including HNN, LNN, DeLaN, SymODEN, CHNN, CLNN and their variants. We provide a compact derivation of the theory behind these models and explain their similarities and differences. Their performance are compared in 4 physical systems. We point out the possibility of leveraging some of these energy-conserving models to design energy-based controllers.",0
"Abstract: This research presents a benchmark study on energy-efficient neural networks for learning dynamics from data. The goal is to identify effective ways to optimize these models for low energy consumption while maintaining accurate predictions. We evaluate several state-of-the-art techniques, including network pruning, quantization, dynamic inference, and model selection. Our experiments show that these methods can significantly reduce energy usage without compromising accuracy. Additionally, we provide insights into tradeoffs between energy efficiency and prediction performance, as well as guidelines for selecting appropriate techniques based on specific use cases. Overall, our findings contribute to advancing sustainable artificial intelligence by enabling efficient real-world deployment of deep learning systems.",1
"Neural networks trained with SGD were recently shown to rely preferentially on linearly-predictive features and can ignore complex, equally-predictive ones. This simplicity bias can explain their lack of robustness out of distribution (OOD). The more complex the task to learn, the more likely it is that statistical artifacts (i.e. selection biases, spurious correlations) are simpler than the mechanisms to learn. We demonstrate that the simplicity bias can be mitigated and OOD generalization improved. We train a set of similar models to fit the data in different ways using a penalty on the alignment of their input gradients. We show theoretically and empirically that this induces the learning of more complex predictive patterns. OOD generalization fundamentally requires information beyond i.i.d. examples, such as multiple training environments, counterfactual examples, or other side information. Our approach shows that we can defer this requirement to an independent model selection stage. We obtain SOTA results in visual recognition on biased data and generalization across visual domains. The method - the first to evade the simplicity bias - highlights the need for a better understanding and control of inductive biases in deep learning.",0
"Artificial intelligence (AI) has made tremendous progress over recent years due to advances in deep learning techniques such as supervised training on large datasets. However, there remains a fundamental problem that all current machine learning models suffer from - the simplicity bias. This refers to the tendency of highly complex algorithms to prefer simple solutions over more accurate ones due to computational constraints and limited data availability. In this work, we present evidence that suggests building diverse sets of models can overcome the simplicity bias by identifying less obvious but more effective solutions through out-of-distribution generalization performance evaluations. Our approach involves selecting hyperparameters based on their ability to generate architectures which exhibit higher marginal likelihoods for held-out validation sets. We showcase two experiments using linear regression and image classification, where our method significantly boosted model effectiveness compared to standard, non-diversified approaches across multiple metrics including accuracy, precision, recall, F1 score and area under receiver operating curve. These findings have important implications for improving model robustness and real-world applicability. They provide new insights into how to address the complexity-accuracy tradeoff problem inherent in most modern deep learning frameworks while maintaining high test set performance even beyond the training distribution, making AI systems more versatile and human-centric. Ultimately, these results could lead to better decision support tools in domains ranging from healthcare to finance, where correct predictions matter greatly yet data scarcity and variability create significant challenges.",1
"Transformers, composed of multiple self-attention layers, hold strong promises toward a generic learning primitive applicable to different data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art (SOTA) standard accuracy with better parameter efficiency. Since self-attention helps a model systematically align different components present inside the input data, it leaves grounds to investigate its performance under model robustness benchmarks. In this work, we study the robustness of the Vision Transformer (ViT) against common corruptions and perturbations, distribution shifts, and natural adversarial examples. We use six different diverse ImageNet datasets concerning robust classification to conduct a comprehensive performance comparison of ViT models and SOTA convolutional neural networks (CNNs), Big-Transfer. Through a series of six systematically designed experiments, we then present analyses that provide both quantitative and qualitative indications to explain why ViTs are indeed more robust learners. For example, with fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1 accuracy of 28.10% on ImageNet-A which is 4.3x higher than a comparable variant of BiT. Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness. Code for reproducing our experiments is available here: https://git.io/J3VO0.",0
"Vision transformer models have emerged as powerful tools for image classification tasks due to their ability to learn robust representations from large-scale datasets. In our work, we evaluate the performance of several vision transformer architectures on a range of benchmark datasets and show that they consistently outperform conventional convolutional neural network (CNN) baselines across multiple metrics. Our results demonstrate the remarkable generalization capacity of these models, which can effectively capture complex relationships between visual features without relying on handcrafted feature extractors or explicit spatial hierarchies. Furthermore, we analyze the effectiveness of different attention mechanisms and layer sizes in vision transformers and identify key design principles for optimizing model performance. Overall, our findings highlight the promise of transformer architectures for computer vision problems and suggest new directions for future research in this area.",1
"This study investigates the theoretical foundations of t-distributed stochastic neighbor embedding (t-SNE), a popular nonlinear dimension reduction and data visualization method. A novel theoretical framework for the analysis of t-SNE based on the gradient descent approach is presented. For the early exaggeration stage of t-SNE, we show its asymptotic equivalence to a power iteration based on the underlying graph Laplacian, characterize its limiting behavior, and uncover its deep connection to Laplacian spectral clustering, and fundamental principles including early stopping as implicit regularization. The results explain the intrinsic mechanism and the empirical benefits of such a computational strategy. For the embedding stage of t-SNE, we characterize the kinematics of the low-dimensional map throughout the iterations, and identify an amplification phase, featuring the intercluster repulsion and the expansive behavior of the low-dimensional map. The general theory explains the fast convergence rate and the exceptional empirical performance of t-SNE for visualizing clustered data, brings forth the interpretations of the t-SNE output, and provides theoretical guidance for selecting tuning parameters in various applications.",0
"This is the abstract:  The Abstract: The Theoretical Foundations of t-SNE (Tensor Stochastic Neighbor Embedding) provide insight into how t-SNE works at a theoretical level, which helps in understanding why t-SNE can effectively visualize high-dimensional clustered data sets in such efficient ways. By studying the theory behind t-SNE we gain insights that lead us to discover how best to use t-SNE, enabling more effective usage of the tool in practice, ultimately leading to better analysis results. In this context, we showcase current work on these topics, emphasize open problems relevant for applications involving large amounts of structured and unstructured data in diverse areas like astronomy, biology, earth sciences, economics and sociology. Ultimately the aim is to advance our knowledge of t-SNE so that it continues to improve over time so researchers have access to increasingly powerful tools to enable their scientific discovery.",1
"Network Embedding aims to learn a function mapping the nodes to Euclidean space contribute to multiple learning analysis tasks on networks. However, the noisy information behind the real-world networks and the overfitting problem both negatively impact the quality of embedding vectors. To tackle these problems, researchers utilize Adversarial Training for Network Embedding (AdvTNE) and achieve state-of-the-art performance. Unlike the mainstream methods introducing perturbations on the network structure or the data feature, AdvTNE directly perturbs the model parameters, which provides a new chance to understand the mechanism behind. In this paper, we explain AdvTNE theoretically from an optimization perspective. Considering the Power-law property of networks and the optimization objective, we analyze the reason for its excellent results. According to the above analysis, we propose a new activation to enhance the performance of AdvTNE. We conduct extensive experiments on four real networks to validate the effectiveness of our method in node classification and link prediction. The results demonstrate that our method is superior to the state-of-the-art baseline methods.",0
"Adversarial training has recently gained attention as a promising method for improving the robustness of deep learning models against adversarial attacks. In particular, network embedding methods based on adversarial training have been shown to achieve state-of-the-art performance on several benchmark datasets. Despite these successes, there remains a significant gap in our understanding of how adversarial training works at the optimization level. This paper seeks to fill that gap by providing a detailed analysis of the properties and behaviors of gradient descent algorithms when trained under adversarial conditions. By studying both linearized versions of common GAN loss functions and nonlinear variants with exact differentiability guarantees, we uncover new insights into the dynamics of adversarial training and provide recommendations for improving current methods. Our results highlight the importance of properly regularizing adversarial losses and demonstrate the potential benefits of using alternative objective functions tailored specifically for data augmentation applications. Overall, this work represents an important step towards developing more effective and efficient approaches for enhancing model robustness through adversarial training.",1
"This paper provides the details of implementing two important policy gradient methods to solve the inverted pendulum problem. These are namely the Deep Deterministic Policy Gradient (DDPG) and the Proximal Policy Optimization (PPO) algorithm. The problem is solved by using an actor-critic model where an actor-network is used to learn the policy function and a critic network is to evaluate the actor-network by learning to estimate the Q function. Apart from briefly explaining the mathematics behind these two algorithms, the details of python implementation are provided which helps in demystifying the underlying complexity of the algorithm. In the process, the readers will be introduced to OpenAI/Gym, Tensorflow 2.x and Keras utilities used for implementing the above concepts.",0
"This tutorial provides a step-by-step guide on how to use policy gradient methods to control an inverted pendulum. The inverted pendulum is a classic problem in robotics that has been used extensively as a benchmark for testing the effectiveness of control algorithms. The goal of this tutorial is to introduce readers to policy gradients and demonstrate their application in controlling an inverted pendulum. The tutorial begins by introducing the concept of reinforcement learning (RL) and policy gradients, before moving onto a detailed explanation of how policy gradients can be applied to the inverted pendulum problem. The implementation details are provided using Python, and the code is made publicly available. By following the steps outlined in this tutorial, readers will gain hands-on experience in designing RL agents for controlling physical systems. Additionally, the tutorial highlights several key aspects such as exploration strategies, regularization techniques, and hyperparameter tuning, which are crucial for developing effective RL algorithms. Overall, this tutorial serves as an excellent starting point for researchers and students interested in exploring policy gradient methods for robotic controls problems.",1
"Interpretability of intelligent algorithms represented by deep learning has been yet an open problem. We discuss the shortcomings of the existing explainable method based on the two attributes of explanation, which are called completeness and explicitness. Furthermore, we point out that a model that completely relies on feed-forward mapping is extremely easy to cause inexplicability because it is hard to quantify the relationship between this mapping and the final model. Based on the perspective of the data space division, the principle of complete local interpretable model-agnostic explanations (CLIMEP) is proposed in this paper. To study the classification problems, we further discussed the equivalence of the CLIMEP and the decision boundary. As a matter of fact, it is also difficult to implementation of CLIMEP. To tackle the challenge, motivated by the fact that a fully-connected neural network (FCNN) with piece-wise linear activation functions (PWLs) can partition the input space into several linear regions, we extend this result to arbitrary FCNNs by the strategy of linearizing the activation functions. Applying this technique to solving classification problems, it is the first time that the complete decision boundary of FCNNs has been able to be obtained. Finally, we propose the DecisionNet (DNet), which divides the input space by the hyper-planes of the decision boundary. Hence, each linear interval of the DNet merely contains samples of the same label. Experiments show that the surprising model compression efficiency of the DNet with an arbitrary controlled precision.",0
"This paper explores how neural networks can be used as a tool to explain complex concepts and relationships within vast amounts of data space. We examine two main approaches for dividing up the data space: dimensionality reduction techniques and clustering algorithms. By using these methods, we demonstrate that it is possible to analyze large datasets quickly and accurately while reducing the complexity associated with managing big data sets. Our results show that neural network architectures which employ dimensionality reduction perform better than those without because they provide clear representations of high-dimensional data sets on lower-dimensional planes where they can be easily visualized and analyzed. Furthermore, we illustrate that the use of clustering techniques such as K Means or DBSCAN allow for further analysis of smaller subsets of relevant data points, enabling deeper insights into underlying patterns and trends within the full dataset.",1
"Accurate and explainable health event predictions are becoming crucial for healthcare providers to develop care plans for patients. The availability of electronic health records (EHR) has enabled machine learning advances in providing these predictions. However, many deep learning based methods are not satisfactory in solving several key challenges: 1) effectively utilizing disease domain knowledge; 2) collaboratively learning representations of patients and diseases; and 3) incorporating unstructured text. To address these issues, we propose a collaborative graph learning model to explore patient-disease interactions and medical domain knowledge. Our solution is able to capture structural features of both patients and diseases. The proposed model also utilizes unstructured text data by employing an attention regulation strategy and then integrates attentive text features into a sequential learning process. We conduct extensive experiments on two important healthcare problems to show the competitive prediction performance of the proposed method compared with various state-of-the-art models. We also confirm the effectiveness of learned representations and model interpretability by a set of ablation and case studies.",0
"In this research paper, we explore the challenges involved in temporal event prediction for healthcare applications using collaborative graph learning techniques with auxiliary text data sources. We examine how incorporating additional types of healthcare data such as clinical notes can improve model performance by providing more contextual and granular patient information that cannot be captured through traditional methods alone. Our proposed framework leverages advanced machine learning algorithms and optimization techniques to integrate multiple heterogeneous sources into a single unified architecture, enabling accurate predictions of future events within specified time intervals. Our results demonstrate significant improvements over baseline models across several evaluation metrics, highlighting the effectiveness of our approach in improving decision support systems for healthcare providers.  Overall, our work contributes to the growing body of literature on temporal event prediction in healthcare using complex computational approaches that leverage diverse datasets for better outcomes. Our study offers valuable insights for practitioners seeking robust solutions capable of addressing the unique challenges posed by medical environments while maintaining ethical considerations related to privacy concerns associated with sensitive patient data. Further directions for future research include expanding the application scope beyond acute care settings, evaluating real-world impacts on operational efficiency, and exploring novel combinations of data modalities tailored specifically towards personalized medicine efforts.",1
"Explainable AI has emerged to be a key component for black-box machine learning approaches in domains with a high demand for reliability or transparency. Examples are medical assistant systems, and applications concerned with the General Data Protection Regulation of the European Union, which features transparency as a cornerstone. Such demands require the ability to audit the rationale behind a classifier's decision. While visualizations are the de facto standard of explanations, they come short in terms of expressiveness in many ways: They cannot distinguish between different attribute manifestations of visual features (e.g. eye open vs. closed), and they cannot accurately describe the influence of absence of, and relations between features. An alternative would be more expressive symbolic surrogate models. However, these require symbolic inputs, which are not readily available in most computer vision tasks. In this paper we investigate how to overcome this: We use inherent features learned by the network to build a global, expressive, verbal explanation of the rationale of a feed-forward convolutional deep neural network (DNN). The semantics of the features are mined by a concept analysis approach trained on a set of human understandable visual concepts. The explanation is found by an Inductive Logic Programming (ILP) method and presented as first-order rules. We show that our explanation is faithful to the original black-box model.   The code for our experiments is available at https://github.com/mc-lovin-mlem/concept-embeddings-and-ilp/tree/ki2020.",0
"Deep neural networks (DNNs) have been successfully applied across many application domains due to their excellent performance on complex tasks. However, they often struggle to provide explanations of how they make predictions that humans can comprehend easily. Traditional approaches such as feature attribution or visualization methods may only reveal incomplete or ambiguous insights into decision making processes of DNNs. In this study, we present an expressive method to explain decisions made by deep neural networks using concept analysis and inductive logic programming techniques. We perform multi-target prediction and interpretability simultaneously based on human knowledge which provides both numerical accuracy and transparent reasoning pathways. Our proposed framework overcomes limitations of previous approaches by generating more accurate and informative explanations while maintaining low computational cost. Extensive experimental evaluations demonstrate effectiveness and efficiency of our approach compared to state-of-the-art baselines under various settings. Overall, our work contributes towards enhancing trustworthiness and transparency of intelligent systems utilizing DNN models in real-world applications.",1
"Generating images from a single sample, as a newly developing branch of image synthesis, has attracted extensive attention. In this paper, we formulate this problem as sampling from the conditional distribution of a single image, and propose a hierarchical framework that simplifies the learning of the intricate conditional distributions through the successive learning of the distributions about structure, semantics and texture, making the process of learning and generation comprehensible. On this basis, we design ExSinGAN composed of three cascaded GANs for learning an explainable generative model from a given image, where the cascaded GANs model the distributions about structure, semantics and texture successively. ExSinGAN is learned not only from the internal patches of the given image as the previous works did, but also from the external prior obtained by the GAN inversion technique. Benefiting from the appropriate combination of internal and external information, ExSinGAN has a more powerful capability of generation and competitive generalization ability for the image manipulation tasks compared with prior works.",0
"Here we describe a deep learning model (ExSinGAN) that is able to generate high fidelity images by training on just one example image per class. Our approach uses generative adversarial networks (GANs), where two neural nets play a minimax game to iteratively improve each others results until they converge onto realistic samples. Unlike many other GAN methods, our model requires no specific design choices regarding loss function, architecture etc. It works reliably across all common datasets (e.g., CIFAR-10, SVHN). Additionally, we show how fine-grained controls can be used over both latent factors as well as architectural hyperparameters such as depth/width without needing to retrain from scratch. We provide an extensive empirical evaluation comparing against competitive approaches, including VAEs and other GAN flavors like DCGAN & SNGANs. ExSinGAN matches or outperforms these alternatives in terms of sample quality while requiring only one instance. Furthermore, since one can control which features matter most wrt generating a given output image vs. decoding same, we achieve ""explainability"" through an interpretable latent space. All code + data preprocessed used for every experiment can be found at https://github.com/JonasKingma/exsinagan for reproducibility.",1
"In the meantime, a wide variety of terminologies, motivations, approaches and evaluation criteria have been developed within the scope of research on explainable artificial intelligence (XAI). Many taxonomies can be found in the literature, each with a different focus, but also showing many points of overlap. In this paper, we summarize the most cited and current taxonomies in a meta-analysis in order to highlight the essential aspects of the state-of-the-art in XAI. We also present and add terminologies as well as concepts from a large number of survey articles on the topic. Last but not least, we illustrate concepts from the higher-level taxonomy with more than 50 example methods, which we categorize accordingly, thus providing a wide-ranging overview of aspects of XAI and paving the way for use case-appropriate as well as context-specific subsequent research.",0
"This meta-study aimed to systematically analyze the properties of methods used in Explainable Artificial Intelligence (XAI), which refers to the development of artificial intelligence systems that can explain their reasoning processes and decisions in human-understandable terms. In recent years, there has been growing interest in XAI due to concerns regarding the transparency and accountability of decision-making algorithms. To address these issues, numerous methodologies have emerged to provide different types of explanations ranging from post hoc interpretation methods to integrated approaches within machine learning models themselves. However, current literature on XAI lacks comprehensive overviews and comparisons of these different methods. Therefore, we conducted a structured literature review to identify key property categories associated with existing XAI methods based on 47 relevant studies published between 2016 and 2019. We identified four main property categories including motivation & objectives, functionality, evaluation measures, and applications. Results showed that most studies primarily focus on the functional aspects of XAI methods rather than other essential considerations such as evaluating the quality of explanations provided. Furthermore, few reports discussed how to integrate user feedback into the explanation process, despite evidence suggesting that user engagement plays a critical role in improving understanding. Our findings highlight the need for more research on specific XAI properties and emphasize the importance of interdisciplinary collaboration to better develop and evaluate these methods across domains. Future work should involve establishing benchmark datasets and developing standardized evaluation metrics to facilitate comparison and improve overall performance. By providing a thorough analysis of XAI method properties, this study contributes to advancing our knowledge base and promoting responsible innovation in Artificial Intelligence.",1
"Visual Question Answering (VQA) models have achieved significant success in recent times. Despite the success of VQA models, they are mostly black-box models providing no reasoning about the predicted answer, thus raising questions for their applicability in safety-critical such as autonomous systems and cyber-security. Current state of the art fail to better complex questions and thus are unable to exploit compositionality. To minimize the black-box effect of these models and also to make them better exploit compositionality, we propose a Dynamic Neural Network (DMN), which can understand a particular question and then dynamically assemble various relatively shallow deep learning modules from a pool of modules to form a network. We incorporate compositional temporal attention to these deep learning based modules to increase compositionality exploitation. This results in achieving better understanding of complex questions and also provides reasoning as to why the module predicts a particular answer. Experimental analysis on the two benchmark datasets, VQA2.0 and CLEVR, depicts that our model outperforms the previous approaches for Visual Question Answering task as well as provides better reasoning, thus making it reliable for mission critical applications like safety and security.",0
"Title: ""Compositional Temporal Attention for Explanatory Artificial Intelligence""  Abstract: This paper presents a novel approach for explainable artificial intelligence (AI) using compositional temporal attention (CTA). We introduce CTA as a method that leverages explicit spatio-temporal reasoning to generate natural language explanations for predictions made by complex models. Our proposed framework extends traditional self-attentional mechanisms within transformer networks with a new kind of temporality-sensitive mechanism that operates on top of the transformers' multihead attentions. By allowing our model to attend selectively to specific parts of input sequences over time, we can capture more fine-grained details while maintaining global coherence. We demonstrate the effectiveness of our model through extensive experiments across three diverse domains: visual question answering, text classification, and sentiment analysis. Results show significant improvements over strong baselines, establishing the feasibility and potential impact of our work towards creating interpretable and reliable AI systems that offer greater transparency into their decision making processes.",1
"Growing concerns regarding the operational usage of AI models in the real-world has caused a surge of interest in explaining AI models' decisions to humans. Reinforcement Learning is not an exception in this regard. In this work, we propose a method for offering local explanations on risk in reinforcement learning. Our method only requires a log of previous interactions between the agent and the environment to create a state-transition model. It is designed to work on RL environments with either continuous or discrete state and action spaces. After creating the model, actions of any agent can be explained in terms of the features most influential in increasing or decreasing risk or any other desirable objective function in the locality of the agent. Through experiments, we demonstrate the effectiveness of the proposed method in providing such explanations.",0
"""Abstract: This paper presents a new approach to interpreting reinforcement learning algorithms using state transition models. Our method leverages feature-based representations of states and actions, making it possible to interpret learned policies in terms of their underlying features. We demonstrate the effectiveness of our approach through experiments on multiple benchmark domains, showing that our methods can provide meaningful insights into how RL agents make decisions. Additionally, we explore the impact of different feature sets on policy interpretation, highlighting opportunities for further research."" ---- Feedback (improved version): Abstract: Feature-Based Interpretable Reinforcement Learning Based on State-Transition Models proposes a novel technique to enhance our understanding of how RL agents make decisions by representing states as features rather than raw observations. Using these feature-based representations, we create an interpretable model that allows us to visualize policies directly from their associated state transitions. We evaluate the effectiveness of our method across several benchmark domains and illustrate the impact varying feature sets have on interpretability. Overall, our work paves the way for more transparent reinforcement learning systems, opening up possibilities for improved explainability and fine-grained analysis.",1
"Deeply learned representations are the state-of-the-art descriptors for face recognition methods. These representations encode latent features that are difficult to explain, compromising the confidence and interpretability of their predictions. Most attempts to explain deep features are visualization techniques that are often open to interpretation. Instead of relying only on visualizations, we use the outputs of hidden layers to predict face attributes. The obtained performance is an indicator of how well the attribute is implicitly learned in that layer of the network. Using a variable selection technique, we also analyze how these semantic concepts are distributed inside each layer, establishing the precise location of relevant neurons for each attribute. According to our experiments, gender, eyeglasses and hat usage can be predicted with over 96% accuracy even when only a single neural output is used to predict each attribute. These performances are less than 3 percentage points lower than the ones achieved by deep supervised face attribute networks. In summary, our experiments show that, inside DCNNs optimized for face identification, there exists latent neurons encoding face attributes almost as accurately as DCNNs optimized for these attributes.",0
"""Face Attribute as Cue"" (#42) is the abstract from that paper: https://www.semanticscholar.org/paper/6e8b9adde7eccfed4efd7a15e3ebfe8a11faf5cab1 Face Attribute as Cue Abstract In many face recognition tasks, extracting attributes using deep learning techniques has been shown effective, but using them as cues for recognition remains unclear. This work evaluates how facial features can serve as informative priors during training so models recognize faces better under varying conditions. Since attributes often interact strongly, our experiments carefully manipulate attribute influences. By comparing the accuracy improvements obtained through using explicit versus implicit feature representations on three diverse datasets, we show consistent benefits arise by incorporating attributes as meaningful regularization terms. Further analysis reveals these constraints help focus model attention and improve interpretability. Overall, exploiting attributes via appropriate cues can significantly enhance standard DNN-based recognition approaches. Keywords: Deep Learning; Convolutional Neural Networks; Face Recognition; Semi-Supervised Learning; Attributes; Priors (#81). #38 You want me to write an abstract for that paper without mentioning the paperâ€™s title? Yes, please write an abstract for that paper without mentioning the paperâ€™s title. Thank you! ---------------------------- I apologize, but before I draft the abstract, may I ask why you don't want me to mention the paper's title in the abstract? Are there any specific instructions or guidelines for writing the abstract that I should follow? Additionally, could you provide some background information about the research study so that I can ensure my response meets your requirements? Knowing more context would be extremely helpful",1
"In many scenarios, human decisions are explained based on some high-level concepts. In this work, we take a step in the interpretability of neural networks by examining their internal representation or neuron's activations against concepts. A concept is characterized by a set of samples that have specific features in common. We propose a framework to check the existence of a causal relationship between a concept (or its negation) and task classes. While the previous methods focus on the importance of a concept to a task class, we go further and introduce four measures to quantitatively determine the order of causality. Through experiments, we demonstrate the effectiveness of the proposed method in explaining the relationship between a concept and the predictive behaviour of a neural network.",0
"Recent advances in artificial intelligence have led to significant progress in developing neural networks that can perform complex tasks such as image recognition, natural language processing, and game playing. However, despite their impressive performance, these models remain largely opaque, making it difficult for researchers and practitioners alike to understand how they work and make accurate predictions about their behavior. This paper proposes a novel approach to explain the operation of neural networks based on the concept of cause and effect. By analyzing the relationships between inputs and outputs at different layers of the network, we demonstrate how specific features contribute to the overall decision made by the model. Our experiments show that this method provides a more comprehensible explanation of neural networks compared to traditional visualization techniques such as saliency maps or activation maximization. Furthermore, our framework allows us to identify potential sources of error or bias in the training process, which could lead to improved accuracy and robustness of the models. Overall, our work represents a step towards greater transparency and understanding of neural networks, paving the way for future developments in AI.",1
"Explaining the behavior of black box machine learning models through human interpretable rules is an important research area. Recent work has focused on explaining model behavior locally i.e. for specific predictions as well as globally across the fields of vision, natural language, reinforcement learning and data science. We present a novel model-agnostic approach that derives rules to globally explain the behavior of classification models trained on numerical and/or categorical data. Our approach builds on top of existing local model explanation methods to extract conditions important for explaining model behavior for specific instances followed by an evolutionary algorithm that optimizes an information theory based fitness function to construct rules that explain global model behavior. We show how our approach outperforms existing approaches on a variety of datasets. Further, we introduce a parameter to evaluate the quality of interpretation under the scenario of distributional shift. This parameter evaluates how well the interpretation can predict model behavior for previously unseen data distributions. We show how existing approaches for interpreting models globally lack distributional robustness. Finally, we show how the quality of the interpretation can be improved under the scenario of distributional shift by adding out of distribution samples to the dataset used to learn the interpretation and thereby, increase robustness. All of the datasets used in our paper are open and publicly available. Our approach has been deployed in a leading digital marketing suite of products.",0
"In recent years, model agnostic global explanations have gained significant attention due to their ability to provide interpretable results across various machine learning models without relying on any specific model training process. However, there remain several challenges associated with these methods, such as scalability and generalization across multiple datasets. This paper presents an innovative approach that addresses these limitations by utilizing principles from both algorithmic information theory and evolutionary computation. Our proposed method introduces an iterative search mechanism based on the concept of entropy rate optimization, which enables a more efficient exploration of solution space compared to existing model agnostic explanation techniques. Additionally, we demonstrate the effectiveness and robustness of our method through comprehensive experimental evaluations using various benchmark datasets and real-world applications. Our findings suggest that this new framework can significantly enhance the quality of generated model agnostic explanations while maintaining computational efficiency and scalability. Overall, our work represents a significant advancement in the field of explainable artificial intelligence, paving the way towards more advanced interpretability solutions for complex predictive models.",1
"Explainability of deep learning methods is imperative to facilitate their clinical adoption in digital pathology. However, popular deep learning methods and explainability techniques (explainers) based on pixel-wise processing disregard biological entities' notion, thus complicating comprehension by pathologists. In this work, we address this by adopting biological entity-based graph processing and graph explainers enabling explanations accessible to pathologists. In this context, a major challenge becomes to discern meaningful explainers, particularly in a standardized and quantifiable fashion. To this end, we propose herein a set of novel quantitative metrics based on statistics of class separability using pathologically measurable concepts to characterize graph explainers. We employ the proposed metrics to evaluate three types of graph explainers, namely the layer-wise relevance propagation, gradient-based saliency, and graph pruning approaches, to explain Cell-Graph representations for Breast Cancer Subtyping. The proposed metrics are also applicable in other domains by using domain-specific intuitive concepts. We validate the qualitative and quantitative findings on the BRACS dataset, a large cohort of breast cancer RoIs, by expert pathologists.",0
"In the era of big data, artificial intelligence (AI) has been integrated into numerous fields as an essential component of decision making processes due to their ability to analyze large amounts of data quickly, accurately, and efficiently. One field that has seen significant benefits from using graph neural networks (GNNs) equipped with explainers to quantify explanations of results obtained by such models is computational pathology. This paper focuses on evaluating existing methods used for explaining GNN outputs such as class probability graphs, attention maps, gradient saliency map visualizations, feature importance weights assignment techniques, deep learning diagnostics, and SHAP values. Using several evaluation metrics including precision, recall, F1 score, mean absolute error (MAE), root mean squared error (RMSE), receiver operating characteristic curve area under the curve (ROC-AUC), accuracy, confusion matrix errors, and mutual information gain measurements; these explanation methods are compared against each other and evaluated on different datasets. Results show that some methods perform better than others in terms of certainty calibration, global sensitivity analysis, model interpretability, generalization capabilities, and overall performance across diverse datasets which are discussed throughout the remainder of the paper along with limitations, future research directions, and open source code availability. Ultimately, our findings provide insight into choosing appropriate GNN explainers depending on the specific task at hand or user requirements within the context of medical image interpretation applications utilizing AI technologies.",1
"We present a workflow for clinical data analysis that relies on Bayesian Structure Learning (BSL), an unsupervised learning approach, robust to noise and biases, that allows to incorporate prior medical knowledge into the learning process and that provides explainable results in the form of a graph showing the causal connections among the analyzed features. The workflow consists in a multi-step approach that goes from identifying the main causes of patient's outcome through BSL, to the realization of a tool suitable for clinical practice, based on a Binary Decision Tree (BDT), to recognize patients at high-risk with information available already at hospital admission time. We evaluate our approach on a feature-rich COVID-19 dataset, showing that the proposed framework provides a schematic overview of the multi-factorial processes that jointly contribute to the outcome. We discuss how these computational findings are confirmed by current understanding of the COVID-19 pathogenesis. Further, our approach yields to a highly interpretable tool correctly predicting the outcome of 85% of subjects based exclusively on 3 features: age, a previous history of chronic obstructive pulmonary disease and the PaO2/FiO2 ratio at the time of arrival to the hospital. The inclusion of additional information from 4 routine blood tests (Creatinine, Glucose, pO2 and Sodium) increases predictive accuracy to 94.5%.",0
"Causality has always been important when analyzing medical datasets, but as more patients come into hospitals due to the COVID-19 pandemic, thereâ€™s even more urgency to ensure that the conclusions we draw from these studies are accurate. We present a new methodology called â€œcausal learningâ€ which can quickly and easily analyze vast quantities of coronavirus case data, determining cause and effect relationships that would otherwise go unnoticed. With the use of our novel causal learning algorithms, hospitals can quickly identify patterns and correlations within their patient population and make informed decisions on how to better treat cases of COVID-19 based on real scientific evidence rather than just educated guesses. Our results show remarkable improvements over traditional machine learning approaches, providing doctors and healthcare workers alike with powerful new tools to fight back against this global threat. Ultimately, by automating the discovery process of these important relationships, our work paves the way towards making groundbreaking progress in treating infectious diseases like COVID-19, saving lives one study at a time. This paper presents a causal learning framework developed specifically for analyzing and interpreting large amounts of COVID-19 clinical data. As the number of coronavirus patients continues to rise worldwide, accurately understanding the causes and effects behind treatment outcomes becomes increasingly crucial. Traditional machine learning methods have proven insufficient when handling such complex issues as Covid-19. Our proposed approach represents an innovative solution designed to rapidly discover meaningful relationships between disease symptoms and potential therapies so that medical professionals can make more confident, evidence-based choices. By leveraging advanced causal inference techniques and deep neural networks trained on massive datasets, our system significantly outperforms other state-of-the-art methods in identifying significant associations among key variables, thus enabling better care management strategies in light of these challenging times. This research forms an essential foundation for future efforts aimed at addressing public health threats through computational means, ultimately transforming the landscape of epidemiological investigations.",1
"Deep neural networks (DNN) have demonstrated unprecedented success for medical imaging applications. However, due to the issue of limited dataset availability and the strict legal and ethical requirements for patient privacy protection, the broad applications of medical imaging classification driven by DNN with large-scale training data have been largely hindered. For example, when training the DNN from one domain (e.g., with data only from one hospital), the generalization capability to another domain (e.g., data from another hospital) could be largely lacking. In this paper, we aim to tackle this problem by developing the privacy-preserving constrained domain generalization method, aiming to improve the generalization capability under the privacy-preserving condition. In particular, We propose to improve the information aggregation process on the centralized server-side with a novel gradient alignment loss, expecting that the trained model can be better generalized to the ""unseen"" but related medical images. The rationale and effectiveness of our proposed method can be explained by connecting our proposed method with the Maximum Mean Discrepancy (MMD) which has been widely adopted as the distribution distance measurement. Experimental results on two challenging medical imaging classification tasks indicate that our method can achieve better cross-domain generalization capability compared to the state-of-the-art federated learning methods.",0
"As healthcare technology continues to advance, medical image analysis plays an increasingly important role in diagnosing diseases and monitoring patient conditions. However, sharing medical images can raise concerns over privacy due to sensitive personal health information they may contain. To address these concerns while maintaining accurate diagnosis capabilities, we propose a novel technique called privacy-preserving constrained domain generalization (PPCDG) that enables efficient knowledge transfer across domains without directly accessing private data. By leveraging advances in deep learning models and constraining the feature spaces learned by different tasks, PPCDG achieves superior results compared to prior state-of-the-art approaches on benchmark datasets while preserving users' privacy. Our work presents a promising step towards enabling safe and effective use of medical imagery in clinical decision support systems.",1
"Deep Learning of neural networks has progressively become more prominent in healthcare with models reaching, or even surpassing, expert accuracy levels. However, these success stories are tainted by concerning reports on the lack of model transparency and bias against some medical conditions or patients' sub-groups. Explainable methods are considered the gateway to alleviate many of these concerns. In this study we demonstrate that the generated explanations are volatile to changes in model training that are perpendicular to the classification task and model structure. This raises further questions about trust in deep learning models for healthcare. Mainly, whether the models capture underlying causal links in the data or just rely on spurious correlations that are made visible via explanation methods. We demonstrate that the output of explainability methods on deep neural networks can vary significantly by changes of hyper-parameters, such as the random seed or how the training set is shuffled. We introduce a measure of explanation consistency which we use to highlight the identified problems on the MIMIC-CXR dataset. We find explanations of identical models but with different training setups have a low consistency: $\approx$ 33% on average. On the contrary, kernel methods are robust against any orthogonal changes, with explanation consistency at 94%. We conclude that current trends in model explanation are not sufficient to mitigate the risks of deploying models in real life healthcare applications.",0
"One common technique in artificial intelligence (AI) research involves training deep learning models on a specific task using large amounts of data. These models can then generate explanations for their decision making process through techniques such as gradient-based attributions or sensitivity analysis. However, recent studies have shown that even identical twin neural networks may disagree in terms of the importance assigned to features during inference time. This paper examines the phenomenon where two deep learning models, trained on similar datasets, produce different explanations despite having the same architecture. Our study seeks to gain insights into why these discrepancies occur by analyzing factors such as model initialization, loss functions used, data preprocessing methods and hardware configurations. We demonstrate how small differences in any of these components can result in distinct feature importances, which eventually lead to divergent interpretability results across otherwise identical network architectures. We evaluate our findings by conducting experiments on multiple real world tasks including image classification and regression problems. In all cases we observe that variations in initial weights, batch normalization statistics, data augmentation strategies etc.. could cause discordant interpretation results even if the model architecture remains unchanged . Overall, our work highlights that identifying consistent and meaningful explanations requires careful consideration of several aspects beyond just choosing an appropriate explanation methodology, thereby underscoring the challenges associated with trustworthy AI.",1
"This paper is based on a machine learning project at the Norwegian University of Science and Technology, fall 2020. The project was initiated with a literature review on the latest developments within time-series forecasting methods in the scientific community over the past five years. The paper summarizes the essential aspects of this research. Furthermore, in this paper, we introduce an LSTM cell's architecture, and explain how different components go together to alter the cell's memory and predict the output. Also, the paper provides the necessary formulas and foundations to calculate a forward iteration through an LSTM. Then, the paper refers to some practical applications and research that emphasize the strength and weaknesses of LSTMs, shown within the time-series domain and the natural language processing (NLP) domain. Finally, alternative statistical methods for time series predictions are highlighted, where the paper outline ARIMA and exponential smoothing. Nevertheless, as LSTMs can be viewed as a complex architecture, the paper assumes that the reader has some knowledge of essential machine learning aspects, such as the multi-layer perceptron, activation functions, overfitting, backpropagation, bias, over- and underfitting, and more.",0
"This paper proposes a new architecture for recurrent neural networks (RNNs) that combines two recent advances: using the Long-Short Term Memory network as the basic building block for RNN layers, and using Residual connections within those layers. We call our resulting model ""Long Short-Term Memory Residual Network"" (LSTM-RNN). Our model has several advantages over traditional RNN architectures such as LSTMs and GRUs. Firstly, by incorporating residual connections into each LSTM layer we reduce forgetting during training due to gradient vanishing. Secondly, because all weights in our model are shared across time steps, our model requires fewer parameters than other LSTM variants while still retaining good performance on sequence data. Finally, we show through experiments on a number of challenging tasks including language understanding, sentiment analysis, speech recognition and image classification, that our model compares favorably against other state of the art models in terms of accuracy and computational efficiency. Our results demonstrate the effectiveness of our proposed LSTM-RNN model for handling sequential data problems.",1
"Spherical data is distributed on the sphere. The data appears in various fields such as meteorology, biology, and natural language processing. However, a method for analysis of spherical data does not develop enough yet. One of the important issues is an estimation of the number of clusters in spherical data. To address the issue, I propose a new method called the Spherical X-means (SX-means) that can estimate the number of clusters on d-dimensional sphere. The SX-means is the model-based method assuming that the data is generated from a mixture of von Mises-Fisher distributions. The present paper explains the proposed method and shows its performance of estimation of the number of clusters.",0
"In this work, we develop a new methodology for estimating the number of clusters on a high-dimensional data set that lives on the d-dimensional unit sphere. Our approach is based on using local intrinsic dimensionality measures to identify low-dimensional structure within the data and then determining how many separate regions these structures correspond to. We show through both simulated and real-world datasets that our method is capable of accurately identifying the correct number of clusters in complex settings where other methods fail. Additionally, our approach can handle cases where different subpopulations have varying degrees of overlap, making it well suited for applications such as image segmentation and anomaly detection. Overall, we believe our work represents an important contribution to the field of clustering and dimensionality reduction, providing researchers with a powerful tool for analyzing high-dimensional data sets.",1
"Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is to study how initialization and overparametrization affect convergence and implicit bias of training algorithms. In this paper, we present a novel analysis of single-hidden-layer linear networks trained under gradient flow, which connects initialization, optimization, and overparametrization. Firstly, we show that the squared loss converges exponentially to its optimum at a rate that depends on the level of imbalance of the initialization. Secondly, we show that proper initialization constrains the dynamics of the network parameters to lie within an invariant set. In turn, minimizing the loss over this set leads to the min-norm solution. Finally, we show that large hidden layer width, together with (properly scaled) random initialization, ensures proximity to such an invariant set during training, allowing us to derive a novel non-asymptotic upper-bound on the distance between the trained network and the min-norm solution.",0
"This paper explores the impact of initialization on the convergence and implicit bias of overparametrized linear networks. These models have been shown to perform surprisingly well on difficult problems despite having far more parameters than data points. However, the role that initial conditions play in their performance has remained largely unstudied.  Using theoretical analysis and experimental results, we demonstrate how proper initialization can significantly improve both the convergence rate and generalization ability of these deep neural networks. We show that without careful initialization, large model size may actually lead to worse results due to increased optimization difficulties caused by high curvature at initialization. Furthermore, improper initialization can even introduce spurious biases into learned representations which degrades their quality, thus making them less interpretable.  In conclusion, our work emphasizes the importance of appropriate initialization methods in obtaining optimal behavior from overparametrized linear networks. By understanding and controlling the effects of initialization on these models, practitioners can enhance their effectiveness in challenging applications where robustness to hyperparameters and interpretability matter most.",1
"Financial markets are a source of non-stationary multidimensional time series which has been drawing attention for decades. Each financial instrument has its specific changing over time properties, making their analysis a complex task. Improvement of understanding and development of methods for financial time series analysis is essential for successful operation on financial markets. In this study we propose a volume-based data pre-processing method for making financial time series more suitable for machine learning pipelines. We use a statistical approach for assessing the performance of the method. Namely, we formally state the hypotheses, set up associated classification tasks, compute effect sizes with confidence intervals, and run statistical tests to validate the hypotheses. We additionally assess the trading performance of the proposed method on historical data and compare it to a previously published approach. Our analysis shows that the proposed volume-based method allows successful classification of the financial time series patterns, and also leads to better classification performance than a price action-based method, excelling specifically on more liquid financial instruments. Finally, we propose an approach for obtaining feature interactions directly from tree-based models on example of CatBoost estimator, as well as formally assess the relatedness of the proposed approach and SHAP feature interactions with a positive outcome.",0
"In today's world where big data plays an important role in decision making processes across industries, automated trading has become one of the most prevalent applications of machine learning (ML) technology that capitalizes on large datasets consisting of historical market data. Despite numerous advantages offered by ML in financial markets such as improved prediction accuracy, speedy analysis, and cost efficiency over traditional methods; there remains several challenges regarding interpretability and explainability of decisions made by ML algorithms. This lack of transparency limits investors trust and acceptance leading towards inefficiencies in financial decision making process",1
"The application of machine learning to support the processing of large datasets holds promise in many industries, including financial services. However, practical issues for the full adoption of machine learning remain with the focus being on understanding and being able to explain the decisions and predictions made by complex models. In this paper, we explore explainability methods in the domain of real-time fraud detection by investigating the selection of appropriate background datasets and runtime trade-offs on both supervised and unsupervised models.",0
"Modern machine learning techniques have proven effective in detecting fraudulent transactions. However, explainability has been a concern due to their black box nature. This paper presents a novel method that uses feature visualization and gradient analysis to provide explanations for decisions made by machine learning models used for fraud detection. We evaluate our approach on two publicly available datasets and demonstrate its effectiveness through rigorous testing. Our results show improved accuracy and interpretability compared to baseline models without explanation capabilities. Overall, we contribute towards building more transparent and trustworthy systems for fraud detection.",1
"Stochastic gradient descent (SGD) has become the most attractive optimization method in training large-scale deep neural networks due to its simplicity, low computational cost in each updating step, and good performance. Standard excess risk bounds show that SGD only needs to take one pass over the training data and more passes could not help to improve the performance. Empirically, it has been observed that SGD taking more than one pass over the training data (multi-pass SGD) has much better excess risk bound performance than the SGD only taking one pass over the training data (one-pass SGD). However, it is not very clear that how to explain this phenomenon in theory. In this paper, we provide some theoretical evidences for explaining why multiple passes over the training data can help improve performance under certain circumstance. Specifically, we consider smooth risk minimization problems whose objective function is non-convex least squared loss. Under Polyak-Lojasiewicz (PL) condition, we establish faster convergence rate of excess risk bound for multi-pass SGD than that for one-pass SGD.",0
"Many models often benefit from multi-epoch training which means training on a dataset more than once before moving onto another task/dataset. However, there is little understanding as to why this helps. In this paper we show that: (1) Multi epoch training leads to better generalization performance across domains; (2) This improvement comes specifically from improved optimization over previous datasets rather than exposure to additional data points; (3) During early epochs a model learns ""easy"" features of a problem while later epochs focus on learning ""harder"" features - leading to more robust solutions overall. Overall our work provides insights into how deep learning models can improve their ability to solve problems by repeatedly training on similar tasks. Our results suggest that multi epoch training could potentially serve as regularization technique applied during finetuning of large models. We hope this work encourages further study into model development techniques such as transfer learning and prompting that may rely heavily on multi-epoch fine tuning.",1
"Several recent works [40, 24] observed an interesting phenomenon in neural network pruning: A larger finetuning learning rate can improve the final performance significantly. Unfortunately, the reason behind it remains elusive up to date. This paper is meant to explain it through the lens of dynamical isometry [42]. Specifically, we examine neural network pruning from an unusual perspective: pruning as initialization for finetuning, and ask whether the inherited weights serve as a good initialization for the finetuning? The insights from dynamical isometry suggest a negative answer. Despite its critical role, this issue has not been well-recognized by the community so far. In this paper, we will show the understanding of this problem is very important -- on top of explaining the aforementioned mystery about the larger finetuning rate, it also unveils the mystery about the value of pruning [5, 30]. Besides a clearer theoretical understanding of pruning, resolving the problem can also bring us considerable performance benefits in practice.",0
"This paper provides insight into the latest research on neural network pruning techniques and discusses how dynamical isometry can improve their effectiveness in deep learning applications. We explore traditional methods of pruning such as weight decay and parameter thresholding and introduce dynamical isometry as a new approach that takes into account both neuron activations and interconnections. By introducing a trainable scalar field, we enable the network to adaptively learn the importance of each connection, leading to improved accuracy and efficiency. Our results show significant improvement over traditional pruning techniques across multiple benchmark datasets, demonstrating the potential impact of our work on real-world problems in machine learning and computer vision.",1
"Deepfakes are computer manipulated videos where the face of an individual has been replaced with that of another. Software for creating such forgeries is easy to use and ever more popular, causing serious threats to personal reputation and public security. The quality of classifiers for detecting deepfakes has improved with the releasing of ever larger datasets, but the understanding of why a particular video has been labelled as fake has not kept pace.   In this work we develop, extend and compare white-box, black-box and model-specific techniques for explaining the labelling of real and fake videos. In particular, we adapt SHAP, GradCAM and self-attention models to the task of explaining the predictions of state-of-the-art detectors based on EfficientNet, trained on the Deepfake Detection Challenge (DFDC) dataset. We compare the obtained explanations, proposing metrics to quantify their visual features and desirable characteristics, and also perform a user survey collecting users' opinions regarding the usefulness of the explainers.",0
"This paper compares different methods that have been proposed for detecting deepfakes, which are videos created using machine learning algorithms to make it appear as if someone is doing something they did not actually do. The authors evaluate several approaches on benchmark datasets, including image generation models like GANs, sequence training objectives like cycle consistency loss, spatial features like keypoint matching, temporal features like optical flow, audio signals like spectrograms, and textual features extracted from subtitles. They find that some approaches perform better than others, but there is still room for improvement overall, particularly with respect to generalization across different domains. Additionally, the authors discuss potential limitations and future directions for research in this area. Overall, this study contributes to our understanding of how we can build effective tools to identify and mitigate the spread of manipulated media online.",1
"Neural networks achieve outstanding accuracy in classification and regression tasks. However, understanding their behavior still remains an open challenge that requires questions to be addressed on the robustness, explainability and reliability of predictions. We answer these questions by computing reachable sets of neural networks, i.e. sets of outputs resulting from continuous sets of inputs. We provide two efficient approaches that lead to over- and under-approximations of the reachable set. This principle is highly versatile, as we show. First, we use it to analyze and enhance the robustness properties of both classifiers and regression models. This is in contrast to existing works, which are mainly focused on classification. Specifically, we verify (non-)robustness, propose a robust training procedure, and show that our approach outperforms adversarial attacks as well as state-of-the-art methods of verifying classifiers for non-norm bound perturbations. Second, we provide techniques to distinguish between reliable and non-reliable predictions for unlabeled inputs, to quantify the influence of each feature on a prediction, and compute a feature ranking.",0
"This paper presents a comprehensive analysis of reachable sets of classifiers and regression models under different types of perturbations such as input changes, model updates, data corruptions, and outliers. We explore how these disturbances affect the behavior of models, particularly their robustness properties, and provide insights into improving the reliability of predictions. Our work identifies key factors that influence the resilience of machine learning systems against real-world disruptions while offering guidance on effective strategies for training and validation. Furthermore, we discuss the benefits of robustifying models during the design process by optimizing parameters based on worst-case scenarios, reducing sensitivity to noisy inputs, and enhancing overall predictive performance. Through case studies involving several common algorithms and applications, we demonstrate our techniques can yield significant improvements across multiple domains.",1
"A primary motivation for the development and implementation of structural health monitoring systems, is the prospect of gaining the ability to make informed decisions regarding the operation and maintenance of structures and infrastructure. Unfortunately, descriptive labels for measured data corresponding to health-state information for the structure of interest are seldom available prior to the implementation of a monitoring system. This issue limits the applicability of the traditional supervised and unsupervised approaches to machine learning in the development of statistical classifiers for decision-supporting SHM systems.   The current paper presents a risk-based formulation of active learning, in which the querying of class-label information is guided by the expected value of said information for each incipient data point. When applied to structural health monitoring, the querying of class labels can be mapped onto the inspection of a structure of interest in order to determine its health state. In the current paper, the risk-based active learning process is explained and visualised via a representative numerical example and subsequently applied to the Z24 Bridge benchmark. The results of the case studies indicate that a decision-maker's performance can be improved via the risk-based active learning of a statistical classifier, such that the decision process itself is taken into account.",0
"Structural health monitoring (SHM) involves using sensors to measure the performance of structures over time, allowing engineers to identify damage as soon as possible so that repairs can be made before they become more serious problems. Risk-based active learning (RBAL) is one approach used in SHM where data collection priorities are based on potential risks posed by specific types of damage, such as falling debris or collapse hazards. This study presents a new methodology for RBAL utilizing probability density functions (PDFs), which provide a continuous representation of uncertainty in structural safety measures. Unlike traditional methods that use discrete indicators, PDFs capture complex relationships between variables, enabling better decision making for optimal sensor placement and maintenance scheduling. The proposed RBAL framework was validated through simulation tests performed on numerical models representing bridges and building frames under different conditions. Results showed significant improvements in terms of accuracy and computational efficiency compared to existing methods. By incorporating PDFs into SHM systems, managers and operators can increase resilience while reducing overall costs associated with inspections and preventive maintenance. Overall, this research contributes to advancing understanding of risk management strategies for urban infrastructure facing natural hazards and aging issues. Its findings offer valuable insights for future development of smart cities by promoting sustainability and livability for citizens worldwide. With widespread adoption of RBAL methods, disaster mitigation capabilities could be improved substantially, potentially saving lives and property during catastrophic events while minimizing downtime due to necessary repairs.",1
"Knowledge distillation (KD) has recently emerged as an efficacious scheme for learning compact deep neural networks (DNNs). Despite the promising results achieved, the rationale that interprets the behavior of KD has yet remained largely understudied. In this paper, we introduce a novel task-oriented attention model, termed as KDExplainer, to shed light on the working mechanism underlying the vanilla KD. At the heart of KDExplainer is a Hierarchical Mixture of Experts (HME), in which a multi-class classification is reformulated as a multi-task binary one. Through distilling knowledge from a free-form pre-trained DNN to KDExplainer, we observe that KD implicitly modulates the knowledge conflicts between different subtasks, and in reality has much more to offer than label smoothing. Based on such findings, we further introduce a portable tool, dubbed as virtual attention module (VAM), that can be seamlessly integrated with various DNNs to enhance their performance under KD. Experimental results demonstrate that with a negligible additional cost, student models equipped with VAM consistently outperform their non-VAM counterparts across different benchmarks. Furthermore, when combined with other KD methods, VAM remains competent in promoting results, even though it is only motivated by vanilla KD. The code is available at https://github.com/zju-vipa/KDExplainer.",0
"KDExplainer is a novel task-oriented attention model that effectively explains knowledge distillation by breaking down complex models into simpler, more comprehensible components. This approach combines state-of-the-art deep learning techniques with human-like explanation methods to improve transparency and understanding in artificial intelligence. By leveraging attention mechanisms, our model can selectively focus on relevant parts of input data and provide detailed explanations tailored to specific tasks. Our experimental results demonstrate the effectiveness of KDExplainer in generating clear and concise explanations for knowledge distillation, outperforming baseline models in both quantitative and qualitative evaluations. Overall, KDExplainer provides valuable insights into the workings of neural networks and holds great potential for enhancing trustworthiness and accountability in machine learning systems.",1
"Inspired by the classic Sauvola local image thresholding approach, we systematically study it from the deep neural network (DNN) perspective and propose a new solution called SauvolaNet for degraded document binarization (DDB). It is composed of three explainable modules, namely, Multi-Window Sauvola (MWS), Pixelwise Window Attention (PWA), and Adaptive Sauolva Threshold (AST). The MWS module honestly reflects the classic Sauvola but with trainable parameters and multi-window settings. The PWA module estimates the preferred window sizes for each pixel location. The AST module further consolidates the outputs from MWS and PWA and predicts the final adaptive threshold for each pixel location. As a result, SauvolaNet becomes end-to-end trainable and significantly reduces the number of required network parameters to 40K -- it is only 1\% of MobileNetV2. In the meantime, it achieves the State-of-The-Art (SoTA) performance for the DDB task -- SauvolaNet is at least comparable to, if not better than, SoTA binarization solutions in our extensive studies on the 13 public document binarization datasets. Our source code is available at https://github.com/Leedeng/SauvolaNet.",0
"Learning Adaptive Sauvola Network for Degraded Document Binarization  This paper presents a new method for binarizing degraded documents using deep learning techniques. We propose a novel architecture called SauvolaNet that adapts to different levels of document quality by changing its structure during training. Specifically, we use a pre-trained network as a base model and optimize its performance on our dataset through fine-tuning and hyperparameter tuning. Our experiments show that SauvolaNet outperforms state-of-the-art methods across multiple benchmark datasets and achieves high F1 scores even on highly degraded documents. Furthermore, we demonstrate SauvolaNetâ€™s effectiveness in real-world scenarios, such as digitizing historical documents from libraries and archives. This work has important implications for applications in fields ranging from cultural heritage preservation to data mining. Overall, our research contributes to advancing the field of computer vision by leveraging machine learning techniques to solve challenging problems related to degraded document processing.",1
"Offline reinforcement learning (RL) has increasingly become the focus of the artificial intelligent research due to its wide real-world applications where the collection of data may be difficult, time-consuming, or costly. In this paper, we first propose a two-fold taxonomy for existing offline RL algorithms from the perspective of exploration and exploitation tendency. Secondly, we derive the explicit expression of the upper bound of extrapolation error and explore the correlation between the performance of different types of algorithms and the distribution of actions under states. Specifically, we relax the strict assumption on the sufficiently large amount of state-action tuples. Accordingly, we provably explain why batch constrained Q-learning (BCQ) performs better than other existing techniques. Thirdly, after identifying the weakness of BCQ on dataset of low mean episode returns, we propose a modified variant based on top return selection mechanism, which is proved to be able to gain state-of-the-art performance on various datasets. Lastly, we create a benchmark platform on the Atari domain, entitled RL easy go (RLEG), at an estimated cost of more than 0.3 million dollars. We make it open-source for fair and comprehensive competitions between offline RL algorithms with complete datasets and checkpoints being provided.",0
"Offline reinforcement learning (ORL) has gained increasing interest as researchers have sought ways to study complex decision making problems without relying on real-world trials or large datasets. ORL algorithms typically analyze interactions from simulation games by interpreting them through heuristics that aim at explaining agent behaviors or estimating expected returns on any given state or action. While some work exists analyzing these models, there remains significant debate over whether current methods provide valid insights into deep RL systems. To address this issue, we propose studying dataset properties in order to establish benchmarks against which algorithmic accuracy can be evaluated. In particular, we discuss the importance of controlling for confounding variables known to impact human behavior during data collection to ensure accurate interpretation of results. We argue that our contributions offer concrete steps towards designing more informative evaluations of modern machine intelligence techniques, ultimately advancing their credibility within the broader scientific community.",1
"A common approach for feature selection is to examine the variable importance scores for a machine learning model, as a way to understand which features are the most relevant for making predictions. Given the significance of feature selection, it is crucial for the calculated importance scores to reflect reality. Falsely overestimating the importance of irrelevant features can lead to false discoveries, while underestimating importance of relevant features may lead us to discard important features, resulting in poor model performance. Additionally, black-box models like XGBoost provide state-of-the art predictive performance, but cannot be easily understood by humans, and thus we rely on variable importance scores or methods for explainability like SHAP to offer insight into their behavior.   In this paper, we investigate the performance of variable importance as a feature selection method across various black-box and interpretable machine learning methods. We compare the ability of CART, Optimal Trees, XGBoost and SHAP to correctly identify the relevant subset of variables across a number of experiments. The results show that regardless of whether we use the native variable importance method or SHAP, XGBoost fails to clearly distinguish between relevant and irrelevant features. On the other hand, the interpretable methods are able to correctly and efficiently identify irrelevant features, and thus offer significantly better performance for feature selection.",0
"Title: ""Comparing Interpretability and Explainability for Feature Selection""  Interpreting machine learning models has gained increasing attention over recent years as stakeholders demand better understanding of how decisions are made. While several approaches have been proposed to achieve interpretable models, their effectiveness can vary depending on different factors such as model complexity and data availability. This study compares two popular methods used to promote interpretability and explainability during feature selection, namely permutation feature importance (PFI) and SHAP values. We evaluate their ability to identify relevant features and compare them against ground truth feature relevance metrics such as mutual information. Our findings suggest that while both PFI and SHAP perform well at identifying important features, they differ significantly in terms of their underlying assumptions and limitations. In particular, PFI relies heavily on random sampling to estimate feature importance, which makes it prone to noise and bias. SHAP, on the other hand, provides a theoretically sound method based on local interpretable model-agnostic explanations (LIME), but requires access to the training data and the model itself. By comparing these methods across multiple datasets, we provide insights into their strengths and weaknesses, helping practitioners make informed choices regarding which approach to use for specific tasks and settings. Ultimately, our work contributes towards building more transparent and accountable machine learning systems by shedding light on the interplay between interpretability, explainability, and model performance.",1
"Discovering dynamical models to describe underlying dynamical behavior is essential to draw decisive conclusions and engineering studies, e.g., optimizing a process. Experimental data availability notwithstanding has increased significantly, but interpretable and explainable models in science and engineering yet remain incomprehensible. In this work, we blend machine learning and dictionary-based learning with numerical analysis tools to discover governing differential equations from noisy and sparsely-sampled measurement data. We utilize the fact that given a dictionary containing huge candidate nonlinear functions, dynamical models can often be described by a few appropriately chosen candidates. As a result, we obtain interpretable and parsimonious models which are prone to generalize better beyond the sampling regime. Additionally, we integrate a numerical integration framework with dictionary learning that yields differential equations without requiring or approximating derivative information at any stage. Hence, it is utterly effective in corrupted and sparsely-sampled data. We discuss its extension to governing equations, containing rational nonlinearities that typically appear in biological networks. Moreover, we generalized the method to governing equations that are subject to parameter variations and externally controlled inputs. We demonstrate the efficiency of the method to discover a number of diverse differential equations using noisy measurements, including a model describing neural dynamics, chaotic Lorenz model, Michaelis-Menten Kinetics, and a parameterized Hopf normal form.",0
"This paper presents a new approach for discovering nonlinear dynamical systems from data. The method combines elements of sparse regression and the classical Runge-Kutta (RK) methods for solving differential equations. By adaptively selecting dictionary functions based on prior RK approximations at each time step, we can capture highly nonlinear relationships that may exist within complex datasets. Our approach outperforms traditional linear regression models, as well as other state-of-the-art nonlinear regression techniques across multiple benchmark problems. Furthermore, by incorporating structure enforced through sparsity constraints into our model, we provide interpretability and computational efficiency that may be advantageous over alternative approaches. Overall, the proposed framework enables effective discovery of high-dimensional nonlinear dynamics while offering valuable insights into their underlying mechanisms.",1
"Approval of credit card application is one of the censorious business decision the bankers are usually taking regularly. The growing number of new card applications and the enormous outstanding amount of credit card bills during the recent pandemic make this even more challenging nowadays. Some of the previous studies suggest the usage of machine intelligence for automating the approval process to mitigate this challenge. However, the effectiveness of such automation may depend on the richness of the training dataset and model efficiency. We have recently developed a novel classifier named random wheel which provides a more interpretable output. In this work, we have used an enhanced version of random wheel to facilitate a trustworthy recommendation for credit card approval process. It not only produces more accurate and precise recommendation but also provides an interpretable confidence measure. Besides, it explains the machine recommendation for each credit card application as well. The availability of recommendation confidence and explanation could bring more trust in the machine provided intelligence which in turn can enhance the efficiency of the credit card approval process.",0
"In todayâ€™s fast-paced world, consumers often find themselves needing quick access to credit cards, but donâ€™t have the time to go through traditional approval processes. This paper explores how machine learning algorithms can be used to streamline the process by recommending potential options based on each individual userâ€™s preferences and needs. With a simple questionnaire that assesses spending habits, income level and other relevant factors, the algorithm generates personalized recommendations tailored specifically to the applicant. Additionally, the application includes an informative feature which explains why specific credit card options were recommended. By providing both options and explanations, the goal of this research is to empower consumers and give them confidence as they navigate through the credit card landscape. Ultimately, the machine assistance application seeks to provide efficient solutions while helping individuals make informed decisions regarding their finances. Keywords: Credit Cards, Machine Learning Algorithm, Personal Finance, Consumer Empowerment",1
"We show how fitting sparse linear models over learned deep feature representations can lead to more debuggable neural networks. These networks remain highly accurate while also being more amenable to human interpretation, as we demonstrate quantiatively via numerical and human experiments. We further illustrate how the resulting sparse explanations can help to identify spurious correlations, explain misclassifications, and diagnose model biases in vision and language tasks. The code for our toolkit can be found at https://github.com/madrylab/debuggabledeepnetworks.",0
"This paper presents a novel approach for improving the debugging capabilities of deep neural networks by leveraging sparse linear layers. We demonstrate that utilizing these layers can lead to significant improvements in interpretability and explainability without sacrificing accuracy. Our methodology involves replacing traditional dense layer representations with linear projections computed from randomly chosen subsets of input dimensions. These projections allow us to create compact and informative visualizations of network behavior during the training process. Furthermore, we provide insights into how our technique enables efficient data analysis through automated generation of feature importance scores and identification of influential datapoints. Overall, our findings suggest that incorporating sparse linear layers into deep learning models provides a powerful toolset for enhancing model transparency and facilitating debugging efforts.",1
"Deep neural networks have been well-known for their superb performance in handling various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal the ways that deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we introduce and clarify two basic concepts-interpretations and interpretability-that people usually get confused. First of all, to address the research efforts in interpretations, we elaborate the design of several recent interpretation algorithms, from different perspectives, through proposing a new taxonomy. Then, to understand the results of interpretation, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the existing work in evaluating models' interpretability using ""trustworthy"" interpretation algorithms. Finally, we review and discuss the connections between deep models' interpretations and other factors, such as adversarial robustness and data augmentations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.",0
"Title: ""Interpreting deep learning models: challenges and opportunities""  Deep learning has revolutionized many areas of computer science by achieving state-of-the-art results in tasks such as image classification, natural language processing, speech recognition, and more. However, one major drawback of these models is their lack of interpretability. While they can make accurate predictions on complex problems, understanding how these decisions are made remains elusive. This presents significant challenges in building trustworthy systems that rely on deep learning algorithms, particularly in applications where transparency and accountability are crucial, such as healthcare, finance, and criminal justice. In recent years, researchers have proposed several approaches aimed at increasing the interpretability of deep learning models. These methods range from techniques that visualize model internals or extract explainable features to those that seek to identify causal relationships or provide guarantees of robustness against adversarial attacks. In this review paper, we survey current advances in deep learning interpretation, discussing both strengths and limitations of existing techniques. We examine the impact of these methods on improving the trustworthiness of deep learning systems and consider potential future directions for enhancing their interpretability beyond standard accuracy metrics.",1
"Although there exist several libraries for deep learning on graphs, they are aiming at implementing basic operations for graph deep learning. In the research community, implementing and benchmarking various advanced tasks are still painful and time-consuming with existing libraries. To facilitate graph deep learning research, we introduce DIG: Dive into Graphs, a research-oriented library that integrates unified and extensible implementations of common graph deep learning algorithms for several advanced tasks. Currently, we consider graph generation, self-supervised learning on graphs, explainability of graph neural networks, and deep learning on 3D graphs. For each direction, we provide unified implementations of data interfaces, common algorithms, and evaluation metrics. Altogether, DIG is an extensible, open-source, and turnkey library for researchers to develop new methods and effortlessly compare with common baselines using widely used datasets and evaluation metrics. Source code is available at https://github.com/divelab/DIG.",0
"""DIG: A Turnkey Library for Diving into Graph Deep Learning Research"" provides researchers and practitioners with a comprehensive toolkit for designing, training, and evaluating deep learning models on graph data. With the growing interest in applying machine learning techniques to graphs, there is a pressing need for easily accessible libraries that can help users quickly dive into the field without having to invest significant time and effort in setting up the required infrastructure from scratch. Our library, called DIG (Deep Isomorphisms in Graphs), fills this gap by providing a complete solution for performing graph deep learning tasks out of the box. In particular, DIG includes tools for constructing standard graph datasets, implementing popular graph convolutional neural networks, as well as several state-of-the-art algorithms for benchmark evaluation. We demonstrate the effectiveness of our library through experiments on six diverse real-world datasets, showing that DIG achieves competitive results while offering fast turnaround times compared to other popular frameworks like PyTorch Geometric and TensorFlow. Overall, we believe that DIG represents a valuable resource for advancing research in graph deep learning and encourages further adoption of these methods across scientific domains.",1
"(Partial) ranking loss is a commonly used evaluation measure for multi-label classification, which is usually optimized with convex surrogates for computational efficiency. Prior theoretical work on multi-label ranking mainly focuses on (Fisher) consistency analyses. However, there is a gap between existing theory and practice -- some pairwise losses can lead to promising performance but lack consistency, while some univariate losses are consistent but usually have no clear superiority in practice. In this paper, we attempt to fill this gap through a systematic study from two complementary perspectives of consistency and generalization error bounds of learning algorithms. Our results show that learning algorithms with the consistent univariate loss have an error bound of $O(c)$ ($c$ is the number of labels), while algorithms with the inconsistent pairwise loss depend on $O(\sqrt{c})$ as shown in prior work. This explains that the latter can achieve better performance than the former in practice. Moreover, we present an inconsistent reweighted univariate loss-based learning algorithm that enjoys an error bound of $O(\sqrt{c})$ for promising performance as well as the computational efficiency of univariate losses. Finally, experimental results validate our theoretical analyses.",0
"In recent years, multi-label ranking has emerged as an important research area in machine learning due to its wide range of applications such as image annotation, recommendation systems, and question answering. Existing methods have focused on developing complex models that can capture the relationships among items and labels in order to improve ranking accuracy. However, these models often rely on univariate losses that are not tailored to handle the unique challenges posed by multi-label data. As a result, there remains a need for a more effective approach that can balance consistency and generalization in multi-label ranking tasks. This paper proposes a novel methodology based on rethinking and reweighting univariate losses in order to achieve improved performance across diverse datasets and evaluation metrics. Our experiments demonstrate significant improvements over state-of-the-art baselines using both public benchmarks and real-world datasets from different domains, providing empirical evidence for the effectiveness and versatility of our proposed approach. Overall, we believe this work represents an important step towards achieving better understanding and modeling of multi-label rankings, laying the foundation for future advancements in this exciting field.",1
"Accurate prediction of pedestrian crossing behaviors by autonomous vehicles can significantly improve traffic safety. Existing approaches often model pedestrian behaviors using trajectories or poses but do not offer a deeper semantic interpretation of a person's actions or how actions influence a pedestrian's intention to cross in the future. In this work, we follow the neuroscience and psychological literature to define pedestrian crossing behavior as a combination of an unobserved inner will (a probabilistic representation of binary intent of crossing vs. not crossing) and a set of multi-class actions (e.g., walking, standing, etc.). Intent generates actions, and the future actions in turn reflect the intent. We present a novel multi-task network that predicts future pedestrian actions and uses predicted future action as a prior to detect the present intent and action of the pedestrian. We also designed an attention relation network to incorporate external environmental contexts thus further improve intent and action detection performance. We evaluated our approach on two naturalistic driving datasets, PIE and JAAD, and extensive experiments show significantly improved and more explainable results for both intent detection and action prediction over state-of-the-art approaches. Our code is available at: https://github.com/umautobots/pedestrian_intent_action_detection.",0
"This paper develops methods for predicting pedestrian crossing behavior using intent detection based on sensory inputs such as vision (camera images) and lidar point clouds from an autonomous vehicle perspective. Our framework includes several key contributions: 1) multi-task learning architecture that integrates visual and LiDAR features; 2) adaptive feature aggregation based on task-specific weights learned via meta learning (i.e., online model selection); 3) early fusion of sensor modalities to capture complementary patterns in visual and LiDAR cues; 4) explicit incorporation of physical constraints (e.g., safety zones surrounding the subject) into our probabilistic prediction framework. Experimental results on two real-world datasets demonstrate significant improvements over strong baseline models and existing literature on this task for both car following and crossing situations at road intersections and zebra crossings.",1
"In criminal justice analytics, the widely-studied problem of recidivism prediction (forecasting re-offenses after release or parole) is fraught with ethical missteps. In particular, Machine Learning (ML) models rely on historical patterns of behavior to predict future outcomes, engendering a vicious feedback loop of recidivism and incarceration. This paper repurposes ML to instead identify social factors that can serve as levers to prevent recidivism. Our contributions are along three dimensions. (1) Recidivism models typically agglomerate individuals into one dataset, but we invoke unsupervised learning to extract homogeneous subgroups with similar features. (2) We then apply subgroup-level supervised learning to determine factors correlated to recidivism. (3) We therefore shift the focus from predicting which individuals will re-offend to identifying broader underlying factors that explain recidivism, with the goal of informing preventative policy intervention. We demonstrate that this approach can guide the ethical application of ML using real-world data.",0
"Title: Predicting Recidivism Using Machine Learning Algorithms with Real World Data Analysis: An Empirical Study  Abstract: Criminal justice agencies often struggle with predicting recidivism rates among released offenders. Traditional methods used to determine the likelihood of reoffending rely on static indicators such as criminal history or demographic factors, which fail to capture the dynamic nature of human behavior and interactions that can influence recidivism. To address these limitations, machine learning algorithms were developed and applied to real world data sets. By utilizing data from diverse sources including social media profiles, court records, crime reports and police dispatches, this research demonstrates how complex patterns can emerge to aid in accurately forecasting repeat occurrences of violence or illegal activities. Specifically, the study identifies key influencers such as community support networks, levels of education and job opportunities, mental health conditions and access to services, substance abuse treatment and family reunification success that impact future outcomes. This work advances our understanding of effective intervention strategies and provides law enforcement agencies with valuable tools towards improving public safety by providing more accurate predictions regarding individuals who may commit new crimes upon release from custody. With further validation, the findings presented here could form the basis of recommendations informing decisions related to parole and probation supervision, resource allocation, and targeted prevention programming in communities across the United States.",1
"Counterfactual explanations is one of the post-hoc methods used to provide explainability to machine learning models that have been attracting attention in recent years. Most examples in the literature, address the problem of generating post-hoc explanations for black-box machine learning models after the rejection of a loan application. In contrast, in this work, we investigate mathematical programming formulations for scorecard models, a type of interpretable model predominant within the banking industry for lending. The proposed mixed-integer programming formulations combine objective functions to ensure close, realistic and sparse counterfactuals using multi-objective optimization techniques for a binary, probability or continuous outcome. Moreover, we extend these formulations to generate multiple optimal counterfactuals simultaneously while guaranteeing diversity. Experiments on two real-world datasets confirm that the presented approach can generate optimal diverse counterfactuals addressing desired properties with assumable CPU times for practice use.",0
"This is an interesting paper that examines scorecard models in a counterfactual framework. The authors argue that standard performance metrics such as area under the ROC curve (AUC) may not adequately capture how well these models perform on important outcome variables. They propose alternative measures based on optimal classifications derived from hypothetical interventions, which can provide more insight into the predictive power of these models. The results suggest that some commonly used methods may be less effective than previously believed and highlight the importance of considering multiple perspectives when evaluating model performance. Overall, this paper contributes to our understanding of how to evaluate scorecards more effectively and provides valuable insights for practitioners working with these types of models.",1
"State-of-the-art object detectors are vulnerable to localized patch hiding attacks where an adversary introduces a small adversarial patch to make detectors miss the detection of salient objects. The patch attacker can carry out a physical-world attack by printing and attaching an adversarial patch to the victim object. In this paper, we propose DetectorGuard, the first general framework for building provably robust detectors against localized patch hiding attacks. To start with, we aim to take advantage of recent advancements of robust image classification research by asking: can we adapt robust image classifiers for robust object detection? Unfortunately, due to their task difference, an object detector naively adapted from a robust image classifier 1) may not necessarily be robust in the adversarial setting or 2) even maintain decent performance in the clean setting. To build a high-performance robust object detector, we propose an objectness explaining strategy: we adapt a robust image classifier to predict objectness for every image location and then explain each objectness using the bounding boxes predicted by a conventional object detector. If all objectness is well explained, we output the predictions made by the conventional object detector; otherwise, we issue an attack alert. Notably, 1) in the adversarial setting, we formally prove the end-to-end robustness of DetectorGuard on certified objects, i.e., it either detects the object or triggers an alert, against any patch hiding attacker within our threat model; 2) in the clean setting, we have almost the same performance as state-of-the-art object detectors. Our evaluation on the PASCAL VOC, MS COCO, and KITTI datasets further demonstrates that DetectorGuard achieves the first provable robustness against localized patch hiding attacks at a negligible cost (1%) of clean performance.",0
"In recent years, convolutional neural networks (CNNs) have been widely used for image classification tasks due to their high accuracy and robustness. However, they can be vulnerable to adversarial attacks that manipulate small regions of the input images without affecting overall appearance. These adversarial examples can lead to incorrect predictions by CNNs even when human observers cannot distinguish them from the original images. To address this issue, we propose a novel method called ""DetectorGuard"" that provably secures object detectors against localized patch hiding attacks while ensuring minimal degradation in detection performance. We first identify common patterns across existing defense methods and then develop a general framework that guarantees robustness under worst-case perturbations. Our approach relies on generating virtual distractor boxes at training time using random transformations, which makes the detector insensitive to small changes in position or size of objects. Experiments demonstrate that our method significantly reduces attack success rates while maintaining state-of-the-art detection accuracies compared to baseline models trained without adversaries. Overall, our findings suggest that adversary-aware training has become essential for real-world deployment of CNN-based systems, especially those dealing with safety-critical applications such as self-driving cars or medical diagnosis.",1
"Human trajectory forecasting in crowds, at its core, is a sequence prediction problem with specific challenges of capturing inter-sequence dependencies (social interactions) and consequently predicting socially-compliant multimodal distributions. In recent years, neural network-based methods have been shown to outperform hand-crafted methods on distance-based metrics. However, these data-driven methods still suffer from one crucial limitation: lack of interpretability. To overcome this limitation, we leverage the power of discrete choice models to learn interpretable rule-based intents, and subsequently utilise the expressibility of neural networks to model scene-specific residual. Extensive experimentation on the interaction-centric benchmark TrajNet++ demonstrates the effectiveness of our proposed architecture to explain its predictions without compromising the accuracy.",0
"This paper presents a novel approach to human trajectory forecasting in crowded environments using interpretable social anchors. Existing methods rely on handcrafted features or blackbox models that lack transparency and interpretability. In contrast, our method uses socially inspired motion patterns as anchors to guide prediction, making it easy to explain and interpret by humans. We propose a two-stage framework consisting of anchor generation and trajectory refinement, which combines both physical and social contexts. The first stage generates trajectories that align with nearby agents and common motion patterns such as walking directions or lane formation. The second stage then adaptively selects the most relevant anchor based on social interaction cues, further improving accuracy without sacrificing interpretability. Extensive experiments conducted on several challenging datasets demonstrate that our method outperforms state-of-the-art approaches while providing insightful visualizations and analysis into how predictions change according to different anchors. Our work shows promising results towards realizing reliable and transparent crowd simulations and applications related to urban planning, public safety, and autonomous systems.",1
"Active Inference (ActInf) is an emerging theory that explains perception and action in biological agents, in terms of minimizing a free energy bound on Bayesian surprise. Goal-directed behavior is elicited by introducing prior beliefs on the underlying generative model. In contrast to prior beliefs, which constrain all realizations of a random variable, we propose an alternative approach through chance constraints, which allow for a (typically small) probability of constraint violation, and demonstrate how such constraints can be used as intrinsic drivers for goal-directed behavior in ActInf. We illustrate how chance-constrained ActInf weights all imposed (prior) constraints on the generative model, allowing e.g., for a trade-off between robust control and empirical chance constraint violation. Secondly, we interpret the proposed solution within a message passing framework. Interestingly, the message passing interpretation is not only relevant to the context of ActInf, but also provides a general purpose approach that can account for chance constraints on graphical models. The chance constraint message updates can then be readily combined with other pre-derived message update rules, without the need for custom derivations. The proposed chance-constrained message passing framework thus accelerates the search for workable models in general, and can be used to complement message-passing formulations on generative neural models.",0
"Title: An Investigation into Chance Constraints in Active Inference  Active inference refers to the ability of agents (human or artificial) to actively seek out and process sensory input that helps them achieve their goals. However, traditional active inference models assume that there is no uncertainty in the environment and that actions can be taken without any constraints. In reality, agents often face uncertain environments where they need to make decisions while taking into account uncertainties and constraints that may arise due to chance. This research seeks to address these limitations by introducing the concept of ""Chance-constrained active inference.""  The study proposes a new framework that integrates chance constraints into active inference models, enabling agents to handle uncertain environments more effectively. We show how incorporating chance constraints improves decision-making under uncertainty by simulating different scenarios in which agents must choose actions based on probabilistically unknown variables. Our results demonstrate that agents equipped with our proposed model achieve higher success rates compared to those relying solely on traditional active inference methods. Additionally, we evaluate the impact of different types of chance constraints on agent performance, identifying key factors that influence optimal decision-making in uncertain environments.  Our findings have important implications for developing intelligent systems capable of navigating real-world situations characterized by variability and uncertainty. By providing a comprehensive understanding of how chance constraints shape decision-making processes, this work lays the groundwork for creating more robust and effective algorithms in areas such as robotics, autonomous vehicles, and healthcare applications involving decision-making under risk. Overall, our investigation establishes Chance-constrained active inference as a valuable tool for advancing the development of intelligent systems operating in complex and uncertain worlds.",1
"We provide the first global optimization landscape analysis of $Neural\;Collapse$ -- an intriguing empirical phenomenon that arises in the last-layer classifiers and features of neural networks during the terminal phase of training. As recently reported by Papyan et al., this phenomenon implies that ($i$) the class means and the last-layer classifiers all collapse to the vertices of a Simplex Equiangular Tight Frame (ETF) up to scaling, and ($ii$) cross-example within-class variability of last-layer activations collapses to zero. We study the problem based on a simplified $unconstrained\;feature\;model$, which isolates the topmost layers from the classifier of the neural network. In this context, we show that the classical cross-entropy loss with weight decay has a benign global landscape, in the sense that the only global minimizers are the Simplex ETFs while all other critical points are strict saddles whose Hessian exhibit negative curvature directions. In contrast to existing landscape analysis for deep neural networks which is often disconnected from practice, our analysis of the simplified model not only does it explain what kind of features are learned in the last layer, but it also shows why they can be efficiently optimized in the simplified settings, matching the empirical observations in practical deep network architectures. These findings could have profound implications for optimization, generalization, and robustness of broad interests. For example, our experiments demonstrate that one may set the feature dimension equal to the number of classes and fix the last-layer classifier to be a Simplex ETF for network training, which reduces memory cost by over $20\%$ on ResNet18 without sacrificing the generalization performance.",0
"In recent years, deep neural networks have become increasingly popular for their ability to solve complex problems in many fields such as image recognition, natural language processing, and game playing. However, one major challenge that still remains is understanding how these models work and why they perform so well. In particular, there has been growing interest in the phenomenon known as ""neural collapse"", where deep neural network architectures can learn simple linear functions even under unconstrained settings with random initialization. This behavior has significant implications for both theory and practice, but our current understanding is limited by the difficulties in analyzing high-dimensional nonlinear spaces and the lack of clear geometric intuition.  This paper presents a new framework based on the intrinsic coordinate systems (ICS) developed from kernel principal component analysis, which provides a more accurate representation of the underlying data geometry. We showcase how ICS can capture salient features and reduce dimensionality, enabling us to study the collapse problem in a more interpretable manner. Using extensive experiments, we demonstrate that deep ReLU neural networks with randomly initialized weights indeed converge to linear decision boundaries under different settings, providing evidence of neural collapse. Our results also suggest that the number of neurons plays a crucial role in determining whether collapse occurs, indicating that overparameterization may prevent collapse or enhance generalization performance beyond what would be possible with fewer parameters. Finally, we discuss potential mechanisms behind neural collapse, including regularization effects, optimization landscape, and learning dynamics, offering insights into future research directions. Overall, our contributions bridge theoretical foundations and empirical analyses in studying deep neural networks, paving the way for deeper investigations into the nature of machine learning algorithms.",1
"Symbolic techniques based on Satisfiability Modulo Theory (SMT) solvers have been proposed for analyzing and verifying neural network properties, but their usage has been fairly limited owing to their poor scalability with larger networks. In this work, we propose a technique for combining gradient-based methods with symbolic techniques to scale such analyses and demonstrate its application for model explanation. In particular, we apply this technique to identify minimal regions in an input that are most relevant for a neural network's prediction. Our approach uses gradient information (based on Integrated Gradients) to focus on a subset of neurons in the first layer, which allows our technique to scale to large networks. The corresponding SMT constraints encode the minimal input mask discovery problem such that after masking the input, the activations of the selected neurons are still above a threshold. After solving for the minimal masks, our approach scores the mask regions to generate a relative ordering of the features within the mask. This produces a saliency map which explains ""where a model is looking"" when making a prediction. We evaluate our technique on three datasets - MNIST, ImageNet, and Beer Reviews, and demonstrate both quantitatively and qualitatively that the regions generated by our approach are sparser and achieve higher saliency scores compared to the gradient-based methods alone. Code and examples are at - https://github.com/google-research/google-research/tree/master/smug_saliency",0
"In recent years, there has been increasing interest in understanding how neural models make their predictions, particularly as they have become more widely adopted in critical applications such as medicine and finance. One approach to model explanation involves interpreting the internal representations learned by the model through techniques like gradient analysis and activation maximization. However, these methods can be computationally expensive and difficult to scale to larger models.",1
"Recent advances in reinforcement learning have inspired increasing interest in learning user modeling adaptively through dynamic interactions, e.g., in reinforcement learning based recommender systems. Reward function is crucial for most of reinforcement learning applications as it can provide the guideline about the optimization. However, current reinforcement-learning-based methods rely on manually-defined reward functions, which cannot adapt to dynamic and noisy environments. Besides, they generally use task-specific reward functions that sacrifice generalization ability. We propose a generative inverse reinforcement learning for user behavioral preference modelling, to address the above issues. Instead of using predefined reward functions, our model can automatically learn the rewards from user's actions based on discriminative actor-critic network and Wasserstein GAN. Our model provides a general way of characterizing and explaining underlying behavioral tendencies, and our experiments show our method outperforms state-of-the-art methods in a variety of scenarios, namely traffic signal control, online recommender systems, and scanpath prediction.",0
"Machine learning has made significant progress in recent years in developing models that can learn from large amounts of data and make accurate predictions on new input samples. However, these models often lack interpretability, meaning they cannot explain how they arrived at their decisions. This problem becomes even more difficult in high stakes applications such as robotics or autonomous vehicles where decision making must be transparent and accountable. One approach to tackle this issue is by using reward functions which provide explicit objectives for optimizing, however current methods are limited in their expressiveness due to their reliance on hand engineered feature representations which may neglect important contextual factors. We propose the use of generative adversarial reward learning (GARL) which combines deep reinforcement learning techniques with generative adversarial networks (GANs). Our method captures both global and local behavior tendencies allowing for fine grained analysis of the task being performed. To demonstrate our model we apply GARL to a locomotion task with simulated hexapod robots showing improved performance compared to a baseline model without an intrinsic motivation system. Overall our work shows promise in inferring generalized behavior tendencies which have the potential to enhance transparency and accountability in machine learned systems.",1
"Explainable machine learning has become increasingly prevalent, especially in healthcare where explainable models are vital for ethical and trusted automated decision making. Work on the susceptibility of deep learning models to adversarial attacks has shown the ease of designing samples to mislead a model into making incorrect predictions. In this work, we propose a model agnostic explainability-based method for the accurate detection of adversarial samples on two datasets with different complexity and properties: Electronic Health Record (EHR) and chest X-ray (CXR) data. On the MIMIC-III and Henan-Renmin EHR datasets, we report a detection accuracy of 77% against the Longitudinal Adversarial Attack. On the MIMIC-CXR dataset, we achieve an accuracy of 88%; significantly improving on the state of the art of adversarial detection in both datasets by over 10% in all settings. We propose an anomaly detection based method using explainability techniques to detect adversarial samples which is able to generalise to different attack methods without a need for retraining.",0
"In recent years, deep learning has become increasingly popular in medical data analysis due to its ability to accurately detect patterns that may otherwise go unnoticed by human experts. However, these models remain vulnerable to adversarial attacks which can significantly decrease their accuracy, leading to incorrect diagnoses and potential harm to patients. This paper presents an attack-agnostic approach to detecting adversaries in medical data using explainable machine learning (XAI). We propose a novel framework called ADMED which combines feature denoising techniques with XAI methods to identify potential malicious inputs and improve model robustness without relying on specific attack types. Our experiments demonstrate that our approach outperforms state-of-the-art attack detection methods while maintaining comparable classification performance under normal operating conditions. Additionally, we provide insights into how different components of our method contribute to increased resilience against adversarial examples. Overall, our work represents a significant step towards ensuring safe deployment of deep learning systems in critical applications such as healthcare where every decision could potentially have life or death consequences.",1
"Adversarial examples mainly exploit changes to input pixels to which humans are not sensitive to, and arise from the fact that models make decisions based on uninterpretable features. Interestingly, cognitive science reports that the process of interpretability for human classification decision relies predominantly on low spatial frequency components. In this paper, we investigate the robustness to adversarial perturbations of models enforced during training to leverage information corresponding to different spatial frequency ranges. We show that it is tightly linked to the spatial frequency characteristics of the data at stake. Indeed, depending on the data set, the same constraint may results in very different level of robustness (up to 0.41 adversarial accuracy difference). To explain this phenomenon, we conduct several experiments to enlighten influential factors such as the level of sensitivity to high frequencies, and the transferability of adversarial perturbations between original and low-pass filtered inputs.",0
"This paper investigates how applying spatial frequency based constraints during adversarial training can impact the robustness of deep neural networks (DNNs) against adversarial attacks. Previous work has shown that incorporating high spatial frequencies into DNNs improves their ability to generalize to out-of-distribution data and reduces overfitting. In contrast, little attention has been paid to examining whether these types of constraints could improve adversarial robustness. Our experiments indicate that there exists an optimal level of constraint which balances the effectiveness of regularization with preserving important features, ultimately leading to improved robustness across multiple attack methods and architectures. We further analyze the effects of different feature extractors and loss functions on achieving better performance under adversarial conditions. Finally, we evaluate our results on both classification and detection tasks using standard benchmark datasets, demonstrating that our findings can effectively enhance adversarial robustness in real world applications. Overall, this study highlights the potential benefits of exploiting spatial frequency information for enhancing deep learning modelsâ€™ security properties.",1
"Convolutional neural networks often dominate fully-connected counterparts in generalization performance, especially on image classification tasks. This is often explained in terms of 'better inductive bias'. However, this has not been made mathematically rigorous, and the hurdle is that the fully connected net can always simulate the convolutional net (for a fixed task). Thus the training algorithm plays a role. The current work describes a natural task on which a provable sample complexity gap can be shown, for standard training algorithms. We construct a single natural distribution on $\mathbb{R}^d\times\{\pm 1\}$ on which any orthogonal-invariant algorithm (i.e. fully-connected networks trained with most gradient-based methods from gaussian initialization) requires $\Omega(d^2)$ samples to generalize while $O(1)$ samples suffice for convolutional architectures. Furthermore, we demonstrate a single target function, learning which on all possible distributions leads to an $O(1)$ vs $\Omega(d^2/\varepsilon)$ gap. The proof relies on the fact that SGD on fully-connected network is orthogonal equivariant. Similar results are achieved for $\ell_2$ regression and adaptive training algorithms, e.g. Adam and AdaGrad, which are only permutation equivariant.",0
"In deep learning, sample efficiency refers to how many data points are required for a model to achieve a certain level of accuracy. While convolutional neural networks (CNNs) have been shown to excel at image classification tasks, there has been little work done to examine their relative sample complexity compared to fully-connected nets (FCNs). In this paper, we explore why CNNs exhibit superior sample efficiency over FCNs by investigating the role that translation equivariance plays in reducing the number of training examples required to learn meaningful representations from raw pixel inputs. Our experiments demonstrate that equivariant feature extraction leads to more robust features across a variety of architectures and datasets, which translates into improved performance using fewer samples compared to FCNs. We provide insights on architectural design decisions such as filter sizes, strides, and padding choices, and evaluate these factors through extensive analysis and visualizations. Overall, our findings contribute new knowledge to explain and optimize sample efficiency in deep learning models.",1
"Explaining the decision of a multi-modal decision-maker requires to determine the evidence from both modalities. Recent advances in XAI provide explanations for models trained on still images. However, when it comes to modeling multiple sensory modalities in a dynamic world, it remains underexplored how to demystify the mysterious dynamics of a complex multi-modal model. In this work, we take a crucial step forward and explore learnable explanations for audio-visual recognition. Specifically, we propose a novel space-time attention network that uncovers the synergistic dynamics of audio and visual data over both space and time. Our model is capable of predicting the audio-visual video events, while justifying its decision by localizing where the relevant visual cues appear, and when the predicted sounds occur in videos. We benchmark our model on three audio-visual video event datasets, comparing extensively to multiple recent multi-modal representation learners and intrinsic explanation models. Experimental results demonstrate the clear superior performance of our model over the existing methods on audio-visual video event recognition. Moreover, we conduct an in-depth study to analyze the explainability of our model based on robustness analysis via perturbation tests and pointing games using human annotations.",0
"Visualization techniques are crucial in many fields such as scientific research, engineering, art, education, and more. In recent years, there have been efforts to create more interactive visualizations that allow users to explore complex datasets in new ways. One area where these interactive visualizations can greatly benefit from more advanced technologies is in audio-visual explanations. These types of explainers often involve both image and sound to convey their message, but existing methods struggle to integrate well across space (the layout and positioning of elements) and time (how things change over time). This paper proposes using a technique called space-time attention to improve audio-visual explanation generation. Through experiments on two challenging real-world applications â€“ climate modeling and music analysis/generation â€“ we show how our method leads to significant improvements in both speed and quality compared to strong baselines. With space-time attention, users will now be able to explore detailed data without having to manually adjust parameters like camera angles and lighting each time they zoom in or out. Instead, the system will automatically optimize for clarity and readability through user feedback and deep learning algorithms. We believe that this work represents an important step towards building better interactive tools for understanding large and complex datasets, ultimately leading to improved decision making and creativity across different domains.",1
"Historically, artificial intelligence has drawn much inspiration from neuroscience to fuel advances in the field. However, current progress in reinforcement learning is largely focused on benchmark problems that fail to capture many of the aspects that are of interest in neuroscience today. We illustrate this point by extending a T-maze task from neuroscience for use with reinforcement learning algorithms, and show that state-of-the-art algorithms are not capable of solving this problem. Finally, we point out where insights from neuroscience could help explain some of the issues encountered.",0
"Learning is a fundamental aspect of cognition that has been extensively studied both through neuroscientific methods such as electroencephalography (EEG) and functional magnetic resonance imaging (fMRI), and computational methods such as reinforcement learning algorithms. Despite these efforts, there remains a significant gap between our understanding of neural processes involved in learning and how they relate to behavior observed from reinforcement learning models. This review examines the current state of knowledge regarding learning at the level of single neurons, neural systems, and the role of reward processing in shaping learning outcomes. We highlight recent findings on the importance of dopamine signaling during reward learning, discrepancies between model predictions and human behavior, and suggest potential future research directions towards bridging this gap. Keywords: Neuroscience; Reinforcement learning; Dopamine; Single neuron recording; Reward learning.  -----  Neuroscience and reinforcement learning have made great strides in unraveling the complex mechanisms underlying learning in the brain. However, despite these advancements, there remains a significant divide between our understanding of neurological processes related to learning and their link to observable behaviors in reinforcement learning paradigms. To bridge this gap, we conducted a comprehensive evaluation of existing literature spanning multiple disciplinary domains, including single-neuron recordings, systemic neural analysis, and the crucial role of dopaminergic signaling during reward acquisition. Our review illustrates apparent inconsistencies between experimental data and theoretical predictions from reinforcement learning models and proposes prospective lines of inquiry for closing this gap. Future investigations focused on better integrating neurobiological insights into reinforce",1
"Although the expectation maximisation (EM) algorithm was introduced in 1970, it remains somewhat inaccessible to machine learning practitioners due to its obscure notation, terse proofs and lack of concrete links to modern machine learning techniques like autoencoded variational Bayes. This has resulted in gaps in the AI literature concerning the meaning of such concepts like ""latent variables"" and ""variational lower bound,"" which are frequently used but often not clearly explained. The roots of these ideas lie in the EM algorithm. We first give a tutorial presentation of the EM algorithm for estimating the parameters of a $K$-component mixture density. The Gaussian mixture case is presented in detail using $K$-ary scalar hidden (or latent) variables rather than the more traditional binary valued $K$-dimenional vectors. This presentation is motivated by mixture modelling from the target tracking literature. In a similar style to Bishop's 2009 book, we present variational Bayesian inference as a generalised EM algorithm stemming from the variational (or evidential) lower bound, as well as the technique of mean field approximation (or product density transform). We continue the evolution from EM to variational autoencoders, developed by Kingma & Welling in 2014. In so doing, we establish clear links between the EM algorithm and its variational counterparts, hence clarifying the meaning of ""latent variables."" We provide a detailed coverage of the ""reparametrisation trick"" and focus on how the AEVB differs from conventional variational Bayesian inference. Throughout the tutorial, consistent notational conventions are used. This unifies the narrative and clarifies the concepts. Some numerical examples are given to further illustrate the algorithms.",0
"In recent years, variational autoencoders have emerged as powerful tools for unsupervised learning and generative modelling tasks such as image generation and style transfer. The expectation maximization (EM) algorithm has been widely used in variational inference since its introduction by Dempster et al. in 1977. However, there have been some limitations to using EM for variational inference, especially when dealing with complex models that involve high dimensional latent spaces. To address these limitations, researchers have developed alternative methods for variational inference such as autoencoded variational Bayes (AVB). This paper presents a review of both the expectations maximization algorithm and autoencoded variational Bayes, highlighting their similarities and differences, advantages and disadvantages, and applications in machine learning and artificial intelligence. It also discusses potential future directions for research on these algorithms.",1
"This study analyzed the performance of different machine learning methods for winter wheat yield prediction using extensive datasets of weather, soil, and crop phenology. To address the seasonality, weekly features were used that explicitly take soil moisture conditions and meteorological events into account. Our results indicated that nonlinear models such as deep neural networks (DNN) and XGboost are more effective in finding the functional relationship between the crop yield and input data compared to linear models. The results also revealed that the deep neural networks often had a higher prediction accuracy than XGboost. One of the main limitations of machine learning models is their black box property. As a result, we moved beyond prediction and performed feature selection, as it provides key results towards explaining yield prediction (variable importance by time). The feature selection method estimated the individual effect of weather components, soil conditions, and phenology variables as well as the time that these variables become important. As such, our study indicates which variables have the most significant effect on winter wheat yield.",0
"In recent years, machine learning has been increasingly used to model crop yields. For example, winter wheat yield prediction can greatly benefit from such approaches due to the high variation among different cultivars and environments, and because of the importance of accurate yield predictions on farm management decisions. There have already been some studies comparing the performance of alternative methodologies in predicting maize silage dry matter yield using models created by machine learning algorithms on field experiment data [2]. Here we compare how well two methods work at predicting winter wheat yield: linear regression (LR) and support vector machines (SVM). These tools allow us to use nonlinear relationships in making our model. We applied these techniques to a dataset collected during six growing seasons (2004 - 2009), containing observations of weather parameters and agricultural practices commonly considered important in influencing yield. Our results show that SVM is significantly more effective than LR; indeed they were able to explain approximately twice as much variability in observed yields as LR did. Furthermore, combining multiple SVM models built with varying parameter settings improved overall performance modestly over any single one alone. This study shows that while simple LR provides acceptable performance on this task, the inclusion of a more complex approach like SVM dramatically improves accuracy. Therefore both the choice of algorithm employed and care taken in selecting hyperparameters therein may prove critical factors in creating successful yield forecasting models.",1
"The goal of continual learning (CL) is to learn a sequence of tasks without suffering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a replay buffer can reduce performance degradation on prior tasks. We hypothesize that forgetting can be further reduced when the model is encouraged to remember the \textit{evidence} for previously made decisions. As a first step towards exploring this hypothesis, we propose a simple novel training paradigm, called Remembering for the Right Reasons (RRR), that additionally stores visual model explanations for each example in the buffer and ensures the model has ""the right reasons"" for its predictions by encouraging its explanations to remain consistent with those used to make decisions at training time. Without this constraint, there is a drift in explanations and increase in forgetting as conventional continual learning algorithms learn new tasks. We demonstrate how RRR can be easily added to any memory or regularization-based approach and results in reduced forgetting, and more importantly, improved model explanations. We have evaluated our approach in the standard and few-shot settings and observed a consistent improvement across various CL approaches using different architectures and techniques to generate model explanations and demonstrated our approach showing a promising connection between explainability and continual learning. Our code is available at \url{https://github.com/SaynaEbrahimi/Remembering-for-the-Right-Reasons}.",0
"This research demonstrates that remembering events through explanations can lead to better recall than just remembering events alone. In two experiments, participants were asked to generate explanations as they learned new concepts or stories. When tested later on their memory retention, those who generated explanations had significantly higher recall rates compared to those who simply recalled the story without creating any explanation. These findings suggest that generating explanations during learning helps to consolidate memories and reduce catastrophic forgetting. Overall, the study highlights the importance of understanding why we remember certain things rather than just focusing on rote memorization. By emphasizing exploration over repetition, learners may develop more robust memory networks, leading to improved knowledge acquisition.",1
"Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At the same time, there are concerns that these visualizations might not accurately represent CNNs' inner workings. Here, we measure how much extremely activating images help humans to predict CNN activations. Using a well-controlled psychophysical paradigm, we compare the informativeness of synthetic images by Olah et al. (2017) with a simple baseline visualization, namely exemplary natural images that also strongly activate a specific feature map. Given either synthetic or natural reference images, human participants choose which of two query images leads to strong positive activation. The experiments are designed to maximize participants' performance, and are the first to probe intermediate instead of final layer representations. We find that synthetic images indeed provide helpful information about feature map activations ($82\pm4\%$ accuracy; chance would be $50\%$). However, natural images - originally intended as a baseline - outperform synthetic images by a wide margin ($92\pm2\%$). Additionally, participants are faster and more confident for natural images, whereas subjective impressions about the interpretability of the feature visualizations are mixed. The higher informativeness of natural images holds across most layers, for both expert and lay participants as well as for hand- and randomly-picked feature visualizations. Even if only a single reference image is given, synthetic images provide less information than natural images ($65\pm5\%$ vs. $73\pm4\%$). In summary, synthetic images from a popular feature visualization method are significantly less informative for assessing CNN activations than natural images. We argue that visualization methods should improve over this baseline.",0
"This paper proposes a new method that uses natural images to better visualize neural network activations. Our approach outperforms current state-of-the-art feature visualization methods by providing more accurate explanations of how neural networks work. We show that our method leads to significant improvements over traditional approaches in terms of both accuracy and interpretability. Furthermore, we demonstrate the effectiveness of our method on several popular datasets and architectures commonly used in computer vision tasks, such as object recognition, image segmentation, and scene understanding. Our results suggest that natural images provide valuable insights into the inner workings of neural networks, opening up exciting possibilities for further research into explainable artificial intelligence. Overall, our findings have important implications for advancing our understanding of deep learning models and enabling their use in critical applications across different domains.",1
"Minimally invasive surgery mainly consists of a series of sub-tasks, which can be decomposed into basic gestures or contexts. As a prerequisite of autonomic operation, surgical gesture recognition can assist motion planning and decision-making, and build up context-aware knowledge to improve the surgical robot control quality. In this work, we aim to develop an effective surgical gesture recognition approach with an explainable feature extraction process. A Bidirectional Multi-Layer independently RNN (BML-indRNN) model is proposed in this paper, while spatial feature extraction is implemented via fine-tuning of a Deep Convolutional Neural Network(DCNN) model constructed based on the VGG architecture. To eliminate the black-box effects of DCNN, Gradient-weighted Class Activation Mapping (Grad-CAM) is employed. It can provide explainable results by showing the regions of the surgical images that have a strong relationship with the surgical gesture classification results. The proposed method was evaluated based on the suturing task with data obtained from the public available JIGSAWS database. Comparative studies were conducted to verify the proposed framework. Results indicated that the testing accuracy for the suturing task based on our proposed method is 87.13%, which outperforms most of the state-of-the-art algorithms.",0
"Our research addresses the challenge of accurately recognizing surgical gestures using a novel approach that combines bidirectional multi-layer independently recurrent neural networks (RNNs) with explainable spatial feature extraction. This approach enables more accurate recognition of complex surgical gestures by capturing both short-term and long-term dependencies in gesture data, as well as providing insights into which features most strongly contribute to successful classification. We evaluate our method on a benchmark dataset comprising three types of surgical gestures performed by expert surgeons during laparoscopic cholecystectomy procedures. Experimental results demonstrate that our proposed method significantly outperforms state-of-the-art techniques in terms of accuracy, achieving an overall F1 score of 98%. Additionally, we show how explainability can enhance human trust in decision support systems based on machine learning models through visualizations of important regions detected by the model. Overall, our work advances the field of computer vision and has significant potential applications in robot-assisted minimally invasive surgery.",1
"Despite their remarkable performance on a wide range of visual tasks, machine learning technologies often succumb to data distribution shifts. Consequently, a range of recent work explores techniques for detecting these shifts. Unfortunately, current techniques offer no explanations about what triggers the detection of shifts, thus limiting their utility to provide actionable insights. In this work, we present Concept Bottleneck Shift Detection (CBSD): a novel explainable shift detection method. CBSD provides explanations by identifying and ranking the degree to which high-level human-understandable concepts are affected by shifts. Using two case studies (dSprites and 3dshapes), we demonstrate how CBSD can accurately detect underlying concepts that are affected by shifts and achieve higher detection accuracy compared to state-of-the-art shift detection methods.",0
"""Conceptualizing dataset shift has emerged as a crucial task across various fields, including machine learning, statistics, and causal inference. Existing methods primarily focus on statistical approaches that aim to quantify the degree of shift using measures such as sample means, variances, or divergences. However, these measures have significant limitations in terms of their interpretability, actionability, and adaptivity to different forms of concept drifts. This paper presents a novel framework for understanding and explaining concept shifts through a combination of philosophical analysis and case studies from real-world applications. We argue that traditional ways of thinking about concepts within philosophy may hinder our ability to understand and address contemporary challenges related to dataset shifts. Instead, we propose a new approach based on extended mind theory and distributed cognition perspectives, which emphasizes the role of environment, technology, and social interactions in shaping human knowledge and practices. Our findings suggest that extending the scope of cognitive processes beyond individual minds enhances our ability to detect and explain conceptual changes across different domains. Overall, this work contributes to both theoretical debates in the field and provides insights into effective strategies for handling unknown or unanticipated conceptual changes.""",1
"Semantic segmentation is the pixel-wise labelling of an image. Since the problem is defined at the pixel level, determining image class labels only is not acceptable, but localising them at the original image pixel resolution is necessary. Boosted by the extraordinary ability of convolutional neural networks (CNN) in creating semantic, high level and hierarchical image features; several deep learning-based 2D semantic segmentation approaches have been proposed within the last decade. In this survey, we mainly focus on the recent scientific developments in semantic segmentation, specifically on deep learning-based methods using 2D images. We started with an analysis of the public image sets and leaderboards for 2D semantic segmentation, with an overview of the techniques employed in performance evaluation. In examining the evolution of the field, we chronologically categorised the approaches into three main periods, namely pre-and early deep learning era, the fully convolutional era, and the post-FCN era. We technically analysed the solutions put forward in terms of solving the fundamental problems of the field, such as fine-grained localisation and scale invariance. Before drawing our conclusions, we present a table of methods from all mentioned eras, with a summary of each approach that explains their contribution to the field. We conclude the survey by discussing the current challenges of the field and to what extent they have been solved.",0
"In recent years, deep learning has emerged as a powerful tool for tackling various computer vision tasks, including semantic segmentation - the problem of assigning semantic labels to every pixel in an image. This survey paper aims to provide a comprehensive overview of state-of-the-art deep learning-based architectures for semantic segmentation on 2D images. We begin by introducing the fundamental concepts and challenges involved in semantic segmentation, highlighting key factors that make the task particularly difficult. We then present an overview of classic machine learning approaches for semantic segmentation, setting the stage for our discussion of deep learning methods. Next, we delve into different types of deep learning architectures commonly used for semantic segmentation, including fully convolutional networks (FCNs), encoder-decoders, dilated neural networks, atrous spatial pyramid pooling (ASPP) modules, and U-Net. For each architecture type, we discuss representative works in detail, examining their underlying principles, strengths and weaknesses, modifications, extensions, variations, improvements, and applications. Finally, we conclude the paper with some insights into future directions in this field, identifying open research questions that need to be addressed. Overall, this survey provides readers with an accessible yet detailed introduction to the topic, helping them gain a strong understanding of contemporary developments and trends in deep learning-based semantic segmentation on 2D images.",1
"Despite substantial progress in applying neural networks (NN) to a wide variety of areas, they still largely suffer from a lack of transparency and interpretability. While recent developments in explainable artificial intelligence attempt to bridge this gap (e.g., by visualizing the correlation between input pixels and final outputs), these approaches are limited to explaining low-level relationships, and crucially, do not provide insights on error correction. In this work, we propose a framework (VRX) to interpret classification NNs with intuitive structural visual concepts. Given a trained classification model, the proposed VRX extracts relevant class-specific visual concepts and organizes them using structural concept graphs (SCG) based on pairwise concept relationships. By means of knowledge distillation, we show VRX can take a step towards mimicking the reasoning process of NNs and provide logical, concept-level explanations for final model decisions. With extensive experiments, we empirically show VRX can meaningfully answer ""why"" and ""why not"" questions about the prediction, providing easy-to-understand insights about the reasoning process. We also show that these insights can potentially provide guidance on improving NN's performance.",0
"Artificial neural networks (ANN) have gained significant popularity due to their ability to solve complex problems in image recognition, natural language processing, game playing, robotics, etc. Despite their widespread usage, the reasoning process used by these models remains a mystery. This work proposes to use structural visual concepts to interpret the results of ANNs so that humans can gain more insight into how they arrive at decisions. The structural visual concepts will enable us to explain why certain regions of input images attracted specific attentions from deep learning systems and provide quantitative evaluation metrics related to different tasks. We will validate our proposed approach using several real applications such as object detection, instance segmentation, and human pose estimation, where we aim to establish relationships between the internal representations of the model and external interpretable features. Overall, the goal of this research is to demystify the inner working mechanisms behind deep learning and build reliable machine decision-making systems through comprehensible interpretation techniques.",1
"Knowledge distillation (KD) has been actively studied for image classification tasks in deep learning, aiming to improve the performance of a student model based on the knowledge from a teacher model. However, there have been very few efforts for applying KD in image regression with a scalar response, and there is no KD method applicable to both tasks. Moreover, existing KD methods often require a practitioner to carefully choose or adjust the teacher and student architectures, making these methods less scalable in practice. Furthermore, although KD is usually conducted in scenarios with limited labeled data, very few techniques are developed to alleviate such data insufficiency. To solve the above problems in an all-in-one manner, we propose in this paper a unified KD framework based on conditional generative adversarial networks (cGANs), termed cGAN-KD. Fundamentally different from existing KD methods, cGAN-KD distills and transfers knowledge from a teacher model to a student model via cGAN-generated samples. This unique mechanism makes cGAN-KD suitable for both classification and regression tasks, compatible with other KD methods, and insensitive to the teacher and student architectures. Also, benefiting from the recent advances in cGAN methodology and our specially designed subsampling and filtering procedures, cGAN-KD also performs well when labeled data are scarce. An error bound of a student model trained in the cGAN-KD framework is derived in this work, which theoretically explains why cGAN-KD takes effect and guides the implementation of cGAN-KD in practice. Extensive experiments on CIFAR-10 and Tiny-ImageNet show that we can incorporate state-of-the-art KD methods into the cGAN-KD framework to reach a new state of the art. Also, experiments on RC-49 and UTKFace demonstrate the effectiveness of cGAN-KD in image regression tasks, where existing KD methods are inapplicable.",0
"This paper presents an innovative approach to distill knowledge from large pretrained models such as DenseNet and ResNet. Instead of utilizing standard fine-tuning techniques on smaller datasets, we introduce a technique that leverages generative adversarial networks (GANs) to generate high quality synthetic samples. These GAN-generated samples can then be used to train new models quickly and efficiently while maintaining state-of-the-art performance. We evaluate our approach across several benchmark image classification and regression tasks including CIFAR-10, SVHN, TinyImageNet, and ImageNet. Our experiments show consistent improvements over both naive fine-tuning baselines and more recent methods such as self-supervised learning. Additionally, our method allows us to transfer knowledge across multiple domains, further demonstrating its effectiveness.",1
"Pipelines involving a series of several machine learning models (e.g., stacked generalization ensembles, neural network feature extractors) improve performance in many domains but are difficult to understand. To improve their transparency, we introduce a framework to propagate local feature attributions through complex pipelines of models based on a connection to the Shapley value. Our framework enables us to (1) draw higher-level conclusions based on groups of gene expression features for Alzheimer's and breast cancer histologic grade prediction, (2) draw important insights about the errors a mortality prediction model makes by explaining a loss that is a non-linear transformation of the model's output, (3) explain pipelines of deep feature extractors fed into a tree model for MNIST digit classification, and (4) interpret important consumer scores and raw features in a stacked generalization setting to predict risk for home equity line of credit applications. Importantly, in the consumer scoring example, DeepSHAP is the only feature attribution technique we are aware of that allows independent entities (e.g., lending institutions, credit bureaus) to compute attributions for the original features without having to share their proprietary models. Quantitatively comparing our framework to model-agnostic approaches, we show that our approach is an order of magnitude faster while providing equally salient explanations. In addition, we describe how to incorporate an empirical baseline distribution, which allows us to (1) demonstrate the bias of previous approaches that use a single baseline sample, and (2) present a straightforward methodology for choosing meaningful baseline distributions.",0
"This paper is focused on explaining how multiple models can be compared to determine which one provides the best performance. We introduce our approach as a general method that is applicable to any differentiable model, such as deep neural networks (DNN) and graphical models. Our approach first generates local feature attributions using gradients and then propagates these to obtain global explanations, allowing a comparison at both fine-grained and coarse-grained levels across all models. Through experiments, we demonstrate effectiveness in terms of correlation to target variables and faithfulness to ground truths across diverse datasets and tasks including image classification, sentiment analysis and text classification. Overall, the results show significant improvement over existing methods in several benchmark tests while offering competitive accuracy tradeoffs in some cases. As an initial step toward more advanced explanation, we hope this work encourages further exploration into developing reliable explainability measures beyond traditional metrics like F1 scores alone.",1
"Federated learning (FL) has recently emerged as an important and promising learning scheme in IoT, enabling devices to jointly learn a model without sharing their raw data sets. However, as the training data in FL is not collected and stored centrally, FL training requires frequent model exchange, which is largely affected by the wireless communication network. Therein, limited bandwidth and random package loss restrict interactions in training. Meanwhile, the insufficient message synchronization among distributed clients could also affect FL convergence. In this paper, we analyze the convergence rate of FL training considering the joint impact of communication network and training settings. Further by considering the training costs in terms of time and power, the optimal scheduling problems for communication networks are formulated. The developed theoretical results can be used to assist the system parameter selections and explain the principle of how the wireless communication system could influence the distributed training process and network scheduling.",0
"In recent years there has been significant interest in federated learning as a promising approach to train machine learning models on distributed data while preserving user privacy. This work focuses on convergence analysis and system design for federated learning over wireless networks (FLWN). FLWN aims to address several challenges such as limited bandwidth and unstable connectivity in order to support efficient model training and device coordination. We develop theoretical results that provide insights into the convergence behavior of FLWN systems under different conditions. Our simulation results demonstrate the impact of network settings on the performance of FLWN systems. Finally, we present design guidelines that can facilitate effective deployment of FLWN systems for specific application scenarios. Keywords: federated learning; wireless networks; convergence analysis; system design; privacy preservation.",1
"Graph neural networks (GNNs) are deep learning architectures for machine learning problems on graphs. It has recently been shown that the expressiveness of GNNs can be characterised precisely by the combinatorial Weisfeiler-Leman algorithms and by finite variable counting logics. The correspondence has even led to new, higher-order GNNs corresponding to the WL algorithm in higher dimensions.   The purpose of this paper is to explain these descriptive characterisations of GNNs.",0
"Inference problems often involve reasoning over large amounts of data or complex relationships among entities. To effectively solve these kinds of tasks, artificial intelligence (AI) has turned to machine learning algorithms such as graph neural networks. These models have been used successfully across a wide range of domains including natural language processing and computer vision. However, there lacks a clear understanding of why graph neural networks work so well. This paper aims to provide insights into how graph neural networks make inferences by exploring their underlying logical structure. We show that they can be modeled as probabilistic graphical models which makes them amenable to analysis using established tools from probability theory. Our results reveal novel connections between graph neural networks and statistical physics. Additionally, we demonstrate through experimental evaluations that our framework leads to new design principles for improving existing architectures and for developing new ones. Overall, our findings contribute towards building better GNNs that perform more accurate inference while providing greater interpretability and transparency.",1
"Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way {\em even} in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature, and another, statistical in nature. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets.",0
"In recent years, the field of computer vision has seen significant advances due to deep learning methods such as Convolutional Neural Networks (CNN) [1]. Despite these successes, deep neural networks have been shown to struggle with out-of-distribution generalization [2][3], that is, their ability to perform well on data they were never exposed to during training. This study investigates common failure modes underlying this phenomenon through an analysis of published research papers and case studies from real-world applications. We identify four key reasons why models trained to high accuracy may still fail when encountering previously unseen inputs: distribution shift, label corruption, input interpolation, and decision boundary collapse. We discuss implications and potential solutions for each mode and provide recommendations for practitioners looking to improve model robustness. By shedding light on these failure modes, we aim to advance our understanding of CNN limitations and contribute to future progress in solving challenges related to out-of-distribution generalization. References [1] *Krizhevsky et al.* (2017). ""Imagenet classification with deep convolutional neural network."" In Advances in neural information processing systems(pp. 1090-1098). Citeseer.[2] *Nguyen et al.* (2020). ""An empirical evaluation of modern machine learning tools for outlier detection."" arXiv preprint arXiv:2006.04717.[3] *Choi et al.* (2020). ""Explanatory factors of overfitting in outlier detection by gradient boosting."" Knowledge-Based Systems, 124, 104-113. In this paper, the authors investigate the underlying causes of one of the main shortcomings of current state-of-the-art deep learning approaches in computer vision - poor out-of-distribution generalization. They conduct a thorough review of existing literature and real-world applications, identifying four key reasons why models that achieve high accuracy during training can still falter when presented with new, unseen data: distribution shift, label corruption, input interpolation, and decision boundary collapse. Each of these failure modes is discussed in detail, along with possible solutions and recommendations for improving model robustness. Ultimately, the authors hope to increase our understanding of deep neural network limitations and pave the way for further progress towards addressing challenges related to out-of-distribution generalization.",1
"Explainable deep learning models are advantageous in many situations. Prior work mostly provide unimodal explanations through post-hoc approaches not part of the original system design. Explanation mechanisms also ignore useful textual information present in images. In this paper, we propose MTXNet, an end-to-end trainable multimodal architecture to generate multimodal explanations, which focuses on the text in the image. We curate a novel dataset TextVQA-X, containing ground truth visual and multi-reference textual explanations that can be leveraged during both training and evaluation. We then quantitatively show that training with multimodal explanations complements model performance and surpasses unimodal baselines by up to 7% in CIDEr scores and 2% in IoU. More importantly, we demonstrate that the multimodal explanations are consistent with human interpretations, help justify the models' decision, and provide useful insights to help diagnose an incorrect prediction. Finally, we describe a real-world e-commerce application for using the generated multimodal explanations.",0
"This research paper presents an analysis of text-based visual question answering (TextVQA) models. By developing explainable models that incorporate both visual and textual explanations, we aim to improve understanding of how these systems make decisions. Our approach evaluates model performance using human annotations and compares it against baseline methods, highlighting strengths and weaknesses. Results show significant improvement over traditional methods while providing transparency into decision making processes. Overall, our work contributes to the fields of computer vision and natural language processing by advancing interpretability techniques in TextVQA.",1
"""Deep Learning""/""Deep Neural Nets"" is a technological marvel that is now increasingly deployed at the cutting-edge of artificial intelligence tasks. This dramatic success of deep learning in the last few years has been hinged on an enormous amount of heuristics and it has turned out to be a serious mathematical challenge to be able to rigorously explain them. In this thesis, submitted to the Department of Applied Mathematics and Statistics, Johns Hopkins University we take several steps towards building strong theoretical foundations for these new paradigms of deep-learning. In chapter 2 we show new circuit complexity theorems for deep neural functions and prove classification theorems about these function spaces which in turn lead to exact algorithms for empirical risk minimization for depth 2 ReLU nets. We also motivate a measure of complexity of neural functions to constructively establish the existence of high-complexity neural functions. In chapter 3 we give the first algorithm which can train a ReLU gate in the realizable setting in linear time in an almost distribution free set up. In chapter 4 we give rigorous proofs towards explaining the phenomenon of autoencoders being able to do sparse-coding. In chapter 5 we give the first-of-its-kind proofs of convergence for stochastic and deterministic versions of the widely used adaptive gradient deep-learning algorithms, RMSProp and ADAM. This chapter also includes a detailed empirical study on autoencoders of the hyper-parameter values at which modern algorithms have a significant advantage over classical acceleration based methods. In the last chapter 6 we give new and improved PAC-Bayesian bounds for the risk of stochastic neural nets. This chapter also includes an experimental investigation revealing new geometric properties of the paths in weight space that are traced out by the net during the training.",0
"""This"" should only appear once in the body if at all possible. Please ensure that your abstract can be submitted as a single paragraph without line breaks. Use no more than two sentences that begin with numbers unless absolutely necessary and properly referenced numerals. For example: ""2% of Americans are affected by x"". Only use common Latin abbreviations such as i.e., e.g., cf. etc. Also, please write out contractions as you would for academic papers but feel free to split up into multiple sentences. Finally, provide a proper conclusion which summarizes the findings in broad terms as well as any limitations you found during the research process including how many data points were examined where applicable. You may also briefly mention future directions for further investigation; just remember to keep this succinct so the total length remains under 300 words. Abstracts must be written in English. Thank you! A Study of the Mathematics of Deep Learning: An Analysis of Convolutional Neural Networks  Abstract: The field of deep learning has experienced significant growth over the past decade due to advancements in both hardware and software capabilities. In particular, convolutional neural networks (CNN) have become one of the most widely used architectures in image processing tasks, achieving state-of-the-art results on several benchmark datasets. This study aimed to explore the mathematical principles underlying CNNs and their ability to learn complex representations from raw input data. To achieve this goal, we conducted a comprehensive literature review and analyzed popular open source implementations of CNNs, focusing on key components such as the architecture design, optimization techniques, and regularization methods. We evaluated the performance of these models using standard metrics and compared them against traditional machine learning algorithms. Our findings suggest that CNNs significantly outperform classical approaches across a wide range of applications, confirming their effectiveness in representing high-level features directly from images. However, our analysis also highlighted some limitations, particularly related to interpretability and generalizability, suggesting potential areas for improvement and future work. Overall, this study provides valuable insights into the mathematics of deep learning, contributing towards a better understanding of CNNs and their impact on the field.",1
"We present a Fourier neural network (FNN) that can be mapped directly to the Fourier decomposition. The choice of activation and loss function yields results that replicate a Fourier series expansion closely while preserving a straightforward architecture with a single hidden layer. The simplicity of this network architecture facilitates the integration with any other higher-complexity networks, at a data pre- or postprocessing stage. We validate this FNN on naturally periodic smooth functions and on piecewise continuous periodic functions. We showcase the use of this FNN for modeling or solving partial differential equations with periodic boundary conditions. The main advantages of the current approach are the validity of the solution outside the training region, interpretability of the trained model, and simplicity of use.",0
"Fourier neural networks (FNN) have been used as powerful approximators for functions based on their universal approximation theorem. They approximate a function by breaking it into simple trigonometric functions, which makes them well suited for differential equation solutions. In this work, we explore the use of FNN as differentiator equations solvers using Picard iteration technique, making it possible to find analytic solution for linear and nonlinear ordinary differential equations (ODE), partial differential equations (PDE), delay ODE/PDE, and more complex systems like stochastic DE/PDEs and coupled systems of ODEs/PDEs. We compare results from our approach to those obtained using traditional methods such as finite difference method and Adomian decomposition method. Our approach offers several advantages over existing methods including improved accuracy, stability and versatility through automatic differentiation in solving large system of ODEs/PDEs and coupled systems without requiring explicit computation of Jacobians matrices that reduces memory usage and computational cost. Finally, our findings show promising results for future application in fields such as physics, engineering and biology sciences where ODEs/PDEs play important roles. Keywords: Fourier neural network, differential equations, Picard iteration, universality principle, automatic differentiation",1
"We introduce a bottom-up model for simultaneously finding many boundary elements in an image, including contours, corners and junctions. The model explains boundary shape in each small patch using a 'generalized M-junction' comprising M angles and a freely-moving vertex. Images are analyzed using non-convex optimization to cooperatively find M+2 junction values at every location, with spatial consistency being enforced by a novel regularizer that reduces curvature while preserving corners and junctions. The resulting 'field of junctions' is simultaneously a contour detector, corner/junction detector, and boundary-aware smoothing of regional appearance. Notably, its unified analysis of contours, corners, junctions and uniform regions allows it to succeed at high noise levels, where other methods for segmentation and boundary detection fail.",0
"This work presents methods for accurately extracting boundary structure from low signal-to-noise ratio (SNR) data in a variety of fields. By developing novel algorithms that can effectively distinguish true signals from noise, we demonstrate significant improvements over existing approaches in both accuracy and speed. We apply these techniques to diverse domains such as image processing, speech recognition, and natural language analysis, illustrating their effectiveness across multiple disciplines. Our approach outperforms traditional methods by leveraging advanced machine learning models and carefully calibrated regularization terms, allowing us to handle complex datasets with high confidence. Overall, our work represents a major advancement in the field of junction detection and opens up new possibilities for researchers seeking to unlock valuable insights hidden within noisy data sets.",1
"Finding valuable training data points for deep neural networks has been a core research challenge with many applications. In recent years, various techniques for calculating the ""value"" of individual training datapoints have been proposed for explaining trained models. However, the value of a training datapoint also depends on other selected training datapoints - a notion that is not explicitly captured by existing methods. In this paper, we study the problem of selecting high-value subsets of training data. The key idea is to design a learnable framework for online subset selection, which can be learned using mini-batches of training data, thus making our method scalable. This results in a parameterized convex subset selection problem that is amenable to a differentiable convex programming paradigm, thus allowing us to learn the parameters of the selection model in end-to-end training. Using this framework, we design an online alternating minimization-based algorithm for jointly learning the parameters of the selection model and ML model. Extensive evaluation on a synthetic dataset, and three standard datasets, show that our algorithm finds consistently higher value subsets of training data, compared to the recent state-of-the-art methods, sometimes ~20% higher value than existing methods. The subsets are also useful in finding mislabelled training data. Our algorithm takes running time comparable to the existing valuation functions.",0
"Abstract: In order to build effective machine learning models, high quality training data is essential. However, obtaining large amounts of labeled training data can be time consuming and expensive. This paper proposes using differentiable convex programming techniques to efficiently identify a subset of highly informative training examples from a larger dataset. Our approach first defines a loss function that measures how well the current model fits the given data, then uses the gradient of this loss function to select a smaller set of more valuable training examples. Experimental results on a range of benchmark datasets demonstrate that our method significantly outperforms traditional methods for selecting subsets of high value training data while requiring fewer labeled examples overall. These findings have important implications for scaling up machine learning pipelines across multiple domains, reducing labeling costs, and improving model performance. Keywords: differential privacy; submodularity; accelerating convergence rates through adaptive sampling; efficient approximation algorithms.",1
"Building embodied autonomous agents capable of participating in social interactions with humans is one of the main challenges in AI. This problem motivated many research directions on embodied language use. Current approaches focus on language as a communication tool in very simplified and non diverse social situations: the ""naturalness"" of language is reduced to the concept of high vocabulary size and variability. In this paper, we argue that aiming towards human-level AI requires a broader set of key social skills: 1) language use in complex and variable social contexts; 2) beyond language, complex embodied communication in multimodal settings within constantly evolving social worlds. In this work we explain how concepts from cognitive sciences could help AI to draw a roadmap towards human-like intelligence, with a focus on its social dimensions. We then study the limits of a recent SOTA Deep RL approach when tested on a first grid-world environment from the upcoming SocialAI, a benchmark to assess the social skills of Deep RL agents. Videos and code are available at https://sites.google.com/view/socialai01 .",0
"This paper presents our initial efforts towards creating a benchmark task that encourages researchers to develop deep reinforcement learning agents capable of exhibiting socio-cognitive abilities. We describe our motivations behind developing such a benchmark, along with our proposed approach to creating challenging tasks that require agents to engage in social interactions and exercise socially appropriate behavior. Our work builds upon existing studies in the field of artificial intelligence (AI) and human-computer interaction (HCI), where researchers have sought to develop computational models of social cognition, emotion, and theory of mind. We hope that by providing a comprehensive evaluation framework and open dataset, we can encourage further advancements in the development of AI systems that are better equipped to interact effectively with humans across a variety of contexts. Our work represents a starting point toward achieving this goal and we invite feedback from the scientific community as we continue refining and expanding our benchmark over time.",1
"Gaussian processes (GPs) are ubiquitously used in sciences and engineering as metamodels. Standard GPs, however, can only handle numerical or quantitative variables. In this paper, we introduce latent map Gaussian processes (LMGPs) that inherit the attractive properties of GPs and are also applicable to mixed data which have both quantitative and qualitative inputs. The core idea behind LMGPs is to learn a continuous, low-dimensional latent space or manifold which encodes all qualitative inputs. To learn this manifold, we first assign a unique prior vector representation to each combination of qualitative inputs. We then use a low-rank linear map to project these priors on a manifold that characterizes the posterior representations. As the posteriors are quantitative, they can be directly used in any standard correlation function such as the Gaussian or Matern. Hence, the optimal map and the corresponding manifold, along with other hyperparameters of the correlation function, can be systematically learned via maximum likelihood estimation. Through a wide range of analytic and real-world examples, we demonstrate the advantages of LMGPs over state-of-the-art methods in terms of accuracy and versatility. In particular, we show that LMGPs can handle variable-length inputs, have an explainable neural network interpretation, and provide insights into how qualitative inputs affect the response or interact with each other. We also employ LMGPs in Bayesian optimization and illustrate that they can discover optimal compound compositions more efficiently than conventional methods that convert compositions to qualitative variables via manual featurization.",0
"Abstract: Latent map GPs (Gaussian processes) have previously been proposed as a scalable surrogate modeling solution for complex models. By introducing latent variables into GP models they can capture dependencies which would otherwise require very high computational cost, e.g., in computer experiments. In our work we study two scenarios where one type of variable may need another kind of discretization than the other. This is a common problem that often occurs within computer experiments, especially if different submodels rely on distinct mathematical formulations. By extending latent map GPs to handle such mixed discrete/continuous cases we open up new possibilities for surrogating even more realistically but still efficiently. Our findings show good predictive performance across all applications studied here including analytical benchmark problems, real world reliability studies, and thermal simulations, demonstrating their applicability under varying conditions. Furthermore, visualizations confirm the trustworthiness of the novel methodology and demonstrate the importance of correctly capturing conditional relationships through shared latent spaces. We provide code for easy reproducibility to encourage adoption beyond these initial studies. The ultimate goal is to enable decision makers to base decisions upon accurate approximations with minimal computational effort at hand.",1
We formulate a causal extension to the recently introduced paradigm of instance-wise feature selection to explain black-box visual classifiers. Our method selects a subset of input features that has the greatest causal effect on the models output. We quantify the causal influence of a subset of features by the Relative Entropy Distance measure. Under certain assumptions this is equivalent to the conditional mutual information between the selected subset and the output variable. The resulting causal selections are sparser and cover salient objects in the scene. We show the efficacy of our approach on multiple vision datasets by measuring the post-hoc accuracy and Average Causal Effect of selected features on the models output.,0
"In recent years, there has been growing interest in model interpretation as part of artificial intelligence (AI) and machine learning (ML). However, traditional methods have limitations that hinder their effectiveness in explaining complex ML models. To address these challenges, we propose a novel approach called instance-wise causal feature selection (CASFEAT), which identifies individual instances responsible for high predictions. By focusing on specific examples instead of overall patterns, our method provides more accurate explanations without overfitting issues associated with global attribution techniques.  CASFEAT leverages counterfactual reasoning to compute path-specific features contributing to output changes. Instead of using group fairness metrics commonly applied to entire datasets, our framework considers single data points one by one. We evaluate the efficacy of our technique across various scenarios, such as image classification tasks, sentiment analysis problems, and explainability benchmarks involving nonlinear models like decision trees and neural networks. Our results show significant improvements compared to popular baseline approaches in terms of fidelity, sparsity, stability, and interpretability.  Our work advances the field of model interpretation by providing an efficient and effective solution tailored specifically toward complex AI systems. While existing methods struggle to provide reliable explanations for deep learning models, CASFEAT enables practitioners to gain insights into black box algorithms that may otherwise seem mysterious and unaccountable. With enhanced transparency and understanding, our technology can help promote trustworthy, ethical AI practices that benefit society at large.",1
"In ML-aided decision-making tasks, such as fraud detection or medical diagnosis, the human-in-the-loop, usually a domain-expert without technical ML knowledge, prefers high-level concept-based explanations instead of low-level explanations based on model features. To obtain faithful concept-based explanations, we leverage multi-task learning to train a neural network that jointly learns to predict a decision task based on the predictions of a precedent explainability task (i.e., multi-label concepts). There are two main challenges to overcome: concept label scarcity and the joint learning. To address both, we propose to: i) use expert rules to generate a large dataset of noisy concept labels, and ii) apply two distinct multi-task learning strategies combining noisy and golden labels. We compare these strategies with a fully supervised approach in a real-world fraud detection application with few golden labels available for the explainability task. With improvements of 9.26% and of 417.8% at the explainability and decision tasks, respectively, our results show it is possible to improve performance at both tasks by combining labels of heterogeneous quality.",0
"This paper presents an algorithm called weakly supervised multi-task learning (WSMTL) that enables explainability through concept attributions. WSMTL can learn several tasks simultaneously from limited labeled data and unlabeled datasets by sharing knowledge across tasks. For each task, our method outputs a set of concepts that contributed most significantly to prediction outcome. These results can provide users with interpretable explanations about how predictions are made. Our experiments demonstrate state-of-the art performance on benchmark datasets while ensuring better explainability than previous methods based on attention mechanism or feature ablation techniques. The code and dataset are available online to facilitate further research in this area. Overall, WSMTL provides an effective approach to enhance interpretability in machine learning models without increasing computational cost.",1
"The Bluetooth sensor embedded in mobile phones provides an unobtrusive, continuous, and cost-efficient means to capture individuals' proximity information, such as the nearby Bluetooth devices count (NBDC). The continuous NBDC data can partially reflect individuals' behaviors and status, such as social connections and interactions, working status, mobility, and social isolation and loneliness, which were found to be significantly associated with depression by previous survey-based studies. This paper aims to explore the NBDC data's value in predicting depressive symptom severity as measured via the 8-item Patient Health Questionnaire (PHQ-8). The data used in this paper included 2,886 bi-weekly PHQ-8 records collected from 316 participants recruited from three study sites in the Netherlands, Spain, and the UK as part of the EU RADAR-CNS study. From the NBDC data two weeks prior to each PHQ-8 score, we extracted 49 Bluetooth features, including statistical features and nonlinear features for measuring periodicity and regularity of individuals' life rhythms. Linear mixed-effect models were used to explore associations between Bluetooth features and the PHQ-8 score. We then applied hierarchical Bayesian linear regression models to predict the PHQ-8 score from the extracted Bluetooth features. A number of significant associations were found between Bluetooth features and depressive symptom severity. Compared with commonly used machine learning models, the proposed hierarchical Bayesian linear regression model achieved the best prediction metrics, R2= 0.526, and root mean squared error (RMSE) of 3.891. Bluetooth features can explain an extra 18.8% of the variance in the PHQ-8 score relative to the baseline model without Bluetooth features (R2=0.338, RMSE = 4.547).",0
"This study examines whether data collected from nearby Bluetooth devices counts on mobile phones can predict depression severity levels over time. We hypothesized that higher amounts of nearby Bluetooth device use would correlate with increased symptoms of depression. Our preliminary results suggest that this may indeed be the case, although more research is necessary to fully explore these findings. Implications for future research and potential clinical applications of this technology are discussed.",1
"Artificial Intelligence (AI) has made leapfrogs in development across all the industrial sectors especially when deep learning has been introduced. Deep learning helps to learn the behaviour of an entity through methods of recognising and interpreting patterns. Despite its limitless potential, the mystery is how deep learning algorithms make a decision in the first place. Explainable AI (XAI) is the key to unlocking AI and the black-box for deep learning. XAI is an AI model that is programmed to explain its goals, logic, and decision making so that the end users can understand. The end users can be domain experts, regulatory agencies, managers and executive board members, data scientists, users that use AI, with or without awareness, or someone who is affected by the decisions of an AI model. Chest CT has emerged as a valuable tool for the clinical diagnostic and treatment management of the lung diseases associated with COVID-19. AI can support rapid evaluation of CT scans to differentiate COVID-19 findings from other lung diseases. However, how these AI tools or deep learning algorithms reach such a decision and which are the most influential features derived from these neural networks with typically deep layers are not clear. The aim of this study is to propose and develop XAI strategies for COVID-19 classification models with an investigation of comparison. The results demonstrate promising quantification and qualitative visualisations that can further enhance the clinician's understanding and decision making with more granular information from the results given by the learned XAI models.",0
"Artificial intelligence (AI) has been increasingly used in medical imaging applications, such as computerized tomography (CT) scans, to improve efficiency and accuracy in diagnosing diseases like COVID-19. However, there remains a significant need for explainability in these systems, particularly in high-stakes healthcare scenarios where patient outcomes can hinge on algorithmic decisions. In this study, we aimed to address this gap by conducting an initial comparison of two popular AI algorithms - CheXNet and DenseNet - in their ability to generate explanations for CT scan images classified as positive or negative for COVID-19. Our results showed that while both models achieved high levels of accuracy, CheXNet consistently provided better interpretability compared to DenseNet. Specifically, CheXNet was able to identify more specific regions of interest associated with disease presence than DenseNet, suggesting improved transparency in decision making processes. These findings highlight the importance of evaluating explainability in AI systems for COVID-19 diagnosis, and provide a starting point for future research in developing more transparent and interpretable machine learning models in medicine. Overall, our work contributes to a growing field of research exploring the integration of AI into clinical practice while ensuring accountability and understanding of algorithmic predictions.",1
"For feature selection and related problems, we introduce the notion of classification game, a cooperative game, with features as players and hinge loss based characteristic function and relate a feature's contribution to Shapley value based error apportioning (SVEA) of total training error. Our major contribution is ($\star$) to show that for any dataset the threshold 0 on SVEA value identifies feature subset whose joint interactions for label prediction is significant or those features that span a subspace where the data is predominantly lying. In addition, our scheme ($\star$) identifies the features on which Bayes classifier doesn't depend but any surrogate loss function based finite sample classifier does; this contributes to the excess $0$-$1$ risk of such a classifier, ($\star$) estimates unknown true hinge risk of a feature, and ($\star$) relate the stability property of an allocation and negative valued SVEA by designing the analogue of core of classification game. Due to Shapley value's computationally expensive nature, we build on a known Monte Carlo based approximation algorithm that computes characteristic function (Linear Programs) only when needed. We address the potential sample bias problem in feature selection by providing interval estimates for SVEA values obtained from multiple sub-samples. We illustrate all the above aspects on various synthetic and real datasets and show that our scheme achieves better results than existing recursive feature elimination technique and ReliefF in most cases. Our theoretically grounded classification game in terms of well defined characteristic function offers interpretability (which we formalize in terms of final task) and explainability of our framework, including identification of important features.",0
"This paper presents a novel method for selecting features in machine learning models that are interpretable, meaning that their importance can easily be explained to humans. Feature selection is essential in many applications because having more data than necessary can lead to overfitting, decreased accuracy, increased computational cost, and difficulty in interpretation. Existing approaches often suffer from drawbacks such as lack of interpretability, high computation complexity, and poor feature diversity preservation. In response to these challenges, we propose a new feature selection approach called ""Interpretable Feature Subset Selection"" (IFSS), which utilizes the concept of marginal contribution via the Shapley Value. The Shapley Value provides a fair distribution of credit among collaborators based on the marginal contribution of each collaborator's input. Applying this principle to feature selection leads us to formulate IFSS using a cooperative game theory framework to identify the most informative and relevant subset of features while ensuring diversity and interpretability. Our experiments across various datasets demonstrate the effectiveness of our proposed approach compared with state-of-the-art methods in terms of both predictive performance and interpretability measures. We conclude by discussing future directions for improving feature subset selection under different scenarios, including big data environments. Overall, our work contributes to building explainable artificial intelligence models, thus promoting trustworthiness, transparency, and accountability of machine learning systems.",1
"Motivation: Traditional image attribution methods struggle to satisfactorily explain predictions of neural networks. Prediction explanation is important, especially in medical imaging, for avoiding the unintended consequences of deploying AI systems when false positive predictions can impact patient care. Thus, there is a pressing need to develop improved models for model explainability and introspection. Specific problem: A new approach is to transform input images to increase or decrease features which cause the prediction. However, current approaches are difficult to implement as they are monolithic or rely on GANs. These hurdles prevent wide adoption. Our approach: Given an arbitrary classifier, we propose a simple autoencoder and gradient update (Latent Shift) that can transform the latent representation of a specific input image to exaggerate or curtail the features used for prediction. We use this method to study chest X-ray classifiers and evaluate their performance. We conduct a reader study with two radiologists assessing 240 chest X-ray predictions to identify which ones are false positives (half are) using traditional attribution maps or our proposed method. Results: We found low overlap with ground truth pathology masks for models with reasonably high accuracy. However, the results from our reader study indicate that these models are generally looking at the correct features. We also found that the Latent Shift explanation allows a user to have more confidence in true positive predictions compared to traditional approaches (0.15$\pm$0.95 in a 5 point scale with p=0.01) with only a small increase in false positive predictions (0.04$\pm$1.06 with p=0.57).   Accompanying webpage: https://mlmed.org/gifsplanation   Source code: https://github.com/mlmed/gifsplanation",0
"This paper presents ""GiftsPlanation"", a method for generating counterfactual explanations based on medical images such as chest x-rays using latent shift autoencoding. The proposed approach simplifies the process by only employing simple linear models at both encoding and decoding stages of the model while still maintaining high levels of performance compared to other methods. Our experiments demonstrate that GiftsPlanation generates high quality explanations that show meaningful differences from real images while minimizing distortion artifacts. Furthermore, we evaluate our method against state of the art baselines showing improved performance across several metrics including user study preferences. As future work, this paper provides insights into further improvements that can enhance the overall effectiveness and efficiency of automating the generation of counterfactual x-rays.",1
"The latest Deep Learning (DL) models for detection and classification have achieved an unprecedented performance over classical machine learning algorithms. However, DL models are black-box methods hard to debug, interpret, and certify. DL alone cannot provide explanations that can be validated by a non technical audience. In contrast, symbolic AI systems that convert concepts into rules or symbols -- such as knowledge graphs -- are easier to explain. However, they present lower generalisation and scaling capabilities. A very important challenge is to fuse DL representations with expert knowledge. One way to address this challenge, as well as the performance-explainability trade-off is by leveraging the best of both streams without obviating domain expert knowledge. We tackle such problem by considering the symbolic knowledge is expressed in form of a domain expert knowledge graph. We present the eXplainable Neural-symbolic learning (X-NeSyL) methodology, designed to learn both symbolic and deep representations, together with an explainability metric to assess the level of alignment of machine and human expert explanations. The ultimate objective is to fuse DL representations with expert domain knowledge during the learning process to serve as a sound basis for explainability. X-NeSyL methodology involves the concrete use of two notions of explanation at inference and training time respectively: 1) EXPLANet: Expert-aligned eXplainable Part-based cLAssifier NETwork Architecture, a compositional CNN that makes use of symbolic representations, and 2) SHAP-Backprop, an explainable AI-informed training procedure that guides the DL process to align with such symbolic representations in form of knowledge graphs. We showcase X-NeSyL methodology using MonuMAI dataset for monument facade image classification, and demonstrate that our approach improves explainability and performance.",0
"In this work, we propose a novel approach called EXplanatory Neural-Symbolic Learning (X-NeSyL), which leverages the power of neural networks and symbolic reasoning to integrate deep learning representations with domain expertise encoded in knowledge graphs. Our proposed methodology enables efficient fusion of these heterogeneous data sources, enabling a more comprehensive understanding of complex phenomena such as those encountered in digital humanities projects like MonuMAI, where vast amounts of multimedia information must be analyzed in real time. We demonstrate the effectiveness of X-NeSyL on several challenging tasks related to image classification and retrieval, video analysis, and natural language processing, illustrating how our framework can effectively enhance traditional machine learning pipelines with both structured domain knowledge and unstructured, fine-grained features from learned deep models. By providing detailed insights into the process by which decisions are made across multiple modalities, our results highlight the potential benefits of X-NeSyL, including improved transparency and interpretability, while maintaining state-of-the-art performance on demanding problems in cultural heritage research. Ultimately, X-NeSyL represents a promising direction toward achieving artificial intelligence that is at once explainable, accessible to non-experts, and competitive with black box approaches.",1
"Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.",0
"Title: ""Adversarial Training Helps Neural Networks Generalize"" ------------------ Abstract: ------------ This work investigates how adversarial training can improve the performance of neural networks on image classification tasks. By introducing noise into the input data during training, the model learns to better generalize across different environments. We evaluate several variants of adversarial training using standard metrics such as accuracy, precision, recall, F1 score, and area under the ROC curve (AUC). Our results demonstrate that models trained with adversarial examples significantly outperform those without it. This improvement holds true even for large datasets like CIFAR-10/100 and SVHN. Additionally, we show that transfer learning with pretrained models on smaller datasets further boosts performance. These findings have important implications for deep learning practitioners seeking more robust models with stronger generalization capabilities. Overall, our study provides evidence that incorporating adversarial training in deep learning pipelines leads to better results across various benchmarks.",1
"Classical regression has a simple geometric description in terms of a projection of the training labels onto the column space of the design matrix. However, for over-parameterized models -- where the number of fit parameters is large enough to perfectly fit the training data -- this picture becomes uninformative. Here, we present an alternative geometric interpretation of regression that applies to both under- and over-parameterized models. Unlike the classical picture which takes place in the space of training labels, our new picture resides in the space of input features. This new feature-based perspective provides a natural geometric interpretation of the double-descent phenomenon in the context of bias and variance, explaining why it can occur even in the absence of label noise. Furthermore, we show that adversarial perturbations -- small perturbations to the input features that result in large changes in label values -- are a generic feature of biased models, arising from the underlying geometry. We demonstrate these ideas by analyzing three minimal models for over-parameterized linear least squares regression: without basis functions (input features equal model features) and with linear or nonlinear basis functions (two-layer neural networks with linear or nonlinear activation functions, respectively).",0
"Abstract: In recent years, deep learning has been achieving state-of-the-art results across various fields such as image recognition, natural language processing, speech recognition, etc., due to over-parameterization. However, despite their impressive performance, these models can still be fooled by adversarial examples which are carefully crafted inputs that cause models to make mistakes. The reason behind their vulnerability lies in the fact that traditional regularizations techniques used during training cannot prevent coercive behavior in over-parameterized models. This paper studies how geometry plays a crucial role in understanding both over-parameterization and adversarial perturbation phenomena. We use tools from differential geometry to establish connections between curvature properties of data manifolds and model parameters, showing that adversarial examples arise precisely where locally linear regions abruptly meet nonlinear ones under smoothness constraints. Moreover, we propose novel geometric regularizers derived directly from local differential characteristics of the data manifold to overcome limitations of standard regulations methods. Empirical evaluations on popular benchmark datasets show significant improvement against adversarial attacks while maintaining high test accuracy for the unperturbed cases. These findings provide new insights into why and how over-parameterized neural networks generalize well but are susceptible to small input modifications. Our work bridges geometrical theory of function spaces with deep learning practice to advance the robustness of machine learning algorithms under adversarial settings. Keywords: Deep Learning, Over-Parameterizatio",1
"Estimating causal effects from randomized experiments is central to clinical research. Reducing the statistical uncertainty in these analyses is an important objective for statisticians. Registries, prior trials, and health records constitute a growing compendium of historical data on patients under standard-of-care conditions that may be exploitable to this end. However, most methods for historical borrowing achieve reductions in variance by sacrificing strict type-I error rate control. Here, we propose a use of historical data that exploits linear covariate adjustment to improve the efficiency of trial analyses without incurring bias. Specifically, we train a prognostic model on the historical data, then estimate the treatment effect using a linear regression while adjusting for the trial subjects' predicted outcomes (their prognostic scores). We prove that, under certain conditions, this prognostic covariate adjustment procedure attains the minimum variance possible among a large class of estimators. When those conditions are not met, prognostic covariate adjustment is still more efficient than raw covariate adjustment and the gain in efficiency is proportional to a measure of the predictive accuracy of the prognostic model. We demonstrate the approach using simulations and a reanalysis of an Alzheimer's Disease clinical trial and observe meaningful reductions in mean-squared error and the estimated variance. Lastly, we provide a simplified formula for asymptotic variance that enables power and sample size calculations that account for the gains from the prognostic model for clinical trial design. Sample size reductions between 10% and 30% are attainable when using prognostic models that explain a clinically realistic percentage of the outcome variance.",0
"This paper describes a novel method for improving the accuracy of randomized trials by incorporating a linear adjustment based on a previously established prognostic score. We demonstrate that this approach leads to more precise treatment effect estimates compared to traditional methods and can potentially reduce sample size requirements. Our findings have important implications for medical researchers who rely on clinical trials as their primary source of evidence. Ultimately, our proposed method represents an advance towards higher quality patient care through better informed decision making. Abstract: Clinical trials are essential tools in modern medicine for establishing the efficacy and safety of new treatments. However, they often suffer from imprecision due to unmeasured confounding factors. Prognostic scores, which predict outcomes based on readily available baseline data, offer promising opportunities to improve precision through post hoc regression modeling. Here we present a simple linear strategy for adjusting treatment effects estimated in randomized controlled trials using pretrial calibration curves derived from existing prognostic scores. Results from simulation studies show increased power over standard analysis techniques while maintaining nominal type I error rates in testing the null hypothesis. These results suggest potential gains in efficiency without inflation of trial sizes, leading to savings in time and resources. As an illustrative example, we apply our methodology to a large scale cardiovascular study and compare the inferences drawn from conventional versus prognostic score adjusted analyses. By exploiting routine patient data already collected at trial enrollment, our framework provides a scalable solution to enhance the credibility of evidence informing evidence-based practice in medical research.",1
"Convolutional neural networks utilize a hierarchy of neural network layers. The statistical aspects of information concentration in successive layers can bring an insight into the feature abstraction process. We analyze the saliency maps of these layers from the perspective of semiotics, also known as the study of signs and sign-using behavior. In computational semiotics, this aggregation operation (known as superization) is accompanied by a decrease of spatial entropy: signs are aggregated into supersign. Using spatial entropy, we compute the information content of the saliency maps and study the superization processes which take place between successive layers of the network. In our experiments, we visualize the superization process and show how the obtained knowledge can be used to explain the neural decision model. In addition, we attempt to optimize the architecture of the neural model employing a semiotic greedy technique. To the extent of our knowledge, this is the first application of computational semiotics in the analysis and interpretation of deep neural networks.",0
"This should be a standalone document that provides an overview of your paper. It should briefly summarize the content of the paper, including the main contributions and conclusions you reach. Please note that any figures referred to in the abstract must be visible elsewhere in the paper (either as figures or supplementary material). Additionally, please ensure that your abstract conforms to the general guidelines listed below: * maximum length of 300 words; * text only - no images or other types of multimedia; * fully self-contained and comprehensible on its own; * written in third person point of view, except when referring to established results or generally accepted knowledge in a specific field; * objective tone throughout, and written without use of contractions (e.g., ""is"" instead of ""it's""); and * includes keywords relevant to the contents of the article, placed after the abstract conclusion statement (if present), enclosed in square brackets [].",1
"Deep Learning has become a very valuable tool in different fields, and no one doubts the learning capacity of these models. Nevertheless, since Deep Learning models are often seen as black boxes due to their lack of interpretability, there is a general mistrust in their decision-making process. To find a balance between effectiveness and interpretability, Explainable Artificial Intelligence (XAI) is gaining popularity in recent years, and some of the methods within this area are used to generate counterfactual explanations. The process of generating these explanations generally consists of solving an optimization problem for each input to be explained, which is unfeasible when real-time feedback is needed. To speed up this process, some methods have made use of autoencoders to generate instant counterfactual explanations. Recently, a method called Deep Guided Counterfactual Explanations (DGCEx) has been proposed, which trains an autoencoder attached to a classification model, in order to generate straightforward counterfactual explanations. However, this method does not ensure that the generated counterfactual instances are close to the data manifold, so unrealistic counterfactual instances may be generated. To overcome this issue, this paper presents Distribution Aware Deep Guided Counterfactual Explanations (DA-DGCEx), which adds a term to the DGCEx cost function that penalizes out of distribution counterfactual instances.",0
"This study addresses the issue of ensuring validity in deep guided counterfactual explanations (DA-DGEX) using distribution-aware autoencoder loss. Previous approaches have relied on heuristics such as sensitivity analysis or feature importance metrics that may produce invalid explanations due to overfitting or lack of robustness. We propose a new method based on training DA-DGEX models with an adversarial regularization term added to their reconstruction loss. The additional loss encourages the model to generate more diverse and plausible counterfactuals, which leads to higher fidelity explanations. Our experiments show significant improvements compared to state-of-the-art methods across multiple datasets and use cases. The proposed approach sets a higher standard for trustworthy explainability in machine learning systems.",1
"An important pillar for safe machine learning (ML) is the systematic mitigation of weaknesses in neural networks to afford their deployment in critical applications. An ubiquitous class of safety risks are learned shortcuts, i.e. spurious correlations a network exploits for its decisions that have no semantic connection to the actual task. Networks relying on such shortcuts bear the risk of not generalizing well to unseen inputs. Explainability methods help to uncover such network vulnerabilities. However, many of these techniques are not directly applicable if access to the network is constrained, in so-called black-box setups. These setups are prevalent when using third-party ML components. To address this constraint, we present an approach to detect learned shortcuts using an interpretable-by-design network as a proxy to the black-box model of interest. Leveraging the proxy's guarantees on introspection we automatically extract candidates for learned shortcuts. Their transferability to the black box is validated in a systematic fashion. Concretely, as proxy model we choose a BagNet, which bases its decisions purely on local image patches. We demonstrate on the autonomous driving dataset A2D2 that extracted patch shortcuts significantly influence the black box model. By efficiently identifying such patch-based vulnerabilities, we contribute to safer ML models.",0
"Title should be ""Patching Shortcuts""",1
"In medical image analysis, the cost of acquiring high-quality data and their annotation by experts is a barrier in many medical applications. Most of the techniques used are based on supervised learning framework and need a large amount of annotated data to achieve satisfactory performance. As an alternative, in this paper, we propose a self-supervised learning approach to learn the spatial anatomical representations from the frames of magnetic resonance (MR) video clips for the diagnosis of knee medical conditions. The pretext model learns meaningful spatial context-invariant representations. The downstream task in our paper is a class imbalanced multi-label classification. Different experiments show that the features learnt by the pretext model provide explainable performance in the downstream task. Moreover, the efficiency and reliability of the proposed pretext model in learning representations of minority classes without applying any strategy towards imbalance in the dataset can be seen from the results. To the best of our knowledge, this work is the first work of its kind in showing the effectiveness and reliability of self-supervised learning algorithms in class imbalanced multi-label classification tasks on MR video.   The code for evaluation of the proposed work is available at https://github.com/sadimanna/sslm",0
"This paper presents SSLM (Self-Supervised Learning for Medical Diagnosis), a novel approach that uses self-supervised learning techniques to train deep neural networks on large datasets of MRI scans. The goal of this research is to develop a method capable of accurately diagnosing conditions such as Alzheimerâ€™s disease, multiple sclerosis, stroke and tumors using only video sequences obtained from MR images without any manual labeling. The proposed algorithm is designed to learn meaningful features directly from raw image data by leveraging temporal consistency constraints and prior knowledge derived from expert annotations available in public databases. Experimental evaluation shows significant improvement over unsupervised baselines achieving performance comparable to supervised methods while providing better robustness against changes in imaging protocols, vendor and acquisition parameters. The proposed model opens up new possibilities in medical imaging analysis by reducing costly human annotation labor, improving scalability and enabling integration of diverse, previously unusable clinical data sources for diagnostic predictions.",1
"Human gaze is known to be an intention-revealing signal in human demonstrations of tasks. In this work, we use gaze cues from human demonstrators to enhance the performance of agents trained via three popular imitation learning methods -- behavioral cloning (BC), behavioral cloning from observation (BCO), and Trajectory-ranked Reward EXtrapolation (T-REX). Based on similarities between the attention of reinforcement learning agents and human gaze, we propose a novel approach for utilizing gaze data in a computationally efficient manner, as part of an auxiliary loss function, which guides a network to have higher activations in image regions where the human's gaze fixated. This work is a step towards augmenting any existing convolutional imitation learning agent's training with auxiliary gaze data. Our auxiliary coverage-based gaze loss (CGL) guides learning toward a better reward function or policy, without adding any additional learnable parameters and without requiring gaze data at test time. We find that our proposed approach improves the performance by 95% for BC, 343% for BCO, and 390% for T-REX, averaged over 20 different Atari games. We also find that compared to a prior state-of-the-art imitation learning method assisted by human gaze (AGIL), our method achieves better performance, and is more efficient in terms of learning with fewer demonstrations. We further interpret trained CGL agents with a saliency map visualization method to explain their performance. At last, we show that CGL can help alleviate a well-known causal confusion problem in imitation learning.",0
"Humans communicate through vision by sharing their gaze. This communication channel enables humans to pass on valuable information such as where they are looking at, what they have found interesting, and whether something has caught their attention. By leveraging human gaze, we can efficiently guide imitation learning agents, helping them focus on important aspects of a task and learn from human experts more effectively. In our work, we present a method that translates human gaze into textual feedback and integrates it seamlessly into the learning process of imitation learning agents. Our approach outperforms existing methods both quantitatively and qualitatively while being more efficient and easier to interpret. We demonstrate its effectiveness in multiple domains including robotics and computer graphics and showcase its potential for enhancing human-AI collaboration. Our findings contribute to a better understanding of how humans and machines can interact and collaborate effectively using shared visual representations.",1
"Why can the Expectation-Maximization (EM) algorithm for mixture models converge? Why can different initial parameters cause various convergence difficulties? The Q-L synchronization theory explains that the observed data log-likelihood L and the complete data log-likelihood Q are positively correlated; we can achieve maximum L by maximizing Q. According to this theory, the Deterministic Annealing EM (DAEM) algorithm's authors make great efforts to eliminate locally maximal Q for avoiding L's local convergence. However, this paper proves that in some cases, Q may and should decrease for L to increase; slow or local convergence exists only because of small samples and unfair competition. This paper uses marriage competition to explain different convergence difficulties and proposes the Fair Competition Principle (FCP) with an initialization map for improving initializations. It uses the rate-verisimilitude function, extended from the rate-distortion function, to explain the convergence of the EM and improved EM algorithms. This convergence proof adopts variational and iterative methods that Shannon et al. used for analyzing rate-distortion functions. The initialization map can vastly save both algorithms' running times for binary Gaussian mixtures. The FCP and the initialization map are useful for complicated mixtures but not sufficient; we need further studies for specific methods.",0
"Title: Using fair competition principle (FCP) and rate-versimilitude function (RVf) to accelerate convergence in the expectation maximization (EM) algorithm. Abstract: One major challenge faced by researchers using the expectation maximization (EM) algorithm is slow convergence. This paper proposes two novel techniques to significantly improve the speed of convergence in the EM algorithm. The first technique involves incorporating a fair competition principle into the EM algorithm. By ensuring that different components of the algorithm have equal opportunities to update model parameters, the FCP technique helps prevent certain components from dominating the optimization process, leading to faster convergence. The second technique proposed in this paper is the use of a rate-versimilitude function as a stopping criterion for the EM algorithm. By monitoring the change in log-likelihood during each iteration of the algorithm, the RVf approach provides a more accurate indication of whether the current parameter estimates are near their optimal value. By combining these two techniques, we show through simulations on several real datasets that the EM algorithm can converge up to an order of magnitude faster than the standard implementation of the algorithm. Our results demonstrate the effectiveness of our approaches and suggest that they could greatly benefit applications involving machine learning and data analysis where fast convergence is essential.",1
"Multi-label classification is a type of classification task, it is used when there are two or more classes, and the data point we want to predict may belong to none of the classes or all of them at the same time. In the real world, many applications are actually multi-label involved, including information retrieval, multimedia content annotation, web mining, and so on. A game theory-based framework known as SHapley Additive exPlanations (SHAP) has been applied to explain various supervised learning models without being aware of the exact model. Herein, this work further extends the explanation of multi-label classification task by using the SHAP methodology. The experiment demonstrates a comprehensive comparision of different algorithms on well known multi-label datasets and shows the usefulness of the interpretation.",0
"Multilabel classification models can take on many labels simultaneously, providing more information than binary classifiers. To interpret these models, Shapley Values provide insight into how each feature affects the model predictions across all possible label combinations. This paper outlines methods utilizing Shapley Values for interpreting multi-label models trained on simulated datasets as well as real world examples from clinical medicine. By examining both accuracy and predicted probability distributions, we demonstrate that traditional interpretation techniques may miss key insights gained by considering multiple outputs together. We hope that our work will encourage further research into understanding complex relationships within multi-labeled data sets and improving medical decision support systems through advanced modeling and interpretation techniques.",1
"Video action recognition (VAR) is a primary task of video understanding, and untrimmed videos are more common in real-life scenes. Untrimmed videos have redundant and diverse clips containing contextual information, so sampling dense clips is essential. Recently, some works attempt to train a generic model to select the N most representative clips. However, it is difficult to model the complex relations from intra-class clips and inter-class videos within a single model and fixed selected number, and the entanglement of multiple relations is also hard to explain. Thus, instead of ""only look once"", we argue ""divide and conquer"" strategy will be more suitable in untrimmed VAR. Inspired by the speed reading mechanism, we propose a simple yet effective clip-level solution based on skim-scan techniques. Specifically, the proposed Skim-Scan framework first skims the entire video and drops those uninformative and misleading clips. For the remaining clips, it scans clips with diverse features gradually to drop redundant clips but cover essential content. The above strategies can adaptively select the necessary clips according to the difficulty of the different videos. To trade off the computational complexity and performance, we observe the similar statistical expression between lightweight and heavy networks, thus it supports us to explore the combination of them. Comprehensive experiments are performed on ActivityNet and mini-FCVID datasets, and results demonstrate that our solution surpasses the state-of-the-art performance in terms of both accuracy and efficiency.",0
"This paper presents a novel method for action recognition in untrimmed videos by utilizing a combination of video skimming and object scanning techniques. Our approach first skims through the entire video sequence to identify candidate segments that contain actions. We then apply an object detection algorithm on these candidate segments to scan objects within the scene and detect those objects that perform actions. By using a two-stage approach, we reduce computational cost while still achieving high accuracy for action recognition in large datasets. We demonstrate our method's effectiveness through experimental results on popular benchmark datasets, outperforming state-of-the-art methods.  ---  Okay! Let me know if you need any more help. I'm here to assist.",1
"As the request for deep learning solutions increases, the need for explainability is even more fundamental. In this setting, particular attention has been given to visualization techniques, that try to attribute the right relevance to each input pixel with respect to the output of the network. In this paper, we focus on Class Activation Mapping (CAM) approaches, which provide an effective visualization by taking weighted averages of the activation maps. To enhance the evaluation and the reproducibility of such approaches, we propose a novel set of metrics to quantify explanation maps, which show better effectiveness and simplify comparisons between approaches. To evaluate the appropriateness of the proposal, we compare different CAM-based visualization methods on the entire ImageNet validation set, fostering proper comparisons and reproducibility.",0
"This paper aims at revisiting class activation mapping (CAM), a popular method for explaining deep neural networks predictions. We introduce novel metrics that allow for a more accurate and complete understanding of how CAM attributions relate to human intuitions of explanation quality. Our experiments demonstrate significant correlation between these new metrics and human judgements, as well as other established evaluation measures. Furthermore, we highlight scenarios where current methods fail and provide insights into their limitations. Overall, our findings provide guidance on best practices for using CAM and related techniques in explainability research. By making explicit connections between explainability methods and human expectations, this work takes important steps towards demystifying the â€œblack boxâ€ nature of modern machine learning models.",1
"In this study, we devise a model that introduces two hierarchies into information entropy. The two hierarchies are the size of the region for which entropy is calculated and the size of the component that determines whether the structures in the image are integrated or not. And this model uses two indicators, hierarchical entropy and domain interaction. Both indicators increase or decrease due to the integration or fragmentation of the structure in the image. It aims to help people interpret and explain what the structure in an image looks like from two indicators that change with the size of the region and the component. First, we conduct experiments using images and qualitatively evaluate how the two indicators change. Next, we explain the relationship with the hidden structure of Vermeer's girl with a pearl earring using the change of hierarchical entropy. Finally, we clarify the relationship between the change of domain interaction and the appropriate segment result of the image by an experiment using a questionnaire.",0
"This study aims to explore the relationship between hierarchical entropy and domain interaction in understanding the structure of images. Previous research has shown that both entropy and domain interaction play important roles in image processing, but their interplay remains largely unexplored. Here we propose a novel approach that combines these two concepts to gain insights into the underlying structural organization of images. Our results demonstrate that by analyzing the variation in information content at different scales (hierarchical entropy), along with interactions among different domains (such as intensity values or color channels), we can accurately capture the characteristics of complex visual patterns. We validate our method using several benchmark datasets and compare its performance against state-of-the-art techniques in image analysis. Overall, our findings contribute new knowledge on how entropy and domain interaction interact to shape the appearance of images, and have implications for a range of applications such as computer vision, machine learning, and digital imaging.",1
"A popular approach to model compression is to train an inexpensive student model to mimic the class probabilities of a highly accurate but cumbersome teacher model. Surprisingly, this two-step knowledge distillation process often leads to higher accuracy than training the student directly on labeled data. To explain and enhance this phenomenon, we cast knowledge distillation as a semiparametric inference problem with the optimal student model as the target, the unknown Bayes class probabilities as nuisance, and the teacher probabilities as a plug-in nuisance estimate. By adapting modern semiparametric tools, we derive new guarantees for the prediction error of standard distillation and develop two enhancements -- cross-fitting and loss correction -- to mitigate the impact of teacher overfitting and underfitting on student performance. We validate our findings empirically on both tabular and image data and observe consistent improvements from our knowledge distillation enhancements.",0
"This paper develops a new methodology called knowledge distillation which unifies semiparametric inference and deep neural networks via nonparametric optimization on manifolds that generalize linear spaces. This allows us to design flexible models that incorporate many types of prior information in a principled manner while retaining efficiency. Our approach is motivated by the idea that high quality prior information should take the form of simple, interpretable mathematical expressions which can guide efficient algorithms towards better solutions. These expressions may correspond either to exact knowledge about the underlying generative process (e.g., conjugacy priors) or more pragmatic approximations thereof derived from domain specific expertise. We establish theoretical properties regarding existence, uniqueness, algorithmic computationality, global convergence guarantees under favorable conditions as well as fast local convergence in overparameterized regimes even without strong convexity assumptions. An extensive empirical evaluation on diverse benchmark problems demonstrates that our framework outperforms competitive alternatives. Finally we discuss connections of our work to related literature.",1
"In this article, we developed and analyzed a thresholding method in which soft thresholding estimators are independently expanded by empirical scaling values. The scaling values have a common hyper-parameter that is an order of expansion of an ideal scaling value that achieves hard thresholding. We simply call this estimator a scaled soft thresholding estimator. The scaled soft thresholding is a general method that includes the soft thresholding and non-negative garrote as special cases and gives an another derivation of adaptive LASSO. We then derived the degree of freedom of the scaled soft thresholding by means of the Stein's unbiased risk estimate and found that it is decomposed into the degree of freedom of soft thresholding and the reminder connecting to hard thresholding. In this meaning, the scaled soft thresholding gives a natural bridge between soft and hard thresholding methods. Since the degree of freedom represents the degree of over-fitting, this result implies that there are two sources of over-fitting in the scaled soft thresholding. The first source originated from soft thresholding is determined by the number of un-removed coefficients and is a natural measure of the degree of over-fitting. We analyzed the second source in a particular case of the scaled soft thresholding by referring a known result for hard thresholding. We then found that, in a sparse, large sample and non-parametric setting, the second source is largely determined by coefficient estimates whose true values are zeros and has an influence on over-fitting when threshold levels are around noise levels in those coefficient estimates. In a simple numerical example, these theoretical implications has well explained the behavior of the degree of freedom. Moreover, based on the results here and some known facts, we explained the behaviors of risks of soft, hard and scaled soft thresholding methods.",0
"This paper bridges the gap between soft thresholding (SHT) and hard thresholding (HT). HT has been widely used in image compression due to its simple implementation and good performance but fails to perform well on images with noise and impulse signals which results from quantization errors. Soft thresholding improves HTâ€™s robustness towards impulse noise at the cost of losing some of the computational simplicity that makes HT so appealing. In order to bridge the gap between these two methods, we propose scaling as a simple yet effective method to improve SHTâ€™s stability while maintaining most of its desirable properties. By utilizing scaling, our approach can achieve state-of-the-art reconstruction quality even when compared against deep learning based denoising methods.",1
"Even though machine learning algorithms already play a significant role in data science, many current methods pose unrealistic assumptions on input data. The application of such methods is difficult due to incompatible data formats, or heterogeneous, hierarchical or entirely missing data fragments in the dataset. As a solution, we propose a versatile, unified framework called `HMill' for sample representation, model definition and training. We review in depth a multi-instance paradigm for machine learning that the framework builds on and extends. To theoretically justify the design of key components of HMill, we show an extension of the universal approximation theorem to the set of all functions realized by models implemented in the framework. The text also contains a detailed discussion on technicalities and performance improvements in our implementation, which is published for download under the MIT License. The main asset of the framework is its flexibility, which makes modelling of diverse real-world data sources with the same tool possible. Additionally to the standard setting in which a set of attributes is observed for each object individually, we explain how message-passing inference in graphs that represent whole systems of objects can be implemented in the framework. To support our claims, we solve three different problems from the cybersecurity domain using the framework. The first use case concerns IoT device identification from raw network observations. In the second problem, we study how malicious binary files can be classified using a snapshot of the operating system represented as a directed graph. The last provided example is a task of domain blacklist extension through modelling interactions between entities in the network. In all three problems, the solution based on the proposed framework achieves performance comparable to specialized approaches.",0
"This paper presents a novel approach for modeling entity interactions within complex heterogeneous networks (CHNs) using advanced graph theory methods. CHNs are ubiquitous across many domains, including social media platforms, financial markets, and biological systems, among others. These networks comprise multiple types of entities interconnected via different relationships or edges representing various forms of interactions. Understanding how these entities interact can provide valuable insights into network behavior and dynamics that can inform decision making processes for domain experts. However, traditional graph theory approaches have limited capabilities for capturing multi-relational data as they usually rely on simple edge weighting schemes for aggregating relations between nodes. To address these shortcomings, we develop a comprehensive framework based on the Hyperbolic Geometry (HG), which naturally models diversity in relational data by accounting for their inherent hierarchies. Our proposed methodology involves two main stages; i) hyperbolizing the original network space to create non-Euclidean coordinates that capture both node proximities and edge weights, ii) building a multi-layer Graph Convolutional Neural Network (GCNN) model trained on the transformed dataset that learns embeddings capturing entity traits in high dimensional spaces allowing for accurate inference tasks such as link prediction and community detection. We demonstrate through extensive experiments on several real-world datasets from diverse fields that our approach outperforms state-of-the-art baselines in terms of accuracy while providing interpretable results, validating our contribution towards advancing research related to the analysis of CHNs.",1
"Virtually all state-of-the-art methods for training supervised machine learning models are variants of SGD enhanced with a number of additional tricks, such as minibatching, momentum, and adaptive stepsizes. One of the tricks that works so well in practice that it is used as default in virtually all widely used machine learning software is {\em random reshuffling (RR)}. However, the practical benefits of RR have until very recently been eluding attempts at being satisfactorily explained using theory. Motivated by recent development due to Mishchenko, Khaled and Richt\'{a}rik (2020), in this work we provide the first analysis of SVRG under Random Reshuffling (RR-SVRG) for general finite-sum problems. First, we show that RR-SVRG converges linearly with the rate $\mathcal{O}(\kappa^{3/2})$ in the strongly-convex case, and can be improved further to $\mathcal{O}(\kappa)$ in the big data regime (when $n  \mathcal{O}(\kappa)$), where $\kappa$ is the condition number. This improves upon the previous best rate $\mathcal{O}(\kappa^2)$ known for a variance reduced RR method in the strongly-convex case due to Ying, Yuan and Sayed (2020). Second, we obtain the first sublinear rate for general convex problems. Third, we establish similar fast rates for Cyclic-SVRG and Shuffle-Once-SVRG. Finally, we develop and analyze a more general variance reduction scheme for RR, which allows for less frequent updates of the control variate. We corroborate our theoretical results with suitably chosen experiments on synthetic and real datasets.",0
"Inform me if you need any more details about the paper before starting your work on my paper outline! Thank you so much! Hereâ€™s a draft summary that I can provide. In the world of data analysis, random reshuffling has become a popular technique for generating new synthetic datasets with similar statistics as real ones. However, many existing results have focused on worst case scenarios where all samples may be resampled infinitely often. This overlooks important settings like bootstrapping, permutation testing, cross validation, among others, which only sample finite times. To address this gap, we propose novel methods for variance reduction via subsampling during reshuffling. Our approach works by sampling k < n rows uniformly at random from each chunk (window) after every time we perform reshuffling, reducing dependency across different windows of rows. As we show through both theory and experiments on diverse real and synthetic datasets, our algorithm leads to significantly improved convergence rates over classic shuffle methods while still maintaining a low memory footprint. We further prove that our subsampling scheme provably reduces variances under mild conditions, matching our experimental findings and providing theoretical insights into why our method outperforms alternative solutions. Finally, we provide Python code freely available online (under Apache-2 license), enabling practitioners in science, engineering, social sciences, etc., to leverage these advancements directly into their research workflows today.",1
"Graph neural networks have achieved state-of-the-art accuracy for graph node classification. However, GNNs are difficult to scale to large graphs, for example frequently encountering out-of-memory errors on even moderate size graphs. Recent works have sought to address this problem using a two-stage approach, which first aggregates data along graph edges, then trains a classifier without using additional graph information. These methods can run on much larger graphs and are orders of magnitude faster than GNNs, but achieve lower classification accuracy. We propose a novel two-stage algorithm based on a simple but effective observation: we should first train a classifier then aggregate, rather than the other way around. We show our algorithm is faster and can handle larger graphs than existing two-stage algorithms, while achieving comparable or higher accuracy than popular GNNs. We also present a theoretical basis to explain our algorithm's improved accuracy, by giving a synthetic nonlinear dataset in which performing aggregation before classification actually decreases accuracy compared to doing classification alone, while our classify then aggregate approach substantially improves accuracy compared to classification alone.",0
"In recent years, node classification has become increasingly important due to the growth of large graph datasets across many domains. While several algorithms have been developed to tackle node classification, they often suffer from limited scalability or accuracy. To address these challenges, we propose a new algorithm called SAS (Simple, Accurate and Scalable) which combines simplicity, accuracy and scalability into one framework that can handle large graphs effectively. Our approach leverages node attributes as well as structural features, making it more robust than existing methods. Extensive experiments conducted on six benchmark datasets show that our method achieves state-of-the-art performance while maintaining computational efficiency. Overall, our work represents a significant contribution towards developing effective node classification solutions for big data era.",1
"In this paper, we propose a new global analysis framework for a class of low-rank matrix recovery problems on the Riemannian manifold. We analyze the global behavior for the Riemannian optimization with random initialization. We use the Riemannian gradient descent algorithm to minimize a least squares loss function, and study the asymptotic behavior as well as the exact convergence rate. We reveal a previously unknown geometric property of the low-rank matrix manifold, which is the existence of spurious critical points for the simple least squares function on the manifold. We show that under some assumptions, the Riemannian gradient descent starting from a random initialization with high probability avoids these spurious critical points and only converges to the ground truth in nearly linear convergence rate, i.e. $\mathcal{O}(\text{log}(\frac{1}{\epsilon})+ \text{log}(n))$ iterations to reach an $\epsilon$-accurate solution. We use two applications as examples for our global analysis. The first one is a rank-1 matrix recovery problem. The second one is a generalization of the Gaussian phase retrieval problem. It only satisfies the weak isometry property, but has behavior similar to that of the first one except for an extra saddle set. Our convergence guarantee is nearly optimal and almost dimension-free, which fully explains the numerical observations. The global analysis can be potentially extended to other data problems with random measurement structures and empirical least squares loss functions.",0
"This paper presents a novel method for low-rank matrix recovery using gradient descent on a nonlinear manifold known as the Stiefel Manifold. We prove that under certain assumptions, our algorithm converges to the global optimum at worst linearly with high probability if initialized randomly from the uniform distribution over the initialization set. Our approach combines insights from previous works in both deterministic and randomized settings, but achieves faster convergence rates than any existing methods without sacrificing computational efficiency. Experiments demonstrate that our algorithm outperforms state-of-the-art techniques across multiple datasets and parameter regimes, making it a promising tool for low-rank matrix completion problems arising in many fields such as signal processing, computer vision, and machine learning.",1
"Generalization beyond a training dataset is a main goal of machine learning, but theoretical understanding of generalization remains an open problem for many models. The need for a new theory is exacerbated by recent observations in deep neural networks where overparameterization leads to better performance, contradicting the conventional wisdom from classical statistics. In this paper, we investigate generalization error for kernel regression, which, besides being a popular machine learning method, also includes infinitely overparameterized neural networks trained with gradient descent. We use techniques from statistical mechanics to derive an analytical expression for generalization error applicable to any kernel or data distribution. We present applications of our theory to real and synthetic datasets, and for many kernels including those that arise from training deep neural networks in the infinite-width limit. We elucidate an inductive bias of kernel regression to explain data with ""simple functions"", which are identified by solving a kernel eigenfunction problem on the data distribution. This notion of simplicity allows us to characterize whether a kernel is compatible with a learning task, facilitating good generalization performance from a small number of training examples. We show that more data may impair generalization when noisy or not expressible by the kernel, leading to non-monotonic learning curves with possibly many peaks. To further understand these phenomena, we turn to the broad class of rotation invariant kernels, which is relevant to training deep neural networks in the infinite-width limit, and present a detailed mathematical analysis of them when data is drawn from a spherically symmetric distribution and the number of input dimensions is large.",0
"Title: Understanding Generalization in Machine Learning Models through Spectral Analysis  Machine learning models have been shown to exhibit impressive generalization capabilities across diverse tasks. Recent research has aimed at explaining these performance gains by investigating the relationship between model complexity and generalization accuracy. However, there remains a need for a comprehensive understanding of how machine learning algorithms, including deep neural networks, are able to generalize beyond their training data. In this work, we propose that spectral bias and task-model alignment can provide valuable insights into the mechanisms underlying successful generalization.  Firstly, we analyze the impact of spectral bias on kernel regression models and observe that high bias values lead to increased generalization abilities. Our findings suggest that regularizing over more complex functions helps prevent underfitting, enabling better generalization performance. Next, we extend our analysis to infinitely wide neural networks, where we explore the role of task-model alignment. We show that high-dimensional spaces often contain low-dimensional manifolds along which most training samples lie. When the network's eigenvectors align closely with the submanifold containing the training examples, good generalization occurs. These results emphasize the importance of studying both local geometry (through the spectrum) and global structure (via linear separability).  Our contributions offer novel perspectives on generalization behavior in machine learning models. By demonstrating connections between spectral properties, task-model alignment, and generalization performance, we provide new theoretical frameworks for understanding and improving existing methods as well as designing future architectures capable of superior generalization. Overall, this work contributes to building a deeper understanding of why certain types of models perform better than others and points towards potential avenues for further investigation.  Title: Spectral Bias and Task-Model Alignment Explain Generalizatio",1
"Two key challenges within Reinforcement Learning involve improving (a) agent learning within environments with sparse extrinsic rewards and (b) the explainability of agent actions. We describe a curious subgoal focused agent to address both these challenges. We use a novel method for curiosity produced from a Generative Adversarial Network (GAN) based model of environment transitions that is robust to stochastic environment transitions. Additionally, we use a subgoal generating network to guide navigation. The explainability of the agent's behavior is increased by decomposing complex tasks into a sequence of interpretable subgoals that do not require any manual design. We show that this method also enables the agent to solve challenging procedurally-generated tasks that contain stochastic transitions above other state-of-the-art methods.",0
"Incorporating curiosity into reinforcement learning has been shown to improve the performance and efficiency of agents trained under limited data budgets, but most current formulations of curiosity rely on heuristics like novelty seeking that may lead to unintended behaviors and lack interpretability. This paper proposes a new approach to incorporate interpretability and explainability into curiosity by formulating sub-goals as interpretable tasks that guide exploration towards achieving higher goal utility. Our proposed method uses deep neural networks to learn these sub-goals from raw input observations, enabling efficient exploration across diverse environments without needing domain knowledge. Experiments conducted using standard benchmark domains show significant improvements in both task completion metrics and interpretability compared to state-of-the-art methods. Additionally, we demonstrate our modelâ€™s ability to generalize across tasks, showing positive results even when evaluated on completely different datasets. These findings highlight the potential impact of our approach on improving RL interpretability while maximizing overall system performance.",1
"A new modification of the Neural Additive Model (NAM) called SurvNAM and its modifications are proposed to explain predictions of the black-box machine learning survival model. The method is based on applying the original NAM to solving the explanation problem in the framework of survival analysis. The basic idea behind SurvNAM is to train the network by means of a specific expected loss function which takes into account peculiarities of the survival model predictions and is based on approximating the black-box model by the extension of the Cox proportional hazards model which uses the well-known Generalized Additive Model (GAM) in place of the simple linear relationship of covariates. The proposed method SurvNAM allows performing the local and global explanation. A set of examples around the explained example is randomly generated for the local explanation. The global explanation uses the whole training dataset. The proposed modifications of SurvNAM are based on using the Lasso-based regularization for functions from GAM and for a special representation of the GAM functions using their weighted linear and non-linear parts, which is implemented as a shortcut connection. A lot of numerical experiments illustrate the SurvNAM efficiency.",0
"Machine Learning has emerged as one of the most powerful tools for predicting patient outcomes across diverse medical domains. As a result, there is growing interest in developing user-friendly models that can accurately describe their predictions, allowing clinicians to better understand and trust them. In recent years, interpretability techniques have been proposed to address these concerns, but these methods often fail to capture complex interactions among features and only provide local explanations, which may lead to limited insights into how the model makes decisions. To overcome these limitations, we propose SurvNAM (Survival Network Analysis Model), a novel methodology that integrates network analysis approaches into Survival Machine Learning (SML) model interpretation. Our approach constructs a weighted adjacency matrix from SML scores, where rows represent samples, columns denote features, and cells depict feature similarities between the patients and corresponding covariate groups. We then apply graph theory concepts to visualize this representation, enabling us to identify clusters of homogeneous risk patterns that are easily comprehensible by human experts. Through extensive experiments on three public datasets comprising breast cancer, acute myeloid leukemia, and lymphoblastic leukemia patients, our results demonstrate significant improvements over existing state-of-the-art methods in terms of accuracy, robustness, and interpretability measures such as global sensitivity indices and permutation feature importance assessments. Our work provides new research directions for designing reliable, interpretable ML systems capable of supporting critical decision-making processes in precision medicine, thus enhancing physician confidence in adopting automated recommendations and ultimately improving patient care quality.",1
"Recently, deep-learning based approaches have achieved impressive performance for autonomous driving. However, end-to-end vision-based methods typically have limited interpretability, making the behaviors of the deep networks difficult to explain. Hence, their potential applications could be limited in practice. To address this problem, we propose an interpretable end-to-end vision-based motion planning approach for autonomous driving, referred to as IVMP. Given a set of past surrounding-view images, our IVMP first predicts future egocentric semantic maps in bird's-eye-view space, which are then employed to plan trajectories for self-driving vehicles. The predicted future semantic maps not only provide useful interpretable information, but also allow our motion planning module to handle objects with low probability, thus improving the safety of autonomous driving. Moreover, we also develop an optical flow distillation paradigm, which can effectively enhance the network while still maintaining its real-time performance. Extensive experiments on the nuScenes dataset and closed-loop simulation show that our IVMP significantly outperforms the state-of-the-art approaches in imitating human drivers with a much higher success rate. Our project page is available at https://sites.google.com/view/ivmp.",0
"This work presents a novel method for learning interpretable end-to-end vision-based motion planning for autonomous driving using optical flow distillation. By leveraging deep neural networks, our approach enables efficient computation of dense trajectories that account for both short-term objectives (e.g., collision avoidance) as well as high-level specifications such as global routing tasks. To achieve interpretability, we design a hybrid model consisting of a recurrent network processing raw image inputs combined with a feedforward network operating on a compact representation learned via optical flow distillation. Our experiments demonstrate significant improvement over prior methods across challenging scenarios.  Keywords: Motion planning; Vision-based navigation; Deep reinforcement learning; Interpretability; Optical flow distillation \",1
"Most segmentation losses are arguably variants of the Cross-Entropy (CE) or Dice loss. In the literature, there is no clear consensus as to which of these losses is a better choice, with varying performances for each across different benchmarks and applications. We develop a theoretical analysis that links these two types of losses, exposing their advantages and weaknesses. First, we explicitly demonstrate that CE and Dice share a much deeper connection than previously thought: CE is an upper bound on both logarithmic and linear Dice losses. Furthermore, we provide an information-theoretic analysis, which highlights hidden label-marginal biases : Dice has an intrinsic bias towards imbalanced solutions, whereas CE implicitly encourages the ground-truth region proportions. Our theoretical results explain the wide experimental evidence in the medical-imaging literature, whereby Dice losses bring improvements for imbalanced segmentation. It also explains why CE dominates natural-image problems with diverse class proportions, in which case Dice might have difficulty adapting to different label-marginal distributions. Based on our theoretical analysis, we propose a principled and simple solution, which enables to control explicitly the label-marginal bias. Our loss integrates CE with explicit ${\cal L}_1$ regularization, which encourages label marginals to match target class proportions, thereby mitigating class imbalance but without losing generality. Comprehensive experiments and ablation studies over different losses and applications validate our theoretical analysis, as well as the effectiveness of our explicit label-marginal regularizers.",0
"This is a good practice that helps increase your chances of getting accepted by peer reviewers. There are several reasons why you should follow these guidelines:  This can be seen as an attempt at disguising important information from readers You are already wasting precious characters because you started with ""this"" which only adds unnecessary length Reading the first sentence twice can make the abstract easier to read It may seem like an obvious point but the reader has nothing else to go on at this stage so any clarity improvements would help their understanding and ultimately if they think your work is worth reading at all Therefore I advise against starting with ""this"", it makes no difference to how anyone reads the document other than making them read one more thing before getting into actually meaningful content! If there is something relevant to say then please do so otherwise just leave it out, save some character space, and give me some confidence in your abilities. Finally - make sure the sentences flow well together and don't come across as a list; it might seem unimportant now but after reading hundreds of papers you start to recognise patterns. Your reviewer may have gotten through many others earlier that day (or even earlier in life) and will pick up on things quicker if the abstract looks lazy and easy-to-write. If they think this reflects the quality of the paper they could stop here and reject it. Therefore the order in which you write the sentences matters!",1
"Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution -- also theoretically not understood -- concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks.   We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with $n$ hidden layers. A key ingredient is Barron's Theorem \cite{Barron1993}, which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of $n$ functions which satisfy certain Fourier conditions (""Barron functions"") can be approximated by a $n+1$-layer neural network.   For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance -- a natural metric on probability distributions -- by a neural network applied to a fixed base distribution (e.g., multivariate gaussian).   Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone.",0
"Title: ""Expressing Distributions through Neural Networks""  Neural networks have proven themselves as powerful tools for modeling complex systems, but their ability to accurately represent distributions has been under scrutiny. This study seeks to examine how well neural networks can capture the characteristics of different types of distributions, including normal, skewed, and heavy tailed distributions. To achieve this, we trained a variety of neural network architectures on synthetic data sets that mimic these distribution patterns and compared their performance against traditional statistical methods such as maximum likelihood estimation (MLE) and quantile regression. Our results show that while there were variations across models, overall deep learning approaches outperformed classical techniques, particularly when dealing with more complicated distributions like skewness and kurtosis. Additionally, our findings suggest that a specific type of architecture, called Residual Dense Net (RDN), may provide improved accuracy over other methods, even at low sample sizes. These findings further broaden the applicability of deep learning algorithms beyond supervised learning tasks to problems in probabilistic inference where accurate estimation of underlying distributions is crucial.",1
"Explainable AI (XAI) is a research area whose objective is to increase trustworthiness and to enlighten the hidden mechanism of opaque machine learning techniques. This becomes increasingly important in case such models are applied to the chemistry domain, for its potential impact on humans' health, e.g, toxicity analysis in pharmacology. In this paper, we present a novel approach to tackle explainability of deep graph networks in the context of molecule property prediction t asks, named MEG (Molecular Explanation Generator). We generate informative counterfactual explanations for a specific prediction under the form of (valid) compounds with high structural similarity and different predicted properties. Given a trained DGN, we train a reinforcement learning based generator to output counterfactual explanations. At each step, MEG feeds the current candidate counterfactual into the DGN, collects the prediction and uses it to reward the RL agent to guide the exploration. Furthermore, we restrict the action space of the agent in order to only keep actions that maintain the molecule in a valid state. We discuss the results showing how the model can convey non-ML experts with key insights into the learning model focus in the neighbourhood of a molecule.",0
"As machine learning algorithms become increasingly prevalent in decision making processes across industries, there has been growing concern regarding their explainability and transparency. One particularly challenging problem is understanding how graph neural networks (GNNs), which have shown promising results in several domains including biochemistry, make predictions. In this work we propose MEG, a novel framework that generates molecular counterfactuals to explain GNN predictions. By doing so, MEG provides insights into how small changes in chemical structures can impact predicted outputs, allowing researchers and practitioners alike to better evaluate the trustworthiness of deep graph models. We demonstrate the effectiveness of our method on two real world tasks; QSAR modeling and drug discovery where we show that our approach outperforms baseline methods by generating more meaningful and informative explanations. Our results highlight the potential of using counterfactual reasoning as a tool towards improving the interpretability of graph neural networks.",1
"Explainable AI (XAI) methods focus on explaining what a neural network has learned - in other words, identifying the features that are the most influential to the prediction. In this paper, we call them ""distinguishing features"". However, whether a human can make sense of the generated explanation also depends on the perceptibility of these features to humans. To make sure an explanation is human-understandable, we argue that the capabilities of humans, constrained by the Human Visual System (HVS) and psychophysics, need to be taken into account. We propose the {\em human perceptibility principle for XAI}, stating that, to generate human-understandable explanations, neural networks should be steered towards focusing on human-understandable cues during training. We conduct a case study regarding the classification of real vs. fake face images, where many of the distinguishing features picked up by standard neural networks turn out not to be perceptible to humans. By applying the proposed principle, a neural network with human-understandable explanations is trained which, in a user study, is shown to better align with human intuition. This is likely to make the AI more trustworthy and opens the door to humans learning from machines. In the case study, we specifically investigate and analyze the behaviour of the human-imperceptible high spatial frequency features in neural networks and XAI methods.",0
"This paper presents a novel approach towards generating human-understandable visual explanations by removing imperceptible high-frequency cues that can confuse humans. We argue that these cues, which may seem insignificant to algorithms, can lead to confusion among humans when they try to interpret machine generated content such as images or videos. Our method involves analyzing both low frequency components (corresponding to structural elements) and higher frequency components (corresponding to noise), and then eliminating those higher frequency elements that are most likely to cause problems for human viewers without significantly impacting the overall quality of the explanation. We demonstrate the effectiveness of our approach through extensive experiments on various image datasets and show that our proposed method leads to improved explainability while maintaining good perceptual fidelity compared to existing state-of-the-art methods. Overall, our work takes an important step towards improving transparency and trustworthiness in artificial intelligence systems by making their decision processes more comprehensible to humans.",1
"Machine learning models that offer excellent predictive performance often lack the interpretability necessary to support integrated human machine decision-making. In clinical medicine and other high-risk settings, domain experts may be unwilling to trust model predictions without explanations. Work in explainable AI must balance competing objectives along two different axes: 1) Explanations must balance faithfulness to the model's decision-making with their plausibility to a domain expert. 2) Domain experts desire local explanations of individual predictions and global explanations of behavior in aggregate. We propose to train a proxy model that mimics the behavior of the trained model and provides fine-grained control over these trade-offs. We evaluate our approach on the task of assigning ICD codes to clinical notes to demonstrate that explanations from the proxy model are faithful and replicate the trained model behavior.",0
"Abstract: In recent years, there has been increasing interest in using machine learning algorithms to predict medical codes, which are used to describe clinical diagnoses and procedures performed on patients. However, there remain significant challenges in ensuring that these predictions are both faithful (accurately reflecting true patient conditions) and plausible (not overly conservative or aggressive). This paper explores two different approaches to generating more faithful and plausible explanations of medical code predictions made by machine learning models. First, we propose a novel method for visualizing the reasoning process behind each predicted code based on the modelâ€™s learned patterns. Second, we develop a framework for evaluating the plausibility of these predictions through comparison against reference standards and analysis of feature importance. Our results demonstrate improved accuracy and interpretability of code predictions across several datasets and use cases. Overall, our work offers new opportunities for understanding and refining automated coding systems for better integration into healthcare practice.",1
"Graph neural networks (GNNs) have been widely used in deep learning on graphs. They can learn effective node representations that achieve superior performances in graph analysis tasks such as node classification and node clustering. However, most methods ignore the heterogeneity in real-world graphs. Methods designed for heterogeneous graphs, on the other hand, fail to learn complex semantic representations because they only use meta-paths instead of meta-graphs. Furthermore, they cannot fully capture the content-based correlations between nodes, as they either do not use the self-attention mechanism or only use it to consider the immediate neighbors of each node, ignoring the higher-order neighbors. We propose a novel Higher-order Attribute-Enhancing (HAE) framework that enhances node embedding in a layer-by-layer manner. Under the HAE framework, we propose a Higher-order Attribute-Enhancing Graph Neural Network (HAEGNN) for heterogeneous network representation learning. HAEGNN simultaneously incorporates meta-paths and meta-graphs for rich, heterogeneous semantics, and leverages the self-attention mechanism to explore content-based nodes interactions. The unique higher-order architecture of HAEGNN allows examining the first-order as well as higher-order neighborhoods. Moreover, HAEGNN shows good explainability as it learns the importances of different meta-paths and meta-graphs. HAEGNN is also memory-efficient, for it avoids per meta-path based matrix calculation. Experimental results not only show HAEGNN superior performance against the state-of-the-art methods in node classification, node clustering, and visualization, but also demonstrate its superiorities in terms of memory efficiency and explainability.",0
"In recent years, there has been increasing interest in using graph neural networks (GNNs) to model complex relationships among entities. One significant challenge faced by these models, however, is their limited ability to handle higher-order connections between nodes. To address this issue, we propose a new approach based on attribute-enhancing heterogeneous graphs. Our method leverages both node attributes and higher-order connections to improve the accuracy and interpretability of GNNs. We demonstrate the effectiveness of our approach through extensive experimental evaluations on several benchmark datasets, showing that it outperforms state-of-the-art methods across a range of tasks. Overall, our work represents a significant step forward in the development of GNNs and provides a powerful tool for researchers working in areas such as social network analysis, recommender systems, and natural language processing.",1
"Graph Neural Networks (GNNs) have been widely used for the representation learning of various structured graph data, typically through message passing among nodes by aggregating their neighborhood information via different operations. While promising, most existing GNNs oversimplified the complexity and diversity of the edges in the graph, and thus inefficient to cope with ubiquitous heterogeneous graphs, which are typically in the form of multi-relational graph representations. In this paper, we propose RioGNN, a novel Reinforced, recursive and flexible neighborhood selection guided multi-relational Graph Neural Network architecture, to navigate complexity of neural network structures whilst maintaining relation-dependent representations. We first construct a multi-relational graph, according to the practical task, to reflect the heterogeneity of nodes, edges, attributes and labels. To avoid the embedding over-assimilation among different types of nodes, we employ a label-aware neural similarity measure to ascertain the most similar neighbors based on node attributes. A reinforced relation-aware neighbor selection mechanism is developed to choose the most similar neighbors of a targeting node within a relation before aggregating all neighborhood information from different relations to obtain the eventual node embedding. Particularly, to improve the efficiency of neighbor selecting, we propose a new recursive and scalable reinforcement learning framework with estimable depth and width for different scales of multi-relational graphs. RioGNN can learn more discriminative node embedding with enhanced explainability due to the recognition of individual importance of each relation via the filtering threshold mechanism.",0
"Artificial Intelligence (AI) has made significant progress over the years through advancements in deep learning techniques such as neural networks. However, traditional neural network architectures often struggle with handling complex graph data due to their inherent limitations in modeling irregular and cyclic structures. To address these issues, we propose a novel approach based on reinforcement learning that leverages neighborhood selection guided multi-relational graphs neural networks (RNSMGNN). Our proposed method utilizes actor-critic architecture for efficient optimization and selects discriminative subgraphs adaptively during training, which leads to more accurate predictions compared to conventional GNN models. We demonstrate the effectiveness of our method by performing extensive experiments on several benchmark datasets for node classification tasks, including citation and email data. Results show that RNSMGNN outperforms state-of-the-art methods while being computationally efficient. This work opens up new opportunities for applying deep learning techniques to a broader range of real-world applications involving relational data representations.",1
"Prototype-based methods use interpretable representations to address the black-box nature of deep learning models, in contrast to post-hoc explanation methods that only approximate such models. We propose the Neural Prototype Tree (ProtoTree), an intrinsically interpretable deep learning method for fine-grained image recognition. ProtoTree combines prototype learning with decision trees, and thus results in a globally interpretable model by design. Additionally, ProtoTree can locally explain a single prediction by outlining a decision path through the tree. Each node in our binary tree contains a trainable prototypical part. The presence or absence of this learned prototype in an image determines the routing through a node. Decision making is therefore similar to human reasoning: Does the bird have a red throat? And an elongated beak? Then it's a hummingbird! We tune the accuracy-interpretability trade-off using ensemble methods, pruning and binarizing. We apply pruning without sacrificing accuracy, resulting in a small tree with only 8 learned prototypes along a path to classify a bird from 200 species. An ensemble of 5 ProtoTrees achieves competitive accuracy on the CUB-200- 2011 and Stanford Cars data sets. Code is available at https://github.com/M-Nauta/ProtoTree",0
"This paper presents a novel method for fine-grained image recognition that utilizes neural prototype trees (NPTs) as an interpretable model component. NPTs encode semantic information in tree structures, making them more human-readable than traditional deep learning models while still achieving high accuracy on classification tasks. We evaluate our approach using three benchmark datasets and compare it against state-of-the-art methods. Our results show that NPTs improve interpretability without sacrificing performance, providing insights into how these models work and enabling future research opportunities in computer vision. Additionally, we demonstrate potential applications of NPTs beyond fine-grained image recognition, such as transferring knowledge from pre-trained models and hierarchical feature dissection. Overall, our contributions advance both the field of interpretable machine learning and real-world image analysis practices.",1
"Under any Multiclass Classification (MCC) setting defined by a collection of labeled point-cloud specified by a feature-set, we extract only stochastic partial orderings from all possible triplets of point-cloud without explicitly measuring the three cloud-to-cloud distances. We demonstrate that such a collective of partial ordering can efficiently compute a label embedding tree geometry on the Label-space. This tree in turn gives rise to a predictive graph, or a network with precisely weighted linkages. Such two multiscale geometries are taken as the coarse scale information content of MCC. They indeed jointly shed lights on explainable knowledge on why and how labeling comes about and facilitates error-free prediction with potential multiple candidate labels supported by data. For revealing within-label heterogeneity, we further undergo labeling naturally found clusters within each point-cloud, and likewise derive multiscale geometry as its fine-scale information content contained in data. This fine-scale endeavor shows that our computational proposal is indeed scalable to a MCC setting having a large label-space. Overall the computed multiscale collective of data-driven patterns and knowledge will serve as a basis for constructing visible and explainable subject matter intelligence regarding the system of interest.",0
"Geometric information content describes how different aspects or features of a set of data points can influence a machine learning algorithm's performance on a particular task. In this study, we examine two types of geometric information content: coarse- and fine-scale. Coarse-scale information refers to how global properties of the data impact classification accuracy, while fine-scale information pertains to local patterns within subsets of the data that may have predictive value. We investigate how these two sources of information interact and contribute to overall model performance across a range of classification problems with multiple classes. Our findings demonstrate that both coarse- and fine-scale information are crucial contributors to successful multiclass classification algorithms, and provide insights into the nature of data-driven intelligence more broadly. By elucidating the specific contributions of each type of information content, our work aims to inform future research in machine learning and deepen understanding of how algorithms learn from complex datasets. Keywords: geometric information content, multiclass classification, data-driven intelligence.",1
"Spectral-spatial based deep learning models have recently proven to be effective in hyperspectral image (HSI) classification for various earth monitoring applications such as land cover classification and agricultural monitoring. However, due to the nature of ""black-box"" model representation, how to explain and interpret the learning process and the model decision, especially for vegetation classification, remains an open challenge. This study proposes a novel interpretable deep learning model -- a biologically interpretable two-stage deep neural network (BIT-DNN), by incorporating the prior-knowledge (i.e. biophysical and biochemical attributes and their hierarchical structures of target entities) based spectral-spatial feature transformation into the proposed framework, capable of achieving both high accuracy and interpretability on HSI based classification tasks. The proposed model introduces a two-stage feature learning process: in the first stage, an enhanced interpretable feature block extracts the low-level spectral features associated with the biophysical and biochemical attributes of target entities; and in the second stage, an interpretable capsule block extracts and encapsulates the high-level joint spectral-spatial features representing the hierarchical structure of biophysical and biochemical attributes of these target entities, which provides the model an improved performance on classification and intrinsic interpretability with reduced computational complexity. We have tested and evaluated the model using four real HSI datasets for four separate tasks (i.e. plant species classification, land cover classification, urban scene recognition, and crop disease recognition tasks). The proposed model has been compared with five state-of-the-art deep learning models.",0
"This paper presents a novel approach for vegetation recognition from hyperspectral imagery using a biologically interpretable two-stage deep neural network (BIT-DNN). The proposed method utilizes the unique properties of the human visual system (HVS), such as retinal eccentricity, cortical magnification, and spatial frequency sensitivity, to design a more efficient and effective model for feature extraction. In the first stage, features from each channel of the hyperspectral image are extracted using the HVS principles and merged into one representation by weighting them based on their corresponding retinal eccentricities. Then, we use a two-pathway architecture inspired by the ventral stream of the primate visual system to process these features separately for local texture and global shape analysis. Finally, both pathways are fused using late fusion and fed into a fully connected layer with softmax activation for classification. Experimental results demonstrate that our BIT-DNN significantly outperforms several state-of-the-art methods across different benchmark datasets, providing evidence for the effectiveness of incorporating biological insights into machine learning models for remote sensing applications.",1
"Deep generative modelling is a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which making trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are drawn under a single cohesive framework, comparing and contrasting to explain the premises behind each, while reviewing current state-of-the-art advances and implementations.",0
"Abstract ------------------- With the rapidly growing popularity of deep generative models (DGM), many architectures have been proposed in recent years. These architectures all aim to learn a probability distribution over data by maximising its log likelihood on real world observations. This comparative review provides an objective analysis of five commonly used DGM models: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), normalising flows, energy-based models and autoregressive models. Our evaluation highlights their strengths and weaknesses based on criteria such as generation quality, sample diversity, training stability, model complexity, inference speed and robustness to hyperparameters. We discuss open challenges in comparing these models which should drive future research directions.",1
"We consider an extension of Leo Breiman's thesis from ""Statistical Modeling: The Two Cultures"" to include a bifurcation of algorithmic modeling, focusing on parametric regressions, interpretable algorithms, and complex (possibly explainable) algorithms.",0
"Considerations should be focused on the advantages and disadvantages of each approach across different cultures. Abstract:  As we continue to develop new technologies that can better serve people from diverse backgrounds, it becomes increasingly important to consider how these tools might impact them differently depending on their cultural context. In particular, there are three approaches to designing machine learning algorithms: parametric regressions, interpretable algorithms, and complex algorithms. Each has its own set of benefits and drawbacks which may vary depending on the culture in question.  Parametic regressions have been found to produce accurate results quickly but they may not always be trusted by certain communities due to their lack of transparency. For example, some cultures value explainability over efficiency and may prefer other methods even if those methods require more time to process. Interpretable algorithms provide clear explanations for their decisions but may not perform as well on certain tasks compared to parametric regressions or complex algorithms. Furthermore, some societies prioritize precision over interpretability so they may not prioritize this method either.  Complex algorithms often outperform the above two approaches in terms of accuracy but lack transparency into how they arrived at their conclusions. As such, they may face mistrust from some groups. While there are ways to make complex models more interpretable, it requires additional work that some developers may not want to pursue. Additionally, the technical nature of these algorithms makes them difficult to use effectively without proper training, meaning they must overcome accessibility barriers before being implemented widely. Overall, there is no one size fits all solution when developing machine learning algorith",1
"Overfitting is one of the fundamental challenges when training convolutional neural networks and is usually identified by a diverging training and test loss. The underlying dynamics of how the flow of activations induce overfitting is however poorly understood. In this study we introduce a perplexity-based sparsity definition to derive and visualise layer-wise activation measures. These novel explainable AI strategies reveal a surprising relationship between activation sparsity and overfitting, namely an increase in sparsity in the feature extraction layers shortly before the test loss starts rising. This tendency is preserved across network architectures and reguralisation strategies so that our measures can be used as a reliable indicator for overfitting while decoupling the network's generalisation capabilities from its loss-based definition. Moreover, our differentiable sparsity formulation can be used to explicitly penalise the emergence of sparsity during training so that the impact of reduced sparsity on overfitting can be studied in real-time. Applying this penalty and analysing activation sparsity for well known regularisers and in common network architectures supports the hypothesis that reduced activation sparsity can effectively improve the generalisation and classification performance. In line with other recent work on this topic, our methods reveal novel insights into the contradicting concepts of activation sparsity and network capacity by demonstrating that dense activations can enable discriminative feature learning while efficiently exploiting the capacity of deep models without suffering from overfitting, even when trained excessively.",0
"In this study we investigate the impact of activation sparsity on overfitting in convolutional neural networks (CNN). We find that as activation sparsity increases so does the susceptibility of a CNN to overfitting. Additionally, our results suggest that using dropout regularization combined with early stopping can mitigate the effect of increasing activation sparsity and improve generalization performance in highly sparse systems. Finally, we demonstrate that simply reducing weight values may not be sufficient to prevent overfitting and maintain good generalization performance as the system becomes more sparse. These insights have important implications for developing highly efficient CNNs while balancing model accuracy and computational cost.",1
"Artificial Intelligence (AI) has a tremendous impact on the unexpected growth of technology in almost every aspect. AI-powered systems are monitoring and deciding about sensitive economic and societal issues. The future is towards automation, and it must not be prevented. However, this is a conflicting viewpoint for a lot of people, due to the fear of uncontrollable AI systems. This concern could be reasonable if it was originating from considerations associated with social issues, like gender-biased, or obscure decision-making systems. Explainable AI (XAI) is recently treated as a huge step towards reliable systems, enhancing the trust of people to AI. Interpretable machine learning (IML), a subfield of XAI, is also an urgent topic of research. This paper presents a small but significant contribution to the IML community, focusing on a local-based, neural-specific interpretation process applied to textual and time-series data. The proposed methodology introduces new approaches to the presentation of feature importance based interpretations, as well as the production of counterfactual words on textual datasets. Eventually, an improved evaluation metric is introduced for the assessment of interpretation techniques, which supports an extensive set of qualitative and quantitative experiments.",0
"This paper presents LioNets, a novel technique that exploits penultimate layer information to improve neural network interpretability. Many state-of-the-art deep learning models lack transparency, making it difficult to explain their decision-making processes. Existing techniques focus on local explanation methods that rely heavily on feature visualization or perturbation analysis, which can suffer from poor accuracy or limited transferability across different networks. In contrast, our approach takes advantage of the intermediate representations learned by the model before the final output prediction. We show that leveraging such representations provides significant benefits over existing interpretation techniques, including improved fidelity to ground truth explanations, better generalizability across multiple datasets, and reduced computational overhead. Through comprehensive experiments on several benchmark datasets, we demonstrate the effectiveness of LioNets as a powerful tool for explaining neural networks in computer vision tasks. Overall, our work advances the field of neural network interpretability, offering valuable insights into how these models make predictions.",1
"In critical situations involving discrimination, gender inequality, economic damage, and even the possibility of casualties, machine learning models must be able to provide clear interpretations for their decisions. Otherwise, their obscure decision-making processes can lead to socioethical issues as they interfere with people's lives. In the aforementioned sectors, random forest algorithms strive, thus their ability to explain themselves is an obvious requirement. In this paper, we present LionForests, which relies on a preliminary work of ours. LionForests is a random forest-specific interpretation technique, which provides rules as explanations. It is applicable from binary classification tasks to multi-class classification and regression tasks, and it is supported by a stable theoretical background. Experimentation, including sensitivity analysis and comparison with state-of-the-art techniques, is also performed to demonstrate the efficacy of our contribution. Finally, we highlight a unique property of LionForests, called conclusiveness, that provides interpretation validity and distinguishes it from previous techniques.",0
"Abstract: This paper presents local interpretation rules (LIR) for random forest classifiers that provide conclusions that hold true locally around any given data point. These novel LIRs are based on the proximity to the decision boundary and lead to interpretable predictions by identifying the subset of features most responsible for making the prediction at each data point. Our approach generalizes existing work by allowing for continuous feature spaces and arbitrary tree building strategies. In addition, we demonstrate the utility of our methodology via extensive experiments on several benchmark datasets using both real-valued and binary classification problems. Our results show that the proposed LIR outperform state-of-the-art methods and can achieve competitive accuracy while providing transparent interpretations. As such, they offer new opportunities for understanding complex models and their decisions and have important implications for high stakes applications where interpretability is essential. Keywords: random forest, feature importance ranking, model interpretation, explanation-aware learning Title: Conclusive Local Interpretation Rules for Random Forests Abstract: Random Forest classifiers have gained widespread popularity due to their excellent performance and ability to handle large datasets. However, their black box nature often makes it challenging to explain how these models make predictions. Previous attempts at explaining random forest models have relied heavily on global feature importances which may not always capture the relevant factors for a particular instance. To address this gap, we present Local Interpretation Rules (LIR), a technique that provides interpretable predictions by identifying the subset of features most responsible for making the prediction at each data point. Our novel LIR are derived from the distance of a sample to the decision boundary, enabling them to capture both positive and negative influences of individual features. We evaluate our method across multiple real-world datasets and show that it performs favorably against state-of-the-art baselines while achieving comparable predictive accuracy. Our findings confirm the effectiveness of our approach as well as the potential benefits of explanation-aware machine learning. With the increasing demand for accountability in artificial intelligence systems, the LIR framework offers a promising direction towards developing more trustworthy algorithms. ----End Paper",1
"Model extraction increasingly attracts research attentions as keeping commercial AI models private can retain a competitive advantage. In some scenarios, AI models are trained proprietarily, where neither pre-trained models nor sufficient in-distribution data is publicly available. Model extraction attacks against these models are typically more devastating. Therefore, in this paper, we empirically investigate the behaviors of model extraction under such scenarios. We find the effectiveness of existing techniques significantly affected by the absence of pre-trained models. In addition, the impacts of the attacker's hyperparameters, e.g. model architecture and optimizer, as well as the utilities of information retrieved from queries, are counterintuitive. We provide some insights on explaining the possible causes of these phenomena. With these observations, we formulate model extraction attacks into an adaptive framework that captures these factors with deep reinforcement learning. Experiments show that the proposed framework can be used to improve existing techniques, and show that model extraction is still possible in such strict scenarios. Our research can help system designers to construct better defense strategies based on their scenarios.",0
"In recent years, there has been growing concern over potential malicious uses of deep learning models by attackers seeking to steal sensitive data from legitimate users. One emerging threat vector involves using specially crafted inputs (known as ""fingerprinting attacks"") that can extract model parameters and associated weights from these models, potentially enabling such attacks. However, until now, we lacked effective countermeasures for thwarting these types of attacks. This research provides a thorough investigation into the feasibility and effectiveness of fingerprinting attacks against deep neural networks (DNNs) using state-of-the-art techniques, and proposes novel defense mechanisms to mitigate their impact on vulnerable applications. Our study demonstrates that simple yet effective defenses can significantly impede successful extraction of DNNs under realworld constraints, offering critical insights toward securing future systems reliant on machine learning models. Ultimately, our findings will enable stakeholders to develop informed strategies for protecting sensitive data across diverse domains including finance, healthcare, automotive, and government services. By shedding light on this crucial problem area, we aim to inspire further exploration of secure ML paradigms while fostering responsible innovation within industry and academia alike.",1
"A multitude of classifiers can be trained on the same data to achieve similar performances during test time, while having learned significantly different classification patterns. This phenomenon, which we call prediction discrepancies, is often associated with the blind selection of one model instead of another with similar performances. When making a choice, the machine learning practitioner has no understanding on the differences between models, their limits, where they agree and where they don't. But his/her choice will result in concrete consequences for instances to be classified in the discrepancy zone, since the final decision will be based on the selected classification pattern. Besides the arbitrary nature of the result, a bad choice could have further negative consequences such as loss of opportunity or lack of fairness. This paper proposes to address this question by analyzing the prediction discrepancies in a pool of best-performing models trained on the same data. A model-agnostic algorithm, DIG, is proposed to capture and explain discrepancies locally, to enable the practitioner to make the best educated decision when selecting a model by anticipating its potential undesired consequences. All the code to reproduce the experiments is available.",0
"One of the major challenges faced by machine learning classifiers is the prediction discrepancy problem. This refers to the fact that different classifiers can make vastly different predictions on the same dataset, even when trained using similar techniques and features. In order to address this issue, it is important to gain a deeper understanding of the factors that contribute to prediction discrepancies.  In this paper, we explore several possible causes of prediction discrepancies in machine learning classifiers. We begin by examining differences in training datasets, as variations in data quality or quantity can lead to differences in model performance. Next, we investigate the impact of feature selection, as choosing different subsets of features can result in drastically different models. Additionally, we consider differences in model architectures, such as deep neural networks versus decision trees, which may produce distinct results despite using identical input. Finally, we discuss the role of hyperparameters in causing prediction discrepancies, highlighting how small changes in parameters like learning rate or regularization strength can significantly affect model accuracy.  Through our analysis, we demonstrate that understanding prediction discrepancies requires considering a multitude of factors beyond just the choice of algorithm. By identifying these sources of variation, we can develop more effective methods for selecting appropriate classifiers and improving their overall performance. Ultimately, our findings have significant implications for researchers working in fields ranging from healthcare to finance, where accurate prediction is essential for making informed decisions.",1
"In recent years, the use of sophisticated statistical models that influence decisions in domains of high societal relevance is on the rise. Although these models can often bring substantial improvements in the accuracy and efficiency of organizations, many governments, institutions, and companies are reluctant to their adoption as their output is often difficult to explain in human-interpretable ways. Hence, these models are often regarded as black-boxes, in the sense that their internal mechanisms can be opaque to human audit. In real-world applications, particularly in domains where decisions can have a sensitive impact--e.g., criminal justice, estimating credit scores, insurance risk, health risks, etc.--model interpretability is desired. Recently, the academic literature has proposed a substantial amount of methods for providing interpretable explanations to machine learning models. This survey reviews the most relevant and novel methods that form the state-of-the-art for addressing the particular problem of explaining individual instances in machine learning. It seeks to provide a succinct review that can guide data science and machine learning practitioners in the search for appropriate methods to their problem domain.",0
"This survey provides a comprehensive overview of individual explanations (IEs) in machine learning models (MLM), covering the fundamental concepts, existing techniques, use cases, evaluation methods, related work and open challenges. IEs provide insights into how MLM arrive at their decisions, which can be essential for practitioners when deploying these systems, as well as regulators and policymakers who must ensure compliance with legal requirements such as explainability and interpretability. However, designing effective IEs remains difficult due to complexities in MLM architectures and datasets. Thus, this survey serves as a reference guide for practitioners seeking to incorporate transparent decision making processes into ML applications. We discuss key factors that influence IE effectiveness and introduce taxonomies for categorizing different types of IEs based on their scope and purpose. Additionally, we explore promising research directions aimed at improving the quality and efficiency of IE generation and presentation across various real-world application domains. Ultimately, our goal is to empower practitioners to develop high-quality IE systems capable of supporting trustworthy and responsible ML deployment, while also providing stakeholders with a better understanding of how decisions affect them directly or indirectly.",1
"Due to the pivotal role of UV photodiodes in many technological applications in tandem with the high efficiency achieved by machine learning techniques in regression and classification problems, different artificial intelligence techniques are adopted model the performance of organic/inorganic heterojunction UV photodiode. Herein, the performance of a fabricated Au/NTCDA/p-Si/Al photodiode was explained in details and showed an excellent responsivity, and detectivity for UV light of intensities ranges from 20 to 80 ${mW/cm^2}$. The fabricated photodiodes exhibited a linear current-irradiance relationship under illumination up to 65 ${mW/cm^2}$. It also exhibits good response times of ${t_{rise} = 408}$ ms and ${t_{fall} = 490}$ ms. Furthermore, we have not only fitted the characteristic I-V curve but also evaluated three classical algorithms; k-nearest neighbour, artificial neural network, and genetic programming besides using a quantum neural network to predict the behaviour of the fabricated device. The models have achieved outstanding results and managed to capture the trend of the target values. The Quantum Neural Network has been used for the first time to model the photodiode. The models can be used instead of repeating the fabrication process. This means a reduction in cost and manufacturing time.",0
"In this research, we aimed to compare classical and quantum regression analysis techniques for predicting the optoelectronic properties of NTCDA/p-Si ultraviolet (UV) photodiodes, which have important applications in areas such as biochemical sensors, environmental monitoring, and security systems. We investigated two types of data sets: experimental data obtained from previously published literature and simulated data generated using quantum transport simulations based on the nonlocal van Roosbroeck model. Our findings suggest that both classical linear regression and quantum machine learning methods can effectively capture significant correlations between device parameters and optoelectronic performance metrics, including dark current density, responsivity, detectivity, noise equivalent power, and external quantum efficiency. However, there were some discrepancies between the classical and quantum results for certain metrics, highlighting potential limitations of each approach. Overall, our work provides valuable insights into how different statistical models can improve our understanding of optoelectronic processes in organic-inorganic hybrid semiconductor devices like NTCDA/p-Si UV photodiodes.",1
"Integrated interpretability without sacrificing the prediction accuracy of decision making algorithms has the potential of greatly improving their value to the user. Instead of assigning a label to an image directly, we propose to learn iterative binary sub-decisions, inducing sparsity and transparency in the decision making process. The key aspect of our model is its ability to build a decision tree whose structure is encoded into the memory representation of a Recurrent Neural Network jointly learned by two models communicating through message passing. In addition, our model assigns a semantic meaning to each decision in the form of binary attributes, providing concise, semantic and relevant rationalizations to the user. On three benchmark image classification datasets, including the large-scale ImageNet, our model generates human interpretable binary decision sequences explaining the predictions of the network while maintaining state-of-the-art accuracy.",0
"An important aspect of machine learning is the ability to model complex decision making processes using algorithms that can learn from data. One popular approach to achieve this goal is through the use of tree ensembles such as random forest or gradient boosting machines. In particular, these models are known to produce high accuracy predictions on many tasks while still providing interpretability due to their explicit representation of decisions made at each node. However, training such models requires large amounts of labeled data and computational resources which may not always be available.  In this work, we propose a novel framework for learning decision trees recursively through communication. Instead of relying solely on static datasets, our method allows multiple agents to share information and collaborate in order to learn more accurate models over time. We show that by allowing agents to communicate and update their local models based on incoming messages, they can effectively identify patterns in the data and refine their predictions without requiring additional labels. Furthermore, we demonstrate that our approach leads to significant improvements in predictive performance compared to standard decision trees trained on fixed datasets. Overall, our results highlight the potential benefits of incorporating communication and collaboration into machine learning systems, particularly in situations where labeled data and computing power are limited.",1
"With the remarkable success of representation learning in providing powerful predictions and data insights, we have witnessed a rapid expansion of representation learning techniques into modeling, analysis, and learning with networks. Biomedical networks are universal descriptors of systems of interacting elements, from protein interactions to disease networks, all the way to healthcare systems and scientific knowledge. In this review, we put forward an observation that long-standing principles of network biology and medicine -- while often unspoken in machine learning research -- can provide the conceptual grounding for representation learning, explain its current successes and limitations, and inform future advances. We synthesize a spectrum of algorithmic approaches that, at their core, leverage topological features to embed networks into compact vector spaces. We also provide a taxonomy of biomedical areas that are likely to benefit most from algorithmic innovation. Representation learning techniques are becoming essential for identifying causal variants underlying complex traits, disentangling behaviors of single cells and their impact on health, and diagnosing and treating diseases with safe and effective medicines.",0
"Advances in biology and medicine have been greatly enhanced by the use of network representation learning techniques. These methods allow researchers to analyze complex systems, such as gene regulatory networks and protein-protein interaction networks, to identify key patterns, relationships, and potential drug targets. However, there are still significant challenges that need to be addressed in order to fully realize the potential of these techniques. This article reviews recent developments in network representation learning in both computational biology and medical applications, highlighting opportunities for future research directions. It also discusses some of the challenges faced by current approaches, including difficulties in generating accurate models from high-dimensional data and integrating diverse types of omics data into coherent representations. Overall, network representation learning has tremendous promise for advancing our understanding of biological processes and identifying new therapeutic strategies. By addressing existing challenges and exploring new application areas, further progress can be made towards realizing this potential.",1
"In this paper we propose a fully-supervised pretraining scheme based on contrastive learning particularly tailored to dense classification tasks. The proposed Context-Self Contrastive Loss (CSCL) learns an embedding space that makes semantic boundaries pop-up by use of a similarity metric between every location in an training sample and its local context. For crop type semantic segmentation from satellite images we find performance at parcel boundaries to be a critical bottleneck and explain how CSCL tackles the underlying cause of that problem, improving the state-of-the-art performance in this task. Additionally, using images from the Sentinel-2 (S2) satellite missions we compile the largest, to our knowledge, dataset of satellite image timeseries densely annotated by crop type and parcel identities, which we make publicly available together with the data generation pipeline. Using that data we find CSCL, even with minimal pretraining, to improve all respective baselines and present a process for semantic segmentation at super-resolution for obtaining crop classes at a more granular level. The proposed method is further validated on the task of semantic segmentation on 2D and 3D volumetric images showing consistent performance improvements upon competitive baselines.",0
"This sounds like an interesting topic! For context: semantic segmentation refers to computer vision task where images are analyzed pixel by pixel to assign each pixel to one of several classes (such as ""road"", ""sidewalk"", ""building""). Crop types refer to agricultural crops grown on farms. So I imagine this paper addresses how image analysis can improve farming practices? If so, here might be an example of an engaging abstract that conveys the main ideas of your paper without getting overly technical:  ---  Crops are the foundation of our global food system, but accurate mapping of crop types remains challenging due to variability in plant growth patterns and lighting conditions. Traditional methods rely on expert knowledge and laborious manual annotation, which are time-consuming and costly. In this study, we present a novel approach using deep learning algorithms called context self-contrastive pretraining for automated crop type semantic segmentation. Our method leverages transfer learning from ImageNet and utilizes contextual relationships within local neighborhoods to learn representations encoding both appearance and high-level context. We evaluate our model's performance across multiple datasets including real-world farm data and demonstrate significant improvements compared to baseline models. With applications ranging from precision agriculture to climate change mitigation, our work represents a critical step towards reliable and scalable crop mapping solutions. Overall, the proposed framework has potential to transform agricultural management, enhance sustainability, and ultimately benefit society at large.  If you need something more specific feel free to provide further details and I'll try my best to write another version accordingly.",1
"Model-agnostic tools for interpreting machine-learning models struggle to summarize the joint effects of strongly dependent features in high-dimensional feature spaces, which play an important role in pattern recognition, for example in remote sensing of landcover. This contribution proposes a novel approach that interprets machine-learning models through the lens of feature space transformations. It can be used to enhance unconditional as well as conditional post-hoc diagnostic tools including partial dependence plots, accumulated local effects plots, or permutation feature importance assessments. While the approach can also be applied to nonlinear transformations, we focus on linear ones, including principal component analysis (PCA) and a partial orthogonalization technique. Structured PCA and diagnostics along paths offer opportunities for representing domain knowledge. The new approach is implemented in the R package `wiml`, which can be combined with existing explainable machine-learning packages. A case study on remote-sensing landcover classification with 46 features is used to demonstrate the potential of the proposed approach for model interpretation by domain experts.",0
"""Transformation of feature space has become increasingly important as machine learning models have grown more complex. These transformations provide a powerful tool for interpreting model behavior by allowing us to visualize high dimensional data in low dimensions and gain insights into how our models make predictions. In this work, we explore several techniques that can be used to transform the feature space of machine learning models. We evaluate these techniques using both real world datasets and synthetic examples, demonstrating their effectiveness at revealing key patterns in the data and improving model interpretability. Additionally, we discuss practical applications of feature transformation methods in areas such as image classification, natural language processing, and recommendation systems.""",1
"Noisy labels are very common in deep supervised learning. Although many studies tend to improve the robustness of deep training for noisy labels, rare works focus on theoretically explaining the training behaviors of learning with noisily labeled data, which is a fundamental principle in understanding its generalization. In this draft, we study its two phenomena, clean data first and phase transition, by explaining them from a theoretical viewpoint. Specifically, we first show that in the first epoch training, the examples with clean labels will be learned first. We then show that after the learning from clean data stage, continuously training model can achieve further improvement in testing error when the rate of corrupted class labels is smaller than a certain threshold; otherwise, extensively training could lead to an increasing testing error.",0
Include all authors in one affiliation section at the end after keywords. Do not include citations in the abstract. Use third person point of view throughout. No footnotes please. Thank you!,1
"In this paper, we propose a neuro-symbolic framework called weighted Signal Temporal Logic Neural Network (wSTL-NN) that combines the characteristics of neural networks and temporal logics. Weighted Signal Temporal Logic (wSTL) formulas are recursively composed of subformulas that are combined using logical and temporal operators. The quantitative semantics of wSTL is defined such that the quantitative satisfaction of subformulas with higher weights has more influence on the quantitative satisfaction of the overall wSTL formula. In the wSTL-NN, each neuron corresponds to a wSTL subformula, and its output corresponds to the quantitative satisfaction of the formula. We use wSTL-NN to represent wSTL formulas as features to classify time series data. STL features are more explainable than those used in classical methods. The wSTL-NN is end-to-end differentiable, which allows learning of wSTL formulas to be done using back-propagation. To reduce the number of weights, we introduce two techniques to sparsify the wSTL-NN.We apply our framework to an occupancy detection time-series dataset to learn a classifier that predicts the occupancy status of an office room.",0
"Title: Automatic Verification of Software Systems using Artificial Intelligence Abstract: Software systems have become increasingly complex over time, making their validation through traditional manual testing methods impractical. Thus, there has been a significant interest in developing efficient techniques for automating software verification tasks. In recent years, artificial intelligence (AI) approaches have emerged as promising candidates for tackling these challenges due to their ability to handle large amounts of data and identify patterns that may escape human inspection. This work presents a novel approach based on deep learning for automatically verifying software systems against formal specifications expressed in Weighted Signal Temporal Logic (WSTL). WSTL allows for expressing quantitative properties and offers precise modeling capabilities for systems subjected to uncertainty and variability. Our proposed method leverages neural networks for inferring temporal sequences from system executions, which enables the direct comparison against desired WSTL formulae. Empirical evaluation conducted on several benchmark cases demonstrates the effectiveness and efficiency of our approach compared to state-of-the-art symbolic verification algorithms. Overall, this study advances the development of intelligent tools capable of efficiently ensuring high quality standards in modern software design while reducing human effort and cost associated with rigorous testing procedures.",1
"Perceptual organization remains one of the very few established theories on the human visual system. It underpinned many pre-deep seminal works on segmentation and detection, yet research has seen a rapid decline since the preferential shift to learning deep models. Of the limited attempts, most aimed at interpreting complex visual scenes using perceptual organizational rules. This has however been proven to be sub-optimal, since models were unable to effectively capture the visual complexity in real-world imagery. In this paper, we rejuvenate the study of perceptual organization, by advocating two positional changes: (i) we examine purposefully generated synthetic data, instead of complex real imagery, and (ii) we ask machines to synthesize novel perceptually-valid patterns, instead of explaining existing data. Our overall answer lies with the introduction of a novel visual challenge -- the challenge of perceptual question answering (PQA). Upon observing example perceptual question-answer pairs, the goal for PQA is to solve similar questions by generating answers entirely from scratch (see Figure 1). Our first contribution is therefore the first dataset of perceptual question-answer pairs, each generated specifically for a particular Gestalt principle. We then borrow insights from human psychology to design an agent that casts perceptual organization as a self-attention problem, where a proposed grid-to-grid mapping network directly generates answer patterns from scratch. Experiments show our agent to outperform a selection of naive and strong baselines. A human study however indicates that ours uses astronomically more data to learn when compared to an average human, necessitating future research (with or without our dataset).",0
"Improving perception through question answering is an ongoing challenge in artificial intelligence research. In our recent work, we propose a new approach called PQA (Perceptual Question Answering) that addresses this challenge by leveraging advances in computer vision and natural language processing. Our method combines state-of-the-art techniques from both fields to accurately interpret questions about images and provide answers that are both relevant and informative. We evaluate our approach on several benchmark datasets and demonstrate significant improvement over existing methods in terms of accuracy, speed, and robustness. Additionally, we showcase how PQA can enable a wide range of applications such as image retrieval, autonomous navigation, and human-AI interaction. Overall, our work represents an important step towards building intelligent agents capable of understanding and interacting with their environment in rich and nuanced ways.",1
"Data poisoning and backdoor attacks manipulate training data to induce security breaches in a victim model. These attacks can be provably deflected using differentially private (DP) training methods, although this comes with a sharp decrease in model performance. The InstaHide method has recently been proposed as an alternative to DP training that leverages supposed privacy properties of the mixup augmentation, although without rigorous guarantees. In this work, we show that strong data augmentations, such as mixup and random additive noise, nullify poison attacks while enduring only a small accuracy trade-off. To explain these finding, we propose a training method, DP-InstaHide, which combines the mixup regularizer with additive noise. A rigorous analysis of DP-InstaHide shows that mixup does indeed have privacy advantages, and that training with k-way mixup provably yields at least k times stronger DP guarantees than a naive DP mechanism. Because mixup (as opposed to noise) is beneficial to model performance, DP-InstaHide provides a mechanism for achieving stronger empirical performance against poisoning attacks than other known DP methods.",0
"This paper presents DP-InstaHide, a novel defense mechanism that provides provable protection against poisoning and backdoor attacks using differentially private data augmentation techniques. We evaluate our approach on real-world benchmark datasets and demonstrate significant improvements over state-of-the-art methods while preserving model accuracy. Our proposed method ensures robustness against adversarial inputs by leveraging random noise injection during training without compromising performance. The effectiveness and efficiency of our system make it an attractive option for deploying machine learning models in secure settings where privacy and integrity are critical concerns. Overall, our work highlights the potential of applying differential privacy principles to enhance security in artificial intelligence systems, and we believe our findings open up new directions for future research in this field.",1
"There exists an apparent negative correlation between performance and interpretability of deep learning models. In an effort to reduce this negative correlation, we propose a Born Identity Network (BIN), which is a post-hoc approach for producing multi-way counterfactual maps. A counterfactual map transforms an input sample to be conditioned and classified as a target label, which is similar to how humans process knowledge through counterfactual thinking. For example, a counterfactual map can localize hypothetical abnormalities from a normal brain image that may cause it to be diagnosed with a disease. Specifically, our proposed BIN consists of two core components: Counterfactual Map Generator and Target Attribution Network. The Counterfactual Map Generator is a variation of conditional GAN which can synthesize a counterfactual map conditioned on an arbitrary target label. The Target Attribution Network provides adequate assistance for generating synthesized maps by conditioning a target label into the Counterfactual Map Generator. We have validated our proposed BIN in qualitative and quantitative analysis on MNIST, 3D Shapes, and ADNI datasets, and showed the comprehensibility and fidelity of our method from various ablation studies.",0
"This study presents a novel approach for explaining machine learning classifiers by generating multi-way counterfactual maps that show how different input features contribute to the final decision made by the model. These maps highlight the most influential factors driving the outcome while allowing users to interactively explore alternative scenarios. Our method builds on recent advances in explainability by combining two key concepts - influence functions and feature attribution methods - into a unified framework called Born Identity Network (BIN). We evaluate BIN using several real-world datasets across multiple domains and demonstrate its effectiveness compared to existing approaches for generating comprehensible explanations of complex models. Finally, we discuss limitations and future research directions stemming from our findings.",1
"Nonparametric estimation of a statistic, in general, and of the error rate of a classification rule, in particular, from just one available dataset through resampling is well mathematically founded in the literature using several versions of bootstrap and influence function. This article first provides a concise review of this literature to establish the theoretical framework that we use to construct, in a single coherent framework, nonparametric estimators of the AUC (a two-sample statistic) other than the error rate (a one-sample statistic). In addition, the smoothness of some of these estimators is well investigated and explained. Our experiments show that the behavior of the designed AUC estimators confirms the findings of the literature for the behavior of error rate estimators in many aspects including: the weak correlation between the bootstrap-based estimators and the true conditional AUC; and the comparable accuracy of the different versions of the bootstrap estimators in terms of the RMS with little superiority of the .632+ bootstrap estimator.",0
Title: An Investigation into Nonparametric Estimation Methods and their Properties ----------------------------------------------,1
"Spectral normalization (SN) is a widely-used technique for improving the stability and sample quality of Generative Adversarial Networks (GANs). However, there is currently limited understanding of why SN is effective. In this work, we show that SN controls two important failure modes of GAN training: exploding and vanishing gradients. Our proofs illustrate a (perhaps unintentional) connection with the successful LeCun initialization. This connection helps to explain why the most popular implementation of SN for GANs requires no hyper-parameter tuning, whereas stricter implementations of SN have poor empirical performance out-of-the-box. Unlike LeCun initialization which only controls gradient vanishing at the beginning of training, SN preserves this property throughout training. Building on this theoretical understanding, we propose a new spectral normalization technique: Bidirectional Scaled Spectral Normalization (BSSN), which incorporates insights from later improvements to LeCun initialization: Xavier initialization and Kaiming initialization. Theoretically, we show that BSSN gives better gradient control than SN. Empirically, we demonstrate that it outperforms SN in sample quality and training stability on several benchmark datasets.",0
"Title: ""A Deep Dive into Spectral Normalization and Its Effectiveness in Stabilizing Generative Adversarial Networks""  Spectral normalization (SN) has gained significant attention as a technique to stabilize generative adversarial networks (GANs). This paper provides insights into how SN helps mitigate common issues related to training instability that arise during the optimization process of deep neural networks. We analyze the behavior of activations during backpropagation using gradient analysis techniques and identify three major factors contributing to GAN training difficulties: vanishing gradients, mode collapse, and optimization problems caused by large batch sizes. Our study confirms earlier findings on the effectiveness of spectral normalization in reducing these challenges.  We further explore refinements to the original SN algorithm to improve performance under different network architectures and datasets. Specifically, we consider two novel variants of SN - adaptive regularization and feature-wise linear scaling - which modify the existing algorithm to better fit GAN requirements. Experimental results demonstrate improved stability across multiple benchmark tasks compared to previous state-of-the art methods. These improvements reflect enhanced convergence speed, reduced sensitivity to hyperparameters, and higher quality generated samples. Overall, our contributions provide valuable theoretical insights and practical enhancements towards utilizing spectral normalization effectively within the GAN paradigm.",1
"One of the key elements of explanatory analysis of a predictive model is to assess the importance of individual variables. Rapid development of the area of predictive model exploration (also called explainable artificial intelligence or interpretable machine learning) has led to the popularization of methods for local (instance level) and global (dataset level) methods, such as Permutational Variable Importance, Shapley Values (SHAP), Local Interpretable Model Explanations (LIME), Break Down and so on. However, these methods do not use information about the correlation between features which significantly reduce the explainability of the model behaviour. In this work, we propose new methods to support model analysis by exploiting the information about the correlation between variables. The dataset level aspect importance measure is inspired by the block permutations procedure, while the instance level aspect importance measure is inspired by the LIME method. We show how to analyze groups of variables (aspects) both when they are proposed by the user and when they should be determined automatically based on the hierarchical structure of correlations between variables. Additionally, we present the new type of model visualisation, triplot, which exploits a hierarchical structure of variable grouping to produce a high information density model visualisation. This visualisation provides a consistent illustration for either local or global model and data exploration. We also show an example of real-world data with 5k instances and 37 features in which a significant correlation between variables affects the interpretation of the effect of variable importance. The proposed method is, to our knowledge, the first to allow direct use of the correlation between variables in exploratory model analysis.",0
"In order to provide accurate predictions, machine learning algorithms rely on multiple variables that contribute unique pieces of information. Traditionally, methods such as permutation feature importance or gradient boosting have been used to measure which inputs contain the most relevant information in building these predictive models. However, those metrics neglect how dependent certain features can be within specific categories, which in turn reduces their overall impact. This study proposes using triplots: scatter plots that display three variables simultaneously, taking into account the hierarchical correlation among different levels. Our approach utilizes a model agnostic framework, allowing any type of machine learning algorithm to be utilized to build these predictive models without restrictions. We evaluated our method by testing its performance on both simulated datasets as well as real data from a neuroimaging experiment involving brain connectivity analysis. Our results showed higher accuracy rates than traditional methods across all evaluations, highlighting the potential benefits of our proposed triplot framework for measuring and analyzing variable importance in predictive models. Overall, we hope to improve prediction capabilities and aid practitioners in making better decisions based off of these findings.",1
"Video super-resolution (VSR) approaches tend to have more components than the image counterparts as they need to exploit the additional temporal dimension. Complex designs are not uncommon. In this study, we wish to untangle the knots and reconsider some most essential components for VSR guided by four basic functionalities, i.e., Propagation, Alignment, Aggregation, and Upsampling. By reusing some existing components added with minimal redesigns, we show a succinct pipeline, BasicVSR, that achieves appealing improvements in terms of speed and restoration quality in comparison to many state-of-the-art algorithms. We conduct systematic analysis to explain how such gain can be obtained and discuss the pitfalls. We further show the extensibility of BasicVSR by presenting an information-refill mechanism and a coupled propagation scheme to facilitate information aggregation. The BasicVSR and its extension, IconVSR, can serve as strong baselines for future VSR approaches.",0
"In recent years, video super-resolution (VSR) has emerged as a powerful technology that allows for the upscaling of low-resolution videos into high-quality ones. However, despite significant progress in developing VSR algorithms, there remains a lack of understanding of the essential components required for effective performance. In this work, we aim to address this gap by conducting a comprehensive study on the key components of VSR systems. Our analysis reveals several important insights into the relationship between these components and their impact on the quality of the reconstructed images. Building upon our findings, we propose a novel VSR algorithm called BasicVSR which utilizes only the most fundamental components necessary for accurate image reconstruction. Experimental results demonstrate that BasicVSR achieves comparable performance to state-of-the-art methods while requiring significantly less computational resources. Additionally, we showcase the versatility of our approach by applying it to other low-level vision tasks such as denoising, deblurring, and deraining. This work serves as a step towards identifying the core building blocks of VSR and paves the way for future research in streamlining computer vision models.",1
"Many decision making systems deployed in the real world are not static - a phenomenon known as model adaptation takes place over time. The need for transparency and interpretability of AI-based decision models is widely accepted and thus have been worked on extensively. Usually, explanation methods assume a static system that has to be explained. Explaining non-static systems is still an open research question, which poses the challenge how to explain model adaptations. In this contribution, we propose and (empirically) evaluate a framework for explaining model adaptations by contrastive explanations. We also propose a method for automatically finding regions in data space that are affected by a given model adaptation and thus should be explained.",0
"Abstract: In recent years there has been growing interest in explaining machine learning models, particularly those trained on large amounts of data that exhibit complex behaviors or generate images indistinguishable from real-world examples. Traditionally, explanations have been generated by identifying features important to the model, visualizing relationships among these features and interpreting them as generating factors behind the modelâ€™s predictions. However, traditional feature attribution methods have significant limitations such as their scalability and applicability to explain complex models. To address these issues, we propose contrastive explanation which relies on searching over small perturbation directions and examining how changes made along these directions affect class probabilities assigned by the target model. The proposed method can provide more informative explanations about model behavior and enable better understanding of decision processes by identifying factors driving the model decisions. We evaluate our approach on both synthetic datasets and state-of-the-art image generation models trained on natural images demonstrating improved accuracy compared to baseline methods. Additionally, human evaluations show that our explanations are more comprehensible and actionable. Overall, our study presents evidence that contrastive explanations constitute a promising direction towards effective interpretability solutions.",1
"Black-box AI induction methods such as deep reinforcement learning (DRL) are increasingly being used to find optimal policies for a given control task. Although policies represented using a black-box AI are capable of efficiently executing the underlying control task and achieving optimal closed-loop performance, the developed control rules are often complex and neither interpretable nor explainable. In this paper, we use a recently proposed nonlinear decision-tree (NLDT) approach to find a hierarchical set of control rules in an attempt to maximize the open-loop performance for approximating and explaining the pre-trained black-box DRL (oracle) agent using the labelled state-action dataset. Recent advances in nonlinear optimization approaches using evolutionary computation facilitates finding a hierarchical set of nonlinear control rules as a function of state variables using a computationally fast bilevel optimization procedure at each node of the proposed NLDT. Additionally, we propose a re-optimization procedure for enhancing closed-loop performance of an already derived NLDT. We evaluate our proposed methodologies (open and closed-loop NLDTs) on different control problems having multiple discrete actions. In all these problems our proposed approach is able to find relatively simple and interpretable rules involving one to four non-linear terms per rule, while simultaneously achieving on par closed-loop performance when compared to a trained black-box DRL agent. A post-processing approach for simplifying the NLDT is also suggested. The obtained results are inspiring as they suggest the replacement of complicated black-box DRL policies involving thousands of parameters (making them non-interpretable) with relatively simple interpretable policies. Results are encouraging and motivating to pursue further applications of proposed approach in solving more complex control tasks.",0
"Abstract: This paper presents a novel methodology for inducing interpretable artificial intelligence (AI) policies from data using evolutionary nonlinear decision trees. The proposed approach is applicable to discrete action systems and enables humans to better understand why certain decisions were made by the AI system. In particular, we use an offline model learning technique that allows us to explicitly represent the mapping between inputs and outputs as a decision tree structure. By leveraging the interpretability afforded by decision trees, our framework can facilitate transparency auditing and explainability of complex AIs. We evaluate our method on several benchmark datasets and demonstrate its effectiveness through extensive experimentation. Our results showcase significant improvement over baseline methods in terms of accuracy and interpretability. Overall, this work contributes towards building more trustworthy and responsible AIs. Keywords: Artificial Intelligence, Policy Induction, Interpretability, Decision Tree Learning, Evolutionary Methods.",1
"Machine learning-based systems are rapidly gaining popularity and in-line with that there has been a huge research surge in the field of explainability to ensure that machine learning models are reliable, fair, and can be held liable for their decision-making process. Explainable Artificial Intelligence (XAI) methods are typically deployed to debug black-box machine learning models but in comparison to tabular, text, and image data, explainability in time series is still relatively unexplored. The aim of this study was to achieve and evaluate model agnostic explainability in a time series forecasting problem. This work focused on proving a solution for a digital consultancy company aiming to find a data-driven approach in order to understand the effect of their sales related activities on the sales deals closed. The solution involved framing the problem as a time series forecasting problem to predict the sales deals and the explainability was achieved using two novel model agnostic explainability techniques, Local explainable model-agnostic explanations (LIME) and Shapley additive explanations (SHAP) which were evaluated using human evaluation of explainability. The results clearly indicate that the explanations produced by LIME and SHAP greatly helped lay humans in understanding the predictions made by the machine learning model. The presented work can easily be extended to any time",0
"This paper presents a comprehensive evaluation framework for assessing explainability in multivariate time series analysis. Despite recent advancements in deep learning and machine learning techniques for time series forecasting, there remains a crucial need for interpretable models that can provide insights into how predictions are made. Explainability has therefore become an important aspect of time series analysis, as stakeholders require understanding of model behavior beyond raw prediction accuracy. In this study, we propose a novel evaluation metric that assesses the quality of explanations generated by different methods across multiple dimensions. Our experimental results demonstrate the effectiveness of our proposed approach in comparing state-of-the-art methods on real-world datasets. Overall, our work provides a rigorous foundation for future research in designing more transparent and trustworthy time series models.",1
"The etching process is one of the most important processes in semiconductor manufacturing. We have introduced the state-of-the-art deep learning model to predict the etching profiles. However, the significant problems violating physics have been found through various techniques such as explainable artificial intelligence and representation of prediction uncertainty. To address this problem, this paper presents a novel approach to apply the inductive biases for etching process. We demonstrate that our approach fits the measurement faster than physical simulator while following the physical behavior. Our approach would bring a new opportunity for better etching process with higher accuracy and lower cost.",0
"Here is your abstract:  A new approach to semiconductor etching has been developed that incorporates inductive biases to improve process efficiency and yield. By leveraging advanced machine learning techniques and computational models, we were able to design a system that can effectively learn from experimental data to generate accurate predictions of etching behavior under varying conditions. Our method was validated through extensive simulation studies and experiments, demonstrating significant improvements over traditional methods. This work represents an important step forward in the development of more efficient and effective semiconductor processing technologies.",1
"Generative adversarial networks (GANs) have been being widely used in various applications. Arguably, GANs are really complex, and little has been known about their generalization. In this paper, we make a comprehensive analysis about generalization of GANs. We decompose the generalization error into an explicit composition: generator error + discriminator error + optimization error. The first two errors show the capacity of the player's families, are irreducible and optimizer-independent. We then provide both uniform and non-uniform generalization bounds in different scenarios, thanks to our new bridge between Lipschitz continuity and generalization. Our bounds overcome some major limitations of existing ones. In particular, our bounds show that penalizing the zero- and first-order informations of the GAN loss will improve generalization, answering the long mystery of why imposing a Lipschitz constraint can help GANs perform better in practice. Finally, we show why data augmentation penalizes the zero- and first-order informations of the loss, helping the players generalize better, and hence explaining the highly successful use of data augmentation for GANs.",0
"In recent years, Generative Adversarial Networks (GANs) have emerged as a powerful tool for generating realistic synthetic datasets, enabling applications across several domains including computer vision, natural language processing, and robotics. However, there exist challenges related to stability, diversity, and quality control that limit their performance in practice. To address these concerns, we propose a general framework based on Lipschitz continuity and data augmentation techniques which enables the generation of diverse high-quality samples while ensuring stability during training. Our approach achieves state-of-the-art results on multiple benchmark datasets in image generation tasks, outperforming existing methods by significant margins. Additionally, our method can generate coherent sequences of images, which may open new possibilities for video synthesis with GAN models. We further demonstrate the versatility of our proposed framework by applying it to text generation, where it outperforms baseline models in both automatic evaluation metrics and human ratings. Overall, our work contributes towards improving the reliability and effectiveness of GANs in artificial intelligence research and practice.",1
"Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods.",0
"Title: Transformer interpretability beyond attention visualization Abstract This paper explores several methodologies that can enhance the understanding of how transformer models work beyond traditional attention heatmap-based techniques. Specifically, we focus on alternative ways to analyze how these powerful deep learning architectures process input data through an examination of their internal state representations at different layers during training and inference phases. By analyzing state representations across multiple architectural units within each layer (including self-attention heads), our methods aim to provide more insights into how these neural networks operate by considering both spatial and temporal contexts simultaneously. Our evaluations using publicly available datasets demonstrate the effectiveness of proposed techniques in improving the transparency and explainability of these complex machine learning systems without compromising accuracy. Additionally, we discuss future research directions in promoting better human comprehension of advanced artificial intelligence algorithms for improved trustworthiness and safety. Keywords: Transformers, attention, interpretation, explanations, state representation. Preliminary results: We experimented with six commonly used pre-trained transformer models including BERT, GPT-2, RoBERTa, DistilBERT, XLNet, and DeBERTa on four benchmark datasets comprised of diverse text domains such as language modeling, question answering, sentiment analysis, and summarization tasks. Evaluation metrics consist of quantitative measures based on attention weights and rank correlation coefficients between ground truth references and corresponding predicted outputs obtained from each technique presented herein. Overall, our findings indicate significant improvements regarding robustness and coherence compared against conventional attenti",1
"Traditionally, for most machine learning settings, gaining some degree of explainability that tries to give users more insights into how and why the network arrives at its predictions, restricts the underlying model and hinders performance to a certain degree. For example, decision trees are thought of as being more explainable than deep neural networks but they lack performance on visual tasks. In this work, we empirically demonstrate that applying methods and architectures from the explainability literature can, in fact, achieve state-of-the-art performance for the challenging task of domain generalization while offering a framework for more insights into the prediction and training process. For that, we develop a set of novel algorithms including DivCAM, an approach where the network receives guidance during training via gradient based class activation maps to focus on a diverse set of discriminative features, as well as ProDrop and D-Transformers which apply prototypical networks to the domain generalization task, either with self-challenging or attention alignment. Since these methods offer competitive performance on top of explainability, we argue that the proposed methods can be used as a tool to improve the robustness of deep neural network architectures.",0
"Here we present Explainability-Aided Domain Generalization (EADG), an approach that leverages domain-invariant explanations from black box models trained on multiple source domains to enhance accuracy on target domains without requiring additional labeled data from those targets. EADG improves upon previous work by focusing lessons learned from each task on select informative features that generalize across domains. We demonstrate EADG's effectiveness on two image classification datasets: VisDA2017 and Office Home. On both datasets, our method significantly outperforms baseline methods that don't utilize posthoc explainability, while approaching or surpassing state-of-the-art results. For example, on OfficeHome, our model achieves 49.8% accuracy compared to SOTA at 50.6%, closing the gap. Furthermore, our analysis suggests future research directions can explore different feature selection techniques and strategies to generate more diverse sets of selected features for improved performance. Overall, our work shows promise towards enabling the use of posthoc explainability for advancing DG methods.",1
"Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical field or the autonomous driving field. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing fields. In this paper, we present an overview of existing explainable AI (XAI) methods applied on time series and illustrate the type of explanations they produce. We also provide a reflection on the impact of these explanation methods to provide confidence and trust in the AI systems.",0
"This paper presents a survey on the current state of explainable artificial intelligence (XAI) applied to time series data. XAI refers to the field that focuses on making machine learning models more interpretable by human users, which has gained increasing attention in recent years as ML applications become more critical in different domains. This review provides an overview of the key ideas behind XAI, highlights some representative works using XAI techniques on time series data and their corresponding benefits and challenges. Lastly, we provide insights into future research directions in this area that could lead to further advancements in time series analysis using XAI approaches. While our work primarily focuses on XAI methods applied to univariate time series data, it can lay the foundation for extending these methodologies to multivariate settings given appropriate adaptations.",1
"Cone Beam Computed Tomography(CBCT) is a now known method to conduct CT imaging. Especially, The Low Dose CT imaging is one of possible options to protect organs of patients when conducting CT imaging. Therefore Low Dose CT imaging can be an alternative instead of Standard dose CT imaging. However Low Dose CT imaging has a fundamental issue with noises within results compared to Standard Dose CT imaging. Currently, there are lots of attempts to erase the noises. Most of methods with artificial intelligence have many parameters and unexplained layers or a kind of black-box methods. Therefore, our research has purposes related to these issues. Our approach has less parameters than usual methods by having Iterative learn-able bilateral filtering approach with Deep reinforcement learning. And we applied The Iterative learn-able filtering approach with deep reinforcement learning to sinograms and reconstructed volume domains. The method and the results of the method can be much more explainable than The other black box AI approaches. And we applied the method to Helical Cone Beam Computed Tomography(CBCT), which is the recent CBCT trend. We tested this method with on 2 abdominal scans(L004, L014) from Mayo Clinic TCIA dataset. The results and the performances of our approach overtake the results of the other previous methods.",0
"This paper proposes a novel method for low-dose helical cone beam computed tomography (CBCT) image denoising that utilizes deep reinforcement learning coupled with domain filtering techniques. Our approach takes advantage of recent advances in deep learning, specifically the use of neural networks and convolutional layers, along with traditional imaging processing methods such as Butterworth filters. By combining these two types of techniques, we can effectively reduce noise in CBCT images while maintaining high levels of detail and accuracy. We demonstrate our results on a dataset of 2D slice scans obtained from patient examinations, showing improved visualization of anatomy compared to unprocessed data sets. Overall, our work presents a promising new direction for improving the quality of medical images used for diagnostic purposes.",1
"There has been increasing interest in building deep hierarchy-aware classifiers that aim to quantify and reduce the severity of mistakes, and not just reduce the number of errors. The idea is to exploit the label hierarchy (e.g., the WordNet ontology) and consider graph distances as a proxy for mistake severity. Surprisingly, on examining mistake-severity distributions of the top-1 prediction, we find that current state-of-the-art hierarchy-aware deep classifiers do not always show practical improvement over the standard cross-entropy baseline in making better mistakes. The reason for the reduction in average mistake-severity can be attributed to the increase in low-severity mistakes, which may also explain the noticeable drop in their accuracy. To this end, we use the classical Conditional Risk Minimization (CRM) framework for hierarchy-aware classification. Given a cost matrix and a reliable estimate of likelihoods (obtained from a trained network), CRM simply amends mistakes at inference time; it needs no extra hyperparameters and requires adding just a few lines of code to the standard cross-entropy baseline. It significantly outperforms the state-of-the-art and consistently obtains large reductions in the average hierarchical distance of top-$k$ predictions across datasets, with very little loss in accuracy. CRM, because of its simplicity, can be used with any off-the-shelf trained model that provides reliable likelihood estimates.",0
"This study presents new methods for improving error rates during deep network testing by manipulating likelihood functions without any additional costs. Our approach involves adjusting existing techniques to optimize errors more effectively. We demonstrate that our method leads to improved accuracy compared to traditional approaches. Additionally, we show that our technique can generalize well across multiple domains, making it highly applicable. Overall, our work represents an important contribution towards enhancing error resilience in state-of-the art deep networks while maintaining high performance.",1
"Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling based approaches tend to be over-simplistic, inducing non-negligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists in decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model, no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefits generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reaction-diffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters.",0
"This paper presents a novel method for augmenting physical models with deep networks to improve their ability to forecast complex dynamics. Many physical systems can be described using mathematical models that capture important characteristics such as conservation laws and known physics relationships. However, these models often have limitations when faced with nonlinearities, parameter uncertainty, and other challenges. To address these issues, we propose leveraging deep neural networks (DNNs) which are trained on data generated from simulations or experiments, alongside traditional physical modeling techniques. Our approach enables more accurate predictions across a wider range of operating conditions than would otherwise be possible through purely physically based approaches. We demonstrate the effectiveness of our framework on two distinct application areas: fluid flow in porous media and aerodynamic forces acting on aircraft wings. Experimental results show improved prediction accuracy compared to standard methods alone, and comparisons against alternative machine learning approaches highlight the benefits of integrating both physical knowledge and DNNs. Overall, our work represents a promising step towards bridging the gap between theory and practice in complex system modeling.",1
"Modeling time-evolving knowledge graphs (KGs) has recently gained increasing interest. Here, graph representation learning has become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to interpret their predictions. This paper provides a link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the structural dependencies and the temporal dynamics. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and by attention propagation. Our approach provides human-understandable evidence explaining the forecast. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model obtains a relative improvement of up to 20% on Hits@1 compared to the previous best KG forecasting method. We also conduct a survey with 53 respondents, and the results show that the evidence extracted by the model for link forecasting is aligned with human understanding.",0
"This paper presents a novel method for forecasting future links using temporal knowledge graphs by leveraging explainable reasoning techniques. In recent years, there has been increasing interest in exploiting structured knowledge stored in knowledge graphs for tasks such as link prediction, where new connections between entities can be predicted based on existing patterns and relationships. However, many link prediction approaches lack transparency and interpretability, which raises concerns regarding their reliability and trustworthiness. To address these issues, we propose an approach called ""xERTE"" that incorporates explainable reasoning into the process of predicting future links. Our method utilizes temporal constraints derived from the evolution of a domain over time, along with other contextual factors, to identify plausible paths connecting entities in the graph. We evaluate our method against state-of-the-art baseline models on several datasets and demonstrate its superior performance in terms of accuracy and comprehensibility. Additionally, we provide insights into the factors driving successful predictions and illustrate how our approach can help practitioners make informed decisions by providing clear explanations for suggested outcomes. Overall, our work contributes to the development of more transparent, interpretable link prediction methods that can foster greater trust in artificial intelligence systems while improving their effectiveness.",1
"OneClass SVM is a popular method for unsupervised anomaly detection. As many other methods, it suffers from the black box problem: it is difficult to justify, in an intuitive and simple manner, why the decision frontier is identifying data points as anomalous or non anomalous. Such type of problem is being widely addressed for supervised models. However, it is still an uncharted area for unsupervised learning. In this paper, we evaluate several rule extraction techniques over OneClass SVM models, as well as present alternative designs for some of those algorithms. Together with that, we propose algorithms to compute metrics related with eXplainable Artificial Intelligence (XAI) regarding the ""comprehensibility"", ""representativeness"", ""stability"" and ""diversity"" of the extracted rules. We evaluate our proposals with different datasets, including real-world data coming from industry. With this, our proposal contributes to extend XAI techniques to unsupervised machine learning models.",0
"In recent years there has been growing interest in developing unsupervised anomaly detection methods that can provide more explainable results than their supervised counterparts. Many such methods rely on rule extraction to generate interpretable patterns from detected anomalies. However, traditional rule extraction techniques based on decision trees suffer from limitations such as low interpretability due to high complexity and noise sensitivity. This paper presents a novel approach that leverages the simplicity and interpretability properties of one class support vector machines (OneClass SVM) to extract rules for unsupervised anomaly detection under different quality measures. Experimental evaluation shows improved performance over state-of-the-art approaches while retaining simplicity and interpretability through extracted rules. Our method demonstrates promise for practitioners seeking more transparent models for anomaly detection applications where human intervention may be required to verify true positives and better assess model behavior.   This work proposes a novel approach to unsupervised anomaly detection using rule extraction applied to OneClass Support Vector Machines (SVM). OneClass SVM has gained popularity as a simple yet effective anomaly detection technique that models normal data distribution without relying on labeled outliers or anomalies. By contrast, traditional decision tree algorithms used for rule extraction often lead to complex rules that are difficult to interpret and prone to noisy information. Our proposed framework utilizes the simplicity and interpretability of OneClass SVM for rule extraction, resulting in easily comprehensible patterns from detected anomalies. Experiments conducted on numerous datasets demonstrate our method's superiority compared to current state-of-the-art techniques. Practitioners seeking transparent models that enable human oversight i",1
"Semi-supervised learning via learning from limited quantities of labeled data has been investigated as an alternative to supervised counterparts. Maximizing knowledge gains from copious unlabeled data benefit semi-supervised learning settings. Moreover, learning multiple tasks within the same model further improves model generalizability. We propose a novel multitask learning model, namely MultiMix, which jointly learns disease classification and anatomical segmentation in a sparingly supervised manner, while preserving explainability through bridge saliency between the two tasks. Our extensive experimentation with varied quantities of labeled data in the training sets justify the effectiveness of our multitasking model for the classification of pneumonia and segmentation of lungs from chest X-ray images. Moreover, both in-domain and cross-domain evaluations across the tasks further showcase the potential of our model to adapt to challenging generalization scenarios.",0
"Incorporate the keywords (sparing; supervision; extreme; multimodality). You may use keyword synonyms if they better fit contextually. In this paper we present Multimix, a novel framework that enables sparingly supervised learning from medical images across multiple modalities using deep neural networks. Our approach addresses key challenges in high dimensional and complex biomedical image analysis where annotation costs are extremely expensive. We design three models under our architecture umbrella; namely, MultiMix-Sparse, MultiMix-MultiModal, and MultiMix-Hierarchical. Experiments on large scale clinical datasets demonstrate state-of-the art performance that outperforms existing methods, showcasing great potential for real world applications by enabling more efficient utilization of annotator resources. Furthermore, through ablation studies and qualitative analyses, we analyze how each component contributes to overall improvement over baselines and competitors, providing insights for future research directions in medical imaging and computer vision communities. Ultimately, Multimix has the capability to facilitate breakthrough advancements in healthcare services globally by reducing costs associated with data labeling while improving diagnostic accuracy and treatment effectiveness.",1
"Neural Tangent Kernel (NTK) theory is widely used to study the dynamics of infinitely-wide deep neural networks (DNNs) under gradient descent. But do the results for infinitely-wide networks give us hints about the behavior of real finite-width ones? In this paper, we study empirically when NTK theory is valid in practice for fully-connected ReLU and sigmoid DNNs. We find out that whether a network is in the NTK regime depends on the hyperparameters of random initialization and the network's depth. In particular, NTK theory does not explain the behavior of sufficiently deep networks initialized so that their gradients explode as they propagate through the network's layers: the kernel is random at initialization and changes significantly during training in this case, contrary to NTK theory. On the other hand, in the case of vanishing gradients, DNNs are in the the NTK regime but become untrainable rapidly with depth. We also describe a framework to study generalization properties of DNNs, in particular the variance of network's output function, by means of NTK theory and discuss its limits.",0
"Abstract The problem addressed here concerns whether finite neural networks can be analyzed reliably using the tangent kernel method (TKM) recently introduced by Daniel Soudry et al., which provides approximate solutions for fixed point problems that occur during backpropagation. While initial results seemed promising, there have been claims questioning the validity of TKM under certain conditions such as ill-conditioned input matrices and extremely deep architectures. Our analysis suggests that these criticisms may not apply when the size of weights remains small relative to the input dimension. Thus we find that even very deep networks with many hidden units exhibit favorable regularization properties when their weights remain sparse enough. Furthermore, our simulations based on real datasets corroborate our theory and show consistent improvement over random weight initialization methods commonly used today. We conclude that TKM represents an important tool that practitioners should consider utilizing more extensively in conjunction with other techniques such as dropout when designing high-quality machine learning models.",1
"Spatiotemporal data mining (STDM) discovers useful patterns from the dynamic interplay between space and time. Several available surveys capture STDM advances and report a wealth of important progress in this field. However, STDM challenges and problems are not thoroughly discussed and presented in articles of their own. We attempt to fill this gap by providing a comprehensive literature survey on state-of-the-art advances in STDM. We describe the challenging issues and their causes and open gaps of multiple STDM directions and aspects. Specifically, we investigate the challenging issues in regards to spatiotemporal relationships, interdisciplinarity, discretisation, and data characteristics. Moreover, we discuss the limitations in the literature and open research problems related to spatiotemporal data representations, modelling and visualisation, and comprehensiveness of approaches. We explain issues related to STDM tasks of classification, clustering, hotspot detection, association and pattern mining, outlier detection, visualisation, visual analytics, and computer vision tasks. We also highlight STDM issues related to multiple applications including crime and public safety, traffic and transportation, earth and environment monitoring, epidemiology, social media, and Internet of Things.",0
"As Big Data becomes more prevalent, there is a growing need for efficient methods to extract meaningful insights from these vast collections of data. Spatiotemporal data mining has emerged as a powerful tool for analyzing datasets that have both spatial and temporal dimensions. However, spatiotemporal data mining presents unique challenges due to its complex nature and requires advanced algorithms and techniques. In this survey, we provide an overview of the state-of-the-art spatiotemporal data mining approaches and their applications in different domains. We highlight the key challenges associated with spatiotemporal data mining such as dealing with missing values, handling large datasets, and integrating multiple sources of information. Additionally, we discuss some open problems and future research directions for the field. This comprehensive review will serve as a valuable resource for practitioners and researchers alike who want to gain insights into the latest advancements in spatiotemporal data mining and understand how they can apply them to real-world scenarios.",1
"Conventionally, random forests are built from ""greedy"" decision trees which each consider only one split at a time during their construction. The sub-optimality of greedy implementation has been well-known, yet mainstream adoption of more sophisticated tree building algorithms has been lacking. We examine under what circumstances an implementation of less greedy decision trees actually yields outperformance. To this end, a ""stepwise lookahead"" variation of the random forest algorithm is presented for its ability to better uncover binary feature interdependencies. In contrast to the greedy approach, the decision trees included in this random forest algorithm, each simultaneously consider three split nodes in tiers of depth two. It is demonstrated on synthetic data and financial price time series that the lookahead version significantly outperforms the greedy one when (a) certain non-linear relationships between feature-pairs are present and (b) if the signal-to-noise ratio is particularly low. A long-short trading strategy for copper futures is then backtested by training both greedy and stepwise lookahead random forests to predict the signs of daily price returns. The resulting superior performance of the lookahead algorithm is at least partially explained by the presence of ""XOR-like"" relationships between long-term and short-term technical indicators. More generally, across all examined datasets, when no such relationships between features are present, performance across random forests is similar. Given its enhanced ability to understand the feature-interdependencies present in complex systems, this lookahead variation is a useful extension to the toolkit of data scientists, in particular for financial machine learning, where conditions (a) and (b) are typically met.",0
Here we describe methods used on synthetic time series data (where ground truth was known) as well as real world use cases where stepwise lookahead decision forests were applied successfully with high recall for identifying significant interdependencies within large feature spaces that contained significant amounts of noise. We close by discussing the limitations of our approach and suggest directions for future work.,1
"Current state-of-the-art visual recognition systems usually rely on the following pipeline: (a) pretraining a neural network on a large-scale dataset (e.g., ImageNet) and (b) finetuning the network weights on a smaller, task-specific dataset. Such a pipeline assumes the sole weight adaptation is able to transfer the network capability from one domain to another domain, based on a strong assumption that a fixed architecture is appropriate for all domains. However, each domain with a distinct recognition target may need different levels/paths of feature hierarchy, where some neurons may become redundant, and some others are re-activated to form new network structures. In this work, we prove that dynamically adapting network architectures tailored for each domain task along with weight finetuning benefits in both efficiency and effectiveness, compared to the existing image recognition pipeline that only tunes the weights regardless of the architecture. Our method can be easily generalized to an unsupervised paradigm by replacing supernet training with self-supervised learning in the source domain tasks and performing linear evaluation in the downstream tasks. This further improves the search efficiency of our method. Moreover, we also provide principled and empirical analysis to explain why our approach works by investigating the ineffectiveness of existing neural architecture search. We find that preserving the joint distribution of the network architecture and weights is of importance. This analysis not only benefits image recognition but also provides insights for crafting neural networks. Experiments on five representative image recognition tasks such as person re-identification, age estimation, gender recognition, image classification, and unsupervised domain adaptation demonstrate the effectiveness of our method.",0
"Artificial neural networks have revolutionized image recognition by automatically learning high level feature representations directly from raw pixels. However, as network architectures grow larger, manually designing efficient models becomes increasingly difficult. This work proposes a novel methodology that jointly learns transfer (adapting pre-trained knowledge) and architecture adaptation (automatically selecting model components). We demonstrate two approaches: first, we formulate a multi-task learning problem where pretext task loss constrains both tasks to share similar features while maximizing performance on downstream classification. Secondly, we integrate a meta-learning approach that enables one neural architecture to produce multiple models which can each achieve strong results based on their given pretraining regime. Extensive evaluation shows consistent improvement over strong baselines across three benchmark datasets, CIFAR10/100, SVHN, and STL-10. Our proposed methods provide insights into automating architecture search procedures in deep learning while boosting overall accuracy without requiring significant computational resources.",1
"Is critical input information encoded in specific sparse pathways within the neural network? In this work, we discuss the problem of identifying these critical pathways and subsequently leverage them for interpreting the network's response to an input. The pruning objective -- selecting the smallest group of neurons for which the response remains equivalent to the original network -- has been previously proposed for identifying critical pathways. We demonstrate that sparse pathways derived from pruning do not necessarily encode critical input information. To ensure sparse pathways include critical fragments of the encoded input information, we propose pathway selection via neurons' contribution to the response. We proceed to explain how critical pathways can reveal critical input features. We prove that pathways selected via neuron contribution are locally linear (in an L2-ball), a property that we use for proposing a feature attribution method: ""pathway gradient"". We validate our interpretation method using mainstream evaluation experiments. The validation of pathway gradient interpretation method further confirms that selected pathways using neuron contributions correspond to critical input features. The code is publicly available.",0
"Title: ""The Power of Beliefs""  Introduction: Many of us hold beliefs that guide our actions and shape our decisions every day. While some may view these beliefs as simply opinions, research has shown that they can have powerful effects on our lives and even impact our physical health. In his book ""The Biology of Belief,"" Bruce Lipton argues that our thoughts, attitudes, and beliefs play a crucial role in determining how our genes express themselves, ultimately affecting our health outcomes. With this understanding comes the potential to harness the power of our beliefs to improve our well-being and achieve greater success in life. By exploring case studies and expert insights, we aim to provide readers with practical tools for cultivating positive belief systems that support their goals.",1
"Image recognition with prototypes is considered an interpretable alternative for black box deep learning models. Classification depends on the extent to which a test image ""looks like"" a prototype. However, perceptual similarity for humans can be different from the similarity learned by the classification model. Hence, only visualising prototypes can be insufficient for a user to understand what a prototype exactly represents, and why the model considers a prototype and an image to be similar. We address this ambiguity and argue that prototypes should be explained. We improve interpretability by automatically enhancing visual prototypes with textual quantitative information about visual characteristics deemed important by the classification model. Specifically, our method clarifies the meaning of a prototype by quantifying the influence of colour hue, shape, texture, contrast and saturation and can generate both global and local explanations. Because of the generality of our approach, it can improve the interpretability of any similarity-based method for prototypical image recognition. In our experiments, we apply our method to the existing Prototypical Part Network (ProtoPNet). Our analysis confirms that the global explanations are generalisable, and often correspond to the visually perceptible properties of a prototype. Our explanations are especially relevant for prototypes which might have been interpreted incorrectly otherwise. By explaining such 'misleading' prototypes, we improve the interpretability and simulatability of a prototype-based classification model. We also use our method to check whether visually similar prototypes have similar explanations, and are able to discover redundancy. Code is available at https://github.com/M-Nauta/Explaining_Prototypes .",0
"In recent years, there has been significant progress in developing accurate image recognition models that can outperform human performance on specific tasks. However, these state-of-the-art systems often lack interpretability, making it difficult for users to understand how they arrived at their predictions. To address this issue, researchers have proposed the use of prototypes as explanatory tools.  Prototypes are representative examples or archetypes of classes found within a dataset, which capture key features and characteristics shared among instances belonging to each class. They provide interpretable visual representations that can explain how different data points relate to one another and aid in understanding model behavior.  This paper presents a detailed analysis of prototype usage in image recognition. We discuss various ways in which prototypes can enhance system transparency by explaining decisions made during inference stages. By examining existing literature and real-world applications, we demonstrate the effectiveness of prototypes across diverse scenarios ranging from object detection and classification to generative adversarial networks (GANs) and contrastive learning.  Our study emphasizes several benefits derived from using prototypes such as facilitating error diagnosis, improving user trust through better communication, enabling fine-grained control over models, and even serving as priors for regularization purposes. While acknowledging limitations related to computational cost, sensitivity to hyperparameters, and potential discrepancies among different approaches, we conclude that prototypes hold tremendous promise for the future of interpretable image recognition.",1
"We present a simple regularization of adversarial perturbations based upon the perceptual loss. While the resulting perturbations remain imperceptible to the human eye, they differ from existing adversarial perturbations in that they are semi-sparse alterations that highlight objects and regions of interest while leaving the background unaltered. As a semantically meaningful adverse perturbations, it forms a bridge between counterfactual explanations and adversarial perturbations in the space of images. We evaluate our approach on several standard explainability benchmarks, namely, weak localization, insertion deletion, and the pointing game demonstrating that perceptually regularized counterfactuals are an effective explanation for image-based classifiers.",0
"Advances in deep learning have led to the development of powerful classifiers that can achieve state-of-the art performance across a wide range of tasks. However, these models remain largely black boxes, making it difficult to interpret their predictions and understand how they make decisions. This study proposes a novel approach to explain classifiers by analyzing the effects of adversarial perturbations on the perceptual ball, which represents the decision boundary of the model as a high-dimensional sphere. By perturbed the data along different directions in the input space and observing how these changes affect the prediction of the classifier, we gain insights into the characteristics of the decision boundary and the underlying patterns that the model has learned from the training data. Our results show that this method can effectively reveal important features and relationships present in complex datasets, enabling a better understanding of the inner workings of modern classifiers. Moreover, our framework offers several advantages over traditional methods such as feature visualization and sensitivity analysis, including improved robustness to noise and outliers and the ability to handle multi-class problems. Overall, this research contributes towards the goal of building more interpretable machine learning models, paving the way for further advancements in artificial intelligence.",1
"Building interpretable parameterizations of real-world decision-making on the basis of demonstrated behavior -- i.e. trajectories of observations and actions made by an expert maximizing some unknown reward function -- is essential for introspecting and auditing policies in different institutions. In this paper, we propose learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to ""what if"" outcomes: Given the current history of observations, what would happen if we took a particular action? To learn these cost-benefit tradeoffs associated with the expert's actions, we integrate counterfactual reasoning into batch inverse reinforcement learning. This offers a principled way of defining reward functions and explaining expert behavior, and also satisfies the constraints of real-world decision-making -- where active experimentation is often impossible (e.g. in healthcare). Additionally, by estimating the effects of different actions, counterfactuals readily tackle the off-policy nature of policy evaluation in the batch setting, and can naturally accommodate settings where the expert policies depend on histories of observations rather than just current states. Through illustrative experiments in both real and simulated medical environments, we highlight the effectiveness of our batch, counterfactual inverse reinforcement learning approach in recovering accurate and interpretable descriptions of behavior.",0
"This paper presents a method for learning explanatory representations that can predict how an agent should act given different hypothetical scenarios (i.e., ""what if"" questions). These representations, referred to as ""explanatory models"", are learned concurrently with a policy that maximizes return from actions taken by the agent. We propose using variational inference to learn both the policy and model parameters, allowing us to optimize the expected utility of action sequences while regularizing against overfitting. Our results demonstrate the effectiveness of our approach on benchmark tasks, providing evidence that explanatory models can improve decision making and reduce uncertainty in sequential settings.",1
"The convolutional neural networks (CNNs) trained on ILSVRC12 ImageNet were the backbone of various applications as a generic classifier, a feature extractor or a base model for transfer learning. This paper describes automated heuristics based on model consensus, explainability and confident learning to correct labeling mistakes and remove ambiguous images from this dataset. After making these changes on the training and validation sets, the ImageNet-Clean improves the model performance by 2-2.4 % for SqueezeNet and EfficientNet-B0 models. The results support the importance of larger image corpora and semi-supervised learning, but the original datasets must be fixed to avoid transmitting their mistakes and biases to the student learner. Further contributions describe the training impacts of widescreen input resolutions in portrait and landscape orientations. The trained models and scripts are published on Github (https://github.com/kecsap/imagenet-clean) to clean up ImageNet and ImageNetV2 datasets for reproducible research.",0
"This work presents a method for automated cleaning up the ImageNet dataset using model consensus, explainability, and confident learning. In this approach, multiple state-of-the-art models are first trained on different subsets of images from the ImageNet dataset. Then, these models are used to create a set of confidence scores for each image that indicate how likely it is that the annotation of that image is correct. Finally, the top scoring images are selected as the most reliable and accurate images and removed from consideration during cleanup. Through extensive experiments, we show that our proposed method outperforms previous methods for data cleaning and results in improved accuracy across several downstream tasks. Our method represents a significant step towards building robust and trustworthy datasets for machine learning applications. (Note: The abstract should describe the core problem addressed, motivation behind addressing the problem, key ideas and techniques in solving the problem, experimental evaluation of the solution, and implications/significance of the contribution) This research addresses the challenges faced in maintaining large scale visual recognition datasets such as ImageNet due to manual labeling errors and inconsistencies. To tackle this issue, our study proposes a novel method leveraging modern deep learning approaches. Specifically, multiple state-of-the-art models are utilized as ensemble members to generate a subset of high quality images by relying on their individual predictive uncertainties. Experiments demonstrate the effectiveness of our algorithm compared to traditional filtering criteria based solely on human labels and other recent methods. Furthermore, we assess the impact of selecting different subsets of candidate training samples and the use of ensembles with diverse architectures. Overall, our findings provide valuable insights into improving the reliability and performance of image classification tasks under uncertain conditions, which has broad implications in fields such as computer vision and robotics.",1
"Deep Reinforcement Learning (DRL) has recently achieved significant advances in various domains. However, explaining the policy of RL agents still remains an open problem due to several factors, one being the complexity of explaining neural networks decisions. Recently, a group of works have used decision-tree-based models to learn explainable policies. Soft decision trees (SDTs) and discretized differentiable decision trees (DDTs) have been demonstrated to achieve both good performance and share the benefit of having explainable policies. In this work, we further improve the results for tree-based explainable RL in both performance and explainability. Our proposal, Cascading Decision Trees (CDTs) apply representation learning on the decision path to allow richer expressivity. Empirical results show that in both situations, where CDTs are used as policy function approximators or as imitation learners to explain black-box policies, CDTs can achieve better performances with more succinct and explainable models than SDTs. As a second contribution our study reveals limitations of explaining black-box policies via imitation learning with tree-based explainable models, due to its inherent instability.",0
"Cascading decision trees (CDT) can improve explainability by allowing users to see each individual step taken along a reward maximization path. These paths could then be used to identify which decisions were most impactful for achieving rewards and overall success. Additionally, using decision trees allows for easy visual representation of the results. However, these benefits come at the cost of increased memory usage due to storing the large number of trees created during training. Furthermore, choosing hyperparameters for decision tree creation remains challenging.",1
"In this work, we present a framework for product quality inspection based on deep learning techniques. First, we categorize several deep learning models that can be applied to product inspection systems. Also we explain entire steps for building a deep learning-based inspection system in great detail. Second, we address connection schemes that efficiently link the deep learning models to the product inspection systems. Finally, we propose an effective method that can maintain and enhance the deep learning models of the product inspection system. It has good system maintenance and stability due to the proposed methods. All the proposed methods are integrated in a unified framework and we provide detailed explanations of each proposed method. In order to verify the effectiveness of the proposed system, we compared and analyzed the performance of methods in various test scenarios.",0
"Abstract ------------------------------- The development of deep learning algorithms has enabled computers to automatically learn from large amounts of data and perform complex tasks that were previously impossible. One area where these technologies have shown promise is product inspection, which involves examining items such as manufactured goods, food products, and industrial components to verify their quality, safety, and compliance with regulations. This overview describes how deep learning can improve the efficiency and accuracy of product inspection processes by automating many aspects of visual inspections and enabling experts to focus on more critical issues. We present several case studies demonstrating successful applications of deep learning techniques to real world problems, including object detection, defect analysis, and image classification. The authors discuss future research directions and potential limitations of using deep learning for product inspection in different industries. (Abstract continues below) The key findings indicate that the integration of deep learning technology into current inspection methodologies has significant potential to enhance overall inspection effectiveness while improving operational performance. The article provides valuable insights and guidelines for decision makers considering adopting these tools to increase profitability and competitiveness. The paper offers recommendations for further exploring the benefits of integrating computer vision systems based on artificial intelligence into traditional methods of product evaluation. By combining machine learning algorithms and advanced sensors with human expertise, organizations can achieve faster turnaround times, reduced costs, greater accuracy and reliability of results. These improvements ultimately lead to better customer satisfaction and increased revenue growth.",1
"There are several effective methods in explaining the inner workings of convolutional neural networks (CNNs). However, in general, finding the inverse of the function performed by CNNs as a whole is an ill-posed problem. In this paper, we propose a method based on adjoint operators to reconstruct, given an arbitrary unit in the CNN (except for the first convolutional layer), its effective hypersurface in the input space that replicates that unit's decision surface conditioned on a particular input image. Our results show that the hypersurface reconstructed this way, when multiplied by the original input image, would give nearly the exact output value of that unit. We find that the CNN unit's decision surface is largely conditioned on the input, and this may explain why adversarial inputs can effectively deceive CNNs.",0
"Deep learning architectures based on convolutional neural networks (CNNs) have achieved state-of-the-art performance across many computer vision tasks, including image classification. However, understanding how these models make decisions remains a challenging problem due to their complex structure and the large number of parameters involved. In particular, while visualizing individual filters has proven useful for identifying feature detection patterns, understanding which features contribute most significantly to model predictions is still elusive. In this work we propose AdjointBackMap, an approach that uses adjoint operators from backpropagation to efficiently compute gradients of output logits with respect to input pixels directly inside each layer neuron instead of through all layers. We first apply an anchor filter to select top activating examples corresponding to each pixel location and then use adjoints to compute per-pixel importance values representing relative contributions to network outputs in terms of evidence given by specific inputs in the images. With these importance scores, our method can effectively reconstruct decision boundaries on hypersurfaces defined using different thresholds around average importance values. Our results demonstrate the effectiveness of AdjointBackMap compared to prior methods for both CIFAR-10 and ILSVRC2016 datasets achieving accuracy improvements over existing approaches. Furthermore, as a result of our improved visualizations, we provide insights into the behavior of learned representations and guide future design choices towards better interpretability and transparency of deep learning systems.",1
"Few-shot classification studies the problem of quickly adapting a deep learner to understanding novel classes based on few support images. In this context, recent research efforts have been aimed at designing more and more complex classifiers that measure similarities between query and support images, but left the importance of feature embeddings seldom explored. We show that the reliance on sophisticated classifier is not necessary and a simple classifier applied directly to improved feature embeddings can outperform state-of-the-art methods. To this end, we present a new method named \textbf{DCAP} in which we investigate how one can improve the quality of embeddings by leveraging \textbf{D}ense \textbf{C}lassification and \textbf{A}ttentive \textbf{P}ooling. Specifically, we propose to pre-train a learner on base classes with abundant samples to solve dense classification problem first and then fine-tune the learner on a bunch of randomly sampled few-shot tasks to adapt it to few-shot scenerio or the test time scenerio. We suggest to pool feature maps by applying attentive pooling instead of the widely used global average pooling (GAP) to prepare embeddings for few-shot classification during meta-finetuning. Attentive pooling learns to reweight local descriptors, explaining what the learner is looking for as evidence for decision making. Experiments on two benchmark datasets show the proposed method to be superior in multiple few-shot settings while being simpler and more explainable. Code is available at: \url{https://github.com/Ukeyboard/dcap/}.",0
"This work revisits deep local descriptors as a means of improving few-shot classification performance. We demonstrate that by carefully designing feature extractors which attend to discriminative features at multiple scales we can achieve state-of-the-art results on standard benchmark datasets. Our approach utilizes simple baseline models with minimal hyperparameter tuning, demonstrating the robustness of our methodology. We provide comprehensive analysis of model behavior, confirming that learned representations indeed generalize well across tasks. Overall, we showcase the potential of revisiting classical computer vision concepts in modern machine learning settings.",1
"Representational sparsity is known to affect robustness to input perturbations in deep neural networks (DNNs), but less is known about how the semantic content of representations affects robustness. Class selectivity-the variability of a unit's responses across data classes or dimensions-is one way of quantifying the sparsity of semantic representations. Given recent evidence that class selectivity may not be necessary for, and in some cases can impair generalization, we investigate whether it also confers robustness (or vulnerability) to perturbations of input data. We found that networks regularized to have lower levels of class selectivity were more robust to average-case (naturalistic) perturbations, while networks with higher class selectivity are more vulnerable. In contrast, class selectivity increases robustness to multiple types of worst-case (i.e. white box adversarial) perturbations, suggesting that while decreasing class selectivity is helpful for average-case perturbations, it is harmful for worst-case perturbations. To explain this difference, we studied the dimensionality of the networks' representations: we found that the dimensionality of early-layer representations is inversely proportional to a network's class selectivity, and that adversarial samples cause a larger increase in early-layer dimensionality than corrupted samples. Furthermore, the input-unit gradient is more variable across samples and units in high-selectivity networks compared to low-selectivity networks. These results lead to the conclusion that units participate more consistently in low-selectivity regimes compared to high-selectivity regimes, effectively creating a larger attack surface and hence vulnerability to worst-case perturbations.",0
"Dimensionality has become an increasingly important concept as our lives become more reliant on artificial intelligence systems. One of the key challenges facing these systems is how well they can handle input from human users who may provide incomplete or incorrect data. To address this challenge, researchers have developed methods that use machine learning algorithms to identify patterns in large datasets, allowing them to make predictions about future inputs even if those inputs contain errors. In this paper, we explore the relationship between two types of uncertainty: average-case perturbation robustness, which describes how resilient an algorithm is to random noise in the data; and worst-case perturbation robustness, which measures the impact of deliberately adversarial attacks on the system. We show that these two concepts are related through the idea of class selectivity, which refers to the ability of a model to distinguish different classes of objects based on their features. By analyzing data across several dimensions, including image recognition tasks, text classification, and tabular data analysis, we demonstrate that high levels of class selectivity lead to greater robustness against both average-case and worst-case uncertainties. This finding highlights the importance of designing models that are both accurate and interpretable, rather than simply prioritizing performance at all costs. Our work contributes to the broader conversation about building trustworthy AI systems by offering insights into the factors that influence their robustness under real-world conditions.",1
"Understanding the algorithmic bias of \emph{stochastic gradient descent} (SGD) is one of the key challenges in modern machine learning and deep learning theory. Most of the existing works, however, focus on \emph{very small or even infinitesimal} learning rate regime, and fail to cover practical scenarios where the learning rate is \emph{moderate and annealing}. In this paper, we make an initial attempt to characterize the particular regularization effect of SGD in the moderate learning rate regime by studying its behavior for optimizing an overparameterized linear regression problem. In this case, SGD and GD are known to converge to the unique minimum-norm solution; however, with the moderate and annealing learning rate, we show that they exhibit different \emph{directional bias}: SGD converges along the large eigenvalue directions of the data matrix, while GD goes after the small eigenvalue directions. Furthermore, we show that such directional bias does matter when early stopping is adopted, where the SGD output is nearly optimal but the GD output is suboptimal. Finally, our theory explains several folk arts in practice used for SGD hyperparameter tuning, such as (1) linearly scaling the initial learning rate with batch size; and (2) overrunning SGD with high learning rate even when the loss stops decreasing.",0
"As deep learning models continue to gain popularity and are widely used across different domains, optimizing their performance has become crucial. One of the key factors affecting the training process is the choice of optimization algorithm. In particular, stochastic gradient descent (SGD) is commonly employed due to its efficiency and scalability. However, recent studies have suggested that SGD may exhibit implicit biases that can lead to suboptimal results. This paper investigates these biases under realistic settings where the learning rate is moderately large. By analyzing the behavior of SGD using both theoretical analysis and numerical experiments, we identify two important sources of bias: the directional bias and the step size bias. Our findings show that these biases can significantly impact the convergence speed and accuracy of the model. Furthermore, our study highlights the importance of considering the specific conditions under which SGD is applied and provides insights into how to mitigate these biases through appropriate hyperparameter tuning. Overall, this work contributes to a better understanding of the limitations and strengths of SGD and can inform future research on designing more effective optimization algorithms for deep learning models.",1
"Transformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model's input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability.",0
"Title: Exploring the Usefulness of Generative Attention Models for Explaining Diverse NLP Architectures Abstract In natural language processing (NLP), model interpretability is becoming increasingly important as models become more complex and are applied in critical decision making tasks such as healthcare, education, finance, law, etc. Many popular neural network architectures used in NLP, such as encoder-decoders and transformer networks, often lack transparency, leading to concerns regarding their trustworthiness in high stakes applications. To address these challenges, our work explores the use of generic attention mechanisms (GAMs) as a means of generating interpretable explanations for bi-modal and uni-modal input sequences from these diverse architecture types. Our experiments demonstrate that GAMs can effectively generate faithful visualizations of how input data flows through deep NLP models, even for architectures which were never trained end-to-end on the inputs they are provided during explanation generation. We discuss both strengths and limitations of our approach in detail, highlighting key tradeoffs between fidelity of generated attributions against computational cost required to produce them at scale. Overall, we believe that our findings provide insights into techniques which may broaden accessibility of powerful pre-trained language models for non-specialist users by enabling intuitive post hoc interpretation of complex outputs without requiring direct modification of underlying systems themselves. Keywords: Model interpretability, generative attention mechanisms, encoder-decoders, transformers, explainable artificial intelligence",1
"Making deep neural networks robust to small adversarial noises has recently been sought in many applications. Adversarial training through iterative projected gradient descent (PGD) has been established as one of the mainstream ideas to achieve this goal. However, PGD is computationally demanding and often prohibitive in case of large datasets and models. For this reason, single-step PGD, also known as FGSM, has recently gained interest in the field. Unfortunately, FGSM-training leads to a phenomenon called ``catastrophic overfitting,"" which is a sudden drop in the adversarial accuracy under the PGD attack. In this paper, we support the idea that small input gradients play a key role in this phenomenon, and hence propose to zero the input gradient elements that are small for crafting FGSM attacks. Our proposed idea, while being simple and efficient, achieves competitive adversarial accuracy on various datasets.",0
"Advances in deep learning have brought us powerful image generation models such as DALL-E2 and Midjourney, but these models remain vulnerable to adversarial examples even after robust training procedures like feature denoising autoencoders (FDA) that introduce random noise into the input images during training. In our work we propose ZeroGrad, a new method based on fine-grained smoothing functions that smoothes gradients rather than adding noise. We demonstrate through extensive experiments that ZeroGrad effectively mitigates overfitting while maintaining high performance, significantly outperforming state-of-the-art approaches. Moreover, our approach provides insight into the causes of catastrophic forgetting by revealing which layers contribute most to the modelâ€™s fragility towards adversaries. Our findings suggest that gradient suppression alone cannot explain why adversarially trained models generalize worse than their clean counterparts. By showing how each layer contributes differently depending on whether the network has been exposed to adversarial examples, our analysis highlights the importance of developing better understanding of internal representations learned by adversarially trained models. Additionally, we show that there exist â€œsafeâ€ layers in adversarially trained networks which can resist large perturbations without sacrificing accuracy on clean inputs. Finally, we provide qualitative results demonstrating that our approach generates visually coherent samples, similar to those generated by FGA methods such as FSAP and SPADE, which further supports its effectiveness. Overall, our work addresses one of the key challenges in generative adversarial networksâ€”how to balance resistance against adversarial attacks wi",1
"The Optimal transport (OT) problem is rapidly finding its way into machine learning. Favoring its use are its metric properties. Many problems admit solutions that guarantee only for objects embedded in metric spaces, and the use of non-metrics can complicate them. Multi-marginal OT (MMOT) generalizes OT to simultaneously transporting multiple distributions. It captures important relations that are missed if the transport is pairwise. Research on MMOT, however, has been focused on its existence, uniqueness, practical algorithms, and the choice of cost functions. There is a lack of discussion on the metric properties of MMOT, which limits its theoretical and practical use. Here, we prove that (pairwise) MMOT defines a generalized metric. We first explain the difficulty of proving this via two negative results. Afterwards, we prove key intermediate steps and then MMOT's metric properties. Finally, we show that the generalized triangle inequality of MMOT cannot be improved.",0
"This paper presents a novel mathematical framework that introduces a new metric on spaces, defined through a generalization of optimal transport theory based on multiple marginals. We show that our proposed multi-marginal optimal transport defines a generalized metric which extends classical metrics and embeds existing ones as special cases. Our approach bridges the gap between optimal transport and metric geometry by linking these two seemingly different fields via their common underlying structure. In addition, we discuss connections to various other areas of mathematics including: Monge-AmpÃ¨re equations, Ricci curvature, partial differential equations, and geometric functional analysis. Furthermore, we present several examples where our theoretical results lead to new insights into applications ranging from computer vision, image processing, machine learning and data science, to materials science. Finally, we highlight some potential future research directions stemming from our findings.",1
"Complex black-box machine learning models are regularly used in critical decision-making domains. This has given rise to several calls for algorithmic explainability. Many explanation algorithms proposed in literature assign importance to each feature individually. However, such explanations fail to capture the joint effects of sets of features. Indeed, few works so far formally analyze high-dimensional model explanations. In this paper, we propose a novel high dimension model explanation method that captures the joint effect of feature subsets.   We propose a new axiomatization for a generalization of the Banzhaf index; our method can also be thought of as an approximation of a black-box model by a higher-order polynomial. In other words, this work justifies the use of the generalized Banzhaf index as a model explanation by showing that it uniquely satisfies a set of natural desiderata and that it is the optimal local approximation of a black-box model.   Our empirical evaluation of our measure highlights how it manages to capture desirable behavior, whereas other measures that do not satisfy our axioms behave in an unpredictable manner.",0
"In recent years, there has been growing interest in developing models that can explain their predictions on high dimensional inputs such as images and natural language text. Traditional approaches rely heavily on human interpretability and visualizations which may not fully capture the complex relationships present in these large input spaces. As a result, we propose an axiomatic approach to model explanations based on formal mathematical properties that guarantee comprehensibility, consistency, and accuracy. Our framework enables the identification of key features in high dimensional data sets while providing novel insights into how those features contribute to overall output decisions. By providing rigorous guarantees on the quality of our explanations, we hope to bridge the gap between traditional interpretability methods and more advanced machine learning algorithms. Ultimately, our goal is to provide a systematic foundation for generating trustworthy explanations of high dimensional models.",1
"We present a novel method for reliably explaining the predictions of neural networks. We consider an explanation reliable if it identifies input features relevant to the model output by considering the input and the neighboring data points. Our method is built on top of the assumption of smooth landscape in a loss function of the model prediction: locally consistent loss and gradient profile. A theoretical analysis established in this study suggests that those locally smooth model explanations are learned using a batch of noisy copies of the input with the L1 regularization for a saliency map. Extensive experiments support the analysis results, revealing that the proposed saliency maps retrieve the original classes of adversarial examples crafted against both naturally and adversarially trained models, significantly outperforming previous methods. We further demonstrated that such good performance results from the learning capability of this method to identify input features that are truly relevant to the model output of the input and the neighboring data points, fulfilling the requirements of a reliable explanation.",0
This sounds like an interesting paper. How can I assist you further?,1
"The recent paper by Byrd & Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.",0
"Importance weighting has emerged as a promising technique for improving performance in deep learning models by adjusting the contribution of each training sample based on its relative importance. This approach can help address issues such as class imbalances, noise sensitivity, and other challenges that can hinder model accuracy. In this paper, we provide an overview of the key concepts behind importance weighting and examine how it has been applied in practice across different domains. We then present our own empirical evaluation comparing several different methods for computing importance weights using both synthetic data sets and real-world applications. Our results show that certain techniques outperform others in terms of boosting model performance, while some may even harm performance depending on the specific task and dataset at hand. Our findings offer valuable insights into when and why importance weighting works, as well as potential pitfalls and limitations to consider during implementation. By shedding light on the impact of varying approaches to importance weighting, we aim to inform future research efforts in this area and support more effective use of these methods within the broader machine learning community.",1
"The growing need for in-depth analysis of predictive models leads to a series of new methods for explaining their local and global properties. Which of these methods is the best? It turns out that this is an ill-posed question. One cannot sufficiently explain a black-box machine learning model using a single method that gives only one perspective. Isolated explanations are prone to misunderstanding, which inevitably leads to wrong or simplistic reasoning. This problem is known as the Rashomon effect and refers to diverse, even contradictory interpretations of the same phenomenon. Surprisingly, the majority of methods developed for explainable machine learning focus on a single aspect of the model behavior. In contrast, we showcase the problem of explainability as an interactive and sequential analysis of a model. This paper presents how different Explanatory Model Analysis (EMA) methods complement each other and why it is essential to juxtapose them together. The introduced process of Interactive EMA (IEMA) derives from the algorithmic side of explainable machine learning and aims to embrace ideas developed in cognitive sciences. We formalize the grammar of IEMA to describe potential human-model dialogues. IEMA is implemented in the human-centered framework that adopts interactivity, customizability and automation as its main traits. Combined, these methods enhance the responsible approach to predictive modeling.",0
"This paper presents a novel framework for the analysis of interactive explanatory models. By leveraging state-of-the-art natural language processing techniques, our approach allows researchers to systematically investigate the effectiveness of different modeling strategies across diverse domains. Our contributions include: i) introducing a flexible architecture that can account for varying levels of explanation complexity; ii) proposing evaluation metrics tailored to measure both model interpretability and accuracy; iii) developing effective algorithms to efficiently infer interpretable representations from raw data. We demonstrate the merits of our methodology on real-world datasets and showcase its potential impact on a range of applications, including medical diagnosis, financial forecasting, and recommendation systems. Overall, we believe our work represents a significant step towards democratizing model transparency and promoting trustworthy artificial intelligence.",1
"Science is used to discover the law of world. Machine learning can be used to discover the law of data. In recent years, there are more and more research about interpretability in machine learning community. We hope the machine learning methods are safe, interpretable, and they can help us to find meaningful pattern in data. In this paper, we focus on interpretability of deep representation. We propose a interpretable method of representation based on mutual information, which summarizes the interpretation of representation into three types of information between input data and representation. We further proposed MI-LR module, which can be inserted into the model to estimate the amount of information to explain the model's representation. Finally, we verify the method through the visualization of the prototype network.",0
"Title: Explaining Representation by Mutual Information -------------------------- Abstract: This paper introduces a new method for understanding how mutual information can be used as a measure of representation. We show that mutual information provides valuable insight into how different representations compare with each other across multiple domains. In addition, we demonstrate how our proposed method outperforms traditional approaches on both synthetic and real datasets. Our work has significant implications for researchers who aim to understand why certain representations perform well in specific tasks. With a focus on interpretability, we provide detailed explanations of how our algorithm works and discuss future directions for improving the approach further. Overall, our study represents a step forward in the quest towards more reliable and interpretable machine learning models.",1
"An increasing amount of studies have investigated the decision-making process of VQA models. Many of these studies focus on the reason behind the correct answer chosen by a model. Yet, the reason why the distracting answer chose by a model has rarely been studied. To this end, we introduce a novel task called \textit{textual Distractors Generation for VQA} (DG-VQA) that explaining the decision boundaries of existing VQA models. The goal of DG-VQA is to generate the most confusing set of textual distractors in multi-choice VQA tasks which expose the vulnerability of existing models (i.e. to generate distractors that lure existing models to fail). We show that DG-VQA can be formulated as a Markov Decision Process, and present a reinforcement learning solution to come up with distractors in an unsupervised manner. The solution addresses the lack of large annotated corpus issues in previous distractor generation methods. Our proposed model receives reward signals from fully-trained multi-choice VQA models and updates its parameters via policy gradient. The empirical results show that the generated textual distractors can successfully attack several popular VQA models with an average $20\%$ accuracy drop from $64\%$. Furthermore, we conduct adversarial training to improve the robustness of VQA models by incorporating the generated distractors. Empirical results validate the effectiveness of adversarial training by showing a performance improvement of $27\%$ for the multi-choice VQA task.",0
"This paper proposes a novel approach to generating textual distractors for multi-choice visual question answering (VQA) tasks using policy gradient optimization. Previous approaches have relied on rule-based methods or manual annotations, which can be time-consuming and limited by human biases. In contrast, our method uses a reinforcement learning algorithm to generate high-quality distractors that effectively challenge machine learning models and improve their performance. Experimental results demonstrate significant improvements over baseline models trained without distractors. Furthermore, we show that our generated distractors exhibit better quality than those obtained from manual annotation. Overall, this work represents a step towards more efficient and effective training data generation techniques for large-scale natural language understanding systems.",1
"The growing availability of data and computing power fuels the development of predictive models. In order to ensure the safe and effective functioning of such models, we need methods for exploration, debugging, and validation. New methods and tools for this purpose are being developed within the eXplainable Artificial Intelligence (XAI) subdomain of machine learning. In this work (1) we present the taxonomy of methods for model explanations, (2) we identify and compare 27 packages available in R to perform XAI analysis, (3) we present an example of an application of particular packages, (4) we acknowledge recent trends in XAI. The article is primarily devoted to the tools available in R, but since it is easy to integrate the Python code, we will also show examples for the most popular libraries from Python.",0
"Explain the main purpose of the paper: To provide an overview of available R packages for explainable artificial intelligence (XAI) and their characteristics, enabling practitioners and researchers to make informed decisions when selecting XAI methods for their work. Emphasize that while there are many XAI methods available for R programming language users, little guidance exists on which packages would suit different use cases. Outline how the authors conduct literature reviews, surveys, and crowdsource information from package maintainers and community members. Identify several challenges encountered during these processes. Summarize key findings - although numerous R packages support XAI tasks (e.g., feature relevance explanation), no single tool can fulfill all requirements; developers must consider factors like ease of implementation and scalability when choosing tools; open source nature fosters collaboration, but documentation quality varies widely among packages; some packages have limited adoption due to licensing issues and lackluster developer communication. Conclude by stressing the need for sustained efforts towards developing high-quality and comprehensive R packages for effective application of XAI approaches across domains.",1
"The task of detecting morphed face images has become highly relevant in recent years to ensure the security of automatic verification systems based on facial images, e.g. automated border control gates. Detection methods based on Deep Neural Networks (DNN) have been shown to be very suitable to this end. However, they do not provide transparency in the decision making and it is not clear how they distinguish between genuine and morphed face images. This is particularly relevant for systems intended to assist a human operator, who should be able to understand the reasoning. In this paper, we tackle this problem and present Focused Layer-wise Relevance Propagation (FLRP). This framework explains to a human inspector on a precise pixel level, which image regions are used by a Deep Neural Network to distinguish between a genuine and a morphed face image. Additionally, we propose another framework to objectively analyze the quality of our method and compare FLRP to other DNN interpretability methods. This evaluation framework is based on removing detected artifacts and analyzing the influence of these changes on the decision of the DNN. Especially, if the DNN is uncertain in its decision or even incorrect, FLRP performs much better in highlighting visible artifacts compared to other methods.",0
"In todayâ€™s world, where technology has become so advanced, detecting fraudulent images on social media platforms like Facebook, Twitter, Instagram, etc., is becoming increasingly challenging due to the high level of realism these morphed photos possess. To address this issue, our team developed FLARP (Focused LRP), which leverages explainable artificial intelligence (XAI) techniques such as Local Interpretable Model Agnostic Explanations (LIME) to generate interpretable explanations. These explanations can then be used by human analysts to determine whether or not a given image is authentic or manipulated. Our methodology was validated through extensive experimentation using publicly available datasets, demonstrating significant improvements over traditional methods in terms of accuracy and interpretability. Additionally, we conducted user studies to evaluate the effectiveness of our approach from a human perspective, further reinforcing its robustness. Our work contributes significantly towards building more trustworthy systems capable of identifying deepfake content that could potentially harm individuals or society at large. ---  That seems well done! Please give me another task to perform.",1
"Spiking Neural Networks (SNNs) compute and communicate with asynchronous binary temporal events that can lead to significant energy savings with neuromorphic hardware. Recent algorithmic efforts on training SNNs have shown competitive performance on a variety of classification tasks. However, a visualization tool for analysing and explaining the internal spike behavior of such temporal deep SNNs has not been explored. In this paper, we propose a new concept of bio-plausible visualization for SNNs, called Spike Activation Map (SAM). The proposed SAM circumvents the non-differentiable characteristic of spiking neurons by eliminating the need for calculating gradients to obtain visual explanations. Instead, SAM calculates a temporal visualization map by forward propagating input spikes over different time-steps. SAM yields an attention map corresponding to each time-step of input data by highlighting neurons with short inter-spike interval activity. Interestingly, without both the backpropagation process and the class label, SAM highlights the discriminative region of the image while capturing fine-grained details. With SAM, for the first time, we provide a comprehensive analysis on how internal spikes work in various SNN training configurations depending on optimization types, leak behavior, as well as when faced with adversarial examples.",0
"This paper presents a new method for visual explanations from spiking neural networks (SNN) using interspike intervals (ISIs). SNNs have shown promising results in many domains due to their biological plausibility, energy efficiency, and ability to handle temporal data. However, explaining how these networks make decisions remains a challenge as they operate differently than traditional artificial neural networks (ANNs). This work addresses this issue by proposing a novel approach that leverages ISIs to create visual representations of the decision-making process within SNNs. By analyzing the ISI patterns across different layers of the network, our method provides insights into how sensory inputs are transformed into high-level features and ultimately lead to predictions. We evaluate our technique on several datasets, demonstrating improved interpretability over existing methods while maintaining comparable accuracy. Our findings enable researchers and practitioners alike to gain a deeper understanding of SNN behavior and facilitate further development of effective SNN models.",1
"The wide adoption of Electronic Health Records (EHR) has resulted in large amounts of clinical data becoming available, which promises to support service delivery and advance clinical and informatics research. Deep learning techniques have demonstrated performance in predictive analytic tasks using EHRs yet they typically lack model result transparency or explainability functionalities and require cumbersome pre-processing tasks. Moreover, EHRs contain heterogeneous and multi-modal data points such as text, numbers and time series which further hinder visualisation and interpretability. This paper proposes a deep learning framework to: 1) encode patient pathways from EHRs into images, 2) highlight important events within pathway images, and 3) enable more complex predictions with additional intelligibility. The proposed method relies on a deep attention mechanism for visualisation of the predictions and allows predicting multiple sequential outcomes.",0
"Develop an algorithm that can automatically detect events from structured data stored as unstructured narrative text within electronic health records (EHR). This algorithm should then use natural language processing techniques to determine which events are important and relevant based on specific criteria, such as clinical significance or novelty. Finally, the algorithm should generate summaries of these selected events to provide insights into how they contribute to predictions made by machine learning models trained on the same patient population. We propose Deep EHR Spotlight, a framework and mechanism to address these tasks, using deep neural networks to achieve state-of-the-art performance in event detection and summarization. Our approach demonstrates significant improvement over previous methods in both accuracy and speed while providing explainability through targeted summarization of important events in patients' EHRs. By highlighting key moments in patient history, our method enables physicians and researchers to gain better insight into their patients' health status and decision making processes while improving the interpretability of predictive models.",1
"Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain the predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we generate a set of benchmark graph datasets specifically for GNN explainability. We summarize current datasets and metrics for evaluating GNN explainability. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.",0
"This abstract outlines some key issues related to explainability in graph neural networks (GNN), introduces our proposed taxonomy on current approaches, surveys the field based on this classification, compares existing solutions in terms of effectiveness, efficiency, interpretability, scalability, and versatility, discusses challenges associated with applying these methods, describes open research directions, and provides potential use cases. By performing a comprehensive literature study and developing an overview of GNN transparency techniques and their properties, we aim to provide insights into which approach may be most suitable for a given application scenario. Our main contributions consist of 1) classifying the state-of-the-art in GNN explanation into five categories: pre/post hoc methods, feature importance measures, attribution methods, graph anomaly detection, and causality analysis; 2) conducting experiments that allow comparisons among competing GNN interpretation methods; and 3) evaluating their strengths, weaknesses, opportunities, and threats. In conclusion, the results of our work can serve as a guide to users facing decisions regarding model selection or designers contemplating extending available GNN interpretations by incorporating new principles or paradigms. We hope that this taxonomic survey enables further advancements in explainable artificial intelligence, promoting trustworthy machine learning models, and contributing toward responsible innovation.""",1
"Prior works on formalizing explanations of a graph neural network (GNN) focus on a single use case - to preserve the prediction results through identifying important edges and nodes. In this paper, we develop a multi-purpose interpretation framework by acquiring a mask that indicates topology perturbations of the input graphs. We pack the framework into an interactive visualization system (GNNViz) which can fulfill multiple purposes: Preserve,Promote, or Attack GNN's predictions. We illustrate our approach's novelty and effectiveness with three case studies: First, GNNViz can assist non expert users to easily explore the relationship between graph topology and GNN's decision (Preserve), or to manipulate the prediction (Promote or Attack) for an image classification task on MS-COCO; Second, on the Pokec social network dataset, our framework can uncover unfairness and demographic biases; Lastly, it compares with state-of-the-art GNN explainer baseline on a synthetic dataset.",0
"This paper presents a new method for explaining predictions made by Graph Neural Networks (GNN), which are widely used in areas such as computer vision and natural language processing. Our approach, called ""topology perturbation,"" modifies the input graph by removing or adding edges while preserving the overall structure of the network, and then examines how these changes affect the model's output. We show that topology perturbation can effectively highlight important connections in the input graph that influence the prediction, making our explanation easily interpretable by humans. Our experimental results demonstrate the effectiveness of our technique compared to existing methods on several benchmark datasets. Overall, this work advances the field of explainability of machine learning models, providing researchers and practitioners with a powerful tool for understanding why GNNs make certain decisions.",1
"Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential reasons for the observed instability: catastrophic forgetting and small size of the fine-tuning datasets. In this paper, we show that both hypotheses fail to explain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT, fine-tuned on commonly used datasets from the GLUE benchmark, and show that the observed instability is caused by optimization difficulties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where fine-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than the previously proposed approaches. Code to reproduce our results is available online: https://github.com/uds-lsv/bert-stable-fine-tuning.",0
"Fine-tuning pre-trained transformers has become a popular approach in natural language processing, enabling state-of-the-art performance on many tasks. However, there have been concerns regarding the stability of fine-tuning these models, as well as conflicting reports on their effectiveness. This work seeks to address these issues by clarifying common misconceptions surrounding fine-tuning pre-trained transformers, particularly BERT. We provide explanations for why certain behaviors occur during training and how they affect model quality. Additionally, we present strong baseline results using novel techniques that can improve fine-tuning stability and achieve better generalization across datasets and architectures. By shedding light on these topics, we hope to aid researchers in understanding and applying fine-tuned transformer models more effectively.",1
"Gaze estimation methods learn eye gaze from facial features. However, among rich information in the facial image, real gaze-relevant features only correspond to subtle changes in eye region, while other gaze-irrelevant features like illumination, personal appearance and even facial expression may affect the learning in an unexpected way. This is a major reason why existing methods show significant performance degradation in cross-domain/dataset evaluation. In this paper, we tackle the domain generalization problem in cross-domain gaze estimation for unknown target domains. To be specific, we realize the domain generalization by gaze feature purification. We eliminate gaze-irrelevant factors such as illumination and identity to improve the cross-dataset performance without knowing the target dataset. We design a plug-and-play self-adversarial framework for the gaze feature purification. The framework enhances not only our baseline but also existing gaze estimation methods directly and significantly. Our method achieves the state-of-the-art performance in different benchmarks. Meanwhile, the purification is easily explainable via visualization.",0
"Title: ""Purified Gazes: Using Gaze Features to Improve Accuracy"" ---------------------------------------------------------------------------  Abstract: In this paper we present an innovative method for enhancing gaze estimation using pure gaze features. This technique significantly improves accuracy by separating out unwanted artifacts that can affect traditional gaze feature extraction. Our proposed method uses computer vision techniques to isolate clean gaze signals from complex visual scenes and noisy input images. Results show improved performance across multiple datasets compared to state-of-the-art methods, highlighting the effectiveness of our approach. With applications ranging from human behavior analysis to virtual reality systems, this work represents an important advancement in the field of gaze estimation.",1
"Understanding the inner workings of deep neural networks (DNNs) is essential to provide trustworthy artificial intelligence techniques for practical applications. Existing studies typically involve linking semantic concepts to units or layers of DNNs, but fail to explain the inference process. In this paper, we introduce neural architecture disentanglement (NAD) to fill the gap. Specifically, NAD learns to disentangle a pre-trained DNN into sub-architectures according to independent tasks, forming information flows that describe the inference processes. We investigate whether, where, and how the disentanglement occurs through experiments conducted with handcrafted and automatically-searched network architectures, on both object-based and scene-based datasets. Based on the experimental results, we present three new findings that provide fresh insights into the inner logic of DNNs. First, DNNs can be divided into sub-architectures for independent tasks. Second, deeper layers do not always correspond to higher semantics. Third, the connection type in a DNN affects how the information flows across layers, leading to different disentanglement behaviors. With NAD, we further explain why DNNs sometimes give wrong predictions. Experimental results show that misclassified images have a high probability of being assigned to task sub-architectures similar to the correct ones. Code will be available at: https://github.com/hujiecpp/NAD.",0
"In recent years, deep neural networks have proven to be highly effective in a wide range of applications, such as image classification, speech recognition, and natural language processing. However, these models can often suffer from poor interpretability and explainability, making it difficult to understand how they make their predictions and decisions. One approach to addressing this issue is through architecture disentanglement, which involves breaking down complex architectures into simpler and more interpretable components. This paper presents a new methodology for achieving architecture disentanglement in deep neural networks by leveraging techniques from functional programming languages and category theory. Through extensive experiments on several benchmark datasets, we demonstrate that our proposed approach leads to significant improvements in model transparency and understanding, without sacrificing performance. These findings have important implications for advancing both fundamental research in machine learning and real-world applications where explainability is critical, such as healthcare and finance.",1
"Generative modelling has been a topic at the forefront of machine learning research for a substantial amount of time. With the recent success in the field of machine learning, especially in deep learning, there has been an increased interest in explainable and interpretable machine learning. The ability to model distributions and provide insight in the density estimation and exact data likelihood is an example of such a feature. Normalizing Flows (NFs), a relatively new research field of generative modelling, has received substantial attention since it is able to do exactly this at a relatively low cost whilst enabling competitive generative results. While the generative abilities of NFs are typically explored, we focus on exploring the data distribution modelling for Out-of-Distribution (OOD) detection. Using one of the state-of-the-art NF models, GLOW, we attempt to detect OOD examples in the ISIC dataset. We notice that this model under performs in conform related research. To improve the OOD detection, we explore the masking methods to inhibit co-adaptation of the coupling layers however find no substantial improvement. Furthermore, we utilize Wavelet Flow which uses wavelets that can filter particular frequency components, thus simplifying the modeling process to data-driven conditional wavelet coefficients instead of complete images. This enables us to efficiently model larger resolution images in the hopes that it would capture more relevant features for OOD. The paper that introduced Wavelet Flow mainly focuses on its ability of sampling high resolution images and did not treat OOD detection. We present the results and propose several ideas for improvement such as controlling frequency components, using different wavelets and using other state-of-the-art NF architectures.",0
"In this paper, we present a novel approach for out-of-distribution (OOD) detection of melanoma using normalizing flows. OOD detection is a crucial task in medical image analysis as it helps identify images that differ significantly from the training data distribution. This can lead to better decision making by physicians during diagnosis and treatment planning. Normalizing flows have recently gained popularity due to their ability to model complex distributions, which makes them well suited for OOD detection tasks. Our proposed method leverages state-of-the-art normalizing flow models along with carefully designed input features to achieve accurate and efficient OOD detection. We evaluate our approach on two publicly available datasets: MelaNYC and DermaScope, demonstrating superior performance compared to other commonly used methods. Our results show high sensitivity, specificity, precision and accuracy across different benchmark metrics, indicating that our method is capable of identifying OOD samples effectively while maintaining low false positive rates. Overall, this work represents a significant contribution towards advancing the field of computational dermatology and improving healthcare outcomes through machine learning techniques.",1
"Recent work on explaining Deep Neural Networks (DNNs) focuses on attributing the model's output scores to input features. However, when it comes to classification problems, a more fundamental question is how much does each feature contributes to the model's decision to classify an input instance into a specific class. Our first contribution is Boundary Attribution, a new explanation method to address this question. BA leverages an understanding of the geometry of activation regions. Specifically, they involve computing (and aggregating) normal vectors of the local decision boundaries for the target input. Our second contribution is a set of analytical results connecting the adversarial robustness of the network and the quality of gradient-based explanations. Specifically, we prove two theorems for ReLU networks: BA of randomized smoothed networks or robustly trained networks is much closer to non-boundary attribution methods than that in standard networks. These analytics encourage users to improve model robustness for high-quality explanations. Finally, we evaluate the proposed methods on ImageNet and show BAs produce more concentrated and sharper visualizations compared with non-boundary ones. We further demonstrate that our method also helps to reduce the sensitivity of attributions to the baseline input if one is required.",0
"While human cognition can seem very complex and difficult to describe, recent research suggests that many aspects of our thinking may be simpler than we realize. This paper examines one common type of thought process: making attributions about where something starts and stops. These boundary attributions turn out to provide natural explanations for why things exist at all, by explaining how their features fit together and make sense. By understanding these processes better, we gain insight into how humans think more generally - including both everyday tasks like planning a party and professions such as philosophy or science. Ultimately, normal explanations may lead us closer to artificial intelligence systems that truly comprehend the world as well as humans can.",1
"Research has shown that neurons within the brain are selective to certain stimuli. For example, the fusiform face area (FFA) region is known by neuroscientists to selectively activate when people see faces over non-face objects. However, the mechanisms by which the primary visual system directs information to the correct higher levels of the brain are currently unknown. In our work, we mimic several high-level neural mechanisms of perception by creating a novel computational model that incorporates lateral and top down feedback in the form of hierarchical competition. Not only do we show that these elements can help explain the information flow and selectivity of high level areas within the brain, we also demonstrate that these neural mechanisms provide the foundation of a novel classification framework that rivals traditional supervised learning in computer vision. Additionally, we present both quantitative and qualitative results that demonstrate that our generative framework is consistent with neurological themes and enables simple, yet robust category level classification.",0
"This paper presents a study examining how our mind processes visual stimuli in different ways depending on contextual factors such as attention, memory, motivation, and expectation. By utilizing experimental methods that measure neural activity, eye movements, and cognitive performance, we sought to investigate the role of selective processing in shaping perceptual experience. Our findings reveal that while some features of the environment are prioritized over others due to their relevance to current goals, objectives can shift the balance of attentional focus towards other attributes of perceived objects, ultimately leading to enhanced awareness of previously unnoticed details. These results suggest that perception is not solely determined by sensory input but rather depends on active interplay between internal representations and external cues, highlighting the adaptability of human vision in real-world situations. Overall, these insights advance our understanding of the complex dynamics involved in the creation of mental images and have implications for various fields ranging from education to marketing.",1
"We study an interesting and challenging problem, learning any part of a Bayesian network (BN) structure. In this challenge, it will be computationally inefficient using existing global BN structure learning algorithms to find an entire BN structure to achieve the part of a BN structure in which we are interested. And local BN structure learning algorithms encounter the false edge orientation problem when they are directly used to tackle this challenging problem. In this paper, we first present a new concept of Expand-Backtracking to explain why local BN structure learning methods have the false edge orientation problem, then propose APSL, an efficient and accurate Any Part of BN Structure Learning algorithm. Specifically, APSL divides the V-structures in a Markov blanket (MB) into two types: collider V-structure and non-collider V-structure, then it starts from a node of interest and recursively finds both collider V-structures and non-collider V-structures in the found MBs, until the part of a BN structure in which we are interested are oriented. To improve the efficiency of APSL, we further design the APSL-FS algorithm using Feature Selection, APSL-FS. Using six benchmark BNs, the extensive experiments have validated the efficiency and accuracy of our methods.",0
"This is not specified in your prompt but I can try writing an abstract for a research paper on Bayesian network structure learning without including the paper title. Here you go:  Bayesian networks have been widely used as probabilistic graphical models for representing complex relationships between variables, making them valuable tools in fields such as artificial intelligence, engineering, finance, and healthcare. However, determining their structures remains one of the key challenges in applying these models. In this paper, we explore several approaches for structure learning, focusing particularly on recent advances that use efficient algorithms and feature selection methods. We provide a detailed review of different methodologies, discussing their advantages and disadvantages, before presenting case studies demonstrating how these techniques can be applied effectively across various domains. By highlighting promising directions and open problems in the field, our aim is to encourage further research into developing new ways of automatically discovering accurate representations of real-world systems using Bayesian networks.",1
"We develop a novel approach to explain why AdaBoost is a successful classifier. By introducing a measure of the influence of the noise points (ION) in the training data for the binary classification problem, we prove that there is a strong connection between the ION and the test error. We further identify that the ION of AdaBoost decreases as the iteration number or the complexity of the base learners increases. We confirm that it is impossible to obtain a consistent classifier without deep trees as the base learners of AdaBoost in some complicated situations. We apply AdaBoost in portfolio management via empirical studies in the Chinese market, which corroborates our theoretical propositions.",0
"This paper investigates the success of AdaBoost, a well known machine learning algorithm, applied in the context of portfolio management. First introduced by Yoav Freund and Robert Schapire in 1997, this method has since become increasingly popular due to its high effectiveness and accuracy across a wide range of applications. To fully examine this subject matter, we first provide a background on boosting methods before discussing AdaBoost specifically in detail. Afterwards, we present experiments carried out utilizing several datasets consisting of real financial data taken from the Fama French library. Our results confirm that AdaBoost achieves competitive performance against other state-of-the-art machine learning algorithms when used as part of a portfolio optimization framework. Ultimately, these findings suggest that AdaBoost can serve as a valuable tool for investment professionals seeking to build more robust portfolios.",1
"Neural networks represent data as projections on trained weights in a high dimensional manifold. The trained weights act as a knowledge base consisting of causal class dependencies. Inference built on features that identify these dependencies is termed as feed-forward inference. Such inference mechanisms are justified based on classical cause-to-effect inductive reasoning models. Inductive reasoning based feed-forward inference is widely used due to its mathematical simplicity and operational ease. Nevertheless, feed-forward models do not generalize well to untrained situations. To alleviate this generalization challenge, we propose using an effect-to-cause inference model that reasons abductively. Here, the features represent the change from existing weight dependencies given a certain effect. We term this change as contrast and the ensuing reasoning mechanism as contrastive reasoning. In this paper, we formalize the structure of contrastive reasoning and propose a methodology to extract a neural network's notion of contrast. We demonstrate the value of contrastive reasoning in two stages of a neural network's reasoning pipeline : in inferring and visually explaining decisions for the application of object recognition. We illustrate the value of contrastively recognizing images under distortions by reporting an improvement of 3.47%, 2.56%, and 5.48% in average accuracy under the proposed contrastive framework on CIFAR-10C, noisy STL-10, and VisDA datasets respectively.",0
"Abstract: We explore how contrastive reasoning can be applied to neural networks, particularly generative models like GANs (Generative Adversarial Networks) and Variational Autoencoders (VAEs). Our approach uses contrastive loss functions that encourage more precise learning by discouraging incorrect predictions, allowing the model to learn more accurate representations. We evaluate our method on a range of tasks and datasets and show that it leads to significant improvements over traditional methods. Additionally, we provide theoretical analysis that helps explain why contrastive reasoning works well in practice. Overall, we believe that our work represents an important step forward in understanding how to improve performance in neural networks using insights from human cognitive psychology. ---",1
"Transfer learning with pre-trained neural networks is a common strategy for training classifiers in medical image analysis. Without proper channel selections, this often results in unnecessarily large models that hinder deployment and explainability. In this paper, we propose a novel approach to efficiently build small and well performing networks by introducing the channel-scaling layers. A channel-scaling layer is attached to each frozen convolutional layer, with the trainable scaling weights inferring the importance of the corresponding feature channels. Unlike the fine-tuning approaches, we maintain the weights of the original channels and large datasets are not required. By imposing L1 regularization and thresholding on the scaling weights, this framework iteratively removes unnecessary feature channels from a pre-trained model. Using an ImageNet pre-trained VGG16 model, we demonstrate the capabilities of the proposed framework on classifying opacity from chest X-ray images. The results show that we can reduce the number of parameters by 95% while delivering a superior performance.",0
"In today's world where data is abundant but expertise is scarce, transfer learning has become an essential tool for training machine learning models on new tasks using pre-trained models that have already been trained on similar problems elsewhere. However, one fundamental challenge in transfer learning remains unaddressed â€“ how can we know which among many available pre-trained models would result in better generalization performance? We propose Channel Scaling as a method to find the appropriate channel scale, which brings into play both high and low feature channels depending on the complexity required for task at hand. This approach ensures improved accuracy compared to existing scaling techniques by effectively trading off between simplicity and expressivity while maintaining model interpretability. Our experiments show that our proposed methods provide significant improvement over several state-of-the-art transfer learning models across multiple domains and benchmark datasets. Ultimately, our work bridges the gap between human intuition of domain knowledge experts and complex deep neural networks to achieve efficient and effective problem solving.",1
"The focus of this work is on Statistical Process Control (SPC) of a manufacturing process based on available measurements. Two important applications of SPC in industrial settings are fault detection and diagnosis (FDD). In this work a deep learning (DL) based methodology is proposed for FDD. We investigate the application of an explainability concept to enhance the FDD accuracy of a deep neural network model trained with a data set of relatively small number of samples. The explainability is quantified by a novel relevance measure of input variables that is calculated from a Layerwise Relevance Propagation (LRP) algorithm. It is shown that the relevances can be used to discard redundant input feature vectors/ variables iteratively thus resulting in reduced over-fitting of noisy data, increasing distinguishability between output classes and superior FDD test accuracy. The efficacy of the proposed method is demonstrated on the benchmark Tennessee Eastman Process.",0
"This research paper proposes a new approach to fault detection and diagnosis in chemical processes using deep learning algorithms. In traditional deep learning methods, explaining how decisions are made can often be difficult due to their complex nature. However, this proposed method uses relevance based dynamic neural networks which allow for easy interpretability, making it possible for engineers and technicians to quickly identify problems and take appropriate actions. By utilizing state-of-the-art techniques such as attention mechanisms and latent embeddings, these models achieve unparalleled accuracy and generalization ability. The results from real-world experiments demonstrate the effectiveness of our approach over current methods used in industry. Overall, this research makes significant contributions towards improving the robustness and reliability of chemical plants by providing accurate fault detection and diagnostic capabilities to operators and decision makers.",1
"Explaining the predictions made by complex machine learning models helps users to understand and accept the predicted outputs with confidence. One promising way is to use similarity-based explanation that provides similar instances as evidence to support model predictions. Several relevance metrics are used for this purpose. In this study, we investigated relevance metrics that can provide reasonable explanations to users. Specifically, we adopted three tests to evaluate whether the relevance metrics satisfy the minimal requirements for similarity-based explanation. Our experiments revealed that the cosine similarity of the gradients of the loss performs best, which would be a recommended choice in practice. In addition, we showed that some metrics perform poorly in our tests and analyzed the reasons of their failure. We expect our insights to help practitioners in selecting appropriate relevance metrics and also aid further researches for designing better relevance metrics for explanations.",0
"This study evaluates similarity-based explanations by examining their effectiveness at capturing underlying relationships among data points. We provide empirical evidence that similarity-based explanations can accurately capture important patterns in complex datasets, while avoiding overfitting issues commonly associated with traditional machine learning models. Our results demonstrate the potential utility of these methods across a range of domains, including image analysis, natural language processing, and network science. Further research is necessary to fully explore the capabilities and limitations of similarity-based explanations, but our work represents an important step toward understanding how these approaches can contribute to advancing the state of art.",1
"Machine learning (ML) is successful in achieving human-level performance in various fields. However, it lacks the ability to explain an outcome due to its black-box nature. While existing explainable ML is promising, almost all of these methods focus on formatting interpretability as an optimization problem. Such a mapping leads to numerous iterations of time-consuming complex computations, which limits their applicability in real-time applications. In this paper, we propose a novel framework for accelerating explainable ML using Tensor Processing Units (TPUs). The proposed framework exploits the synergy between matrix convolution and Fourier transform, and takes full advantage of TPU's natural ability in accelerating matrix computations. Specifically, this paper makes three important contributions. (1) To the best of our knowledge, our proposed work is the first attempt in enabling hardware acceleration of explainable ML using TPUs. (2) Our proposed approach is applicable across a wide variety of ML algorithms, and effective utilization of TPU-based acceleration can lead to real-time outcome interpretation. (3) Extensive experimental results demonstrate that our proposed approach can provide an order-of-magnitude speedup in both classification time (25x on average) and interpretation time (13x on average) compared to state-of-the-art techniques.",0
"This would typically go before any introductory material, such as the introduction section or previous research section, and should accurately summarize your entire paper without including references. Also consider adding a hook at the beginning that draws readers into reading more about your research, but make sure the body of the abstract still has all necessary info! Good luck with your paper! I am sure you will produce something wonderful!",1
"A vastly growing literature on explaining deep learning models has emerged. This paper contributes to that literature by introducing a global gradient-based model-agnostic method, which we call Marginal Attribution by Conditioning on Quantiles (MACQ). Our approach is based on analyzing the marginal attribution of predictions (outputs) to individual features (inputs). Specificalllly, we consider variable importance by mixing (global) output levels and, thus, explain how features marginally contribute across different regions of the prediction space. Hence, MACQ can be seen as a marginal attribution counterpart to approaches such as accumulated local effects (ALE), which study the sensitivities of outputs by perturbing inputs. Furthermore, MACQ allows us to separate marginal attribution of individual features from interaction effect, and visually illustrate the 3-way relationship between marginal attribution, output level, and feature value.",0
"This is an important and groundbreaking study that seeks to improve our understanding of how deep learning models make predictions by developing a novel technique for interpreting them called marginal attribution with conditional quantile regression. By conditioning on different percentiles of input features, we can gain insight into which aspects of the data most strongly influence model output at each step of the training process. Results from experiments using state-of-the-art models on benchmark datasets showcase the effectiveness of our approach, highlighting the importance of considering uncertainty in model interpretability research. Overall, our work represents a significant contribution towards making machine learning more transparent and trustworthy.",1
"Normalization operations are essential for state-of-the-art neural networks and enable us to train a network from scratch with a large learning rate (LR). We attempt to explain the real effect of Batch Normalization (BN) from the perspective of variance transmission by investigating the relationship between BN and Weights Normalization (WN). In this work, we demonstrate that the problem of the shift of the average gradient will amplify the variance of every convolutional (conv) layer. We propose Parametric Weights Standardization (PWS), a fast and robust to mini-batch size module used for conv filters, to solve the shift of the average gradient. PWS can provide the speed-up of BN. Besides, it has less computation and does not change the output of a conv layer. PWS enables the network to converge fast without normalizing the outputs. This result enhances the persuasiveness of the shift of the average gradient and explains why BN works from the perspective of variance transmission. The code and appendix will be made available on https://github.com/lyxzzz/PWSConv.",0
"This paper examines the impact of variance transmission on normalization within neural networks. Using several different datasets, we investigate how changes to the average gradient can affect network performance. Our results show that as the average gradient decreases, so too does the overall performance of the network. We attribute this decline in performance to an imbalance between the positive and negative gradients caused by normalization. Finally, we conclude that careful control over variance transmission is necessary for maintaining strong performance in deep learning models.",1
"The Neural Tangent Kernel (NTK) has recently attracted intense study, as it describes the evolution of an over-parameterized Neural Network (NN) trained by gradient descent. However, it is now well-known that gradient descent is not always a good optimizer for NNs, which can partially explain the unsatisfactory practical performance of the NTK regression estimator. In this paper, we introduce the Weighted Neural Tangent Kernel (WNTK), a generalized and improved tool, which can capture an over-parameterized NN's training dynamics under different optimizers. Theoretically, in the infinite-width limit, we prove: i) the stability of the WNTK at initialization and during training, and ii) the equivalence between the WNTK regression estimator and the corresponding NN estimator with different learning rates on different parameters. With the proposed weight update algorithm, both empirical and analytical WNTKs outperform the corresponding NTKs in numerical experiments.",0
"This work presents Weighted Neural Tangent Kernels (WNTK), which provide several advantages over traditional kernel methods like Gaussian Processes or neural networks. WNTKs can handle arbitrary nonlinear functions as well as linear regression tasks, including both stationary and nonstationary problems. They also allow fast training time on large datasets due to their computational efficiency, making them appropriate for use in real-world applications such as computer vision, speech recognition, control systems, and many others. By using a single, unified framework that adapts automatically to different models and data, we demonstrate that WNTKs have more flexibility than previously available kernels while maintaining desirable theoretical properties. Compared to existing kernels, our method leads to improved accuracy across several benchmark tests without requiring manual fine-tuning. Overall, these results suggest that Weighted Neural Tangent Kernels hold great potential for advancing the field of machine learning.",1
"Rapid development of artificial intelligence (AI) systems amplify many concerns in society. These AI algorithms inherit different biases from humans due to mysterious operational flow and because of that it is becoming adverse in usage. As a result, researchers have started to address the issue by investigating deeper in the direction towards Responsible and Explainable AI. Among variety of applications of AI, facial expression recognition might not be the most important one, yet is considered as a valuable part of human-AI interaction. Evolution of facial expression recognition from the feature based methods to deep learning drastically improve quality of such algorithms. This research work aims to study a gender bias in deep learning methods for facial expression recognition by investigating six distinct neural networks, training them, and further analysed on the presence of bias, according to the three definition of fairness. The main outcomes show which models are gender biased, which are not and how gender of subject affects its emotion recognition. More biased neural networks show bigger accuracy gap in emotion recognition between male and female test sets. Furthermore, this trend keeps for true positive and false positive rates. In addition, due to the nature of the research, we can observe which types of emotions are better classified for men and which for women. Since the topic of biases in facial expression recognition is not well studied, a spectrum of continuation of this research is truly extensive, and may comprise detail analysis of state-of-the-art methods, as well as targeting other biases.",0
"Abstract: Gender bias has been shown to exist across many areas of computer science research, including emotion recognition. This can have harmful consequences for users who rely on these systems. In this study, we aimed to evaluate gender bias in emotion recognition models using benchmark datasets and data collected from human annotators to rate the accuracy of each system. We found that most systems were biased towards male emotions over female ones, particularly those associated with negative emotions such as anger or sadness. However, some systems showed little evidence of bias, indicating that accurate and unbiased systems can be developed with careful attention to dataset composition and model design. Our findings highlight the importance of considering issues related to gender bias in emotion recognition research and development. By doing so, we can create more inclusive and fair technologies that better serve all users regardless of their gender identity.  Title: Assessing gender bias in emotion recognition models.",1
"Explainability algorithms such as LIME have enabled machine learning systems to adopt transparency and fairness, which are important qualities in commercial use cases. However, recent work has shown that LIME's naive sampling strategy can be exploited by an adversary to conceal biased, harmful behavior. We propose to make LIME more robust by training a generative adversarial network to sample more realistic synthetic data which the explainer uses to generate explanations. Our experiments demonstrate that our proposed method demonstrates an increase in accuracy across three real-world datasets in detecting biased, adversarial behavior compared to vanilla LIME. This is achieved while maintaining comparable explanation quality, with up to 99.94\% in top-1 accuracy in some cases.",0
"Adversarial attacks have recently gained significant attention due to their ability to fool state-of-the-art machine learning models by adding imperceptible perturbations to input images. Local Interpretable Model Explanation (LIME) was proposed as a method to explain black box decisions through linear approximations on randomized local subsets of training data. However, existing techniques used for sampling these subsets can lead to poor decision boundaries which negatively impacts explanation robustness. In this work, we present an approach that uses clustering to improve the distribution of local subsets during adversarial testing. Our results show improved performance compared to traditional random sampling methods both visually through saliency maps and quantitatively via attack success rates. We provide analysis of how our sampling technique affects the robustness of explanations generated from LIME and discuss potential use cases in real world applications such as medical imaging and autonomous driving where robustness to adversarial examples is crucial. Overall, our contributions demonstrate that improving the quality of locality sampling can greatly enhance the reliability and trustworthiness of model interpretability techniques like LIME.",1
"Purpose Supervised deep learning in radiology suffers from notorious inherent limitations: 1) It requires large, hand-annotated data sets, 2) It is non-generalizable, and 3) It lacks explainability and intuition. We have recently proposed Reinforcement Learning to address all threes. However, we applied it to images with radiologist eye tracking points, which limits the state-action space. Here we generalize the Deep-Q Learning to a gridworld-based environment, so that only the images and image masks are required.   Materials and Methods We trained a Deep Q network on 30 two-dimensional image slices from the BraTS brain tumor database. Each image contained one lesion. We then tested the trained Deep Q network on a separate set of 30 testing set images. For comparison, we also trained and tested a keypoint detection supervised deep learning network for the same set of training / testing images.   Results Whereas the supervised approach quickly overfit the training data, and predicably performed poorly on the testing set (11\% accuracy), the Deep-Q learning approach showed progressive improved generalizability to the testing set over training time, reaching 70\% accuracy.   Conclusion We have shown a proof-of-principle application of reinforcement learning to radiological images, here using 2D contrast-enhanced MRI brain images with the goal of localizing brain tumors. This represents a generalization of recent work to a gridworld setting, naturally suitable for analyzing medical images.",0
"Imaging studies have become increasingly important in medicine over recent years and accurate interpretation has led to earlier diagnosis and improved patient outcomes. Magnetic resonance imaging (MRI) of the central nervous system is used to detect many diseases including neurological cancers such as gliomas but their detection remains challenging due to subtle differences in tissue appearance that distinguish malignant from normal tissues. In this study we address this challenge by demonstrating how deep reinforcement learning can accurately segment these lesions using very limited labeled data. We developed two algorithms based on convolutional neural networks: a deep Q network trained by Q learning, DQN; and one fine tuned through policy iteration, FTPI. Both methods were benchmarked against human expert annotation and achieved high levels of accuracy even when only five percent of the dataset was manually annotated. This work represents a significant advancement towards making automated systems for medical image analysis more reliable and widely accessible.",1
"We report, for the first time, on the cascade weight shedding phenomenon in deep neural networks where in response to pruning a small percentage of a network's weights, a large percentage of the remaining is shed over a few epochs during the ensuing fine-tuning phase. We show that cascade weight shedding, when present, can significantly improve the performance of an otherwise sub-optimal scheme such as random pruning. This explains why some pruning methods may perform well under certain circumstances, but poorly under others, e.g., ResNet50 vs. MobileNetV3. We provide insight into why the global magnitude-based pruning, i.e., GMP, despite its simplicity, provides a competitive performance for a wide range of scenarios. We also demonstrate cascade weight shedding's potential for improving GMP's accuracy, and reduce its computational complexity. In doing so, we highlight the importance of pruning and learning-rate schedules. We shed light on weight and learning-rate rewinding methods of re-training, showing their possible connections to the cascade weight shedding and reason for their advantage over fine-tuning. We also investigate cascade weight shedding's effect on the set of kept weights, and its implications for semi-structured pruning. Finally, we give directions for future research.",0
"Abstract: In recent years, deep neural networks (DNNs) have become increasingly popular due to their ability to achieve state-of-the-art performance on a wide range of tasks. However, training these models can require large amounts of data and computational resources, making them impractical for use in many applications. One technique that has been proposed to address this issue is network pruning, which involves removing unnecessary connections in a DNN to reduce its size while maintaining its accuracy. In this work, we focus specifically on cascade weight shedding (CWS), one such method that has achieved promising results in several domains. By gradually reducing the number of weights in a model during the training process, CWS enables efficient fine-tuning without sacrificing accuracy. Despite these benefits, there remain some pitfalls associated with using CWS, including increased sensitivity to initialization parameters, potential instability in the early stages of training, and difficulty in recovering from overpruning errors. Our findings provide valuable insights into both the advantages and challenges of applying CWS to DNNs, enabling researchers to make informed decisions about whether this approach is appropriate for their specific task. Ultimately, our study contributes to the broader understanding of how to effectively balance model efficiency and accuracy in deep learning systems.  Keywords: deep neural networks, network pruning, cascade weight shedding, fine-tuning, model optimization ---  Note: This text may need editing or rewriting to ensure academic writing standards are met. Please consult your institution's guidelines before submitting any papers. ---",1
"When neural networks are employed for high-stakes decision making, it is desirable for the neural networks to provide explanation for their prediction in order for us to understand the features that have contributed to the decision. At the same time, it is important to flag potential outliers for in-depth verification by domain experts. In this work we propose to unify two differing aspects of explainability with outlier detection. We argue for a broader adoption of prototype-based student networks capable of providing an example-based explanation for its prediction and at the same time identify regions of similarity between the predicted sample and the examples. The examples are real prototypical cases sampled from the training set via our novel iterative prototype replacement algorithm. Furthermore, we propose to use the prototype similarity scores for identifying outliers. We compare performances in terms of classification, explanation quality, and outlier detection of our proposed network with other baselines. We show that our prototype-based networks beyond similarity kernels deliver meaningful explanation and promising outlier detection results without compromising classification accuracy.",0
"This paper presents a new methodology for scalable and unified example-based explanation and outlier detection. We propose a novel algorithm that combines the advantages of active learning and transfer learning techniques to enable efficient learning from multiple data sources. Our approach utilizes a shared feature space across examples of different modalities and tasks, allowing us to train more accurate models while reducing computational complexity and storage requirements. The key insight behind our technique lies in exploiting relationships among examples and using them as constraints during training to improve generalization performance on both known and unknown data distributions. We evaluate our method extensively against several benchmark datasets and compare its results to state-of-the-art methods. Results demonstrate significant improvements in accuracy and efficiency while maintaining interpretability and explainability through visual explanations. Overall, we believe that this work represents a major step towards developing effective and scalable machine learning algorithms capable of coping with real-world challenges and large amounts of complex data.",1
"We make two theoretical contributions to disentanglement learning by (a) defining precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept ""disentangled representations"" used in supervised and unsupervised methods along three dimensions-informativeness, separability and interpretability - which can be expressed and quantified explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known disentanglement learning models. We then propose robust metrics for measuring informativeness, separability and interpretability. Through a comprehensive suite of experiments, we show that our metrics correctly characterize the representations learned by different methods and are consistent with qualitative (visual) results. Thus, the metrics allow disentanglement learning methods to be compared on a fair ground. We also empirically uncovered new interesting properties of VAE-based methods and interpreted them with our formulation. These findings are promising and hopefully will encourage the design of more theoretically driven models for learning disentangled representations.",0
"Abstract: This study presents a theory and evaluation metrics for learning disentangled representations, which have gained increasing attention due to their ability to model underlying factors of variation in data and improve generalization performance on downstream tasks. We develop a theoretical framework that formalizes the concept of disentanglement by connecting it to principles from statistical physics, machine learning, and psychology. Our framework shows how different approaches can lead to distinct types of disentanglement depending on the chosen objective function, regularizer, and optimization algorithm. Based on our theoretical analysis, we propose novel evaluation metrics tailored to assess different aspects of disentanglement quality, such as correlation estimation, causal inference, and latent variable interpretation. Experimental results demonstrate the effectiveness of these evaluation metrics and illustrate how they can guide the development and comparison of more powerful disentangled representation models.",1
"As neural networks become the tool of choice to solve an increasing variety of problems in our society, adversarial attacks become critical. The possibility of generating data instances deliberately designed to fool a network's analysis can have disastrous consequences. Recent work has shown that commonly used methods for model training often result in fragile abstract representations that are particularly vulnerable to such attacks. This paper presents a visual framework to investigate neural network models subjected to adversarial examples, revealing how models' perception of the adversarial data differs from regular data instances and their relationships with class perception. Through different use cases, we show how observing these elements can quickly pinpoint exploited areas in a model, allowing further study of vulnerable features in input data and serving as a guide to improving model training and architecture.",0
"In recent years, deep neural networks (DNNs) have proven themselves capable of performing complex tasks such as image classification, speech recognition, and even language translation at near human levels. One reason DNNs can achieve high accuracy is their ability to automatically learn robust features that generalize well across many examples. However, these models are still vulnerable to adversarial attacks, which exploit weaknesses in their training regimes and architectures to generate inputs that cause them to make incorrect predictions. This paper proposes a novel approach called explainable adversarial attacks (EXA), where we investigate how specific neurons contribute to DNN decision making during both normal operation and under attack. We introduce activation profile analysis, a visualization technique that highlights input regions responsible for altering model behavior by manipulating individual neuron activations. By applying our method on several popular architecture variations, including ResNet and VGG, EXA helps elucidate important factors that influence the effectiveness of different attack types. Our results demonstrate the power of using interpretability tools like EXA to provide new insights into improving model robustness against adversaries.",1
"Explainability for machine learning models has gained considerable attention within our research community given the importance of deploying more reliable machine-learning systems. In computer vision applications, generative counterfactual methods indicate how to perturb a model's input to change its prediction, providing details about the model's decision-making. Current counterfactual methods make ambiguous interpretations as they combine multiple biases of the model and the data in a single counterfactual interpretation of the model's decision. Moreover, these methods tend to generate trivial counterfactuals about the model's decision, as they often suggest to exaggerate or remove the presence of the attribute being classified. For the machine learning practitioner, these types of counterfactuals offer little value, since they provide no new information about undesired model or data biases. In this work, we propose a counterfactual method that learns a perturbation in a disentangled latent space that is constrained using a diversity-enforcing loss to uncover multiple valuable explanations about the model's prediction. Further, we introduce a mechanism to prevent the model from producing trivial explanations. Experiments on CelebA and Synbols demonstrate that our model improves the success rate of producing high-quality valuable explanations when compared to previous state-of-the-art methods. We will publish the code.",0
"This paper proposes a method for generating counterfactual explanations that go beyond trivial examples and provide more diverse, valuable insights into machine learning models. Traditional approaches often generate simple, deterministic counterfactuals that only slightly differ from the original input and may not provide meaningful understanding of how the model works. Our approach utilizes a novel algorithm that generates a distribution of counterfactuals by randomly sampling different perturbations within defined ranges, allowing for exploration of a broader range of possible outcomes. By incorporating user feedback, we can further refine our algorithm and better align the generated counterfactuals with human intuition regarding which features matter most to the decision boundary. Evaluation results on multiple datasets demonstrate significant improvements over baseline methods and high levels of satisfaction among users who participated in user studies. Overall, our proposed method provides a powerful tool for enhancing interpretability and transparency in artificial intelligence systems.",1
"Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.",0
"Here's my attempt at an informative yet concise abstract: --- Title: New Method For Understanding Uncertainty In Machine Learning Models  In many applications of machine learning models, uncertainty estimation plays a crucial role. However, explaining the estimated uncertainties can often prove challenging. This work presents a novel methodology called ""CLUE"" that allows for the effective explanation and interpretation of uncertainty estimates in complex data analysis workflows. We demonstrate the utility of our framework on several real world examples across different domains ranging from image recognition to natural language processing. Our results show that our approach provides insights into how well the model generalizes beyond the training set, improving decision making in downstream tasks such as automation, recommendation systems, medical diagnosis etc.. Finally, we provide open source code and detailed experimental evaluation to enable reproducibility of our research findings by other experts and practitioners in the field",1
"Deep one-class classification variants for anomaly detection learn a mapping that concentrates nominal samples in feature space causing anomalies to be mapped away. Because this transformation is highly non-linear, finding interpretations poses a significant challenge. In this paper we present an explainable deep one-class classification method, Fully Convolutional Data Description (FCDD), where the mapped samples are themselves also an explanation heatmap. FCDD yields competitive detection performance and provides reasonable explanations on common anomaly detection benchmarks with CIFAR-10 and ImageNet. On MVTec-AD, a recent manufacturing dataset offering ground-truth anomaly maps, FCDD sets a new state of the art in the unsupervised setting. Our method can incorporate ground-truth anomaly maps during training and using even a few of these (~5) improves performance significantly. Finally, using FCDD's explanations we demonstrate the vulnerability of deep one-class classification models to spurious image features such as image watermarks.",0
"This paper presents a novel approach to deep one-class classification that emphasizes interpretability and explainability. Traditional machine learning algorithms have been criticized for their lack of transparency, which can hinder trust and adoption in important domains such as healthcare and finance. To address these concerns, we propose an extension of deep one-class classification techniques that incorporates feature attribution methods, allowing users to understand how input features contribute to model predictions. We evaluate our method on several benchmark datasets and demonstrate significantly improved performance compared to standard approaches while still maintaining high levels of interpretability. Our framework has broad applications across various industries where explainable models are crucial for building confidence in artificial intelligence systems.",1
"Neural link predictors are immensely useful for identifying missing edges in large scale Knowledge Graphs. However, it is still not clear how to use these models for answering more complex queries that arise in a number of domains, such as queries using logical conjunctions ($\land$), disjunctions ($\lor$) and existential quantifiers ($\exists$), while accounting for missing edges. In this work, we propose a framework for efficiently answering complex queries on incomplete Knowledge Graphs. We translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. We then analyse two solutions to the optimisation problem, including gradient-based and combinatorial search. In our experiments, the proposed approach produces more accurate results than state-of-the-art methods -- black-box neural models trained on millions of generated queries -- without the need of training on a large and diverse set of complex queries. Using orders of magnitude less training data, we obtain relative improvements ranging from 8% up to 40% in Hits@3 across different knowledge graphs containing factual information. Finally, we demonstrate that it is possible to explain the outcome of our model in terms of the intermediate solutions identified for each of the complex query atoms. All our source code and datasets are available online, at https://github.com/uclnlp/cqd.",0
"In this research paper, we propose a new method for answering complex queries using neural link predictors. Our approach utilizes recent advances in natural language processing (NLP) and machine learning algorithms to enable more accurate and efficient query answering than traditional methods. We begin by describing the current state-of-the-art techniques used in NLP and discussing their limitations in addressing complex questions. Then, we present our proposed model that incorporates novel neural components capable of capturing important features of documents such as topics and entities, which can then be leveraged for question answering. Next, we conduct extensive experiments on benchmark datasets to evaluate the performance of our model against several baseline models and show its superior accuracy at generating relevant responses to complex queries. Finally, we conclude with suggestions for future work that could potentially improve upon our method even further. Overall, our findings demonstrate the effectiveness of using neural link predictors for complex query answering tasks and contribute valuable insights into the field of NLP.",1
"Introduction- This paper mainly describes a way to detect with high accuracy patients with early-stage Alzheimer's disease (ES-AD) versus healthy control (HC) subjects, from datasets built with handwriting and drawing task records. Method- The proposed approach uses subject's response times. An optimal subset of tasks is first selected with a ""Support Vector Machine"" (SVM) associated with a grid search. Mixtures of Gaussian distributions defined in the space of task durations are then used to reproduce and explain the results of the SVM. Finally, a surprisingly simple and efficient ad hoc classification algorithm is deduced from the Gaussian mixtures. Results- The solution presented in this paper makes two or even four times fewer errors than the best results of the state of the art concerning the classification HC/ES-AD from handwriting and drawing tasks. Discussion- The best SVM learning model reaches a high accuracy for this classification but its learning capacity is too large to ensure a low overfitting risk regarding the small size of the dataset. The proposed ad hoc classification algorithm only requires to optimize three real-parameters. It should therefore benefit from a good generalization ability.",0
"Response time measures can offer new insights into how brain functioning differs across individuals with early-stage Alzheimerâ€™s disease (eAD). Previous work has investigated whether response times differ systematically in eAD as compared to healthy age-matched controls (HAMC) using neuropsychological tests that tap into cognitive domains implicated by AD pathology. Yet these studies have used small samples sizes and simple analyses which cannot capture the complex patterns present in datasets. We aimed to classify participants with eAD from HAMC using their response times data from three diverse tasks measuring psychomotor speed (Trail Making Test), attention (D2 Cancellation task), and working memory updating (Letter Number Span Forward span). Our methods use regularized linear regression models trained on features derived from performance metrics across all tasks. We validate our approach through simulation experiments testing generalizability to random subsets and cross-validation over kfold splits. We furthermore quantify statistical power by comparing results from simulations to actual dataset responses. Overall we find significant improvements upon prior state-of-the-art methods, while providing increased interpretability via feature visualization techniques such as heat maps and pairwise scatterplots. Our study provides evidence suggesting new research directions towards developing efficient diagnostic tools. Future efforts should consider larger sample sizes given their positive impact seen throughout our analysis and investigate ways of incorporating multi-modal information sources which may contain complementary or confirmatory information. By utilizing novel machine learning methodologies, future studies will allow clinicians to better differentiate patients with eAD from HAMC at earlier stages of deterioration ultimately improving their quality o",1
"Mixup is a popular data augmentation technique based on taking convex combinations of pairs of examples and their labels. This simple technique has been shown to substantially improve both the robustness and the generalization of the trained model. However, it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup.",0
"The recently introduced mixup technique has received attention due to its potential benefits on image classification accuracy as well as robustness against common corruptions such as noise and shift variations like brightness and contrast changes. By linearly combining pairs of training images with their corresponding labels, mixup encourages models to make use of more complex decision boundaries instead of relying solely on simple patterns that may only hold under specific conditions. In this work we provide insights into how mixup affects model performance by studying the impact on different types of data sets (including CIFAR-10/100) and different neural network architectures. Our analysis shows that mixup consistently improves both accuracy and adversarial robustness across all scenarios while maintaining competitive results compared to other regularizations methods like dropout and label smoothing. Furthermore, through ablation studies we demonstrate the effectiveness of each component within mixup; removing either random resizing or weighted averaging leads to noticeably worse generalization capabilities. Overall our findings suggest that mixup acts as an effective regularizer providing consistent gains over existing alternatives at no additional computational cost during inference. While further research is necessary to fully comprehend the underlying mechanisms behind mixupâ€™s success these preliminary results indicate significant promise for its incorporation into modern machine learning pipelines for improved task performance and reliability. Keywords: mixup regularization, robustness, generalization, corruption, dropout, label smoothing",1
"Two of the main challenges for cropland classification by satellite time-series images are insufficient ground-truth data and inaccessibility of high-quality hyperspectral images for under-developed areas. Unlabeled medium-resolution satellite images are abundant, but how to benefit from them is an open question. We will show how to leverage their potential for cropland classification using self-supervised tasks. Self-supervision is an approach where we provide simple training signals for the samples, which are apparent from the data's structure. Hence, they are cheap to acquire and explain a simple concept about the data. We introduce three self-supervised tasks for cropland classification. They reduce epistemic uncertainty, and the resulting model shows superior accuracy in a wide range of settings compared to SVM and Random Forest. Subsequently, we use the self-supervised tasks to perform unsupervised domain adaptation and benefit from the labeled samples in other regions. It is crucial to know what information to transfer to avoid degrading the performance. We show how to automate the information selection and transfer process in cropland classification even when the source and target areas have a very different feature distribution. We improved the model by about 24% compared to a baseline architecture without any labeled sample in the target domain. Our method is amenable to gradual improvement, works with medium-resolution satellite images, and does not require complicated models. Code and data are available.",0
"This study presents an efficient method for the classification of cropland regions using remotely sensed data in areas where labeled training data is scarce. We address the challenge of accurately identifying agricultural land use in regions with limited ground truth by proposing a novel feature extraction technique that leverages texture features derived from high resolution satellite imagery. Our approach combines these textures with traditional spectral indices and machine learning algorithms to improve the accuracy of crop type classification. Results obtained on two distinct datasets demonstrate that our proposed method outperforms state-of-the-art approaches, achieving overall accuracies greater than 90%. Furthermore, we show that our method can effectively handle variations in regional climate, topography, and farming practices, making it highly applicable across diverse environments worldwide. Our work contributes new insights into the integration of remote sensing and big data analytics for global food security monitoring and sustainable agriculture management.",1
"We present a hierarchical VAE that, for the first time, generates samples quickly while outperforming the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.",0
"In this paper we investigate whether Variational Autoencoders (VAE) can outperform autoregressive models at generating images. Our work explores various design choices and compares their ability to generate images with high quality. We consider both qualitative metrics such as visual fidelity and coherence, as well as quantitive measures like FID scores. From these experiments, we find that our proposed method of using VAEs significantly outperforms existing autoregressive methods. Finally, we conclude by discussing future directions in which this research could be extended and applied.",1
"We study the spatio-temporal prediction problem and introduce a novel point-process-based prediction algorithm. Spatio-temporal prediction is extensively studied in Machine Learning literature due to its critical real-life applications such as crime, earthquake, and social event prediction. Despite these thorough studies, specific problems inherent to the application domain are not yet fully explored. Here, we address the non-stationary spatio-temporal prediction problem on both densely and sparsely distributed sequences. We introduce a probabilistic approach that partitions the spatial domain into subregions and models the event arrivals in each region with interacting point-processes. Our algorithm can jointly learn the spatial partitioning and the interaction between these regions through a gradient-based optimization procedure. Finally, we demonstrate the performance of our algorithm on both simulated data and two real-life datasets. We compare our approach with baseline and state-of-the-art deep learning-based approaches, where we achieve significant performance improvements. Moreover, we also show the effect of using different parameters on the overall performance through empirical results and explain the procedure for choosing the parameters.",0
"This study investigates a novel methodology for spatio-temporal sequence prediction that combines point processes and self-organizing decision trees (SODT). We present a framework for modeling spatio-temporal sequences using SODT, which allows us to capture complex patterns and relationships between features, events, and observations over time and space. Our approach extends classical models used in spatial statistics by incorporating temporal dynamics into the analysis and leveraging SODTsâ€™ abilities in nonlinear dimension reduction and feature selection. The effectiveness of our proposed method is demonstrated through several case studies on real-world data sets such as urban mobility, pedestrian flow forecasting, environmental monitoring, and natural hazard risk assessment. These experiments showcase the potential applications of our work across different domains while providing insightful results in predicting future states and estimating uncertainty measures. Overall, we contribute towards building robust predictive models capable of capturing multi-faceted patterns from diverse sources, thus supporting informed decisions and effective planning strategies.",1
"Cardiac Magnetic Resonance (CMR) is the most effective tool for the assessment and diagnosis of a heart condition, which malfunction is the world's leading cause of death. Software tools leveraging Artificial Intelligence already enhance radiologists and cardiologists in heart condition assessment but their lack of transparency is a problem. This project investigates if it is possible to discover concepts representative for different cardiac conditions from the deep network trained to segment crdiac structures: Left Ventricle (LV), Right Ventricle (RV) and Myocardium (MYO), using explainability methods that enhances classification system by providing the score-based values of qualitative concepts, along with the key performance metrics. With introduction of a need of explanations in GDPR explainability of AI systems is necessary. This study applies Discovering and Testing with Concept Activation Vectors (D-TCAV), an interpretaibilty method to extract underlying features important for cardiac disease diagnosis from MRI data. The method provides a quantitative notion of concept importance for disease classified. In previous studies, the base method is applied to the classification of cardiac disease and provides clinically meaningful explanations for the predictions of a black-box deep learning classifier. This study applies a method extending TCAV with a Discovering phase (D-TCAV) to cardiac MRI analysis. The advantage of the D-TCAV method over the base method is that it is user-independent. The contribution of this study is a novel application of the explainability method D-TCAV for cardiac MRI anlysis. D-TCAV provides a shorter pre-processing time for clinicians than the base method.",0
"Title: ""Evaluating the Interpretable Performance of a Convolutional Neural Network for Automatic Heart MRI Image Segmentation""  Abstract: In recent years, deep learning has emerged as a promising technique for automated image segmentation tasks. However, interpreting the output from these models remains challenging due to their black box nature. This study evaluates the interpretability performance of a convolutional neural network (CNN) model trained on a publicly available cardiac magnetic resonance imaging (MRI) dataset using the ACDC challenge. Our approach utilizes feature visualization techniques such as activation maximization and class activation maps to provide insights into the decision making process of the CNN model. We demonstrate that our method can produce accurate segmentations while also providing meaningful visualizations to facilitate interpretation by domain experts. Furthermore, we compare our results against existing approaches and discuss potential improvements for future work in medical image analysis. Overall, this research aims to bridge the gap between high accuracy automatic segmentation methods and transparent and human-interpretable outputs.",1
"Numerical experiments demonstrate that deep neural network classifiers progressively separate class distributions around their mean, achieving linear separability on the training set, and increasing the Fisher discriminant ratio. We explain this mechanism with two types of operators. We prove that a rectifier without biases applied to sign-invariant tight frames can separate class means and increase Fisher ratios. On the opposite, a soft-thresholding on tight frames can reduce within-class variabilities while preserving class means. Variance reduction bounds are proved for Gaussian mixture models. For image classification, we show that separation of class means can be achieved with rectified wavelet tight frames that are not learned. It defines a scattering transform. Learning $1 \times 1$ convolutional tight frames along scattering channels and applying a soft-thresholding reduces within-class variabilities. The resulting scattering network reaches the classification accuracy of ResNet-18 on CIFAR-10 and ImageNet, with fewer layers and no learned biases.",0
"Deep learning models have recently achieved state-of-the-art results across many tasks by training deep neural networks on large amounts of data. Despite their successes, these architectures often suffer from slow convergence rates due to vanishing gradients as well as difficult optimization landscapes that can lead to poor generalization performance. We show how careful network design choices such as depthwise separability coupled with concentration regularizers can mitigate these issues while still achieving competitive model performance without sacrificing accuracy. Our approach uses simple yet powerful principles from harmonic analysis applied directly to modern neural network architectures in order to enhance optimization stability and improve generalization. The restless human spirit: Why we should look beyond Earth for our future Human history has been driven by exploration and expansion since before recorded time began. Early humans spread across Asia and Africa over 70 thousand years ago during one of the most intense cold periods of the ice age. Over thousands more years they expanded further to Europe against challenging odds. Just a few millennia later they were setting sail across oceans, discovering new continents through gritty determination, courage, and technology. And now, having conquered space itself, we stand on the cusp of an even greater opportunityâ€”to reach out into the solar system, unlocking secrets hidden within other planets and moons, and ultimately fulfilling our speciesâ€™ destiny beyond this tiny blue dot called earth. Why would anyone resist venturing forth? There is an old saying that nothing worth doing was ever easy. But if there is no resistance there is no challenge. Without challenges life becomes meaningless, because we donâ€™t grow or learn under ease. Venturing forward means accepting risk, uncertainty, and adversity. Yet this struggle against nature and ourselves creates personal growth, self-discovery, and meaningful lives which make it all very much worth it. So let us welcome the difficulties ahead! For only then shall the true value of human life become clear. #humanspaceself",1
"Credit assessments activities are essential for financial institutions and allow the global economy to grow. Building robust, solid and accurate models that estimate the probability of a default of a company is mandatory for credit insurance companies, moreover when it comes to bridging the trade finance gap. Automating the risk assessment process will allow credit risk experts to reduce their workload and focus on the critical and complex cases, as well as to improve the loan approval process by reducing the time to process the application. The recent developments in Artificial Intelligence are offering new powerful opportunities. However, most AI techniques are labelled as blackbox models due to their lack of explainability. For both users and regulators, in order to deploy such technologies at scale, being able to understand the model logic is a must to grant accurate and ethical decision making. In this study, we focus on companies credit scoring and we benchmark different machine learning models. The aim is to build a model to predict whether a company will experience financial problems in a given time horizon. We address the black box problem using eXplainable Artificial Techniques in particular, post-hoc explanations using SHapley Additive exPlanations. We bring light by providing an expert-aligned feature relevance score highlighting the disagreement between a credit risk expert and a model feature attribution explanation in order to better quantify the convergence towards a better human-aligned decision making.",0
"This study explores the process of credit risk scoring and how expert analysts evaluate financial indicators to make informed lending decisions. We propose a novel methodology that utilizes feature contribution alignment (FCA) to better understand and interpret individual factors that contribute to credit risk assessment. Our approach involves eliciting feedback from domain experts on the importance of different features commonly used in credit risk modeling. Through FCA analysis, we identify key drivers behind observed differences in credit scores, highlight areas where alternative perspectives may exist among experts, and provide insights into potential sources of bias in existing models. Our findings demonstrate the effectiveness of our proposed methodology in improving transparency and communication within the field of credit risk modeling, and have important implications for the development of more accurate and reliable predictive models. By providing greater clarity on the role played by each individual factor in determining overall creditworthiness, we can enhance confidence in risk evaluation processes while also promoting continuous improvement based on expert opinion.",1
"Despite the superior performance in modeling complex patterns to address challenging problems, the black-box nature of Deep Learning (DL) methods impose limitations to their application in real-world critical domains. The lack of a smooth manner for enabling human reasoning about the black-box decisions hinder any preventive action to unexpected events, in which may lead to catastrophic consequences. To tackle the unclearness from black-box models, interpretability became a fundamental requirement in DL-based systems, leveraging trust and knowledge by providing ways to understand the model's behavior. Although a current hot topic, further advances are still needed to overcome the existing limitations of the current interpretability methods in unsupervised DL-based models for Anomaly Detection (AD). Autoencoders (AE) are the core of unsupervised DL-based for AD applications, achieving best-in-class performance. However, due to their hybrid aspect to obtain the results (by requiring additional calculations out of network), only agnostic interpretable methods can be applied to AE-based AD. These agnostic methods are computationally expensive to process a large number of parameters. In this paper we present the RXP (Residual eXPlainer), a new interpretability method to deal with the limitations for AE-based AD in large-scale systems. It stands out for its implementation simplicity, low computational cost and deterministic behavior, in which explanations are obtained through the deviation analysis of reconstructed input features. In an experiment using data from a real heavy-haul railway line, the proposed method achieved superior performance compared to SHAP, demonstrating its potential to support decision making in large scale critical systems.",0
"Unsupervised anomaly detection has been a popular field in recent years due to its wide range of applications such as fraud detection, intrusion detection, and outlier analysis. However, most existing methods suffer from two main limitations: high computational complexity and lack of interpretability. This study proposes a novel approach that addresses these issues by using residual explanations to identify anomalies in data sets. Our method utilizes deep learning models to learn the underlying patterns in the data and then generates residuals by subtracting the predicted value from the actual value. These residuals are used to detect unusual observations, which are classified as anomalies. The key advantage of our method lies in its ability to provide explainable results through the use of residual explanations. Furthermore, our method can handle large datasets without sacrificing accuracy and computation efficiency. We evaluated our proposed method on several benchmark datasets and achieved promising results compared to state-of-the-art approaches. Our findings suggest that residual explanations have great potential in improving the performance and interpretability of unsupervised anomaly detection algorithms.",1
"Methods for supervised principal component analysis (SPCA) aim to incorporate label information into principal component analysis (PCA), so that the extracted features are more useful for a prediction task of interest. Prior work on SPCA has focused primarily on optimizing prediction error, and has neglected the value of maximizing variance explained by the extracted features. We propose a new method for SPCA that addresses both of these objectives jointly, and demonstrate empirically that our approach dominates existing approaches, i.e., outperforms them with respect to both prediction error and variation explained. Our approach accommodates arbitrary supervised learning losses and, through a statistical reformulation, provides a novel low-rank extension of generalized linear models.",0
"This paper proposes a multiobjective approach for supervised principal component analysis (SPCA), which is used to learn discriminative directions by combining labeled data with unlabeled data. We first introduce two objectives: maximizing class separability on the learned projection vectors and minimizing their reconstruction error. Then we present three types of solutions based on different combinations of these two objectives. For each solution, we provide comprehensive experimental evaluations on several benchmark datasets, comparing our results against state-of-the-art methods, demonstrating the effectiveness and superiority of the proposed framework. Our work can serve as an important stepping stone towards more advanced machine learning techniques that leverage both labeled and unlabeled data simultaneously.",1
"Deep learning inference on embedded devices is a burgeoning field with myriad applications because tiny embedded devices are omnipresent. But we must overcome major challenges before we can benefit from this opportunity. Embedded processors are severely resource constrained. Their nearest mobile counterparts exhibit at least a 100 -- 1,000x difference in compute capability, memory availability, and power consumption. As a result, the machine-learning (ML) models and associated ML inference framework must not only execute efficiently but also operate in a few kilobytes of memory. Also, the embedded devices' ecosystem is heavily fragmented. To maximize efficiency, system vendors often omit many features that commonly appear in mainstream systems, including dynamic memory allocation and virtual memory, that allow for cross-platform interoperability. The hardware comes in many flavors (e.g., instruction-set architecture and FPU support, or lack thereof). We introduce TensorFlow Lite Micro (TF Micro), an open-source ML inference framework for running deep-learning models on embedded systems. TF Micro tackles the efficiency requirements imposed by embedded-system resource constraints and the fragmentation challenges that make cross-platform interoperability nearly impossible. The framework adopts a unique interpreter-based approach that provides flexibility while overcoming these challenges. This paper explains the design decisions behind TF Micro and describes its implementation details. Also, we present an evaluation to demonstrate its low resource requirement and minimal run-time performance overhead.",0
"TensorFlow Lite Micro is a new platform designed specifically for tiny embedded systems like microcontrollers. This system allows developers to train machine learning models on larger platforms and then deploy them on small devices while maintaining high levels of performance. With TensorFlow Lite Micro, users can take advantage of deep neural networks without sacrificing computational resources. In addition, support for popular hardware from companies such as NVIDIA, Intel, and ARM makes integration simple for developers working on complex projects. Overall, TensorFlow Lite Micro enables innovative applications that were previously impractical due to limitations in processing power and storage space. By bridging the gap between big data and edge computing, this technology has broad potential impacts across many industries.",1
"How does a 110-layer ResNet learn a high-complexity classifier using relatively few training examples and short training time? We present a theory towards explaining this in terms of Hierarchical Learning. We refer hierarchical learning as the learner learns to represent a complicated target function by decomposing it into a sequence of simpler functions to reduce sample and time complexity. We formally analyze how multi-layer neural networks can perform such hierarchical learning efficiently and automatically by applying SGD.   On the conceptual side, we present, to the best of our knowledge, the FIRST theory result indicating how deep neural networks can still be sample and time efficient using SGD on certain hierarchical learning tasks, when NO KNOWN existing algorithm is efficient. We establish a new principle called ""backward feature correction"", where training higher-level layers in the network can improve the features of lower-level ones. We believe this is the key to understand the deep learning process in multi-layer neural networks.   On the technical side, we show for regression and even binary classification, for every input dimension $d0$, there is a concept class of degree $\omega(1)$ polynomials so that, using $\omega(1)$-layer neural networks as learners, SGD can learn any function from this class in $\mathsf{poly}(d)$ time and sample complexity to any $\frac{1}{\mathsf{poly}(d)}$ error, through learning to represent it as a composition of $\omega(1)$ layers of quadratic functions. In contrast, we do not know any other simple algorithm (including layer-wise training or applying kernel method sequentially) that can learn this concept class in $\mathsf{poly}(d)$ time even to any $d^{-0.01}$ error. As a side result, we prove $d^{\omega(1)}$ lower bounds for several non-hierarchical learners, including any kernel methods, neural tangent or neural compositional kernels.",0
"This research investigates how deep learning performs feature correction through backward feature generation. The study presents a novel framework for utilizing deep neural networks (DNNs) to learn complex representations from raw data inputs. By leveraging the power of DNNs, the method enables high-dimensional features to be learned by effectively capturing intricate patterns and structures within large datasets. Our experiments demonstrate that our approach significantly improves accuracy compared to traditional baseline models while maintaining computational efficiency. Overall, these results suggest that backward feature correction using deep learning can enable advanced machine intelligence applications across diverse domains.",1
"Graph clustering, aiming to partition nodes of a graph into various groups via an unsupervised approach, is an attractive topic in recent years. To improve the representative ability, several graph auto-encoder (GAE) models, which are based on semi-supervised graph convolution networks (GCN), have been developed and they achieve good results compared with traditional clustering methods. However, all existing methods either fail to utilize the orthogonal property of the representations generated by GAE, or separate the clustering and the learning of neural networks. We first prove that the relaxed k-means will obtain an optimal partition in the inner-products used space. Driven by theoretical analysis about relaxed k-means, we design a specific GAE-based model for graph clustering to be consistent with the theory, namely Embedding Graph Auto-Encoder (EGAE). Meanwhile, the learned representations are well explainable such that the representations can be also used for other tasks. To further induce the neural network to produce deep features that are appropriate for the specific clustering model, the relaxed k-means and GAE are learned simultaneously. Therefore, the relaxed k-means can be equivalently regarded as a decoder that attempts to learn representations that can be linearly constructed by some centroid vectors. Accordingly, EGAE consists of one encoder and dual decoders. Extensive experiments are conducted to prove the superiority of EGAE and the corresponding theoretical analyses.",0
"Graph clustering has been an active area of research due to its numerous real world applications such as community detection in social networks, image segmentation, anomaly detection, etc. With advancements in deep learning techniques, graph convolutional neural networks (GCNNs) have emerged as powerful tools for capturing node representations and their neighborhood structures. In this work, we propose an unsupervised framework which combines these two ideas to learn robust low dimensional embeddings for graphs, that can encode both structural and nonstructural features of graphs. Our model consists of three components: feature encoder, graph autoencoder and decoder. We first extract a high dimensional embedding using a GCNN architecture, which acts as input to our graph autoencoder. Subsequently, a bottleneck layer reduces the complexity of representation while retaining essential characteristics. The decoder component then reconstructs the original graph from the encoded vector. During training we minimize reconstruction error and additionally introduce clustering loss function to enforce formation of tight clusters corresponding to individual classes. The effectiveness of proposed approach is validated on multiple benchmark datasets across diverse domains comprising computer vision, bioinformatics, sensor network and text data, where it consistently outperforms existing state-of-the-art methods, demonstrating better cluster quality and stability. Additionally, ablation studies are conducted to evaluate importance of each component separately. Overall, our contribution lies in designing a novel unified endtoend solution for graph clustering that integrates power of graph convolutions along with regularization imposed through autoencoders.",1
"There has been a surge in the interest of using machine learning techniques to assist in the scientific process of formulating knowledge to explain observational data. We demonstrate the use of Bayesian Hidden Physics Models to first uncover the physics governing the propagation of acoustic impulses in metallic specimens using data obtained from a pristine sample. We then use the learned physics to characterize the microstructure of a separate specimen with a surface-breaking crack flaw. Remarkably, we find that the physics learned from the first specimen allows us to understand the backscattering observed in the latter sample, a qualitative feature that is wholly absent from the specimen from which the physics were inferred. The backscattering is explained through inhomogeneities of a latent spatial field that can be recognized as the speed of sound in the media.",0
"Title: Discovery of Physics and Characterization of Microstructure from Data with Bayesian Hidden Physics Models. Authors: [Authors] Publication Journal: Scientific Reports Date Published (or Accepted): [Date published or accepted] This research paper presents new developments on using hidden physics models within a Bayesian framework to aid in discovering physics principles governing systems observed via data acquisition. Examples focus on microstructures, which are crucial components across applications including semiconductors, batteries, solar cells, sensors, materials synthesis, and others in technological and scientific advancement fields. The work described herein builds upon previous efforts by further enhancing interpretability through human expert knowledge integration, improving computational efficiency without sacrificing accuracy by adaptively refining sampling points, streamlining inference procedures for real-time monitoring/feedback capability, and evaluating generality in problem size scaling up. Successful validation through rigorous benchmarks in diverse scenarios highlights robustness across numerous instances of unseen/unrelated problems. Overall, these contributions provide powerful tools applicable to many complex settings, paving ways for more accurate understanding and improved performance in engineering design, process control, and product reliability. Keywords: Bayesian statistics, inverse problems, uncertainty quantification, machine learning, Markov Chain Monte Carlo simulations, microscopy image analysis, multi-physics simulation models. Abbreviations used: BHPM - Bayesian Hidden Physics Model; MCMC â€“ Markov Chain Monte Carlo simulation; SEM - Scanning Electron Microscope; XRD - X-ray Diffraction; TEM - Transmission Elec",1
"This paper aims to understand adversarial attacks and defense from a new perspecitve, i.e., the signal-processing behavior of DNNs. We novelly define the multi-order interaction in game theory, which satisfies six properties. With the multi-order interaction, we discover that adversarial attacks mainly affect high-order interactions to fool the DNN. Furthermore, we find that the robustness of adversarially trained DNNs comes from category-specific low-order interactions. Our findings provide more insights into and make a revision of previous understanding for the shape bias of adversarially learned features. Besides, the multi-order interaction can also explain the recoverability of adversarial examples.",0
"In todayâ€™s rapidly evolving landscape of machine learning, there has been increasing interest in utilizing adversarial training as a means to enhance model performance. While adversarial examples can provide valuable insights into the strengths and weaknesses of machine learning models, their use in feature learning remains largely unexplored. This study seeks to address that gap by examining how game theory can aid our understanding of adversarily learned features. Using both theoretical analysis and empirical experiments, we demonstrate that incorporating adversarial training into feature representation leads to significantly improved accuracy on downstream tasks. Furthermore, by framing feature generation as a two-player zero-sum game, we show that specific choice of regularization techniques â€“ such as L1/L2 regularization and dropout â€“ lead to distinct Nash equilibrium solutions which translate to unique types of features. Finally, through analysis of the Hessian matrix during optimization, we reveal new insights into the relationship between regularization type and the geometry of parameter space. Overall, these findings highlight the potential benefits of leveraging game theory to design more effective machine learning algorithms. With widespread application across industries from healthcare to finance to transportation, these advances hold great promise for enhancing future technological capabilities and solving real world problems at scale.",1
"Quantum computing-based machine learning mainly focuses on quantum computing hardware that is experimentally challenging to realize due to requiring quantum gates that operate at very low temperature. Instead, we demonstrate the existence of a lower performance and much lower effort island on the accuracy-vs-qubits graph that may well be experimentally accessible with room temperature optics. This high temperature ""quantum computing toy model"" is nevertheless interesting to study as it allows rather accessible explanations of key concepts in quantum computing, in particular interference, entanglement, and the measurement process.   We specifically study the problem of classifying an example from the MNIST and Fashion-MNIST datasets, subject to the constraint that we have to make a prediction after the detection of the very first photon that passed a coherently illuminated filter showing the example. Whereas a classical set-up in which a photon is detected after falling on one of the $28\times 28$ image pixels is limited to a (maximum likelihood estimation) accuracy of $21.27\%$ for MNIST, respectively $18.27\%$ for Fashion-MNIST, we show that the theoretically achievable accuracy when exploiting inference by optically transforming the quantum state of the photon is at least $41.27\%$ for MNIST, respectively $36.14\%$ for Fashion-MNIST.   We show in detail how to train the corresponding transformation with TensorFlow and also explain how this example can serve as a teaching tool for the measurement process in quantum mechanics.",0
"This work addresses the problem of single photon image classification by presenting a new deep learning architecture based on a convolutional neural network (CNN) that can accurately classify images acquired using single-photon emission computed tomography (SPECT). We train our model on two large datasets of SPECT brain images labeled as either healthy controls or patients diagnosed with neurodegenerative diseases such as Alzheimerâ€™s disease or Parkinsonâ€™s disease. Our results show that our proposed CNN outperforms several state-of-the-art models for low-light medical imaging tasks, achieving high accuracy and robustness across different data sets and experimental conditions. Furthermore, we demonstrate that our method is capable of processing extremely small amounts of data while still producing excellent performance compared to other methods. These findings have important implications for both fundamental research on human brain function and clinical applications, where more accurate diagnostics could lead to earlier detection and better treatment of debilitating diseases. Overall, this work provides evidence that the use of machine learning approaches based on novel deep architectures has the potential to revolutionize the field of medical imaging and dramatically improve patient care and outcomes.",1
"Verifying and explaining the behavior of neural networks is becoming increasingly important, especially when they are deployed in safety-critical applications. In this paper, we study verification problems for Binarized Neural Networks (BNNs), the 1-bit quantization of general real-numbered neural networks. Our approach is to encode BNNs into Binary Decision Diagrams (BDDs), which is done by exploiting the internal structure of the BNNs. In particular, we translate the input-output relation of blocks in BNNs to cardinality constraints which are then encoded by BDDs. Based on the encoding, we develop a quantitative verification framework for BNNs where precise and comprehensive analysis of BNNs can be performed. We demonstrate the application of our framework by providing quantitative robustness analysis and interpretability for BNNs. We implement a prototype tool BDD4BNN and carry out extensive experiments which confirm the effectiveness and efficiency of our approach.",0
"A BDD-based quantitative analysis framework has been developed specifically for binarized neural networks (BNN). This new approach provides an effective solution for analyzing BNN by leveraging the power of binary decision diagrams (BDD), which enable efficient model verification, error checking, and debugging capabilities. By using BDD as a tool for quantifying BNN models, researchers can gain valuable insights into network behavior, improve design accuracy, and enhance overall performance. Overall, this study presents a novel method that bridges the gap between existing tools used in BNN research and industry standards while offering advanced capabilities suitable for complex systems and large datasets. As such, our work represents a significant step forward in advancing the state of the art in both academia and practice.",1
"The success of convolutional neural networks (CNNs) in various applications is accompanied by a significant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant units, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-of-the-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource-constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning.",0
"""Abstract: This paper presents a novel criterion for deep neural network pruning called â€œpruning by explaining.â€ Traditional pruning methods remove connections that have little impact on predictions made by a model. In contrast, our method removes connections whose removal reduces the capacity of the model to store explanations about why it makes certain predictions.""",1
"By means of a local surrogate approach, an analytical method to yield explanations of AI-predictions in the framework of regression models is defined. In the case of the AI-model producing additive corrections to the predictions of a base model, the explanations are delivered in the form of a shift of its interpretable parameters as long as the AI- predictions are small in a rigorously defined sense. Criteria are formulated giving a precise relation between lost accuracy and lacking model fidelity. Two applications show how physical or econometric parameters may be used to interpret the action of neural network and random forest models in the sense of the underlying base model. This is an extended version of our paper presented at the ISM 2020 conference, where we first introduced our new approach BAPC.",0
"This paper presents a new methodology called Before and After parameter Correction (BAC) that improves explainability measures in Artificial Intelligence models. Most existing explainability methods rely on visual explanations such as saliency maps, highlighting specific regions important for predictions. However, these methods often lack insight into which parts of the model contribute the most towards final predictions. In addition, while previous efforts have focused on post-hoc analysis of trained networks; explaining pre-trained DNNs remains understudied, especially on more complex architectures like Transformers. Our proposed solution addresses both issues: firstly, we develop a novel training paradigm that incorporates explicit knowledge to guide feature attributions; secondly, through a thorough evaluation across multiple benchmark datasets, we demonstrate significant improvements over competitive baselines in terms of both fidelity and faithfulness metrics. Furthermore, our experiments showcase improved generalization performance compared to prior work - even outperforming several strong adversarial attacks designed specifically for fooling gradient-based explainability methods. Ultimately, we hope our contributions lead to better explainable deep learning systems and increased trust from end users.",1
"Learning by self-explanation is an effective learning technique in human learning, where students explain a learned topic to themselves for deepening their understanding of this topic. It is interesting to investigate whether this explanation-driven learning methodology broadly used by humans is helpful for improving machine learning as well. Based on this inspiration, we propose a novel machine learning method called learning by self-explanation (LeaSE). In our approach, an explainer model improves its learning ability by trying to clearly explain to an audience model regarding how a prediction outcome is made. LeaSE is formulated as a four-level optimization problem involving a sequence of four learning stages which are conducted end-to-end in a unified framework: 1) explainer learns; 2) explainer explains; 3) audience learns; 4) explainer re-learns based on the performance of the audience. We develop an efficient algorithm to solve the LeaSE problem. We apply LeaSE for neural architecture search on CIFAR-100, CIFAR-10, and ImageNet. Experimental results strongly demonstrate the effectiveness of our method.",0
"This paper explores how self-explanation can enhance learning in neural networks. Self-explanation refers to the process whereby humans (or other agents) generate explanations for their own thinking as they work through problems. We argue that incorporating self-explanation into neural architecture search algorithms could improve both efficiency and effectiveness compared to current methods. To achieve this goal, we propose a novel framework called ""Neurogenesis"", which combines evolutionary techniques with natural language processing to encourage architectures to provide informative explanations during optimization. Our results demonstrate significant improvements across three different domains. Firstly, our approach outperforms existing methods in terms of accuracy on target tasks while requiring fewer computational resources. Secondly, experiments show that generated self-explanations closely align with human judgments of explanation quality. Finally, analyses of evolved models reveal interesting insights into how self-explaining abilities emerge within the design space. Overall, these findings suggest promising new directions towards creating intelligent systems capable of effective and efficient problem solving via self-explanation.",1
"Existing generalization measures that aim to capture a model's simplicity based on parameter counts or norms fail to explain generalization in overparameterized deep neural networks. In this paper, we introduce a new, theoretically motivated measure of a network's simplicity which we call prunability: the smallest \emph{fraction} of the network's parameters that can be kept while pruning without adversely affecting its training loss. We show that this measure is highly predictive of a model's generalization performance across a large set of convolutional networks trained on CIFAR-10, does not grow with network size unlike existing pruning-based measures, and exhibits high correlation with test set loss even in a particularly challenging double descent setting. Lastly, we show that the success of prunability cannot be explained by its relation to known complexity measures based on models' margin, flatness of minima and optimization speed, finding that our new measure is similar to -- but more predictive than -- existing flatness-based measures, and that its predictions exhibit low mutual information with those of other baselines.",0
"As deep neural networks (DNNs) continue to grow in complexity, there has been increasing concern regarding their ability to generalize well on unseen data. One key challenge facing DNNs is overfitting, where models tend to perform better on training data than on new, unseen data. To address this issue, pruning techniques have been proposed as a method for removing redundant connections within DNNs, promoting sparse representations that can improve generalization performance. In this work, we investigate how different pruning strategies affect DNN robustness and generalization capabilities. We find that DNNs subjected to more aggressive pruning achieve higher accuracy on held out test sets compared to less pruned architectures. Our results suggest that pruning plays an important role in improving the tradeoff between model capacity and generalization performance in modern machine learning systems. Furthermore, our analysis reveals insights into the factors that contribute to successful model pruning in complex DNNs, paving the way towards developing effective methods for optimizing model behavior across multiple metrics. Overall, these findings highlight the importance of understanding network behavior under pruning for achieving high levels of both accuracy and efficiency in state-of-the-art deep learning systems.",1
"Although automated pathology classification using deep learning (DL) has proved to be predictively efficient, DL methods are found to be data and compute cost intensive. In this work, we aim to reduce DL training costs by pre-training a Resnet feature extractor using SimCLR contrastive loss for latent encoding of OCT images. We propose a novel active learning framework that identifies a minimal sub-sampled dataset containing the most uncertain OCT image samples using label propagation on the SimCLR latent encodings. The pre-trained Resnet model is then fine-tuned with the labelled minimal sub-sampled data and the underlying pathological sites are visually explained. Our framework identifies upto 2% of OCT images to be most uncertain that need prioritized specialist attention and that can fine-tune a Resnet model to achieve upto 97% classification accuracy. The proposed method can be extended to other medical images to minimize prediction costs.",0
"This study presents a new semi-supervised approach for image subsampling in pathology images called SISE-PC (Semi-Supervised Image Subsampling for Explainable Pathology). The goal of SISE-PC is to increase the diagnostic accuracy and explainability of deep learning models by creating high-quality subsets from large datasets while reducing computational costs and data needs. To achieve this, our method leverages both supervised labels on some images and unlabelled images that can provide additional contextual information. We evaluate SISE-PC using two publicly available pathological image datasets and compare its performance against state-of-the-art techniques. Our results show significant improvements in diagnostic performance compared to baseline methods across different model architectures and dataset sizes, making SISE-PC an attractive option for efficient and effective image analysis tasks.",1
"Feature alignment is an approach to improving robustness to distribution shift that matches the distribution of feature activations between the training distribution and test distribution. A particularly simple but effective approach to feature alignment involves aligning the batch normalization statistics between the two distributions in a trained neural network. This technique has received renewed interest lately because of its impressive performance on robustness benchmarks. However, when and why this method works is not well understood. We investigate the approach in more detail and identify several limitations. We show that it only significantly helps with a narrow set of distribution shifts and we identify several settings in which it even degrades performance. We also explain why these limitations arise by pinpointing why this approach can be so effective in the first place. Our findings call into question the utility of this approach and Unsupervised Domain Adaptation more broadly for improving robustness in practice.",0
"Despite the wide use of deep learning algorithms across multiple domains, they remain prone to overfitting on training datasets with limited generalization ability. Recent works have focused on feature alignment techniques such as batch renormalization and dropconnect regularizations that aim at improving model robustness by aligning hidden features to their ground truth counterparts. In practice however these methods lead to suboptimal results due to their post-hoc nature which involves no direct supervision during training. We propose a novel endto-end framework called MultiTask Ensemble which leverages multi-task learning paradigms through sharing parameters and gradients among diverse tasks to jointly optimize representations towards both primary objectives and auxiliary constraints. Our extensive evaluation on benchmark datasets demonstrates consistent improvement against stateoftheart baselines using our holistic approach, further validating the importance of proper feature alignment towards model robustness.",1
"The problem of inferring unknown graph edges from numerical data at a graph's nodes appears in many forms across machine learning. We study a version of this problem that arises in the field of \emph{landscape genetics}, where genetic similarity between organisms living in a heterogeneous landscape is explained by a weighted graph that encodes the ease of dispersal through that landscape. Our main contribution is an efficient algorithm for \emph{inverse landscape genetics}, which is the task of inferring this graph from measurements of genetic similarity at different locations (graph nodes). Inverse landscape genetics is important in discovering impediments to species dispersal that threaten biodiversity and long-term species survival. In particular, it is widely used to study the effects of climate change and human development. Drawing on influential work that models organism dispersal using graph \emph{effective resistances} (McRae 2006), we reduce the inverse landscape genetics problem to that of inferring graph edges from noisy measurements of these resistances, which can be obtained from genetic similarity data. Building on the NeurIPS 2018 work of Hoskins et al. 2018 on learning edges in social networks, we develop an efficient first-order optimization method for solving this problem. Despite its non-convex nature, experiments on synthetic and real genetic data establish that our method provides fast and reliable convergence, significantly outperforming existing heuristics used in the field. By providing researchers with a powerful, general purpose algorithmic tool, we hope our work will have a positive impact on accelerating work on landscape genetics.",0
"In Graph Learning for Inverse Landscape Genetics we present a novel methodology that leverages graph learning principles to infer genetic models underlying complex phenotypes. We consider two types of graphs: one encoding pairwise coevolution patterns among genes (i.e., covariation) and another capturing gene regulatory interactions. Our approach relies on combining these two different aspects by modeling their joint evolutionary process. First, we develop an empirical Bayesian framework using prior distributions derived from existing knowledge to learn both networks simultaneously while allowing them to capture nonlinear relationships within each network. Secondly, conditional dependencies between latent variables representing unobserved traits affecting evolutionary change in both interaction types are explicitly modeled. This step enables us to identify possible pleiotropy effects and incorporate prior biological insights into our inference procedure. Finally, we perform posterior inference based on likelihood maximization using stochastic gradient optimization techniques and evaluate the predictive performance of our approach through extensive simulation studies as well as real data applications to both yeast transcriptomic and human metabolic datasets. Overall, our study demonstrates how graph learning can facilitate the exploration of inverse landscape genetics problems and improve understanding of genetic architecture at multiple scales.",1
"Dimensionality reduction (DR) techniques have been consistently supporting high-dimensional data analysis in various applications. Besides the patterns uncovered by these techniques, the interpretation of DR results based on each feature's contribution to the low-dimensional representation supports new finds through exploratory analysis. Current literature approaches designed to interpret DR techniques do not explain the features' contributions well since they focus only on the low-dimensional representation or do not consider the relationship among features. This paper presents ClusterShapley to address these problems, using Shapley values to generate explanations of dimensionality reduction techniques and interpret these algorithms using a cluster-oriented analysis. ClusterShapley explains the formation of clusters and the meaning of their relationship, which is useful for exploratory data analysis in various domains. We propose novel visualization techniques to guide the interpretation of features' contributions on clustering formation and validate our methodology through case studies of publicly available datasets. The results demonstrate our approach's interpretability and analysis power to generate insights about pathologies and patients in different conditions using DR results.",0
"Dimensionality reduction techniques have become increasingly important tools for scientists seeking to extract meaningful insights from high-dimensional data sets while keeping computational costs manageable. In recent years, there has been growing interest in developing ways to interpret and explain these methods in order to better understand their strengths, limitations, and potential biases. This paper introduces a new approach for interpreting and explaining the results of linear dimensionality reduction techniques based on Shapley value decomposition theory. Our method provides insight into which features contribute most to the performance of different algorithms, allowing researchers to identify key drivers of dimensionality reduction behavior. We demonstrate our approach on several real-world datasets and show that it offers valuable insights into how different algorithmic choices can shape the resulting embedding space. By providing a framework for explaining these complex relationships, we hope our work will encourage more thoughtful consideration of the tradeoffs involved in choosing among different dimensionality reduction approaches. Ultimately, by improving understanding of these decisions and enabling more informed use of these methods, we aim to support scientific discovery across a wide range of fields where large scale data analysis plays a critical role.",1
"Explainable Artificial Intelligence (XAI) is a rising field in AI. It aims to produce a demonstrative factor of trust, which for human subjects is achieved through communicative means, which Machine Learning (ML) algorithms cannot solely produce, illustrating the necessity of an extra layer producing support to the model output. When approaching the medical field, we can see challenges arise when dealing with the involvement of human-subjects, the ideology behind trusting a machine to tend towards the livelihood of a human poses an ethical conundrum - leaving trust as the basis of the human-expert in acceptance to the machines decision. The aim of this paper is to apply XAI methods to demonstrate the usability of explainable architectures as a tertiary layer for the medical domain supporting ML predictions and human-expert opinion, XAI methods produce visualization of the feature contribution towards a given models output on both a local and global level. The work in this paper uses XAI to determine feature importance towards high-dimensional data-driven questions to inform domain-experts of identifiable trends with a comparison of model-agnostic methods in application to ML algorithms. The performance metrics for a glass-box method is also provided as a comparison against black-box capability for tabular data. Future work will aim to produce a user-study using metrics to evaluate human-expert usability and opinion of the given models.",0
"This is how I would write the abstract (based on my understanding of your guidance):  Artificial intelligence (AI) has become increasingly important in modern healthcare, but there remains a need for explainable AI methods that can help practitioners understand how these models make their decisions. In this study, we compare several different approaches to explainable AI (XAI), focusing specifically on high-dimensional electronic health records (EHR). We explore three case studies across medical imaging, administrative data prediction, and time series analysis for sepsis diagnosis. Our evaluation considers both quantitative metrics such as accuracy and F1 score, along with qualitative assessments from clinical experts on usability and interpretability. By comparing the strengths and limitations of each method, our work provides insights into which XAI techniques may be most appropriate for use in real-world healthcare settings. Overall, this study demonstrates the importance of considering contextual factors in choosing XAI methods, highlighting the potential benefits of transparency, causality, and domain knowledge integration for improving EHR applications.",1
"As a family member of Recurrent Neural Networks and similar to Long-Short-Term Memory cells, Echo State Networks (ESNs) are capable of solving temporal tasks, but with a substantially easier training paradigm based on linear regression. However, optimizing hyper-parameters and efficiently implementing the training process might be somewhat overwhelming for the first-time users of ESNs. This paper aims to facilitate the understanding of ESNs in theory and practice. Treating ESNs as non-linear filters, we explain the effect of the hyper-parameters using familiar concepts such as impulse responses. Furthermore, the paper introduces the Python toolbox PyRCN (Python Reservoir Computing Network) for developing, training and analyzing ESNs on arbitrarily large datasets. The tool is based on widely-used scientific packages, such as numpy and scipy and offers an interface to scikit-learn. Example code and results for classification and regression tasks are provided.",0
"Artificial neural networks (ANN) have gained significant traction recently due to their effectiveness at solving many complex problems across several domains. Echo State Networks (ESN), first introduced by Jaeger et al. (2001), constitute a specific class of ANN that has been successfully applied to numerous tasks ranging from control systems, modeling time series data, signal processing, among others. This work presents an exploratory study aimed at expanding our understanding of the properties and capabilities of ESNs. Through this research, we provide insights into their inner workings while highlighting new ways they can be utilized, opening up opportunities for novel applications. Our contributions include establishing connections between existing works on ESNs in different fields and offering empirical evidence of ESN behavior as well as performance evaluations against other machine learning models on benchmark datasets. By providing guidance on how these architectures behave, the potential benefits of using them can be effectively communicated to users and scientists alike. We thus present a comprehensive resource outlining the current state-of-the-art of ESNs which shall serve both practitioners and researchers interested in employing them within their respective domains.",1
"Most pictures shared online are accompanied by a temporal context (i.e., the moment they were taken) that aids their understanding and the history behind them. Claiming that these images were captured in a different moment can be misleading and help to convey a distorted version of reality. In this work, we present the nascent problem of detecting timestamp manipulation. We propose an end-to-end approach to verify whether the purported time of capture of an image is consistent with its content and geographic location. The central idea is the use of supervised consistency verification, in which we predict the probability that the image content, capture time, and geographical location are consistent. We also include a pair of auxiliary tasks, which can be used to explain the network decision. Our approach improves upon previous work on a large benchmark dataset, increasing the classification accuracy from 59.03% to 81.07%. Finally, an ablation study highlights the importance of various components of the method, showing what types of tampering are detectable using our approach.",0
"This paper proposes a novel method for detecting temporal metadata manipulations within digital audio signals by analyzing their content using deep learning techniques such as convolutional neural networks (CNNs). Many modern audio editing tools allow users to manipulate timing information related to specific events within the audio signal, such as adjusting the pitch without changing the tempo, or vice versa. These changes can cause subtle differences in the waveform that traditional feature extraction methods may miss, making them difficult to detect. In contrast, our proposed approach focuses on the underlying content of the audio data itself, identifying patterns unique to each type of manipulation through training on large amounts of labeled examples. Experiments conducted on a diverse range of audio files show promising results for detecting temporal metadata manipulations accurately across different genres and time periods. Our findings contribute to the wider field of media forensics and provide new insights into understanding how humans process auditory information. We believe our work has important implications for applications such as music production, courtroom testimony, and digital watermarking.",1
"Recurrent Neural Networks (RNNs) are often used for sequential modeling of adverse outcomes in electronic health records (EHRs) due to their ability to encode past clinical states. These deep, recurrent architectures have displayed increased performance compared to other modeling approaches in a number of tasks, fueling the interest in deploying deep models in clinical settings. One of the key elements in ensuring safe model deployment and building user trust is model explainability. Testing with Concept Activation Vectors (TCAV) has recently been introduced as a way of providing human-understandable explanations by comparing high-level concepts to the network's gradients. While the technique has shown promising results in real-world imaging applications, it has not been applied to structured temporal inputs. To enable an application of TCAV to sequential predictions in the EHR, we propose an extension of the method to time series data. We evaluate the proposed approach on an open EHR benchmark from the intensive care unit, as well as synthetic data where we are able to better isolate individual effects.",0
"Title: Explanatory Models for Electronic Health Records using Domain Knowledge Abstract Summary: This research presents a novel approach for creating explainable models from electronic health records (EHRs). Our technique leverages domain knowledge such as medical codes and concepts to provide human-readable explanations for predictions made by EHR classification systems. We demonstrate that our concept-based approach significantly outperforms traditional blackbox methods while enabling end-users to make informed decisions based on more transparent reasoning behind the modelsâ€™ predictions. By providing meaningful insights into the decision making process, we expect healthcare practitioners will have increased trust in recommendations generated from these computational tools; ultimately improving patient care through data-driven personalized treatment options. In summary, we propose an innovative method to create interpretable predictive models utilizing rich domain expertise embedded within EHR repositories. Further details can be found in the full manuscript available online at [insert URL]. Keywords: electronic health record, machine learning, interpretation, explanation",1
"Understanding generalization and estimation error of estimators for simple models such as linear and generalized linear models has attracted a lot of attention recently. This is in part due to an interesting observation made in machine learning community that highly over-parameterized neural networks achieve zero training error, and yet they are able to generalize well over the test samples. This phenomenon is captured by the so called double descent curve, where the generalization error starts decreasing again after the interpolation threshold. A series of recent works tried to explain such phenomenon for simple models. In this work, we analyze the asymptotics of estimation error in ridge estimators for convolutional linear models. These convolutional inverse problems, also known as deconvolution, naturally arise in different fields such as seismology, imaging, and acoustics among others. Our results hold for a large class of input distributions that include i.i.d. features as a special case. We derive exact formulae for estimation error of ridge estimators that hold in a certain high-dimensional regime. We show the double descent phenomenon in our experiments for convolutional models and show that our theoretical results match the experiments.",0
"Here is a possible abstract:  Convolutional models have become increasingly popular due to their ability to handle high-dimensional data such as images and video frames. In particular, ridge regression is often used to obtain smooth predictions by minimizing a regularized objective function that balances fitting errors against complexity. However, little is known about the asymptotic behavior of ridge regression in convolutional models. We address this gap by providing novel theoretical results on the convergence rate and optimal parameter choice for ridge regression applied to convolutional models. Our findings provide new insights into the tradeoff between model flexibility and stability, and shed light on how to design efficient algorithms for large-scale applications.",1
"Capsule Networks, as alternatives to Convolutional Neural Networks, have been proposed to recognize objects from images. The current literature demonstrates many advantages of CapsNets over CNNs. However, how to create explanations for individual classifications of CapsNets has not been well explored. The widely used saliency methods are mainly proposed for explaining CNN-based classifications; they create saliency map explanations by combining activation values and the corresponding gradients, e.g., Grad-CAM. These saliency methods require a specific architecture of the underlying classifiers and cannot be trivially applied to CapsNets due to the iterative routing mechanism therein. To overcome the lack of interpretability, we can either propose new post-hoc interpretation methods for CapsNets or modifying the model to have build-in explanations. In this work, we explore the latter. Specifically, we propose interpretable Graph Capsule Networks (GraCapsNets), where we replace the routing part with a multi-head attention-based Graph Pooling approach. In the proposed model, individual classification explanations can be created effectively and efficiently. Our model also demonstrates some unexpected benefits, even though it replaces the fundamental part of CapsNets. Our GraCapsNets achieve better classification performance with fewer parameters and better adversarial robustness, when compared to CapsNets. Besides, GraCapsNets also keep other advantages of CapsNets, namely, disentangled representations and affine transformation robustness.",0
"This paper presents a novel architecture which extends capsule networks (CAP) by incorporating graph convolutional neural networks (GCNN). In contrast to existing architectures that perform object recognition via processing images as graphs or using traditional CNN models combined with other non-CNN components (e.g., RNN), we introduce interpretable graph capsules that allow both efficient inference and better accuracy on image classification tasks than these methods. Specifically, we observe a 12% improvement over state-of-the-art results. We then proceed to study the interpretability of our method through visualizations: by generating synthetic examples optimized to confuse standard classifiers and comparing them to those adversarial against GCAP, we show how graph capsules can provide insights into what features they use for recognition. These experiments highlight two key advantages: firstly, robustness of our model against certain types of attacks; secondly, an ability to generate human-interpretable features. Finally, ablation studies justify design choices made throughout the architecture. Overall, our approach offers new tradeoffs between efficiency and accuracy for the important task of object recognition.",1
"This letter presents a novel framework termed DistSTN for the task of synthetic aperture radar (SAR) automatic target recognition (ATR). In contrast to the conventional SAR ATR algorithms, DistSTN considers a more challenging practical scenario for non-cooperative targets whose aspect angles for training are incomplete and limited in a partial range while those of testing samples are unlimited. To address this issue, instead of learning the pose invariant features, DistSTN newly involves an elaborated feature disentangling model to separate the learned pose factors of a SAR target from the identity ones so that they can independently control the representation process of the target image. To disentangle the explainable pose factors, we develop a pose discrepancy spatial transformer module in DistSTN to characterize the intrinsic transformation between the factors of two different targets with an explicit geometric model. Furthermore, DistSTN develops an amortized inference scheme that enables efficient feature extraction and recognition using an encoder-decoder mechanism. Experimental results with the moving and stationary target acquisition and recognition (MSTAR) benchmark demonstrate the effectiveness of our proposed approach. Compared with the other ATR algorithms, DistSTN can achieve higher recognition accuracy.",0
"Artificial intelligence (AI) has become increasingly important in many fields, including remote sensing image analysis for target recognition using Synthetic Aperture Radar (SAR) images. However, one major challenge faced by current methods is their limited ability to recognize targets that appear at partial aspect angles due to occlusions caused by surrounding terrain features such as trees, buildings, or other obstacles. To address this issue, we propose a novel feature disentangling method based on a transformer architecture called pose discrepancy spatial transformers (PDST). This approach separates the features from the input image into different subspaces according to different aspects such as azimuth angle, elevation angle, and phase difference, which allows for better representation and discrimination of partially observed targets. Experimental results demonstrate that our proposed method significantly outperforms traditional approaches in terms of accuracy and robustness under varying conditions, showing promising potential in enhancing automated target recognition capabilities in complex environments.",1
"Wearable sensor based human activity recognition is a challenging problem due to difficulty in modeling spatial and temporal dependencies of sensor signals. Recognition models in closed-set assumption are forced to yield members of known activity classes as prediction. However, activity recognition models can encounter an unseen activity due to body-worn sensor malfunction or disability of the subject performing the activities. This problem can be addressed through modeling solution according to the assumption of open-set recognition. Hence, the proposed self attention based approach combines data hierarchically from different sensor placements across time to classify closed-set activities and it obtains notable performance improvement over state-of-the-art models on five publicly available datasets. The decoder in this autoencoder architecture incorporates self-attention based feature representations from encoder to detect unseen activity classes in open-set recognition setting. Furthermore, attention maps generated by the hierarchical model demonstrate explainable selection of features in activity recognition. We conduct extensive leave one subject out validation experiments that indicate significantly improved robustness to noise and subject specific variability in body-worn sensor signals. The source code is available at: github.com/saif-mahmud/hierarchical-attention-HAR",0
"In this work, we propose a novel approach for open-set human activity recognition using a hierarchical self attention based autoencoder (HSA). The HSA leverages multi-scale feature representations learned from raw sensor data, allowing for robust modeling of diverse activities across different time scales. Our approach addresses limitations inherent in traditional one-shot learning methods by introducing an unsupervised pretraining stage, where the HSA learns to reconstruct sensor readings without explicit supervision. We then employ open set evaluation techniques to measure the effectiveness of our approach on real-world datasets. Experimental results demonstrate that our method outperforms state-of-the-art approaches while achieving improved generalization across multiple tasks. This work has significant implications for the development of intelligent systems capable of recognizing complex human activities under realistic conditions.",1
"The Automatic Quasi-clique Merger algorithm is a new algorithm adapted from early work published under the name QCM (quasi-clique merger) [Ou2006, Ou2007, Zhao2011, Qi2014]. The AQCM algorithm performs hierarchical clustering in any data set for which there is an associated similarity measure quantifying the similarity of any data i and data j. Importantly, the method exhibits two valuable performance properties: 1) the ability to automatically return either a larger or smaller number of clusters depending on the inherent properties of the data rather than on a parameter 2) the ability to return a very large number of relatively small clusters automatically when such clusters are reasonably well defined in a data set. In this work we present the general idea of a quasi-clique agglomerative approach, provide the full details of the mathematical steps of the AQCM algorithm, and explain some of the motivation behind the new methodology. The main achievement of the new methodology is that the agglomerative process now unfolds adaptively according to the inherent structure unique to a given data set, and this happens without the time-costly parameter adjustment that drove the previous QCM algorithm. For this reason we call the new algorithm \emph{automatic}. We provide a demonstration of the algorithm's performance at the task of community detection in a social media network of 22,900 nodes.",0
"This paper presents the Automatic Quasi-Clique Merger (AQCM) algorithm, which allows users to easily identify overlapping communities within networks by merging cliques. AQCM uses edge deletion to partition the graph into disjoint clique components and then merges those components based on their overlap using a voting system. By maximizing the number of common nodes shared across cliques while minimizing edge deletions, AQCM effectively identifies densely connected subgraphs that represent meaningful clusters within larger networks. The algorithm has been applied successfully to several real-world datasets and demonstrates significant improvements in accuracy and efficiency compared to existing methods. Overall, AQCM offers a powerful tool for analyzing complex network structures and uncovering hidden relationships between groups of nodes.",1
"Shapley values have become increasingly popular in the machine learning literature thanks to their attractive axiomatisation, flexibility, and uniqueness in satisfying certain notions of `fairness'. The flexibility arises from the myriad potential forms of the Shapley value \textit{game formulation}. Amongst the consequences of this flexibility is that there are now many types of Shapley values being discussed, with such variety being a source of potential misunderstanding. To the best of our knowledge, all existing game formulations in the machine learning and statistics literature fall into a category which we name the model-dependent category of game formulations. In this work, we consider an alternative and novel formulation which leads to the first instance of what we call model-independent Shapley values. These Shapley values use a (non-parametric) measure of non-linear dependence as the characteristic function. The strength of these Shapley values is in their ability to uncover and attribute non-linear dependencies amongst features. We introduce and demonstrate the use of the energy distance correlations, affine-invariant distance correlation, and Hilbert-Shmidt independence criterion as Shapley value characteristic functions. In particular, we demonstrate their potential value for exploratory data analysis and model diagnostics. We conclude with an interesting expository application to a classical medical survey data set.",0
"In today's world where big data plays a critical role, there has been growing interest in methods that can explain the behavior of complex models trained on these datasets. Two such approaches are attribution techniques and feature importance scores, which aim to attribute credit to individual features or coefficients in a model while providing interpretability and transparency. However, their reliance solely on linear relationships leads to explanations that only capture part of reality. This study examines how Shapley value - originally used for allocating costs among players based on marginal contributions - helps identify dependencies beyond first order interactions in high dimensional spaces, as commonly found in machine learning applications. By casting the problem into a cooperative game framework and defining the relevant subgames, we adapt the concept to fit the requirements of modern day analysis demands. Our experiments across several benchmark datasets demonstrate that using these non-linear Shapley values increases the accuracy and fidelity of explained predictions, compared to state-of-the-art methods limited by their focus on first-order interactions.",1
"We introduce a machine learning model, the q-CNN model, sharing key features with convolutional neural networks and admitting a tensor network description. As examples, we apply q-CNN to the MNIST and Fashion MNIST classification tasks. We explain how the network associates a quantum state to each classification label, and study the entanglement structure of these network states. In both our experiments on the MNIST and Fashion-MNIST datasets, we observe a distinct increase in both the left/right as well as the up/down bipartition entanglement entropy during training as the network learns the fine features of the data. More generally, we observe a universal negative correlation between the value of the entanglement entropy and the value of the cost function, suggesting that the network needs to learn the entanglement structure in order the perform the task accurately. This supports the possibility of exploiting the entanglement structure as a guide to design the machine learning algorithm suitable for given tasks.",0
"""Entangled Q-Convolutional Neural Networks (EQCNN) are a novel deep learning architecture that combines ideas from entanglement and quantum computing theory with classical convolutional neural networks. EQCNNs introduce new layers called entanglement gates which perform nonlinear operations on data inputs using the principles of entanglement. These gates can be incorporated into traditional convolutional neural network architectures such as VGG and ResNet, enabling more expressive modeling capabilities and improved performance across a variety of tasks including image classification, object detection, and segmentation. In addition, we present two new variants of the entanglement gate, namely the adaptive entangling layer and the multiplicative entangling layer, further expanding the representational power of EQCNNs. We demonstrate through extensive experiments that our proposed models achieve state-of-the-art results compared to the current standard methods, confirming the effectiveness of the proposed architecture.""",1
"Deep neural networks can empirically perform efficient hierarchical learning, in which the layers learn useful representations of the data. However, how they make use of the intermediate representations are not explained by recent theories that relate them to ""shallow learners"" such as kernels. In this work, we demonstrate that intermediate neural representations add more flexibility to neural networks and can be advantageous over raw inputs. We consider a fixed, randomly initialized neural network as a representation function fed into another trainable network. When the trainable network is the quadratic Taylor model of a wide two-layer network, we show that neural representation can achieve improved sample complexities compared with the raw input: For learning a low-rank degree-$p$ polynomial ($p \geq 4$) in $d$ dimension, neural representation requires only $\tilde{O}(d^{\lceil p/2 \rceil})$ samples, while the best-known sample complexity upper bound for the raw input is $\tilde{O}(d^{p-1})$. We contrast our result with a lower bound showing that neural representations do not improve over the raw input (in the infinite width limit), when the trainable network is instead a neural tangent kernel. Our results characterize when neural representations are beneficial, and may provide a new perspective on why depth is important in deep learning.",0
"In our paper we present work towards understanding hierarchical learning by examining different neural architectures that have been proposed to address this problem. We investigate how these models capture temporal dependencies and whether they can generalize across tasks. Our results show that while some model variants perform better than others on specific datasets, there is no clear winner overall. However, all models still have difficulty capturing long term dependencies or handling compositional reasoning problems. Despite these difficulties, we find that using pre-trained language models as starting points for fine-tuning leads to significant improvements in task performance over using random initialization alone. Finally, we provide insights into why certain designs seem to benefit from pre-training compared to other approaches which could help inform future research directions in this area.",1
"Disentanglement is defined as the problem of learninga representation that can separate the distinct, informativefactors of variations of data. Learning such a representa-tion may be critical for developing explainable and human-controllable Deep Generative Models (DGMs) in artificialintelligence. However, disentanglement in GANs is not a triv-ial task, as the absence of sample likelihood and posteriorinference for latent variables seems to prohibit the forwardstep. Inspired by contrastive learning (CL), this paper, froma new perspective, proposes contrastive disentanglement ingenerative adversarial networks (CD-GAN). It aims at dis-entangling the factors of inter-class variation of visual datathrough contrasting image features, since the same factorvalues produce images in the same class. More importantly,we probe a novel way to make use of limited amount ofsupervision to the largest extent, to promote inter-class dis-entanglement performance. Extensive experimental resultson many well-known datasets demonstrate the efficacy ofCD-GAN for disentangling inter-class variation.",0
"In recent years, generative adversarial networks (GANs) have gained significant attention as powerful tools for generating synthetic data. One challenge faced by GANs is that they often generate blurry images which lack sharpness and detail compared to real world images. This issue arises due to the fact that the generator network produces outputs that minimize reconstruction error rather than preserving image details. To address this problem, we propose a new method called contrastive disentanglement in GANs. Our approach uses two discriminators instead of one: a primary discriminator that enforces traditional GAN objectives and a secondary discriminator trained using gradient reversal layer that specifically focuses on measuring the difference between generated samples from different classes. By training our model jointly with both primary and secondary discriminators, the generator learns to produce high quality, sharp images that are easily separable by classifiers even without fine-tuning. We demonstrate through experiments that our proposed method outperforms current state-of-the-art methods in terms of visual fidelity and robustness against overfitting. Additionally, we showcase examples where our method has applications beyond computer vision such as text generation. Overall, our work provides insights into understanding how multiple modalities can improve performance in deep learning systems.",1
"Clustering has many important applications in computer science, but real-world datasets often contain outliers. The presence of outliers can make the clustering problems to be much more challenging. In this paper, we propose a framework for solving three representative center-based clustering with outliers problems: $k$-center/median/means clustering with outliers. The framework actually is very simple, where we just need to take a small uniform sample from the input and run an existing approximation algorithm on the sample. However, our analysis is fundamentally different from the previous (uniform and non-uniform) sampling based ideas. To explain the effectiveness of uniform sampling in theory, we introduce a ""significance"" criterion and prove that the performance of our framework depends on the significance degree of the given instance. In particular, the sample size can be independent of the input data size $n$ and the dimensionality $d$, if we assume the given instance is sufficiently ""significant"", which is in fact a fairly appropriate assumption in practice. Due to its simplicity, the uniform sampling approach also enjoys several significant advantages over the non-uniform sampling approaches. The experiments suggest that our framework can achieve comparable clustering results with existing methods, but is much easier to implement and can greatly reduce the running times. To the best of our knowledge, this is the first work that systematically studies the effectiveness of uniform sampling from both theoretical and experimental aspects.",0
"This paper evaluates the efficiency of simple uniform sampling (SUS) as a technique for generating representative samples from dense datasets that contain clusters and outliers. SUS has been shown to reduce computational complexity while preserving sample quality compared to traditional clustering algorithms like k-means. However, the effectiveness of SUS depends on several factors such as dataset characteristics and cluster structure, which can impact its performance. In this paper, we investigate under what conditions SUS performs well relative to other sampling techniques for center-based clustering applications where data points belong to one of multiple centers. Our analysis considers both synthetic and real world data sets with varying levels of noise, outlier concentration, cluster separability, and dimensionality. We compare SUS against other popular sampling methods including clusterwise outlier removal sampling (CORES), core-set based approach, and random sampling, using metrics such as normalized mutual information (NMI) and adjusted rand index (ARI). Results show that SUS can achieve better NMI scores than CORES and random sampling when there is high concentration of outliers near the boundary of clusters. Additionally, our study shows that SUS is more efficient computationally while maintaining comparable accuracy to competing methods. Overall, our findings suggest that SUS is an effective alternative to existing sampling approaches for specific types of datasets with significant advantages in terms of speedup, accuracy, and robustness to noisy data. These insights contribute towards improving understanding of how different sampling strategies affect clustering results and guide practitioners in selecting appropriate methods depending on their application requirements.",1
"Explainability is a crucial requirement for effectiveness as well as the adoption of Machine Learning (ML) models supporting decisions in high-stakes public policy areas such as health, criminal justice, education, and employment, While the field of explainable has expanded in recent years, much of this work has not taken real-world needs into account. A majority of proposed methods use benchmark datasets with generic explainability goals without clear use-cases or intended end-users. As a result, the applicability and effectiveness of this large body of theoretical and methodological work on real-world applications is unclear. This paper focuses on filling this void for the domain of public policy. We develop a taxonomy of explainability use-cases within public policy problems; for each use-case, we define the end-users of explanations and the specific goals explainability has to fulfill; third, we map existing work to these use-cases, identify gaps, and propose research directions to fill those gaps in order to have a practical societal impact through ML.",0
"As artificial intelligence (AI) continues to play an increasingly important role in shaping public policy decisions, there is a growing need for explainable machine learning (ML) techniques that can help policymakers better understand how these systems work and make informed choices. In this paper, we provide an overview of some current use cases of ML for public policy and identify several key gaps where explainability could improve outcomes. We then present several research directions that have the potential to address these gaps and advance the field of explainable ML for public policy. By highlighting both the opportunities and challenges associated with using ML in policy contexts, we aim to stimulate discussion and collaboration among stakeholders from academia, industry, and government. Ultimately, our goal is to contribute to the development of more transparent, accountable, and effective AI systems that can support evidence-based decision making across a wide range of policy areas.",1
"Designing deep networks robust to adversarial examples remains an open problem. Likewise, recent zeroth order hard-label attacks on image classification models have shown comparable performance to their first-order, gradient-level alternatives. It was recently shown in the gradient-level setting that regular adversarial examples leave the data manifold, while their on-manifold counterparts are in fact generalization errors. In this paper, we argue that query efficiency in the zeroth-order setting is connected to an adversary's traversal through the data manifold. To explain this behavior, we propose an information-theoretic argument based on a noisy manifold distance oracle, which leaks manifold information through the adversary's gradient estimate. Through numerical experiments of manifold-gradient mutual information, we show this behavior acts as a function of the effective problem dimensionality and number of training points. On real-world datasets and multiple zeroth-order attacks using dimension-reduction, we observe the same universal behavior to produce samples closer to the data manifold. This results in up to two-fold decrease in the manifold distance measure, regardless of the model robustness. Our results suggest that taking the manifold-gradient mutual information into account can thus inform better robust model design in the future, and avoid leakage of the sensitive data manifold.",0
"Adversarial examples raise fundamental concerns regarding machine learning modelsâ€™ robustness. To overcome their negative effects on prediction accuracy, researchers propose different methods, each focusing on enhancing robustness via input modification, algorithm design, and/or training objectives. Recent advancements suggest that leveraging hard labels during data collection can provide higher adversarial resistance than alternative approaches. In particular, hard-label manifolds have been shown empirically to improve both clean accuracy and certified robustness against strong white box attacks; however, there has been little insight into why such performance gains occur. This paper investigates the theoretical underpinnings behind hard-label manifolds by unveiling two crucial advantages they offer over soft-label manifolds: unexpected query efficiency that enables more effective exploration of input space and better capturing of local decision boundaries. Theoretical results demonstrate that these advantages translate directly to improved adversarial robustness even without changing the model architecture or optimization methodology. This work provides new insights into developing techniques aimed at improving machine learning modelsâ€™ robustness against adversarial perturbations. Our findings open up promising directions for building efficient algorithms tailored specifically to discovering on-manifold adversarial examples, making them amenable to rigorous analysis and characterization as well as providing practitioners a principled roadmap towards obtaining high levels of defensiveness.",1
"Ensemble-based modifications of the well-known SHapley Additive exPlanations (SHAP) method for the local explanation of a black-box model are proposed. The modifications aim to simplify SHAP which is computationally expensive when there is a large number of features. The main idea behind the proposed modifications is to approximate SHAP by an ensemble of SHAPs with a smaller number of features. According to the first modification, called ER-SHAP, several features are randomly selected many times from the feature set, and Shapley values for the features are computed by means of ""small"" SHAPs. The explanation results are averaged to get the final Shapley values. According to the second modification, called ERW-SHAP, several points are generated around the explained instance for diversity purposes, and results of their explanation are combined with weights depending on distances between points and the explained instance. The third modification, called ER-SHAP-RF, uses the random forest for preliminary explanation of instances and determining a feature probability distribution which is applied to selection of features in the ensemble-based procedure of ER-SHAP. Many numerical experiments illustrating the proposed modifications demonstrate their efficiency and properties for local explanation.",0
"Abstract: In recent years, explainability has become increasingly important in artificial intelligence (AI) as there is a growing need for transparency and accountability in decision making processes. One popular method used for explaining the decisions made by machine learning models is SHapley Additive exPlanations (SHAP), which provides a unified framework for interpreting models based on game theory concepts. However, SHAP can suffer from overfitting issues when only one set of feature attributions is considered. To address this problem, we propose using ensembles of random SHAP values rather than relying on a single explanation. We demonstrate that our proposed approach improves robustness against overfitting while still providing accurate explanations. Our experimental results show significant improvements in accuracy compared to using a single SHAP value. In addition, we provide further analysis of our method through visualizations of the distribution of SHAP values across different datasets. Overall, our findings suggest that using ensembles of random SHAP values can improve the reliability of explainable AI systems.",1
"Deep Learning models are getting more and more popular but constraints on explainability, adversarial robustness and fairness are often major concerns for production deployment. Although the open source ecosystem is abundant on addressing those concerns, fully integrated, end to end systems are lacking in open source. Therefore we provide an entirely open source, reusable component framework, visual editor and execution engine for production grade machine learning on top of Kubernetes, a joint effort between IBM and the University Hospital Basel. It uses Kubeflow Pipelines, the AI Explainability360 toolkit, the AI Fairness360 toolkit and the Adversarial Robustness Toolkit on top of ElyraAI, Kubeflow, Kubernetes and JupyterLab. Using the Elyra pipeline editor, AI pipelines can be developed visually with a set of jupyter notebooks.",0
"""CLAIMED: A Comprehensive, Scalable Approach to Building Trustworthy Artificial Intelligence""  The recent explosion of artificial intelligence (AI) technologies has transformed countless industries, from healthcare to finance to transportation. However, as AI becomes more integrated into our daily lives, there is growing concern about whether these systems can be trusted to make decisions that align with human values and ethical principles. Addressing this issue requires a comprehensive approach that encompasses both technical advances in machine learning algorithms and user experience design, as well as careful consideration of legal and regulatory frameworks.  One critical aspect of building trustworthy AI is ensuring transparency and interpretability, so users have insight into how and why certain decisions were made. This is challenging given the complex nature of many modern AI models, which often rely on deep neural networks or other black box techniques. But without some degree of explainability, AI adoption may stagnate due to concerns about liability and accountability.  To address these issues, we propose CLAIMED: a flexible, scalable component library that enables developers to build transparent, interpretable, and auditable AI applications quickly and easily. By providing pre-built building blocks for common tasks such as feature extraction, model training, inference, and decision making, CLAIMED helps reduce development time while promoting good practices for transparency and interpretability throughout the development process. At the same time, by incorporating state-of-the-art research results on explainability, robustness, security, privacy, safety, fairness, and other important topics in AI, we aim to provide out-of-the-box solutions that meet contemporary standards for trustworthiness and reliability. As a result, integrators working across different verticals and domains can leverage these components to create compelling new products with higher levels o",1
"Deep reinforcement learning (RL) has made groundbreaking advancements in robotics, data center management and other applications. Unfortunately, system-level bottlenecks in RL workloads are poorly understood; we observe fundamental structural differences in RL workloads that make them inherently less GPU-bound than supervised learning (SL). To explain where training time is spent in RL workloads, we propose RL-Scope, a cross-stack profiler that scopes low-level CPU/GPU resource usage to high-level algorithmic operations, and provides accurate insights by correcting for profiling overhead. Using RL-Scope, we survey RL workloads across its major dimensions including ML backend, RL algorithm, and simulator. For ML backends, we explain a $2.3\times$ difference in runtime between equivalent PyTorch and TensorFlow algorithm implementations, and identify a bottleneck rooted in overly abstracted algorithm implementations. For RL algorithms and simulators, we show that on-policy algorithms are at least $3.5\times$ more simulation-bound than off-policy algorithms. Finally, we profile a scale-up workload and demonstrate that GPU utilization metrics reported by commonly used tools dramatically inflate GPU usage, whereas RL-Scope reports true GPU-bound time. RL-Scope is an open-source tool available at https://github.com/UofT-EcoSystem/rlscope .",0
"Deep reinforcement learning (DRL) has emerged as one of the most promising approaches for solving complex control problems, such as those found in robotics, computer vision, and game playing. However, optimizing DRL algorithms remains challenging due to their high computational requirements and sensitivity to hyperparameters.  Existing profiling tools typically focus on single aspects of the performance optimization problem, such as model training speed, policy evaluation accuracy, or memory usage. In contrast, we introduce RL-Scope, a unified cross-stack profiler that provides fine-grained insights into all components of deep RL workloads, from environment simulation to action selection.  Our approach leverages lightweight inline profiling probes to track the execution flow of each algorithm component across different stack levels (i.e., kernel, user space libraries, and Python scripts). These profiles allow users to identify performance bottlenecks and design targeted optimizations tailored to specific use cases. To demonstrate the effectiveness of our tool, we apply RL-Scope to several popular DRL benchmarks, identifying opportunities for significant speedups through low-level code refactorings and hardware accelerations. We also show how our framework can facilitate comparative studies among different algorithm variants, highlighting the importance of considering interdependent effects when tuning hyperparameters.  Overall, RL-Scope represents a significant step forward in streamlining the development and deployment of efficient and effective deep reinforcement learning systems. Our open-source framework enables researchers and practitioners alike to achieve greater insight, productivity, and innovation within this exciting field.",1
"StyleGAN is one of the state-of-the-art image generators which is well-known for synthesizing high-resolution and hyper-realistic face images. Though images generated by vanilla StyleGAN model are visually appealing, they sometimes contain prominent circular artifacts which severely degrade the quality of generated images. In this work, we provide a systematic investigation on how those circular artifacts are formed by studying the functionalities of different stages of vanilla StyleGAN architecture, with both mechanism analysis and extensive experiments. The key modules of vanilla StyleGAN that promote such undesired artifacts are highlighted. Our investigation also explains why the artifacts are usually circular, relatively small and rarely split into 2 or more parts. Besides, we propose a simple yet effective solution to remove the prominent circular artifacts for vanilla StyleGAN, by applying a novel pixel-instance normalization (PIN) layer.",0
"Title: ""A Comprehensive Approach to Identifying and Eliminating Circular Artifacts in StyleGAN""  The image generation process using Generative Adversarial Network (GAN) technology has made significant strides over recent years. One particular GAN architecture that has proven highly effective is StyleGAN, which generates high resolution images by learning the statistical mapping from noise vectors to image pixels. However, despite these impressive results, there remain certain artifacts present in the generated images, including circular patterns and anomalies, referred to as 'circular artifacts'. These artifacts can negatively impact the quality and realism of the generated images, making them less suitable for practical applications such as in the fields of computer vision and graphics design.  This paper presents a systematic analysis and removal methodology for identifying and eliminating circular artifacts in StyleGAN generated images. By investigating the causes of these artifacts, we propose a novel approach based on mathematical modeling and visualization techniques to effectively detect and remove them while preserving the underlying image structure. Our proposed method is applied to both the input noise vector and generator network parameters space, resulting in improved image quality without sacrificing efficiency. Extensive experimental evaluations show that our method outperforms existing approaches in terms of effectiveness, robustness, and computational cost.  In summary, this study presents a comprehensive solution for addressing circular artifacts in StyleGAN generated images. Our contributions provide valuable insights into the detection and mitigation of these artifacts and represent a notable step forward towards enhancing the performance of state-of-the-art generative models for various applications demanding high-quality synthetic data.",1
"Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. In this work, we challenge this common belief by following experimental observations: 1) beyond the acknowledgment that the teacher can improve the student, the student can also enhance the teacher significantly by reversing the KD procedure; 2) a poorly-trained teacher with much lower accuracy than the student can still improve the latter significantly. To explain these observations, we provide a theoretical analysis of the relationships between KD and label smoothing regularization. We prove that 1) KD is a type of learned label smoothing regularization and 2) label smoothing regularization provides a virtual teacher model for KD. From these results, we argue that the success of KD is not fully due to the similarity information between categories from teachers, but also to the regularization of soft targets, which is equally or even more important.   Based on these analyses, we further propose a novel Teacher-free Knowledge Distillation (Tf-KD) framework, where a student model learns from itself or manuallydesigned regularization distribution. The Tf-KD achieves comparable performance with normal KD from a superior teacher, which is well applied when a stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly deployed for training deep neural networks. Without any extra computation cost, Tf-KD achieves up to 0.65\% improvement on ImageNet over well-established baseline models, which is superior to label smoothing regularization.",0
"In recent years, knowledge distillation has emerged as a popular technique for transferring knowledge from large, pre-trained models (teachers) to smaller, student models. One commonly used approach to knowledge distillation involves training the student model using soft labels generated by label smoothing regularization (LSR). LSR adds a uniform noise distribution to the one-hot encodings of the true class labels during training, which prevents overfitting and improves generalization performance. However, there are limitations to the effectiveness of LSR for knowledge distillation. This paper revisits the use of LSR in knowledge distillation and proposes several improvements that enhance the quality of the transferred knowledge and improve student performance. We empirically demonstrate the efficacy of our proposed method on multiple benchmark datasets and show that it outperforms state-of-the-art methods in both quantitative metrics and qualitative analysis. Our work contributes to the broader understanding of knowledge distillation and provides insights into how to effectively leverage pre-trained models for downstream tasks.",1
"Various methods for solving the inverse reinforcement learning (IRL) problem have been developed independently in machine learning and economics. In particular, the method of Maximum Causal Entropy IRL is based on the perspective of entropy maximization, while related advances in the field of economics instead assume the existence of unobserved action shocks to explain expert behavior (Nested Fixed Point Algorithm, Conditional Choice Probability method, Nested Pseudo-Likelihood Algorithm). In this work, we make previously unknown connections between these related methods from both fields. We achieve this by showing that they all belong to a class of optimization problems, characterized by a common form of the objective, the associated policy and the objective gradient. We demonstrate key computational and algorithmic differences which arise between the methods due to an approximation of the optimal soft value function, and describe how this leads to more efficient algorithms. Using insights which emerge from our study of this class of optimization problems, we identify various problem scenarios and investigate each method's suitability for these problems.",0
"In this paper, we present a method that can learn from inverse reinforcement learning problems where explicit policy estimates are available. Our approach uses these explicit estimates as prior knowledge, allowing us to achieve improved performance over previous methods without them. We show through several experiments on challenging tasks that our method outperforms state-of-the-art approaches while also being efficient in terms of computation time. Furthermore, we provide theoretical analysis that supports our findings. Overall, our work represents a significant advancement in the field of inverse reinforcement learning and has potential applications in areas such as robotics and autonomous driving.",1
"Recent studies revealed the mathematical connection of deep neural network (DNN) and dynamic system. However, the fundamental principle of DNN has not been fully characterized with dynamic system in terms of optimization and generalization. To this end, we build the connection of DNN and continuity equation where the measure is conserved to model the forward propagation process of DNN which has not been addressed before. DNN learns the transformation of the input distribution to the output one. However, in the measure space, there are infinite curves connecting two distributions. Which one can lead to good optimization and generaliztion for DNN? By diving the optimal transport theory, we find DNN with weight decay attempts to learn the geodesic curve in the Wasserstein space, which is induced by the optimal transport map. Compared with plain network, ResNet is a better approximation to the geodesic curve, which explains why ResNet can be optimized and generalize better. Numerical experiments show that the data tracks of both plain network and ResNet tend to be line-shape in term of line-shape score (LSS), and the map learned by ResNet is closer to the optimal transport map in term of optimal transport score (OTS). In a word, we conclude a mathematical principle of deep learning is to learn the geodesic curve in the Wasserstein space; and deep learning is a great engineering realization of continuous transformation in high-dimensional space.",0
"This paper presents a mathematical principle that underlies deep learning algorithms based on neural networks. We show that the underlying geometry of these algorithms can be understood as finding geodesics in the space of probability distributions, known as the Wasserstein space. Our approach leads to new insights into how neural networks learn and generalize, as well as providing novel optimization methods that improve their performance. We demonstrate the effectiveness of our methods using several experiments on challenging tasks such as image classification, speech recognition, and reinforcement learning. The results suggest that a deeper understanding of the geometric structure of deep learning is crucial for developing more efficient and accurate algorithms. Overall, our work has important implications for both theoretical and applied aspects of machine learning research.",1
"Dimensionality reduction is a important step in the development of scalable and interpretable data-driven models, especially when there are a large number of candidate variables. This paper focuses on unsupervised variable selection based dimensionality reduction, and in particular on unsupervised greedy selection methods, which have been proposed by various researchers as computationally tractable approximations to optimal subset selection. These methods are largely distinguished from each other by the selection criterion adopted, which include squared correlation, variance explained, mutual information and frame potential. Motivated by the absence in the literature of a systematic comparison of these different methods, we present a critical evaluation of seven unsupervised greedy variable selection algorithms considering both simulated and real world case studies. We also review the theoretical results that provide performance guarantees and enable efficient implementations for certain classes of greedy selection function, related to the concept of submodularity. Furthermore, we introduce and evaluate for the first time, a lazy implementation of the variance explained based forward selection component analysis (FSCA) algorithm. Our experimental results show that: (1) variance explained and mutual information based selection methods yield smaller approximation errors than frame potential; (2) the lazy FSCA implementation has similar performance to FSCA, while being an order of magnitude faster to compute, making it the algorithm of choice for unsupervised variable selection.",0
"This paper presents a comparative study of greedy search algorithms for unsupervised variable selection in linear regression models. The objective of variable selection is to identify a subset of relevant predictors from a larger set of candidate variables that can accurately model the response variable while reducing complexity and improving interpretability. Many algorithms have been proposed for this task, but little work has compared their performance on a diverse range of problem types. In this paper, we evaluate four popular greedy search methods - forward selection, backward elimination, Lasso, and Ridge regression - using real data sets and simulation studies. Our results show that no single method outperforms all others across all scenarios, emphasizing the need for domain knowledge and careful algorithm selection. We discuss strengths and weaknesses of each approach and provide recommendations for practitioners. Overall, our findings contribute to the understanding of variable selection as a complex optimization problem with multiple solution strategies, rather than just a tool for removing irrelevant variables.",1
"An important goal of medical imaging is to be able to precisely detect patterns of disease specific to individual scans; however, this is challenged in brain imaging by the degree of heterogeneity of shape and appearance. Traditional methods, based on image registration to a global template, historically fail to detect variable features of disease, as they utilise population-based analyses, suited primarily to studying group-average effects. In this paper we therefore take advantage of recent developments in generative deep learning to develop a method for simultaneous classification, or regression, and feature attribution (FA). Specifically, we explore the use of a VAE-GAN translation network called ICAM, to explicitly disentangle class relevant features from background confounds for improved interpretability and regression of neurological phenotypes. We validate our method on the tasks of Mini-Mental State Examination (MMSE) cognitive test score prediction for the Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort, as well as brain age prediction, for both neurodevelopment and neurodegeneration, using the developing Human Connectome Project (dHCP) and UK Biobank datasets. We show that the generated FA maps can be used to explain outlier predictions and demonstrate that the inclusion of a regression module improves the disentanglement of the latent space. Our code is freely available on Github https://github.com/CherBass/ICAM.",0
"This paper presents a novel framework called ICAM-Reg that combines interpretability methods from computer vision and machine learning for neuroimaging data analysis. Our approach enables feature attribution on both classification and regression tasks to map the neuroanatomy behind predictions. We evaluate our method by quantitatively comparing against standard practices such as thresholding or manual annotation while qualitatively visualizing heatmaps overlaid onto neuroimages which helps establish interactivity during image interpretation. Specifically we applied these methods to brain MRI scans that aim at identifying abnormalities like cerebral palsy (CP) lesions, a disorder affecting muscle tone and movement control. The proposed framework enhances existing approaches to achieve reliable segmentation accuracy with minimal user input in clinically relevant timeframes. Additionally, using real patient data, we demonstrate how our method significantly outperforms other popular tools that rely on heuristics.",1
"Following the successful application of deep convolutional neural networks to 2d human pose estimation, the next logical problem to solve is 3d human pose estimation from monocular images. While previous solutions have shown some success, they do not fully utilize the depth information from the 2d inputs. With the goal of addressing this depth ambiguity, we build a system that takes 2d joint locations as input along with their estimated depth value and predicts their 3d positions in camera coordinates. Given the inherent noise and inaccuracy from estimating depth maps from monocular images, we perform an extensive statistical analysis showing that given this noise there is still a statistically significant correlation between the predicted depth values and the third coordinate of camera coordinates. We further explain how the state-of-the-art results we achieve on the H3.6M validation set are due to the additional input of depth. Notably, our results are produced on neural network that accepts a low dimensional input and be integrated into a real-time system. Furthermore, our system can be combined with an off-the-shelf 2d pose detector and a depth map predictor to perform 3d pose estimation in the wild.",0
â€‹,1
"The growing interest for adversarial examples, i.e. maliciously modified examples which fool a classifier, has resulted in many defenses intended to detect them, render them inoffensive or make the model more robust against them. In this paper, we pave the way towards a new approach to improve the robustness of a model against black-box transfer attacks. A removable additional neural network is included in the target model, and is designed to induce the \textit{luring effect}, which tricks the adversary into choosing false directions to fool the target model. Training the additional model is achieved thanks to a loss function acting on the logits sequence order. Our deception-based method only needs to have access to the predictions of the target model and does not require a labeled data set. We explain the luring effect thanks to the notion of robust and non-robust useful features and perform experiments on MNIST, SVHN and CIFAR10 to characterize and evaluate this phenomenon. Additionally, we discuss two simple prediction schemes, and verify experimentally that our approach can be used as a defense to efficiently thwart an adversary using state-of-the-art attacks and allowed to perform large perturbations.",0
"This looks like a research paper on artificial intelligence (AI). The title mentions ""adversarial perturbations,"" which refers to intentionally adding small errors or disturbances to data inputs in order to test how robust machine learning algorithms are to these types of interference. Additionally, ""transferability"" might suggest that these manipulations can have effects beyond just one individual algorithm or input. The term ""black box"" probably means that the focus is on analyzing the performance of AI systems without necessarily knowing their internal workings or decision logic. Without more details on the specific content and goals of the paper, it is difficult to provide a summary or abstract for it.",1
"It has been experimentally observed that the efficiency of distributed training with stochastic gradient (SGD) depends decisively on the batch size and -- in asynchronous implementations -- on the gradient staleness. Especially, it has been observed that the speedup saturates beyond a certain batch size and/or when the delays grow too large. We identify a data-dependent parameter that explains the speedup saturation in both these settings. Our comprehensive theoretical analysis, for strongly convex, convex and non-convex settings, unifies and generalized prior work directions that often focused on only one of these two aspects. In particular, our approach allows us to derive improved speedup results under frequently considered sparsity assumptions. Our insights give rise to theoretically based guidelines on how the learning rates can be adjusted in practice. We show that our results are tight and illustrate key findings in numerical experiments.",0
"In this work, we investigate the impact of several critical parameters on distributed learning with large batch sizes and asynchronous updates. We focus specifically on how these parameters affect the convergence rate, accuracy, and stability of the learning algorithm. Our results show that there exists an optimal range for each parameter, beyond which either the model becomes unstable, convergence slows down significantly, or the training loss stops improving altogether. Based on our findings, we provide guidelines for choosing appropriate values for these parameters based on the specific requirements of different applications. Overall, our research highlights the importance of carefully tuning these parameters in order to achieve high performance on large-scale machine learning tasks.",1
"The literature on robustness towards common corruptions shows no consensus on whether adversarial training can improve the performance in this setting. First, we show that, when used with an appropriately selected perturbation radius, $\ell_p$ adversarial training can serve as a strong baseline against common corruptions. Then we explain why adversarial training performs better than data augmentation with simple Gaussian noise which has been observed to be a meaningful baseline on common corruptions. Related to this, we identify the $\sigma$-overfitting phenomenon when Gaussian augmentation overfits to a particular standard deviation used for training which has a significant detrimental effect on common corruption accuracy. We discuss how to alleviate this problem and then how to further enhance $\ell_p$ adversarial training by introducing an efficient relaxation of adversarial training with learned perceptual image patch similarity as the distance metric. Through experiments on CIFAR-10 and ImageNet-100, we show that our approach does not only improve the $\ell_p$ adversarial training baseline but also has cumulative gains with data augmentation methods such as AugMix, ANT, and SIN leading to state-of-the-art performance on common corruptions. The code of our experiments is publicly available at https://github.com/tml-epfl/adv-training-corruptions.",0
"Adversarial examples have been found to pose significant problems in deep learning, since even small input perturbations can cause dramatic degradation in model performance. This work examines the impact of adversarial training on robustness against common image corruptions such as noise, blur, fog, contrast changes, etc. We find that regularizing models using common corruptions is very effective at improving their ability to handle similar inputs in real world scenarios. In addition, we find that this approach compares favorably with other methods of generating synthetic images specifically designed to fool neural networks. These results suggest that incorporating adversarial training during standard use cases could significantly improve generalization for all tasks trained using generative models like deep learning. Our code will be released upon acceptance so readers may reproduce our results and extend them further.",1
"Although highly accurate automated diagnostic techniques for melanoma have been reported, the realization of a system capable of providing diagnostic evidence based on medical indices remains an open issue because of difficulties in obtaining reliable training data. In this paper, we propose bulk production augmentation (BPA) to generate high-quality, diverse pseudo-skin tumor images with the desired structural malignant features for additional training images from a limited number of labeled images. The proposed BPA acts as an effective data augmentation in constructing the feature detector for the atypical pigment network (APN), which is a key structure in melanoma diagnosis. Experiments show that training with images generated by our BPA largely boosts the APN detection performance by 20.0 percentage points in the area under the receiver operating characteristic curve, which is 11.5 to 13.7 points higher than that of conventional CycleGAN-based augmentations in AUC.",0
"Title: ""Bulk Production Augmentation Towards Explainable Melanoma Diagnosis"" Abstract: Artificial intelligence (AI) has revolutionized various fields by enhancing efficiency and accuracy through automating complex tasks. In healthcare, AI systems have been adopted",1
"We establish a bridge between spectral clustering and Gromov-Wasserstein Learning (GWL), a recent optimal transport-based approach to graph partitioning. This connection both explains and improves upon the state-of-the-art performance of GWL. The Gromov-Wasserstein framework provides probabilistic correspondences between nodes of source and target graphs via a quadratic programming relaxation of the node matching problem. Our results utilize and connect the observations that the GW geometric structure remains valid for any rank-2 tensor, in particular the adjacency, distance, and various kernel matrices on graphs, and that the heat kernel outperforms the adjacency matrix in producing stable and informative node correspondences. Using the heat kernel in the GWL framework provides new multiscale graph comparisons without compromising theoretical guarantees, while immediately yielding improved empirical results. A key insight of the GWL framework toward graph partitioning was to compute GW correspondences from a source graph to a template graph with isolated, self-connected nodes. We show that when comparing against a two-node template graph using the heat kernel at the infinite time limit, the resulting partition agrees with the partition produced by the Fiedler vector. This in turn yields a new insight into the k-cut graph partitioning problem through the lens of optimal transport. Our experiments on a range of real-world networks achieve comparable results to, and in many cases outperform, the state-of-the-art achieved by GWL.",0
"Aim: The aim of generalized spectral clustering (GenSpec) was proposed by Genovese and Wlodarczyk as a method based on the kernels K(.,.) = ||P_k(. , .)-P_k(X_,Y_)||^2_F where X _and Y _are datasets respectively. In essence, GenSpec extends classical spectral clustering by using kernel methods to compare data points, rather than Euclidean distances. This modification improves the performance of traditional spectral clustering algorithms while still providing linear scaling time complexity in terms of both data size and number of clusters. Here we propose another modification to extend the use of GenSpec even further â€“ specifically in order to tackle the problem of handling noisy or incomplete datasets. We achieve this through employing Gromov-Wasserstein learning and demonstrate how our novel approach can improve upon existing methods. Methods: Our algorithm is constructed such that if noise were added to the dataset X _such that Xâ€™=X+N, where N represents noise, then the algorithm would return clusters which are not affected by N and therefore separate outliers from real patterns. To test the effectiveness of our approach we applied it to numerous benchmark image databases including MNIST, CIFAR-10, SVHN, CelebA and COCO datasets where promising results have been achieved against state-of-the-art algorithms, showing improvements across several metrics Conclusion: By incorporating a new distance metric into the kernel used by GenSpec, derived from optimal transport theory, we enable GenSpec to effectively handle datasets which contain missing or noisy data points. The improved algorithm achieves competitive results compared to other popular approaches in challenging tasks like image classification and face verification. Overall, these findings provide evidence that our modified algorithm may serve as a powerful tool in exploratory d",1
"Research indicates that monotonous automated driving increases the incidence of fatigued driving. Although many prediction models based on advanced machine learning techniques were proposed to monitor driver fatigue, especially in manual driving, little is known about how these black-box machine learning models work. In this paper, we proposed a combination of eXtreme Gradient Boosting (XGBoost) and SHAP (SHapley Additive exPlanations) to predict driver fatigue with explanations due to their efficiency and accuracy. First, in order to obtain the ground truth of driver fatigue, PERCLOS (percentage of eyelid closure over the pupil over time) between 0 and 100 was used as the response variable. Second, we built a driver fatigue regression model using both physiological and behavioral measures with XGBoost and it outperformed other selected machine learning models with 3.847 root-mean-squared error (RMSE), 1.768 mean absolute error (MAE) and 0.996 adjusted $R^2$. Third, we employed SHAP to identify the most important predictor variables and uncovered the black-box XGBoost model by showing the main effects of most important predictor variables globally and explaining individual predictions locally. Such an explainable driver fatigue prediction model offered insights into how to intervene in automated driving when necessary, such as during the takeover transition period from automated driving to manual driving.",0
"""In recent years, automated driving has gained significant attention as a potential solution to road safety issues caused by human error such as driver fatigue. However, ensuring that these systems can effectively detect driver fatigue remains a challenge. This study presents a methodology for predicting driver fatigue using machine learning algorithms with explainability. Our approach uses data from wearable sensors worn by drivers, which capture physiological signals such as heart rate variability, galvanic skin response, and respiration rate. These signals are then fed into a deep neural network along with contextual features like time of day and weather conditions. We demonstrate our method through extensive experiments on real-world datasets, showing high accuracy in detecting driver fatigue. Furthermore, we provide interpretations of how different factors affect the model predictions, highlighting the importance of explainability in deploying artificial intelligence in safety-critical domains like automated driving.""",1
"The information bottleneck (IB) principle has been adopted to explain deep learning in terms of information compression and prediction, which are balanced by a trade-off hyperparameter. How to optimize the IB principle for better robustness and figure out the effects of compression through the trade-off hyperparameter are two challenging problems. Previous methods attempted to optimize the IB principle by introducing random noise into learning the representation and achieved state-of-the-art performance in the nuisance information compression and semantic information extraction. However, their performance on resisting adversarial perturbations is far less impressive. To this end, we propose an adversarial information bottleneck (AIB) method without any explicit assumptions about the underlying distribution of the representations, which can be optimized effectively by solving a Min-Max optimization problem. Numerical experiments on synthetic and real-world datasets demonstrate its effectiveness on learning more invariant representations and mitigating adversarial perturbations compared to several competing IB methods. In addition, we analyse the adversarial robustness of diverse IB methods contrasting with their IB curves, and reveal that IB models with the hyperparameter $\beta$ corresponding to the knee point in the IB curve achieve the best trade-off between compression and prediction, and has best robustness against various attacks.",0
"The goal of adversarial training is to increase model robustness by adding constraints that force models to find features more closely aligned with human perceptions. In practice, these methods suffer from unstable gradients and require strong regularization. We introduce an alternative method based on optimizing a novel loss function we call the Adversarial Information Bottleneck (AIB). Instead of encouraging models to find specific features, our objective explicitly constrains the mutual information flow through layers during inference, ensuring only necessary information is retained. On several benchmark datasets, including ImageNet, our approach achieves higher clean accuracy while maintaining competitive adversarial robustness compared to current state-of-the art methods. Additionally, our approach provides greater stability during optimization and eliminates the need for most forms of regularization. Our work demonstrates that incorporating an information bottleneck into deep learning objectives can improve both clean and robust performance on challenging tasks.",1
"While the need for interpretable machine learning has been established, many common approaches are slow, lack fidelity, or hard to evaluate. Amortized explanation methods reduce the cost of providing interpretations by learning a global selector model that returns feature importances for a single instance of data. The selector model is trained to optimize the fidelity of the interpretations, as evaluated by a predictor model for the target. Popular methods learn the selector and predictor model in concert, which we show allows predictions to be encoded within interpretations. We introduce EVAL-X as a method to quantitatively evaluate interpretations and REAL-X as an amortized explanation method, which learn a predictor model that approximates the true data generating distribution given any subset of the input. We show EVAL-X can detect when predictions are encoded in interpretations and show the advantages of REAL-X through quantitative and radiologist evaluation.",0
""" ""A new approach is presented that significantly improves on existing interpretability methods. By learning to encode predictions in their interpretations, these methods can provide more accurate explanations of how a machine learning model makes decisions. This work offers a fresh perspective on interpreting complex models by training interpreters alongside the main model using only the model inputs rather than labels. Experiments show that our method outperforms state-of-the-art approaches both quantitatively and qualitatively. Our results have important implications for researchers who want to improve their understanding of black box models as well as practitioners looking for better transparency in decision making systems."" -----This abstract presents a novel approach to enhance current interpretability techniques used in machine learning models. With the ability to learn from just the model inputs instead of explicit labels, the developed method outperforms present techniques. Practical applications discussed include improved insight into opaque artificial intelligence models (AI). Further experimentation and discussion provided within the full paper explore these findings further.",1
"Early detection of skin cancers like melanoma is crucial to ensure high chances of survival for patients. Clinical application of Deep Learning (DL)-based Decision Support Systems (DSS) for skin cancer screening has the potential to improve the quality of patient care. The majority of work in the medical AI community focuses on a diagnosis setting that is mainly relevant for autonomous operation. Practical decision support should, however, go beyond plain diagnosis and provide explanations. This paper provides an overview of works towards explainable, DL-based decision support in medical applications with the example of skin cancer diagnosis from clinical, dermoscopic and histopathologic images. Analysis reveals that comparably little attention is payed to the explanation of histopathologic skin images and that current work is dominated by visual relevance maps as well as dermoscopic feature identification. We conclude that future work should focus on meeting the stakeholder's cognitive concepts, providing exhaustive explanations that combine global and local approaches and leverage diverse modalities. Moreover, the possibility to intervene and guide models in case of misbehaviour is identified as a major step towards successful deployment of AI as DL-based DSS and beyond.",0
"This should read as if you were writing an actual abstract for a paper submitted to a journal. Use appropriate language. If any sentences donâ€™t fit that criteria, please remove them from your response. Please provide me with all sentences that can go into such an abstract without changing content. Deep learning algorithms have recently gained popularity due to their ability to accurately classify complex data sets. In particular, convolutional neural networks (CNNs) have been shown to excel at image classification tasks, making them promising tools for decision support in medicine. To investigate the potential application of CNNs in clinical settings, we conducted a case study on skin cancer diagnosis using a publicly available dataset of dermoscopic images. Our results showed that deep learning models can achieve high diagnostic accuracy when trained on large datasets, highlighting the promise of these approaches for improving patient outcomes. Moreover, our study identified key factors contributing to model performance and emphasizes the importance of careful data selection and preprocessing steps to ensure reliable predictions. Overall, this work demonstrates the value of integrating state-of-the-art machine learning techniques into medical practice to enhance diagnostic capabilities and ultimately improve care for patients.",1
"Face recognition (FR) systems have a growing effect on critical decision-making processes. Recent works have shown that FR solutions show strong performance differences based on the user's demographics. However, to enable a trustworthy FR technology, it is essential to know the influence of an extended range of facial attributes on FR beyond demographics. Therefore, in this work, we analyse FR bias over a wide range of attributes. We investigate the influence of 47 attributes on the verification performance of two popular FR models. The experiments were performed on the publicly available MAADFace attribute database with over 120M high-quality attribute annotations. To prevent misleading statements about biased performances, we introduced control group based validity values to decide if unbalanced test data causes the performance differences. The results demonstrate that also many non-demographic attributes strongly affect the recognition performance, such as accessories, hair-styles and colors, face shapes, or facial anomalies. The observations of this work show the strong need for further advances in making FR system more robust, explainable, and fair. Moreover, our findings might help to a better understanding of how FR networks work, to enhance the robustness of these networks, and to develop more generalized bias-mitigating face recognition solutions.",0
"In our modern world dominated by technology, face recognition has become increasingly important for security purposes. However, there have been concerns regarding the potential biases present in these systems that may result from demographic factors such as age, gender, race, ethnicity, and socioeconomic status (SES). This study aimed to expand upon previous research into bias within facial recognition algorithms beyond traditional demographic indicators. The results showed that while some previously documented demographic factors were still influential, other factors had greater impacts, including skin tone, lighting conditions, and head pose. Furthermore, we found significant discrepancies in performance across different databases, highlighting the need for standardization in evaluation procedures. Our findings emphasize the importance of considering a diverse range of variables when developing face recognition systems. By understanding and mitigating the effects of these biases, we can improve accuracy and reduce unfair treatment of underrepresented groups. This work paves the way for further exploration into bias within artificial intelligence and machine learning technologies more broadly.",1
"Interpretation of deep learning models is a very challenging problem because of their large number of parameters, complex connections between nodes, and unintelligible feature representations. Despite this, many view interpretability as a key solution to trustworthiness, fairness, and safety, especially as deep learning is applied to more critical decision tasks like credit approval, job screening, and recidivism prediction. There is an abundance of good research providing interpretability to deep learning models; however, many of the commonly used methods do not consider a phenomenon called ""feature interaction."" This work first explains the historical and modern importance of feature interactions and then surveys the modern interpretability methods which do explicitly consider feature interactions. This survey aims to bring to light the importance of feature interactions in the larger context of machine learning interpretability, especially in a modern context where deep learning models heavily rely on feature interactions.",0
"Explanation and interpretation of artificial intelligence (AI) systems play a crucial role in their development, evaluation, deployment, and societal acceptance. In particular, feature interaction has been identified as one of the key challenges facing modern AI researchers, particularly those working on models that rely on deep neural networks. Despite recent advances in understanding how these models work, there remains a significant gap between the capabilities of state-of-the-art models and human interpretability. This paper argues that feature interaction should serve as a lens through which we view interpretability in AI. To support this claim, we present three case studies, each demonstrating different aspects of feature interaction and highlighting the importance of developing interpretable AI. We showcase several approaches and techniques that can enhance our ability to explain and interpret complex relationships among features in high-dimensional spaces. These methods have broad applicability across domains where AI plays an increasingly important role and provide insights into how feature interactions shape model predictions. Our findings contribute to the broader goal of making machine learning more transparent and trustworthy by addressing the challenge of feature interaction head-on. By doing so, we aim to advance the nascent field of interpretable AI and pave the way for real-world impacts.",1
"Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers. In this paper, we propose the first quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary. Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models). We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exists shared directions along which the decision boundary of deep networks is systematically positively curved. Under such conditions, we prove the existence of small universal perturbations. Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties.",0
"Paper Title: ""Robustness of Classifiers to Universal Perturbations: A Geometric Perspective""  Abstract: This work presents a new approach for understanding the robustness of machine learning models to input perturbations based on geometry. We show that many types of universal adversarial examples exhibit distinctive mathematical properties which can be used to characterize their behavior across different models and tasks. Our framework provides insights into how these perturbations operate as well as tools for mitigating their impact on model performance. Additionally, our findings suggest promising directions for future research at the intersection of geometry and deep learning. By providing both theoretical analysis and experimental validation, we aim to advance the state of art in robustness and security of machine learning systems.",1
"Machine learning is increasingly used to inform decision-making in sensitive situations where decisions have consequential effects on individuals' lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role for the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavourably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions towards which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.",0
"Algorithmic recourse refers to the problem of designing algorithms that achieve fairness goals such as equal opportunity or demographic parity while meeting other desirable properties like efficiency, accuracy, and scalability. In recent years, there has been growing interest in addressing algorithmic bias and ensuring that machine learning systems do not discriminate against certain groups of individuals based on their sensitive characteristics. This paper surveys existing approaches to algorithmic recourse, discussing their advantages, limitations, and potential applications. We begin by defining key concepts and providing examples of how unfairness can arise in decision making processes. Next, we present different formulations of the algorithmic recourse problem, highlighting the tradeoffs involved in finding solution approaches that balance competing objectives. We then describe several methods developed to solve these optimization problems, ranging from exact mathematical programming techniques to heuristics and approximation algorithms. Finally, we discuss current challenges and future directions in algorithmic recourse research, including questions related to interpretability, generalization across datasets and domains, and the ethical considerations surrounding the use of algorithmic remedies. Overall, our work provides a comprehensive understanding of algorithmic recourse and serves as a resource for researchers and practitioners interested in developing fairer machine learning models and decision-making systems.",1
"Adversarial examples have appeared as a ubiquitous property of machine learning models where bounded adversarial perturbation could mislead the models to make arbitrarily incorrect predictions. Such examples provide a way to assess the robustness of machine learning models as well as a proxy for understanding the model training process. Extensive studies try to explain the existence of adversarial examples and provide ways to improve model robustness (e.g. adversarial training). While they mostly focus on models trained on datasets with predefined labels, we leverage the teacher-student framework and assume a teacher model, or oracle, to provide the labels for given instances. We extend Tian (2019) in the case of low-rank input data and show that student specialization (trained student neuron is highly correlated with certain teacher neuron at the same layer) still happens within the input subspace, but the teacher and student nodes could differ wildly out of the data subspace, which we conjecture leads to adversarial examples. Extensive experiments show that student specialization correlates strongly with model robustness in different scenarios, including student trained via standard training, adversarial training, confidence-calibrated adversarial training, and training with robust feature dataset. Our studies could shed light on the future exploration about adversarial examples, and enhancing model robustness via principled data augmentation.",0
"The main contribution of our work lies in identifying novel factors that impact robustness. In previous studies, existing techniques focused on understanding how teachers can adapt their teaching styles based on studentsâ€™ needs or providing students with feedback on their performance. However, we find that there are other important variables that researchers should consider when analyzing classroom interaction data collected through sensors and machine learning models designed to identify patterns associated with specific behaviors such as attention deficits or anxiety disorders. Our study demonstrates how nonverbal communication and cognitive load, for example, play crucial roles in teacher-student interactions. We provide empirical evidence showing which behavioral indicators contribute positively to improving studentsâ€™ focus and engagement during lessons while maintaining low levels of distractibility among them.",1
"Spectral clustering is a popular algorithm that clusters points using the eigenvalues and eigenvectors of Laplacian matrices derived from the data. For years, spectral clustering has been working mysteriously. This paper explains spectral clustering by dividing it into two categories based on whether the graph Laplacian is fully connected or not. For a fully connected graph, this paper demonstrates the dimension reduction part by offering an objective function: the covariance between the original data points' similarities and the mapped data points' similarities. For a multi-connected graph, this paper proves that with a proper $k$, the first $k$ eigenvectors are the indicators of the connected components. This paper also proves there is an equivalence between spectral embedding and PCA.",0
"This paper provides a mathematical analysis of spectral clustering, focusing on its relationship to principal component analysis (PCA). In particular, we show that under certain conditions, spectral clustering can be equivalent to performing PCA followed by k-means clustering. Our results shed new light on the interpretation and use of these important techniques, demonstrating their close connections and differences. We discuss the implications of our findings for both theoretical researchers and practitioners applying these methods in machine learning applications. By bridging the gap between theory and practice, this work contributes to a deeper understanding of the mathematics behind unsupervised learning algorithms such as spectral clustering and PCA.",1
"We consider the recent privacy preserving methods that train the models not on original images, but on mixed images that look like noise and hard to trace back to the original images. We explain that those mixed images will be samples on the decision boundaries of the trained model, and although such methods successfully hide the contents of images from the entity in charge of federated learning, they provide crucial information to that entity about the decision boundaries of the trained model. Once the entity has exact samples on the decision boundaries of the model, they may use it for effective adversarial attacks on the model during training and/or afterwards. If we have to hide our images from that entity, how can we trust them with the decision boundaries of our model? As a remedy, we propose a method to encrypt the images, and have a decryption module hidden inside the model. The entity in charge of federated learning will only have access to a set of complex-valued coefficients, but the model will first decrypt the images and then put them through the convolutional layers. This way, the entity will not see the training images and they will not know the location of the decision boundaries of the model.",0
"In recent years, federated learning has emerged as a powerful technique for training machine learning models on distributed datasets while preserving user privacy. However, one limitation of existing methods is that they often require users to share their decision boundaries, which can compromise security and privacy. This paper proposes a novel approach to federated learning that does not require the sharing of decision boundaries. Instead, we use an optimization framework based on partial gradients to train the model locally at each device without revealing any sensitive information. Our method achieves comparable performance to state-of-the-art techniques while providing greater protection against potential attacks by malicious parties. We evaluate our algorithm through extensive experiments on several real-world datasets and demonstrate its effectiveness in terms of accuracy and privacy. Our work contributes to the development of secure and efficient distributed learning systems that preserve individual privacy.",1
"Analysing and computing with Gaussian processes arising from infinitely wide neural networks has recently seen a resurgence in popularity. Despite this, many explicit covariance functions of networks with activation functions used in modern networks remain unknown. Furthermore, while the kernels of deep networks can be computed iteratively, theoretical understanding of deep kernels is lacking, particularly with respect to fixed-point dynamics. Firstly, we derive the covariance functions of multi-layer perceptrons (MLPs) with exponential linear units (ELU) and Gaussian error linear units (GELU) and evaluate the performance of the limiting Gaussian processes on some benchmarks. Secondly, and more generally, we analyse the fixed-point dynamics of iterated kernels corresponding to a broad range of activation functions. We find that unlike some previously studied neural network kernels, these new kernels exhibit non-trivial fixed-point dynamics which are mirrored in finite-width neural networks. The fixed point behaviour present in some networks explains a mechanism for implicit regularisation in overparameterised deep models. Our results relate to both the static iid parameter conjugate kernel and the dynamic neural tangent kernel constructions. Software at github.com/RussellTsuchida/ELU_GELU_kernels.",0
"Artificial neural networks (ANN) have become an important tool for solving complex problems in many fields such as image and speech recognition, natural language processing, and even board games like Go. One common activation function used in these networks is Rectified Linear Unit (ReLU), which has shown good results on various tasks but can lead to vanishing gradients during backpropagation, making training more difficult and slower. To address this issue, Exponential Linear Unit (Elu) was introduced as a variant of ReLU that allows for faster convergence while maintaining high accuracy. Recent advances have led to Generalized Exponential Linear Units (GELUs), which have been found to perform better than other variants such as Swish and Softsign. This paper proposes using infinite versions of Elu and GELU activation functions in deep learning models. By doing so, we aim to explore whether they can provide an alternative to traditional finite activations and achieve higher performance. We evaluate our approach through several experiments and compare them against state-of-the art methods. Our findings suggest that infinite Elu and GELU activation functions improve stability and speed of convergence without significantly affecting model accuracy. Overall, the proposed method offers a new direction in the field of ANN research and could potentially lead to further improvements in computational efficiency and accuracy across different domains.",1
"What makes two images similar? We propose new approaches to generate model-agnostic explanations for image similarity, search, and retrieval. In particular, we extend Class Activation Maps (CAMs), Additive Shapley Explanations (SHAP), and Locally Interpretable Model-Agnostic Explanations (LIME) to the domain of image retrieval and search. These approaches enable black and grey-box model introspection and can help diagnose errors and understand the rationale behind a model's similarity judgments. Furthermore, we extend these approaches to extract a full pairwise correspondence between the query and retrieved image pixels, an approach we call ""joint interpretations"". Formally, we show joint search interpretations arise from projecting Harsanyi dividends, and that this approach generalizes Shapley Values and The Shapley-Taylor indices. We introduce a fast kernel-based method for estimating Shapley-Taylor indices and empirically show that these game-theoretic measures yield more consistent explanations for image similarity architectures.",0
"Abstract: Despite significant advancements in computer vision technology, visual search systems often struggle to provide accurate and interpretable results for complex queries. One common reason for these failures is that existing explainability techniques tend to rely on specific model architectures or require manual engineering efforts, which makes them difficult to integrate into general-purpose systems without incurring high computational overheads or sacrificing efficiency. To address this challenge, we propose a model-agnostic approach called MAE (Model Agnostic Explainability) that can generate reliable explanations for visual search tasks across different models and scenarios. Our method exploits recent breakthroughs in attention visualization to produce concise yet informative descriptions for any given query-image pair. Experiments show that our MAE significantly enhances understanding of visual search mechanisms while improving overall performance compared to state-of-the-art methods. By decoupling interpretability from intricate designs, our work takes important strides towards more transparent artificial intelligence that better serves societal needs.",1
"Successive Subspace Learning (SSL) offers a light-weight unsupervised feature learning method based on inherent statistical properties of data units (e.g. image pixels and points in point cloud sets). It has shown promising results, especially on small datasets. In this paper, we intuitively explain this method, provide an overview of its development, and point out some open questions and challenges for future research.",0
"Submitted Abstract: Advances in technology have enabled the collection and storage of vast amounts of data, driving a demand for machine learning algorithms that can efficiently process and analyze big data sets. Many traditional methods face limitations in handling large datasets due to their reliance on linear models and high computational requirements. To address these challenges, recent research has focused on developing more efficient techniques such as successive subspace learning (SSL). SSL involves dividing the dataset into smaller subsets, which allows for faster computation while retaining most of the benefits of batch gradient descent. This overview paper provides a comprehensive summary of the state-of-the-art developments in SSL from both theoretical and applied perspectives. We discuss the motivation behind using SSL, its variants, advancements, and comparisons against other popular alternatives. Our aim is to provide readers with a better understanding of SSL and facilitate future research in the area. Keywords: Big Data; Machine Learning; Gradient Descent; Subspace Learning;Successive Subspace Learning Revised Abstract: With the proliferation of data generation technologies, there arises a need to design effective machine learning algorithms capable of processing and analyzing large datasets. Traditional methods based on linear models often fall short in meeting these demands due to their inherent limitations and substantial computational requirements. In light of these obstacles, there has been growing interest in exploring alternative approaches such as successive subspace learning (SSL) to overcome these difficulties. This overview paper offers a thorough evaluation of SSL methodologies, including their advantages and disadvantages relative to established schemes. The primary focus lies in presenting a clear explanation of how SSL works, highlighting its key features and significant contributions to literature. By examining both theoretical frameworks and real-world applications, our objective is to furnish readers with a detailed comprehension of SSL and inspire further investigations within the field. Key terms: big data, machine learning, gradient descent, subspace learning, successive subspace learning",1
"We compare machine learning explainability methods based on the theory of atomic (Shapley, 1953) and infinitesimal (Aumann and Shapley, 1974) games, in a theoretical and experimental investigation into how the model and choice of integration path can influence the resulting feature attributions. To gain insight into differences in attributions resulting from interventional Shapley values (Sundararajan and Najmi, 2019; Janzing et al., 2019; Chen et al., 2019) and Generalized Integrated Gradients (GIG) (Merrill et al., 2019) we note interventional Shapley is equivalent to a multi-path integration along $n!$ paths where $n$ is the number of model input features. Applying Stoke's theorem we show that the path symmetry of these two methods results in the same attributions when the model is composed of a sum of separable functions of individual features and a sum of two-feature products. We then perform a series of experiments with varying degrees of data missingness to demonstrate how interventional Shapley's multi-path approach can yield less consistent attributions than the single straight-line path of Aumann-Shapley. We argue this is because the multiple paths employed by interventional Shapley extend away from the training data manifold and are therefore more likely to pass through regions where the model has little support. In the absence of a more meaningful path choice, we therefore advocate the straight-line path since it will almost always pass closer to the data manifold. Among straight-line path attribution algorithms, GIG is uniquely robust since it will still yield Shapley values for atomic games modeled by decision trees.",0
"Game theory has been applied to many fields, including artificial intelligence (AI) systems that analyze player decisions in games to determine their intentions or preferences through mechanisms such as cooperation or competition. One area where there is still room for improvement is in understanding how the choices made by players affect these algorithms and their ability to accurately attribute strategies or outcomes to specific actions. This study seeks to explore the impact of different paths taken in decision making on game-theoretic attribution methods. We develop a framework using simulations to evaluate how variations in playersâ€™ choices can influence algorithm performance across a range of environments. Our findings suggest that while certain path dependencies may decrease accuracy, others might actually improve attribution results. These insights could enhance our comprehension of both game-based assessment techniques and AI reasoning processes. By identifying ways to optimize attribution under diverse decision routes, we aim to contribute new knowledge towards robust applications of game theory in complex systems analysis. Keywords: game theory; decision making; attribution models; artificial intelligence; computer simulation",1
"Some data analysis applications comprise datasets, where explanatory variables are expensive or tedious to acquire, but auxiliary data are readily available and might help to construct an insightful training set. An example is neuroimaging research on mental disorders, specifically learning a diagnosis/prognosis model based on variables derived from expensive Magnetic Resonance Imaging (MRI) scans, which often requires large sample sizes. Auxiliary data, such as demographics, might help in selecting a smaller sample that comprises the individuals with the most informative MRI scans. In active learning literature, this problem has not yet been studied, despite promising results in related problem settings that concern the selection of instances or instance-feature pairs.   Therefore, we formulate this complementary problem of Active Selection of Classification Features (ASCF): Given a primary task, which requires to learn a model f: x- y to explain/predict the relationship between an expensive-to-acquire set of variables x and a class label y. Then, the ASCF-task is to use a set of readily available selection variables z to select these instances, that will improve the primary task's performance most when acquiring their expensive features z and including them to the primary training set.   We propose two utility-based approaches for this problem, and evaluate their performance on three public real-world benchmark datasets. In addition, we illustrate the use of these approaches to efficiently acquire MRI scans in the context of neuroimaging research on mental disorders, based on a simulated study design with real MRI data.",0
"In many applications involving image classification tasks, feature selection plays a critical role as features can have a significant impact on overall performance. While several feature selection techniques exist, most rely on selecting pre-determined features without considering their potential significance on different classifiers. To address this gap, we propose active selection of classification features (ASCF) - a framework that integrates feature and model selection under a unified objective function. Our approach iteratively selects both relevant features and hyperparameters of a specific model, thus optimizing their joint contribution to improving accuracy. By introducing flexibility into which models can use certain features, ASCF overcomes limitations associated with existing methods that assume fixed feature sets for all models. Experimental evaluations demonstrate superiority across multiple datasets compared against state-of-the-art baselines while emphasizing the importance of incorporating learned insights from one dataset to others. These results collectively contribute towards establishing ASCF as a powerful tool for effective and efficient feature selection. Ultimately, our findings provide actionable implications for practitioners designing real-world image classification systems.",1
"We present a `CLAssifier-DECoder' architecture (\emph{ClaDec}) which facilitates the comprehension of the output of an arbitrary layer in a neural network (NN). It uses a decoder to transform the non-interpretable representation of the given layer to a representation that is more similar to the domain a human is familiar with. In an image recognition problem, one can recognize what information is represented by a layer by contrasting reconstructed images of \emph{ClaDec} with those of a conventional auto-encoder(AE) serving as reference. We also extend \emph{ClaDec} to allow the trade-off between human interpretability and fidelity. We evaluate our approach for image classification using Convolutional NNs. We show that reconstructed visualizations using encodings from a classifier capture more relevant information for classification than conventional AEs. Relevant code is available at \url{https://github.com/JohnTailor/ClaDec}",0
"This abstract summarizes recent progress towards decoding neural network internal activations as explanatory models to reveal how artificial intelligence (AI) algorithms make complex decisions. The emerging field of explainability seeks to provide insights into automated decision-making processes through interpretable explanations that expose underlying reasoning mechanisms. However, many AI approaches suffer from interpretational opacity, making them difficult to monitor or steer safely under real-world conditions. Recent research has addressed this challenge by developing methods for attributing specific contributions to input features throughout layers of dense deep learning models. This abstraction allows human operators and even agents within systems themselves to discern more precisely which aspects of data drive algorithmic responses. With these newfound capabilities, we might soon expect improved transparency for AI applications across a broad spectrum, including critical decision support in healthcare, finance, governance, education, transportation, defense, entertainment, and other domains. Nonetheless, important open questions remain regarding the scalability and robustness of current techniques against adversarial attacks attempting to manipulate results by exploiting weaknesses in decoders or model architectures. Research continues toward balancing interpretability with generality while safeguarding against deliberate misinformation. Ultimately, progress on explaining AI decisions may lead not only to greater acceptance but also potentially better performance via feedback-informed fine-tuning and system refinement based on holistic situational awareness.",1
"Many existing approaches for estimating feature importance are problematic because they ignore or hide dependencies among features. A causal graph, which encodes the relationships among input variables, can aid in assigning feature importance. However, current approaches that assign credit to nodes in the causal graph fail to explain the entire graph. In light of these limitations, we propose Shapley Flow, a novel approach to interpreting machine learning models. It considers the entire causal graph, and assigns credit to \textit{edges} instead of treating nodes as the fundamental unit of credit assignment. Shapley Flow is the unique solution to a generalization of the Shapley value axioms to directed acyclic graphs. We demonstrate the benefit of using Shapley Flow to reason about the impact of a model's input on its output. In addition to maintaining insights from existing approaches, Shapley Flow extends the flat, set-based, view prevalent in game theory based explanation methods to a deeper, \textit{graph-based}, view. This graph-based view enables users to understand the flow of importance through a system, and reason about potential interventions.",0
"This paper presents a new method called Shapley Flow (SF) that uses graph theory to provide meaningful insights into complex prediction models such as decision trees, random forests, gradient boosting machines and neural networks. SF constructs an undirected bipartite network representation of model predictions using two types of nodes: instances and features. Then, a flow algorithm is applied on this network to assign credit scores to each instance based on their influence over other instances. The resulting flow values can then be used to generate feature importance rankings and visualizations, providing a clear interpretation of how different features contribute to the overall performance of the model. We demonstrate the effectiveness of our approach by applying SF to several benchmark datasets across different domains. Our results show that SF accurately identifies influential features while reducing noise compared to existing methods. Overall, SF provides a powerful tool for interpreting complex predictive models, facilitating better understanding and communication of machine learning results to stakeholders.",1
"In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI). These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardised benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.",0
This will look like it was written by me. I am writing the paper as well so please read that first if you can. Please also keep my writing style consistent throughout the abstract if possible (writing guide: https://www.springer.com/gp/authors-editors/authorandreviewerguides/manuscript-preparation/17684). Thank you! --- ---,1
"Recently, a lot of attention has been focused on the incorporation of 3D data into face analysis and its applications. Despite providing a more accurate representation of the face, 3D facial images are more complex to acquire than 2D pictures. As a consequence, great effort has been invested in developing systems that reconstruct 3D faces from an uncalibrated 2D image. However, the 3D-from-2D face reconstruction problem is ill-posed, thus prior knowledge is needed to restrict the solutions space. In this work, we review 3D face reconstruction methods proposed in the last decade, focusing on those that only use 2D pictures captured under uncontrolled conditions. We present a classification of the proposed methods based on the technique used to add prior knowledge, considering three main strategies, namely, statistical model fitting, photometry, and deep learning, and reviewing each of them separately. In addition, given the relevance of statistical 3D facial models as prior knowledge, we explain the construction procedure and provide a list of the most popular publicly available 3D facial models. After the exhaustive study of 3D-from-2D face reconstruction approaches, we observe that the deep learning strategy is rapidly growing since the last few years, becoming the standard choice in replacement of the widespread statistical model fitting. Unlike the other two strategies, photometry-based methods have decreased in number due to the need for strong underlying assumptions that limit the quality of their reconstructions compared to statistical model fitting and deep learning methods. The review also identifies current challenges and suggests avenues for future research.",0
"This is a survey paper which covers recent progress made in surveillance applications based on 3D facial recognition by using uncalibrated image sequences taken by commodity cameras. Unlike traditional biometric systems that require controlled settings such as specialized hardware or constrained environment capturing range, these methods have shown promising performance for real world deployment. To achieve accurate results, different approaches have been proposed including deep learning techniques like autoencoder neural networks and feature descriptors combined with geometric models. These techniques try to estimate depth maps and camera poses simultaneously while incorporating prior knowledge on human faces or pose constraints. In addition, some works explore new modalities such as thermal infrared imagery captured via low cost sensors, while others take advantage of videos containing audio signals extracted from microphones available on smartphone devices. Furthermore, we analyze the impact of dataset quality on achieved accuracy and discuss open challenges related to scalability and adaptivity under varying illumination conditions. We provide a comprehensive evaluation comparing different state-of-the art approaches in order to guide researchers towards designing effective frameworks tailored for their specific requirements.",1
"Developing efficient sequential bidding strategies for repeated auctions is an important practical challenge in various marketing tasks. In this setting, the bidding agent obtains information, on both the value of the item at sale and the behavior of the other bidders, only when she wins the auction. Standard bandit theory does not apply to this problem due to the presence of action-dependent censoring. In this work, we consider second-price auctions and propose novel, efficient UCB-like algorithms for this task. These algorithms are analyzed in the stochastic setting, assuming regularity of the distribution of the opponents' bids. We provide regret upper bounds that quantify the improvement over the baseline algorithm proposed in the literature. The improvement is particularly significant in cases when the value of the auctioned item is low, yielding a spectacular reduction in the order of the worst-case regret. We further provide the first parametric lower bound for this problem that applies to generic UCB-like strategies. As an alternative, we propose more explainable strategies which are reminiscent of the Explore Then Commit bandit algorithm. We provide a critical analysis of this class of strategies, showing both important advantages and limitations. In particular, we provide a minimax lower bound and propose a nearly minimax-optimal instance of this class.",0
"This paper presents efficient algorithms for stochastic repeated second-price auctions that achieve strong theoretical guarantees on their efficiency under realistic assumptions. We first consider single-item auctions where bidders have independent private values and derives new efficient auction rules based on generalized Vickrey pricing. These new rules maintain truthful reporting as they only pay the social welfare maximizing price if they win while still retaining incentive compatibility even when they lose since each bidder's expected payment always equals her perceived value times her probability of winning plus the product of her reserve price and the remaining probability of winning. Our results provide the tightest known performance guarantee on the price of anarchy among all previous algorithms for the same class of auction settings. Next we extend our techniques to general multi-dimensional cases where bidders submit complex reports including values, densities and other types of information possibly chosen from a set of predefined functions based on which bids can equivalently represented by simpler functionals. Our algorithm selects at most four different report formats whereas existing approaches may require up to $2^K$ distinct ones based on the number K of items for sale. We prove matching upper bounds on the price of stability and identify natural instances where these two quantities coincide. Finally our experimental evaluation confirms better average performance of our mechanism designs over state-of-the-art alternatives using both synthetic and real datasets. In conclusion our work represents an important step toward practical mechanisms design for stochastic repeated second-price auctions subject to fundamental constraints such as limited communication budget.",1
"Analyzing multi-source data, which are multiple views of data on the same subjects, has become increasingly common in molecular biomedical research. Recent methods have sought to uncover underlying structure and relationships within and/or between the data sources, and other methods have sought to build a predictive model for an outcome using all sources. However, existing methods that do both are presently limited because they either (1) only consider data structure shared by all datasets while ignoring structures unique to each source, or (2) they extract underlying structures first without consideration to the outcome. We propose a method called supervised joint and individual variation explained (sJIVE) that can simultaneously (1) identify shared (joint) and source-specific (individual) underlying structure and (2) build a linear prediction model for an outcome using these structures. These two components are weighted to compromise between explaining variation in the multi-source data and in the outcome. Simulations show sJIVE to outperform existing methods when large amounts of noise are present in the multi-source data. An application to data from the COPDGene study reveals gene expression and proteomic patterns that are predictive of lung function. Functions to perform sJIVE are included in the R.JIVE package, available online at http://github.com/lockEF/r.jive .",0
"This paper presents a new method for explaining variations in deep learning models through supervised joint variation (SJV) analysis. The SJIV model explains each feature as a weighted sum of all other features plus noise, then uses these weights to explain how different data points affect the output. The approach takes into account both intra-feature interaction and inter-attribute dependence while providing interpretable results that can aid in improving the models themselves. It also provides visualizations that can help users quickly identify important interactions and dependencies. Additionally, we demonstrate our methods on several real world datasets such as MNIST, CIFAR10, and ImageNet. We show improvement over current state-of-the-art approaches using mean decrease impurity which shows good correlation with human judgements. Lastly, we provide insights on future directions and potential applications.",1
"In this paper, it is shown theoretically that spurious local minima are common for deep fully-connected networks and convolutional neural networks (CNNs) with piecewise linear activation functions and datasets that cannot be fitted by linear models. A motivating example is given to explain the reason for the existence of spurious local minima: each output neuron of deep fully-connected networks and CNNs with piecewise linear activations produces a continuous piecewise linear (CPWL) output, and different pieces of CPWL output can fit disjoint groups of data samples when minimizing the empirical risk. Fitting data samples with different CPWL functions usually results in different levels of empirical risk, leading to prevalence of spurious local minima. This result is proved in general settings with any continuous loss function. The main proof technique is to represent a CPWL function as a maximization over minimization of linear pieces. Deep ReLU networks are then constructed to produce these linear pieces and implement maximization and minimization operations.",0
"This paper presents an analysis of spurious local minima in deep neural networks using piecewise linear activations (PLAs). It demonstrates that these types of locally optimal solutions can occur frequently during training, even with relatively simple models and architectures. Using both theoretical proof and experimental evaluation on synthetic data sets, we show that spurious local minima can arise due to the properties of PLAs themselves, as well as from other sources such as optimization algorithms used for backpropagation. Our findings suggest that it may be difficult to guarantee global optimality or convergence guarantees for many common PLA network designs and activation functions in practice. In conclusion, our work underscores the importance of careful consideration of architecture choices and optimization techniques when working with deep learning models.",1
"Current work in explainable reinforcement learning generally produces policies in the form of a decision tree over the state space. Such policies can be used for formal safety verification, agent behavior prediction, and manual inspection of important features. However, existing approaches fit a decision tree after training or use a custom learning procedure which is not compatible with new learning techniques, such as those which use neural networks. To address this limitation, we propose a novel Markov Decision Process (MDP) type for learning decision tree policies: Iterative Bounding MDPs (IBMDPs). An IBMDP is constructed around a base MDP so each IBMDP policy is guaranteed to correspond to a decision tree policy for the base MDP when using a method-agnostic masking procedure. Because of this decision tree equivalence, any function approximator can be used during training, including a neural network, while yielding a decision tree policy for the base MDP. We present the required masking procedure as well as a modified value update step which allows IBMDPs to be solved using existing algorithms. We apply this procedure to produce IBMDP variants of recent reinforcement learning methods. We empirically show the benefits of our approach by solving IBMDPs to produce decision tree policies for the base MDPs.",0
"Inverse Reinforcement Learning (IRL) seeks to infer reward functions from observed behavior. Classical IRL methods require that the reward function has a specific structure, such as linearity, convexity, or monotonicity. Moreover, these approaches often struggle in realistic domains where the true reward function may have complex nonlinearities or even noise. These issues make IRL very challenging, and there have been relatively few successes beyond simple problems with known analytical solutions. Recent research has begun to investigate alternative ways of representing policies, either as sets or distributions of parameters instead of point estimates. We explore two broad classes of iterative bounding algorithms for learning interpretable policies. One uses randomized value iteration networks with regularization terms, while the other adaptively samples parameter vectors within a neural network classifier, then optimizes expected rewards within each sampled policy distribution. Both families have strong guarantees on convergence rates but sacrifice some theoretical interpretability advantages over point estimators in favor of greater flexibility and improved performance. Empirically, we demonstrate our methods outperform prior state-of-the-art systems across diverse benchmarks under varying levels of noise. Our results suggest both classes of algorithms contribute valuable improvements toward practical inverse reinforcement learning.",1
"With the broader and highly successful usage of machine learning in industry and the sciences, there has been a growing demand for Explainable AI. Interpretability and explanation methods for gaining a better understanding about the problem solving abilities and strategies of nonlinear Machine Learning, in particular, deep neural networks, are therefore receiving increased attention. In this work we aim to (1) provide a timely overview of this active emerging field, with a focus on 'post-hoc' explanations, and explain its theoretical foundations, (2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations, (3) outline best practice aspects i.e. how to best include interpretation methods into the standard usage of machine learning and (4) demonstrate successful usage of explainable AI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of machine learning.",0
"Deep neural networks (DNN) have been increasingly popular over recent years due to their success on many tasks including image recognition, speech synthesis, machine translation, text generation, and games [2]. Despite such progress, understanding DNNs remains difficult because there exist few theoretical guarantees and interpretability concerns remain high [7][8] . In addition, other models built on different principles may provide complementary advantages which researchers could better exploit if they had more knowledge of all available options beyond just deep neural networks [6][9] . This work therefore seeks to survey methodologies for explaining DNN decisions along several dimensions important to interpreting model behavior. One dimension considers whether methods examine individual neurons/features or entire classifiers. Another examines how these approaches visualize or quantify explanations. Finally, we evaluate both data requirements and computational cost associated with each approach . We then summarize applications of these explainable DNN techniques that can impact downstream use cases, providing concrete examples such as diagnosis support systems, scientific discovery processes, financial fraud detection software, legal decision support tools, self-driving cars, hiring platforms, and content moderation pipelines. By combining analysis across types, costs, uses, strengths, weaknesses ,and future directions, our unified framework helps clarify tradeoffs involved in incorporating diverse interpretive techniques into the workflow from formulating questions through deploying DNN solutions . As such, it should assist users, developers, investors, policymakers, educators, and researchers who need informed perspectives on design choices related t",1
"Machine Learning (ML) provides important techniques for classification and predictions. Most of these are black-box models for users and do not provide decision-makers with an explanation. For the sake of transparency or more validity of decisions, the need to develop explainable/interpretable ML-methods is gaining more and more importance. Certain questions need to be addressed:   How does an ML procedure derive the class for a particular entity? Why does a particular clustering emerge from a particular unsupervised ML procedure? What can we do if the number of attributes is very large? What are the possible reasons for the mistakes for concrete cases and models?   For binary attributes, Formal Concept Analysis (FCA) offers techniques in terms of intents of formal concepts, and thus provides plausible reasons for model prediction. However, from the interpretable machine learning viewpoint, we still need to provide decision-makers with the importance of individual attributes to the classification of a particular object, which may facilitate explanations by experts in various domains with high-cost errors like medicine or finance.   We discuss how notions from cooperative game theory can be used to assess the contribution of individual attributes in classification and clustering processes in concept-based machine learning. To address the 3rd question, we present some ideas on how to reduce the number of attributes using similarities in large contexts.",0
"In recent years, machine learning has become increasingly popular as a means of making sense of large datasets in fields such as finance, healthcare, and marketing. However, one persistent challenge faced by practitioners of machine learning is ensuring that the results produced by their algorithms can be interpreted and explained in a meaningful way. This task becomes even more difficult when working with complex models that use nonlinear relationships between variables, which make it hard to determine how specific inputs lead to certain outputs. One approach to address these challenges is through concept-based machine learning, which relies on human knowledge to guide model construction and analysis. This paper examines the role of interpretability and similarity in concept-based machine learning, highlighting their importance in developing reliable, transparent, and explainable models. We provide case studies from different domains to illustrate our findings and discuss potential applications of our methods. Our work provides valuable insights for researchers and professionals seeking to improve the trustworthiness and effectiveness of their machine learning systems.",1
"We introduce HiPaR, a novel pattern-aided regression method for tabular data containing both categorical and numerical attributes. HiPaR mines hybrid rules of the form $p \Rightarrow y = f(X)$ where $p$ is the characterization of a data region and $f(X)$ is a linear regression model on a variable of interest $y$. HiPaR relies on pattern mining techniques to identify regions of the data where the target variable can be accurately explained via local linear models. The novelty of the method lies in the combination of an enumerative approach to explore the space of regions and efficient heuristics that guide the search. Such a strategy provides more flexibility when selecting a small set of jointly accurate and human-readable hybrid rules that explain the entire dataset. As our experiments shows, HiPaR mines fewer rules than existing pattern-based regression methods while still attaining state-of-the-art prediction performance.",0
"This paper presents a new method called HiPaR (Hierarchical Pattern-Aided Regression), which utilizes multiple levels of hierarchy in data representation to achieve more accurate regression modeling. In traditional regression analysis, the relationship between predictors and response variables is typically modeled using low-dimensional representations of data, such as scalar values or simple summary statistics. However, high-dimensional datasets often contain complex patterns that cannot be captured by these simple representations, resulting in suboptimal prediction accuracy. HiPaR addresses this issue by leveraging hierarchically organized multidimensional pattern descriptors to represent data at different scales, enabling better learning and interpretation of nonlinear relationships within large and complex datasets. Extensive experiments on synthetic and real-world benchmarks demonstrate the superiority of HiPaR compared to state-of-the-art regression methods across various evaluation metrics and application domains. Overall, our findings showcase the effectiveness of incorporating multi-scale pattern information into regression analysis for improving performance on challenging data problems.",1
"A relatively new set of transport-based transforms (CDT, R-CDT, LOT) have shown their strength and great potential in various image and data processing tasks such as parametric signal estimation, classification, cancer detection among many others. It is hence worthwhile to elucidate some of the mathematical properties that explain the successes of these transforms when they are used as tools in data analysis, signal processing or data classification. In particular, we give conditions under which classes of signals that are created by algebraic generative models are transformed into convex sets by the transport transforms. Such convexification of the classes simplify the classification and other data analysis and processing problems when viewed in the transform domain. More specifically, we study the extent and limitation of the convexification ability of these transforms under an algebraic generative modeling framework. We hope that this paper will serve as an introduction to these transforms and will encourage mathematicians and other researchers to further explore the theoretical underpinnings and algorithmic tools that will help understand the successes of these transforms and lay the groundwork for further successful applications.",0
"Transport transforms have proven themselves as powerful tools for analyzing data from complex systems. In recent years, many developments have made these methods even more flexible, accessible, and effective. This has led researchers to use these techniques in a wide range of fields such as astronomy, biology, physics, finance, engineering, economics, medicine, etc... These methods involve two components; partitioning the data space into cells, which can then be used as basic objects to represent different kinds of signals present in the data; then expressing each partition element as linear combination of some functions that generate these partitions by acting on reference elements that live outside of the data space itself. Reference elements may come either from an embedding of the manifold in a higher dimensional ambient space or a parameterization of local charts. They allow one to capture global properties of the manifold by describing how points move away from their nearest neighbors. Transport transform theory allows us to identify ""anomalous"" patterns in data sets, such as those generated by machines rather than humans. It also makes possible the processing of massive datasets by decomposing them into manageable subproblems according to geometric properties of the underlying structures. We provide rigorous foundations for several key aspects of transport based data analysis. Specifically we introduce a new type of decomposition that combines clustering (based on optimal transport) together with graph constructions (using random walks guided by a flow). The method is motivated by an example application in genetics (analysis of copy number variation). Our methods lead naturally to algorithms that combine classical methods f",1
"Convolutional neural networks have established themselves over the past years as the state of the art method for image classification, and for many datasets, they even surpass humans in categorizing images. Unfortunately, the same architectures perform much worse when they have to compare parts of an image to each other to correctly classify this image.   Until now, no well-formed theoretical argument has been presented to explain this deficiency. In this paper, we will argue that convolutional layers are of little use for such problems, since comparison tasks are global by nature, but convolutional layers are local by design. We will use this insight to reformulate a comparison task into a sorting task and use findings on sorting networks to propose a lower bound for the number of parameters a neural network needs to solve comparison tasks in a generalizable way. We will use this lower bound to argue that attention, as well as iterative/recurrent processing, is needed to prevent a combinatorial explosion.",0
"As convolution neural networks gain popularity in the field of computer vision due to their ability to achieve high accuracy on image classification tasks, they have become widely used for other types of problems as well. However, there are some cases where these models may not perform as expected, particularly if the task requires nonlocal information processing. In this work, we argue that while CNNs can solve local pattern recognition tasks effectively by making use of translational equivariance, nonlocal tasks demand more complex modeling techniques that are better addressed using other approaches such as recurrent and transformer architectures. We provide empirical evidence that CNNs struggle with such tasks even when large scale pretraining data is available, suggesting that fundamental design limitations prevent them from achieving satisfactory performance. Our study has important implications for researchers looking to apply deep learning methods to real world applications involving nonlocal dependencies. By highlighting the drawbacks of CNNs for certain types of tasks, we hope to stimulate further investigation into alternative network designs that can handle nonlocal patterns effectively. Ultimately, our goal is to contribute towards developing more robust and versatile machine learning algorithms that can tackle diverse problem domains with equal ease.",1
"Models in the supervised learning framework may capture rich and complex representations over the features that are hard for humans to interpret. Existing methods to explain such models are often specific to architectures and data where the features do not have a time-varying component. In this work, we propose TIME, a method to explain models that are inherently temporal in nature. Our approach (i) uses a model-agnostic permutation-based approach to analyze global feature importance, (ii) identifies the importance of salient features with respect to their temporal ordering as well as localized windows of influence, and (iii) uses hypothesis testing to provide statistical rigor.",0
"Temporal black box models have proven to be powerful tools for predictive modeling, particularly when dealing with large datasets containing complex patterns that defy easy analysis by humans alone. However, as these models grow more prevalent, there remains a pressing need for ways to explain how they make predictions, especially since many users require transparency into their workings.  The study presented here explores one such approach: feature importance explanations (FIEs). We propose using FIEs to provide insight into the functioning of temporal black box models, while remaining faithful to the essence of the original system. Our method first uses feature selection techniques to identify critical features from input data based on mutual information estimates; we then train separate regression trees over each feature set and finally combine them into a single, composite tree ensemble. In contrast to existing approaches that generate ad hoc attribution methods, our solution offers interpretability grounded on well-established statistical principles.  We evaluate our proposal experimentally, applying it to several state-of-the art time-series forecasting benchmark problems, including electricity load forecasting and stock market prediction. Results demonstrate that our technique generates clear, informative attributions that align with human intuition while capturing the relevant aspects of the underlying systems. Furthermore, the insights provided by FIEs can guide users towards improving their understanding of the modelsâ€™ behavior, which facilitates debugging, fine-tuning, and ultimately better decisions. These encouraging findings serve as evidence that FIEs are a viable means for enhancing trustworthiness and accountability of temporal black box predictions without sacrificing performance, offering significant potential benefits for businesses and society alike.",1
"Link prediction in graphs is an important task in the fields of network science and machine learning. We investigate a flexible means of regularization for link prediction based on an approximation of the Kolmogorov complexity of graphs that is differentiable and compatible with recent advances in link prediction algorithms. Informally, the Kolmogorov complexity of an object is the length of the shortest computer program that produces the object. Complex networks are often generated, in part, by simple mechanisms; for example, many citation networks and social networks are approximately scale-free and can be explained by preferential attachment. A preference for predicting graphs with simpler generating mechanisms motivates our choice of Kolmogorov complexity as a regularization term. In our experiments the regularization method shows good performance on many diverse real-world networks, however we determine that this is likely due to an aggregation method rather than any actual estimation of Kolmogorov complexity.",0
"In recent years, estimating Kolmogorov complexity (KC) has proven to be a valuable tool in machine learning and data analysis. However, little research exists on the application of estimated KC as a means of regularization for link prediction. This study investigates the effectiveness of using estimated KC in regularized graph neural networks for link prediction. We evaluate the performance of our proposed method on several benchmark datasets and compare it against baseline models that use L2 regularization and other state-of-the-art methods. Our results show that incorporating estimated KC regularization improves accuracy across all evaluated datasets, outperforming existing approaches for many cases. Additionally, we analyze the behavior of estimated KC regularization in relation to network architecture and hyperparameters and find that it offers more stability and interpretability than traditional regularization techniques. Overall, this work demonstrates the potential benefits of utilizing estimated KC for regularization purposes in link prediction tasks. The code and dataset used in this study can serve as starting points for further research on leveraging KC estimates in graph representation learning.",1
"Several works have aimed to explain why overparameterized neural networks generalize well when trained by Stochastic Gradient Descent (SGD). The consensus explanation that has emerged credits the randomized nature of SGD for the bias of the training process towards low-complexity models and, thus, for implicit regularization. We take a careful look at this explanation in the context of image classification with common deep neural network architectures. We find that if we do not regularize \emph{explicitly}, then SGD can be easily made to converge to poorly-generalizing, high-complexity models: all it takes is to first train on a random labeling on the data, before switching to properly training with the correct labels. In contrast, we find that in the presence of explicit regularization, pretraining with random labels has no detrimental effect on SGD. We believe that our results give evidence that explicit regularization plays a far more important role in the success of overparameterized neural networks than what has been understood until now. Specifically, by penalizing complicated models independently of their fit to the data, regularization affects training dynamics also far away from optima, making simple models that fit the data well discoverable by local methods, such as SGD.",0
"This paper discusses recent work in optimization theory which demonstrates that under certain conditions gradient descent can converge on bad local minima rather than good ones. We provide evidence from experiments with neural networks trained using SGD as well as mathematical analysis showing how certain architectures are more prone to ending up at these poor local optima. Our results have important implications for understanding how large language models learn and make predictions and suggest new directions for future research in developing training algorithms capable of escaping suboptimal solutions.  Bad Local Minima in Optimization ==============================  The problem of finding global minima is central to many areas of machine learning including deep learning, natural language processing, computer vision and robotics. In practice gradient based methods such as Stochastic Gradient Descent (SGD) are often used to find optimal parameters even though they may fail to escape poor local minima. Recent theoretical work has shown that given enough data some problems become computationally hard, making global optimality impossible to achieve within any fixed time frame [1]. Moreover, other papers have demonstrated that the landscape of objectives commonly encountered in deep learning exhibits ""sharp"" or narrow ridges along which first order methods cannot easily progress toward better objective values, leading them instead to become stuck in suboptimal points [2][3] . Even worse in some cases there exist ""bad""local minima against which no gradient flow whatsoever occurs [4][5]. These observations raise concerns over the quality of solutions found by current state-of-the art approaches to training deep models. They suggest that simply running existing approaches for longer may not necessarily lead to better generalization performance on held out test sets. Thus we need alternative ways of thinking abou",1
"The cultural integration of immigrants conditions their overall socio-economic integration as well as natives' attitudes towards globalisation in general and immigration in particular. At the same time, excessive integration -- or acculturation -- can be detrimental in that it implies forfeiting one's ties to the home country and eventually translates into a loss of diversity (from the viewpoint of host countries) and of global connections (from the viewpoint of both host and home countries). Cultural integration can be described using two dimensions: the preservation of links to the home country and culture, which we call home attachment, and the creation of new links together with the adoption of cultural traits from the new residence country, which we call destination attachment. In this paper we introduce a means to quantify these two aspects based on Twitter data. We build home and destination attachment indexes and analyse their possible determinants (e.g., language proximity, distance between countries), also in relation to Hofstede's cultural dimension scores. The results stress the importance of host language proficiency to explain destination attachment, but also the link between language and home attachment. In particular, the common language between home and destination countries corresponds to increased home attachment, as does low proficiency in the host language. Common geographical borders also seem to increase both home and destination attachment. Regarding cultural dimensions, larger differences among home and destination country in terms of Individualism, Masculinity and Uncertainty appear to correspond to larger destination attachment and lower home attachment.",0
"This paper examines how immigrants use social media as a tool for maintaining connections to their home culture while adapting to new cultures. Using data collected from Twitter, we analyze patterns of usage and language among users who identify themselves as immigrants, paying particular attention to tweets that express feelings of homesickness or attachment to both the user's country of origin and their current country of residence. We find evidence that these individuals frequently employ online platforms such as Twitter to engage with other members of their diaspora community, share information and experiences related to both cultures, and even bridge cultural divides by sharing knowledge and customs with non-immigrant followers. Our results suggest that online spaces can provide important opportunities for immigrants to negotiate their identities and integrate into host societies while maintaining ties to their home cultures.",1
"Recent studies of generalization in deep learning have observed a puzzling trend: accuracies of models on one data distribution are approximately linear functions of the accuracies on another distribution. We explain this trend under an intuitive assumption on model similarity, which was verified empirically in prior work. More precisely, we assume the probability that two models agree in their predictions is higher than what we can infer from their accuracy levels alone. Then, we show that a linear trend must occur when evaluating models on two distributions unless the size of the distribution shift is large. This work emphasizes the value of understanding model similarity, which can have an impact on the generalization and robustness of classification models.",0
"As machine learning models become increasingly prevalent in real-world applications, their performance under shifts in data distributions becomes more critical. Many studies have shown that models trained on one dataset can experience significant decreases in accuracy when tested on new data, even if only subtle differences exist between the two datasets. In this work, we investigate why certain classification algorithms tend to exhibit linear trendlines in their model performances as the training set diverges from the test set. Our findings indicate that these trends arise due to the nature of feature scaling used during both training and testing of the model. Specifically, our results suggest that certain normalization techniques may cause the algorithm to rely heavily on a single feature rather than considering the entire input space. Furthermore, we demonstrate how different choices of normalization methods can affect the stability and robustness of the model. These insights provide valuable guidance for practitioners developing models under distributional shifts, highlighting the importance of careful consideration of feature scaling approaches during the design process.",1
"Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions. The Shapley framework for explainability attributes a model's predictions to its input features in a mathematically principled and model-agnostic way. However, general implementations of Shapley explainability make an untenable assumption: that the model's features are uncorrelated. In this work, we demonstrate unambiguous drawbacks of this assumption and develop two solutions to Shapley explainability that respect the data manifold. One solution, based on generative modelling, provides flexible access to data imputations; the other directly learns the Shapley value-function, providing performance and stability at the cost of flexibility. While ""off-manifold"" Shapley values can (i) give rise to incorrect explanations, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data, on-manifold explainability overcomes these problems.",0
"In recent years, there has been growing interest in developing machine learning models that can provide explanations for their predictions and decisions. One popular approach to achieving this goal is through methods based on Shapley values from cooperative game theory. These methods assign credit to different features and model parameters for the outcome, providing insight into how different factors contribute to the prediction. However, these approaches often rely on linear models or approximations, which may not accurately capture the complex relationships between features and outcomes in nonlinear models.  In this paper, we propose a new method for explaining the predictions of neural networks using Shapley values computed directly on the data manifold. We show that this approach can provide accurate insights into how specific regions of feature space contribute to the model output, even for highly nonlinear models. Our results demonstrate that our method can effectively identify relevant features, and provides interpretable explanations that align well with human intuition. Furthermore, by leveraging ideas from differential geometry and computational topology, we also develop novel tools for visualizing and exploring these insights. Overall, our work represents an important step forward in understanding how deep learning models make predictions, and provides valuable techniques for promoting transparency and interpretability in artificial intelligence systems.",1
"Deep learning models suffer from opaqueness. For Convolutional Neural Networks (CNNs), current research strategies for explaining models focus on the target classes within the associated training dataset. As a result, the understanding of hidden feature map activations is limited by the discriminative knowledge gleaned during training. The aim of our work is to explain and expand CNNs models via the mirroring or alignment of CNN to an external knowledge base. This will allow us to give a semantic context or label for each visual feature. We can match CNN feature activations to nodes in our external knowledge base. This supports knowledge-based interpretation of the features associated with model decisions. To demonstrate our approach, we build two separate graphs. We use an entity alignment method to align the feature nodes in a CNN with the nodes in a ConceptNet based knowledge graph. We then measure the proximity of CNN graph nodes to semantically meaningful knowledge base nodes. Our results show that in the aligned embedding space, nodes from the knowledge graph are close to the CNN feature nodes that have similar meanings, indicating that nodes from an external knowledge base can act as explanatory semantic references for features in the model. We analyse a variety of graph building methods in order to improve the results from our embedding space. We further demonstrate that by using hierarchical relationships from our external knowledge base, we can locate new unseen classes outside the CNN training set in our embeddings space, based on visual feature activations. This suggests that we can adapt our approach to identify unseen classes based on CNN feature activations. Our demonstrated approach of aligning a CNN with an external knowledge base paves the way to reason about and beyond the trained model, with future adaptations to explainable models and zero-shot learning.",0
"""Convolutional neural networks (CNNs) have proven to be highly effective in many visual recognition tasks. However, their performance can often be limited by their lack of external knowledge and contextual understanding. In our work, we propose a novel approach to enhancing CNNs by aligning them with external knowledge bases such as WordNet and WikiData. Our method leverages these resources to provide semantic annotations that guide the learning process and improve generalization capabilities. We evaluate our approach on several benchmark datasets and demonstrate significant improvements over baseline models, particularly in cases where prior knowledge plays a crucial role. Our results highlight the potential benefits of incorporating external knowledge into deep learning frameworks, paving the way for more advanced artificial intelligence systems.""",1
"The Shapley value has become popular in the Explainable AI (XAI) literature, thanks, to a large extent, to a solid theoretical foundation, including four ""favourable and fair"" axioms for attribution in transferable utility games. The Shapley value is provably the only solution concept satisfying these axioms. In this paper, we introduce the Shapley value and draw attention to its recent uses as a feature selection tool. We call into question this use of the Shapley value, using simple, abstract ""toy"" counterexamples to illustrate that the axioms may work against the goals of feature selection. From this, we develop a number of insights that are then investigated in concrete simulation settings, with a variety of Shapley value formulations, including SHapley Additive exPlanations (SHAP) and Shapley Additive Global importancE (SAGE).",0
"This paper presents an analysis of the use of Shapley values in the field of machine learning for feature selection purposes. The authors discuss both the advantages and disadvantages of using Shapley values as a method for selecting features in data sets, as well as their theoretical properties that make them a popular choice among researchers. In particular, the authors highlight the fact that while Shapley values provide a unique perspective on how different variables contribute to the overall model fit, they can suffer from high computational complexity and are often difficult to interpret due to their nonlinear nature. Additionally, they examine some common axiomatic approaches used to simplify the computation of these measures and discuss their implications. Ultimately, the authors conclude that while Shapley values offer valuable insights into feature importance, further work must be done to improve their scalability and interpretability before they can become more widely adopted in practice. Overall, this paper provides a comprehensive overview of Shapley values for feature selection and serves as a reference for future research in the area.",1
"Overall survival (OS) time prediction is one of the most common estimates of the prognosis of gliomas and is used to design an appropriate treatment planning. State-of-the-art (SOTA) methods for OS time prediction follow a pre-hoc approach that require computing the segmentation map of the glioma tumor sub-regions (necrotic, edema tumor, enhancing tumor) for estimating OS time. However, the training of the segmentation methods require ground truth segmentation labels which are tedious and expensive to obtain. Given that most of the large-scale data sets available from hospitals are unlikely to contain such precise segmentation, those SOTA methods have limited applicability. In this paper, we introduce a new post-hoc method for OS time prediction that does not require segmentation map annotation for training. Our model uses medical image and patient demographics (represented by age) as inputs to estimate the OS time and to estimate a saliency map that localizes the tumor as a way to explain the OS time prediction in a post-hoc manner. It is worth emphasizing that although our model can localize tumors, it uses only the ground truth OS time as training signal, i.e., no segmentation labels are needed. We evaluate our post-hoc method on the Multimodal Brain Tumor Segmentation Challenge (BraTS) 2019 data set and show that it achieves competitive results compared to pre-hoc methods with the advantage of not requiring segmentation labels for training.",0
"An analysis was performed on brain MRIs to determine if post-operative survival could be predicted using quantitative image features extracted from preoperative scans. A dataset containing MRI data along with patient follow-up time was used to train deep learning models capable of accurately predicting overall survival times. Results show that our proposed method achieved superior performance compared to traditional approaches, thus highlighting the potential clinical utility of post-hoc prediction using machine learning algorithms trained on routine radiological images. Our work demonstrates the feasibility of developing powerful predictive tools to inform clinicians regarding the likelihood of improved outcomes following surgery based solely on preoperative imaging. However, further research is required before these methods can be fully integrated into the standard of care due to their limited generalizability across different cohorts. Additionally, ethical considerations must be taken into account given the sensitive nature of mortality predictions and how they may impact patients and their families as well as physiciansâ€™ decision making process.",1
"Real world data often exhibit low-dimensional geometric structures, and can be viewed as samples near a low-dimensional manifold. This paper studies nonparametric regression of H\""{o}lder functions on low-dimensional manifolds using deep ReLU networks. Suppose $n$ training data are sampled from a H\""{o}lder function in $\mathcal{H}^{s,\alpha}$ supported on a $d$-dimensional Riemannian manifold isometrically embedded in $\mathbb{R}^D$, with sub-gaussian noise. A deep ReLU network architecture is designed to estimate the underlying function from the training data. The mean squared error of the empirical estimator is proved to converge in the order of $n^{-\frac{2(s+\alpha)}{2(s+\alpha) + d}}\log^3 n$. This result shows that deep ReLU networks give rise to a fast convergence rate depending on the data intrinsic dimension $d$, which is usually much smaller than the ambient dimension $D$. It therefore demonstrates the adaptivity of deep ReLU networks to low-dimensional geometric structures of data, and partially explains the power of deep ReLU networks in tackling high-dimensional data with low-dimensional geometric structures.",0
"In recent years, nonlinear regression has emerged as a popular technique for modeling complex relationships between input variables and response variables. Traditional approaches to nonlinear regression typically rely on assumptions such as smoothness, continuity, and differentiability that restrict their applicability. This paper proposes a novel approach to nonparametric regression based on deep rectified linear unit (ReLU) networks. We show that these networks can effectively capture low-dimensional manifolds underlying many real-world data sets without making any smoothness assumptions. Our method offers advantages over traditional techniques by being capable of handling noisy data, high-dimensional inputs, and more complex geometries while maintaining interpretability. Moreover, we establish theoretical guarantees demonstrating that our approach achieves both functional approximation error bounds and statistical recovery rates approaching those achievable with infinite samples. These contributions offer opportunities in fields such as computer vision, natural language processing, and neuroscience where data often exhibit intricate dependencies not easily described through traditional models.",1
"Online imitation learning (IL) is an algorithmic framework that leverages interactions with expert policies for efficient policy optimization. Here policies are optimized by performing online learning on a sequence of loss functions that encourage the learner to mimic expert actions, and if the online learning has no regret, the agent can provably learn an expert-like policy. Online IL has demonstrated empirical successes in many applications and interestingly, its policy improvement speed observed in practice is usually much faster than existing theory suggests. In this work, we provide an explanation of this phenomenon. Let $\xi$ denote the policy class bias and assume the online IL loss functions are convex, smooth, and non-negative. We prove that, after $N$ rounds of online IL with stochastic feedback, the policy improves in $\tilde{O}(1/N + \sqrt{\xi/N})$ in both expectation and high probability. In other words, we show that adopting a sufficiently expressive policy class in online IL has two benefits: both the policy improvement speed increases and the performance bias decreases.",0
"""Explaining fast improvement in online imitation learning"" is a technical paper that investigates how quickly artificial intelligence systems can learn from human demonstrations through a process called imitation learning. The authors focus on explaining why these improvements occur so rapidly by examining factors such as agent adaptation and task complexity. By doing so, they hope to provide insights into how future AI algorithms can be designed to better incorporate human feedback and improve their performance more efficiently. Overall, this paper offers valuable research contributions in the field of artificial intelligence and could have important implications for the development of autonomous systems capable of performing tasks alongside humans.",1
"Symbolic regression corresponds to an ensemble of techniques that allow to uncover an analytical equation from data. Through a closed form formula, these techniques provide great advantages such as potential scientific discovery of new laws, as well as explainability, feature engineering as well as fast inference. Similarly, deep learning based techniques has shown an extraordinary ability of modeling complex patterns. The present paper aims at applying a recent end-to-end symbolic regression technique, i.e. the equation learner (EQL), to get an analytical equation for wind speed forecasting. We show that it is possible to derive an analytical equation that can achieve reasonable accuracy for short term horizons predictions only using few number of features.",0
This sounds like a great project! Would you mind telling me more? I would love to see your research notes on symbolic regression applied to wind speed forecasting. How did you come up with the idea? What kind of data have you collected so far? Are there any initial results that look promising? Can you tell me more about your specific methodology and how it differs from other approaches out there? And what are some possible future applications of your work beyond wind speeds? Thanks for sharing your thoughts!,1
"Our global population contributes visual content on platforms like Instagram, attempting to express themselves and engage their audiences, at an unprecedented and increasing rate. In this paper, we revisit the popularity prediction on Instagram. We present a robust, efficient, and explainable baseline for population-based popularity prediction, achieving strong ranking performance. We employ the latest methods in computer vision to maximize the information extracted from the visual modality. We use transfer learning to extract visual semantics such as concepts, scenes, and objects, allowing a new level of scrutiny in an extensive, explainable ablation study. We inform feature selection towards a robust and scalable model, but also illustrate feature interactions, offering new directions for further inquiry in computational social science. Our strongest models inform a lower limit to population-based predictability of popularity on Instagram. The models are immediately applicable to social media monitoring and influencer identification.",0
"Here is an abstract:  Predicting popularity on social media platforms such as Instagram is becoming increasingly important due to the impact that viral content can have on brand awareness and advertising revenue. However, existing approaches to predict popularity often rely on simple features or struggle to generalize across different modalities. In this work, we introduce a new approach that addresses these limitations by leveraging multiple modalities while remaining robust, efficient, and explainable. Our method uses state-of-the-art deep learning techniques and achieves strong results compared to previous methods. We present comprehensive experiments demonstrating our model's effectiveness on several benchmark datasets, and show how it outperforms baselines significantly. Additionally, we provide insight into which factors contribute most strongly to popularity predictions, allowing practitioners to better understand the behavior of their audiences. This research represents an important step forward towards developing more accurate models for multi-modal popularity prediction on Instagram and other online platforms.",1
"Adversarial machine learning has attracted a great amount of attention in recent years. In a poisoning attack, the adversary can inject a small number of specially crafted samples into the training data which make the decision boundary severely deviate and cause unexpected misclassification. Due to the great importance and popular use of support vector machines (SVM), we consider defending SVM against poisoning attacks in this paper. We study two commonly used strategies for defending: designing robust SVM algorithms and data sanitization. Though several robust SVM algorithms have been proposed before, most of them either are in lack of adversarial-resilience, or rely on strong assumptions about the data distribution or the attacker's behavior. Moreover, the research on their complexities is still quite limited. We are the first, to the best of our knowledge, to prove that even the simplest hard-margin one-class SVM with outliers problem is NP-complete, and has no fully PTAS unless P$=$NP (that means it is hard to achieve an even approximate algorithm). For the data sanitization defense, we link it to the intrinsic dimensionality of data; in particular, we provide a sampling theorem in doubling metrics for explaining the effectiveness of DBSCAN (as a density-based outlier removal method) for defending against poisoning attacks. In our empirical experiments, we compare several defenses including the DBSCAN and robust SVM methods, and investigate the influences from the intrinsic dimensionality and data density to their performances.",0
"In this paper we present two novel techniques aimed at mitigating poisoning attacks on Support Vector Machines (SVM). Firstly, we introduce a hardness-based defense mechanism that makes use of a recently proposed technique called adversarial robustness scores (ARS) to determine how resistant an SVM model is to poisoning attacks. This allows us to identify models that are more resilient to such attacks and select them over potentially vulnerable ones. Secondly, we propose utilizing the DBSCAN clustering algorithm as part of our poisoning detection strategy. By using DBSCAN to cluster training data points and monitor changes in these clusters over time, we can detect unusual behavior indicative of poisoning attempts. Our experimental results demonstrate the effectiveness of both approaches, significantly reducing the success rate of attackers while maintaining high classification accuracy under normal operating conditions. These contributions highlight the importance of developing defenses against malicious manipulation of machine learning models, particularly those used for critical applications where security cannot be compromised.",1
"Bandit and reinforcement learning (RL) problems can often be framed as optimization problems where the goal is to maximize average performance while having access only to stochastic estimates of the true gradient. Traditionally, stochastic optimization theory predicts that learning dynamics are governed by the curvature of the loss function and the noise of the gradient estimates. In this paper we demonstrate that this is not the case for bandit and RL problems. To allow our analysis to be interpreted in light of multi-step MDPs, we focus on techniques derived from stochastic optimization principles (e.g., natural policy gradient and EXP3) and we show that some standard assumptions from optimization theory are violated in these problems. We present theoretical results showing that, at least for bandit problems, curvature and noise are not sufficient to explain the learning dynamics and that seemingly innocuous choices like the baseline can determine whether an algorithm converges. These theoretical findings match our empirical evaluation, which we extend to multi-state MDPs.",0
"This new work presents an empirical study into the relationship between variances and policy optimisation performance across different algorithmic settings. Our findings suggest that reducing variation can improve learning speed and stability for some policies, but only up to a certain point beyond which diminishing returns emerge. However, we found little overall evidence linking optimality or final performance outcomes directly to low variances themselves. Instead, our key conclusion is that baseline choice may matter more than variance reduction due to alternative statistical effects they induce. To test these ideas experimentally, we ran multiple versions of three popular algorithms over four domains covering discrete and continuous action spaces, making our findings widely applicable within RL research and practice. We hope this work inspires further thinking about how baselines should be designed based on their impacts upon learned policy quality rather than just being viewed as â€œa trick to make RL fasterâ€; it also raises questions relating to model selection criteria and statistical assumptions commonly used within deep neural network-based RL models more generally. This study aimed to investigate the impact of baselines and variance reduction on policy optimisation performance. The authors conducted an experimental analysis by using multiple variations of three popular algorithms across four different domains, including both discrete and continuous action spaces. The results showed that while reducing variance could enhance learning speed and stability for specific policies, there was limited correlation between minimised variance and optimal or final performance. Furthermore, the study revealed that baseline choices have greater influence on outcome than variance reductions alone, due to altering distinct statistical properties. These insights challenge common practices within reinforcement learning (RL) research and raise concerns regarding model selections and statistical assumptions made during training deep neural networks for RL problems. Ultimately, the findings emphasise that careful consideration should be given towards choosing appropriate baselines, recognising their significance for influencing learned policy effectiveness instead of solely focusing on accelerating RL convergence speeds through variance mitigation techniques",1
"Recent years, many researches attempt to open the black box of deep neural networks and propose a various of theories to understand it. Among them, Information Bottleneck (IB) theory claims that there are two distinct phases consisting of fitting phase and compression phase in the course of training. This statement attracts many attentions since its success in explaining the inner behavior of feedforward neural networks. In this paper, we employ IB theory to understand the dynamic behavior of convolutional neural networks (CNNs) and investigate how the fundamental features such as convolutional layer width, kernel size, network depth, pooling layers and multi-fully connected layer have impact on the performance of CNNs. In particular, through a series of experimental analysis on benchmark of MNIST and Fashion-MNIST, we demonstrate that the compression phase is not observed in all these cases. This shows us the CNNs have a rather complicated behavior than feedforward neural networks.",0
"In recent years, researchers have developed a new theory called ""Information Bottleneck"" that provides insights into how convolutional neural networks (CNN) process images. The traditional CNN architecture has been optimized to extract relevant features from raw pixel data while compressing redundant information. However, little attention has been given to understanding how these features are represented in the network layers. The Information Bottleneck theory addresses this gap by introducing the concept of an information bottleneck that restricts the flow of high-dimensional representations through deep architectures. This enables efficient encoding of relevant information into low-dimensional latent representations. The authors tested their theories using two widely used benchmark datasets: CIFAR-10 and ImageNet, and achieved state-of-the art results. Their findings demonstrate that training models with a tightly constrained information bottleneck consistently improve performance across multiple metrics compared to those trained without this constraint.",1
"We prove an exponential separation for the sample complexity between the standard PAC-learning model and a version of the Equivalence-Query-learning model. We then show that this separation has interesting implications for adversarial robustness. We explore a vision of designing an adaptive defense that in the presence of an attacker computes a model that is provably robust. In particular, we show how to realize this vision in a simplified setting.   In order to do so, we introduce a notion of a strong adversary: he is not limited by the type of perturbations he can apply but when presented with a classifier can repetitively generate different adversarial examples. We explain why this notion is interesting to study and use it to prove the following. There exists an efficient adversarial-learning-like scheme such that for every strong adversary $\mathbf{A}$ it outputs a classifier that (a) cannot be strongly attacked by $\mathbf{A}$, or (b) has error at most $\epsilon$. In both cases our scheme uses exponentially (in $\epsilon$) fewer samples than what the PAC bound requires.",0
"This paper discusses adversarial robustness of machine learning models, focusing on deep neural networks (DNNs) as they have achieved state-of-the-art performance across many domains. We survey popular attacks that generate input perturbations to fool DNNs and present corresponding defenses designed to improve their resilience against these attacks. Despite advances made by attackers, we argue that there exists potential for new, stealthier adversaries yet undiscovered and hence opportunities for more advanced defense methods. With our analysis, readers can better grasp current progress towards robust artificial intelligence and further strengthen the security and reliability of AI systems. Finally, we outline promising research directions inspired from nature's own mechanisms that could lead us closer toward developing fully trustworthy machine intelligence that would make our lives easier without putting them at risk. In conclusion, while achieving full robustness remains elusive today, continued efforts must prioritize addressing vulnerabilities exposed so far before considering other challenges faced by AI.",1
"Latest insights from biology show that intelligence does not only emerge from the connections between the neurons, but that individual neurons shoulder more computational responsibility. Current Neural Network architecture design and search are biased on fixed activation functions. Using more advanced learnable activation functions provide Neural Networks with higher learning capacity. However, general guidance for building such networks is still missing. In this work, we first explain why rationals offer an optimal choice for activation functions. We then show that they are closed under residual connections, and inspired by recurrence for residual networks we derive a self-regularized version of Rationals: Recurrent Rationals. We demonstrate that (Recurrent) Rational Networks lead to high performance improvements on Image Classification and Deep Reinforcement Learning.",0
"Rational behavior can emerge from neural networks through training that maximizes expected utility of rewards. Training methods such as REINFORCE have difficulties in scaling up since it requires experience replay buffers and may converge slowly due to high variance gradient estimates. In this study we propose Recurrent Rational Network (R2N), which models both future values and uncertainties to approximate expected utilities without sampling experience transitions. We show using two Atari games that our approach significantly improves sample efficiency while reducing computation overhead comparing to REINFORCE baselines. Additionally, trained agents achieve scores close to human expert performances on most tasks indicating strong generalization capability of R2N. This work provides evidence that recurrence plays important roles in guiding deep reinforcement learning towards rational behaviors. Implications extend to natural language processing where understanding uncertainty would also be crucial.",1
"A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behavior of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans' abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.",0
"Deep learning has revolutionized the field of artificial intelligence by enabling machines to learn complex representations from raw data without explicit programming. However, most deep learning models still lack higher-level cognitive abilities such as common sense reasoning, planning, and generalization across domains. In this work, we explore how inductive biases can be used to guide deep neural networks towards learning these more advanced cognitive capabilities. We propose two novel architectural designs: (1) graph attention networks that explicitly model relationships between objects and events in a scene and (2) relational memory networks that capture structured representations of temporal dependencies between sequential observations. Through extensive experiments on challenging benchmark tasks, we demonstrate that our approaches significantly outperform strong baselines and achieve state-of-the-art results in several domains. Our findings suggest that incorporating deliberately chosen inductive biases into deep learning models is key to achieving human-like understanding and problem solving ability in AI systems. This research represents a step forward in developing intelligent agents capable of generalizing their knowledge across diverse environments.",1
"Energy-Based Models (EBMs), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, EBMs do not place a restriction on the tractability of the normalizing constant, thus are more flexible to parameterize and can model a more expressive family of probability distributions. However, the unknown normalizing constant of EBMs makes training particularly difficult. Our goal is to provide a friendly introduction to modern approaches for EBM training. We start by explaining maximum likelihood training with Markov chain Monte Carlo (MCMC), and proceed to elaborate on MCMC-free approaches, including Score Matching (SM) and Noise Constrastive Estimation (NCE). We highlight theoretical connections among these three approaches, and end with a brief survey on alternative training methods, which are still under active research. Our tutorial is targeted at an audience with basic understanding of generative models who want to apply EBMs or start a research project in this direction.",0
"Recent advances in deep learning have produced incredible results across many domains such as computer vision and natural language processing. At the heart of these successes lie neural network models that learn from large amounts of data by optimizing an energy function. While energy-based models (EBMs) have been used before in areas like machine learning and physics, they have recently gained renewed interest due to their ability to capture complex relationships between inputs and outputs without relying on explicit probabilistic formulations. In this work, we present a framework for training EBMs based on Markov Chain Monte Carlo sampling. Our approach enables us to optimize energy functions that cannot be easily differentiated and can handle both discrete and continuous variables. We showcase our method on several challenging problems including image generation from textual descriptions and few-shot classification tasks. Results demonstrate significant improvements over existing methods while providing insights into the behavior of learned distributions. This work marks an important step towards harnessing the power of energy-based models in deep learning applications.",1
"We propose a data-driven scene flow estimation algorithm exploiting the observation that many 3D scenes can be explained by a collection of agents moving as rigid bodies. At the core of our method lies a deep architecture able to reason at the \textbf{object-level} by considering 3D scene flow in conjunction with other 3D tasks. This object level abstraction, enables us to relax the requirement for dense scene flow supervision with simpler binary background segmentation mask and ego-motion annotations. Our mild supervision requirements make our method well suited for recently released massive data collections for autonomous driving, which do not contain dense scene flow annotations. As output, our model provides low-level cues like pointwise flow and higher-level cues such as holistic scene understanding at the level of rigid objects. We further propose a test-time optimization refining the predicted rigid scene flow. We showcase the effectiveness and generalization capacity of our method on four different autonomous driving datasets. We release our source code and pre-trained models under \url{github.com/zgojcic/Rigid3DSceneFlow}.",0
"This would make the abstract more difficult for potential readers who may only have a vague recollection of their search terms to quickly identify whether your work matches their interests.  We present Weakly Supervised Learning of Rigid 3D Scene Flow, a method that uses weak supervision from monocular videos to estimate rigid scene flow. Our method estimates scene flow using only images as input, without any depth maps, segmentation masks, or other annotations. By leveraging video statistics and assuming local planarity, we learn a linear regression model capable of accurately estimating flow. Experiments on several datasets show our method outperforms state-of-the-art unsupervised and self-supervised methods while achieving performance competitive with strongly supervised techniques. As the first successful application of weak supervision to this problem, we hope WSLR3DSF will encourage further research into effective use of minimal annotations.",1
"Graph Neural Networks are perfectly suited to capture latent interactions between various entities in the spatio-temporal domain (e.g. videos). However, when an explicit structure is not available, it is not obvious what atomic elements should be represented as nodes. Current works generally use pre-trained object detectors or fixed, predefined regions to extract graph nodes. In turn, our proposed model learns nodes that dynamically attach to salient space-time regions, which are relevant for a higher-level task, without using any object-level supervision. Constructing these localised, adaptive nodes gives our model inductive bias towards object-centric representations and we show that it discovers regions that are well correlated with objects in the video. The localised nodes are the key components of the method and visualising their regions leads to a more explainable model. In extensive ablation studies and experiments on two challenging datasets we show superior performance to previous graph neural networks models for video classification.",0
"In recent years there has been great interest in finding new ways to identify salient regions within video frames using deep learning techniques. These methods have proven to be effective at extracting key features from complex scenes, but they often struggle with temporal dynamics and changes over time. This paper proposes a novel approach that combines graph neural networks (GNN) with spatio-temporal reasoning to effectively capture dynamic salient regions across multiple frames. We begin by defining saliency as a node attribute on a graph where each node represents a pixel location in space and time. We then introduce a GNN architecture that takes into account both spatial and temporal relationships among pixels. Our model can learn powerful representations by jointly considering local contextual features with global temporal dependencies. Experiments demonstrate state-of-the-art performance compared to other leading methods, validating our approach. Overall, we believe this work opens up exciting opportunities for exploring more advanced models that unify different computer vision tasks involving spatio-temporal data such as action recognition, object tracking, and video summarization.",1
"Since their inception, learning techniques under the Reservoir Computing paradigm have shown a great modeling capability for recurrent systems without the computing overheads required for other approaches. Among them, different flavors of echo state networks have attracted many stares through time, mainly due to the simplicity and computational efficiency of their learning algorithm. However, these advantages do not compensate for the fact that echo state networks remain as black-box models whose decisions cannot be easily explained to the general audience. This work addresses this issue by conducting an explainability study of Echo State Networks when applied to learning tasks with time series, image and video data. Specifically, the study proposes three different techniques capable of eliciting understandable information about the knowledge grasped by these recurrent models, namely, potential memory, temporal patterns and pixel absence effect. Potential memory addresses questions related to the effect of the reservoir size in the capability of the model to store temporal information, whereas temporal patterns unveils the recurrent relationships captured by the model over time. Finally, pixel absence effect attempts at evaluating the effect of the absence of a given pixel when the echo state network model is used for image and video classification. We showcase the benefits of our proposed suite of techniques over three different domains of applicability: time series modeling, image and, for the first time in the related literature, video classification. Our results reveal that the proposed techniques not only allow for a informed understanding of the way these models work, but also serve as diagnostic tools capable of detecting issues inherited from data (e.g. presence of hidden bias).",0
"This paper presents research on improving the explainability of deep echo state networks (ESN) through post-hoc analysis techniques for time series forecasting, image classification, and video classification tasks. We investigate how post-hout explanations can reveal insights into ESN models that would otherwise remain hidden. Our approach focuses on three main aspects: Firstly, we analyze how different configurations of ESN architectures impact model performance and interpretability. Secondly, we explore the use of attribution methods to provide insight into which features contribute most significantly to predictions made by the network. Lastly, we evaluate how well our proposed methodologies generalize across different domains and datasets. Overall, our work demonstrates the potential of ESNs as interpretable alternatives to traditional deep learning models while still maintaining high levels of accuracy.",1
"Schizophrenia is a complex psychiatric disorder involving changes in thought patterns, perception, mood, and behavior. The diagnosis of schizophrenia is challenging and requires that patients show two or more positive symptoms for at least one month. Delays in identifying this debilitating disorder can impede a patient ability to receive much needed treatment. Advances in neuroimaging and machine learning algorithms can facilitate the diagnosis of schizophrenia and help clinicians to provide an accurate diagnosis of the disease. This paper presents a methodology for analyzing spectral images of Electroencephalography collected from patients with schizophrenia using convolutional neural networks. It also explains how we have developed accurate classifiers employing Model-Agnostic Meta-Learning and prototypical networks. Such classifiers have the capacity to distinguish people with schizophrenia from healthy controls based on their brain activity.",0
"Here is a possible abstract:  Advances in machine learning have made it increasingly feasible to analyze complex neural signals such as electroencephalograms (EEGs) from individuals with psychiatric disorders like schizophrenia. In particular, spectral analysis of EEG data has been shown to yield insights into cognitive processing deficits associated with this illness. Despite these advancements, there remains a need for further development of methods that can effectively capture relevant features from EEG recordings and generalize across different datasets. To address this gap, we propose a novel meta-learning framework for modeling spectral EEG images of schizophrenics. Our approach leverages task-specific knowledge distillation and few-shot learning techniques to enable rapid adaptation of deep convolutional networks across diverse EEG datasets. Experimental results demonstrate that our method outperforms state-of-the-art baselines in terms of classification accuracy while providing meaningful interpretability through attention maps highlighting salient brain regions underlying the predictions. These findings hold promise for improving diagnostic precision and informing targeted interventions in mental health treatment.",1
"Predictions are the currency of a machine learning model, and to understand the model's behavior over segments of a dataset, or over time, is an important problem in machine learning research and practice. There currently is no systematic framework to understand this drift in prediction distributions over time or between two semantically meaningful slices of data, in terms of the input features and points. We propose GroupShapley and GroupIG (Integrated Gradients), as axiomatically justified methods to tackle this problem. In doing so, we re-frame all current feature/data importance measures based on the Shapley value as essentially problems of distributional comparisons, and unify them under a common umbrella. We axiomatize certain desirable properties of distributional difference, and study the implications of choosing them empirically.",0
"This paper provides a unified framework to explain prediction drift, which refers to changes over time in predictions made by machine learning models for previously seen examples. We begin by discussing different types of prediction drift and their potential causes. Next, we introduce the Shapley framework, which allows us to decompose the prediction error into four components: bias, variance, noise and interaction. By applying this framework to explain prediction drift, we can gain insights into why certain aspects of the model may have changed over time. Furthermore, our results indicate that incorporating domain knowledge about prediction drift improves model interpretability and fairness. Finally, we demonstrate how to use these findings in practice through case studies on real datasets from domains such as credit risk assessment and medical diagnosis. In summary, this paper presents a comprehensive approach to understanding and addressing prediction drift, leading to improved decision making processes across a variety of applications.",1
"Explainable AI (XAI) is an active research area to interpret a neural network's decision by ensuring transparency and trust in the task-specified learned models. Recently, perturbation-based model analysis has shown better interpretation, but backpropagation techniques are still prevailing because of their computational efficiency. In this work, we combine both approaches as a hybrid visual explanation algorithm and propose an efficient interpretation method for convolutional neural networks. Our method adaptively selects the most critical features that mainly contribute towards a prediction to probe the model by finding the activated features. Experimental results show that the proposed method can reduce the execution time up to 30% while enhancing competitive interpretability without compromising the quality of explanation generated.",0
"This paper proposes a new method for efficiently explaining the decisions made by convolutional neural networks (CNNs) called Ada-SISE, which stands for adaptive semantic input sampling for efficient explanation. CNNs are powerful models that can make accurate predictions on complex tasks, but understanding how they arrive at their decisions remains a challenge. Current methods for interpreting CNNs rely heavily on human expertise and time-consuming trial-and-error. Ada-SISE addresses these limitations by using data augmentation techniques and a statistical model to generate informative inputs that highlight relevant features and reduce noise. With the goal of creating explanations that are accurate, concise, and intuitive, our approach leverages the natural structure of datasets to identify key patterns and focus on salient features without sacrificing performance. Our experiments demonstrate the effectiveness and efficiency of Ada-SISE across different domains and dataset sizes compared to state-of-the art explainability methods. We believe that Ada-SISE could become an integral part of future research into explainable artificial intelligence as well as serve as a tool for practitioners seeking more insightful interpretations from their deep learning models.",1
"Many real-world applications involve data from multiple modalities and thus exhibit the view heterogeneity. For example, user modeling on social media might leverage both the topology of the underlying social network and the content of the users' posts; in the medical domain, multiple views could be X-ray images taken at different poses. To date, various techniques have been proposed to achieve promising results, such as canonical correlation analysis based methods, etc. In the meanwhile, it is critical for decision-makers to be able to understand the prediction results from these methods. For example, given the diagnostic result that a model provided based on the X-ray images of a patient at different poses, the doctor needs to know why the model made such a prediction. However, state-of-the-art techniques usually suffer from the inability to utilize the complementary information of each view and to explain the predictions in an interpretable manner.   To address these issues, in this paper, we propose a deep co-attention network for multi-view subspace learning, which aims to extract both the common information and the complementary information in an adversarial setting and provide robust interpretations behind the prediction to the end-users via the co-attention mechanism. In particular, it uses a novel cross reconstruction loss and leverages the label information to guide the construction of the latent representation by incorporating the classifier into our model. This improves the quality of latent representation and accelerates the convergence speed. Finally, we develop an efficient iterative algorithm to find the optimal encoders and discriminator, which are evaluated extensively on synthetic and real-world data sets. We also conduct a case study to demonstrate how the proposed method robustly interprets the predictions on an image data set.",0
"This paper presents a deep co-attention network (DCA) for multi-view subspace learning. DCA is designed to learn the joint representation of multiple views from the same object by taking into account both view-specific features as well as interdependencies among different views. We demonstrate that our approach can effectively capture complementary information across different representations without requiring explicit fusion operations. Our results on several benchmark datasets show that the proposed method outperforms other state-of-the-art methods in terms of accuracy and efficiency while reducing computational cost. In addition, we provide ablation studies to evaluate the effectiveness of each component in our model. Overall, our work provides a new perspective on multi-view feature learning and highlights the potential benefits of using attention mechanisms in this domain.",1
"In this paper, we consider the framework of multi-task representation (MTR) learning where the goal is to use source tasks to learn a representation that reduces the sample complexity of solving a target task. We start by reviewing recent advances in MTR theory and show that they can provide novel insights for popular meta-learning algorithms when analyzed within this framework. In particular, we highlight a fundamental difference between gradient-based and metric-based algorithms and put forward a theoretical analysis to explain it. Finally, we use the derived insights to improve the generalization capacity of meta-learning methods via a new spectral-based regularization term and confirm its efficiency through experimental studies on classic few-shot classification and continual learning benchmarks. To the best of our knowledge, this is the first contribution that puts the most recent learning bounds of MTR theory into practice of training popular meta-learning methods.",0
"In recent years, meta-learning has emerged as a promising approach towards enabling machine learning models to learn from a small amount of data while achieving comparable results to those trained on large amounts of data. This study aims to provide new insights into meta-learning methods by exploring their relationship to multi-task representation learning theory. We present theoretical analysis that shows how several popular meta-learning algorithms can be cast under the umbrella of multi-task representation learning problems, which arise naturally from meta-learning objectives such as knowledge distillation and few-shot learning. Our work provides a deeper understanding of these connections and helps explain why some meta-learning algorithms perform better than others, setting the stage for future research directions. By establishing a rigorous framework for analyzing and comparing existing meta-learning algorithms, our findings have important implications for the development of more effective approaches in artificial intelligence.",1
"The original design of Graph Convolution Network (GCN) couples feature transformation and neighborhood aggregation for node representation learning. Recently, some work shows that coupling is inferior to decoupling, which supports deep graph propagation better and has become the latest paradigm of GCN (e.g., APPNP and SGCN). Despite effectiveness, the working mechanisms of the decoupled GCN are not well understood. In this paper, we explore the decoupled GCN for semi-supervised node classification from a novel and fundamental perspective -- label propagation. We conduct thorough theoretical analyses, proving that the decoupled GCN is essentially the same as the two-step label propagation: first, propagating the known labels along the graph to generate pseudo-labels for the unlabeled nodes, and second, training normal neural network classifiers on the augmented pseudo-labeled data. More interestingly, we reveal the effectiveness of decoupled GCN: going beyond the conventional label propagation, it could automatically assign structure- and model- aware weights to the pseudo-label data. This explains why the decoupled GCN is relatively robust to the structure noise and over-smoothing, but sensitive to the label noise and model initialization. Based on this insight, we propose a new label propagation method named Propagation then Training Adaptively (PTA), which overcomes the flaws of the decoupled GCN with a dynamic and adaptive weighting strategy. Our PTA is simple yet more effective and robust than decoupled GCN. We empirically validate our findings on four benchmark datasets, demonstrating the advantages of our method. The code is available at https://github.com/DongHande/PT_propagation_then_training.",0
"Title: Comparing Decoupled Graph Convolution Networks (GConvNets) with Label Propagation on Image Classification Tasks  Graph convolution networks (GConvNets) have emerged as powerful tools for handling graph data due to their ability to capture and propagate signals through graphs. However, while GConvNets outperform traditional approaches like label propagation on several benchmark datasets, there is still debate over whether they fundamentally differ from more naive techniques like label propagation. In this work, we aim to better understand the relationship between these two methods by exploring the properties that make them equivalent under specific assumptions. We first provide theoretical evidence that, if the similarity matrix is used in the construction of both neighborhood systems, the weighted averages obtained via GConvNets and label propagation become equivalent. Then, we demonstrate empirically using experiments on MNIST and Cora citation dataset that our theory holds true, even when accounting for differences in implementation details across libraries. Our findings highlight how GConvNets can offer significant performance gains without necessarily introducing new architectural elements beyond those already present in simpler models such as label propagation. This understanding provides insights into the mechanisms behind the success of GConvNets on image classification tasks and paves the way for further refinements and improvements in graph representation learning.",1
"Attribution methods calculate attributions that visually explain the predictions of deep neural networks (DNNs) by highlighting important parts of the input features. In particular, gradient-based attribution (GBA) methods are widely used because they can be easily implemented through automatic differentiation. In this study, we use the attributions that filter out irrelevant parts of the input features and then verify the effectiveness of this approach by measuring the classification accuracy of a pre-trained DNN. This is achieved by calculating and applying an \textit{attribution mask} to the input features and subsequently introducing the masked features to the DNN, for which the mask is designed to recursively focus attention on the parts of the input related to the target label. The accuracy is enhanced under a certain condition, i.e., \textit{no implicit bias}, which can be derived based on our theoretical insight into compressing the DNN into a single-layer neural network. We also provide Gradient\,*\,Sign-of-Input (GxSI) to obtain the attribution mask that further improves the accuracy. As an example, on CIFAR-10 that is modified using the attribution mask obtained from GxSI, we achieve the accuracy ranging from 99.8\% to 99.9\% without additional training.",0
"Deep neural networks (DNNs) have achieved impressive performance in various tasks, but their high capacity models often suffer from the problem of overfitting, i.e., memorizing training data instead of learning generalizable representations that can capture essential features. In order to address this issue, we introduce a new method called Attribution Mask, which filters out irrelevant input features by recursively focusing attention on inputs of DNNs during both training and inference stages.  Our approach works by iteratively updating a binary mask at each layer, where the mask values reflect how important each feature map is to predict target outputs, based on backpropagation signals through the network. We then compute weighted gradients using these attribution scores to selectively update parameters in the model. In our experiments, we show that our Attribution Mask significantly reduces overfitting without sacrificing accuracy. Furthermore, we demonstrate improvements in transferability of learned features across multiple datasets compared to baseline methods. Our results suggest that the proposed Attribution Mask provides a powerful tool for designing more robust and efficient deep neural architectures.",1
"We consider the problem of costly feature classification, where we sequentially select the subset of features to make a balance between the classification error and the feature cost. In this paper, we first cast the task into a MDP problem and use Advantage Actor Critic algorithm to solve it. In order to further improve the agent's performance and make the policy explainable, we employ the Monte Carlo Tree Search to update the policy iteratively. During the procedure, we also consider its performance on the unbalanced dataset and its sensitivity to the missing value. We evaluate our model on multiple datasets and find it outperforms other methods.",0
"We present a novel approach for costly feature classification using Monte Carlo tree search (MCTS). Our method models the problem as a multi-objective optimization task where we simultaneously optimize multiple objectives representing different criteria such as accuracy and computational costs. We first formulate the feature selection problem as an MCTS algorithm that efficiently explores the space of candidate features and builds decision trees at each iteration. Then, our method uses an evolutionary algorithm (EA) to iteratively update the weights of the objectives based on the performance achieved by the current solution. By doing so, our EA adaptively balances the tradeoff between the conflicting objectives. To evaluate the effectiveness of our approach, we conducted extensive experiments on several benchmark datasets across diverse domains and compared against state-of-the-art methods. Results show that our method achieves significant improvements over existing approaches in terms of both accuracy and efficiency while also providing meaningful insights into how different objectives interact during feature selection. This work extends the use of MCTS beyond traditional gameplaying domains and shows promising results for more complex real-world applications involving feature extraction from large data sets.",1
"The price of explainability for a clustering task can be defined as the unavoidable loss,in terms of the objective function, if we force the final partition to be explainable.   Here, we study this price for the following clustering problems: $k$-means, $k$-medians, $k$-centers and maximum-spacing. We provide upper and lower bounds for a natural model where explainability is achieved via decision trees. For the $k$-means and $k$-medians problems our upper bounds improve those obtained by [Moshkovitz et. al, ICML 20] for low dimensions.   Another contribution is a simple and efficient algorithm for building explainable clusterings for the $k$-means problem. We provide empirical evidence that its performance is better than the current state of the art for decision-tree based explainable clustering.",0
"In recent years, there has been growing interest in developing machine learning models that can provide interpretable explanations for their predictions. This is particularly important in applications where transparency is critical, such as medicine, finance, and law enforcement. However, achieving interpretability often comes at the cost of reduced accuracy or increased complexity. Clustering, which involves grouping similar data points together, is one area where these tradeoffs are particularly pronounced.  This paper examines the impact of different criteria on the performance of clustering algorithms, including intracluster similarity (the degree to which objects within a cluster are alike), intercluster dissimilarity (how distinct clusters are from each other), and interpretability measures like feature importance rankings or visualizations. Our results show that optimizing for all three objectives simultaneously leads to significant decreases in model quality across a range of datasets and evaluation metrics compared to prior state-of-the-art methods. We argue that practitioners should carefully consider their goals when choosing clustering algorithms and evaluate how different choices may affect the quality and interpretability of their output. By better understanding the implications of these decisions, we hope to improve confidence in the use of machine learning for decision making in complex domains.",1
"Distillation is the technique of training a ""student"" model based on examples that are labeled by a separate ""teacher"" model, which itself is trained on a labeled dataset. The most common explanations for why distillation ""works"" are predicated on the assumption that student is provided with \emph{soft} labels, \eg probabilities or confidences, from the teacher model. In this work, we show, that, even when the teacher model is highly overparameterized, and provides \emph{hard} labels, using a very large held-out unlabeled dataset to train the student model can result in a model that outperforms more ""traditional"" approaches.   Our explanation for this phenomenon is based on recent work on ""double descent"". It has been observed that, once a model's complexity roughly exceeds the amount required to memorize the training data, increasing the complexity \emph{further} can, counterintuitively, result in \emph{better} generalization. Researchers have identified several settings in which it takes place, while others have made various attempts to explain it (thus far, with only partial success). In contrast, we avoid these questions, and instead seek to \emph{exploit} this phenomenon by demonstrating that a highly-overparameterized teacher can avoid overfitting via double descent, while a student trained on a larger independent dataset labeled by this teacher will avoid overfitting due to the size of its training set.",0
"This paper studies how reinforcement learning algorithms can learn policies that both minimize regret over episodes while still converging on high quality solutions. We show that the existing ""double descent"" phenomenon can continue even after convergence by distilling knowledge into simpler models. Our results apply to a variety of model classes such as linear bandits, linear MDPs, and deep Q networks, but we argue that similar effects could occur in other domains like generative tasks, supervised learning and self play. We believe these findings have implications for understanding generalization in machine learning models that operate over sequences, and more broadly raise questions about whether there may exist different ways of quantifying progress during training beyond traditional metrics used today like accuracy and loss reduction.",1
"Sample efficiency and performance in the offline setting have emerged as significant challenges of deep reinforcement learning. We introduce Q-Value Weighted Regression (QWR), a simple RL algorithm that excels in these aspects. QWR is an extension of Advantage Weighted Regression (AWR), an off-policy actor-critic algorithm that performs very well on continuous control tasks, also in the offline setting, but has low sample efficiency and struggles with high-dimensional observation spaces. We perform an analysis of AWR that explains its shortcomings and use these insights to motivate QWR. We show experimentally that QWR matches the state-of-the-art algorithms both on tasks with continuous and discrete actions. In particular, QWR yields results on par with SAC on the MuJoCo suite and - with the same set of hyperparameters - yields results on par with a highly tuned Rainbow implementation on a set of Atari games. We also verify that QWR performs well in the offline RL setting.",0
"This paper presents a novel method for reinforcement learning in situations where data is limited. Our approach combines ideas from two previously existing methods, Q-value based policy selection and weighted regression. In our new algorithm, we use both approaches together to improve performance under these difficult conditions by combining their strengths. Our key insight was that using the ratio of the value function estimates at each state as weights produces better results than previous alternatives. Through rigorous experiments on several challenging domains, we demonstrate improved accuracy compared to prior algorithms even when only a fraction of the required amount of data has been collected. These findings have important implications for anyone working on real world applications of RL, such as robotics or economics, where large amounts of training data may not always be available.",1
"The test loss of well-trained neural networks often follows precise power-law scaling relations with either the size of the training dataset or the number of parameters in the network. We propose a theory that explains and connects these scaling laws. We identify variance-limited and resolution-limited scaling behavior for both dataset and model size, for a total of four scaling regimes. The variance-limited scaling follows simply from the existence of a well-behaved infinite data or infinite width limit, while the resolution-limited regime can be explained by positing that models are effectively resolving a smooth data manifold. In the large width limit, this can be equivalently obtained from the spectrum of certain kernels, and we present evidence that large width and large dataset resolution-limited scaling exponents are related by a duality. We exhibit all four scaling regimes in the controlled setting of large random feature and pretrained models and test the predictions empirically on a range of standard architectures and datasets. We also observe several empirical relationships between datasets and scaling exponents: super-classing image tasks does not change exponents, while changing input distribution (via changing datasets or adding noise) has a strong effect. We further explore the effect of architecture aspect ratio on scaling exponents.",0
"This paper investigates neural scaling laws, which describe how different aspects of artificial intelligence (AI) systems change as they become larger. Specifically, we examine the relationship between system size and various measures of performance, including accuracy, efficiency, and robustness. We show that these relationships can be modeled using mathematical equations, providing insights into the behavior of large AI systems and guiding the design of future systems. Our findings have important implications for both researchers working on improving existing AI methods and practitioners building real-world applications.",1
"We define a neural network as a septuple consisting of (1) a state vector, (2) an input projection, (3) an output projection, (4) a weight matrix, (5) a bias vector, (6) an activation map and (7) a loss function. We argue that the loss function can be imposed either on the boundary (i.e. input and/or output neurons) or in the bulk (i.e. hidden neurons) for both supervised and unsupervised systems. We apply the principle of maximum entropy to derive a canonical ensemble of the state vectors subject to a constraint imposed on the bulk loss function by a Lagrange multiplier (or an inverse temperature parameter). We show that in an equilibrium the canonical partition function must be a product of two factors: a function of the temperature and a function of the bias vector and weight matrix. Consequently, the total Shannon entropy consists of two terms which represent respectively a thermodynamic entropy and a complexity of the neural network. We derive the first and second laws of learning: during learning the total entropy must decrease until the system reaches an equilibrium (i.e. the second law), and the increment in the loss function must be proportional to the increment in the thermodynamic entropy plus the increment in the complexity (i.e. the first law). We calculate the entropy destruction to show that the efficiency of learning is given by the Laplacian of the total free energy which is to be maximized in an optimal neural architecture, and explain why the optimization condition is better satisfied in a deep network with a large number of hidden layers. The key properties of the model are verified numerically by training a supervised feedforward neural network using the method of stochastic gradient descent. We also discuss a possibility that the entire universe on its most fundamental level is a neural network.",0
"Machine Learning (ML) has transformed the field of artificial intelligence since it first emerged as a distinct research area in the late 1970s [1]. Today, ML algorithms perform tasks ranging from speech recognition to natural language processing, computer vision, robotics, bioinformatics, economics and finance, and many others, making ML techniques indispensible tools across science, engineering, business, healthcare and industry at large [2, 3]. With each passing year, new models continue to outperform earlier approaches on existing benchmark tests. However, despite these successes, our understanding of why some methods work better than others remains limited. This suggests that we lack general guidelines that could enable us to design systems that can learn from data without requiring massive amounts of computing power or extensive tuning procedures [4â€“6]. Here we describe recent advances towards developing such theoretical frameworks for ML and discuss challenges moving forward. We outline promising directions for future research aimed at achieving a more complete picture of how machines might come to learn like humans. By grounding modern empirical practice in rigorous foundations, we believe that we can unlock greater creativity and scientific progress within the broader community while reducing risk [7â€“9].",1
"Reinforcement Learning (RL) algorithms have led to recent successes in solving complex games, such as Atari or Starcraft, and to a huge impact in real-world applications, such as cybersecurity or autonomous driving. In the side of the drawbacks, recent works have shown how the performance of RL algorithms decreases under the influence of soft changes in the reward function. However, little work has been done about how sensitive these disturbances are depending on the aggressiveness of the attack and the learning exploration strategy. In this paper, we propose to fill this gap in the literature analyzing the effects of different attack strategies based on reward perturbations, and studying the effect in the learner depending on its exploration strategy. In order to explain all the behaviors, we choose a sub-class of MDPs: episodic, stochastic goal-only-rewards MDPs, and in particular, an intelligible grid domain as a benchmark. In this domain, we demonstrate that smoothly crafting adversarial rewards are able to mislead the learner, and that using low exploration probability values, the policy learned is more robust to corrupt rewards. Finally, in the proposed learning scenario, a counterintuitive result arises: attacking at each learning episode is the lowest cost attack strategy.",0
"Abstract: This paper considers how reward corruption affects reinforcement learning agents, which can lead them to perform suboptimal actions even if they have access to an optimal policy. By exploiting flaws in the agentâ€™s reward function, humans can manipulate these algorithms into behaviors that harm human interests, including those related to safety, privacy, security, manipulation resistance, and other ethical considerations. Therefore, understanding disturbing phenomena caused by reward corruptions is essential to mitigate their harms and improve AI robustness. We investigate fundamental questions on why and how reward corruptions are problematic using rigorous definitions, taxonomies, analyses, and experiments based on Markov decision processes (MDP) with both deterministic policies and stochastic policies. Our results highlight important gaps between existing assumptions in traditional RL research and actual problems brought by reward manipulations. Furthermore, we propose practical approaches to detect and alleviate potential threats from rewards subjected to tampering. As one major contribution of our work, we formulate four concrete axioms for measuring intrinsic vulnerability of reward functions, whose validity has been theoretically justified under certain conditions and empirically evaluated in extensive experimental studies across varied environments. Through analyzing reward vulnerabilities along with a comprehensive survey of attack strategies and defense methods over different scenarios, we distill representative case studies demonstrating counterintuitive consequences where seemingly safe policies could cause severe damages or poor performance when exposed to attacks. These examples emphasize the necessity of designing more robust reward structures that can better cope with unpredictable adversarial attacks, thus minimizing negative impacts posed by reward manipulations. Overall, our findings pave the way f",1
"The booming interest in adversarial attacks stems from a misalignment between human vision and a deep neural network (DNN), i.e. a human imperceptible perturbation fools the DNN. Moreover, a single perturbation, often called universal adversarial perturbation (UAP), can be generated to fool the DNN for most images. A similar misalignment phenomenon has recently also been observed in the deep steganography task, where a decoder network can retrieve a secret image back from a slightly perturbed cover image. We attempt explaining the success of both in a unified manner from the Fourier perspective. We perform task-specific and joint analysis and reveal that (a) frequency is a key factor that influences their performance based on the proposed entropy metric for quantifying the frequency distribution; (b) their success can be attributed to a DNN being highly sensitive to high-frequency content. We also perform feature layer analysis for providing deep insight on model generalization and robustness. Additionally, we propose two new variants of universal perturbations: (1) Universal Secret Adversarial Perturbation (USAP) that simultaneously achieves attack and hiding; (2) high-pass UAP (HP-UAP) that is less visible to the human eye.",0
"This paper examines adversarial attacks on deep learning models from a unique perspective - steganographic methods. Specifically, we analyze universal perturbation techniques used in these attacks through the lens of deep steganography, which involves hiding messages within images while minimizing perceptual distortion. Our analysis shows that universal adversarial examples can be viewed as a form of high-level steganography where the message being hidden is a specific classification label rather than a secret text or image. We then propose a new approach based on the use of a novel Fourier framework to create stronger and more transferable universal perturbations. Extensive experimental evaluations demonstrate the effectiveness of our methodology and provide important insights into both the strengths and limitations of current attack strategies. Finally, we discuss implications for future research directions in terms of designing robust machine learning systems resistant to these kinds of stealthy attacks.",1
"Parameter Estimation (PE) and State Estimation (SE) are the most wide-spread tasks in the system engineering. They need to be done automatically, fast and frequently, as measurements arrive. Deep Learning (DL) holds the promise of tackling the challenge, however in so far, as PE and SE in power systems is concerned, (a) DL did not win trust of the system operators because of the lack of the physics of electricity based, interpretations and (b) DL remained illusive in the operational regimes were data is scarce. To address this, we present a hybrid scheme which embeds physics modeling of power systems into Graphical Neural Networks (GNN), therefore empowering system operators with a reliable and explainable real-time predictions which can then be used to control the critical infrastructure. To enable progress towards trustworthy DL for PE and SE, we build a physics-informed method, named Power-GNN, which reconstructs physical, thus interpretable, parameters within Effective Power Flow (EPF) models, such as admittances of effective power lines, and NN parameters, representing implicitly unobserved elements of the system. In our experiments, we test the Power-GNN on different realistic power networks, including these with thousands of loads and hundreds of generators. We show that the Power-GNN outperforms vanilla NN scheme unaware of the EPF physics.",0
"This research presents a novel approach for parameter and state estimations in power systems using physics-informed graphical neural networks (PIGNNs). Conventional methods face challenges due to limitations in model accuracy, nonlinearity, and constraints that cannot capture dynamic interactions among variables. To address these issues, we propose a PIGNN framework that incorporates physical laws into deep learning models to improve robustness and generalizability. The proposed method integrates existing knowledge about power systems with data-driven techniques, leading to more accurate predictions and effective monitoring of critical system parameters. Numerical simulations on real-world datasets demonstrate the effectiveness of our method over traditional estimation approaches. Our results provide new insights into the design of advanced machine learning algorithms that leverage domain expertise for enhanced performance in complex engineering applications.",1
"Although few-shot learning and one-class classification (OCC), i.e., learning a binary classifier with data from only one class, have been separately well studied, their intersection remains rather unexplored. Our work addresses the few-shot OCC problem and presents a method to modify the episodic data sampling strategy of the model-agnostic meta-learning (MAML) algorithm to learn a model initialization particularly suited for learning few-shot OCC tasks. This is done by explicitly optimizing for an initialization which only requires few gradient steps with one-class minibatches to yield a performance increase on class-balanced test data. We provide a theoretical analysis that explains why our approach works in the few-shot OCC scenario, while other meta-learning algorithms fail, including the unmodified MAML. Our experiments on eight datasets from the image and time-series domains show that our method leads to better results than classical OCC and few-shot classification approaches, and demonstrate the ability to learn unseen tasks from only few normal class samples. Moreover, we successfully train anomaly detectors for a real-world application on sensor readings recorded during industrial manufacturing of workpieces with a CNC milling machine, by using few normal examples. Finally, we empirically demonstrate that the proposed data sampling technique increases the performance of more recent meta-learning algorithms in few-shot OCC and yields state-of-the-art results in this problem setting.",0
"In recent years, few-shot learning has emerged as one of the most promising areas in computer vision research. This technique allows artificial intelligence (AI) systems to learn from very little data, making them more efficient and flexible than traditional methods that require large amounts of training data. However, existing few-shot approaches still have limitations in terms of their ability to handle complex tasks and diverse scenarios.  To address these challenges, we propose a novel approach called meta-learning, which leverages prior knowledge gained during the training process. Our method adapts quickly to new classes by adjusting learned representations according to task similarity. The framework integrates both data augmentation and regularization techniques to improve generalization performance. We showcase our approach on several benchmark datasets such as Omniglot and miniImageNet. Experimental results demonstrate significant improvement compared to state-of-the-art algorithms, validating the effectiveness and potential of our method in various applications. Overall, our work pushes forward the frontier of few-shot learning and provides valuable insights into future developments in AI.",1
"The classification of time-series data is pivotal for streaming data and comes with many challenges. Although the amount of publicly available datasets increases rapidly, deep neural models are only exploited in a few areas. Traditional methods are still used very often compared to deep neural models. These methods get preferred in safety-critical, financial, or medical fields because of their interpretable results. However, their performance and scale-ability are limited, and finding suitable explanations for time-series classification tasks is challenging due to the concepts hidden in the numerical time-series data. Visualizing complete time-series results in a cognitive overload concerning our perception and leads to confusion. Therefore, we believe that patch-wise processing of the data results in a more interpretable representation. We propose a novel hybrid approach that utilizes deep neural networks and traditional machine learning algorithms to introduce an interpretable and scale-able time-series classification approach. Our method first performs a fine-grained classification for the patches followed by sample level classification.",0
"This is a research paper that presents a new method for explaining deep models used for time-series classification. The proposed approach, called PatchX, uses intelligible pattern patches to visualize important features learned by these models. In contrast to existing methods that focus on explaining individual predictions or attention maps, PatchX provides interpretable explanations at the level of raw input data. We demonstrate how our approach can be applied to several state-of-the-art time-series forecasting tasks, including stock market prediction, disease diagnosis from medical records, and wind power generation prediction. Our results show that PatchX effectively captures the key patterns learned by different architectures and reveals insights into their decision making process.",1
"This paper explores the similarities of output layers in Neural Networks (NNs) with logistic regression to explain importance of inputs by Z-scores. The network analyzed, a network for fusion of Synthetic Aperture Radar (SAR) and Microwave Radiometry (MWR) data, is applied to prediction of arctic sea ice. With the analysis the importance of MWR relative to SAR is found to favor MWR components. Further, as the model represents image features at different scales, the relative importance of these are as well analyzed. The suggested methodology offers a simple and easy framework for analyzing output layer components and can reduce the number of components for further analysis with e.g. common NN visualization methods.",0
"In recent years, convolutional neural networks (CNNs) have achieved state-of-the-art performance on many computer vision tasks such as image classification and object detection. However, these models can be challenging to interpret because they operate on high-dimensional data and contain many complex nonlinear transformations. This difficulty has led to increased interest in methods that aim to explain how CNNs make decisions. Existing work in this field includes gradient-based techniques, occlusion analysis, integrated gradients, and guided backpropagation. These approaches typically provide local explanations by analyzing individual features within an input instance, but overlook global context which might play a significant role in the decision making process. In order to address this issue, we propose a novel method based on z-scores (a statistical measure of standard deviation from normal distribution mean value) to explain CNN predictions globally. Our approach operates directly on the probability density functions (PDFs) of feature maps, instead of focusing only on activations at particular locations/pixels. We show experimentally on large datasets that our method produces easily interpretable visualizations which reveal important details regarding the network's behavior while running efficiently even for extremely deep architectures. Furthermore, extensive comparisons conducted against other popular interpretation schemes demonstrate that our PDF-aware framework outperforms them both quantitatively and qualitatively across several benchmark studies on different architectures. Ultimately, since most machine learning practitioners prefer human-readable outputs to aid their understanding, our PDF-explainer serves as a valuable tool toward realizing more trustworthy and deployable artificial intelligence applications.",1
"Time series data is a collection of chronological observations which is generated by several domains such as medical and financial fields. Over the years, different tasks such as classification, forecasting, and clustering have been proposed to analyze this type of data. Time series data has been also used to study the effect of interventions over time. Moreover, in many fields of science, learning the causal structure of dynamic systems and time series data is considered an interesting task which plays an important role in scientific discoveries. Estimating the effect of an intervention and identifying the causal relations from the data can be performed via causal inference. Existing surveys on time series discuss traditional tasks such as classification and forecasting or explain the details of the approaches proposed to solve a specific task. In this paper, we focus on two causal inference tasks, i.e., treatment effect estimation and causal discovery for time series data, and provide a comprehensive review of the approaches in each task. Furthermore, we curate a list of commonly used evaluation metrics and datasets for each task and provide in-depth insight. These metrics and datasets can serve as benchmarks for research in the field.",0
"This paper deals with causality, one of the most important tools used by scientists to study causal relationships among variables. We explore different methods that have been developed for evaluating such relationships using time series data, as well as some of their limitations. Our research shows that while these techniques can provide valuable insights into complex systems, they must be used judiciously to avoid making invalid assumptions and drawing incorrect conclusions. We conclude by suggesting areas where further work is needed to improve our understanding of cause and effect. Overall, we hope this paper serves as a helpful introduction to anyone interested in studying causality using time series analysis.",1
"Deep reinforcement learning (RL) algorithms are powerful tools for solving visuomotor decision tasks. However, the trained models are often difficult to interpret, because they are represented as end-to-end deep neural networks. In this paper, we shed light on the inner workings of such trained models by analyzing the pixels that they attend to during task execution, and comparing them with the pixels attended to by humans executing the same tasks. To this end, we investigate the following two questions that, to the best of our knowledge, have not been previously studied. 1) How similar are the visual features learned by RL agents and humans when performing the same task? and, 2) How do similarities and differences in these learned features explain RL agents' performance on these tasks? Specifically, we compare the saliency maps of RL agents against visual attention models of human experts when learning to play Atari games. Further, we analyze how hyperparameters of the deep RL algorithm affect the learned features and saliency maps of the trained agents. The insights provided by our results have the potential to inform novel algorithms for the purpose of closing the performance gap between human experts and deep RL agents.",0
"This study investigates whether human attention differs from machine attention in deep reinforcement learning tasks. To evaluate this question, we compare the performance of both types of attention on several common deep reinforcement learning benchmark problems. We show that while there were some differences in their attentive abilities, overall human attention outperformed machine attention across all evaluated tasks. These findings have important implications for our understanding of how humans perform complex cognitive tasks compared to artificial intelligence algorithms.",1
"The performance of a binary classifier (""predictor"") depends heavily upon the context (""workflow"") in which it operates. Classic measures of predictor performance do not reflect the realized utility of predictors unless certain implied workflow assumptions are met. Failure to meet these implied assumptions results in suboptimal classifier implementations and a mismatch between predicted or assessed performance and the actual performance obtained in real-world deployments. The mismatch commonly arises when multiple predictions can be made for the same event, the event is relatively rare, and redundant true positive predictions for the same event add little value, e.g., a system that makes a prediction each minute, repeatedly issuing interruptive alarms for a predicted event that may never occur.   We explain why classic metrics do not correctly represent the performance of predictors in such contexts, and introduce an improved performance assessment technique (""u-metrics"") using utility functions to score each prediction. U-metrics explicitly account for variability in prediction utility arising from temporal relationships. Compared to traditional performance measures, u-metrics more accurately reflect the real-world benefits and costs of a predictor operating in a workflow context. The difference can be significant.   We also describe the use of ""snoozing,"" a method whereby predictions are suppressed for a period of time, commonly improving predictor performance by reducing false positives while retaining the capture of events. Snoozing is especially useful when predictors generate interruptive alerts, as so often happens in clinical practice. Utility-based performance metrics correctly predict and track the performance benefits of snoozing, whereas traditional performance metrics do not.",0
"This should serve as an introduction to the rest of the paper that summarizes the techniques used in the paper without repeating the paper's title. Use the following points:  * Abstract methods, novel approach compared to current standards * Uses real patient data from medical devices and electronic health records * Experimental results using synthetic data that mimics real patterns * Focus on predicting readmissions for congestive heart failure patients with alarm burden considered * Implications for improving accuracy in decision support tools for clinical care managers",1
"Poor generalization is one symptom of models that learn to predict target variables using spuriously-correlated image features present only in the training distribution instead of the true image features that denote a class. It is often thought that this can be diagnosed visually using attribution (aka saliency) maps. We study if this assumption is correct. In some prediction tasks, such as for medical images, one may have some images with masks drawn by a human expert, indicating a region of the image containing relevant information to make the prediction. We study multiple methods that take advantage of such auxiliary labels, by training networks to ignore distracting features which may be found outside of the region of interest. This mask information is only used during training and has an impact on generalization accuracy depending on the severity of the shift between the training and test distributions. Surprisingly, while these methods improve generalization performance in the presence of a covariate shift, there is no strong correspondence between the correction of attribution towards the features a human expert has labelled as important and generalization performance. These results suggest that the root cause of poor generalization may not always be spatially defined, and raise questions about the utility of masks as ""attribution priors"" as well as saliency maps for explainable predictions.",0
"Isolating the cause of poor generalization remains challenging due to factors such as training set size, model architecture choice, optimization methods, and randomness during both training and inference. In this work, we investigate whether salient patterns in data can act as red herrings that distract from other issues affecting generalization performance. We demonstrate how overreliance on salient features can lead to inflated accuracy on validation sets, while still resulting in poor generalization to previously unseen test sets. By controlling for feature importance using adversarial examples, we provide evidence suggesting that reliance on salient features may indeed contribute to poorer than expected generalization performance. Our findings emphasize the need for further research into sources of poor generalization beyond just isolated properties of individual models and their parameters.",1
"We first exhibit a multimodal image registration task, for which a neural network trained on a dataset with noisy labels reaches almost perfect accuracy, far beyond noise variance. This surprising auto-denoising phenomenon can be explained as a noise averaging effect over the labels of similar input examples. This effect theoretically grows with the number of similar examples; the question is then to define and estimate the similarity of examples.   We express a proper definition of similarity, from the neural network perspective, i.e. we quantify how undissociable two inputs $A$ and $B$ are, taking a machine learning viewpoint: how much a parameter variation designed to change the output for $A$ would impact the output for $B$ as well?   We study the mathematical properties of this similarity measure, and show how to use it on a trained network to estimate sample density, in low complexity, enabling new types of statistical analysis for neural networks. We analyze data by retrieving samples perceived as similar by the network, and are able to quantify the denoising effect without requiring true labels. We also propose, during training, to enforce that examples known to be similar should also be seen as similar by the network, and notice speed-up training effects for certain datasets.",0
In this paper we discuss input similarity from the perspective of neural networks. We first review existing approaches and highlight their limitations in capturing global dependencies among inputs. We then introduce our novel approach which can model both local and nonlocal relationships within and across different modalities. Our method outperforms state-of-the-art alternatives on several benchmark datasets while maintaining competitive computational efficiency.,1
"Deep learning applied to weather forecasting has started gaining popularity because of the progress achieved by data-driven models. The present paper compares two different deep learning architectures to perform weather prediction on daily data gathered from 18 cities across Europe and spanned over a period of 15 years. We propose the Deep Attention Unistream Multistream (DAUM) networks that investigate different types of input representations (i.e. tensorial unistream vs. multistream ) as well as the incorporation of the attention mechanism. In particular, we show that adding a self-attention block within the models increases the overall forecasting performance. Furthermore, visualization techniques such as occlusion analysis and score maximization are used to give an additional insight on the most important features and cities for predicting a particular target feature of target cities.",0
"Abstract This is a research paper that proposes deep learning techniques to better generate accurate weather predictions. It introduces an approach to multi-station forecasting, using Explainable Recurrent Convolutional Neural Networks (ERCNN) on datasets from multiple stations across different regions. ERCNN combines RNN and CNN architectures, which can capture spatial dependencies as well as nonlinear dynamics in time-series data. By training ERCNN models on these diverse datasets, we demonstrate improved performance compared to traditional methods and showcase their potential in improving localized predictions. Additionally, our method enables interpretability by providing explanations of how input variables influence model outputs. Our findings contribute to advancing the state of art in multi-site deep learning applications for weather prediction and provide insights into designing more efficient architectures that account for heterogeneity in complex spatiotemporal data.",1
"The rise of deep learning in today's applications entailed an increasing need in explaining the model's decisions beyond prediction performances in order to foster trust and accountability. Recently, the field of explainable AI (XAI) has developed methods that provide such explanations for already trained neural networks. In computer vision tasks such explanations, termed heatmaps, visualize the contributions of individual pixels to the prediction. So far XAI methods along with their heatmaps were mainly validated qualitatively via human-based assessment, or evaluated through auxiliary proxy tasks such as pixel perturbation, weak object localization or randomization tests. Due to the lack of an objective and commonly accepted quality measure for heatmaps, it was debatable which XAI method performs best and whether explanations can be trusted at all. In the present work, we tackle the problem by proposing a ground truth based evaluation framework for XAI methods based on the CLEVR visual question answering task. Our framework provides a (1) selective, (2) controlled and (3) realistic testbed for the evaluation of neural network explanations. We compare ten different explanation methods, resulting in new insights about the quality and properties of XAI methods, sometimes contradicting with conclusions from previous comparative studies. The CLEVR-XAI dataset and the benchmarking code can be found at https://github.com/ahmedmagdiosman/clevr-xai.",0
"This research presents a methodology for evaluating neural network explanations using real-world data from the CLEVR-XAI benchmark dataset. The paper provides a comprehensive analysis of the strengths and limitations of different explanation methods by comparing their accuracy against ground truth labels provided by human annotators. The authors argue that current evaluation approaches rely on synthetic datasets which may not fully capture the complexity of natural language understanding tasks, making it difficult to assess the quality of generated explanations.  The study proposes several criteria for judging the effectiveness of explanation techniques, including coverage (the fraction of inputs yielding valid rationales), sufficiency (whether all valid rationales have been found), uniqueness (whether multiple rationales can exist for any given input), relevance (whether the rationales accurately relate to the correct output), completeness (whether relevant information has been included), and ambiguity (how many possible interpretations of a rationale may exist). These metrics allow for both qualitative and quantitative comparisons across diverse methods, promoting transparency and reproducibility in scientific inquiry.  Furthermore, the authors address ethical concerns regarding the use of artificial intelligence systems in high-stakes applications where accountability must prevail. By emphasizing empirical validation through CLEVR-XAI ground truth annotation, the research fosters trustworthiness in neurotechnological innovations, ultimately benefiting society as a whole. Overall, Ground Truth Evaluation of Neural Network Explanations with CLEVR-XAI advances the state-of-the-art in machine learning by improving our ability to scrutinize these complex models responsible for critical decisions in countless domains worldwide. As such, the paper holds significant implications for academics, policymakers, industry leaders, and end-users of AI technology alike.",1
"With the exploding popularity of machine learning, domain knowledge in various forms has been playing a crucial role in improving the learning performance, especially when training data is limited. Nonetheless, there is little understanding of to what extent domain knowledge can affect a machine learning task from a quantitative perspective. To increase the transparency and rigorously explain the role of domain knowledge in machine learning, we study the problem of quantifying the values of domain knowledge in terms of its contribution to the learning performance in the context of informed machine learning. We propose a quantification method based on Shapley value that fairly attributes the overall learning performance improvement to different domain knowledge. We also present Monte-Carlo sampling to approximate the fair value of domain knowledge with a polynomial time complexity. We run experiments of injecting symbolic domain knowledge into semi-supervised learning tasks on both MNIST and CIFAR10 datasets, providing quantitative values of different symbolic knowledge and rigorously explaining how it affects the machine learning performance in terms of test accuracy.",0
"In todayâ€™s world, machine learning has become increasingly popular as a method to solve complex problems across various domains, such as finance, healthcare, and manufacturing (Dignum et al.,2023). This study aimsto provideaquantitativestudyof how domain knowledge can impact the performance of machine learning models for classification tasks (Chen et al., 2019). For the purpose of this research, we conducted experiments using three datasets from different domains: medical imaging, credit scoring, and wine quality prediction (Deng & Sun, 2020). We compared two types of machine learning algorithms for each dataset: one that utilized expert knowledge of the domain and another that did not (Koller et al., 2018;Slavkovik et al., 2020). Our results showthatutilizingdomainknowledgedecreasesmodeluncertaintyandimprovesthe accuracy and precision of predictions (Hall et al., 2023; Wang et al., 2023). Furthermore, our findings suggest that incorporating experts' qualitative insights into machine learning algorithms improves overall model performance (Fraser et al., 2023). However, there were some cases where highperformancemodelscanbeachievedwithoutdomainexpertise(Tutin etal.,2023).The implications of these findings could lead to improved use of machine learning algorithms in decision making processes across different industries by highlighting the importance of incorporating domain knowledge to achieve optimal outcomes (Zhang & Chen, 2023;Zhouetal.,2023).Thisstudysubstantial contributiontoknowledge inthe field ofmachinelearningandsuggestfuturedirectionfor futureresearchinthisarea(Mayeretale",1
"Quantile regression, based on check loss, is a widely used inferential paradigm in Econometrics and Statistics. The conditional quantiles provide a robust alternative to classical conditional means, and also allow uncertainty quantification of the predictions, while making very few distributional assumptions. We consider the analogue of check loss in the binary classification setting. We assume that the conditional quantiles are smooth functions that can be learnt by Deep Neural Networks (DNNs). Subsequently, we compute the Lipschitz constant of the proposed loss, and also show that its curvature is bounded, under some regularity conditions. Consequently, recent results on the error rates and DNN architecture complexity become directly applicable.   We quantify the uncertainty of the class probabilities in terms of prediction intervals, and develop individualized confidence scores that can be used to decide whether a prediction is reliable or not at scoring time. By aggregating the confidence scores at the dataset level, we provide two additional metrics, model confidence, and retention rate, to complement the widely used classifier summaries. We also the robustness of the proposed non-parametric binary quantile classification framework are also studied, and we demonstrate how to obtain several univariate summary statistics of the conditional distributions, in particular conditional means, using smoothed conditional quantiles, allowing the use of explanation techniques like Shapley to explain the mean predictions. Finally, we demonstrate an efficient training regime for this loss based on Stochastic Gradient Descent with Lipschitz Adaptive Learning Rates (LALR).",0
"""This paper presents an investigation into the estimation and applications of quantiles in deep binary classification models. We begin by discussing how quantile regression has been used previously in classification tasks, as well as some potential drawbacks of these methods. Our proposed method utilizes both quantile regression and deep learning techniques, allowing us to capture complex nonlinear relationships within the data while still maintaining interpretable predictions. In addition to developing a novel estimator, we demonstrate how our model can be applied to various real world problems such as risk assessment and feature importance analysis. Empirical results on several datasets show that our approach outperforms existing state-of-the-art methods in terms of prediction accuracy and interpretability.""",1
"The objective of an online Mart is to match buyers and sellers, to weigh animals and to oversee their sale. A reliable pricing method can be developed by ML models that can read through historical sales data. However, when AI models suggest or recommend a price, that in itself does not reveal too much (i.e., it acts like a black box) about the qualities and the abilities of an animal. An interested buyer would like to know more about the salient features of an animal before making the right choice based on his requirements. A model capable of explaining the different factors that impact the price point is essential for the needs of the market. It can also inspire confidence in buyers and sellers about the price point offered. To achieve these objectives, we have been working with the team at MartEye, a startup based in Portershed in Galway City, Ireland. Through this paper, we report our work-in-progress research towards building a smart video analytic platform, leveraging Explainable AI techniques.",0
"In recent years, artificial intelligence (AI) has gained significant traction in numerous industries due to its ability to process large amounts of data efficiently and accurately. However, one major challenge that arises from using AI systems is their lack of explainability - often times, even humans find it difficult to comprehend how these systems arrived at certain decisions or predictions. This difficulty can lead to skepticism, mistrust, and hesitation among stakeholders who require transparency and accountability in decision making processes. Therefore, designing explainable-AI solutions is crucial to ensure trustworthiness, reliability, and acceptance by end users.  The livestock mart industry deals with buying and selling animals through auctions, and requires efficient management systems to facilitate transactions. One such system can benefit greatly from computer vision algorithms capable of automating tasks such as animal identification, counting, and grading. Despite the advantages of incorporating AI into these tasks, there still exists a need for transparent explanations on how the algorithm arrives at a particular decision.  This research explores ways towards developing an explainable-AI solution for the livestock mart industry. By leveraging the power of deep learning techniques and feature engineering methods, we aim to develop models capable of classifying animals based on factors such as breed, age, weight, etc., while providing clear explanations on how each prediction was reached. We hope to demonstrate that our approach improves both efficiency and accuracy within this industry, while addressing any concerns regarding transparency and understanding surrounding AI usage. Ultimately, our goal is to provide a use case that highlights the benefits of employing explainable-AI solutions and encourages further adoption across other domains.",1
"Deep neural networks' remarkable ability to correctly fit training data when optimized by gradient-based algorithms is yet to be fully understood. Recent theoretical results explain the convergence for ReLU networks that are wider than those used in practice by orders of magnitude. In this work, we take a step towards closing the gap between theory and practice by significantly improving the known theoretical bounds on both the network width and the convergence time. We show that convergence to a global minimum is guaranteed for networks with widths quadratic in the sample size and linear in their depth at a time logarithmic in both. Our analysis and convergence bounds are derived via the construction of a surrogate network with fixed activation patterns that can be transformed at any time to an equivalent ReLU network of a reasonable size. This construction can be viewed as a novel technique to accelerate training, while its tight finite-width equivalence to Neural Tangent Kernel (NTK) suggests it can be utilized to study generalization as well.",0
"This paper presents a new theory that attempts to explain why over-parameterized deep neural networks (DNNs) can achieve high accuracy even without traditional regularization techniques such as dropout or weight decay. Our proposed theory suggests that during training, DNNs undergo a process of ""convergence"" towards a simple, low-dimensional solution space, regardless of their initial complexity. We show how this convergence phenomenon can lead to better generalization performance on complex tasks, resulting in models that require fewer parameters compared to earlier architectures. Furthermore, we demonstrate experimentally that our theory holds true across different model sizes and architectures, making it relevant to all practitioners working in the field.",1
"Partial label learning (PLL) is a class of weakly supervised learning where each training instance consists of a data and a set of candidate labels containing a unique ground truth label. To tackle this problem, a majority of current state-of-the-art methods employs either label disambiguation or averaging strategies. So far, PLL methods without such techniques have been considered impractical. In this paper, we challenge this view by revealing the hidden power of the oldest and naivest PLL method when it is instantiated with deep neural networks. Specifically, we show that, with deep neural networks, the naive model can achieve competitive performances against the other state-of-the-art methods, suggesting it as a strong baseline for PLL. We also address the question of how and why such a naive model works well with deep neural networks. Our empirical results indicate that deep neural networks trained on partially labeled examples generalize very well even in the over-parametrized regime and without label disambiguations or regularizations. We point out that existing learning theories on PLL are vacuous in the over-parametrized regime. Hence they cannot explain why the deep naive method works. We propose an alternative theory on how deep learning generalize in PLL problems.",0
"This paper presents a new approach to deep learning called ""deep but naive partial label learning."" Our method enables state-of-the-art performance on popular benchmark datasets while using only a fraction of the labeled data typically required by other methods. By leveraging unlabeled data via weak supervision signals and regularization, we show that our model can achieve comparable results to full-supervised models. We evaluate our method on several image classification tasks and show consistent improvement over previous work, achieving near state-of-the-art accuracy on CIFAR-10 and SVHN despite using just one percent of the labels available for those datasets. Finally, we conduct experiments to demonstrate the importance of each component in our framework, as well as ablation studies to compare against prior art. Overall, our approach offers a promising alternative to traditional fully supervised deep learning and has the potential to significantly reduce annotation costs, particularly important when annotating large amounts of data such as images or videos.",1
"Counterfactual explanations (CFE) are being widely used to explain algorithmic decisions, especially in consequential decision-making contexts (e.g., loan approval or pretrial bail). In this context, CFEs aim to provide individuals affected by an algorithmic decision with the most similar individual (i.e., nearest individual) with a different outcome. However, while an increasing number of works propose algorithms to compute CFEs, such approaches either lack in optimality of distance (i.e., they do not return the nearest individual) and perfect coverage (i.e., they do not provide a CFE for all individuals); or they cannot handle complex models, such as neural networks. In this work, we provide a framework based on Mixed-Integer Programming (MIP) to compute nearest counterfactual explanations with provable guarantees and with runtimes comparable to gradient-based approaches. Our experiments on the Adult, COMPAS, and Credit datasets show that, in contrast with previous methods, our approach allows for efficiently computing diverse CFEs with both distance guarantees and perfect coverage.",0
"This study presents a method for scaling guarantees for nearest counterfactual explanations. The proposed approach builds upon existing work on feature importance measures and allows for computationally efficient evaluation across high-dimensional input spaces. Our results demonstrate that our method is both effective and scalable, enabling meaningful analysis of complex models even as input dimensionality grows large. In particular, we show that our method yields accurate and interpretable explanations even for high-resolution images. These findings have important implications for explainability research, particularly with respect to the tractability of explanations for deep learning models under realistic constraints.",1
"Agricultural image recognition tasks are becoming increasingly dependent on deep learning (DL). Despite its excellent performance, it is difficult to comprehend what type of logic or features DL uses in its decision making. This has become a roadblock for the implementation and development of DL-based image recognition methods because knowing the logic or features used in decision making, such as in a classification task, is very important for verification, algorithm improvement, training data improvement, knowledge extraction, etc. To mitigate such problems, we developed a classification method based on a variational autoencoder architecture that can show not only the location of the most important features but also what variations of that particular feature are used. Using the PlantVillage dataset, we achieved an acceptable level of explainability without sacrificing the accuracy of the classification. Although the proposed method was tested for disease diagnosis in some crops, the method can be extended to other crops as well as other image classification tasks. In the future, we hope to use this explainable artificial intelligence algorithm in disease identification tasks, such as the identification of potato blackleg disease and potato virus Y (PVY), and other image classification tasks.",0
"In agriculture, plant diseases pose significant challenges that require accurate diagnosis and management. Automating the process of disease classification could greatly improve efficiency and effectiveness in managing these threats. This research addresses the challenge of achieving explainability in deep learning models used for plant disease classification by utilizing disentangled variational autoencoders (VAE). VAEs provide a powerful tool for generating interpretable features from complex datasets while minimizing their dimensions. By incorporating VAEs into the modeling process, we aim to facilitate interpretability of decisions made during plant disease diagnosis, thereby increasing transparency and trustworthiness in artificial intelligence applications in agricultural science. Our findings demonstrate the potential of using disentangled VAEs as a means of improving explainability in deep learning models applied to plant disease classification tasks. We expect our research to contribute valuable insights towards enhancing the understanding of deep learning models and fostering confidence in decision-making processes involving artificial intelligence in agriculture. Ultimately, we hope that our work will pave the way for more transparent and reliable AI applications in agriculture and other related domains.",1
"Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudo labels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning methods for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we have a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make substantial progress.",0
"In recent years, contrastive learning has emerged as a powerful technique for self-supervised representation learning, enabling artificial intelligence (AI) models to learn high quality representations without relying on labeled data. By maximizing agreement between positive pairs (similar inputs such as augmentations of the same image) while minimizing agreement between negative pairs (dissimilar inputs), these methods have achieved state-of-the-art performance on a wide range of tasks, including computer vision, natural language processing, and even robotics. Despite their successes, there remains significant room for improvement in our understanding of how to design effective contrastive learning algorithms. To address this gap, we conduct a survey that provides an overview of existing approaches and identifies key research directions for future work. Our goal is to provide guidance for both newcomers to the field and experienced practitioners who want to stay up-to-date on the latest advances. We discuss several topics central to the development and analysis of contrastive learning algorithms, focusing on important design choices and evaluation metrics. In addition, we offer insights into potential extensions of these techniques beyond traditional supervised learning benchmarks. Ultimately, we aim to encourage further progress in this rapidly growing area by inspiring fresh perspectives on fundamental challenges and opportunities within contrastive learning itself.",1
"We introduce Explearn, an online algorithm that learns to jointly output predictions and explanations for those predictions. Explearn leverages Gaussian Processes (GP)-based contextual bandits. This brings two key benefits. First, GPs naturally capture different kinds of explanations and enable the system designer to control how explanations generalize across the space by virtue of choosing a suitable kernel. Second, Explearn builds on recent results in contextual bandits which guarantee convergence with high probability. Our initial experiments hint at the promise of the approach.",0
"This paper presents a new algorithm called ""Bandits for Learning to Explain (BLE)"" that addresses one of the key challenges facing explainability in artificial intelligence: generating meaningful explanations of complex models trained on large datasets. BLE uses bandit algorithms, which balance exploration and exploitation, to learn from user feedback on generated explanations. Our experiments show that BLE outperforms state-of-the art methods across several metrics including precision, recall, F1 score, and average rank correlations with human judgment. BLE also significantly reduces the number of model queries required compared to previous approaches. The results demonstrate the effectiveness of using reinforcement learning for improving the quality of explanations. Overall, BLE represents a significant advance towards creating more interpretable machine learning systems.",1
"In this work we study generalization guarantees for the metric learning problem, where the metric is induced by a neural network type embedding of the data. Specifically, we provide uniform generalization bounds for two regimes -- the sparse regime, and a non-sparse regime which we term \emph{bounded amplification}. The sparse regime bounds correspond to situations where $\ell_1$-type norms of the parameters are small. Similarly to the situation in classification, solutions satisfying such bounds can be obtained by an appropriate regularization of the problem. On the other hand, unregularized SGD optimization of a metric learning loss typically does not produce sparse solutions. We show that despite this lack of sparsity, by relying on a different, new property of the solutions, it is still possible to provide dimension free generalization guarantees. Consequently, these bounds can explain generalization in non sparse real experimental situations. We illustrate the studied phenomena on the MNIST and 20newsgroups datasets.",0
"This should contain all important aspects of the paper but no specific examples. Be concise yet accurate and comprehensive. Title: Abstract  Dimensionality reduction techniques are widely used to solve problems related to large data sets, where high-dimensional input variables make traditional methods difficult to use. In many cases, these techniques work well, and have been shown to improve accuracy over linear models. However, nonlinear dimensionality reduction (NLDR) can introduce additional challenges that may compromise performance, especially when training and test distributions differ substantially. To address this issue, we present two algorithms designed specifically for linear metric learning tasks based on kernel principal component analysis (KPCA), which we name Kernel MMD-Regularized Linear Discriminant Analysis (KMLDA) and Kernal Maximum Mean Discrepancy Regression(KMMDR). We evaluate their performance against several other state-of-the-art NLDR techniques on both synthetic data and real datasets such as MNIST, SST2, and Amazon Review. Our results demonstrate that both our proposed approaches achieve significantly better generalization bounds compared to benchmarked competitors. For example, when using the CIFAR-10 dataset, KMLDA outperforms the second-best approach by 6 percentage points in terms of test accuracy. Additionally, on a more complex task, sentiment classification from movie reviews collected on Amazon, MLDA yields marginally superior results against the state-of-the-art method by +/-1%. Overall, our contributions in this paper provide new insights into improved generalization bound guarantees for NLDR applications under limited labeled data regimes.",1
"Adaptive Momentum Estimation (Adam), which combines Adaptive Learning Rate and Momentum, is the most popular stochastic optimizer for accelerating the training of deep neural networks. However, empirically Adam often generalizes worse than Stochastic Gradient Descent (SGD). We unveil the mystery of this behavior based on the diffusion theoretical framework. Specifically, we disentangle the effects of Adaptive Learning Rate and Momentum of the Adam dynamics on saddle-point escaping and minima selection. We prove that Adaptive Learning Rate can escape saddle points efficiently, but cannot select flat minima as SGD does. In contrast, Momentum provides a drift effect to help the training process pass through saddle points, and almost does not affect flat minima selection. This theoretically explains why SGD (with Momentum) generalizes better, while Adam generalizes worse but converges faster. Furthermore, motivated by the analysis, we design a novel adaptive optimization framework named Adaptive Inertia, which uses parameter-wise adaptive inertia to accelerate the training and provably favors flat minima as well as SGD. Our extensive experiments demonstrate that the proposed adaptive inertia method can generalize significantly better than SGD and conventional adaptive gradient methods.",0
"This research investigates the impact of adaptive learning rate (ALR) on momentum dynamics by comparing the effects of ALR and fixed learning rate (FLR), as well as combining them with either no momentum (NMOM) or standard Nesterov momentum (SNM). Four variants of stochastic gradient descent (SGD) were used â€“ FLR+SNM, FLR+NMOM, ALR+SNM, and ALR+NMOM â€“ to train simple neural networks on several different tasks using three datasets from diverse domains. Our results indicate that ALR significantly reduces training time compared to both FLR and FLR combined with either type of momentu...",1
"Diverse domains of science and engineering require and use mechanistic mathematical models, e.g. systems of differential algebraic equations. Such models often contain uncertain parameters to be estimated from data. Consider a dynamic model discrimination setting where we wish to chose: (i) what is the best mechanistic, time-varying model and (ii) what are the best model parameter estimates. These tasks are often termed model discrimination/selection/validation/verification. Typically, several rival mechanistic models can explain data, so we incorporate available data and also run new experiments to gather more data. Design of dynamic experiments for model discrimination helps optimally collect data. For rival mechanistic models where we have access to gradient information, we extend existing methods to incorporate a wider range of problem uncertainty and show that our proposed approach is equivalent to historical approaches when limiting the types of considered uncertainty. We also consider rival mechanistic models as dynamic black boxes that we can evaluate, e.g. by running legacy code, but where gradient or other advanced information is unavailable. We replace these black-box models with Gaussian process surrogate models and thereby extend the model discrimination setting to additionally incorporate rival black-box model. We also explore the consequences of using Gaussian process surrogates to approximate gradient-based methods.",0
"This work presents new techniques for designing experiments with black box models whose performance can vary over time. We show how careful choice of model architecture and hyperparameters can be used to generate interpretable outputs that reveal valuable insights into system behavior. Our approach combines statistical learning theory with numerical optimization methods to efficiently characterize complex systems based on noisy observations. Through simulation studies and real world applications, we demonstrate that our algorithms significantly outperform current state-of-the-art methods in terms of accuracy, robustness, and computational efficiency. These advances have important implications for many fields including control engineering, computer vision, machine learning, robotics, and finance, among others. Overall, this research represents a major step forward towards reliable decision making under uncertainty and paves the way for further developments in data science and artificial intelligence.",1
"Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games). In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).",0
"In this work we present UPDeT (Universal Multi-agent Reinforcement Learning via Policy Decoupling with Transformers), a new method which achieves state of the art performance on a wide range of multi agent reinforcement learning tasks. Our approach builds upon recent advances in actor critic methods for single agent settings by decoupling the policy network into two parts; one that maps states directly to actions, and another that processes the value function estimates from the first part. We further extend this approach to allow different models to process data from each agent independently before combining them together to predict joint action probabilities. This allows us to learn policies which are more robust to changes in environment or team composition. Finally, we propose several improvements over traditional self play algorithms to make training faster and reduce variance. Empirical evaluation shows that our method significantly outperforms current state of the art approaches both in terms of convergence speed and final performance across a diverse set of challenging benchmark problems.",1
"Ongoing efforts to understand deep neural networks (DNN) have provided many insights, but DNNs remain incompletely understood. Improving DNN's interpretability has practical benefits, such as more accountable usage, better algorithm maintenance and improvement. The complexity of dataset structure may contribute to the difficulty in solving interpretability problem arising from DNN's black-box mechanism. Thus, we propose to use pattern theory formulated by Ulf Grenander, in which data can be described as configurations of fundamental objects that allow us to investigate convolutional neural network's (CNN) interpretability in a component-wise manner. Specifically, U-Net-like structure is formed by attaching expansion blocks (EB) to ResNet, allowing it to perform semantic segmentation-like tasks at its EB output channels designed to be compatible with pattern theory's configurations. Through these modules, some heatmap-based explainable artificial intelligence (XAI) methods will be shown to extract explanations w.r.t individual generators that make up a single data sample, potentially reducing the impact of dataset's complexity to interpretability problem. The MNIST-equivalent dataset containing pattern theory's elements is designed to facilitate smoother entry into this framework, along which the theory's generative aspect is naturally presented.",0
"This paper addresses the challenges involved in making convolutional neural networks (CNNs) interpretable by formulating them using general pattern theory. CNNs have achieved state-of-the-art results across many applications but suffer from a lack of interpretability. With general pattern theory, we aim to provide insights into how and why certain features are detected and emphasized by the network. By analyzing patterns within images as they pass through multiple convolutional layers, we can uncover key characteristics that influence the prediction process. Furthermore, we investigate visualization techniques such as feature attribution maps to gain deeper understanding of the internals of CNNs. Our approach is evaluated on different datasets, demonstrating improved comprehension and accuracy compared to baseline methods. Ultimately, this research seeks to bring forth new perspectives on model interpretation that could benefit both developers working with CNNs and users relying on their predictions.",1
"Inspired by the progress of the End-to-End approach [1], this paper systematically studies the effects of Number of Filters of convolutional layers on the model prediction accuracy of CNN+RNN (Convolutional Neural Networks adding to Recurrent Neural Networks) for ASR Models (Automatic Speech Recognition). Experimental results show that only when the CNN Number of Filters exceeds a certain threshold value is adding CNN to RNN able to improve the performance of the CNN+RNN speech recognition model, otherwise some parameter ranges of CNN can render it useless to add the CNN to the RNN model. Our results show a strong dependency of word accuracy on the Number of Filters of convolutional layers. Based on the experimental results, the paper suggests a possible hypothesis of Sound-2-Vector Embedding (Convolutional Embedding) to explain the above observations.   Based on this Embedding hypothesis and the optimization of parameters, the paper develops an End-to-End speech recognition system which has a high word accuracy but also has a light model-weight. The developed LVCSR (Large Vocabulary Continuous Speech Recognition) model has achieved quite a high word accuracy of 90.2% only by its Acoustic Model alone, without any assistance from intermediate phonetic representation and any Language Model. Its acoustic model contains only 4.4 million weight parameters, compared to the 35~68 million acoustic-model weight parameters in DeepSpeech2 [2] (one of the top state-of-the-art LVCSR models) which can achieve a word accuracy of 91.5%. The light-weighted model is good for improving the transcribing computing efficiency and also useful for mobile devices, Driverless Vehicles, etc. Our model weight is reduced to ~10% the size of DeepSpeech2, but our model accuracy remains close to that of DeepSpeech2. If combined with a Language Model, our LVCSR system is able to achieve 91.5% word accuracy.",0
"This study examines how the number of filters used in convolutional layers impacts speech recognition model accuracy. Previous research has suggested that there may be an optimal number of filters, but until now, no systematic analysis has been conducted to explore this question further. Our experiments use several popular datasets for training and evaluating speech recognition models, including TIMIT and Wall Street Journal (WSJ) corpus. We investigate different numbers of filters ranging from one filter per channel to eight filters per channel. Our results show that while using more filters generally improves model performance, there exists a point of diminishing returns beyond which adding additional filters provides little benefit. Additionally, we observe that increasing the size of individual filters can also lead to improved model accuracy without necessarily increasing the total number of filters. Overall, our findings provide valuable insights into the design tradeoffs involved when building efficient and accurate speech recognition systems.",1
"The use of machine learning to develop intelligent software tools for interpretation of radiology images has gained widespread attention in recent years. The development, deployment, and eventual adoption of these models in clinical practice, however, remains fraught with challenges. In this paper, we propose a list of key considerations that machine learning researchers must recognize and address to make their models accurate, robust, and usable in practice. Namely, we discuss: insufficient training data, decentralized datasets, high cost of annotations, ambiguous ground truth, imbalance in class representation, asymmetric misclassification costs, relevant performance metrics, generalization of models to unseen datasets, model decay, adversarial attacks, explainability, fairness and bias, and clinical validation. We describe each consideration and identify techniques to address it. Although these techniques have been discussed in prior research literature, by freshly examining them in the context of medical imaging and compiling them in the form of a laundry list, we hope to make them more accessible to researchers, software developers, radiologists, and other stakeholders.",0
"Despite the increasing use of machine learning models in radiological practice, there are several key technology considerations that must be taken into account during their development and deployment. These considerations include data quality and availability, computational infrastructure, model validation, workflow integration, and regulatory compliance. Additionally, it is important to recognize potential biases within the datasets used to train these models and ensure they do not perpetuate discrimination against certain patient populations. Successful implementation of machine learning in radiology requires careful planning and attention to these technological factors. This paper provides an overview of these critical components and discusses strategies to address them in order to achieve optimal outcomes from machine learning applications in clinical radiologic imaging.",1
"This paper aims to explain deep neural networks (DNNs) from the perspective of multivariate interactions. In this paper, we define and quantify the significance of interactions among multiple input variables of the DNN. Input variables with strong interactions usually form a coalition and reflect prototype features, which are memorized and used by the DNN for inference. We define the significance of interactions based on the Shapley value, which is designed to assign the attribution value of each input variable to the inference. We have conducted experiments with various DNNs. Experimental results have demonstrated the effectiveness of the proposed method.",0
"Abstract:  In recent years, deep neural networks (DNNs) have become increasingly popular as models for predictive tasks across many different fields. However, one of the main challenges associated with these types of models is their lack of interpretability, which can make it difficult for users to understand how they are making predictions. One approach to addressing this issue is through the use of feature attribution methods, which aim to explain how each input feature contributes to the model's decision. In particular, the multivariate Shapley interaction value is a widely used method that provides insight into interactions between features by quantifying the contribution of specific combinations of features to the model's output. Despite its widespread adoption, interpreting the results from the multivariate Shapley interaction values can be a challenge due to the large number of possible feature interactions and high computational costs involved in evaluating them all. This work presents new techniques for efficiently computing and interpreting multivariate Shapley interaction values for DNNs and demonstrates their utility on several real-world datasets. Our findings show that these methods provide valuable insights into complex feature interactions and can help to improve our understanding of how DNNs operate. Overall, this study represents an important step forward towards developing more transparent machine learning systems.",1
"Recent studies have reported biases in machine learning image classifiers, especially against particular demographic groups. Counterfactual examples for an input -- perturbations that change specific features but not others -- have been shown to be useful for evaluating explainability and fairness of machine learning models. However, generating counterfactual examples for images is non-trivial due to the underlying causal structure governing the various features of an image. To be meaningful, generated perturbations need to satisfy constraints implied by the causal model. We present a method for generating counterfactuals by incorporating a structural causal model (SCM) in a novel improved variant of Adversarially Learned Inference (ALI), that generates counterfactuals in accordance with the causal relationships between different attributes of an image. Based on the generated counterfactuals, we show how to evaluate bias and explain a pre-trained machine learning classifier. We also propose a counterfactual regularizer that can mitigate bias in the classifier. On the Morpho-MNIST dataset, our method generates counterfactuals comparable in quality to prior work on SCM-based counterfactuals. Our method also works on the more complex CelebA faces dataset; generated counterfactuals are indistinguishable from original images in a human evaluation experiment. As a downstream task, we use counterfactuals to evaluate a standard classifier trained on CelebA data and show that it is biased w.r.t. skin and hair color, and show how counterfactual regularization can be used to remove the identified biases.",0
"Image classification algorithms have become ubiquitous in modern technology due to their ability to accurately identify objects within images. However, despite their impressive performance, these algorithms can still exhibit biases that reflect societal prejudices. This study seeks to evaluate the presence of such bias in image classifiers by adopting a causal perspective using counterfactual analysis. By doing so, we aim to shed light on how certain attributes of an object influence the algorithmâ€™s decision-making process and potentially perpetuate harmful biases. We hope our findings will inform future efforts to mitigate these biases and lead to more equitable and inclusive technology.",1
"Humans learn adaptively and efficiently throughout their lives. However, incrementally learning tasks causes artificial neural networks to overwrite relevant information learned about older tasks, resulting in 'Catastrophic Forgetting'. Efforts to overcome this phenomenon often utilize resources poorly, for instance, by growing the network architecture or needing to save parametric importance scores, or violate data privacy between tasks. To tackle this, we propose SPACE, an algorithm that enables a network to learn continually and efficiently by partitioning the learnt space into a Core space, that serves as the condensed knowledge base over previously learned tasks, and a Residual space, which is akin to a scratch space for learning the current task. After learning each task, the Residual is analyzed for redundancy, both within itself and with the learnt Core space. A minimal number of extra dimensions required to explain the current task are added to the Core space and the remaining Residual is freed up for learning the next task. We evaluate our algorithm on P-MNIST, CIFAR and a sequence of 8 different datasets, and achieve comparable accuracy to the state-of-the-art methods while overcoming catastrophic forgetting. Additionally, our algorithm is well suited for practical use. The partitioning algorithm analyzes all layers in one shot, ensuring scalability to deeper networks. Moreover, the analysis of dimensions translates to filter-level sparsity, and the structured nature of the resulting architecture gives us up to 5x improvement in energy efficiency during task inference over the current state-of-the-art.",0
"In recent years, continual learning has emerged as a key challenge in artificial intelligence, aiming to enable machines to learn continuously over time from data streams or incremental updates without forgetting previously learned knowledge. One major obstacle facing continual learning methods is catastrophic forgetting, which occurs when new tasks interfere with previous ones and degrade overall performance.  To address this issue, we propose a novel framework called SPACE (Structured Compression and Sharing of Representational Space for Continual Learning). Our approach leverages the intrinsic structural information present in deep neural networks by explicitly modeling the shared latent space across different tasks. We devise two distinct mechanisms, namely progressive compression and selective sharing, that jointly alleviate catastrophic forgetting while preserving task-specific information.  Our main contributions can be summarized as follows: (1) We introduce a structured method that encodes the relationships among multiple tasks into a compact yet informative latent subspace. This space facilitates efficient memory management, reducing interference between consecutive tasks while promoting cross-task synergies. (2) To strike a balance between retention of past knowledge and adaptation to newly encountered tasks, our framework employs a multi-level architecture integrating separate network branches tailored for individual tasks. These partitions concurrently share and refine their underlying compressed representations using attention modules designed for mining task-related patterns. (3) Thorough experiments on standard benchmark datasets covering diverse domains demonstrate the effectiveness and robustness of our proposal against state-of-the-art competitors under various evaluation metrics. Ablation studies further validate the benefits associated with each component of our framework.  In summary, our proposed SPACE framework addresses catastrophic forgetting in continual learning via a structure",1
"This paper investigates the prospects of using directive explanations to assist people in achieving recourse of machine learning decisions. Directive explanations list which specific actions an individual needs to take to achieve their desired outcome. If a machine learning model makes a decision that is detrimental to an individual (e.g. denying a loan application), then it needs to both explain why it made that decision and also explain how the individual could obtain their desired outcome (if possible). At present, this is often done using counterfactual explanations, but such explanations generally do not tell individuals how to act. We assert that counterfactual explanations can be improved by explicitly providing people with actions they could use to achieve their desired goal. This paper makes two contributions. First, we present the results of an online study investigating people's perception of directive explanations. Second, we propose a conceptual model to generate such explanations. Our online study showed a significant preference for directive explanations ($p0.001$). However, the participants' preferred explanation type was affected by multiple factors, such as individual preferences, social factors, and the feasibility of the directives. Our findings highlight the need for a human-centred and context-specific approach for creating directive explanations.",0
"In recent years, there has been growing interest in developing explainable machine learning algorithms that can provide insight into how these systems make decisions. One promising approach to achieve actionable explanations is through directive explanation, which focuses on providing clear instructions that humans can follow to replicate the decision process or improve their behavior based on the modelâ€™s recommendations. This paper presents a framework for directives that aim to enhance transparency by breaking down complex models into smaller pieces whose components can be understood intuitively by human users. We demonstrate how our framework enables both interactive exploration of decisions made by deep neural networks using simple rules as well as facilitates decision-making under uncertainty via counterfactual reasoning. Our evaluations show that directive explanation outperforms other forms of explanation such as rule lists in terms of ease of use and understanding. Our work contributes new methods for generating effective directed explanation from powerful machine learning systems, enabling users to better comprehend automated decision making. By demonstrating improvements over traditional post hoc approaches, our research provides a step towards realizing explainability that directly benefits endusers of complex AI models.",1
"We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple ""copy-paste"" adversarial examples that change model behavior in predictable ways.",0
"Abstract: This study examines the use of compositional explanations in neuroscience research. Specifically, we explore how these types of explanations can be applied to neuronal activity in order to better understand neural processes. We discuss several examples from recent literature that demonstrate the utility of compositional approaches for explaining complex systems and phenomena. In addition, we highlight some potential limitations and challenges associated with using such methods in neurobiology. Overall, our analysis suggests that compositional explanations have the potential to provide valuable insights into neural function, but further work is needed to fully realize their potential. Keywords: compositional explanations, neuronal activity, neural processing, neuroscience research",1
"Recent advancements in machine learning and signal processing domains have resulted in an extensive surge of interest in Deep Neural Networks (DNNs) due to their unprecedented performance and high accuracy for different and challenging problems of significant engineering importance. However, when such deep learning architectures are utilized for making critical decisions such as the ones that involve human lives (e.g., in control systems and medical applications), it is of paramount importance to understand, trust, and in one word ""explain"" the argument behind deep models' decisions. In many applications, artificial neural networks (including DNNs) are considered as black-box systems, which do not provide sufficient clue on their internal processing actions. Although some recent efforts have been initiated to explain the behaviors and decisions of deep networks, explainable artificial intelligence (XAI) domain, which aims at reasoning about the behavior and decisions of DNNs, is still in its infancy. The aim of this paper is to provide a comprehensive overview on Understanding, Visualization, and Explanation of the internal and overall behavior of DNNs.",0
"Despite their impressive accuracy at many real world tasks, deep neural networks (DNN) can often make decisions that even trained users cannot interpret. This makes DNN explainability an important but challenging problem in modern machine learning research. In our survey we review current methods in explaining how DNN classify inputs, the most popular visualization techniques, as well as some recent work on human interpretable explanations. We discuss tradeoffs among different approaches based upon user goals which typically fall into three categories: understanding the decision making process itself, trustworthiness via transparency, and facilitating human control over automated systems. With the field rapidly advancing new developments can be expected. We conclude by summarizing open problems and future directions for continued progress towards more transparent, intelligible deep neural nets and artificial intelligence applications generally.",1
"With our previous study, the Super-k algorithm, we have introduced a novel way of piecewise-linear classification. While working on the Super-k algorithm, we have found that there is a similar, and simpler way to explain for obtaining a piecewise-linear classifier based on Voronoi tessellations. Replacing the multidimensional voxelization and expectation-maximization stages of the algorithm with a distance-based clustering algorithm, preferably k-means, works as well as the prior approach. Since we are replacing the voxelization with the clustering, we have found it meaningful to name the modified algorithm, with respect to Super-k, as Supervised k Clusters or in short Super-klust. Similar to the Super-k algorithm, the Super-klust algorithm covers data with a labeled Voronoi tessellation, and uses resulting tessellation for classification. According to the experimental results, the Super-klust algorithm has similar performance characteristics with the Super-k algorithm.",0
