"Multi-objective task scheduling (MOTS) is the task scheduling while optimizing multiple and possibly contradicting constraints. A challenging extension of this problem occurs when every individual task is a multi-objective optimization problem by itself. While deep reinforcement learning (DRL) has been successfully applied to complex sequential problems, its application to the MOTS domain has been stymied by two challenges. The first challenge is the inability of the DRL algorithm to ensure that every item is processed identically regardless of its position in the queue. The second challenge is the need to manage large queues, which results in large neural architectures and long training times. In this study we present MERLIN, a robust, modular and near-optimal DRL-based approach for multi-objective task scheduling. MERLIN applies a hierarchical approach to the MOTS problem by creating one neural network for the processing of individual tasks and another for the scheduling of the overall queue. In addition to being smaller and with shorted training times, the resulting architecture ensures that an item is processed in the same manner regardless of its position in the queue. Additionally, we present a novel approach for efficiently applying DRL-based solutions on very large queues, and demonstrate how we effectively scale MERLIN to process queue sizes that are larger by orders of magnitude than those on which it was trained. Extensive evaluation on multiple queue sizes show that MERLIN outperforms multiple well-known baselines by a large margin (>22%).",0
"Multi-objective task scheduling (MOTS) involves optimizing conflicting constraints while scheduling tasks. This problem becomes more complex when individual tasks themselves are multi-objective optimization problems. Despite the success of deep reinforcement learning (DRL) in sequential problems, two challenges hinder its application in MOTS. The first challenge is the difficulty of ensuring identical processing of every item in the queue, regardless of its position. The second challenge is the management of large queues, which requires extensive training times and neural architectures. In this study, we propose MERLIN, a modular and efficient DRL-based approach for multi-objective task scheduling. MERLIN uses a hierarchical approach by creating separate neural networks for individual task processing and queue scheduling. This approach ensures that items are processed consistently and efficiently, regardless of their position in the queue, and allows for efficient scaling to larger queue sizes. Our evaluation shows that MERLIN outperforms well-known baselines by over 22% on multiple queue sizes.",1
"We consider the problem of batch multi-task reinforcement learning with observed context descriptors, motivated by its application to personalized medical treatment. In particular, we study two general classes of learning algorithms: direct policy learning (DPL), an imitation-learning based approach which learns from expert trajectories, and model-based learning. First, we derive sample complexity bounds for DPL, and then show that model-based learning from expert actions can, even with a finite model class, be impossible. After relaxing the conditions under which the model-based approach is expected to learn by allowing for greater coverage of state-action space, we provide sample complexity bounds for model-based learning with finite model classes, showing that there exist model classes with sample complexity exponential in their statistical complexity. We then derive a sample complexity upper bound for model-based learning based on a measure of concentration of the data distribution. Our results give formal justification for imitation learning over model-based learning in this setting.",0
"The focus of our study is on batch multi-task reinforcement learning that involves context descriptors. We are interested in its application to personalized medical treatment. We examine two types of learning algorithms: direct policy learning (DPL), which uses imitation-learning and learns from expert trajectories, and model-based learning. Initially, we establish the sample complexity bounds for DPL. We then demonstrate that the model-based approach may not be feasible, even with a finite model class, when learning from expert actions. However, we relax the conditions and allow for a more extensive coverage of the state-action space, which enables us to provide sample complexity bounds for model-based learning with finite model classes. We also show that some model classes have sample complexity that is exponential in their statistical complexity. Finally, we determine a sample complexity upper bound for model-based learning based on the concentration of the data distribution. In conclusion, our findings provide a solid basis for preferring imitation learning over model-based learning in this specific context.",1
"In this paper, we introduce a new reinforcement learning (RL) based neural architecture search (NAS) methodology for effective and efficient generative adversarial network (GAN) architecture search. The key idea is to formulate the GAN architecture search problem as a Markov decision process (MDP) for smoother architecture sampling, which enables a more effective RL-based search algorithm by targeting the potential global optimal architecture. To improve efficiency, we exploit an off-policy GAN architecture search algorithm that makes efficient use of the samples generated by previous policies. Evaluation on two standard benchmark datasets (i.e., CIFAR-10 and STL-10) demonstrates that the proposed method is able to discover highly competitive architectures for generally better image generation results with a considerably reduced computational burden: 7 GPU hours. Our code is available at https://github.com/Yuantian013/E2GAN.",0
"This paper presents a novel approach to generative adversarial network (GAN) architecture search using a reinforcement learning (RL) based neural architecture search (NAS) methodology. The main idea is to treat the GAN architecture search problem as a Markov decision process (MDP) to enable smoother architecture sampling and a more effective RL-based search algorithm that targets the potential global optimal architecture. To improve efficiency, an off-policy GAN architecture search algorithm is utilized to efficiently use samples generated by previous policies. The proposed method is evaluated on two benchmark datasets (CIFAR-10 and STL-10) and is shown to discover highly competitive architectures with generally better image generation results while significantly reducing computational burden to 7 GPU hours. The code for this method is available at https://github.com/Yuantian013/E2GAN.",1
"Offline reinforcement learning (RL purely from logged data) is an important avenue for deploying RL techniques in real-world scenarios. However, existing hyperparameter selection methods for offline RL break the offline assumption by evaluating policies corresponding to each hyperparameter setting in the environment. This online execution is often infeasible and hence undermines the main aim of offline RL. Therefore, in this work, we focus on \textit{offline hyperparameter selection}, i.e. methods for choosing the best policy from a set of many policies trained using different hyperparameters, given only logged data. Through large-scale empirical evaluation we show that: 1) offline RL algorithms are not robust to hyperparameter choices, 2) factors such as the offline RL algorithm and method for estimating Q values can have a big impact on hyperparameter selection, and 3) when we control those factors carefully, we can reliably rank policies across hyperparameter choices, and therefore choose policies which are close to the best policy in the set. Overall, our results present an optimistic view that offline hyperparameter selection is within reach, even in challenging tasks with pixel observations, high dimensional action spaces, and long horizon.",0
"The utilization of offline reinforcement learning (RL) from pre-recorded data is crucial for implementing RL techniques in real-life situations. However, current methods for selecting hyperparameters for offline RL violate the offline assumption by assessing policies that correspond to each hyperparameter configuration in the environment. Such online execution is often impractical, which undermines the primary objective of offline RL. Consequently, this study concentrates on selecting optimal policies from a range of policies trained using different hyperparameters, based solely on logged data, i.e., offline hyperparameter selection. Our comprehensive empirical evaluation reveals that: 1) hyperparameter choices significantly affect offline RL algorithms, 2) factors such as the offline RL algorithm and Q value estimation method can substantially impact hyperparameter selection, and 3) with careful control of these factors, we can accurately rank policies across hyperparameter choices and select policies that closely resemble the best policy in the set. In summary, our findings demonstrate the feasibility of offline hyperparameter selection, even in challenging tasks that involve pixel observations, high-dimensional action spaces, and long horizons.",1
"The act of explaining across two parties is a feedback loop, where one provides information on what needs to be explained and the other provides an explanation relevant to this information. We apply a reinforcement learning framework which emulates this format by providing explanations based on the explainee's current mental model. We conduct novel online human experiments where explanations generated by various explanation methods are selected and presented to participants, using policies which observe participants' mental models, in order to optimize an interpretability proxy. Our results suggest that mental model-based policies (anchored in our proposed state representation) may increase interpretability over multiple sequential explanations, when compared to a random selection baseline. This work provides insight into how to select explanations which increase relevant information for users, and into conducting human-grounded experimentation to understand interpretability.",0
"The exchange of information between two parties during an explanation is like a feedback loop. One party provides the information that needs to be explained, while the other offers an explanation that is relevant to that information. To mimic this process, we utilize a reinforcement learning framework that generates explanations based on the recipient's current understanding. We conducted online experiments with human subjects to compare different explanation methods. Our policies observed the participants' mental models to optimize the interpretability of the explanations presented. Our findings indicate that mental model-based policies, using the state representation we proposed, can enhance interpretability over multiple sequential explanations compared to a random selection baseline. This research provides valuable insights into selecting explanations that deliver relevant information to users, as well as the importance of conducting human-centered experiments to assess interpretability.",1
This paper presents an approach to exploring a multi-objective reinforcement learning problem with Model-Agnostic Meta-Learning. The environment we used consists of a 2D vehicle equipped with a LIDAR sensor. The goal of the environment is to reach some pre-determined target location but also effectively avoid any obstacles it may find along its path. We also compare this approach against a baseline TD3 solution that attempts to solve the same problem.,0
"In this paper, we propose utilizing Model-Agnostic Meta-Learning to tackle a multi-objective reinforcement learning problem. Our approach involves employing a 2D vehicle equipped with a LIDAR sensor in a specific environment. The objective is to reach a predetermined target location while effectively avoiding obstacles in its path. Furthermore, we conduct a comparison between our proposed approach and a baseline TD3 solution that aims to solve the same problem.",1
"Deep reinforcement learning includes a broad family of algorithms that parameterise an internal representation, such as a value function or policy, by a deep neural network. Each algorithm optimises its parameters with respect to an objective, such as Q-learning or policy gradient, that defines its semantics. In this work, we propose an algorithm based on meta-gradient descent that discovers its own objective, flexibly parameterised by a deep neural network, solely from interactive experience with its environment. Over time, this allows the agent to learn how to learn increasingly effectively. Furthermore, because the objective is discovered online, it can adapt to changes over time. We demonstrate that the algorithm discovers how to address several important issues in RL, such as bootstrapping, non-stationarity, and off-policy learning. On the Atari Learning Environment, the meta-gradient algorithm adapts over time to learn with greater efficiency, eventually outperforming the median score of a strong actor-critic baseline.",0
"The family of algorithms known as deep reinforcement learning employs a deep neural network to parameterize an internal representation, like a policy or value function, which is optimized with respect to an objective, such as Q-learning or policy gradient. Our proposal is an algorithm that utilizes meta-gradient descent to discover its own objective, which can be flexibly parameterized by a deep neural network, based on interactive experience with its environment. This allows the agent to learn how to learn more effectively over time, with the ability to adapt to changes as they occur. We demonstrate that this algorithm addresses important issues in RL, such as non-stationarity, bootstrapping, and off-policy learning. The meta-gradient algorithm adapts over time to learn with greater efficiency and eventually outperforms a strong actor-critic baseline's median score on the Atari Learning Environment.",1
"We propose a novel adaptive transfer learning framework, learning to transfer learn (L2TL), to improve performance on a target dataset by careful extraction of the related information from a source dataset. Our framework considers cooperative optimization of shared weights between models for source and target tasks, and adjusts the constituent loss weights adaptively. The adaptation of the weights is based on a reinforcement learning (RL) selection policy, guided with a performance metric on the target validation set. We demonstrate that L2TL outperforms fine-tuning baselines and other adaptive transfer learning methods on eight datasets. In the regimes of small-scale target datasets and significant label mismatch between source and target datasets, L2TL shows particularly large benefits.",0
"Our proposed approach, called Learning to Transfer Learn (L2TL), is a unique adaptive transfer learning framework that enhances performance on a target dataset by strategically extracting relevant information from a source dataset. Our framework optimizes shared weights between models for both source and target tasks and dynamically adjusts constituent loss weights. We use a reinforcement learning (RL) selection policy to adapt weight adjustments based on target validation set performance. Our results demonstrate that L2TL surpasses fine-tuning baselines and other adaptive transfer learning methods across eight datasets. L2TL is especially advantageous for small target datasets and considerable label mismatch between source and target datasets.",1
"Applying probabilistic models to reinforcement learning (RL) enables the application of powerful optimisation tools such as variational inference to RL. However, existing inference frameworks and their algorithms pose significant challenges for learning optimal policies, e.g., the absence of mode capturing behaviour in pseudo-likelihood methods and difficulties learning deterministic policies in maximum entropy RL based approaches. We propose VIREL, a novel, theoretically grounded probabilistic inference framework for RL that utilises a parametrised action-value function to summarise future dynamics of the underlying MDP. This gives VIREL a mode-seeking form of KL divergence, the ability to learn deterministic optimal polices naturally from inference and the ability to optimise value functions and policies in separate, iterative steps. In applying variational expectation-maximisation to VIREL we thus show that the actor-critic algorithm can be reduced to expectation-maximisation, with policy improvement equivalent to an E-step and policy evaluation to an M-step. We then derive a family of actor-critic methods from VIREL, including a scheme for adaptive exploration. Finally, we demonstrate that actor-critic algorithms from this family outperform state-of-the-art methods based on soft value functions in several domains.",0
"The use of probabilistic models in reinforcement learning (RL) allows for the implementation of effective optimization tools like variational inference. However, current inference frameworks and their algorithms present significant difficulties for learning optimal policies, such as the lack of mode capturing in pseudo-likelihood methods and the complications involved in learning deterministic policies in maximum entropy RL. Our proposed solution, VIREL, is a novel probabilistic inference framework for RL that utilizes a parametrized action-value function to summarize future dynamics of the MDP. VIREL possesses a mode-seeking form of KL divergence, enabling it to learn deterministic optimal policies naturally from inference and optimize value functions and policies in separate, iterative steps. By applying variational expectation-maximization to VIREL, we demonstrate that the actor-critic algorithm can be reduced to expectation-maximization, with policy improvement equivalent to an E-step and policy evaluation to an M-step. We also derive a family of actor-critic methods from VIREL, including a scheme for adaptive exploration. Finally, we show that actor-critic algorithms from this family outperform existing state-of-the-art methods based on soft value functions in several domains.",1
"We formulate counting as a sequential decision problem and present a novel crowd counting model solvable by deep reinforcement learning. In contrast to existing counting models that directly output count values, we divide one-step estimation into a sequence of much easier and more tractable sub-decision problems. Such sequential decision nature corresponds exactly to a physical process in reality scale weighing. Inspired by scale weighing, we propose a novel 'counting scale' termed LibraNet where the count value is analogized by weight. By virtually placing a crowd image on one side of a scale, LibraNet (agent) sequentially learns to place appropriate weights on the other side to match the crowd count. At each step, LibraNet chooses one weight (action) from the weight box (the pre-defined action pool) according to the current crowd image features and weights placed on the scale pan (state). LibraNet is required to learn to balance the scale according to the feedback of the needle (Q values). We show that LibraNet exactly implements scale weighing by visualizing the decision process how LibraNet chooses actions. Extensive experiments demonstrate the effectiveness of our design choices and report state-of-the-art results on a few crowd counting benchmarks. We also demonstrate good cross-dataset generalization of LibraNet. Code and models are made available at: https://git.io/libranet",0
"In this paper, we introduce a new approach to crowd counting that involves treating it as a sequential decision problem and using deep reinforcement learning to solve it. Unlike previous models that directly output count values, our method breaks down the estimation into a series of simpler sub-decisions. This approach mirrors the physical process of scale weighing and inspired us to create a new model called LibraNet. LibraNet treats the crowd count as a weight and learns to place appropriate weights on a virtual scale by analyzing the crowd image features and the weights already on the scale. The model selects an action from a pre-defined action pool based on the state of the scale and feedback received from the needle. We demonstrate that LibraNet successfully implements scale weighing and achieves state-of-the-art results on several crowd counting benchmarks. We also show that the model generalizes well across different datasets. The code and models for LibraNet are available at: https://git.io/libranet.",1
"The concept of utilizing multi-step returns for updating value functions has been adopted in deep reinforcement learning (DRL) for a number of years. Updating value functions with different backup lengths provides advantages in different aspects, including bias and variance of value estimates, convergence speed, and exploration behavior of the agent. Conventional methods such as TD-lambda leverage these advantages by using a target value equivalent to an exponential average of different step returns. Nevertheless, integrating step returns into a single target sacrifices the diversity of the advantages offered by different step return targets. To address this issue, we propose Mixture Bootstrapped DQN (MB-DQN) built on top of bootstrapped DQN, and uses different backup lengths for different bootstrapped heads. MB-DQN enables heterogeneity of the target values that is unavailable in approaches relying only on a single target value. As a result, it is able to maintain the advantages offered by different backup lengths. In this paper, we first discuss the motivational insights through a simple maze environment. In order to validate the effectiveness of MB-DQN, we perform experiments on the Atari 2600 benchmark environments, and demonstrate the performance improvement of MB-DQN over a number of baseline methods. We further provide a set of ablation studies to examine the impacts of different design configurations of MB-DQN.",0
"For several years, deep reinforcement learning (DRL) has been incorporating the idea of using multi-step returns to update value functions. This technique offers advantages in various aspects, such as the bias and variance of value estimates, the speed of convergence, and the exploration behavior of the agent. While conventional methods like TD-lambda use a target value that is an exponential average of different step returns, this approach sacrifices the diversity of benefits that different step return targets offer. To overcome this problem, we suggest Mixture Bootstrapped DQN (MB-DQN), which is based on bootstrapped DQN and employs different backup lengths for various bootstrapped heads. MB-DQN allows for heterogeneity of target values, which is not possible with approaches that rely on a single target value. As a result, it maintains the advantages offered by different backup lengths. In this paper, we first discuss the motivational insights using a simple maze environment. To demonstrate the effectiveness of MB-DQN, we conduct experiments on the Atari 2600 benchmark environments and show that MB-DQN outperforms several baseline methods. Additionally, we perform a set of ablation studies to explore the effects of different design configurations of MB-DQN.",1
"Efficient software testing is essential for productive software development and reliable user experiences. As human testing is inefficient and expensive, automated software testing is needed. In this work, we propose a Reinforcement Learning (RL) framework for functional software testing named DRIFT. DRIFT operates on the symbolic representation of the user interface. It uses Q-learning through Batch-RL and models the state-action value function with a Graph Neural Network. We apply DRIFT to testing the Windows 10 operating system and show that DRIFT can robustly trigger the desired software functionality in a fully automated manner. Our experiments test the ability to perform single and combined tasks across different applications, demonstrating that our framework can efficiently test software with a large range of testing objectives.",0
"In order to ensure effective software development and dependable user experiences, it is crucial to conduct software testing efficiently. Manual testing by humans can be both costly and inefficient, making it necessary to automate the testing process. This paper presents DRIFT, a functional software testing framework based on Reinforcement Learning (RL). DRIFT operates using a symbolic representation of the user interface and employs Q-learning with Batch-RL, utilizing a Graph Neural Network to model the state-action value function. Our study applies DRIFT to test the Windows 10 operating system, demonstrating its ability to trigger desired software functionality in an entirely automated fashion. Our experiments evaluate DRIFT's ability to perform individual and combined tasks across diverse applications, establishing its effectiveness in testing software with a broad range of testing objectives.",1
"Fetal MRI is heavily constrained by unpredictable and substantial fetal motion that causes image artifacts and limits the set of viable diagnostic image contrasts. Current mitigation of motion artifacts is predominantly performed by fast, single-shot MRI and retrospective motion correction. Estimation of fetal pose in real time during MRI stands to benefit prospective methods to detect and mitigate fetal motion artifacts where inferred fetal motion is combined with online slice prescription with low-latency decision making. Current developments of deep reinforcement learning (DRL), offer a novel approach for fetal landmarks detection. In this task 15 agents are deployed to detect 15 landmarks simultaneously by DRL. The optimization is challenging, and here we propose an improved DRL that incorporates priors on physical structure of the fetal body. First, we use graph communication layers to improve the communication among agents based on a graph where each node represents a fetal-body landmark. Further, additional reward based on the distance between agents and physical structures such as the fetal limbs is used to fully exploit physical structure. Evaluation of this method on a repository of 3-mm resolution in vivo data demonstrates a mean accuracy of landmark estimation within 10 mm of ground truth as 87.3%, and a mean error of 6.9 mm. The proposed DRL for fetal pose landmark search demonstrates a potential clinical utility for online detection of fetal motion that guides real-time mitigation of motion artifacts as well as health diagnosis during MRI of the pregnant mother.",0
"The movement of the fetus during MRI scans causes disturbances in the images, making it difficult to obtain accurate diagnostic images. The current methods used to correct these disturbances include fast, single-shot MRI and retrospective motion correction. However, these methods have limitations. To overcome these limitations, real-time estimation of fetal pose during MRI can be used to detect and correct fetal motion artifacts. Deep reinforcement learning (DRL) is a new approach that can be used to detect fetal landmarks. In this study, 15 agents were deployed to detect 15 landmarks simultaneously using DRL. To optimize the process, the physical structure of the fetal body was incorporated into the DRL algorithm. Graph communication layers were also used to improve communication among agents. The proposed method was tested on a repository of 3-mm resolution in vivo data, and the results showed an accuracy of 87.3% and a mean error of 6.9 mm. This method has the potential to be useful for online detection of fetal motion during MRI scans, which can guide real-time mitigation of motion artifacts and improve health diagnosis during MRI of pregnant women.",1
"Zap Q-learning is a recent class of reinforcement learning algorithms, motivated primarily as a means to accelerate convergence. Stability theory has been absent outside of two restrictive classes: the tabular setting, and optimal stopping. This paper introduces a new framework for analysis of a more general class of recursive algorithms known as stochastic approximation. Based on this general theory, it is shown that Zap Q-learning is consistent under a non-degeneracy assumption, even when the function approximation architecture is nonlinear. Zap Q-learning with neural network function approximation emerges as a special case, and is tested on examples from OpenAI Gym. Based on multiple experiments with a range of neural network sizes, it is found that the new algorithms converge quickly and are robust to choice of function approximation architecture.",0
"Recently, a type of reinforcement learning algorithms called Zap Q-learning has been developed to enhance convergence speed. However, stability theory has only been applied to two narrow categories: the tabular context and optimal stopping. This research introduces a novel framework for analyzing a wider range of recursive algorithms, known as stochastic approximation. Utilizing this theory, it is demonstrated that Zap Q-learning is dependable under a non-degeneracy condition, even with nonlinear function approximation architecture, such as neural networks. The effectiveness of this method is evaluated using OpenAI Gym examples, and the results indicate that the new algorithms converge rapidly and are resilient to variations in function approximation architecture, as evidenced by numerous experiments utilizing various neural network sizes.",1
"Inverse Reinforcement Learning addresses the problem of inferring an expert's reward function from demonstrations. However, in many applications, we not only have access to the expert's near-optimal behavior, but we also observe part of her learning process. In this paper, we propose a new algorithm for this setting, in which the goal is to recover the reward function being optimized by an agent, given a sequence of policies produced during learning. Our approach is based on the assumption that the observed agent is updating her policy parameters along the gradient direction. Then we extend our method to deal with the more realistic scenario where we only have access to a dataset of learning trajectories. For both settings, we provide theoretical insights into our algorithms' performance. Finally, we evaluate the approach in a simulated GridWorld environment and on the MuJoCo environments, comparing it with the state-of-the-art baseline.",0
"The issue of inferring an expert's reward function from demonstrations is tackled by Inverse Reinforcement Learning. However, in some cases, not only can we observe the expert's almost optimal behavior, but we can also witness her learning process. This paper proposes a novel algorithm for this scenario, aiming to retrieve the agent's optimized reward function through a series of policies produced during learning. Our method is based on the assumption that the observed agent is adjusting her policy parameters in the gradient direction. We also extend our approach to handle the more practical situation where we only have access to a learning trajectory dataset. We provide theoretical insights into the performance of our algorithms for both situations. Finally, we assess our approach in a simulated GridWorld environment and MuJoCo environments, comparing it with a state-of-the-art baseline.",1
"While learning in an unknown Markov Decision Process (MDP), an agent should trade off exploration to discover new information about the MDP, and exploitation of the current knowledge to maximize the reward. Although the agent will eventually learn a good or optimal policy, there is no guarantee on the quality of the intermediate policies. This lack of control is undesired in real-world applications where a minimum requirement is that the executed policies are guaranteed to perform at least as well as an existing baseline. In this paper, we introduce the notion of conservative exploration for average reward and finite horizon problems. We present two optimistic algorithms that guarantee (w.h.p.) that the conservative constraint is never violated during learning. We derive regret bounds showing that being conservative does not hinder the learning ability of these algorithms.",0
"When learning in an unfamiliar Markov Decision Process (MDP), an agent must balance exploration to uncover new information about the MDP and exploitation of current knowledge to maximize the reward. The agent will eventually learn a good or optimal policy, but the quality of intermediate policies cannot be guaranteed. This lack of control is not ideal for real-world applications where it is necessary for executed policies to perform at least as well as an existing baseline. This paper introduces the concept of conservative exploration for average reward and finite horizon problems. It presents two optimistic algorithms that ensure (with high probability) that the conservative constraint is never violated during learning. Regret bounds are derived to demonstrate that being conservative does not hinder the learning ability of these algorithms.",1
"In state of the art model-free off-policy deep reinforcement learning, a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions. We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simplified Markov Decision Process for which exact Q-values can be computed efficiently as more data comes in. The subgraph and its associated Q-values can be represented as a QGraph. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in temporal difference learning, our method QG-DDPG is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. QGraphs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm's sensitivity to the replay memory capacity.",0
"Cutting-edge model-free off-policy deep reinforcement learning utilizes a replay memory to save past experience and generate all network updates, even when the state and action spaces are continuous. However, the replay memory can only contain a finite number of transitions. To address this issue, we utilize a data graph to represent these transitions and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we can construct a simplified Markov Decision Process that enables us to efficiently compute exact Q-values as more data is received. The subgraph and its corresponding Q-values are referred to as a QGraph. We demonstrate that the Q-value for each transition in the simplified MDP serves as a lower bound for the Q-value of the same transition in the original continuous Q-learning problem. By incorporating these lower bounds into temporal difference learning, our approach, QG-DDPG, is less susceptible to soft divergence, enhances sample efficiency, and is more resilient to hyperparameters. QGraphs also retain information from transitions that have been overwritten in the replay memory, reducing the algorithm's dependence on replay memory capacity.",1
"In this paper, we focus on a prediction-based novelty estimation strategy upon the deep reinforcement learning (DRL) framework, and present a flow-based intrinsic curiosity module (FICM) to exploit the prediction errors from optical flow estimation as exploration bonuses. We propose the concept of leveraging motion features captured between consecutive observations to evaluate the novelty of observations in an environment. FICM encourages a DRL agent to explore observations with unfamiliar motion features, and requires only two consecutive frames to obtain sufficient information when estimating the novelty. We evaluate our method and compare it with a number of existing methods on multiple benchmark environments, including Atari games, Super Mario Bros., and ViZDoom. We demonstrate that FICM is favorable to tasks or environments featuring moving objects, which allow FICM to utilize the motion features between consecutive observations. We further ablatively analyze the encoding efficiency of FICM, and discuss its applicable domains comprehensively.",0
"The main focus of this paper is to introduce a strategy for estimating novelty based on prediction using deep reinforcement learning (DRL). To achieve this, we propose a flow-based intrinsic curiosity module (FICM) that utilizes prediction errors from optical flow estimation as exploration bonuses. Our approach involves evaluating the novelty of observations in an environment by leveraging motion features captured between consecutive observations. FICM encourages exploration of observations with unfamiliar motion features and requires just two consecutive frames to estimate novelty. We evaluate our method on multiple benchmark environments, including Atari games, Super Mario Bros., and ViZDoom, and compare it with existing methods. Our results show that FICM performs better in tasks or environments featuring moving objects, which allow it to utilize motion features between consecutive observations. We also analyze the encoding efficiency of FICM and discuss its applicability in various domains.",1
"Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, product recommendation, assortment, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.",0
"Thompson sampling is an efficient algorithm that tackles online decision problems by balancing the need to exploit known information for immediate performance with the desire to gather new information for future performance improvement. Its versatility has led to widespread adoption, and this tutorial delves into the algorithm and its practical applications. Through examples such as Bernoulli bandit problems, shortest path problems, and reinforcement learning in Markov decision processes, we explore the complexities of using Thompson sampling to inform beliefs about different actions. We also discuss the algorithm's effectiveness in various scenarios and its relationship to alternative algorithms.",1
"Efficient exploration remains a challenging problem in reinforcement learning, especially for those tasks where rewards from environments are sparse. A commonly used approach for exploring such environments is to introduce some ""intrinsic"" reward. In this work, we focus on model uncertainty estimation as an intrinsic reward for efficient exploration. In particular, we introduce an implicit generative modeling approach to estimate a Bayesian uncertainty of the agent's belief of the environment dynamics. Each random draw from our generative model is a neural network that instantiates the dynamic function, hence multiple draws would approximate the posterior, and the variance in the future prediction based on this posterior is used as an intrinsic reward for exploration. We design a training algorithm for our generative model based on the amortized Stein Variational Gradient Descent. In experiments, we compare our implementation with state-of-the-art intrinsic reward-based exploration approaches, including two recent approaches based on an ensemble of dynamic models. In challenging exploration tasks, our implicit generative model consistently outperforms competing approaches regarding data efficiency in exploration.",0
"Reinforcement learning presents a significant challenge when it comes to efficient exploration, particularly in tasks where the rewards from the environment are few and far between. To address this issue, the introduction of an ""intrinsic"" reward has become a common approach. This study focuses on using model uncertainty estimation as an intrinsic reward to improve exploration efficiency. The authors propose an implicit generative modeling method to estimate the Bayesian uncertainty of the agent's belief in the environment dynamics, whereby each random draw from the generative model is a neural network that represents the dynamic function. Multiple draws approximate the posterior, and the variance in future prediction based on this posterior is used as an intrinsic reward for exploration. The authors develop a training algorithm for their generative model based on the amortized Stein Variational Gradient Descent. They conduct experiments to compare their approach with state-of-the-art intrinsic reward-based exploration methods, including two recent approaches based on an ensemble of dynamic models. The results show that their implicit generative model outperforms the competing approaches in terms of exploration data efficiency in challenging tasks.",1
"Adoption of machine learning (ML)-enabled cyber-physical systems (CPS) are becoming prevalent in various sectors of modern society such as transportation, industrial, and power grids. Recent studies in deep reinforcement learning (DRL) have demonstrated its benefits in a large variety of data-driven decisions and control applications. As reliance on ML-enabled systems grows, it is imperative to study the performance of these systems under malicious state and actuator attacks. Traditional control systems employ resilient/fault-tolerant controllers that counter these attacks by correcting the system via error observations. However, in some applications, a resilient controller may not be sufficient to avoid a catastrophic failure. Ideally, a robust approach is more useful in these scenarios where a system is inherently robust (by design) to adversarial attacks. While robust control has a long history of development, robust ML is an emerging research area that has already demonstrated its relevance and urgency. However, the majority of robust ML research has focused on perception tasks and not on decision and control tasks, although the ML (specifically RL) models used for control applications are equally vulnerable to adversarial attacks. In this paper, we show that a well-performing DRL agent that is initially susceptible to action space perturbations (e.g. actuator attacks) can be robustified against similar perturbations through adversarial training.",0
"Various sectors of modern society, such as transportation, industrial, and power grids, are increasingly adopting machine learning (ML)-enabled cyber-physical systems (CPS). Recent research into deep reinforcement learning (DRL) has demonstrated its benefits in a wide range of data-driven decisions and control applications. As reliance on ML-enabled systems grows, it is crucial to examine their performance under malicious state and actuator attacks. While traditional control systems use resilient/fault-tolerant controllers to counter these attacks, in some cases, this may not be sufficient to prevent a catastrophic failure. Instead, a robust approach that designs systems to be inherently resilient to adversarial attacks is preferable. While robust control has a long history of development, robust ML research is an emerging area that has already demonstrated its relevance and urgency. However, most research on robust ML has focused on perception tasks and not on decision and control tasks, despite the vulnerability of ML models (specifically RL) used for control applications to adversarial attacks. In this paper, we demonstrate that adversarial training can robustify a well-performing DRL agent that is initially vulnerable to action space perturbations (e.g. actuator attacks) against similar perturbations.",1
"A wide range of image captioning models has been developed, achieving significant improvement based on popular metrics, such as BLEU, CIDEr, and SPICE. However, although the generated captions can accurately describe the image, they are generic for similar images and lack distinctiveness, i.e., cannot properly describe the uniqueness of each image. In this paper, we aim to improve the distinctiveness of image captions through training with sets of similar images. First, we propose a distinctiveness metric -- between-set CIDEr (CIDErBtw) to evaluate the distinctiveness of a caption with respect to those of similar images. Our metric shows that the human annotations of each image are not equivalent based on distinctiveness. Thus we propose several new training strategies to encourage the distinctiveness of the generated caption for each image, which are based on using CIDErBtw in a weighted loss function or as a reinforcement learning reward. Finally, extensive experiments are conducted, showing that our proposed approach significantly improves both distinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide variety of image captioning baselines. These results are further confirmed through a user study.",0
"Numerous models for generating image captions have been developed and have shown significant improvements according to popular metrics like BLEU, CIDEr, and SPICE. However, the captions generated tend to lack distinctiveness, meaning they are generic and cannot properly describe the unique features of each image even though they accurately depict the image. This paper aims to address this issue by proposing a distinctiveness metric, called CIDErBtw, to measure the distinctiveness of a caption with respect to similar images. The authors propose several training strategies to improve the distinctiveness of generated captions, including using CIDErBtw in a weighted loss function or as a reinforcement learning reward. Extensive experiments are conducted, demonstrating that the proposed approach significantly enhances both distinctiveness and accuracy for a variety of image captioning models, as measured by CIDErBtw, retrieval metrics, and a user study.",1
"This paper considers the problem of designing optimal algorithms for reinforcement learning in two-player zero-sum games. We focus on self-play algorithms which learn the optimal policy by playing against itself without any direct supervision. In a tabular episodic Markov game with $S$ states, $A$ max-player actions and $B$ min-player actions, the best existing algorithm for finding an approximate Nash equilibrium requires $\tilde{\mathcal{O}}(S^2AB)$ steps of game playing, when only highlighting the dependency on $(S,A,B)$. In contrast, the best existing lower bound scales as $\Omega(S(A+B))$ and has a significant gap from the upper bound. This paper closes this gap for the first time: we propose an optimistic variant of the \emph{Nash Q-learning} algorithm with sample complexity $\tilde{\mathcal{O}}(SAB)$, and a new \emph{Nash V-learning} algorithm with sample complexity $\tilde{\mathcal{O}}(S(A+B))$. The latter result matches the information-theoretic lower bound in all problem-dependent parameters except for a polynomial factor of the length of each episode. In addition, we present a computational hardness result for learning the best responses against a fixed opponent in Markov games---a learning objective different from finding the Nash equilibrium.",0
"The focus of this paper is on the development of optimal algorithms for reinforcement learning in two-player zero-sum games by considering the problem of designing such algorithms. The paper specifically concentrates on self-play algorithms that do not require any direct supervision and learn the optimal policy by playing against itself. The current state of the art algorithm for finding an approximate Nash equilibrium in a tabular episodic Markov game with S states, A max-player actions, and B min-player actions requires $\tilde{\mathcal{O}}(S^2AB)$ steps of game playing, while the best existing lower bound scales as $\Omega(S(A+B))$, which has a considerable gap from the upper bound. This paper aims to close this gap by introducing an optimistic variant of the Nash Q-learning algorithm with a sample complexity of $\tilde{\mathcal{O}}(SAB)$ and a new Nash V-learning algorithm with a sample complexity of $\tilde{\mathcal{O}}(S(A+B))$. The latter algorithm matches the information-theoretic lower bound in all problem-dependent parameters except for a polynomial factor of the episode's length. Additionally, the paper also presents a computational hardness result for learning the best responses against a fixed opponent in Markov games, which is a different learning objective from finding the Nash equilibrium.",1
"This paper introduces single-partition adaptive Q-learning (SPAQL), an algorithm for model-free episodic reinforcement learning (RL), which adaptively partitions the state-action space of a Markov decision process (MDP), while simultaneously learning a time-invariant policy (i. e., the mapping from states to actions does not depend explicitly on the episode time step) for maximizing the cumulative reward. The trade-off between exploration and exploitation is handled by using a mixture of upper confidence bounds (UCB) and Boltzmann exploration during training, with a temperature parameter that is automatically tuned as training progresses. The algorithm is an improvement over adaptive Q-learning (AQL). It converges faster to the optimal solution, while also using fewer arms. Tests on episodes with a large number of time steps show that SPAQL has no problems scaling, unlike AQL. Based on this empirical evidence, we claim that SPAQL may have a higher sample efficiency than AQL, thus being a relevant contribution to the field of efficient model-free RL methods.",0
"The aim of this study is to present the single-partition adaptive Q-learning (SPAQL) algorithm, a model-free episodic reinforcement learning (RL) approach that partitions the state-action space of a Markov decision process (MDP) adaptively while learning a time-invariant policy. This policy maximizes the cumulative reward, and exploration-exploitation trade-off is handled using a combination of upper confidence bounds (UCB) and Boltzmann exploration, with a temperature parameter automatically adjusted as training progresses. SPAQL improves upon adaptive Q-learning (AQL), converges faster to the optimal solution, and uses fewer arms. Unlike AQL, SPAQL scales well in episodes with a large number of time steps, indicating that it has higher sample efficiency. Thus, SPAQL is a valuable addition to the field of efficient model-free RL methods, according to empirical evidence.",1
"Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay -- greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.",0
"Our comprehension of experience replay in deep reinforcement learning (RL) algorithms is incomplete despite being an essential component of off-policy methods. Hence, we present a comprehensive and systematic analysis of experience replay in Q-learning approaches, with emphasis on two crucial aspects: the replay capacity and the ratio of experience collected to learning updates (replay ratio). Our research challenges conventional wisdom regarding experience replay, where we found that increasing replay capacity considerably improves the performance of some algorithms, while it has no effect on others. Furthermore, we demonstrate that uncorrected n-step returns, which lack theoretical grounding, are more advantageous than other techniques for sifting through larger memory. Additionally, we contextualize previous findings in the literature and measure the significance of controlling the replay ratio across various deep RL algorithms. Lastly, we test several hypotheses concerning the performance benefits of experience replay.",1
"The outcome of Jacobian singular values regularization was studied for supervised learning problems. It also was shown that Jacobian conditioning regularization can help to avoid the ``mode-collapse'' problem in Generative Adversarial Networks. In this paper, we try to answer the following question: Can information about policy conditioning help to shape a more stable and general policy of reinforcement learning agents? To answer this question, we conduct a study of Jacobian conditioning behavior during policy optimization. To the best of our knowledge, this is the first work that research condition number in reinforcement learning agents. We propose a conditioning regularization algorithm and test its performance on the range of continuous control tasks. Finally, we compare algorithms on the CoinRun environment with separated train end test levels to analyze how conditioning regularization contributes to agents' generalization.",0
"The study focused on examining the effects of Jacobian singular values regularization on supervised learning problems. Additionally, it was observed that this regularization technique could address the ""mode-collapse"" issue in Generative Adversarial Networks. The researchers aimed to investigate if information about policy conditioning could enhance the stability and generality of reinforcement learning agents' policies. To achieve this goal, they analyzed the behavior of Jacobian conditioning during policy optimization, which has not been explored in previous research. They introduced a conditioning regularization algorithm and evaluated its performance on various continuous control tasks. Finally, they compared the effectiveness of different algorithms in the CoinRun environment with separate train and test levels to determine how conditioning regularization affects agents' generalization abilities.",1
"One of the key approaches to save samples in reinforcement learning (RL) is to use knowledge from an approximate model such as its simulator. However, how much does an approximate model help to learn a near-optimal policy of the true unknown model? Despite numerous empirical studies of transfer reinforcement learning, an answer to this question is still elusive. In this paper, we study the sample complexity of RL while an approximate model of the environment is provided. For an unknown Markov decision process (MDP), we show that the approximate model can effectively reduce the complexity by eliminating sub-optimal actions from the policy searching space. In particular, we provide an algorithm that uses $\widetilde{O}(N/(1-\gamma)^3/\varepsilon^2)$ samples in a generative model to learn an $\varepsilon$-optimal policy, where $\gamma$ is the discount factor and $N$ is the number of near-optimal actions in the approximate model. This can be much smaller than the learning-from-scratch complexity $\widetilde{\Theta}(SA/(1-\gamma)^3/\varepsilon^2)$, where $S$ and $A$ are the sizes of state and action spaces respectively. We also provide a lower bound showing that the above upper bound is nearly-tight if the value gap between near-optimal actions and sub-optimal actions in the approximate model is sufficiently large. Our results provide a very precise characterization of how an approximate model helps reinforcement learning when no additional assumption on the model is posed.",0
"Using an approximate model, such as a simulator, is a valuable method for saving samples in reinforcement learning (RL). However, it is unclear how much an approximate model aids in learning a near-optimal policy for the true unknown model. This paper seeks to address this question by investigating the sample complexity of RL when an approximate model is provided. Our study shows that for an unknown Markov decision process (MDP), the approximate model can effectively reduce complexity by eliminating sub-optimal actions from the policy search space. We present an algorithm that utilizes $\widetilde{O}(N/(1-\gamma)^3/\varepsilon^2)$ samples in a generative model to learn an $\varepsilon$-optimal policy, where $\gamma$ is the discount factor and $N$ is the number of near-optimal actions in the approximate model. This approach can be significantly less complex than the learning-from-scratch method, which requires $\widetilde{\Theta}(SA/(1-\gamma)^3/\varepsilon^2)$ samples, where $S$ and $A$ represent the sizes of the state and action spaces, respectively. Additionally, we provide a lower bound indicating that the above upper bound is nearly-tight if the value gap between near-optimal actions and sub-optimal actions in the approximate model is sufficiently large. Our results offer a precise understanding of how an approximate model can assist in reinforcement learning without any additional assumptions on the model.",1
"A common assumption in reinforcement learning (RL) is to have access to a generative model (i.e., a simulator of the environment), which allows to generate samples from any desired state-action pair. Nonetheless, in many settings a generative model may not be available and an adaptive exploration strategy is needed to efficiently collect samples from an unknown environment by direct interaction. In this paper, we study the scenario where an algorithm based on the generative model assumption defines the (possibly time-varying) amount of samples $b(s,a)$ required at each state-action pair $(s,a)$ and an exploration strategy has to learn how to generate $b(s,a)$ samples as fast as possible. Building on recent results for regret minimization in the stochastic shortest path (SSP) setting (Cohen et al., 2020; Tarbouriech et al., 2020), we derive an algorithm that requires $\tilde{O}( B D + D^{3/2} S^2 A)$ time steps to collect the $B = \sum_{s,a} b(s,a)$ desired samples, in any unknown and communicating MDP with $S$ states, $A$ actions and diameter $D$. Leveraging the generality of our strategy, we readily apply it to a variety of existing settings (e.g., model estimation, pure exploration in MDPs) for which we obtain improved sample-complexity guarantees, and to a set of new problems such as best-state identification and sparse reward discovery.",0
"Reinforcement learning (RL) commonly assumes access to a generative model (a simulator) that can produce samples from any state-action pair. However, such a model may not always be available, and direct interaction with an unknown environment is necessary. This paper examines a scenario where an algorithm based on the generative model assumption prescribes the number of samples required at each state-action pair, and an exploration strategy must learn how to generate them quickly. By building on recent results for regret minimization in the stochastic shortest path (SSP) setting, we propose an algorithm that can collect the desired samples in an unknown and communicating MDP with improved sample-complexity guarantees. Our approach is general and applies to various settings, including model estimation, pure exploration, best-state identification, and sparse reward discovery.",1
"Off-policy evaluation of sequential decision policies from observational data is necessary in applications of batch reinforcement learning such as education and healthcare. In such settings, however, unobserved variables confound observed actions, rendering exact evaluation of new policies impossible, i.e., unidentifiable. We develop a robust approach that estimates sharp bounds on the (unidentifiable) value of a given policy in an infinite-horizon problem given data from another policy with unobserved confounding, subject to a sensitivity model. We consider stationary or baseline unobserved confounding and compute bounds by optimizing over the set of all stationary state-occupancy ratios that agree with a new partially identified estimating equation and the sensitivity model. We prove convergence to the sharp bounds as we collect more confounded data. Although checking set membership is a linear program, the support function is given by a difficult nonconvex optimization problem. We develop approximations based on nonconvex projected gradient descent and demonstrate the resulting bounds empirically.",0
"In the realm of batch reinforcement learning, specifically in fields like education and healthcare, it is necessary to evaluate sequential decision policies from observational data, even though unobserved variables can complicate matters by confounding observed actions, making exact evaluation of new policies impossible. To address this issue, we have created a robust approach that can estimate sharp bounds on the value of a given policy in an infinite-horizon problem, even when the data being used comes from another policy with unobserved confounding. Our approach is subject to a sensitivity model and takes into account both stationary and baseline unobserved confounding. We compute these bounds by optimizing over the set of all stationary state-occupancy ratios that agree with a new partially identified estimating equation and the sensitivity model. We have demonstrated empirically that our approach yields accurate results, even as we collect more confounded data. While checking set membership is a linear program, the support function is given by a difficult nonconvex optimization problem. To address this, we have developed approximations based on nonconvex projected gradient descent.",1
"As a popular meta-learning approach, the model-agnostic meta-learning (MAML) algorithm has been widely used due to its simplicity and effectiveness. However, the convergence of the general multi-step MAML still remains unexplored. In this paper, we develop a new theoretical framework to provide such convergence guarantee for two types of objective functions that are of interest in practice: (a) resampling case (e.g., reinforcement learning), where loss functions take the form in expectation and new data are sampled as the algorithm runs; and (b) finite-sum case (e.g., supervised learning), where loss functions take the finite-sum form with given samples. For both cases, we characterize the convergence rate and the computational complexity to attain an $\epsilon$-accurate solution for multi-step MAML in the general nonconvex setting. In particular, our results suggest that an inner-stage stepsize needs to be chosen inversely proportional to the number $N$ of inner-stage steps in order for $N$-step MAML to have guaranteed convergence. From the technical perspective, we develop novel techniques to deal with the nested structure of the meta gradient for multi-step MAML, which can be of independent interest.",0
"The MAML algorithm is a popular meta-learning approach that is widely used due to its simplicity and effectiveness. However, the convergence of the general multi-step MAML has not been explored. This paper presents a new theoretical framework that guarantees convergence for two types of objective functions that are commonly used in practice: the resampling case and the finite-sum case. We provide a characterization of the convergence rate and computational complexity to achieve an $\epsilon$-accurate solution for multi-step MAML in the general nonconvex setting for both cases. Our findings suggest that the inner-stage stepsize needs to be chosen inversely proportional to the number of inner-stage steps in order for MAML to have guaranteed convergence. We also develop novel techniques to handle the nested structure of the meta gradient for multi-step MAML, which may be of independent interest.",1
"In this paper, we build upon the weakly-supervised generation mechanism of intermediate attention maps in any convolutional neural networks and disclose the effectiveness of attention modules more straightforwardly to fully exploit their potential. Given an existing neural network equipped with arbitrary attention modules, we introduce a meta critic network to evaluate the quality of attention maps in the main network. Due to the discreteness of our designed reward, the proposed learning method is arranged in a reinforcement learning setting, where the attention actors and recurrent critics are alternately optimized to provide instant critique and revision for the temporary attention representation, hence coined as Deep REinforced Attention Learning (DREAL). It could be applied universally to network architectures with different types of attention modules and promotes their expressive ability by maximizing the relative gain of the final recognition performance arising from each individual attention module, as demonstrated by extensive experiments on both category and instance recognition benchmarks.",0
"This paper aims to enhance the intermediate attention maps generated by any convolutional neural network with weak supervision and explore the full potential of attention modules in a more straightforward manner. To achieve this, we introduce a meta critic network that evaluates the quality of attention maps in the existing neural network. Our proposed learning method is based on reinforcement learning, where attention actors and recurrent critics are optimized alternately to provide instant feedback and revision for the attention representation. We call this approach Deep REinforced Attention Learning (DREAL), and it can be applied to different network architectures with various types of attention modules. Through extensive experiments on both category and instance recognition benchmarks, we demonstrate that DREAL maximizes the relative gain of recognition performance and promotes the expressive ability of each individual attention module.",1
"Multi-step greedy policies have been extensively used in model-based reinforcement learning (RL), both when a model of the environment is available (e.g.,~in the game of Go) and when it is learned. In this paper, we explore their benefits in model-free RL, when employed using multi-step dynamic programming algorithms: $\kappa$-Policy Iteration ($\kappa$-PI) and $\kappa$-Value Iteration ($\kappa$-VI). These methods iteratively compute the next policy ($\kappa$-PI) and value function ($\kappa$-VI) by solving a surrogate decision problem with a shaped reward and a smaller discount factor. We derive model-free RL algorithms based on $\kappa$-PI and $\kappa$-VI in which the surrogate problem can be solved by any discrete or continuous action RL method, such as DQN and TRPO. We identify the importance of a hyper-parameter that controls the extent to which the surrogate problem is solved and suggest a way to set this parameter. When evaluated on a range of Atari and MuJoCo benchmark tasks, our results indicate that for the right range of $\kappa$, our algorithms outperform DQN and TRPO. This shows that our multi-step greedy algorithms are general enough to be applied over any existing RL algorithm and can significantly improve its performance.",0
"Multi-step greedy policies have been widely used in model-based reinforcement learning (RL) to solve the decision-making problems in various scenarios, ranging from games like Go to learned environments. This study delves into the potential of multi-step dynamic programming algorithms, namely $\kappa$-Policy Iteration ($\kappa$-PI) and $\kappa$-Value Iteration ($\kappa$-VI), to improve model-free RL. These algorithms solve the next policy ($\kappa$-PI) and value function ($\kappa$-VI) iteratively by solving a surrogate decision problem with a shaped reward and a smaller discount factor. The study proposes model-free RL algorithms based on $\kappa$-PI and $\kappa$-VI, which can be solved using any discrete or continuous action RL method, such as DQN and TRPO. The study emphasizes the importance of a hyper-parameter that controls the extent to which the surrogate problem is solved and suggests a way to set this parameter. The experimental results on Atari and MuJoCo benchmark tasks demonstrate that the proposed algorithms outperform DQN and TRPO for the right range of $\kappa$. This indicates that multi-step greedy algorithms are versatile and can be applied to any existing RL algorithm, leading to significant performance improvements.",1
"To efficiently run DNNs on the edge/cloud, many new DNN inference accelerators are being designed and deployed frequently. To enhance the resource efficiency of DNNs, model quantization is a widely-used approach. However, different accelerator/HW has different resources leading to the need for specialized quantization strategy of each HW. Moreover, using the same quantization for every layer may be sub-optimal, increasing the designspace of possible quantization choices. This makes manual-tuning infeasible. Recent work in automatically determining quantization for each layer is driven by optimization methods such as reinforcement learning. However, these approaches need re-training the RL for every new HW platform. We propose a new way for autonomous quantization and HW-aware tuning. We propose a generative model, AQGAN, which takes a target accuracy as the condition and generates a suite of quantization configurations. With the conditional generative model, the user can autonomously generate different configurations with different targets in inference time. Moreover, we propose a simplified HW-tuning flow, which uses the generative model to generate proposals and execute simple selection based on the HW resource budget, whose process is fast and interactive. We evaluate our model on five of the widely-used efficient models on the ImageNet dataset. We compare with existing uniform quantization and state-of-the-art autonomous quantization methods. Our generative model shows competitive achieved accuracy, however, with around two degrees less search cost for each design point. Our generative model shows the generated quantization configuration can lead to less than 3.5% error across all experiments.",0
"Numerous DNN inference accelerators are being designed and deployed frequently to effectively run DNNs on the edge/cloud. Model quantization is a widely-used approach to enhance the resource efficiency of DNNs. However, since different accelerators/HW have different resources, there is a need for specialized quantization strategies for each HW. Using the same quantization for every layer may not be optimal, leading to an increased design space of possible quantization choices and making manual-tuning impractical. Recently, optimization methods such as reinforcement learning have been used to automatically determine quantization for each layer, but these approaches require re-training for every new HW platform. We propose an autonomous quantization and HW-aware tuning method using a generative model, AQGAN, which takes a target accuracy as a condition and generates multiple quantization configurations. The user can generate different configurations with different targets during inference time. We also propose a simplified HW-tuning flow that uses the generative model to generate proposals and execute simple selection based on the HW resource budget, which is fast and interactive. We evaluate our model on five widely-used efficient models on the ImageNet dataset and compare it with existing uniform quantization and state-of-the-art autonomous quantization methods. Our generative model achieves competitive accuracy with around two degrees less search cost for each design point. The generated quantization configuration can lead to less than 3.5% error across all experiments.",1
"The choice of the control frequency of a system has a relevant impact on the ability of reinforcement learning algorithms to learn a highly performing policy. In this paper, we introduce the notion of action persistence that consists in the repetition of an action for a fixed number of decision steps, having the effect of modifying the control frequency. We start analyzing how action persistence affects the performance of the optimal policy, and then we present a novel algorithm, Persistent Fitted Q-Iteration (PFQI), that extends FQI, with the goal of learning the optimal value function at a given persistence. After having provided a theoretical study of PFQI and a heuristic approach to identify the optimal persistence, we present an experimental campaign on benchmark domains to show the advantages of action persistence and proving the effectiveness of our persistence selection method.",0
"The frequency at which a system is controlled has a significant impact on the ability of reinforcement learning algorithms to learn a high-performing policy. This paper introduces the concept of action persistence, which involves repeating a particular action for a fixed number of decision steps to modify the control frequency. The impact of action persistence on the optimal policy's performance is analyzed, and a new algorithm called Persistent Fitted Q-Iteration (PFQI) is presented to extend FQI and learn the optimal value function at a specific persistence level. A theoretical study of PFQI is provided, along with a heuristic approach to identify the optimal persistence. An experimental campaign on benchmark domains is carried out to demonstrate the advantages of action persistence and the effectiveness of the persistence selection method.",1
"We analyze the Gambler's problem, a simple reinforcement learning problem where the gambler has the chance to double or lose the bets until the target is reached. This is an early example introduced in the reinforcement learning textbook by Sutton and Barto (2018), where they mention an interesting pattern of the optimal value function with high-frequency components and repeating non-smooth points. It is however without further investigation. We provide the exact formula for the optimal value function for both the discrete and the continuous cases. Though simple as it might seem, the value function is pathological: fractal, self-similar, derivative taking either zero or infinity, and not written as elementary functions. It is in fact one of the generalized Cantor functions, where it holds a complexity that has been uncharted thus far. Our analyses could provide insights into improving value function approximation, gradient-based algorithms, and Q-learning, in real applications and implementations.",0
"We examine the Gambler's problem, a reinforcement learning task in which the gambler can either double or lose their bets until they reach a specific goal. This problem is discussed in Sutton and Barto's (2018) reinforcement learning textbook, where they note an intriguing pattern in the optimal value function consisting of high-frequency components and non-smooth points that repeat. However, no further exploration is done. We present the precise equation for the optimal value function in both the discrete and continuous scenarios. Despite its apparent simplicity, the value function is pathological, exhibiting fractal and self-similar qualities, with a derivative that is either zero or infinity, and cannot be expressed in elementary functions. It is, in fact, a generalized Cantor function, with a complexity that has yet to be fully understood. Our findings could provide valuable insights for improving the accuracy of value function approximations, gradient-based algorithms, and Q-learning in real-world applications and implementations.",1
"Model-based reinforcement learning (RL) is appealing because (i) it enables planning and thus more strategic exploration, and (ii) by decoupling dynamics from rewards, it enables fast transfer to new reward functions. However, learning an accurate Markov Decision Process (MDP) over high-dimensional states (e.g., raw pixels) is extremely challenging because it requires function approximation, which leads to compounding errors. Instead, to avoid compounding errors, we propose learning an abstract MDP over abstract states: low-dimensional coarse representations of the state (e.g., capturing agent position, ignoring other objects). We assume access to an abstraction function that maps the concrete states to abstract states. In our approach, we construct an abstract MDP, which grows through strategic exploration via planning. Similar to hierarchical RL approaches, the abstract actions of the abstract MDP are backed by learned subpolicies that navigate between abstract states. Our approach achieves strong results on three of the hardest Arcade Learning Environment games (Montezuma's Revenge, Pitfall!, and Private Eye), including superhuman performance on Pitfall! without demonstrations. After training on one task, we can reuse the learned abstract MDP for new reward functions, achieving higher reward in 1000x fewer samples than model-free methods trained from scratch.",0
"The use of model-based reinforcement learning (RL) is attractive due to its ability to plan and explore strategically, as well as allowing for quick adaptation to new reward functions by separating dynamics from rewards. However, developing an accurate Markov decision process (MDP) for high-dimensional states, such as raw pixels, is challenging due to function approximation leading to compounded errors. To avoid this, we propose learning an abstract MDP using low-dimensional coarse representations of the state to prevent compounded errors. We create an abstract MDP through strategic exploration and construct abstract actions backed by subpolicies. Our approach achieves exceptional results on challenging games, including superhuman performance on Pitfall! without demonstrations. Additionally, we can use the learned abstract MDP for new reward functions, achieving higher rewards in significantly fewer samples than model-free methods trained from scratch.",1
"The field of reinforcement learning can be split into model-based and model-free methods. Here, we unify these approaches by casting model-free policy optimisation as amortised variational inference, and model-based planning as iterative variational inference, within a `control as hybrid inference' (CHI) framework. We present an implementation of CHI which naturally mediates the balance between iterative and amortised inference. Using a didactic experiment, we demonstrate that the proposed algorithm operates in a model-based manner at the onset of learning, before converging to a model-free algorithm once sufficient data have been collected. We verify the scalability of our algorithm on a continuous control benchmark, demonstrating that it outperforms strong model-free and model-based baselines. CHI thus provides a principled framework for harnessing the sample efficiency of model-based planning while retaining the asymptotic performance of model-free policy optimisation.",0
"The area of reinforcement learning can be categorized into two methods: model-based and model-free. In this study, we combine these approaches by presenting model-free policy optimization as amortized variational inference and model-based planning as iterative variational inference in a framework called ""control as hybrid inference"" (CHI). We introduce an implementation of CHI that naturally balances iterative and amortized inference. Using a simplified experiment, we demonstrate that the proposed algorithm initially operates in a model-based manner and later converges to a model-free algorithm after enough data has been collected. We also prove the scalability of our algorithm on a continuous control benchmark, showing that it outperforms model-free and model-based baselines. Therefore, CHI offers a systematic approach to utilize the sample efficiency of model-based planning while maintaining the asymptotic performance of model-free policy optimization.",1
"Autonomous and semi-autonomous systems for safety-critical applications require rigorous testing before deployment. Due to the complexity of these systems, formal verification may be impossible and real-world testing may be dangerous during development. Therefore, simulation-based techniques have been developed that treat the system under test as a black box during testing. Safety validation tasks include finding disturbances to the system that cause it to fail (falsification), finding the most-likely failure, and estimating the probability that the system fails. Motivated by the prevalence of safety-critical artificial intelligence, this work provides a survey of state-of-the-art safety validation techniques with a focus on applied algorithms and their modifications for the safety validation problem. We present and discuss algorithms in the domains of optimization, path planning, reinforcement learning, and importance sampling. Problem decomposition techniques are presented to help scale algorithms to large state spaces, and a brief overview of safety-critical applications is given, including autonomous vehicles and aircraft collision avoidance systems. Finally, we present a survey of existing academic and commercially available safety validation tools.",0
"Rigorous testing is necessary for autonomous and semi-autonomous systems used in safety-critical applications before deployment. Formal verification may prove impossible due to complexity, and real-world testing can be dangerous during development. Therefore, simulation-based techniques have been developed to treat the system under test as a black box during testing. Safety validation involves identifying disturbances that cause the system to fail, finding the most-likely failure, and estimating the probability of failure. This work surveys the latest safety validation techniques, focusing on applied algorithms and their modifications for the problem. We explore algorithms in optimization, path planning, reinforcement learning, and importance sampling, along with problem decomposition techniques for scaling algorithms to large state spaces. Additionally, we provide an overview of safety-critical applications, including autonomous vehicles and aircraft collision avoidance systems. Finally, we review existing academic and commercially available safety validation tools.",1
"Conventional simulations on multi-exit indoor evacuation focus primarily on how to determine a reasonable exit based on numerous factors in a changing environment. Results commonly include some congested and other under-utilized exits, especially with massive pedestrians. We propose a multi-exit evacuation simulation based on Deep Reinforcement Learning (DRL), referred to as the MultiExit-DRL, which involves in a Deep Neural Network (DNN) framework to facilitate state-to-action mapping. The DNN framework applies Rainbow Deep Q-Network (DQN), a DRL algorithm that integrates several advanced DQN methods, to improve data utilization and algorithm stability, and further divides the action space into eight isometric directions for possible pedestrian choices. We compare MultiExit-DRL with two conventional multi-exit evacuation simulation models in three separate scenarios: 1) varying pedestrian distribution ratios, 2) varying exit width ratios, and 3) varying open schedules for an exit. The results show that MultiExit-DRL presents great learning efficiency while reducing the total number of evacuation frames in all designed experiments. In addition, the integration of DRL allows pedestrians to explore other potential exits and helps determine optimal directions, leading to the high efficiency of exit utilization.",0
"The current approach to simulating indoor evacuation with multiple exits focuses on determining the most suitable exit based on several factors in a dynamic environment. However, these simulations often result in some exits being overcrowded while others are underutilized, especially in situations with a large number of pedestrians. To address this issue, we propose a new simulation model called MultiExit-DRL, which utilizes Deep Reinforcement Learning (DRL) and a Deep Neural Network (DNN) framework for state-to-action mapping. The MultiExit-DRL model applies the Rainbow Deep Q-Network (DQN) algorithm - a DRL algorithm that integrates several advanced DQN methods - to improve data utilization and algorithm stability. Additionally, we divide the action space into eight isometric directions for possible pedestrian choices. We compare the MultiExit-DRL model with two conventional multi-exit evacuation simulation models in three different scenarios: 1) varying pedestrian distribution ratios, 2) varying exit width ratios, and 3) varying open schedules for an exit. Our results show that the MultiExit-DRL model has higher learning efficiency and reduces the total number of evacuation frames in all experiments. Furthermore, the DRL integration allows pedestrians to explore other potential exits and determine optimal directions, leading to better exit utilization.",1
"Edge devices demand low energy consumption, cost and small form factor. To efficiently deploy convolutional neural network (CNN) models on edge device, energy-aware model compression becomes extremely important. However, existing work did not study this problem well because the lack of considering the diversity of dataflow types in hardware architectures. In this paper, we propose EDCompress, an Energy-aware model compression method for various Dataflows. It can effectively reduce the energy consumption of various edge devices, with different dataflow types. Considering the very nature of model compression procedures, we recast the optimization process to a multi-step problem, and solve it by reinforcement learning algorithms. Experiments show that EDCompress could improve 20X, 17X, 37X energy efficiency in VGG-16, MobileNet, LeNet-5 networks, respectively, with negligible loss of accuracy. EDCompress could also find the optimal dataflow type for specific neural networks in terms of energy consumption, which can guide the deployment of CNN models on hardware systems.",0
"To deploy convolutional neural network (CNN) models on edge devices efficiently, it is crucial to consider low energy consumption, cost and small form factor. However, the existing solutions have not addressed this issue adequately due to the lack of diversity in dataflow types in hardware architectures. To overcome this, we propose EDCompress, an Energy-aware model compression method that is suitable for various Dataflows. Our approach effectively reduces the energy consumption of different edge devices by recasting the optimization process to a multi-step problem and solving it using reinforcement learning algorithms. Through experiments, we demonstrate that EDCompress can improve energy efficiency by up to 20X, 17X, and 37X in VGG-16, MobileNet, and LeNet-5 networks, respectively, without compromising on accuracy. Additionally, EDCompress can identify the optimal dataflow type for specific neural networks, which can guide the deployment of CNN models on hardware systems.",1
"In this paper, we study a long-term planning scenario that is based on drone racing competitions held in real life. We conducted this experiment on a framework created for ""Game of Drones: Drone Racing Competition"" at NeurIPS 2019. The racing environment was created using Microsoft's AirSim Drone Racing Lab. A reinforcement learning agent, a simulated quadrotor in our case, has trained with the Policy Proximal Optimization(PPO) algorithm was able to successfully compete against another simulated quadrotor that was running a classical path planning algorithm. Agent observations consist of data from IMU sensors, GPS coordinates of drone obtained through simulation and opponent drone GPS information. Using opponent drone GPS information during training helps dealing with complex state spaces, serving as expert guidance allows for efficient and stable training process. All experiments performed in this paper can be found and reproduced with code at our GitHub repository",0
"This paper delves into a long-term planning scenario that utilizes drone racing competitions in real life. The experiment was carried out on a framework designed for the ""Game of Drones: Drone Racing Competition"" at NeurIPS 2019, where the Microsoft AirSim Drone Racing Lab was utilized to create the racing environment. During the training process, a reinforcement learning agent, specifically a simulated quadrotor, was trained with the Policy Proximal Optimization (PPO) algorithm and was successfully pitted against another simulated quadrotor, which was utilizing a classical path planning algorithm. The agent's observations included data from IMU sensors, GPS coordinates of the drone that were obtained through simulation, and GPS information on the opponent drone. The use of opponent drone GPS information during training helped deal with complex state spaces, thereby acting as expert guidance that allowed for an efficient and stable training process. The code used in all experiments conducted in this paper can be found and replicated at our GitHub repository.",1
"Visualization tools for supervised learning have allowed users to interpret, introspect, and gain intuition for the successes and failures of their models. While reinforcement learning practitioners ask many of the same questions, existing tools are not applicable to the RL setting. In this work, we describe our initial attempt at constructing a prototype of these ideas, through identifying possible features that such a system should encapsulate. Our design is motivated by envisioning the system to be a platform on which to experiment with interpretable reinforcement learning.",0
"The tools used for supervised learning have enabled users to comprehend, self-reflect, and intuitively comprehend the achievements and shortcomings of their models. However, present visualization tools are not suitable for the reinforcement learning setting, even though reinforcement learning practitioners ask comparable questions. This paper outlines our first effort at constructing a prototype of these concepts by identifying potential features that such a system should include. Our design aims to create a platform for experimenting with understandable reinforcement learning.",1
"In this work, we propose a novel approach for reinforcement learning driven by evolutionary computation. Our algorithm, dubbed as Evolutionary-Driven Reinforcement Learning (evo-RL), embeds the reinforcement learning algorithm in an evolutionary cycle, where we distinctly differentiate between purely evolvable (instinctive) behaviour versus purely learnable behaviour. Furthermore, we propose that this distinction is decided by the evolutionary process, thus allowing evo-RL to be adaptive to different environments. In addition, evo-RL facilitates learning on environments with rewardless states, which makes it more suited for real-world problems with incomplete information. To show that evo-RL leads to state-of-the-art performance, we present the performance of different state-of-the-art reinforcement learning algorithms when operating within evo-RL and compare it with the case when these same algorithms are executed independently. Results show that reinforcement learning algorithms embedded within our evo-RL approach significantly outperform the stand-alone versions of the same RL algorithms on OpenAI Gym control problems with rewardless states constrained by the same computational budget.",0
"Our work introduces a unique approach to reinforcement learning that utilizes evolutionary computation. Known as Evolutionary-Driven Reinforcement Learning, or evo-RL, our algorithm incorporates the reinforcement learning process into an evolutionary cycle, distinguishing between instinctive and learnable behavior. This differentiation is determined by the evolutionary process, enabling evo-RL to adapt to various environments. Additionally, evo-RL enables learning in environments without rewards, making it appropriate for real-world problems with incomplete information. To demonstrate its state-of-the-art performance, we compare the performance of various reinforcement learning algorithms within evo-RL to their independent execution. Our results show that RL algorithms embedded in evo-RL outperform their stand-alone versions on OpenAI Gym control problems with rewardless states, while adhering to the same computational budget.",1
"This paper investigates generalisation in multi-agent games, where the generality of the agent can be evaluated by playing against opponents it hasn't seen during training. We propose two new games with concealed information and complex, non-transitive reward structure (think rock/paper/scissors). It turns out that most current deep reinforcement learning methods fail to efficiently explore the strategy space, thus learning policies that generalise poorly to unseen opponents. We then propose a novel hierarchical agent architecture, where the hierarchy is grounded in the game-theoretic structure of the game -- the top level chooses strategic responses to opponents, while the low level implements them into policy over primitive actions. This grounding facilitates credit assignment across the levels of hierarchy. Our experiments show that the proposed hierarchical agent is capable of generalisation to unseen opponents, while conventional baselines fail to generalise whatsoever.",0
"The aim of this study is to examine generalisation in multi-agent games, where the ability of agents to perform against opponents they have not trained against is measured. Two new games with hidden information and intricate, non-transitive reward systems are introduced. However, current deep reinforcement learning methods do not effectively explore the strategy space, resulting in policies that do not generalise well to unseen opponents. To address this, a new hierarchical agent architecture is proposed, where the hierarchy is based on the game-theoretic structure of the game. The top level selects strategic responses to opponents, while the low level implements them into policy over primitive actions. This approach facilitates credit assignment across the hierarchy. The findings indicate that the proposed hierarchical agent can generalise to unseen opponents, whereas conventional methods fail to do so.",1
"The graph structure of biomedical data differs from those in typical knowledge graph benchmark tasks. A particular property of biomedical data is the presence of long-range dependencies, which can be captured by patterns described as logical rules. We propose a novel method that combines these rules with a neural multi-hop reasoning approach that uses reinforcement learning. We conduct an empirical study based on the real-world task of drug repurposing by formulating this task as a link prediction problem. We apply our method to the biomedical knowledge graph Hetionet and show that our approach outperforms several baseline methods.",0
"The biomedical data's graph structure is distinct from the typical knowledge graph benchmark tasks, characterized by the existence of long-range dependencies described as logical rules. To address this, we introduce a fresh technique that merges these rules with a neural multi-hop reasoning approach using reinforcement learning. Our empirical study focuses on drug repurposing, treated as a link prediction issue in a real-world context. Our method is tested on the Hetionet biomedical knowledge graph, and we demonstrate its superiority over various baseline methods.",1
"Reinforcement learning (RL) algorithms typically start tabula rasa, without any prior knowledge of the environment, and without any prior skills. This however often leads to low sample efficiency, requiring a large amount of interaction with the environment. This is especially true in a lifelong learning setting, in which the agent needs to continually extend its capabilities. In this paper, we examine how a pre-trained task-independent language model can make a goal-conditional RL agent more sample efficient. We do this by facilitating transfer learning between different related tasks. We experimentally demonstrate our approach on a set of object navigation tasks.",0
"Usually, reinforcement learning (RL) algorithms commence with a blank slate, devoid of any previous knowledge or abilities in the environment. This approach can result in low sample efficiency, as a considerable amount of interaction with the surroundings is necessary. The inefficiency is more pronounced in lifelong learning scenarios, where the agent must constantly expand its skills. This article assesses how a pre-trained language model that is task-independent can enhance the sample efficiency of a goal-dependent RL agent. We accomplish this by promoting transfer learning across comparable tasks. We conducted experiments on a series of object navigation tasks to demonstrate the effectiveness of our approach.",1
"In face recognition, designing margin-based (e.g., angular, additive, additive angular margins) softmax loss functions plays an important role in learning discriminative features. However, these hand-crafted heuristic methods are sub-optimal because they require much effort to explore the large design space. Recently, an AutoML for loss function search method AM-LFS has been derived, which leverages reinforcement learning to search loss functions during the training process. But its search space is complex and unstable that hindering its superiority. In this paper, we first analyze that the key to enhance the feature discrimination is actually \textbf{how to reduce the softmax probability}. We then design a unified formulation for the current margin-based softmax losses. Accordingly, we define a novel search space and develop a reward-guided search method to automatically obtain the best candidate. Experimental results on a variety of face recognition benchmarks have demonstrated the effectiveness of our method over the state-of-the-art alternatives.",0
"The creation of margin-based softmax loss functions is crucial in face recognition for developing discriminative features. However, manually designing such functions is not optimal due to the extensive effort required to explore the design space. A new method called AM-LFS uses reinforcement learning to search for loss functions during training, but its search space is complex and unstable, limiting its effectiveness. This paper proposes a solution by analyzing the importance of reducing softmax probability to enhance feature discrimination. A unified formulation for margin-based softmax losses is designed, and a novel search space is defined, along with a reward-guided search method to automatically find the best candidate. The results demonstrate the superiority of our method over other state-of-the-art alternatives on various face recognition benchmarks.",1
"In this work, we propose KeRNS: an algorithm for episodic reinforcement learning in non-stationary Markov Decision Processes (MDPs) whose state-action set is endowed with a metric. Using a non-parametric model of the MDP built with time-dependent kernels, we prove a regret bound that scales with the covering dimension of the state-action space and the total variation of the MDP with time, which quantifies its level of non-stationarity. Our method generalizes previous approaches based on sliding windows and exponential discounting used to handle changing environments. We further propose a practical implementation of KeRNS, we analyze its regret and validate it experimentally.",0
"The paper introduces KeRNS, an algorithm designed for episodic reinforcement learning in non-stationary Markov Decision Processes (MDPs) where the state-action set has a metric. It utilizes a non-parametric MDP model constructed with time-dependent kernels and presents a regret bound that depends on the covering dimension of the state-action space and the total variation of the MDP with time, which measures its degree of non-stationarity. KeRNS expands on previous techniques utilizing sliding windows and exponential discounting to address changing environments. The paper also provides a practical implementation of KeRNS, evaluates its regret, and verifies its effectiveness through experimentation.",1
"Reinforcement learning is typically concerned with learning control policies tailored to a particular agent. We investigate whether there exists a single global policy that can generalize to control a wide variety of agent morphologies -- ones in which even dimensionality of state and action spaces changes. We propose to express this global policy as a collection of identical modular neural networks, dubbed as Shared Modular Policies (SMP), that correspond to each of the agent's actuators. Every module is only responsible for controlling its corresponding actuator and receives information from only its local sensors. In addition, messages are passed between modules, propagating information between distant modules. We show that a single modular policy can successfully generate locomotion behaviors for several planar agents with different skeletal structures such as monopod hoppers, quadrupeds, bipeds, and generalize to variants not seen during training -- a process that would normally require training and manual hyperparameter tuning for each morphology. We observe that a wide variety of drastically diverse locomotion styles across morphologies as well as centralized coordination emerges via message passing between decentralized modules purely from the reinforcement learning objective. Videos and code at https://huangwl18.github.io/modular-rl/",0
"The focus of reinforcement learning is typically on developing control policies that are specific to a particular agent. Our research delves into the possibility of a singular global policy that can be applied to control a range of agent morphologies, even those with varying state and action space dimensions. To achieve this, we propose the use of Shared Modular Policies (SMP) which consist of identical modular neural networks, each corresponding to an individual actuator of the agent. Each module is responsible for controlling its actuator and only receives information from its local sensors. We also incorporate message passing between modules to propagate information between distant modules. Our findings demonstrate that a single modular policy can successfully generate locomotion behaviors for various planar agents with differing skeletal structures, including monopod hoppers, quadrupeds, and bipeds. Furthermore, this policy is able to generalize to variants not encountered during training, without requiring additional training or manual hyperparameter tuning for each morphology. We have observed diverse locomotion styles across various morphologies, as well as centralized coordination emerging purely from the reinforcement learning objective. For more information, videos and code can be found at https://huangwl18.github.io/modular-rl/.",1
"Self-play, where the algorithm learns by playing against itself without requiring any direct supervision, has become the new weapon in modern Reinforcement Learning (RL) for achieving superhuman performance in practice. However, the majority of exisiting theory in reinforcement learning only applies to the setting where the agent plays against a fixed environment; it remains largely open whether self-play algorithms can be provably effective, especially when it is necessary to manage the exploration/exploitation tradeoff. We study self-play in competitive reinforcement learning under the setting of Markov games, a generalization of Markov decision processes to the two-player case. We introduce a self-play algorithm---Value Iteration with Upper/Lower Confidence Bound (VI-ULCB)---and show that it achieves regret $\tilde{\mathcal{O}}(\sqrt{T})$ after playing $T$ steps of the game, where the regret is measured by the agent's performance against a \emph{fully adversarial} opponent who can exploit the agent's strategy at \emph{any} step. We also introduce an explore-then-exploit style algorithm, which achieves a slightly worse regret of $\tilde{\mathcal{O}}(T^{2/3})$, but is guaranteed to run in polynomial time even in the worst case. To the best of our knowledge, our work presents the first line of provably sample-efficient self-play algorithms for competitive reinforcement learning.",0
"In Reinforcement Learning (RL), self-play has emerged as a powerful technique for achieving superhuman performance without requiring direct supervision. However, most existing RL theory assumes that the agent plays against a fixed environment, which does not apply to self-play algorithms. It is unclear whether self-play algorithms can effectively manage the exploration/exploitation tradeoff. In this study, we focus on self-play in competitive RL using Markov games, which generalize Markov decision processes to the two-player case. We introduce a self-play algorithm called Value Iteration with Upper/Lower Confidence Bound (VI-ULCB), which achieves a regret of $\tilde{\mathcal{O}}(\sqrt{T})$ after playing $T$ steps against a fully adversarial opponent. We also propose an explore-then-exploit algorithm with a slightly worse regret of $\tilde{\mathcal{O}}(T^{2/3})$, but is guaranteed to run in polynomial time. Our work presents the first line of sample-efficient self-play algorithms for competitive RL.",1
"Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance. Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning. However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and identify variational autoencoders, used by previous investigations, as the cause of the divergence. Following these findings, we propose effective techniques to improve training stability. This results in a simple approach capable of matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.",0
"It has been challenging to teach an agent to solve control tasks directly from high-dimensional images using model-free reinforcement learning (RL). One promising strategy is to learn a latent representation alongside the control policy. However, constructing a high-capacity encoder with limited reward signal is inefficient and can lead to subpar performance. Previous research has shown that auxiliary losses, such as image reconstruction, can aid in efficient representation learning. However, integrating reconstruction loss into an off-policy learning algorithm frequently leads to training instability. We explore the underlying causes and pinpoint variational autoencoders, which were employed by previous studies, as the source of the divergence. Based on these findings, we suggest effective techniques to enhance training stability. As a result, we present a straightforward approach that can match state-of-the-art model-free and model-based algorithms for MuJoCo control tasks. Additionally, our approach demonstrates resilience to observational noise, surpassing existing approaches in this context. Our code, results, and videos are available anonymously at https://sites.google.com/view/sac-ae/home.",1
"Continuous control tasks in reinforcement learning are important because they provide an important framework for learning in high-dimensional state spaces with deceptive rewards, where the agent can easily become trapped into suboptimal solutions. One way to avoid local optima is to use a population of agents to ensure coverage of the policy space, yet learning a population with the ""best"" coverage is still an open problem. In this work, we present a novel approach to population-based RL in continuous control that leverages properties of normalizing flows to perform attractive and repulsive operations between current members of the population and previously observed policies. Empirical results on the MuJoCo suite demonstrate a high performance gain for our algorithm compared to prior work, including Soft-Actor Critic (SAC).",0
"Reinforcement learning relies on continuous control tasks to effectively learn in complex state spaces with misleading rewards, as the agent may struggle to find optimal solutions. To avoid getting stuck in suboptimal solutions, having a population of agents can ensure policy space coverage. However, determining the most effective population for optimal coverage remains a challenge. Our new approach to population-based RL in continuous control uses normalizing flows to generate attractive and repulsive operations among current and past policies, resulting in significant performance improvements compared to previous methods such as Soft-Actor Critic (SAC). This is demonstrated through empirical results on the MuJoCo suite.",1
"A novel method to identify salient computational paths within randomly wired neural networks before training is proposed. The computational graph is pruned based on a node mass probability function defined by local graph measures and weighted by hyperparameters produced by a reinforcement learning-based controller neural network. We use the definition of Ricci curvature to remove edges of low importance before mapping the computational graph to a neural network. We show a reduction of almost $35\%$ in the number of floating-point operations (FLOPs) per pass, with no degradation in performance. Further, our method can successfully regularize randomly wired neural networks based on purely structural properties, and also find that the favourable characteristics identified in one network generalise to other networks. The method produces networks with better performance under similar compression to those pruned by lowest-magnitude weights. To our best knowledge, this is the first work on pruning randomly wired neural networks, as well as the first to utilize the topological measure of Ricci curvature in the pruning mechanism.",0
"This paper presents a new approach for identifying important computational paths in randomly wired neural networks prior to training. The proposed method involves pruning the computational graph using a node mass probability function that considers local graph measures and hyperparameters from a reinforcement learning-based controller neural network. To further simplify the graph, the definition of Ricci curvature is used to eliminate edges of low significance. This approach results in a significant reduction in the number of floating-point operations (FLOPs) per pass without sacrificing performance. Additionally, this method can be used to regulate randomly wired neural networks based on their structural properties and can be applied to other networks. The pruned networks exhibit better performance than those pruned based on the lowest-magnitude weights. This work is the first to utilize the topological measure of Ricci curvature in pruning randomly wired neural networks.",1
"Existing work on risk-sensitive reinforcement learning - both for symmetric and downside risk measures - has typically used direct Monte-Carlo estimation of policy gradients. While this approach yields unbiased gradient estimates, it also suffers from high variance and decreased sample efficiency compared to temporal-difference methods. In this paper, we study prediction and control with aversion to downside risk which we gauge by the lower partial moment of the return. We introduce a new Bellman equation that upper bounds the lower partial moment, circumventing its non-linearity. We prove that this proxy for the lower partial moment is a contraction, and provide intuition into the stability of the algorithm by variance decomposition. This allows sample-efficient, on-line estimation of partial moments. For risk-sensitive control, we instantiate Reward Constrained Policy Optimization, a recent actor-critic method for finding constrained policies, with our proxy for the lower partial moment. We extend the method to use natural policy gradients and demonstrate the effectiveness of our approach on three benchmark problems for risk-sensitive reinforcement learning.",0
"Previous research on risk-sensitive reinforcement learning, including both symmetric and downside risk measures, has often relied on direct Monte-Carlo estimation of policy gradients. However, this method has several drawbacks, including high variance and reduced sample efficiency compared to temporal-difference methods. In this paper, we focus on prediction and control with a preference for avoiding downside risk, which we measure using the lower partial moment of the return. To address the non-linearity of this measure, we introduce a new Bellman equation that provides an upper bound. We show that this bound is a contraction and use variance decomposition to explain the stability of our algorithm. Our method enables efficient on-line estimation of partial moments. For risk-sensitive control, we apply our approach to Reward Constrained Policy Optimization, an actor-critic method for finding constrained policies. We extend this method to use natural policy gradients and demonstrate its effectiveness on three benchmark problems in risk-sensitive reinforcement learning.",1
"We study the problem of learning efficient algorithms that strongly generalize in the framework of neural program induction. By carefully designing the input / output interfaces of the neural model and through imitation, we are able to learn models that produce correct results for arbitrary input sizes, achieving strong generalization. Moreover, by using reinforcement learning, we optimize for program efficiency metrics, and discover new algorithms that surpass the teacher used in imitation. With this, our approach can learn to outperform custom-written solutions for a variety of problems, as we tested it on sorting, searching in ordered lists and the NP-complete 0/1 knapsack problem, which sets a notable milestone in the field of Neural Program Induction. As highlights, our learned model can perform sorting perfectly on any input data size we tested on, with $O(n log n)$ complexity, whilst outperforming hand-coded algorithms, including quick sort, in number of operations even for list sizes far beyond those seen during training.",0
"In the realm of neural program induction, we examine the challenge of developing effective algorithms that can generalize extensively. By carefully configuring the neural model's input/output interfaces and utilizing imitation, we can acquire models that yield accurate outputs for inputs of varying sizes, demonstrating strong generalization. Additionally, through reinforcement learning, we can optimize for program efficiency metrics, uncovering new algorithms that exceed the teacher used in imitation. As evidenced in our experiments with sorting, searching ordered lists, and the NP-complete 0/1 knapsack problem, our method can surpass custom-written solutions, marking a significant milestone in the field of Neural Program Induction. Notably, our learned model can accomplish flawless sorting for any input size tested, with $O(n log n)$ complexity, and outperform hand-coded algorithms, such as quick sort, in terms of operation count, even for lists significantly larger than those seen during training.",1
"This work is devoted to unresolved problems of Artificial General Intelligence - the inefficiency of transfer learning. One of the mechanisms that are used to solve this problem in the area of reinforcement learning is a model-based approach. In the paper we are expanding the schema networks method which allows to extract the logical relationships between objects and actions from the environment data. We present algorithms for training a Delta Schema Network (DSN), predicting future states of the environment and planning actions that will lead to positive reward. DSN shows strong performance of transfer learning on the classic Atari game environment.",0
"The main focus of this study is on the outstanding issues concerning Artificial General Intelligence, particularly the inadequacy of transfer learning. The reinforcement learning field utilizes a model-based approach to address this challenge, and we have extended the schema networks technique in this paper. This technique enables the extraction of logical connections between objects and actions from environmental data. We also introduce algorithms that train a Delta Schema Network (DSN), which anticipates future environmental states and devises action plans resulting in favorable outcomes. DSN has demonstrated exceptional transfer learning performance on classic Atari game environments.",1
"Catastrophic interference is common in many network-based learning systems, and many proposals exist for mitigating it. But, before we overcome interference we must understand it better. In this work, we provide a definition of interference for control in reinforcement learning. We systematically evaluate our new measures, by assessing correlation with several measures of learning performance, including stability, sample efficiency, and online and offline control performance across a variety of learning architectures. Our new interference measure allows us to ask novel scientific questions about commonly used deep learning architectures. In particular we show that target network frequency is a dominating factor for interference, and that updates on the last layer result in significantly higher interference than updates internal to the network. This new measure can be expensive to compute; we conclude with motivation for an efficient proxy measure and empirically demonstrate it is correlated with our definition of interference.",0
"The issue of catastrophic interference is prevalent in network-based learning systems, and various solutions have been proposed to address it. However, in order to effectively tackle interference, it is important to have a deeper understanding of it. This study presents a new definition of interference for control in reinforcement learning and conducts a systematic assessment of its correlation with multiple measures of learning performance. The new interference measure enables the exploration of new scientific inquiries regarding commonly utilized deep learning architectures. The findings suggest that the frequency of target network is a primary factor affecting interference and that updating the last layer results in significantly higher interference compared to internal updates within the network. Although the new measure can be computationally expensive, the study proposes an efficient proxy measure that is correlated with the interference definition.",1
"Sample complexity bounds are a common performance metric in the Reinforcement Learning literature. In the discounted cost, infinite horizon setting, all of the known bounds have a factor that is a polynomial in $1/(1-\gamma)$, where $\gamma < 1$ is the discount factor. For a large discount factor, these bounds seem to imply that a very large number of samples is required to achieve an $\varepsilon$-optimal policy. The objective of the present work is to introduce a new class of algorithms that have sample complexity uniformly bounded for all $\gamma < 1$. One may argue that this is impossible, due to a recent min-max lower bound. The explanation is that this previous lower bound is for a specific problem, which we modify, without compromising the ultimate objective of obtaining an $\varepsilon$-optimal policy. Specifically, we show that the asymptotic covariance of the Q-learning algorithm with an optimized step-size sequence is a quadratic function of $1/(1-\gamma)$; an expected, and essentially known result. The new relative Q-learning algorithm proposed here is shown to have asymptotic covariance that is a quadratic in $1/(1- \rho^* \gamma)$, where $1 - \rho^* > 0$ is an upper bound on the spectral gap of an optimal transition matrix.",0
"In the domain of Reinforcement Learning, sample complexity bounds are commonly used to evaluate performance. In the scenario of infinite horizon and discounted cost, all known bounds have a factor that is a polynomial in $1/(1-\gamma)$, with $\gamma<1$ representing the discount factor. For high discount factors, these bounds suggest that achieving an $\varepsilon$-optimal policy requires an exceptionally large number of samples. The aim of this study is to introduce a new set of algorithms with sample complexity that remains bounded for all $\gamma<1"". Some argue this is impossible due to a recent min-max lower bound. However, this lower bound is specific to a problem we modify without compromising the primary goal of obtaining an $\varepsilon$-optimal policy. Specifically, we demonstrate that the Q-learning algorithm's asymptotic covariance is a quadratic function of $1/(1-\gamma)$ when using an optimized step-size sequence. This is a well-known result. Additionally, the proposed relative Q-learning algorithm has an asymptotic covariance that is a quadratic in $1/(1-\rho^*\gamma)$, where $1-\rho^*>0$ is an upper bound on the spectral gap of an optimal transition matrix.",1
"The \textit{Smoothed Bellman Error Embedding} algorithm~\citep{dai2018sbeed}, known as SBEED, was proposed as a provably convergent reinforcement learning algorithm with general nonlinear function approximation. It has been successfully implemented with neural networks and achieved strong empirical results. In this work, we study the theoretical behavior of SBEED in batch-mode reinforcement learning. We prove a near-optimal performance guarantee that depends on the representation power of the used function classes and a tight notion of the distribution shift. Our results improve upon prior guarantees for SBEED in ~\citet{dai2018sbeed} in terms of the dependence on the planning horizon and on the sample size. Our analysis builds on the recent work of ~\citet{Xie2020} which studies a related algorithm MSBO, that could be interpreted as a \textit{non-smooth} counterpart of SBEED.",0
"SBEED, or the Smoothed Bellman Error Embedding algorithm, is an effective reinforcement learning technique with nonlinear function approximation capabilities. It has been implemented successfully using neural networks and has yielded strong empirical results. This study focuses on the theoretical behavior of SBEED in batch-mode reinforcement learning. Our aim is to provide a near-optimal performance guarantee, which depends on the representation power of the function classes used and a precise notion of the distribution shift. Our results are an improvement over prior guarantees for SBEED, as they reduce the dependence on the planning horizon and sample size. Our analysis is based on the recent work of Xie et al. (2020), which explores a related algorithm called MSBO, a non-smooth counterpart of SBEED.",1
"Formation mechanisms are fundamental to the study of complex networks, but learning them from observations is challenging. In real-world domains, one often has access only to the final constructed graph, instead of the full construction process, and observed graphs exhibit complex structural properties. In this work, we propose GraphOpt, an end-to-end framework that jointly learns an implicit model of graph structure formation and discovers an underlying optimization mechanism in the form of a latent objective function. The learned objective can serve as an explanation for the observed graph properties, thereby lending itself to transfer across different graphs within a domain. GraphOpt poses link formation in graphs as a sequential decision-making process and solves it using maximum entropy inverse reinforcement learning algorithm. Further, it employs a novel continuous latent action space that aids scalability. Empirically, we demonstrate that GraphOpt discovers a latent objective transferable across graphs with different characteristics. GraphOpt also learns a robust stochastic policy that achieves competitive link prediction performance without being explicitly trained on this task and further enables construction of graphs with properties similar to those of the observed graph.",0
"The study of complex networks requires an understanding of their formation mechanisms, but learning these mechanisms from observations can be difficult. Often, only the final constructed graph is available, and observed graphs have intricate structural properties. Our work proposes GraphOpt, an end-to-end framework that simultaneously learns a model of graph formation and discovers an optimization mechanism through a latent objective function. This objective function can explain the observed graph properties and can be transferred to other graphs in the same domain. GraphOpt approaches link formation as a sequential decision-making process and uses a maximum entropy inverse reinforcement learning algorithm. It also utilizes a novel continuous latent action space to improve scalability. Our empirical results demonstrate that GraphOpt can discover a transferable latent objective function and a robust stochastic policy that achieves competitive link prediction performance. Additionally, it can construct graphs with similar properties to the observed graph without explicit training for this task.",1
"Reinforcement Learning (RL) has achieved impressive performance in many complex environments due to the integration with Deep Neural Networks (DNNs). At the same time, Genetic Algorithms (GAs), often seen as a competing approach to RL, had limited success in scaling up to the DNNs required to solve challenging tasks. Contrary to this dichotomic view, in the physical world, evolution and learning are complementary processes that continuously interact. The recently proposed Evolutionary Reinforcement Learning (ERL) framework has demonstrated mutual benefits to performance when combining the two methods. However, ERL has not fully addressed the scalability problem of GAs. In this paper, we show that this problem is rooted in an unfortunate combination of a simple genetic encoding for DNNs and the use of traditional biologically-inspired variation operators. When applied to these encodings, the standard operators are destructive and cause catastrophic forgetting of the traits the networks acquired. We propose a novel algorithm called Proximal Distilled Evolutionary Reinforcement Learning (PDERL) that is characterised by a hierarchical integration between evolution and learning. The main innovation of PDERL is the use of learning-based variation operators that compensate for the simplicity of the genetic representation. Unlike traditional operators, our proposals meet the functional requirements of variation operators when applied on directly-encoded DNNs. We evaluate PDERL in five robot locomotion settings from the OpenAI gym. Our method outperforms ERL, as well as two state-of-the-art RL algorithms, PPO and TD3, in all tested environments.",0
"The combination of Reinforcement Learning (RL) and Deep Neural Networks (DNNs) has yielded impressive results in various complex environments. In contrast, Genetic Algorithms (GAs) have not been as successful in scaling up to solve challenging tasks with DNNs, leading to the perception of competition between the two approaches. However, in the physical world, evolution and learning are interdependent processes that work together to achieve better performance. A recent framework called Evolutionary Reinforcement Learning (ERL) has demonstrated the potential for significant performance benefits when combining RL and GAs. Nonetheless, the scalability issue of GAs persists in ERL due to the use of a simple genetic encoding for DNNs and traditional variation operators, which can cause catastrophic forgetting of learned traits. To address this problem, we propose a new algorithm called Proximal Distilled Evolutionary Reinforcement Learning (PDERL). PDERL integrates evolution and learning hierarchically, using learning-based variation operators that compensate for the genetic representation's simplicity. Our proposed method outperforms ERL and two state-of-the-art RL algorithms, PPO and TD3, in five OpenAI gym robot locomotion settings.",1
"Failure of mission-critical equipment interrupts production and results in monetary loss. The risk of unplanned equipment downtime can be minimized through Predictive Maintenance of revenue generating assets to ensure optimal performance and safe operation of equipment. However, the increased sensorization of the equipment generates a data deluge, and existing machine-learning based predictive model alone becomes inadequate for timely equipment condition predictions. In this paper, a model-free Deep Reinforcement Learning algorithm is proposed for predictive equipment maintenance from an equipment-based sensor network context. Within each equipment, a sensor device aggregates raw sensor data, and the equipment health status is analyzed for anomalous events. Unlike traditional black-box regression models, the proposed algorithm self-learns an optimal maintenance policy and provides actionable recommendation for each equipment. Our experimental results demonstrate the potential for broader range of equipment maintenance applications as an automatic learning framework.",0
"When mission-critical equipment fails, it causes production to halt and can result in financial loss. One way to minimize the risk of unexpected equipment downtime is through Predictive Maintenance of revenue-generating assets. This ensures that the equipment performs optimally and operates safely. However, as more sensors are added to the equipment, there is an overwhelming amount of data to analyze. Existing machine-learning models are not sufficient for timely equipment condition predictions. To address this, a model-free Deep Reinforcement Learning algorithm is proposed in this paper. This algorithm is specifically designed for predictive equipment maintenance using data from an equipment-based sensor network. Each piece of equipment has a sensor device that collects raw data, and the equipment's health status is monitored for any unusual events. Unlike traditional black-box regression models, the proposed algorithm self-learns the optimal maintenance policy and provides actionable recommendations for each piece of equipment. Experimental results show that this automatic learning framework has the potential for a wide range of equipment maintenance applications.",1
"Radio Frequency powered Cognitive Radio Networks (RF-CRN) are likely to be the eyes and ears of upcoming modern networks such as Internet of Things (IoT), requiring increased decentralization and autonomous operation. To be considered autonomous, the RF-powered network entities need to make decisions locally to maximize the network throughput under the uncertainty of any network environment. However, in complex and large-scale networks, the state and action spaces are usually large, and existing Tabular Reinforcement Learning technique is unable to find the optimal state-action policy quickly. In this paper, deep reinforcement learning is proposed to overcome the mentioned shortcomings and allow a wireless gateway to derive an optimal policy to maximize network throughput. When benchmarked against advanced DQN techniques, our proposed DQN configuration offers performance speedup of up to 1.8x with good overall performance.",0
"RF-CRN is expected to serve as the sensory organs of future networks like IoT, which demand greater autonomy and decentralization. To achieve autonomy, RF-powered network entities must make local decisions that optimize network throughput in uncertain environments. However, in large and complex networks, the state and action spaces are usually vast, and the existing Tabular Reinforcement Learning technique is unable to quickly determine the best state-action policy. This paper proposes deep reinforcement learning as a solution to overcome these limitations and enable a wireless gateway to establish an optimal policy that maximizes network throughput. Our proposed DQN configuration offers a performance speedup of up to 1.8x with good overall performance when compared to advanced DQN techniques.",1
"Distances are pervasive in machine learning. They serve as similarity measures, loss functions, and learning targets; it is said that a good distance measure solves a task. When defining distances, the triangle inequality has proven to be a useful constraint, both theoretically--to prove convergence and optimality guarantees--and empirically--as an inductive bias. Deep metric learning architectures that respect the triangle inequality rely, almost exclusively, on Euclidean distance in the latent space. Though effective, this fails to model two broad classes of subadditive distances, common in graphs and reinforcement learning: asymmetric metrics, and metrics that cannot be embedded into Euclidean space. To address these problems, we introduce novel architectures that are guaranteed to satisfy the triangle inequality. We prove our architectures universally approximate norm-induced metrics on $\mathbb{R}^n$, and present a similar result for modified Input Convex Neural Networks. We show that our architectures outperform existing metric approaches when modeling graph distances and have a better inductive bias than non-metric approaches when training data is limited in the multi-goal reinforcement learning setting.",0
"In machine learning, distances are used for various purposes such as measuring similarity, defining loss functions, and setting learning targets. A good distance measure is crucial for solving a task. The triangle inequality is a helpful constraint for defining distances, as it provides theoretical guarantees for convergence and optimality, as well as an inductive bias. However, Euclidean distance, which is commonly used in deep metric learning architectures, fails to model certain types of subadditive distances, such as asymmetric metrics and those that cannot be embedded into Euclidean space. To address these issues, we propose novel architectures that satisfy the triangle inequality and can universally approximate norm-induced metrics on $\mathbb{R}^n$. We also demonstrate that our architectures outperform existing metric approaches when modeling graph distances and have a better inductive bias than non-metric approaches in limited training data settings for multi-goal reinforcement learning.",1
"While policy-based reinforcement learning (RL) achieves tremendous successes in practice, it is significantly less understood in theory, especially compared with value-based RL. In particular, it remains elusive how to design a provably efficient policy optimization algorithm that incorporates exploration. To bridge such a gap, this paper proposes an Optimistic variant of the Proximal Policy Optimization algorithm (OPPO), which follows an ``optimistic version'' of the policy gradient direction. This paper proves that, in the problem of episodic Markov decision process with linear function approximation, unknown transition, and adversarial reward with full-information feedback, OPPO achieves $\tilde{O}(\sqrt{d^2 H^3 T} )$ regret. Here $d$ is the feature dimension, $H$ is the episode horizon, and $T$ is the total number of steps. To the best of our knowledge, OPPO is the first provably efficient policy optimization algorithm that explores.",0
"Although policy-based reinforcement learning (RL) has been successful in practice, it is not as well-understood in theory as value-based RL. This is especially true when it comes to designing an algorithm for policy optimization that incorporates exploration. To address this gap, the paper introduces the Optimistic variant of the Proximal Policy Optimization algorithm (OPPO), which follows a policy gradient direction that is more optimistic. The paper proves that OPPO achieves $\tilde{O}(\sqrt{d^2 H^3 T} )$ regret in the problem of episodic Markov decision process with linear function approximation, unknown transition, and adversarial reward with full-information feedback. This is the first algorithm that can explore and is provably efficient.",1
"What goals should a multi-goal reinforcement learning agent pursue during training in long-horizon tasks? When the desired (test time) goal distribution is too distant to offer a useful learning signal, we argue that the agent should not pursue unobtainable goals. Instead, it should set its own intrinsic goals that maximize the entropy of the historical achieved goal distribution. We propose to optimize this objective by having the agent pursue past achieved goals in sparsely explored areas of the goal space, which focuses exploration on the frontier of the achievable goal set. We show that our strategy achieves an order of magnitude better sample efficiency than the prior state of the art on long-horizon multi-goal tasks including maze navigation and block stacking.",0
"During training for long-horizon tasks, what are the appropriate goals for a multi-goal reinforcement learning agent? If the desired goal distribution at test time is too far away to provide useful feedback, we suggest that the agent should not pursue unattainable goals. Instead, the agent should establish its own intrinsic goals that maximize the entropy of the historical achieved goal distribution. Our approach involves having the agent aim for past achieved goals in sparsely explored areas of the goal space, which concentrates exploration on the frontier of the achievable goal set. We demonstrate that our method achieves sample efficiency that is an order of magnitude better than the previous state of the art for long-horizon multi-goal tasks such as block stacking and maze navigation.",1
"We investigate whether Jacobi preconditioning, accounting for the bootstrap term in temporal difference (TD) learning, can help boost performance of adaptive optimizers. Our method, TDprop, computes a per parameter learning rate based on the diagonal preconditioning of the TD update rule. We show how this can be used in both $n$-step returns and TD($\lambda$). Our theoretical findings demonstrate that including this additional preconditioning information is, surprisingly, comparable to normal semi-gradient TD if the optimal learning rate is found for both via a hyperparameter search. In Deep RL experiments using Expected SARSA, TDprop meets or exceeds the performance of Adam in all tested games under near-optimal learning rates, but a well-tuned SGD can yield similar improvements -- matching our theory. Our findings suggest that Jacobi preconditioning may improve upon typical adaptive optimization methods in Deep RL, but despite incorporating additional information from the TD bootstrap term, may not always be better than SGD.",0
"The aim of our study is to determine whether incorporating Jacobi preconditioning, which considers the bootstrap term in temporal difference (TD) learning, can enhance the efficiency of adaptive optimizers. Our approach, TDprop, calculates a customized learning rate for each parameter by using the diagonal preconditioning of the TD update rule. We demonstrate how this technique can be utilized in both $n$-step returns and TD($\lambda$). Our research indicates that incorporating this extra preconditioning information is surprisingly comparable to normal semi-gradient TD, provided that the optimal learning rate is identified for both methods via a hyperparameter search. In deep reinforcement learning experiments employing Expected SARSA, TDprop outperforms Adam in all evaluated games under near-optimal learning rates. However, a well-tuned SGD can produce comparable improvements, matching our theory. Our results suggest that Jacobi preconditioning may improve upon standard adaptive optimization methods in deep reinforcement learning. Nonetheless, despite incorporating additional information from the TD bootstrap term, it may not always be superior to SGD.",1
"There are several ways to categorise reinforcement learning (RL) algorithms, such as either model-based or model-free, policy-based or planning-based, on-policy or off-policy, and online or offline. Broad classification schemes such as these help provide a unified perspective on disparate techniques and can contextualise and guide the development of new algorithms. In this paper, we utilise the control as inference framework to outline a novel classification scheme based on amortised and iterative inference. We demonstrate that a wide range of algorithms can be classified in this manner providing a fresh perspective and highlighting a range of existing similarities. Moreover, we show that taking this perspective allows us to identify parts of the algorithmic design space which have been relatively unexplored, suggesting new routes to innovative RL algorithms.",0
"Reinforcement learning (RL) algorithms can be classified in various ways, including model-based or model-free, policy-based or planning-based, on-policy or off-policy, and online or offline. These classifications help to provide a consistent view of different techniques and guide the development of new algorithms. This paper proposes a new classification scheme based on amortised and iterative inference, utilizing the control as inference framework. By using this approach, we can classify a wide range of algorithms and identify similarities between them. Additionally, this perspective opens up new and unexplored areas in algorithmic design, presenting new opportunities for innovative RL algorithms.",1
"In recent years, reinforcement learning (RL) systems with general goals beyond a cumulative sum of rewards have gained traction, such as in constrained problems, exploration, and acting upon prior experiences. In this paper, we consider policy optimization in Markov Decision Problems, where the objective is a general concave utility function of the state-action occupancy measure, which subsumes several of the aforementioned examples as special cases. Such generality invalidates the Bellman equation. As this means that dynamic programming no longer works, we focus on direct policy search. Analogously to the Policy Gradient Theorem \cite{sutton2000policy} available for RL with cumulative rewards, we derive a new Variational Policy Gradient Theorem for RL with general utilities, which establishes that the parametrized policy gradient may be obtained as the solution of a stochastic saddle point problem involving the Fenchel dual of the utility function. We develop a variational Monte Carlo gradient estimation algorithm to compute the policy gradient based on sample paths. We prove that the variational policy gradient scheme converges globally to the optimal policy for the general objective, though the optimization problem is nonconvex. We also establish its rate of convergence of the order $O(1/t)$ by exploiting the hidden convexity of the problem, and proves that it converges exponentially when the problem admits hidden strong convexity. Our analysis applies to the standard RL problem with cumulative rewards as a special case, in which case our result improves the available convergence rate.",0
"Recent years have seen a rise in reinforcement learning (RL) systems with objectives that extend beyond a cumulative sum of rewards. These systems have found use in solving constrained problems, exploring new areas, and acting on prior experiences. This paper focuses on policy optimization in Markov Decision Problems, where the objective is a general concave utility function of the state-action occupancy measure. This function encompasses several examples mentioned earlier, rendering the Bellman equation invalid and dynamic programming ineffective. Instead, we concentrate on direct policy search and derive a new Variational Policy Gradient Theorem for RL with general utilities. This theorem establishes that the parametrized policy gradient can be obtained by solving a stochastic saddle point problem involving the Fenchel dual of the utility function. We develop a variational Monte Carlo gradient estimation algorithm to calculate the policy gradient based on sample paths. We show that the variational policy gradient scheme globally converges to the optimal policy for the general objective, even though the optimization problem is nonconvex. Additionally, we prove that it converges exponentially when the problem admits hidden strong convexity. Our analysis applies to the standard RL problem with cumulative rewards and improves the available convergence rate.",1
"Value-based methods constitute a fundamental methodology in planning and deep reinforcement learning (RL). In this paper, we propose to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep RL. In particular, if the underlying system dynamics lead to some global structures of the Q function, one should be capable of inferring the function better by leveraging such structures. Specifically, we investigate the low-rank structure, which widely exists for big data matrices. We verify empirically the existence of low-rank Q functions in the context of control and deep RL tasks. As our key contribution, by leveraging Matrix Estimation (ME) techniques, we propose a general framework to exploit the underlying low-rank structure in Q functions. This leads to a more efficient planning procedure for classical control, and additionally, a simple scheme that can be applied to any value-based RL techniques to consistently achieve better performance on ""low-rank"" tasks. Extensive experiments on control tasks and Atari games confirm the efficacy of our approach. Code is available at https://github.com/YyzHarry/SV-RL.",0
"The use of value-based methods is a crucial aspect of planning and deep reinforcement learning. Our paper proposes the utilization of the state-action value function, or Q function, to enhance both planning and deep RL by exploiting the underlying structures within the Q function. By leveraging the global structures of the Q function, we can infer the function more accurately. We specifically investigate the low-rank structure, which is commonly found in large data matrices. Through empirical evidence in control and deep RL tasks, we confirm the existence of low-rank Q functions. Our main contribution is the development of a general framework that utilizes Matrix Estimation techniques to exploit the low-rank structure within Q functions. The result is a more efficient planning process for classical control and a simple scheme that can improve the performance of value-based RL techniques on ""low-rank"" tasks. Our approach is validated through extensive experiments on control tasks and Atari games. Code for our approach is available at https://github.com/YyzHarry/SV-RL.",1
"Specifying a Reinforcement Learning (RL) task involves choosing a suitable planning horizon, which is typically modeled by a discount factor. It is known that applying RL algorithms with a lower discount factor can act as a regularizer, improving performance in the limited data regime. Yet the exact nature of this regularizer has not been investigated. In this work, we fill in this gap. For several Temporal-Difference (TD) learning methods, we show an explicit equivalence between using a reduced discount factor and adding an explicit regularization term to the algorithm's loss. Motivated by the equivalence, we empirically study this technique compared to standard $L_2$ regularization by extensive experiments in discrete and continuous domains, using tabular and functional representations. Our experiments suggest the regularization effectiveness is strongly related to properties of the available data, such as size, distribution, and mixing rate.",0
"Choosing an appropriate planning horizon is crucial in defining a Reinforcement Learning (RL) task, where the planning horizon is commonly represented by a discount factor. Lowering the discount factor while applying RL algorithms has been known to function as a regularizer, leading to enhanced performance in scenarios with limited data. However, the implications of this regularizer have not been explored in detail. In this study, we bridge this gap by demonstrating an explicit relationship between using a reduced discount factor and introducing an explicit regularization term to the loss of several Temporal-Difference (TD) learning methods. Drawing inspiration from this relationship, we conduct extensive empirical analyses using tabular and functional representations in both discrete and continuous domains to compare this method's effectiveness with standard $L_2$ regularization. Our findings indicate that the effectiveness of regularization is closely linked to the characteristics of the available data, such as size, distribution, and mixing rate.",1
"The principle of optimism in the face of uncertainty underpins many theoretically successful reinforcement learning algorithms. In this paper we provide a general framework for designing, analyzing and implementing such algorithms in the episodic reinforcement learning problem. This framework is built upon Lagrangian duality, and demonstrates that every model-optimistic algorithm that constructs an optimistic MDP has an equivalent representation as a value-optimistic dynamic programming algorithm. Typically, it was thought that these two classes of algorithms were distinct, with model-optimistic algorithms benefiting from a cleaner probabilistic analysis while value-optimistic algorithms are easier to implement and thus more practical. With the framework developed in this paper, we show that it is possible to get the best of both worlds by providing a class of algorithms which have a computationally efficient dynamic-programming implementation and also a simple probabilistic analysis. Besides being able to capture many existing algorithms in the tabular setting, our framework can also address largescale problems under realizable function approximation, where it enables a simple model-based analysis of some recently proposed methods.",0
"Many successful reinforcement learning algorithms are based on the principle of optimism in uncertain situations. This paper presents a general framework for designing, analyzing, and implementing such algorithms for the episodic reinforcement learning problem. The framework uses Lagrangian duality to show that model-optimistic algorithms, which use an optimistic Markov decision process (MDP), are equivalent to value-optimistic dynamic programming algorithms. Previously, these two types of algorithms were thought to be distinct with different advantages. However, the presented framework shows that it is possible to combine the benefits of both by providing a class of algorithms that are both easy to implement and have a simple probabilistic analysis. The framework can be used for tabular settings as well as large-scale problems with function approximation. It also enables a model-based analysis of some recently proposed methods.",1
"In this paper, we analyze the convergence rate of the gradient temporal difference learning (GTD) family of algorithms. Previous analyses of this class of algorithms use ODE techniques to prove asymptotic convergence, and to the best of our knowledge, no finite-sample analysis has been done. Moreover, there has been not much work on finite-sample analysis for convergent off-policy reinforcement learning algorithms. In this paper, we formulate GTD methods as stochastic gradient algorithms w.r.t.~a primal-dual saddle-point objective function, and then conduct a saddle-point error analysis to obtain finite-sample bounds on their performance. Two revised algorithms are also proposed, namely projected GTD2 and GTD2-MP, which offer improved convergence guarantees and acceleration, respectively. The results of our theoretical analysis show that the GTD family of algorithms are indeed comparable to the existing LSTD methods in off-policy learning scenarios.",0
"The aim of this paper is to examine how quickly the gradient temporal difference learning (GTD) algorithms converge. Previous analyses of this algorithm category have used ODE methods to show asymptotic convergence, but there has been no finite-sample analysis done so far. Additionally, there has been limited research on the finite-sample analysis of off-policy reinforcement learning algorithms that converge. In this study, we transform GTD methods into stochastic gradient algorithms with respect to a primal-dual saddle-point objective function, and then perform a saddle-point error analysis to establish finite-sample bounds for their performance. We propose two revised algorithms, projected GTD2 and GTD2-MP, which offer enhanced convergence guarantees and acceleration, respectively. Our theoretical analysis demonstrates that the GTD family of algorithms are comparable to current LSTD methods in off-policy learning situations.",1
"Due to the realization that deep reinforcement learning algorithms trained on high-dimensional tasks can strongly overfit to their training environments, there have been several studies that investigated the generalization performance of these algorithms. However, there has been no similar study that evaluated the generalization performance of algorithms that were specifically designed for generalization, i.e. meta-reinforcement learning algorithms. In this paper, we assess the generalization performance of these algorithms by leveraging high-dimensional, procedurally generated environments. We find that these algorithms can display strong overfitting when they are evaluated on challenging tasks. We also observe that scalability to high-dimensional tasks with sparse rewards remains a significant problem among many of the current meta-reinforcement learning algorithms. With these results, we highlight the need for developing meta-reinforcement learning algorithms that can both generalize and scale.",0
"Several studies have shown that deep reinforcement learning algorithms can overfit when trained on high-dimensional tasks. However, there has been little research into the generalization performance of meta-reinforcement learning algorithms, which are designed to be more adaptable. To evaluate these algorithms, we tested them on complex, procedurally generated environments and found that they can still experience overfitting on difficult tasks. Additionally, many of the current meta-reinforcement learning algorithms struggle to scale to high-dimensional tasks with sparse rewards. Our findings demonstrate the importance of developing meta-reinforcement learning algorithms that can both generalize and scale effectively.",1
"The construction of replication strategies for contingent claims in the presence of risk and market friction is a key problem of financial engineering. In real markets, continuous replication, such as in the model of Black, Scholes and Merton, is not only unrealistic but it is also undesirable due to high transaction costs. Over the last decades stochastic optimal-control methods have been developed to balance between effective replication and losses. More recently, with the rise of artificial intelligence, temporal-difference Reinforcement Learning, in particular variations of $Q$-learning in conjunction with Deep Neural Networks, have attracted significant interest. From a practical point of view, however, such methods are often relatively sample inefficient, hard to train and lack performance guarantees. This motivates the investigation of a stable benchmark algorithm for hedging. In this article, the hedging problem is viewed as an instance of a risk-averse contextual $k$-armed bandit problem, for which a large body of theoretical results and well-studied algorithms are available. We find that the $k$-armed bandit model naturally fits to the $P\&L$ formulation of hedging, providing for a more accurate and sample efficient approach than $Q$-learning and reducing to the Black-Scholes model in the absence of transaction costs and risks.",0
"Developing replication strategies for contingent claims in the presence of risk and market friction is a crucial challenge in financial engineering. The continuous replication approach modeled by Black, Scholes, and Merton is neither practical nor desirable due to the high transaction costs involved in real markets. To address this issue, stochastic optimal-control techniques have been used to balance effectiveness and losses. However, while temporal-difference Reinforcement Learning, specifically variations of $Q$-learning with Deep Neural Networks, have recently gained widespread attention, these methods are often inefficient, difficult to train, and lack performance guarantees. Therefore, it is necessary to investigate a stable benchmark algorithm for hedging. This article suggests viewing the hedging problem as a risk-averse contextual $k$-armed bandit problem, which has a large body of theoretical results and well-studied algorithms available. The $k$-armed bandit model fits naturally to the $P\&L$ formulation of hedging, providing a more accurate and sample-efficient approach than $Q$-learning and reducing to the Black-Scholes model in the absence of transaction costs and risks.",1
"Option discovery and skill acquisition frameworks are integral to the functioning of a Hierarchically organized Reinforcement learning agent. However, such techniques often yield a large number of options or skills, which can potentially be represented succinctly by filtering out any redundant information. Such a reduction can reduce the required computation while also improving the performance on a target task. In order to compress an array of option policies, we attempt to find a policy basis that accurately captures the set of all options. In this work, we propose Option Encoder, an auto-encoder based framework with intelligently constrained weights, that helps discover a collection of basis policies. The policy basis can be used as a proxy for the original set of skills in a suitable hierarchically organized framework. We demonstrate the efficacy of our method on a collection of grid-worlds and on the high-dimensional Fetch-Reach robotic manipulation task by evaluating the obtained policy basis on a set of downstream tasks.",0
"To make a Hierarchically organized Reinforcement learning agent function properly, it is crucial to have frameworks for option discovery and skill acquisition. However, these techniques often produce a large number of options or skills, which can be simplified by removing any repetitive information. This simplification can reduce computation time and improve performance on a specific task. To compress a group of option policies, we aim to identify a policy basis that accurately represents all options. Our proposal is Option Encoder, an auto-encoder based framework that uses intelligently constrained weights to discover a collection of basis policies. This policy basis can be used as a substitute for the original set of skills in a suitable hierarchical framework. We tested our method on various grid-worlds and the high-dimensional Fetch-Reach robotic manipulation task by assessing the policy basis on a set of downstream tasks, proving its effectiveness.",1
"In optical coherence tomography (OCT) volumes of retina, the sequential acquisition of the individual slices makes this modality prone to motion artifacts, misalignments between adjacent slices being the most noticeable. Any distortion in OCT volumes can bias structural analysis and influence the outcome of longitudinal studies. On the other hand, presence of speckle noise that is characteristic of this imaging modality, leads to inaccuracies when traditional registration techniques are employed. Also, the lack of a well-defined ground truth makes supervised deep-learning techniques ill-posed to tackle the problem. In this paper, we tackle these issues by using deep reinforcement learning to correct inter-frame movements in an unsupervised manner. Specifically, we use dueling deep Q-network to train an artificial agent to find the optimal policy, i.e. a sequence of actions, that best improves the alignment by maximizing the sum of reward signals. Instead of relying on the ground-truth of transformation parameters to guide the rewarding system, for the first time, we use a combination of intensity based image similarity metrics. Further, to avoid the agent bias towards speckle noise, we ensure the agent can see retinal layers as part of the interacting environment. For quantitative evaluation, we simulate the eye movement artifacts by applying 2D rigid transformations on individual B-scans. The proposed model achieves an average of 0.985 and 0.914 for normalized mutual information and correlation coefficient, respectively. We also compare our model with elastix intensity based medical image registration approach, where significant improvement is achieved by our model for both noisy and denoised volumes.",0
"When using optical coherence tomography (OCT) to capture retina volumes, motion artifacts can occur due to the sequential acquisition of individual slices, causing misalignments between adjacent slices. Such distortions can negatively affect the accuracy of structural analysis and impact longitudinal studies. Traditional registration techniques are also prone to errors due to the presence of speckle noise, which is characteristic of OCT imaging, and the lack of a well-defined ground truth. To address these issues, we propose using deep reinforcement learning to correct inter-frame movements in an unsupervised manner. Our approach utilizes a dueling deep Q-network to train an artificial agent to find the optimal policy that improves alignment by maximizing reward signals, with intensity based image similarity metrics used instead of transformation parameters as guides. To avoid agent bias towards speckle noise, we ensure that retinal layers are part of the interacting environment. We evaluate our model by simulating eye movement artifacts and achieve an average of 0.985 and 0.914 for normalized mutual information and correlation coefficient, respectively. Comparison with elastix intensity based medical image registration approach shows significant improvement for both noisy and denoised volumes.",1
"Generalization remains a challenging problem for deep reinforcement learning algorithms, which are often trained and tested on the same set of deterministic game environments. When test environments are unseen and perturbed but the nature of the task remains the same, generalization gaps can arise. In this work, we propose and evaluate a surprise minimizing agent on a generalization benchmark to show an additional reward learned from a simple density model can show robustness in procedurally generated game environments that provide constant source of entropy and stochasticity.",0
"Deep reinforcement learning algorithms encounter difficulty in generalizing, as they are typically trained and evaluated on the same set of deterministic game environments. When test environments are unfamiliar and altered while the task remains constant, generalization gaps may emerge. Our research presents a surprise-minimizing agent that we tested on a generalization benchmark. We demonstrate that an extra reward acquired from a basic density model can establish resilience in procedurally generated game environments that generate a steady stream of entropy and stochasticity.",1
"Deployments of Bayesian Optimization (BO) for functions with stochastic evaluations, such as parameter tuning via cross validation and simulation optimization, typically optimize an average of a fixed set of noisy realizations of the objective function. However, disregarding the true objective function in this manner finds a high-precision optimum of the wrong function. To solve this problem, we propose Bayesian Optimization by Sampling Hierarchically (BOSH), a novel BO routine pairing a hierarchical Gaussian process with an information-theoretic framework to generate a growing pool of realizations as the optimization progresses. We demonstrate that BOSH provides more efficient and higher-precision optimization than standard BO across synthetic benchmarks, simulation optimization, reinforcement learning and hyper-parameter tuning tasks.",0
"Usually, when using Bayesian Optimization (BO) for functions that involve stochastic evaluations, like cross validation for parameter tuning and simulation optimization, an average of a set of fixed, noisy realizations of the objective function is optimized. However, this approach ignores the true objective function and may lead to finding the optimal solution for the wrong function. To address this issue, our proposed approach is Bayesian Optimization by Sampling Hierarchically (BOSH), which combines a hierarchical Gaussian process with an information-theoretic framework to generate an expanding pool of realizations during the optimization process. We have shown that BOSH is more effective and accurate than standard BO for various tasks, including synthetic benchmarks, simulation optimization, reinforcement learning, and hyper-parameter tuning.",1
"Resolving the exploration-exploitation trade-off remains a fundamental problem in the design and implementation of reinforcement learning (RL) algorithms. In this paper, we focus on model-free RL using the epsilon-greedy exploration policy, which despite its simplicity, remains one of the most frequently used forms of exploration. However, a key limitation of this policy is the specification of $\varepsilon$. In this paper, we provide a novel Bayesian perspective of $\varepsilon$ as a measure of the uniformity of the Q-value function. We introduce a closed-form Bayesian model update based on Bayesian model combination (BMC), based on this new perspective, which allows us to adapt $\varepsilon$ using experiences from the environment in constant time with monotone convergence guarantees. We demonstrate that our proposed algorithm, $\varepsilon$-\texttt{BMC}, efficiently balances exploration and exploitation on different problems, performing comparably or outperforming the best tuned fixed annealing schedules and an alternative data-dependent $\varepsilon$ adaptation scheme proposed in the literature.",0
"The exploration-exploitation trade-off is a fundamental challenge in developing reinforcement learning (RL) algorithms. Our focus in this study is on model-free RL, which commonly employs the epsilon-greedy exploration policy. However, a major issue with this policy is the need to specify the value of epsilon. To address this, we introduce a Bayesian perspective that views epsilon as a measure of the uniformity of the Q-value function. Using Bayesian model combination (BMC), we develop a closed-form Bayesian model update that enables us to adapt epsilon in constant time with monotone convergence guarantees. Our proposed algorithm, epsilon-BMC, effectively balances exploration and exploitation on diverse problems and achieves comparable or superior results to the best tuned fixed annealing schedules and a data-dependent epsilon adaptation scheme from prior research.",1
"We are interested in how to design reinforcement learning agents that provably reduce the sample complexity for learning new tasks by transferring knowledge from previously-solved ones. The availability of solutions to related problems poses a fundamental trade-off: whether to seek policies that are expected to achieve high (yet sub-optimal) performance in the new task immediately or whether to seek information to quickly identify an optimal solution, potentially at the cost of poor initial behavior. In this work, we focus on the second objective when the agent has access to a generative model of state-action pairs. First, given a set of solved tasks containing an approximation of the target one, we design an algorithm that quickly identifies an accurate solution by seeking the state-action pairs that are most informative for this purpose. We derive PAC bounds on its sample complexity which clearly demonstrate the benefits of using this kind of prior knowledge. Then, we show how to learn these approximate tasks sequentially by reducing our transfer setting to a hidden Markov model and employing spectral methods to recover its parameters. Finally, we empirically verify our theoretical findings in simple simulated domains.",0
"Our focus is on creating reinforcement learning agents that can effectively learn new tasks by leveraging knowledge gained from previously-solved ones, while reducing the sample complexity. However, the availability of related solutions presents a challenge - should the agent prioritize quick performance in the new task, even if it is suboptimal, or should it take time to identify an optimal solution, potentially at the cost of poor initial behavior? In this study, we prioritize the latter approach, assuming that the agent has access to a generative model of state-action pairs. Our algorithm seeks out the most informative state-action pairs to quickly identify an accurate solution based on a set of solved tasks that approximate the target task. We derive PAC bounds to demonstrate the advantages of this approach. We also demonstrate how to learn these approximate tasks sequentially using spectral methods to recover hidden Markov model parameters. Finally, we conduct simulations to validate our theoretical findings in simple domains.",1
"High dimensional black-box optimization has broad applications but remains a challenging problem to solve. Given a set of samples $\{\vx_i, y_i\}$, building a global model (like Bayesian Optimization (BO)) suffers from the curse of dimensionality in the high-dimensional search space, while a greedy search may lead to sub-optimality. By recursively splitting the search space into regions with high/low function values, recent works like LaNAS shows good performance in Neural Architecture Search (NAS), reducing the sample complexity empirically. In this paper, we coin LA-MCTS that extends LaNAS to other domains. Unlike previous approaches, LA-MCTS learns the partition of the search space using a few samples and their function values in an online fashion. While LaNAS uses linear partition and performs uniform sampling in each region, our LA-MCTS adopts a nonlinear decision boundary and learns a local model to pick good candidates. If the nonlinear partition function and the local model fits well with ground-truth black-box function, then good partitions and candidates can be reached with much fewer samples. LA-MCTS serves as a \emph{meta-algorithm} by using existing black-box optimizers (e.g., BO, TuRBO) as its local models, achieving strong performance in general black-box optimization and reinforcement learning benchmarks, in particular for high-dimensional problems.",0
"Solving high dimensional black-box optimization is a problem that has many practical applications, but is still challenging. When attempting to build a global model from a set of samples, such as with Bayesian Optimization, the high-dimensional search space makes this difficult. Alternatively, a greedy search may lead to sub-optimal results. Recent works, like LaNAS, have shown good performance in Neural Architecture Search (NAS) by recursively splitting the search space into regions with high/low function values. This reduces sample complexity empirically. In this paper, we introduce LA-MCTS which extends LaNAS to other domains. Unlike previous approaches, LA-MCTS learns the partition of the search space in an online fashion using a few samples and their function values. LA-MCTS uses a nonlinear decision boundary and learns a local model to pick good candidates. If the partition function and local model fit well with the ground-truth black-box function, good partitions and candidates can be reached with fewer samples. LA-MCTS is a meta-algorithm that uses existing black-box optimizers as its local models, achieving strong performance in general black-box optimization and reinforcement learning benchmarks, particularly for high-dimensional problems.",1
"Bayesian reinforcement learning (BRL) offers a decision-theoretic solution for reinforcement learning. While ""model-based"" BRL algorithms have focused either on maintaining a posterior distribution on models or value functions and combining this with approximate dynamic programming or tree search, previous Bayesian ""model-free"" value function distribution approaches implicitly make strong assumptions or approximations. We describe a novel Bayesian framework, Inferential Induction, for correctly inferring value function distributions from data, which leads to the development of a new class of BRL algorithms. We design an algorithm, Bayesian Backwards Induction, with this framework. We experimentally demonstrate that the proposed algorithm is competitive with respect to the state of the art.",0
"Reinforcement learning can be approached from a decision-theoretic perspective with Bayesian reinforcement learning (BRL). While ""model-based"" BRL methods have focused on either maintaining a posterior distribution on models or value functions and combining with approximate dynamic programming or tree search, previous Bayesian ""model-free"" value function distribution approaches have relied on strong assumptions or approximations. To address this limitation, we introduce a new Bayesian framework called Inferential Induction that allows for the correct inference of value function distributions from data, resulting in a new class of BRL algorithms. Using this framework, we develop an algorithm called Bayesian Backwards Induction and demonstrate through experimentation that it is competitive with state-of-the-art methods.",1
"Accurate and reliable prediction of hospital admission location is important due to resource-constraints and space availability in a clinical setting, particularly when dealing with patients who come from the emergency department. In this work we propose a student-teacher network via reinforcement learning to deal with this specific problem. A representation of the weights of the student network is treated as the state and is fed as an input to the teacher network. The teacher network's action is to select the most appropriate batch of data to train the student network on from a training set sorted according to entropy. By validating on three datasets, not only do we show that our approach outperforms state-of-the-art methods on tabular data and performs competitively on image recognition, but also that novel curricula are learned by the teacher network. We demonstrate experimentally that the teacher network can actively learn about the student network and guide it to achieve better performance than if trained alone.",0
"The precise and trustworthy prediction of where a patient will be admitted to a hospital is crucial because of limited resources and space availability, particularly in emergency cases. To address this issue, we propose a student-teacher network that employs reinforcement learning. The state of the student network, represented by its weights, is used as input for the teacher network, which selects the best training data batch from a set ordered by entropy. Our method surpasses existing techniques on tabular data and performs competently on image recognition, as demonstrated by validation on three datasets. Additionally, the teacher network learns new curricula and can enhance the student network's performance through active learning.",1
"This paper proposes an inverse reinforcement learning (IRL) framework to accelerate learning when the learner-teacher \textit{interaction} is \textit{limited} during training. Our setting is motivated by the realistic scenarios where a helpful teacher is not available or when the teacher cannot access the learning dynamics of the student. We present two different training strategies: Curriculum Inverse Reinforcement Learning (CIRL) covering the teacher's perspective, and Self-Paced Inverse Reinforcement Learning (SPIRL) focusing on the learner's perspective. Using experiments in simulations and experiments with a real robot learning a task from a human demonstrator, we show that our training strategies can allow a faster training than a random teacher for CIRL and than a batch learner for SPIRL.",0
"In this paper, an inverse reinforcement learning (IRL) framework is proposed to expedite the learning process in situations where there is limited interaction between the learner and teacher during training. This approach is motivated by real-life scenarios where a supportive teacher may not be accessible or may not have access to the student's learning dynamics. Two distinct training strategies are introduced: Curriculum Inverse Reinforcement Learning (CIRL), which considers the teacher's perspective, and Self-Paced Inverse Reinforcement Learning (SPIRL), which focuses on the learner's perspective. Through simulations and experiments involving a real robot learning from a human demonstrator, it is demonstrated that these training strategies yield faster results than using a random teacher for CIRL and a batch learner for SPIRL.",1
"This paper proposes the Cooperative Soft Actor Critic (CSAC) method of enabling consecutive reinforcement learning agents to cooperatively solve a long time horizon multi-stage task. This method is achieved by modifying the policy of each agent to maximise both the current and next agent's critic. Cooperatively maximising each agent's critic allows each agent to take actions that are beneficial for its task as well as subsequent tasks. Using this method in a multi-room maze domain, the cooperative policies were able to outperform both uncooperative policies as well as a single agent trained across the entire domain. CSAC achieved a success rate of at least 20\% higher than the uncooperative policies, and converged on a solution at least 4 times faster than the single agent.",0
"The Cooperative Soft Actor Critic (CSAC) approach is proposed in this paper to enable a group of reinforcement learning agents to work together and successfully complete a multi-stage task with a long time horizon. The method involves adjusting each agent's policy to maximize the critic of both the current and next agent, which enables each agent to take actions that benefit not only its own task but also future tasks. The effectiveness of this method was demonstrated in a multi-room maze environment, where the cooperative policies outperformed both uncooperative policies and a single agent trained across the entire domain. The CSAC approach achieved a success rate that was at least 20% higher than the uncooperative policies, and it converged on a solution at least four times faster than the single agent.",1
"In Reinforcement Learning (RL), Convolutional Neural Networks(CNNs) have been successfully applied as function approximators in Deep Q-Learning algorithms, which seek to learn action-value functions and policies in various environments. However, to date, there has been little work on the learning of symmetry-transformation equivariant representations of the input environment state. In this paper, we propose the use of Equivariant CNNs to train RL agents and study their inductive bias for transformation equivariant Q-value approximation. We demonstrate that equivariant architectures can dramatically enhance the performance and sample efficiency of RL agents in a highly symmetric environment while requiring fewer parameters. Additionally, we show that they are robust to changes in the environment caused by affine transformations.",0
"Convolutional Neural Networks (CNNs) have been successfully utilized as function approximators in Deep Q-Learning algorithms in Reinforcement Learning (RL) to learn action-value functions and policies in different environments. However, there has been limited research on learning symmetry-transformation equivariant representations of the input environment state. To address this, we propose the use of Equivariant CNNs to train RL agents and investigate their inductive bias for transformation equivariant Q-value approximation. Our results demonstrate that these architectures can significantly improve the performance and sample efficiency of RL agents in highly symmetric environments while requiring fewer parameters. Furthermore, we found that they are resistant to changes in the environment caused by affine transformations.",1
"Autonomous driving has achieved significant progress in recent years, but autonomous cars are still unable to tackle high-risk situations where a potential accident is likely. In such near-accident scenarios, even a minor change in the vehicle's actions may result in drastically different consequences. To avoid unsafe actions in near-accident scenarios, we need to fully explore the environment. However, reinforcement learning (RL) and imitation learning (IL), two widely-used policy learning methods, cannot model rapid phase transitions and are not scalable to fully cover all the states. To address driving in near-accident scenarios, we propose a hierarchical reinforcement and imitation learning (H-ReIL) approach that consists of low-level policies learned by IL for discrete driving modes, and a high-level policy learned by RL that switches between different driving modes. Our approach exploits the advantages of both IL and RL by integrating them into a unified learning framework. Experimental results and user studies suggest our approach can achieve higher efficiency and safety compared to other methods. Analyses of the policies demonstrate our high-level policy appropriately switches between different low-level policies in near-accident driving situations.",0
"Although there have been significant advancements in autonomous driving, these vehicles are still unable to handle high-risk situations that could result in accidents. Even minor changes to the car's actions in near-accident scenarios can have drastically different outcomes. To prevent unsafe actions during such scenarios, a thorough exploration of the environment is necessary. However, widely-used policy learning methods such as reinforcement learning (RL) and imitation learning (IL) are not scalable and cannot model rapid phase transitions. To address this issue, we propose a hierarchical reinforcement and imitation learning (H-ReIL) approach that utilizes low-level policies learned by IL for discrete driving modes and a high-level policy learned by RL that switches between different modes. Our approach combines the strengths of both IL and RL, resulting in a more efficient and safe driving experience. Experimental results and user studies demonstrate the effectiveness of our approach, and our analyses show that the high-level policy appropriately switches between the different low-level policies in near-accident scenarios.",1
"Deep Deterministic Policy Gradient (DDPG) algorithm is one of the most well-known reinforcement learning methods. However, this method is inefficient and unstable in practical applications. On the other hand, the bias and variance of the Q estimation in the target function are sometimes difficult to control. This paper proposes a Regularly Updated Deterministic (RUD) policy gradient algorithm for these problems. This paper theoretically proves that the learning procedure with RUD can make better use of new data in replay buffer than the traditional procedure. In addition, the low variance of the Q value in RUD is more suitable for the current Clipped Double Q-learning strategy. This paper has designed a comparison experiment against previous methods, an ablation experiment with the original DDPG, and other analytical experiments in Mujoco environments. The experimental results demonstrate the effectiveness and superiority of RUD.",0
"The Deep Deterministic Policy Gradient (DDPG) algorithm is a well-known method for reinforcement learning. However, its practical application is inefficient and unstable, and controlling the bias and variance of the Q estimation in the target function can be challenging. To address these issues, this paper proposes the Regularly Updated Deterministic (RUD) policy gradient algorithm. The paper provides theoretical evidence that the learning process with RUD can more effectively utilize new data in the replay buffer than traditional methods. The low variance of the Q value in RUD is also better suited for the current Clipped Double Q-learning strategy. The paper presents a comparison experiment against previous methods, an ablation experiment with the original DDPG, and other analytical experiments in Mujoco environments. The experimental results demonstrate the effectiveness and superiority of RUD.",1
"We consider reinforcement learning (RL) in episodic MDPs with adversarial full-information reward feedback and unknown fixed transition kernels. We propose two model-free policy optimization algorithms, POWER and POWER++, and establish guarantees for their dynamic regret. Compared with the classical notion of static regret, dynamic regret is a stronger notion as it explicitly accounts for the non-stationarity of environments. The dynamic regret attained by the proposed algorithms interpolates between different regimes of non-stationarity, and moreover satisfies a notion of adaptive (near-)optimality, in the sense that it matches the (near-)optimal static regret under slow-changing environments. The dynamic regret bound features two components, one arising from exploration, which deals with the uncertainty of transition kernels, and the other arising from adaptation, which deals with non-stationary environments. Specifically, we show that POWER++ improves over POWER on the second component of the dynamic regret by actively adapting to non-stationarity through prediction. To the best of our knowledge, our work is the first dynamic regret analysis of model-free RL algorithms in non-stationary environments.",0
"Our focus is on reinforcement learning (RL) in episodic Markov decision processes (MDPs) where there is adversarial full-information reward feedback and the transition kernels are fixed and unknown. We introduce two policy optimization algorithms, POWER and POWER++, that are model-free and establish guarantees for their dynamic regret. Unlike the classical static regret, dynamic regret considers the non-stationarity of environments, and our proposed algorithms achieve dynamic regret that adapts to different non-stationary regimes. The dynamic regret also satisfies adaptive (near-)optimality, matching (near-)optimal static regret under slow-changing environments. The dynamic regret bound has two components: exploration to deal with transition kernel uncertainty and adaptation to handle non-stationary environments. POWER++ improves on POWER by actively predicting and adapting to non-stationarity, making it the first dynamic regret analysis of model-free RL algorithms in non-stationary environments.",1
"Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at https://ramanans1.github.io/plan2explore/",0
"Although reinforcement learning can solve complex tasks, it is limited by task-specific learning and low sample efficiency. Plan2Explore is a novel self-supervised reinforcement learning agent that addresses these issues through a unique approach to exploration and quick adaptation to new tasks, without prior knowledge during exploration. Unlike previous methods that retrospectively assess novelty, Plan2Explore uses planning to proactively seek out future novelty during exploration. After exploration, the agent quickly adapts to multiple downstream tasks with zero or few-shot training. We tested Plan2Explore on challenging control tasks with high-dimensional image inputs and found that it outperforms prior self-supervised exploration methods, almost matching the performance of an oracle with reward access. Our videos and code are available at https://ramanans1.github.io/plan2explore/.",1
"This paper presents a recursive reasoning formalism of Bayesian optimization (BO) to model the reasoning process in the interactions between boundedly rational, self-interested agents with unknown, complex, and costly-to-evaluate payoff functions in repeated games, which we call Recursive Reasoning-Based BO (R2-B2). Our R2-B2 algorithm is general in that it does not constrain the relationship among the payoff functions of different agents and can thus be applied to various types of games such as constant-sum, general-sum, and common-payoff games. We prove that by reasoning at level 2 or more and at one level higher than the other agents, our R2-B2 agent can achieve faster asymptotic convergence to no regret than that without utilizing recursive reasoning. We also propose a computationally cheaper variant of R2-B2 called R2-B2-Lite at the expense of a weaker convergence guarantee. The performance and generality of our R2-B2 algorithm are empirically demonstrated using synthetic games, adversarial machine learning, and multi-agent reinforcement learning.",0
"The aim of this paper is to introduce a Recursive Reasoning-Based BO (R2-B2) algorithm, which utilizes a recursive reasoning formalism of Bayesian optimization (BO) to model the reasoning process in repeated games involving self-interested agents with unknown and complex payoff functions. R2-B2 is a general algorithm that can be applied to different types of games, including constant-sum, general-sum, and common-payoff games. The study shows that R2-B2 can achieve faster asymptotic convergence to no regret by reasoning at level 2 or higher and at a level higher than the other agents. Additionally, the paper proposes a computationally cheaper variant of R2-B2 called R2-B2-Lite, which has a weaker convergence guarantee. The effectiveness and applicability of the R2-B2 algorithm are demonstrated through synthetic games, adversarial machine learning, and multi-agent reinforcement learning.",1
"Learning robotic manipulation through reinforcement learning (RL) using only sparse reward signals is still considered a largely unsolved problem. Leveraging human demonstrations can make the learning process more sample efficient, but obtaining high-quality demonstrations can be costly or unfeasible. In this paper we propose a novel approach that introduces object-level demonstrations, i.e. examples of where the objects should be at any state. These demonstrations are generated automatically through RL hence require no expert knowledge. We observe that, during a manipulation task, an object is moved from an initial to a final position. When seen from the point of view of the object being manipulated, this induces a locomotion task that can be decoupled from the manipulation task and learnt through a physically-realistic simulator. The resulting object-level trajectories, called simulated locomotion demonstrations (SLDs), are then leveraged to define auxiliary rewards that are used to learn the manipulation policy. The proposed approach has been evaluated on 13 tasks of increasing complexity, and has been demonstrated to achieve higher success rate and faster learning rates compared to alternative algorithms. SLDs are especially beneficial for tasks like multi-object stacking and non-rigid object manipulation.",0
"The problem of learning robotic manipulation using only sparse reward signals through reinforcement learning (RL) is still unsolved. While using human demonstrations can make the learning process more efficient, obtaining high-quality demonstrations can be expensive or impractical. In this study, we suggest a new technique that introduces object-level demonstrations, which show where objects should be at any given state. These demonstrations are automatically generated through RL and do not require expert knowledge. During a manipulation task, objects are moved from an original to a final position, inducing a locomotion task that can be learned through a physically-realistic simulator. The resulting simulated locomotion demonstrations (SLDs) are used to create auxiliary rewards that help to learn the manipulation policy. We evaluated our approach on 13 tasks of increasing complexity and found that it achieved higher success rates and faster learning rates than alternative algorithms. SLDs are particularly beneficial for multi-object stacking and non-rigid object manipulation tasks.",1
"In E-commerce, advertising is essential for merchants to reach their target users. The typical objective is to maximize the advertiser's cumulative revenue over a period of time under a budget constraint. In real applications, an advertisement (ad) usually needs to be exposed to the same user multiple times until the user finally contributes revenue (e.g., places an order). However, existing advertising systems mainly focus on the immediate revenue with single ad exposures, ignoring the contribution of each exposure to the final conversion, thus usually falls into suboptimal solutions. In this paper, we formulate the sequential advertising strategy optimization as a dynamic knapsack problem. We propose a theoretically guaranteed bilevel optimization framework, which significantly reduces the solution space of the original optimization space while ensuring the solution quality. To improve the exploration efficiency of reinforcement learning, we also devise an effective action space reduction approach. Extensive offline and online experiments show the superior performance of our approaches over state-of-the-art baselines in terms of cumulative revenue.",0
"Advertising is a crucial aspect of E-commerce that enables merchants to connect with their target audience. The primary goal is to optimize the advertiser's revenue within a set budget over a specific timeframe. In practical applications, an advertisement typically requires repeated exposure to the same user before resulting in revenue generation. However, current advertising systems prioritize immediate revenue from single exposures and overlook the contribution of each exposure towards the final conversion. This leads to suboptimal outcomes. This study introduces a dynamic knapsack problem to optimize sequential advertising strategies. The proposed bilevel optimization framework guarantees superior solutions while reducing the original optimization space. To enhance reinforcement learning exploration efficiency, an effective action space reduction approach is also developed. Both offline and online experiments validate the superior performance of our approaches over state-of-the-art baselines in terms of cumulative revenue.",1
"In preference-based reinforcement learning (RL), an agent interacts with the environment while receiving preferences instead of absolute feedback. While there is increasing research activity in preference-based RL, the design of formal frameworks that admit tractable theoretical analysis remains an open challenge. Building upon ideas from preference-based bandit learning and posterior sampling in RL, we present DUELING POSTERIOR SAMPLING (DPS), which employs preference-based posterior sampling to learn both the system dynamics and the underlying utility function that governs the preference feedback. As preference feedback is provided on trajectories rather than individual state-action pairs, we develop a Bayesian approach for the credit assignment problem, translating preferences to a posterior distribution over state-action reward models. We prove an asymptotic Bayesian no-regret rate for DPS with a Bayesian linear regression credit assignment model. This is the first regret guarantee for preference-based RL to our knowledge. We also discuss possible avenues for extending the proof methodology to other credit assignment models. Finally, we evaluate the approach empirically, showing competitive performance against existing baselines.",0
"Preference-based reinforcement learning (RL) involves an agent interacting with the environment and receiving preferences, as opposed to absolute feedback. Although preference-based RL is gaining research attention, creating formal frameworks that permit tractable theoretical analysis remains a challenge. In this paper, we introduce DUELING POSTERIOR SAMPLING (DPS), which leverages preference-based posterior sampling to learn the utility function and system dynamics that govern preference feedback. Since preference feedback occurs on trajectories rather than individual state-action pairs, we develop a Bayesian approach to solve the credit assignment problem. This translates preferences to a posterior distribution over state-action reward models. We prove an asymptotic Bayesian no-regret rate for DPS with a Bayesian linear regression credit assignment model, which is the first regret guarantee for preference-based RL that we know of. We also explore the possibility of extending the proof methodology to other credit assignment models. Finally, we conduct empirical evaluations that demonstrate competitive performance against existing baselines.",1
"Recently, many reinforcement learning techniques were shown to have provable guarantees in the simple case of linear dynamics, especially in problems like linear quadratic regulators. However, in practice, many reinforcement learning problems try to learn a policy directly from rich, high dimensional representations such as images. Even if there is an underlying dynamics that is linear in the correct latent representations (such as position and velocity), the rich representation is likely to be nonlinear and can contain irrelevant features. In this work we study a model where there is a hidden linear subspace in which the dynamics is linear. For such a model we give an efficient algorithm for extracting the linear subspace with linear dynamics. We then extend our idea to extracting a nonlinear mapping, and empirically verify the effectiveness of our approach in simple settings with rich observations.",0
"In recent times, various reinforcement learning methods have been proven to have reliable assurances in linear dynamics, particularly in problems such as linear quadratic regulators. However, in practical scenarios, many reinforcement learning problems aim to learn a policy directly from intricate and high-dimensional representations like images. Although there may exist an underlying linear dynamics in the correct latent representations such as position and velocity, the rich representations are often nonlinear and can contain irrelevant features. In this study, we examine a model that comprises a hidden linear subspace with linear dynamics. We present an effective algorithm to extract the linear subspace with linear dynamics and extend our approach to extracting a nonlinear mapping. We then demonstrate the efficacy of our method in simple settings with rich observations.",1
"Automating molecular design using deep reinforcement learning (RL) holds the promise of accelerating the discovery of new chemical compounds. Existing approaches work with molecular graphs and thus ignore the location of atoms in space, which restricts them to 1) generating single organic molecules and 2) heuristic reward functions. To address this, we present a novel RL formulation for molecular design in Cartesian coordinates, thereby extending the class of molecules that can be built. Our reward function is directly based on fundamental physical properties such as the energy, which we approximate via fast quantum-chemical methods. To enable progress towards de-novo molecular design, we introduce MolGym, an RL environment comprising several challenging molecular design tasks along with baselines. In our experiments, we show that our agent can efficiently learn to solve these tasks from scratch by working in a translation and rotation invariant state-action space.",0
"The automation of molecular design through deep reinforcement learning (RL) has the potential to speed up the discovery of new chemical compounds. However, current methods only work with molecular graphs and do not consider the location of atoms in space, limiting their ability to generate single organic molecules and rely on heuristic reward functions. To overcome this limitation, we propose a new RL approach for molecular design that uses Cartesian coordinates, expanding the range of molecules that can be created. Our reward function is based on fundamental physical properties such as energy, which we estimate using quantum-chemical methods. To advance de-novo molecular design, we introduce MolGym, an RL environment with challenging molecular design tasks and baselines. Our experiments demonstrate that our agent can efficiently learn to solve these tasks from scratch by operating in a translation and rotation invariant state-action space.",1
"Industry 4.0 systems have a high demand for optimization in their tasks, whether to minimize cost, maximize production, or even synchronize their actuators to finish or speed up the manufacture of a product. Those challenges make industrial environments a suitable scenario to apply all modern reinforcement learning (RL) concepts. The main difficulty, however, is the lack of that industrial environments. In this way, this work presents the concept and the implementation of a tool that allows us to convert any dynamic system modeled as an FSM to the open-source Gym wrapper. After that, it is possible to employ any RL methods to optimize any desired task. In the first tests of the proposed tool, we show traditional Q-learning and Deep Q-learning methods running over two simple environments.",0
"Optimization is crucial for Industry 4.0 systems, whether it involves cutting costs, increasing production, or coordinating actuators to speed up manufacturing. These challenges make industrial settings an ideal place to utilize modern reinforcement learning (RL) concepts. However, a major obstacle is the scarcity of industrial environments. To address this issue, this study introduces a tool that can convert any dynamic system modeled as an FSM to the open-source Gym wrapper, allowing the application of various RL methods to optimize desired tasks. The proposed tool was tested using traditional Q-learning and Deep Q-learning methods on two simple environments, demonstrating its effectiveness.",1
"Model-based reinforcement learning (RL) enjoys several benefits, such as data-efficiency and planning, by learning a model of the environment's dynamics. However, learning a global model that can generalize across different dynamics is a challenging task. To tackle this problem, we decompose the task of learning a global dynamics model into two stages: (a) learning a context latent vector that captures the local dynamics, then (b) predicting the next state conditioned on it. In order to encode dynamics-specific information into the context latent vector, we introduce a novel loss function that encourages the context latent vector to be useful for predicting both forward and backward dynamics. The proposed method achieves superior generalization ability across various simulated robotics and control tasks, compared to existing RL schemes.",0
"There are advantages to using model-based reinforcement learning (RL) that include efficiency of data usage and the ability to plan by creating a model of the environment's dynamics. However, creating a global model that can be applied to different dynamics is a difficult task. To address this challenge, we break the process of creating a global dynamics model into two stages: (a) creating a context latent vector that captures local dynamics, and (b) making predictions about the next state based on this vector. To ensure that the context latent vector contains information specific to the dynamics, we introduce an original loss function that encourages the vector to be useful in predicting both forward and backward dynamics. Compared to current RL methods, our approach achieves superior generalization in various simulated robotics and control tasks.",1
"Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the number of elements in the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme, coined learning what to defer (LwD), where the agent adaptively shrinks or stretch the number of stages by learning to distribute the element-wise decisions of the solution at each stage. We apply the proposed framework to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget.",0
"The development of effective algorithms for combinatorial optimization is widespread across various scientific disciplines. Recently, there has been increased attention on deep reinforcement learning (DRL) frameworks as a novel approach to automate solver design with less reliance on domain expertise. However, current DRL solvers' scalability is limited due to the number of stages required to determine the solution, which is proportional to the number of elements. Our paper introduces a new DRL method, called learning what to defer (LwD), which addresses this limitation by adapting the number of stages through learning to distribute element-wise decisions. We apply LwD to the maximum independent set (MIS) problem and demonstrate its superiority over current DRL schemes and conventional MIS solvers on large-scale graphs with millions of vertices, within a reasonable time frame.",1
"There has been a current trend in reinforcement learning for healthcare literature, where in order to prepare clinical datasets, researchers will carry forward the last results of the non-administered test known as the last-observation-carried-forward (LOCF) value to fill in gaps, assuming that it is still an accurate indicator of the patient's current state. These values are carried forward without maintaining information about exactly how these values were imputed, leading to ambiguity. Our approach models this problem using OpenAI Gym's Mountain Car and aims to address when to observe the patient's physiological state and partly how to intervene, as we have assumed we can only act after following an observation. So far, we have found that for a last-observation-carried-forward implementation of the state space, augmenting the state with counters for each state variable tracking the time since last observation was made, improves the predictive performance of an agent, supporting the notion of ""informative missingness"", and using a neural network based Dynamics Model to predict the most probable next state value of non-observed state variables instead of carrying forward the last observed value through LOCF further improves the agent's performance, leading to faster convergence and reduced variance.",0
"Currently, reinforcement learning in healthcare literature has seen a trend where researchers use the last-observation-carried-forward (LOCF) value to fill in gaps in clinical datasets. This assumes that the LOCF value is an accurate indicator of the patient's current state and does not maintain information about how the values were imputed, resulting in ambiguity. Our approach utilizes OpenAI Gym's Mountain Car to address the problem of when to observe the patient's physiological state and how to intervene. We can only act after observing the patient's state. We found that augmenting the state with counters for each state variable tracking the time since the last observation and using a neural network-based Dynamics Model to predict the most probable next state value of non-observed state variables improves the agent's performance. This leads to faster convergence and reduced variance, supporting the concept of ""informative missingness.""",1
"Model usage is the central challenge of model-based reinforcement learning. Although dynamics model based on deep neural networks provide good generalization for single step prediction, such ability is over exploited when it is used to predict long horizon trajectories due to compounding errors. In this work, we propose a Dyna-style model-based reinforcement learning algorithm, which we called Maximum Entropy Model Rollouts (MEMR). To eliminate the compounding errors, we only use our model to generate single-step rollouts. Furthermore, we propose to generate \emph{diverse} model rollouts by non-uniform sampling of the environment states such that the entropy of the model rollouts is maximized. We mathematically derived the maximum entropy sampling criteria for one data case under Gaussian prior. To accomplish this criteria, we propose to utilize a prioritized experience replay. Our preliminary experiments in challenging locomotion benchmarks show that our approach achieves the same sample efficiency of the best model-based algorithms, matches the asymptotic performance of the best model-free algorithms, and significantly reduces the computation requirements of other model-based methods.",0
"The main difficulty in model-based reinforcement learning is the utilization of models. Although deep neural networks can generate accurate predictions for single steps, predicting longer trajectories leads to compounding errors. To address this issue, we present the Maximum Entropy Model Rollouts (MEMR), a Dyna-style algorithm. MEMR only generates single-step rollouts to avoid compounding errors and uses non-uniform sampling to ensure diverse model rollouts with maximum entropy. We derived the maximum entropy sampling criteria for one data case under a Gaussian prior and propose prioritized experience replay to accomplish this. Our experiments on challenging locomotion benchmarks demonstrate that MEMR achieves the same sample efficiency as the top model-based algorithms, matches the asymptotic performance of the top model-free algorithms, and significantly reduces computation requirements compared to other model-based methods.",1
"We study the exploration problem with approximate linear action-value functions in episodic reinforcement learning under the notion of low inherent Bellman error, a condition normally employed to show convergence of approximate value iteration. First we relate this condition to other common frameworks and show that it is strictly more general than the low rank (or linear) MDP assumption of prior work. Second we provide an algorithm with a high probability regret bound $\widetilde O(\sum_{t=1}^H d_t \sqrt{K} + \sum_{t=1}^H \sqrt{d_t} \IBE K)$ where $H$ is the horizon, $K$ is the number of episodes, $\IBE$ is the value if the inherent Bellman error and $d_t$ is the feature dimension at timestep $t$. In addition, we show that the result is unimprovable beyond constants and logs by showing a matching lower bound. This has two important consequences: 1) it shows that exploration is possible using only \emph{batch assumptions} with an algorithm that achieves the optimal statistical rate for the setting we consider, which is more general than prior work on low-rank MDPs 2) the lack of closedness (measured by the inherent Bellman error) is only amplified by $\sqrt{d_t}$ despite working in the online setting. Finally, the algorithm reduces to the celebrated \textsc{LinUCB} when $H=1$ but with a different choice of the exploration parameter that allows handling misspecified contextual linear bandits. While computational tractability questions remain open for the MDP setting, this enriches the class of MDPs with a linear representation for the action-value function where statistically efficient reinforcement learning is possible.",0
"In this study, we investigate the exploration problem in episodic reinforcement learning using approximate linear action-value functions. Our focus is on the condition of low inherent Bellman error, which is commonly used to demonstrate convergence of approximate value iteration. We demonstrate that this condition is more general than the low rank MDP assumption used in prior work. We present an algorithm that achieves a high probability regret bound, which is a function of the horizon, the number of episodes, the feature dimension at each timestep, and the inherent Bellman error. We also show that this result cannot be improved beyond certain constants and logarithms, as demonstrated by a matching lower bound. Our findings have two important implications: 1) exploration is feasible using only batch assumptions, and our algorithm achieves the optimal statistical rate for the more general setting we consider, which surpasses prior work on low-rank MDPs, and 2) even in the online setting, the lack of closedness, as measured by the inherent Bellman error, is only amplified by the square root of the feature dimension at each timestep. Finally, our algorithm is equivalent to the LinUCB algorithm when the horizon is one, but with a different exploration parameter that enables the handling of misspecified contextual linear bandits. While computational tractability remains a challenge for the MDP setting, our study expands the class of MDPs with a linear representation for the action-value function, making efficient reinforcement learning statistically possible.",1
"Bayesian deep neural networks (DNNs) can provide a mathematically grounded framework to quantify uncertainty in predictions from image captioning models. We propose a Bayesian variant of policy-gradient based reinforcement learning training technique for image captioning models to directly optimize non-differentiable image captioning quality metrics such as CIDEr-D. We extend the well-known Self-Critical Sequence Training (SCST) approach for image captioning models by incorporating Bayesian inference, and refer to it as B-SCST. The ""baseline"" for the policy-gradients in B-SCST is generated by averaging predictive quality metrics (CIDEr-D) of the captions drawn from the distribution obtained using a Bayesian DNN model. We infer this predictive distribution using Monte Carlo (MC) dropout approximate variational inference. We show that B-SCST improves CIDEr-D scores on Flickr30k, MS COCO and VizWiz image captioning datasets, compared to the SCST approach. We also provide a study of uncertainty quantification for the predicted captions, and demonstrate that it correlates well with the CIDEr-D scores. To our knowledge, this is the first such analysis, and it can improve the interpretability of image captioning model outputs, which is critical for practical applications.",0
"Our study proposes the use of Bayesian DNNs to quantify uncertainty in image captioning predictions. We introduce a Bayesian version of the reinforcement learning technique, which optimizes non-differentiable metrics such as CIDEr-D. We name this approach B-SCST and it builds on the well-known Self-Critical Sequence Training method. The policy-gradients in B-SCST are based on a ""baseline"" generated by averaging the CIDEr-D metrics of captions obtained from a Bayesian DNN model. We use Monte Carlo dropout approximate variational inference to infer the predictive distribution. Our results show that B-SCST outperforms SCST on Flickr30k, MS COCO, and VizWiz datasets. Additionally, we present an analysis of uncertainty quantification for predicted captions, which correlates well with CIDEr-D scores. This study is the first of its kind and can enhance the interpretability of image captioning model outputs for practical applications.",1
"The existing action recognition methods are mainly based on clip-level classifiers such as two-stream CNNs or 3D CNNs, which are trained from the randomly selected clips and applied to densely sampled clips during testing. However, this standard setting might be suboptimal for training classifiers and also requires huge computational overhead when deployed in practice. To address these issues, we propose a new framework for action recognition in videos, called {\em Dynamic Sampling Networks} (DSN), by designing a dynamic sampling module to improve the discriminative power of learned clip-level classifiers and as well increase the inference efficiency during testing. Specifically, DSN is composed of a sampling module and a classification module, whose objective is to learn a sampling policy to on-the-fly select which clips to keep and train a clip-level classifier to perform action recognition based on these selected clips, respectively. In particular, given an input video, we train an observation network in an associative reinforcement learning setting to maximize the rewards of the selected clips with a correct prediction. We perform extensive experiments to study different aspects of the DSN framework on four action recognition datasets: UCF101, HMDB51, THUMOS14, and ActivityNet v1.3. The experimental results demonstrate that DSN is able to greatly improve the inference efficiency by only using less than half of the clips, which can still obtain a slightly better or comparable recognition accuracy to the state-of-the-art approaches.",0
"Most existing methods for recognizing actions in videos rely on classifiers at the clip level, such as two-stream CNNs or 3D CNNs. These classifiers are trained on a random selection of clips and tested on densely sampled clips. However, this approach is not optimal for training classifiers and requires significant computational resources for practical deployment. To address these issues, we introduce a new framework for action recognition called Dynamic Sampling Networks (DSN). DSN includes a dynamic sampling module to improve the discriminative power of clip-level classifiers and increase efficiency during testing. DSN consists of a sampling module and a classification module. The sampling module learns a sampling policy to select which clips to retain, while the classification module trains a clip-level classifier to recognize actions based on the selected clips. We use an observation network in an associative reinforcement learning setting to maximize the rewards of correctly predicted selected clips. We conduct extensive experiments on four action recognition datasets, UCF101, HMDB51, THUMOS14, and ActivityNet v1.3, demonstrating that DSN effectively improves inference efficiency by using less than half of the clips, while still achieving comparable or slightly better recognition accuracy than existing state-of-the-art approaches.",1
"We address the problem of Bayesian reinforcement learning using efficient model-based online planning. We propose an optimism-free Bayes-adaptive algorithm to induce deeper and sparser exploration with a theoretical bound on its performance relative to the Bayes optimal policy, with a lower computational complexity. The main novelty is the use of a candidate policy generator, to generate long-term options in the planning tree (over beliefs), which allows us to create much sparser and deeper trees. Experimental results on different environments show that in comparison to the state-of-the-art, our algorithm is both computationally more efficient, and obtains significantly higher reward in discrete environments.",0
"Our focus is on solving the challenge of implementing Bayesian reinforcement learning by utilizing effective model-based online planning. Our solution involves introducing a Bayes-adaptive algorithm that eliminates the optimistic approach, thereby encouraging in-depth and infrequent exploration. This algorithm has a theoretical performance bound that is comparable to the Bayes optimal policy, but with lower computational complexity. Our innovation lies in employing a candidate policy generator to construct extended-term options in the planning tree, which enables us to generate much sparser and deeper trees. We conducted experiments on diverse environments and discovered that our algorithm is superior to the state-of-the-art in terms of computational efficiency and reward attainment in discrete environments.",1
"Deep reinforcement learning (RL) algorithms have recently achieved remarkable successes in various sequential decision making tasks, leveraging advances in methods for training large deep networks. However, these methods usually require large amounts of training data, which is often a big problem for real-world applications. One natural question to ask is whether learning good representations for states and using larger networks helps in learning better policies. In this paper, we try to study if increasing input dimensionality helps improve performance and sample efficiency of model-free deep RL algorithms. To do so, we propose an online feature extractor network (OFENet) that uses neural nets to produce good representations to be used as inputs to deep RL algorithms. Even though the high dimensionality of input is usually supposed to make learning of RL agents more difficult, we show that the RL agents in fact learn more efficiently with the high-dimensional representation than with the lower-dimensional state observations. We believe that stronger feature propagation together with larger networks (and thus larger search space) allows RL agents to learn more complex functions of states and thus improves the sample efficiency. Through numerical experiments, we show that the proposed method outperforms several other state-of-the-art algorithms in terms of both sample efficiency and performance. Codes for the proposed method are available at http://www.merl.com/research/license/OFENet .",0
"Recently, deep reinforcement learning (RL) algorithms have achieved impressive results in various sequential decision-making tasks by utilizing advancements in training large deep networks. However, these methods usually require a significant amount of training data, posing a challenge for real-world applications. One question that arises is whether learning better representations of states and using larger networks can lead to improved policy learning. This paper investigates whether increasing input dimensionality enhances the performance and sample efficiency of model-free deep RL algorithms. To achieve this, the authors propose an online feature extractor network (OFENet) that uses neural nets to generate effective representations for use as inputs to deep RL algorithms. Despite the perceived difficulty of learning with high-dimensional input, the authors demonstrate that RL agents learn more efficiently using the high-dimensional representation than with lower-dimensional state observations. They believe that stronger feature propagation, coupled with larger networks and a larger search space, enables RL agents to learn more complex functions of states, thereby enhancing sample efficiency. Numerical experiments reveal that the proposed method outperforms several other state-of-the-art algorithms in terms of both sample efficiency and performance. The proposed method's codes are accessible at http://www.merl.com/research/license/OFENet.",1
"This paper prescribes a suite of techniques for off-policy Reinforcement Learning (RL) that simplify the training process and reduce the sample complexity. First, we show that simple Deterministic Policy Gradient works remarkably well as long as the overestimation bias is controlled. This is contrast to existing literature which creates sophisticated off-policy techniques. Second, we pinpoint training instabilities, typical of off-policy algorithms, to the greedy policy update step; existing solutions such as delayed policy updates do not mitigate this issue. Third, we show that ideas in the propensity estimation literature can be used to importance-sample transitions from the replay buffer and selectively update the policy to prevent deterioration of performance. We make these claims using extensive experimentation on a set of challenging MuJoCo tasks. A short video of our results can be seen at https://tinyurl.com/scs6p5m .",0
"The aim of this paper is to provide a range of methods for off-policy Reinforcement Learning (RL) that make the training process easier and reduce the amount of samples required. Firstly, the paper demonstrates that using a straightforward Deterministic Policy Gradient works well, as long as the overestimation bias is kept under control, which goes against what other literature has suggested with their complex off-policy techniques. Secondly, the paper identifies that training instabilities in off-policy algorithms are often caused by the greedy policy update step and that existing solutions, like delayed policy updates, do not solve this problem. Thirdly, the paper proposes using ideas from the propensity estimation literature to importance-sample transitions from the replay buffer and selectively update the policy to prevent performance deterioration. Extensive experimentation on difficult MuJoCo tasks is conducted to support these claims. A brief video of the results can be viewed at https://tinyurl.com/scs6p5m.",1
"In recent years, Deep Reinforcement Learning (DRL) algorithms have achieved state-of-the-art performance in many challenging strategy games. Because these games have complicated rules, an action sampled from the full discrete action space will typically be invalid. The usual approach to deal with this problem in policy gradient algorithms is to ""mask out"" invalid actions and just sample from the set of valid actions. The implications of this process, however, remain under-investigated. In this paper, we show that the standard working mechanism of invalid action masking corresponds to valid policy gradient updates. More interestingly, it works by applying a state-dependent differentiable function during the calculation of action probability distribution. Additionally, we show its critical importance to the performance of policy gradient algorithms. Specifically, our experiments show that invalid action masking scales well when the space of invalid actions is large, while the common approach of giving negative rewards for invalid actions will fail. Finally, we provide further insights by evaluating different action masking regimes, such as removing masking after an agent has been trained using masking.",0
"Deep Reinforcement Learning (DRL) algorithms have achieved impressive results in complex strategy games, thanks to their ability to handle intricate rules. However, selecting a valid action from the full discrete action space is challenging, as most actions will be invalid. Policy gradient algorithms typically address this issue by masking out invalid actions and sampling from the set of valid ones. Nevertheless, the implications of this process are not fully understood. This study demonstrates that invalid action masking is a valid policy gradient update mechanism that involves a differentiable function dependent on the state. The research also highlights the critical role played by invalid action masking in policy gradient algorithm performance, especially when dealing with a vast space of invalid actions. Conversely, the common practice of giving negative rewards for invalid actions fails under the same conditions. The study concludes by examining different action masking regimes, including removing masking after an agent has undergone training with masking.",1
"Reinforcement learning using a novel predictive representation is applied to autonomous driving to accomplish the task of driving between lane markings where substantial benefits in performance and generalization are observed on unseen test roads in both simulation and on a real Jackal robot. The novel predictive representation is learned by general value functions (GVFs) to provide out-of-policy, or counter-factual, predictions of future lane centeredness and road angle that form a compact representation of the state of the agent improving learning in both online and offline reinforcement learning to learn to drive an autonomous vehicle with methods that generalizes well to roads not in the training data. Experiments in both simulation and the real-world demonstrate that predictive representations in reinforcement learning improve learning efficiency, smoothness of control and generalization to roads that the agent was never shown during training, including damaged lane markings. It was found that learning a predictive representation that consists of several predictions over different time scales, or discount factors, improves the performance and smoothness of the control substantially. The Jackal robot was trained in a two step process where the predictive representation is learned first followed by a batch reinforcement learning algorithm (BCQ) from data collected through both automated and human-guided exploration in the environment. We conclude that out-of-policy predictive representations with GVFs offer reinforcement learning many benefits in real-world problems.",0
"The task of driving between lane markings can be accomplished in autonomous driving through reinforcement learning that utilizes a new predictive representation. This approach has yielded significant gains in performance and generalization on previously unseen roads in both simulation and on an actual Jackal robot. The predictive representation is developed via general value functions (GVFs), enabling off-policy predictions of future lane centeredness and road angle. This compact representation of the agent's state enhances learning in both online and offline reinforcement learning, promoting the acquisition of driving skills that generalize across different road types. Experimental results show that predictive representations improve learning efficiency, control smoothness, and generalization to roads not seen during training, including those with damaged lane markings. Combining predictions over multiple time scales enhances performance and control smoothness. The Jackal robot is trained in two stages, with the predictive representation learned first, followed by batch reinforcement learning (BCQ) on data collected through both automated and human-guided exploration. The use of out-of-policy predictive representations with GVFs is a promising approach for solving real-world problems through reinforcement learning.",1
"Graph data are pervasive in many real-world applications. Recently, increasing attention has been paid on graph neural networks (GNNs), which aim to model the local graph structures and capture the hierarchical patterns by aggregating the information from neighbors with stackable network modules. Motivated by the observation that different nodes often require different iterations of aggregation to fully capture the structural information, in this paper, we propose to explicitly sample diverse iterations of aggregation for different nodes to boost the performance of GNNs. It is a challenging task to develop an effective aggregation strategy for each node, given complex graphs and sparse features. Moreover, it is not straightforward to derive an efficient algorithm since we need to feed the sampled nodes into different number of network layers. To address the above challenges, we propose Policy-GNN, a meta-policy framework that models the sampling procedure and message passing of GNNs into a combined learning process. Specifically, Policy-GNN uses a meta-policy to adaptively determine the number of aggregations for each node. The meta-policy is trained with deep reinforcement learning (RL) by exploiting the feedback from the model. We further introduce parameter sharing and a buffer mechanism to boost the training efficiency. Experimental results on three real-world benchmark datasets suggest that Policy-GNN significantly outperforms the state-of-the-art alternatives, showing the promise in aggregation optimization for GNNs.",0
"Graph data is prevalent in various real-world applications, and graph neural networks (GNNs) have recently garnered increasing attention. GNNs endeavor to model the local graph structures and capture hierarchical patterns by aggregating information from neighbors using stackable network modules. However, different nodes often require varying iterations of aggregation to capture structural information effectively. To enhance GNN performance, we propose a method to explicitly sample diverse iterations of aggregation for different nodes. Developing an effective aggregation strategy is challenging, given complex graphs and sparse features. Additionally, deriving an efficient algorithm is non-trivial since the sampled nodes need to be fed into different layers of the network. To tackle these challenges, we introduce Policy-GNN, a meta-policy framework that models the sampling procedure and message passing of GNNs into a combined learning process. Policy-GNN uses a meta-policy to adaptively determine the number of aggregations for each node and is trained using deep reinforcement learning (RL) by exploiting the model's feedback. We also include parameter sharing and a buffer mechanism to enhance training efficiency. Our experiments on three real-world benchmark datasets indicate that Policy-GNN outperforms state-of-the-art alternatives, demonstrating its potential for aggregation optimization in GNNs.",1
"Reinforcement learning algorithms usually assume that all actions are always available to an agent. However, both people and animals understand the general link between the features of their environment and the actions that are feasible. Gibson (1977) coined the term ""affordances"" to describe the fact that certain states enable an agent to do certain actions, in the context of embodied agents. In this paper, we develop a theory of affordances for agents who learn and plan in Markov Decision Processes. Affordances play a dual role in this case. On one hand, they allow faster planning, by reducing the number of actions available in any given situation. On the other hand, they facilitate more efficient and precise learning of transition models from data, especially when such models require function approximation. We establish these properties through theoretical results as well as illustrative examples. We also propose an approach to learn affordances and use it to estimate transition models that are simpler and generalize better.",0
"Typically, reinforcement learning algorithms assume that agents have access to all possible actions. However, humans and animals have an innate understanding of the relationship between their environment and the actions that are feasible. This concept, known as ""affordances,"" was coined by Gibson (1977) to refer to the idea that certain conditions enable an agent to perform specific actions. In this study, we introduce a theory of affordances for agents utilizing Markov Decision Processes to learn and plan. Affordances serve a dual purpose by reducing the number of actions available during planning and aiding in the learning of transition models, especially when function approximation is required. We support these claims with theoretical results and examples, as well as propose an approach to learning affordances that leads to simpler and more generalizable transition models.",1
"Decision trees are ubiquitous in machine learning for their ease of use and interpretability. Yet, these models are not typically employed in reinforcement learning as they cannot be updated online via stochastic gradient descent. We overcome this limitation by allowing for a gradient update over the entire tree that improves sample complexity affords interpretable policy extraction. First, we include theoretical motivation on the need for policy-gradient learning by examining the properties of gradient descent over differentiable decision trees. Second, we demonstrate that our approach equals or outperforms a neural network on all domains and can learn discrete decision trees online with average rewards up to 7x higher than a batch-trained decision tree. Third, we conduct a user study to quantify the interpretability of a decision tree, rule list, and a neural network with statistically significant results ($p < 0.001$).",0
"Machine learning relies heavily on decision trees due to their ease of use and interpretability. However, in reinforcement learning, decision trees are not commonly utilized because they cannot be updated online using stochastic gradient descent. Our solution to this problem involves allowing for a gradient update over the entire tree, which not only enhances sample complexity but also enables interpretable policy extraction. To support our approach, we provide theoretical justification for policy-gradient learning by examining the characteristics of gradient descent over differentiable decision trees. Additionally, we demonstrate that our method performs equally or better than a neural network in all domains and can learn discrete decision trees online with average rewards that are up to 7 times higher than those of a batch-trained decision tree. Finally, we conduct a user study to measure the interpretability of a decision tree, rule list, and neural network, which yielded statistically significant results ($p < 0.001$).",1
"Data simulation engines like Unity are becoming an increasingly important data source that allows us to acquire ground truth labels conveniently. Moreover, we can flexibly edit the content of an image in the engine, such as objects (position, orientation) and environments (illumination, occlusion). When using simulated data as training sets, its editable content can be leveraged to mimic the distribution of real-world data, and thus reduce the content difference between the synthetic and real domains. This paper explores content adaptation in the context of semantic segmentation, where the complex street scenes are fully synthesized using 19 classes of virtual objects from a first person driver perspective and controlled by 23 attributes. To optimize the attribute values and obtain a training set of similar content to real-world data, we propose a scalable discretization-and-relaxation (SDR) approach. Under a reinforcement learning framework, we formulate attribute optimization as a random-to-optimized mapping problem using a neural network. Our method has three characteristics. 1) Instead of editing attributes of individual objects, we focus on global attributes that have large influence on the scene structure, such as object density and illumination. 2) Attributes are quantized to discrete values, so as to reduce search space and training complexity. 3) Correlated attributes are jointly optimized in a group, so as to avoid meaningless scene structures and find better convergence points. Experiment shows our system can generate reasonable and useful scenes, from which we obtain promising real-world segmentation accuracy compared with existing synthetic training sets.",0
"The use of data simulation engines like Unity is becoming increasingly important as a source of data, as it allows for convenient acquisition of ground truth labels. Additionally, the content of an image can be flexibly edited in the engine, including objects' positions and orientation, as well as environment features such as illumination and occlusion. By utilizing simulated data as training sets, the editable content can be exploited to imitate the distribution of real-world data, reducing the content disparity between synthetic and real domains. This study focuses on content adaptation for semantic segmentation, using a first-person driver perspective to fully synthesize complex street scenes with 19 classes of virtual objects controlled by 23 attributes. To obtain a training set with similar content to real-world data, a scalable discretization-and-relaxation (SDR) approach is proposed to optimize attribute values. Attribute optimization is formulated as a random-to-optimized mapping problem using a neural network under a reinforcement learning framework. The method has three distinct characteristics: 1) concentrating on global attributes that have a significant impact on scene structure, such as object density and illumination, rather than editing individual object attributes; 2) quantizing attributes to discrete values to reduce search space and training complexity; and 3) jointly optimizing correlated attributes in a group to avoid meaningless scene structures and find better convergence points. The experiment demonstrates that the proposed system can generate reasonable and useful scenes, resulting in promising real-world segmentation accuracy compared to existing synthetic training sets.",1
"Being able to predict human gaze behavior has obvious importance for behavioral vision and for computer vision applications. Most models have mainly focused on predicting free-viewing behavior using saliency maps, but these predictions do not generalize to goal-directed behavior, such as when a person searches for a visual target object. We propose the first inverse reinforcement learning (IRL) model to learn the internal reward function and policy used by humans during visual search. The viewer's internal belief states were modeled as dynamic contextual belief maps of object locations. These maps were learned by IRL and then used to predict behavioral scanpaths for multiple target categories. To train and evaluate our IRL model we created COCO-Search18, which is now the largest dataset of high-quality search fixations in existence. COCO-Search18 has 10 participants searching for each of 18 target-object categories in 6202 images, making about 300,000 goal-directed fixations. When trained and evaluated on COCO-Search18, the IRL model outperformed baseline models in predicting search fixation scanpaths, both in terms of similarity to human search behavior and search efficiency. Finally, reward maps recovered by the IRL model reveal distinctive target-dependent patterns of object prioritization, which we interpret as a learned object context.",0
"The ability to anticipate human gaze patterns is crucial for both behavioral and computer vision applications. While previous models have focused primarily on predicting free-viewing behavior using saliency maps, these predictions do not apply to purposeful behavior such as visual target object searches. Our solution is the first inverse reinforcement learning (IRL) model that learns the internal reward function and policy employed by humans during visual searches. We modeled the viewer's internal belief states as dynamic contextual belief maps that pinpoint object locations. These maps were trained using IRL and then utilized to anticipate behavioral scanpaths for several target categories. To train and assess our IRL model, we developed COCO-Search18, which is currently the most extensive collection of high-quality search fixations. COCO-Search18 consists of ten participants searching for each of 18 target-object categories in 6202 images, resulting in approximately 300,000 goal-directed fixations. When trained and evaluated on COCO-Search18, the IRL model surpassed baseline models in predicting search fixation scanpaths in terms of both similarity to human search behavior and search efficiency. Lastly, the IRL model's reward maps reveal distinctive target-specific patterns of object prioritization that we interpret as learned object context.",1
"We will discuss some statistical noise related phenomena, that were investigated by different authors in the framework of Deep Reinforcement Learning algorithms. The following algorithms are touched: DQN, Double DQN, DDPG, TD3, Hill-Climbing. Firstly, we consider overestimation, that is the harmful property resulting from noise. Then we deal with noise used for exploration, this is the useful noise. We discuss setting the noise parameter in TD3 for typical PyBullet environments associated with articulate bodies such as HopperBulletEnv and Walker2DBulletEnv. In the appendix, in relation with the Hill-Climbing algorithm, we will look at one more example of noise: adaptive noise.",0
"In this article, we will explore various statistical noise phenomena that have been studied by different researchers within the context of Deep Reinforcement Learning algorithms. Specifically, we will examine the following algorithms: DQN, Double DQN, DDPG, TD3, and Hill-Climbing. Our discussion will begin with overestimation, a detrimental outcome caused by noise, followed by exploration noise, which can be beneficial. We will also delve into the process of setting the noise parameter in TD3 for PyBullet environments commonly associated with articulate bodies such as HopperBulletEnv and Walker2DBulletEnv. Additionally, we will provide an example of adaptive noise within the Hill-Climbing algorithm in the appendix.",1
"Generative adversarial imitation learning (GAIL) demonstrates tremendous success in practice, especially when combined with neural networks. Different from reinforcement learning, GAIL learns both policy and reward function from expert (human) demonstration. Despite its empirical success, it remains unclear whether GAIL with neural networks converges to the globally optimal solution. The major difficulty comes from the nonconvex-nonconcave minimax optimization structure. To bridge the gap between practice and theory, we analyze a gradient-based algorithm with alternating updates and establish its sublinear convergence to the globally optimal solution. To the best of our knowledge, our analysis establishes the global optimality and convergence rate of GAIL with neural networks for the first time.",0
"When combined with neural networks, generative adversarial imitation learning (GAIL) has been extremely successful in practical applications. Unlike reinforcement learning, GAIL learns both policy and reward function from expert human demonstrations. However, it is uncertain whether GAIL with neural networks can achieve the globally optimal solution, due to its nonconvex-nonconcave minimax optimization structure. To address this issue, we examine a gradient-based algorithm with alternating updates and prove its sublinear convergence to the globally optimal solution. Our analysis is the first to establish the global optimality and convergence rate of GAIL with neural networks.",1
"Generative adversarial imitation learning (GAIL) is a popular inverse reinforcement learning approach for jointly optimizing policy and reward from expert trajectories. A primary question about GAIL is whether applying a certain policy gradient algorithm to GAIL attains a global minimizer (i.e., yields the expert policy), for which existing understanding is very limited. Such global convergence has been shown only for the linear (or linear-type) MDP and linear (or linearizable) reward. In this paper, we study GAIL under general MDP and for nonlinear reward function classes (as long as the objective function is strongly concave with respect to the reward parameter). We characterize the global convergence with a sublinear rate for a broad range of commonly used policy gradient algorithms, all of which are implemented in an alternating manner with stochastic gradient ascent for reward update, including projected policy gradient (PPG)-GAIL, Frank-Wolfe policy gradient (FWPG)-GAIL, trust region policy optimization (TRPO)-GAIL and natural policy gradient (NPG)-GAIL. This is the first systematic theoretical study of GAIL for global convergence.",0
"The approach of generative adversarial imitation learning (GAIL) is widely used for optimizing policy and reward based on expert trajectories in inverse reinforcement learning. However, there is uncertainty regarding whether a particular policy gradient algorithm applied to GAIL can achieve a global minimizer, i.e., the expert policy. This understanding is limited and has only been proven for linear MDP and reward. In this study, we investigate GAIL's performance under general MDP and nonlinear reward functions that are strongly concave with respect to the reward parameter. We demonstrate global convergence at a sublinear rate for various common policy gradient algorithms implemented with stochastic gradient ascent for reward updates, including PPG-GAIL, FWPG-GAIL, TRPO-GAIL, and NPG-GAIL. This is the first comprehensive theoretical analysis of GAIL's global convergence.",1
"We consider un-discounted reinforcement learning (RL) in Markov decision processes (MDPs) under drifting non-stationarity, i.e., both the reward and state transition distributions are allowed to evolve over time, as long as their respective total variations, quantified by suitable metrics, do not exceed certain variation budgets. We first develop the Sliding Window Upper-Confidence bound for Reinforcement Learning with Confidence Widening (SWUCRL2-CW) algorithm, and establish its dynamic regret bound when the variation budgets are known. In addition, we propose the Bandit-over-Reinforcement Learning (BORL) algorithm to adaptively tune the SWUCRL2-CW algorithm to achieve the same dynamic regret bound, but in a parameter-free manner, i.e., without knowing the variation budgets. Notably, learning non-stationary MDPs via the conventional optimistic exploration technique presents a unique challenge absent in existing (non-stationary) bandit learning settings. We overcome the challenge by a novel confidence widening technique that incorporates additional optimism.",0
"Our focus is on un-discounted reinforcement learning (RL) in Markov decision processes (MDPs) where drifting non-stationarity occurs. This means that the reward and state transition distributions can change over time, but only within certain variation budgets. To address this problem, we introduce the Sliding Window Upper-Confidence bound for Reinforcement Learning with Confidence Widening (SWUCRL2-CW) algorithm, which has a dynamic regret bound when the variation budgets are known. We also propose the Bandit-over-Reinforcement Learning (BORL) algorithm which adapts the SWUCRL2-CW algorithm to achieve the same dynamic regret bound without needing to know the variation budgets. However, learning non-stationary MDPs using optimistic exploration is challenging, unlike in existing (non-stationary) bandit learning settings. To overcome this, we incorporate additional optimism using a novel confidence widening technique.",1
"Motivated by broad applications in reinforcement learning and federated learning, we study local stochastic approximation over a network of agents, where their goal is to find the root of an operator composed of the local operators at the agents. Our focus is to characterize the finite-time performance of this method when the data at each agent are generated from Markov processes, and hence they are dependent. In particular, we provide the convergence rates of local stochastic approximation for both constant and time-varying step sizes. Our results show that these rates are within a logarithmic factor of the ones under independent data. We then illustrate the applications of these results to different interesting problems in multi-task reinforcement learning and federated learning.",0
"Our research delves into local stochastic approximation in a network of agents, driven by its relevance to reinforcement learning and federated learning. The objective is to locate the root of an operator that comprises the local operators at each agent. Our main focus is on examining the performance of this method in a finite amount of time, given that the data generated at each agent is dependent, as it stems from Markov processes. We determine the convergence rates of local stochastic approximation for constant and time-varying step sizes and find that these rates are only slightly lower than those when data is independent, within a logarithmic factor. We then apply these findings to several intriguing problems in multi-task reinforcement learning and federated learning.",1
"We study the problem of adaptive control in partially observable linear quadratic Gaussian control systems, where the model dynamics are unknown a priori. We propose LqgOpt, a novel reinforcement learning algorithm based on the principle of optimism in the face of uncertainty, to effectively minimize the overall control cost. We employ the predictor state evolution representation of the system dynamics and deploy a recently proposed closed-loop system identification method, estimation, and confidence bound construction. LqgOpt efficiently explores the system dynamics, estimates the model parameters up to their confidence interval, and deploys the controller of the most optimistic model for further exploration and exploitation. We provide stability guarantees for LqgOpt and prove the regret upper bound of $\tilde{\mathcal{O}}(\sqrt{T})$ for adaptive control of linear quadratic Gaussian (LQG) systems, where $T$ is the time horizon of the problem.",0
"Our focus is on adaptive control in partially observable linear quadratic Gaussian control systems, which lack prior knowledge of the model dynamics. To minimize the overall control cost, we introduce LqgOpt, a reinforcement learning algorithm that leverages the optimism principle when faced with unknowns. We utilize the predictor state evolution representation of the system dynamics and incorporate a closed-loop system identification method, estimation, and confidence bound construction. By exploring the system dynamics, estimating model parameters within their confidence intervals, and deploying the controller of the most optimistic model, LqgOpt is able to effectively balance exploration and exploitation. We guarantee LqgOpt's stability and prove its regret upper bound, which is $\tilde{\mathcal{O}}(\sqrt{T})$ for adaptive control of linear quadratic Gaussian (LQG) systems with a time horizon of $T$.",1
"We study the problem of system identification and adaptive control in partially observable linear dynamical systems. Adaptive and closed-loop system identification is a challenging problem due to correlations introduced in data collection. In this paper, we present the first model estimation method with finite-time guarantees in both open and closed-loop system identification. Deploying this estimation method, we propose adaptive control online learning (AdaptOn), an efficient reinforcement learning algorithm that adaptively learns the system dynamics and continuously updates its controller through online learning steps. AdaptOn estimates the model dynamics by occasionally solving a linear regression problem through interactions with the environment. Using policy re-parameterization and the estimated model, AdaptOn constructs counterfactual loss functions to be used for updating the controller through online gradient descent. Over time, AdaptOn improves its model estimates and obtains more accurate gradient updates to improve the controller. We show that AdaptOn achieves a regret upper bound of $\text{polylog}\left(T\right)$, after $T$ time steps of agent-environment interaction. To the best of our knowledge, AdaptOn is the first algorithm that achieves $\text{polylog}\left(T\right)$ regret in adaptive control of unknown partially observable linear dynamical systems which includes linear quadratic Gaussian (LQG) control.",0
"The focus of our study is on partially observable linear dynamical systems and the challenges of system identification and adaptive control. Collecting data in an adaptive and closed-loop system is difficult due to the introduction of correlations. Our paper presents a model estimation method that offers finite-time guarantees for both open and closed-loop system identification. We introduce an efficient reinforcement learning algorithm, named AdaptOn, which utilizes this estimation method to learn system dynamics and update its controller through online learning steps. AdaptOn periodically solves a linear regression problem to estimate the model dynamics and constructs counterfactual loss functions using policy re-parameterization and the estimated model for updating the controller through online gradient descent. Over time, AdaptOn improves its model estimates and gradient updates to enhance the controller. Our results demonstrate that AdaptOn achieves a regret upper bound of $\text{polylog}\left(T\right)$ after $T$ time steps of agent-environment interaction. AdaptOn is the first algorithm to achieve $\text{polylog}\left(T\right)$ regret in adaptive control of unknown partially observable linear dynamical systems, including linear quadratic Gaussian (LQG) control.",1
"Model-based reinforcement learning (RL) algorithms allow us to combine model-generated data with those collected from interaction with the real system in order to alleviate the data efficiency problem in RL. However, designing such algorithms is often challenging because the bias in simulated data may overshadow the ease of data generation. A potential solution to this challenge is to jointly learn and improve model and policy using a universal objective function. In this paper, we leverage the connection between RL and probabilistic inference, and formulate such an objective function as a variational lower-bound of a log-likelihood. This allows us to use expectation maximization (EM) and iteratively fix a baseline policy and learn a variational distribution, consisting of a model and a policy (E-step), followed by improving the baseline policy given the learned variational distribution (M-step). We propose model-based and model-free policy iteration (actor-critic) style algorithms for the E-step and show how the variational distribution learned by them can be used to optimize the M-step in a fully model-based fashion. Our experiments on a number of continuous control tasks show that despite being more complex, our model-based (E-step) algorithm, called {\em variational model-based policy optimization} (VMBPO), is more sample-efficient and robust to hyper-parameter tuning than its model-free (E-step) counterpart. Using the same control tasks, we also compare VMBPO with several state-of-the-art model-based and model-free RL algorithms and show its sample efficiency and performance.",0
"The data efficiency problem in reinforcement learning (RL) can be alleviated by combining model-generated data with real system interaction using model-based RL algorithms. However, designing such algorithms is often challenging as the bias in simulated data may overshadow the ease of data generation. To address this challenge, a potential solution is to learn and improve the model and policy jointly using a universal objective function. In this study, we formulated such an objective function as a variational lower-bound of a log-likelihood by leveraging the connection between RL and probabilistic inference. We employed expectation maximization (EM) to iteratively fix a baseline policy and learn a variational distribution, consisting of a model and a policy (E-step), followed by improving the baseline policy given the learned variational distribution (M-step). We proposed model-based and model-free policy iteration style algorithms for the E-step and demonstrated how the variational distribution learned by them can be used to optimize the M-step in a fully model-based fashion. Our experiments on continuous control tasks revealed that our model-based algorithm, called variational model-based policy optimization (VMBPO), is more sample-efficient and robust to hyper-parameter tuning than its model-free counterpart. We also compared VMBPO with several state-of-the-art model-based and model-free RL algorithms on the same control tasks and demonstrated its sample efficiency and performance.",1
"A major challenge in modern reinforcement learning (RL) is efficient control of dynamical systems from high-dimensional sensory observations. Learning controllable embedding (LCE) is a promising approach that addresses this challenge by embedding the observations into a lower-dimensional latent space, estimating the latent dynamics, and utilizing it to perform control in the latent space. Two important questions in this area are how to learn a representation that is amenable to the control problem at hand, and how to achieve an end-to-end framework for representation learning and control. In this paper, we take a few steps towards addressing these questions. We first formulate a LCE model to learn representations that are suitable to be used by a policy iteration style algorithm in the latent space. We call this model control-aware representation learning (CARL). We derive a loss function for CARL that has close connection to the prediction, consistency, and curvature (PCC) principle for representation learning. We derive three implementations of CARL. In the offline implementation, we replace the locally-linear control algorithm (e.g.,~iLQR) used by the existing LCE methods with a RL algorithm, namely model-based soft actor-critic, and show that it results in significant improvement. In online CARL, we interleave representation learning and control, and demonstrate further gain in performance. Finally, we propose value-guided CARL, a variation in which we optimize a weighted version of the CARL loss function, where the weights depend on the TD-error of the current policy. We evaluate the proposed algorithms by extensive experiments on benchmark tasks and compare them with several LCE baselines.",0
"Efficiently controlling dynamical systems from high-dimensional sensory observations is a major challenge in modern reinforcement learning (RL). One promising approach to address this challenge is learning controllable embedding (LCE), which involves embedding observations into a lower-dimensional latent space, estimating latent dynamics, and utilizing it for control. However, two important questions in this area are how to learn a representation that is suitable for the control problem and how to achieve an end-to-end framework for representation learning and control. In this paper, we introduce control-aware representation learning (CARL), a LCE model designed to learn representations that can be used by a policy iteration style algorithm in the latent space. We derive a loss function for CARL based on the prediction, consistency, and curvature (PCC) principle for representation learning. We present three implementations of CARL, including offline, online, and value-guided variations. Our experiments on benchmark tasks show that CARL outperforms several LCE baselines and that our proposed variations lead to significant improvements in performance.",1
"We study minimax optimal reinforcement learning in episodic factored Markov decision processes (FMDPs), which are MDPs with conditionally independent transition components. Assuming the factorization is known, we propose two model-based algorithms. The first one achieves minimax optimal regret guarantees for a rich class of factored structures, while the second one enjoys better computational complexity with a slightly worse regret. A key new ingredient of our algorithms is the design of a bonus term to guide exploration. We complement our algorithms by presenting several structure-dependent lower bounds on regret for FMDPs that reveal the difficulty hiding in the intricacy of the structures.",0
"Our focus is on minimax optimal reinforcement learning in episodic factored Markov decision processes (FMDPs), which are MDPs where the transition components are conditionally independent. We introduce two model-based algorithms, assuming the factorization is known. The first algorithm achieves minimax optimal regret guarantees for a wide range of factored structures, while the second algorithm boasts better computational complexity, albeit with slightly worse regret. A key feature of our algorithms is the incorporation of a bonus term to guide exploration. Additionally, we provide several structure-dependent lower bounds on regret for FMDPs, highlighting the complexity inherent in these intricate structures.",1
"We develop provably efficient reinforcement learning algorithms for two-player zero-sum finite-horizon Markov games with simultaneous moves. To incorporate function approximation, we consider a family of Markov games where the reward function and transition kernel possess a linear structure. Both the offline and online settings of the problems are considered. In the offline setting, we control both players and aim to find the Nash Equilibrium by minimizing the duality gap. In the online setting, we control a single player playing against an arbitrary opponent and aim to minimize the regret. For both settings, we propose an optimistic variant of the least-squares minimax value iteration algorithm. We show that our algorithm is computationally efficient and provably achieves an $\tilde O(\sqrt{d^3 H^3 T} )$ upper bound on the duality gap and regret, where $d$ is the linear dimension, $H$ the horizon and $T$ the total number of timesteps. Our results do not require additional assumptions on the sampling model.   Our setting requires overcoming several new challenges that are absent in Markov decision processes or turn-based Markov games. In particular, to achieve optimism with simultaneous moves, we construct both upper and lower confidence bounds of the value function, and then compute the optimistic policy by solving a general-sum matrix game with these bounds as the payoff matrices. As finding the Nash Equilibrium of a general-sum game is computationally hard, our algorithm instead solves for a Coarse Correlated Equilibrium (CCE), which can be obtained efficiently. To our best knowledge, such a CCE-based scheme for optimism has not appeared in the literature and might be of interest in its own right.",0
"We have developed reinforcement learning algorithms that are efficiently provable for two-player zero-sum finite-horizon Markov games with simultaneous moves. To incorporate function approximation, we consider Markov games where the reward function and transition kernel have a linear structure. We address both offline and online settings of the problem. In the offline setting, we aim to control both players and find the Nash Equilibrium by minimizing the duality gap. In the online setting, we control a single player playing against an arbitrary opponent and aim to minimize the regret. For both settings, we propose an optimistic variant of the least-squares minimax value iteration algorithm. Our algorithm is computationally efficient and can achieve an $\tilde O(\sqrt{d^3 H^3 T} )$ upper bound on the duality gap and regret, where $d$ is the linear dimension, $H$ is the horizon, and $T$ is the total number of timesteps. Our results do not require additional assumptions on the sampling model.   Our approach faces several new challenges that are absent in Markov decision processes or turn-based Markov games. To achieve optimism with simultaneous moves, we construct upper and lower confidence bounds of the value function and then compute the optimistic policy by solving a general-sum matrix game with these bounds as the payoff matrices. As finding the Nash Equilibrium of a general-sum game is computationally hard, our algorithm instead solves for a Coarse Correlated Equilibrium (CCE) which can be efficiently obtained. Such a CCE-based scheme for optimism has not appeared in the literature and may be of interest in its own right.",1
"Model-agnostic meta-learning (MAML) formulates meta-learning as a bilevel optimization problem, where the inner level solves each subtask based on a shared prior, while the outer level searches for the optimal shared prior by optimizing its aggregated performance over all the subtasks. Despite its empirical success, MAML remains less understood in theory, especially in terms of its global optimality, due to the nonconvexity of the meta-objective (the outer-level objective). To bridge such a gap between theory and practice, we characterize the optimality gap of the stationary points attained by MAML for both reinforcement learning and supervised learning, where the inner-level and outer-level problems are solved via first-order optimization methods. In particular, our characterization connects the optimality gap of such stationary points with (i) the functional geometry of inner-level objectives and (ii) the representation power of function approximators, including linear models and neural networks. To the best of our knowledge, our analysis establishes the global optimality of MAML with nonconvex meta-objectives for the first time.",0
"The concept of model-agnostic meta-learning (MAML) involves a two-level optimization problem where a shared prior is utilized to solve subtasks. The outer level seeks to enhance overall performance by finding the optimal shared prior. Despite its successful implementation, MAML has yet to be fully understood in theory due to the nonconvexity of the meta-objective. To bridge this gap, we analyze the optimality gap of stationary points attained via first-order optimization methods for both reinforcement and supervised learning. Our analysis connects the gap with the functional geometry of inner-level objectives and the representation power of function approximators, including linear models and neural networks. Our findings establish, for the first time, the global optimality of MAML with nonconvex meta-objectives.",1
"This paper introduces the deep coordination graph (DCG) for collaborative multi-agent reinforcement learning. DCG strikes a flexible trade-off between representational capacity and generalization by factoring the joint value function of all agents according to a coordination graph into payoffs between pairs of agents. The value can be maximized by local message passing along the graph, which allows training of the value function end-to-end with Q-learning. Payoff functions are approximated with deep neural networks that employ parameter sharing and low-rank approximations to significantly improve sample efficiency. We show that DCG can solve predator-prey tasks that highlight the relative overgeneralization pathology, as well as challenging StarCraft II micromanagement tasks.",0
"The aim of this paper is to introduce the deep coordination graph (DCG) as a means of facilitating collaborative multi-agent reinforcement learning. DCG is designed to balance representational capacity and generalization by utilizing a coordination graph to factor the joint value function of all agents into payoffs between pairs of agents. The value can be maximized through local message passing along the graph, thereby enabling end-to-end training of the value function using Q-learning. Deep neural networks are employed to approximate the payoff functions, with parameter sharing and low-rank approximations being utilized to enhance sample efficiency. The efficacy of DCG is demonstrated through its ability to solve predator-prey tasks that demonstrate relative overgeneralization, as well as challenging StarCraft II micromanagement tasks.",1
"One of the central challenges faced by a reinforcement learning (RL) agent is to effectively learn a (near-)optimal policy in environments with large state spaces having sparse and noisy feedback signals. In real-world applications, an expert with additional domain knowledge can help in speeding up the learning process via \emph{shaping the environment}, i.e., making the environment more learner-friendly. A popular paradigm in literature is \emph{potential-based reward shaping}, where the environment's reward function is augmented with additional local rewards using a potential function. However, the applicability of potential-based reward shaping is limited in settings where (i) the state space is very large, and it is challenging to compute an appropriate potential function, (ii) the feedback signals are noisy, and even with shaped rewards the agent could be trapped in local optima, and (iii) changing the rewards alone is not sufficient, and effective shaping requires changing the dynamics. We address these limitations of potential-based shaping methods and propose a novel framework of \emph{environment shaping using state abstraction}. Our key idea is to compress the environment's large state space with noisy signals to an abstracted space, and to use this abstraction in creating smoother and more effective feedback signals for the agent. We study the theoretical underpinnings of our abstraction-based environment shaping, and show that the agent's policy learnt in the shaped environment preserves near-optimal behavior in the original environment.",0
"Learning an (almost) perfect policy in environments with vast state spaces and scarce and inaccurate feedback signals is a major challenge for reinforcement learning (RL) agents. In practical scenarios, experts possessing domain knowledge can accelerate the learning process by making the environment more learner-friendly, a process known as ""shaping the environment"". One such popular technique is ""potential-based reward shaping"", where the environment's reward function is supplemented with local rewards using a potential function. However, potential-based reward shaping is not always practical when the state space is vast, the feedback signals are noisy, and changing the rewards alone is insufficient. To address these limitations, we propose a novel ""environment shaping using state abstraction"" framework that compresses the environment's large state space with noisy signals to a more concise space and creates better feedback signals for the agent. We explore the theoretical basis of our abstraction-based environment shaping approach and demonstrate that the agent's policy learned in the shaped environment preserves near-optimal behavior in the original environment.",1
"We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning problems whose state-action space is endowed with a metric. We introduce Kernel-UCBVI, a model-based optimistic algorithm that leverages the smoothness of the MDP and a non-parametric kernel estimator of the rewards and transitions to efficiently balance exploration and exploitation. Unlike existing approaches with regret guarantees, it does not use any kind of partitioning of the state-action space. For problems with $K$ episodes and horizon $H$, we provide a regret bound of $O\left( H^3 K^{\max\left(\frac{1}{2}, \frac{2d}{2d+1}\right)}\right)$, where $d$ is the covering dimension of the joint state-action space. We empirically validate Kernel-UCBVI on discrete and continuous MDPs.",0
"In this article, we examine the dilemma of exploration-exploitation in reinforcement learning problems with a finite horizon and a metric state-action space. Our proposed solution is Kernel-UCBVI, which is a model-based optimistic algorithm that makes use of the smoothness of the MDP and a non-parametric kernel estimator of rewards and transitions to balance exploration and exploitation efficiently. Unlike other methods with regret guarantees, Kernel-UCBVI does not require partitioning of the state-action space. We provide a regret bound of $O\left( H^3 K^{\max\left(\frac{1}{2}, \frac{2d}{2d+1}\right)}\right)$ for problems with $K$ episodes and horizon $H$, where $d$ is the covering dimension of the joint state-action space. Our approach has been verified empirically on both discrete and continuous MDPs.",1
"In reinforcement learning, the discount factor $\gamma$ controls the agent's effective planning horizon. Traditionally, this parameter was considered part of the MDP; however, as deep reinforcement learning algorithms tend to become unstable when the effective planning horizon is long, recent works refer to $\gamma$ as a hyper-parameter -- thus changing the underlying MDP and potentially leading the agent towards sub-optimal behavior on the original task. In this work, we introduce \emph{reward tweaking}. Reward tweaking learns a surrogate reward function $\tilde r$ for the discounted setting that induces optimal behavior on the original finite-horizon total reward task. Theoretically, we show that there exists a surrogate reward that leads to optimality in the original task and discuss the robustness of our approach. Additionally, we perform experiments in high-dimensional continuous control tasks and show that reward tweaking guides the agent towards better long-horizon returns although it plans for short horizons.",0
"The discount factor $\gamma$ plays a crucial role in reinforcement learning by determining the agent's effective planning horizon. In traditional approaches, $\gamma$ was considered as part of the MDP. However, with the advent of deep reinforcement learning, it has been observed that long planning horizons can lead to instability in the algorithm. Consequently, recent works treat $\gamma$ as a hyper-parameter, which may cause the agent to exhibit sub-optimal behavior on the original task. To address this issue, we propose a method called reward tweaking. Reward tweaking involves learning a surrogate reward function $\tilde r$ that induces optimal behavior on the original task, even in the discounted setting. We prove that there exists a surrogate reward that leads to optimal behavior in the original task and discuss the robustness of our approach. Our experiments on high-dimensional continuous control tasks demonstrate that reward tweaking leads to better long-horizon returns, even when the agent plans for short horizons.",1
"Animals are able to discover the topological map (graph) of surrounding environment, which will be used for navigation. Inspired by this biological phenomenon, researchers have recently proposed to generate graph representation for Markov decision process (MDP) and use such graphs for planning in reinforcement learning (RL). However, existing graph generation methods suffer from many drawbacks. One drawback is that existing methods do not learn an abstraction for graphs, which results in high memory and computation cost. This drawback also makes generated graph non-robust, which degrades the planning performance. Another drawback is that existing methods cannot be used for facilitating exploration which is important in RL. In this paper, we propose a new method, called topological map abstraction (TOMA), for graph generation. TOMA can generate an abstract graph representation for MDP, which costs much less memory and computation cost than existing methods. Furthermore, TOMA can be used for facilitating exploration. In particular, we propose planning to explore, in which TOMA is used to accelerate exploration by guiding the agent towards unexplored states. A novel experience replay module called vertex memory is also proposed to improve exploration performance. Experimental results show that TOMA can outperform existing methods to achieve the state-of-the-art performance.",0
"Researchers have been studying the way animals create a topological map of their environment to navigate through it. They have proposed using graph representations for Markov decision processes (MDP) in reinforcement learning (RL) to plan routes. However, existing methods have limitations, such as high memory and computation costs due to the lack of graph abstraction and non-robustness, which affects planning performance. Additionally, exploration in RL cannot be facilitated by current methods. A new method called topological map abstraction (TOMA) has been developed, which generates abstract graph representations for MDPs. TOMA is more efficient and can facilitate exploration by guiding the agent towards unexplored states. A new experience replay module called vertex memory has also been introduced to improve exploration performance. Experimental results show that TOMA outperforms existing methods and achieves state-of-the-art performance.",1
"We present a new paradigm for Neural ODE algorithms, called ODEtoODE, where time-dependent parameters of the main flow evolve according to a matrix flow on the orthogonal group O(d). This nested system of two flows, where the parameter-flow is constrained to lie on the compact manifold, provides stability and effectiveness of training and provably solves the gradient vanishing-explosion problem which is intrinsically related to training deep neural network architectures such as Neural ODEs. Consequently, it leads to better downstream models, as we show on the example of training reinforcement learning policies with evolution strategies, and in the supervised learning setting, by comparing with previous SOTA baselines. We provide strong convergence results for our proposed mechanism that are independent of the depth of the network, supporting our empirical studies. Our results show an intriguing connection between the theory of deep neural networks and the field of matrix flows on compact manifolds.",0
"The ODEtoODE paradigm introduces a novel approach to Neural ODE algorithms by utilizing a matrix flow on the orthogonal group O(d) to govern the evolution of time-dependent parameters in the main flow. This nested system ensures stability and effectiveness in training while addressing the gradient vanishing-explosion problem commonly encountered in deep neural network architectures like Neural ODEs. The compact manifold constraint of the parameter-flow results in improved downstream models, as demonstrated through experiments in reinforcement and supervised learning, and is supported by strong convergence results that are independent of network depth. Our findings establish a compelling link between deep neural networks and matrix flows on compact manifolds.",1
"Increasing the scale of reinforcement learning experiments has allowed researchers to achieve unprecedented results in both training sophisticated agents for video games, and in sim-to-real transfer for robotics. Typically such experiments rely on large distributed systems and require expensive hardware setups, limiting wider access to this exciting area of research. In this work we aim to solve this problem by optimizing the efficiency and resource utilization of reinforcement learning algorithms instead of relying on distributed computation. We present the ""Sample Factory"", a high-throughput training system optimized for a single-machine setting. Our architecture combines a highly efficient, asynchronous, GPU-based sampler with off-policy correction techniques, allowing us to achieve throughput higher than $10^5$ environment frames/second on non-trivial control problems in 3D without sacrificing sample efficiency. We extend Sample Factory to support self-play and population-based training and apply these techniques to train highly capable agents for a multiplayer first-person shooter game. The source code is available at https://github.com/alex-petrenko/sample-factory",0
"Previously, researchers were only able to achieve limited results in training advanced agents for video games and sim-to-real transfer for robotics, as such experiments required costly hardware setups and large distributed systems. However, with the increase in scale of reinforcement learning experiments, researchers have achieved unprecedented results in these areas. Nevertheless, access to this exciting field of research has remained limited due to the reliance on expensive hardware setups. To address this issue, we have developed the ""Sample Factory"", a high-throughput training system optimized for a single-machine setting. By utilizing efficient, asynchronous, GPU-based samplers and off-policy correction techniques, we are able to achieve a throughput of over $10^5$ environment frames/second on non-trivial control problems in 3D, without sacrificing sample efficiency. We have also extended the Sample Factory to support self-play and population-based training, and have applied these techniques to train agents for a multiplayer first-person shooter game. The source code for the Sample Factory is available at https://github.com/alex-petrenko/sample-factory.",1
"In reward-poisoning attacks against reinforcement learning (RL), an attacker can perturb the environment reward $r_t$ into $r_t+\delta_t$ at each step, with the goal of forcing the RL agent to learn a nefarious policy. We categorize such attacks by the infinity-norm constraint on $\delta_t$: We provide a lower threshold below which reward-poisoning attack is infeasible and RL is certified to be safe; we provide a corresponding upper threshold above which the attack is feasible. Feasible attacks can be further categorized as non-adaptive where $\delta_t$ depends only on $(s_t,a_t, s_{t+1})$, or adaptive where $\delta_t$ depends further on the RL agent's learning process at time $t$. Non-adaptive attacks have been the focus of prior works. However, we show that under mild conditions, adaptive attacks can achieve the nefarious policy in steps polynomial in state-space size $|S|$, whereas non-adaptive attacks require exponential steps. We provide a constructive proof that a Fast Adaptive Attack strategy achieves the polynomial rate. Finally, we show that empirically an attacker can find effective reward-poisoning attacks using state-of-the-art deep RL techniques.",0
"The goal of reward-poisoning attacks against reinforcement learning (RL) is to manipulate the environment reward at each step (from $r_t$ to $r_t+\delta_t$) in order to force the RL agent to adopt a nefarious policy. These attacks can be classified based on the infinity-norm constraint on $\delta_t$, with a lower threshold indicating safety and an upper threshold indicating feasibility. Feasible attacks can be non-adaptive (dependent only on $(s_t,a_t, s_{t+1})$) or adaptive (dependent on the RL agent's learning process at time $t). Prior work has focused on non-adaptive attacks, but we demonstrate that adaptive attacks can achieve the nefarious policy in a polynomial number of steps, while non-adaptive attacks require an exponential number of steps. We provide a constructive proof for a Fast Adaptive Attack strategy that achieves the polynomial rate. Finally, we show through empirical analysis that attackers can effectively use state-of-the-art deep RL techniques to conduct reward-poisoning attacks.",1
"We study risk-sensitive reinforcement learning in episodic Markov decision processes with unknown transition kernels, where the goal is to optimize the total reward under the risk measure of exponential utility. We propose two provably efficient model-free algorithms, Risk-Sensitive Value Iteration (RSVI) and Risk-Sensitive Q-learning (RSQ). These algorithms implement a form of risk-sensitive optimism in the face of uncertainty, which adapts to both risk-seeking and risk-averse modes of exploration. We prove that RSVI attains an $\tilde{O}\big(\lambda(|\beta| H^2) \cdot \sqrt{H^{3} S^{2}AT} \big)$ regret, while RSQ attains an $\tilde{O}\big(\lambda(|\beta| H^2) \cdot \sqrt{H^{4} SAT} \big)$ regret, where $\lambda(u) = (e^{3u}-1)/u$ for $u>0$. In the above, $\beta$ is the risk parameter of the exponential utility function, $S$ the number of states, $A$ the number of actions, $T$ the total number of timesteps, and $H$ the episode length. On the flip side, we establish a regret lower bound showing that the exponential dependence on $|\beta|$ and $H$ is unavoidable for any algorithm with an $\tilde{O}(\sqrt{T})$ regret (even when the risk objective is on the same scale as the original reward), thus certifying the near-optimality of the proposed algorithms. Our results demonstrate that incorporating risk awareness into reinforcement learning necessitates an exponential cost in $|\beta|$ and $H$, which quantifies the fundamental tradeoff between risk sensitivity (related to aleatoric uncertainty) and sample efficiency (related to epistemic uncertainty). To the best of our knowledge, this is the first regret analysis of risk-sensitive reinforcement learning with the exponential utility.",0
"The focus of our study is on risk-sensitive reinforcement learning in episodic Markov decision processes where the transition kernels are unknown. Our goal is to optimize the total reward while considering the risk measure of exponential utility. We have designed two model-free algorithms, Risk-Sensitive Value Iteration (RSVI) and Risk-Sensitive Q-learning (RSQ), which are efficient and adaptable to both risk-seeking and risk-averse modes of exploration. Through our analysis, we have proven that RSVI has an $\tilde{O}\big(\lambda(|\beta| H^2) \cdot \sqrt{H^{3} S^{2}AT} \big)$ regret, while RSQ has an $\tilde{O}\big(\lambda(|\beta| H^2) \cdot \sqrt{H^{4} SAT} \big)$ regret, where $\lambda(u) = (e^{3u}-1)/u$ for $u>0$. Here, $\beta$ represents the risk parameter of the exponential utility function, $S$ represents the number of states, $A$ represents the number of actions, $T$ represents the total number of timesteps, and $H$ represents the episode length. Additionally, we have established a lower bound on regret that shows that any algorithm with an $\tilde{O}(\sqrt{T})$ regret will have an exponential dependence on $|\beta|$ and $H$. Our study highlights the tradeoff between risk sensitivity and sample efficiency in reinforcement learning and provides the first regret analysis of risk-sensitive reinforcement learning with the exponential utility.",1
"Most reinforcement learning approaches used in behavior generation utilize vectorial information as input. However, this requires the network to have a pre-defined input-size -- in semantic environments this means assuming the maximum number of vehicles. Additionally, this vectorial representation is not invariant to the order and number of vehicles. To mitigate the above-stated disadvantages, we propose combining graph neural networks with actor-critic reinforcement learning. As graph neural networks apply the same network to every vehicle and aggregate incoming edge information, they are invariant to the number and order of vehicles. This makes them ideal candidates to be used as networks in semantic environments -- environments consisting of objects lists. Graph neural networks exhibit some other advantages that make them favorable to be used in semantic environments. The relational information is explicitly given and does not have to be inferred. Moreover, graph neural networks propagate information through the network and can gather higher-degree information. We demonstrate our approach using a highway lane-change scenario and compare the performance of graph neural networks to conventional ones. We show that graph neural networks are capable of handling scenarios with a varying number and order of vehicles during training and application.",0
"The majority of reinforcement learning methods used for behavior generation rely on vectorial information as input. Nonetheless, this necessitates a predetermined input size for the network, which assumes the highest number of vehicles in semantic environments. Furthermore, the vectorial representation is not insensitive to the sequence and quantity of vehicles. To address these drawbacks, we suggest merging graph neural networks with actor-critic reinforcement learning. Graph neural networks are capable of deploying the same network to every vehicle and collecting incoming edge information, making them insensitive to the number and order of vehicles. Hence, they are suitable for networks used in semantic environments, where objects lists are present. Graph neural networks present other advantages, such as explicitly given relational information and the ability to gather higher-degree information. We have demonstrated our approach using a highway lane-change scenario and compared the performance of graph neural networks to conventional ones. We have shown that graph neural networks can cope with scenarios with varying numbers and orders of vehicles during training and application.",1
"Much of the current work on reinforcement learning studies episodic settings, where the agent is reset between trials to an initial state distribution, often with well-shaped reward functions. Non-episodic settings, where the agent must learn through continuous interaction with the world without resets, and where the agent receives only delayed and sparse reward signals, is substantially more difficult, but arguably more realistic considering real-world environments do not present the learner with a convenient ""reset mechanism"" and easy reward shaping. In this paper, instead of studying algorithmic improvements that can address such non-episodic and sparse reward settings, we instead study the kinds of environment properties that can make learning under such conditions easier. Understanding how properties of the environment impact the performance of reinforcement learning agents can help us to structure our tasks in ways that make learning tractable. We first discuss what we term ""environment shaping"" -- modifications to the environment that provide an alternative to reward shaping, and may be easier to implement. We then discuss an even simpler property that we refer to as ""dynamism,"" which describes the degree to which the environment changes independent of the agent's actions and can be measured by environment transition entropy. Surprisingly, we find that even this property can substantially alleviate the challenges associated with non-episodic RL in sparse reward settings. We provide an empirical evaluation on a set of new tasks focused on non-episodic learning with sparse rewards. Through this study, we hope to shift the focus of the community towards analyzing how properties of the environment can affect learning and the ultimate type of behavior that is learned via RL.",0
"The current research on reinforcement learning mainly focuses on episodic settings, where the agent is reset to an initial state distribution with well-shaped reward functions. However, non-episodic settings, where the agent learns continuously without resets and with sparse reward signals, are more challenging but more realistic. This study aims to investigate how the properties of the environment can improve learning in non-episodic and sparse reward settings instead of focusing on algorithmic improvements. By understanding how environment properties impact the performance of reinforcement learning agents, we can structure tasks to make learning easier. We discuss two properties: ""environment shaping"" and ""dynamism."" The latter describes the degree to which the environment changes independently of the agent's actions and can be measured by environment transition entropy. Surprisingly, even this property can significantly alleviate the challenges associated with non-episodic RL in sparse reward settings. We provide empirical evidence through a new set of tasks and hope to shift the community's focus towards analyzing how environment properties affect learning and the type of behavior learned through RL.",1
"Empowered by expressive function approximators such as neural networks, deep reinforcement learning (DRL) achieves tremendous empirical successes. However, learning expressive function approximators requires collecting a large dataset (interventional data) by interacting with the environment. Such a lack of sample efficiency prohibits the application of DRL to critical scenarios, e.g., autonomous driving and personalized medicine, since trial and error in the online setting is often unsafe and even unethical. In this paper, we study how to incorporate the dataset (observational data) collected offline, which is often abundantly available in practice, to improve the sample efficiency in the online setting. To incorporate the possibly confounded observational data, we propose the deconfounded optimistic value iteration (DOVI) algorithm, which incorporates the confounded observational data in a provably efficient manner. More specifically, DOVI explicitly adjusts for the confounding bias in the observational data, where the confounders are partially observed or unobserved. In both cases, such adjustments allow us to construct the bonus based on a notion of information gain, which takes into account the amount of information acquired from the offline setting. In particular, we prove that the regret of DOVI is smaller than the optimal regret achievable in the pure online setting by a multiplicative factor, which decreases towards zero when the confounded observational data are more informative upon the adjustments. Our algorithm and analysis serve as a step towards causal reinforcement learning.",0
"Deep reinforcement learning (DRL) has achieved significant success thanks to expressive function approximators like neural networks. However, gathering the interventional data required to learn these approximators is a time-consuming process, rendering DRL unsuitable for critical scenarios such as autonomous driving and personalized medicine. This is due to the inherent dangers of trial and error in the online setting. The paper proposes using observational data, which is often readily available, to improve sample efficiency in the online setting. To incorporate this confounded observational data, the deconfounded optimistic value iteration (DOVI) algorithm was proposed. DOVI adjusts for confounding bias in the data, which allows for the construction of bonuses based on information gain. The paper proves that DOVI has a smaller regret than the optimal regret achievable in the pure online setting. This analysis is a step towards causal reinforcement learning.",1
"Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this fixed dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. Ablation studies highlight the role of offline dataset size and diversity as well as the algorithm choice in our positive results. Overall, the results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.",0
"The utilization of a fixed offline dataset of logged interactions in off-policy reinforcement learning (RL) is a crucial aspect in practical applications. This research paper examines offline RL by using the complete replay experience of a DQN agent on 60 Atari 2600 games, known as the DQN replay dataset. We demonstrate that recent off-policy deep RL algorithms can outperform the fully trained DQN agent when trained solely on this fixed dataset. To enhance generalization in the offline setting, we introduce Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on multiple Q-value estimates' random convex combinations. The offline REM trained on the DQN replay dataset outperforms strong RL baselines. Our positive results showcase the importance of offline dataset size and diversity, as well as the algorithm choice through ablation studies. In summary, the findings suggest that robust RL algorithms trained on sufficiently large and diverse offline datasets can result in high-quality policies. The DQN replay dataset is accessible and available for use as an offline RL benchmark.",1
"This paper presents a framework to tackle constrained combinatorial optimization problems using deep Reinforcement Learning (RL). To this end, we extend the Neural Combinatorial Optimization (NCO) theory in order to deal with constraints in its formulation.   Notably, we propose defining constrained combinatorial problems as fully observable Constrained Markov Decision Processes (CMDP). In that context, the solution is iteratively constructed based on interactions with the environment. The model, in addition to the reward signal, relies on penalty signals generated from constraint dissatisfaction to infer a policy that acts as a heuristic algorithm. Moreover, having access to the complete state representation during the optimization process allows us to rely on memory-less architectures, enhancing the results obtained in previous sequence-to-sequence approaches. Conducted experiments on the constrained Job Shop and Resource Allocation problems prove the superiority of the proposal for computing rapid solutions when compared to classical heuristic, metaheuristic, and Constraint Programming (CP) solvers.",0
"The objective of this paper is to introduce a framework that utilizes deep Reinforcement Learning (RL) to solve combinatorial optimization problems with constraints. To achieve this, we expand on the Neural Combinatorial Optimization (NCO) theory by incorporating constraint management into its formulation. Our approach involves defining constrained combinatorial problems as fully observable Constrained Markov Decision Processes (CMDP), where the solution is constructed iteratively through interactions with the environment. In addition to the reward signal, our model utilizes penalty signals generated from constraint dissatisfaction to develop a policy that acts as a heuristic algorithm. Our method also benefits from having access to the complete state representation during optimization, allowing us to employ memory-less architectures and improve on previous sequence-to-sequence approaches. Our experiments on the constrained Job Shop and Resource Allocation problems demonstrate the effectiveness of our proposal compared to classical heuristic, metaheuristic, and Constraint Programming (CP) solvers in generating rapid solutions.",1
"We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning (RL). When the state space is large or continuous, traditional tabular approaches are unfeasible and some form of function approximation is mandatory. In this paper, we introduce an optimistically-initialized variant of the popular randomized least-squares value iteration (RLSVI), a model-free algorithm where exploration is induced by perturbing the least-squares approximation of the action-value function. Under the assumption that the Markov decision process has low-rank transition dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by $\widetilde O(d^2 H^2 \sqrt{T})$ where $ d $ are the feature dimension, $ H $ is the horizon, and $ T $ is the total number of steps. To the best of our knowledge, this is the first frequentist regret analysis for randomized exploration with function approximation.",0
"The exploration-exploitation dilemma in finite-horizon reinforcement learning (RL) is examined in this study. Tabular approaches are not practical when the state space is large or continuous, and some form of function approximation is required. A positively-initialized variant of the randomized least-squares value iteration (RLSVI) is introduced, which is a model-free algorithm that incorporates exploration by disturbing the least-squares approximation of the action-value function. Assuming that the Markov decision process has low-rank transition dynamics, the study demonstrates that RLSVI's frequentist regret is limited to $\widetilde O(d^2 H^2 \sqrt{T})$, where $ d $ indicates the feature dimension, $ H $ refers to the horizon, and $ T $ represents the total number of steps. This is the first frequentist regret analysis of randomized exploration with function approximation, to the best of our knowledge.",1
"Multi-agent reinforcement learning (MARL) achieves significant empirical successes. However, MARL suffers from the curse of many agents. In this paper, we exploit the symmetry of agents in MARL. In the most generic form, we study a mean-field MARL problem. Such a mean-field MARL is defined on mean-field states, which are distributions that are supported on continuous space. Based on the mean embedding of the distributions, we propose MF-FQI algorithm that solves the mean-field MARL and establishes a non-asymptotic analysis for MF-FQI algorithm. We highlight that MF-FQI algorithm enjoys a ""blessing of many agents"" property in the sense that a larger number of observed agents improves the performance of MF-FQI algorithm.",0
"Significant empirical successes have been achieved by Multi-agent reinforcement learning (MARL), but it is afflicted by the curse of many agents. This study utilizes the symmetry of agents in MARL and investigates a mean-field MARL problem in its most generic form. The mean-field MARL is defined on mean-field states, which are distributions supported on continuous space. We propose the MF-FQI algorithm based on the mean embedding of the distributions, which solves the mean-field MARL and establishes a non-asymptotic analysis for it. We emphasize that the MF-FQI algorithm has a ""blessing of many agents"" property, meaning that the performance of the algorithm improves with a larger number of observed agents.",1
"The principle of optimism in the face of uncertainty is prevalent throughout sequential decision making problems such as multi-armed bandits and reinforcement learning (RL), often coming with strong theoretical guarantees. However, it remains a challenge to scale these approaches to the deep RL paradigm, which has achieved a great deal of attention in recent years. In this paper, we introduce a tractable approach to optimism via noise augmented Markov Decision Processes (MDPs), which we show can obtain a competitive regret bound: $\tilde{\mathcal{O}}( |\mathcal{S}|H\sqrt{|\mathcal{S}||\mathcal{A}| T } )$ when augmenting using Gaussian noise, where $T$ is the total number of environment steps. This tractability allows us to apply our approach to the deep RL setting, where we rigorously evaluate the key factors for success of optimistic model-based RL algorithms, bridging the gap between theory and practice.",0
"Optimism is a widely used principle in sequential decision making problems like multi-armed bandits and reinforcement learning (RL), and is accompanied by strong theoretical guarantees. However, implementing these approaches in the deep RL paradigm, which has received significant attention in recent years, is challenging. In this study, we present an achievable approach to optimism through noise augmented Markov Decision Processes (MDPs), which can achieve a competitive regret bound: $\tilde{\mathcal{O}}( |\mathcal{S}|H\sqrt{|\mathcal{S}||\mathcal{A}| T } )$ when Gaussian noise is added, where $T$ represents the total number of environment steps. Our feasible approach lets us apply it to the deep RL setting where we assess the essential factors for the success of optimistic model-based RL algorithms, thus connecting theory and practice.",1
"Recently, several approaches have been proposed to solve language generation problems. Transformer is currently state-of-the-art seq-to-seq model in language generation. Reinforcement Learning (RL) is useful in solving exposure bias and the optimisation on non-differentiable metrics in seq-to-seq language learning. However, Transformer is hard to combine with RL as the costly computing resource is required for sampling. We tackle this problem by proposing an off-policy RL learning algorithm where a behaviour policy represented by GRUs performs the sampling. We reduce the high variance of importance sampling (IS) by applying the truncated relative importance sampling (TRIS) technique and Kullback-Leibler (KL)-control concept. TRIS is a simple yet effective technique, and there is a theoretical proof that KL-control helps to reduce the variance of IS. We formulate this off-policy RL based on self-critical sequence training. Specifically, we use a Transformer-based captioning model as the target policy and use an image-guided language auto-encoder as the behaviour policy to explore the environment. The proposed algorithm achieves state-of-the-art performance on the visual paragraph generation and improved results on image captioning.",0
"Language generation problems have been addressed by various methods, with Transformer currently leading in seq-to-seq language generation. Reinforcement Learning (RL) can aid in handling exposure bias and optimizing non-differentiable metrics in seq-to-seq language learning. However, combining RL with Transformer is challenging due to the high computing resources required for sampling. To overcome this, we present an off-policy RL learning algorithm wherein GRUs represent the behaviour policy for sampling. We mitigate the high variance of importance sampling (IS) by using the truncated relative importance sampling (TRIS) technique and Kullback-Leibler (KL)-control concept. TRIS is a simple yet effective technique, and KL-control has a theoretical proof of reducing IS variance. Our approach is based on self-critical sequence training, where a Transformer-based captioning model serves as the target policy and an image-guided language auto-encoder serves as the behaviour policy for exploring the environment. Our proposed algorithm demonstrates state-of-the-art performance in visual paragraph generation and improved results in image captioning.",1
"We study the effect of impairment on stochastic multi-armed bandits and develop new ways to mitigate it. Impairment effect is the phenomena where an agent only accrues reward for an action if they have played it at least a few times in the recent past. It is practically motivated by repetition and recency effects in domains such as advertising (here consumer behavior may require repeat actions by advertisers) and vocational training (here actions are complex skills that can only be mastered with repetition to get a payoff). Impairment can be naturally modelled as a temporal constraint on the strategy space, and we provide two novel algorithms that achieve sublinear regret, each working with different assumptions on the impairment effect. We introduce a new notion called bucketing in our algorithm design, and show how it can effectively address impairment as well as a broader class of temporal constraints. Our regret bounds explicitly capture the cost of impairment and show that it scales (sub-)linearly with the degree of impairment. Our work complements recent work on modeling delays and corruptions, and we provide experimental evidence supporting our claims.",0
"Our research focuses on examining the impact of impairment on stochastic multi-armed bandits and devising innovative approaches to combat it. Impairment is a phenomenon whereby an agent only receives a reward for an action after playing it several times in the recent past. This is motivated by the need for repetition and recency effects in domains such as advertising, where advertisers may need to repeat their actions to influence consumer behavior, and vocational training, where complex skills require repetition to achieve a payoff. We propose two new algorithms that achieve sublinear regret and address impairment by modeling it as a temporal constraint on the strategy space. Our algorithm design introduces the concept of bucketing, which effectively addresses impairment and other temporal constraints. Our regret bounds explicitly account for the cost of impairment and demonstrate that it scales sub-linearly with the level of impairment. Our work complements recent research on modeling delays and corruptions, and our experimental evidence supports our findings.",1
"Having a perfect model to compute the optimal policy is often infeasible in reinforcement learning. It is important in high-stakes domains to quantify and manage risk induced by model uncertainties. Entropic risk measure is an exponential utility-based convex risk measure that satisfies many reasonable properties. In this paper, we propose an entropic risk constrained policy gradient and actor-critic algorithms that are risk-averse to the model uncertainty. We demonstrate the usefulness of our algorithms on several problem domains.",0
"Reinforcement learning often faces the challenge of not having an ideal model for determining the best policy. In domains where the stakes are high, it becomes crucial to assess and control the risk that arises from uncertain models. One viable solution is to use an entropic risk measure, which is a convex risk measure based on exponential utility and satisfies several justifiable properties. In this paper, we introduce entropic risk-constrained policy gradient and actor-critic algorithms that prioritize risk-aversion towards uncertain models. We illustrate the effectiveness of our algorithms in various problem domains.",1
"In this paper, we consider the problem of online learning of Markov decision processes (MDPs) with very large state spaces. Under the assumptions of realizable function approximation and low Bellman ranks, we develop an online learning algorithm that learns the optimal value function while at the same time achieving very low cumulative regret during the learning process. Our learning algorithm, Adaptive Value-function Elimination (AVE), is inspired by the policy elimination algorithm proposed in (Jiang et al., 2017), known as OLIVE. One of our key technical contributions in AVE is to formulate the elimination steps in OLIVE as contextual bandit problems. This technique enables us to apply the active elimination and expert weighting methods from (Dudik et al., 2011), instead of the random action exploration scheme used in the original OLIVE algorithm, for more efficient exploration and better control of the regret incurred in each policy elimination step. To the best of our knowledge, this is the first $\sqrt{n}$-regret result for reinforcement learning in stochastic MDPs with general value function approximation.",0
"The focus of this paper is on the issue of online learning for Markov decision processes (MDPs) that have large state spaces. By assuming that function approximation is realizable and Bellman ranks are low, we have developed an online learning algorithm named Adaptive Value-function Elimination (AVE) that can learn the optimal value function. The algorithm is designed to ensure the cumulative regret during the learning process is very low. The inspiration for AVE comes from the OLIVE algorithm, which was introduced in a paper by Jiang et al. in 2017. One of the main technical contributions of AVE is the formulation of elimination steps in OLIVE as contextual bandit problems. This allows us to use active elimination and expert weighting methods from a paper by Dudik et al. in 2011. These techniques enable more efficient exploration and better control of the regret incurred in each policy elimination step than the random action exploration scheme used in the original OLIVE algorithm. This paper presents the first $\sqrt{n}$-regret result for reinforcement learning in stochastic MDPs with general value function approximation.",1
"Manipulation tasks such as preparing a meal or assembling furniture remain highly challenging for robotics and vision. Traditional task and motion planning (TAMP) methods can solve complex tasks but require full state observability and are not adapted to dynamic scene changes. Recent learning methods can operate directly on visual inputs but typically require many demonstrations and/or task-specific reward engineering. In this work we aim to overcome previous limitations and propose a reinforcement learning (RL) approach to task planning that learns to combine primitive skills. First, compared to previous learning methods, our approach requires neither intermediate rewards nor complete task demonstrations during training. Second, we demonstrate the versatility of our vision-based task planning in challenging settings with temporary occlusions and dynamic scene changes. Third, we propose an efficient training of basic skills from few synthetic demonstrations by exploring recent CNN architectures and data augmentation. Notably, while all of our policies are learned on visual inputs in simulated environments, we demonstrate the successful transfer and high success rates when applying such policies to manipulation tasks on a real UR5 robotic arm.",0
"The execution of tasks like cooking or furniture assembly is still a major obstacle for robots and vision technology. While traditional task and motion planning (TAMP) techniques can handle intricate tasks, they need complete visibility of the state and cannot adapt to dynamic changes in the scene. Recent learning techniques can operate directly on visual inputs, but they usually necessitate multiple demonstrations and/or specific rewards for each task. Our objective is to overcome these past challenges by introducing a reinforcement learning (RL) strategy to task planning, which learns to merge primitive skills. Our approach does not need intermediate rewards or complete task demonstrations during training, unlike previous learning methods. Furthermore, we exhibit the flexibility of our vision-based task planning in difficult settings with temporary occlusions and dynamic scene changes. Finally, we suggest an efficient training of fundamental skills from a few synthetic demonstrations by exploring recent CNN architectures and data augmentation. Interestingly, even though all of our policies are trained on visual inputs in simulated environments, our method demonstrates a successful transfer and high success rates when applied to manipulation tasks on a real UR5 robotic arm.",1
"We derive and analyze learning algorithms for apprenticeship learning, policy evaluation, and policy gradient for average reward criteria. Existing algorithms explicitly require an upper bound on the mixing time. In contrast, we build on ideas from Markov chain theory and derive sampling algorithms that do not require such an upper bound. For these algorithms, we provide theoretical bounds on their sample-complexity and running time.",0
"Learning algorithms for apprenticeship learning, policy evaluation, and policy gradient for average reward criteria are developed and examined. The current algorithms necessitate an upper limit on the mixing time to operate. In contrast, we utilize concepts from Markov chain theory and establish sampling algorithms that eliminate the need for such an upper limit. We also provide theoretical estimates on the sample-complexity and running time of these algorithms.",1
"Reward-free reinforcement learning (RL) is a framework which is suitable for both the batch RL setting and the setting where there are many reward functions of interest. During the exploration phase, an agent collects samples without using a pre-specified reward function. After the exploration phase, a reward function is given, and the agent uses samples collected during the exploration phase to compute a near-optimal policy. Jin et al. [2020] showed that in the tabular setting, the agent only needs to collect polynomial number of samples (in terms of the number states, the number of actions, and the planning horizon) for reward-free RL. However, in practice, the number of states and actions can be large, and thus function approximation schemes are required for generalization. In this work, we give both positive and negative results for reward-free RL with linear function approximation. We give an algorithm for reward-free RL in the linear Markov decision process setting where both the transition and the reward admit linear representations. The sample complexity of our algorithm is polynomial in the feature dimension and the planning horizon, and is completely independent of the number of states and actions. We further give an exponential lower bound for reward-free RL in the setting where only the optimal $Q$-function admits a linear representation. Our results imply several interesting exponential separations on the sample complexity of reward-free RL.",0
"The framework of reward-free reinforcement learning (RL) is appropriate for both batch RL settings and situations where numerous reward functions are of interest. In the exploration phase, an agent accumulates samples without utilizing a specified reward function. Once the exploration phase concludes, a reward function is provided, and the agent employs the samples collected during the exploration phase to calculate a nearly optimal policy. A study by Jin et al. [2020] indicates that, in the tabular setting, the agent only needs to gather a polynomial number of samples. However, in practical applications, the number of states and actions may be vast, necessitating function approximation methods for generalization. In this study, we present both positive and negative outcomes for reward-free RL with linear function approximation. We introduce an algorithm for reward-free RL in the linear Markov decision process setting where the transition and reward both have linear representations. Our algorithm's sample complexity is polynomial in the feature dimension and the planning horizon and is entirely independent of the number of states and actions. Additionally, we establish an exponential lower bound for reward-free RL in the scenario where only the optimal $Q$-function has a linear representation. Our findings imply several intriguing exponential disparities in the sample complexity of reward-free RL.",1
"Value function approximation has demonstrated phenomenal empirical success in reinforcement learning (RL). Nevertheless, despite a handful of recent progress on developing theory for RL with linear function approximation, the understanding of general function approximation schemes largely remains missing. In this paper, we establish a provably efficient RL algorithm with general value function approximation. We show that if the value functions admit an approximation with a function class $\mathcal{F}$, our algorithm achieves a regret bound of $\widetilde{O}(\mathrm{poly}(dH)\sqrt{T})$ where $d$ is a complexity measure of $\mathcal{F}$ that depends on the eluder dimension [Russo and Van Roy, 2013] and log-covering numbers, $H$ is the planning horizon, and $T$ is the number interactions with the environment. Our theory generalizes recent progress on RL with linear value function approximation and does not make explicit assumptions on the model of the environment. Moreover, our algorithm is model-free and provides a framework to justify the effectiveness of algorithms used in practice.",0
"Although value function approximation has been incredibly successful in reinforcement learning, there is still a lack of understanding about general function approximation schemes, despite recent progress in developing theory for RL with linear function approximation. This paper presents a provably efficient RL algorithm that utilizes general value function approximation. Our algorithm achieves a regret bound of $\widetilde{O}(\mathrm{poly}(dH)\sqrt{T})$ if the value functions can be approximated with a function class $\mathcal{F}$. The complexity measure of $\mathcal{F}$ depends on the eluder dimension and log-covering numbers, while $H$ represents the planning horizon and $T$ is the number of interactions with the environment. Our theory extends recent progress on RL with linear value function approximation and does not require explicit assumptions on the environment's model. Additionally, our algorithm is model-free and provides a framework for justifying the effectiveness of practical algorithms.",1
"Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) are among the most successful policy gradient approaches in deep reinforcement learning (RL). While these methods achieve state-of-the-art performance across a wide range of challenging tasks, there is room for improvement in the stabilization of the policy learning and how the off-policy data are used. In this paper we revisit the theoretical foundations of these algorithms and propose a new algorithm which stabilizes the policy improvement through a proximity term that constrains the discounted state-action visitation distribution induced by consecutive policies to be close to one another. This proximity term, expressed in terms of the divergence between the visitation distributions, is learned in an off-policy and adversarial manner. We empirically show that our proposed method can have a beneficial effect on stability and improve final performance in benchmark high-dimensional control tasks.",0
"TRPO and PPO are two effective policy gradient techniques utilized in deep RL, achieving top-notch performance for challenging tasks. However, there is still room for growth in stabilizing policy learning and utilizing off-policy data. We re-examine the theoretical principles behind these algorithms and introduce a novel approach that enhances policy improvement stability by constraining the discounted state-action visitation distribution between consecutive policies. This proximity term, represented as the divergence between the visitation distributions, is learned through an off-policy and adversarial technique. Our experimental results demonstrate that our proposed method can enhance stability and enhance overall performance for benchmark high-dimensional control tasks.",1
"Stochastic variance-reduced gradient (SVRG) is an optimization method originally designed for tackling machine learning problems with a finite sum structure. SVRG was later shown to work for policy evaluation, a problem in reinforcement learning in which one aims to estimate the value function of a given policy. SVRG makes use of gradient estimates at two scales. At the slower scale, SVRG computes a full gradient over the whole dataset, which could lead to prohibitive computation costs. In this work, we show that two variants of SVRG for policy evaluation could significantly diminish the number of gradient calculations while preserving a linear convergence speed. More importantly, our theoretical result implies that one does not need to use the entire dataset in every epoch of SVRG when it is applied to policy evaluation with linear function approximation. Our experiments demonstrate large computational savings provided by the proposed methods.",0
"Initially designed for addressing finite sum structure machine learning problems, Stochastic variance-reduced gradient (SVRG) has been proven to be effective for policy evaluation in reinforcement learning. SVRG utilizes gradient estimates at two levels, with the slower level computing a full gradient over the dataset, which can result in high computation expenses. In this study, we present two SVRG variants for policy evaluation that significantly reduce the number of gradient calculations while maintaining a linear convergence rate. Our theoretical findings suggest that SVRG does not require the complete dataset in each epoch for policy evaluation with linear function approximation. Our experiments illustrate that the proposed methods offer substantial computational savings.",1
"The technological and scientific challenges involved in the development of autonomous vehicles (AVs) are currently of primary interest for many automobile companies and research labs. However, human-controlled vehicles are likely to remain on the roads for several decades to come and may share with AVs the traffic environments of the future. In such mixed environments, AVs should deploy human-like driving policies and negotiation skills to enable smooth traffic flow. To generate automated human-like driving policies, we introduce a model-free, deep reinforcement learning approach to imitate an experienced human driver's behavior. We study a static obstacle avoidance task on a two-lane highway road in simulation (Unity). Our control algorithm receives a stochastic feedback signal from two sources: a model-driven part, encoding simple driving rules, such as lane-keeping and speed control, and a stochastic, data-driven part, incorporating human expert knowledge from driving data. To assess the similarity between machine and human driving, we model distributions of track position and speed as Gaussian processes. We demonstrate that our approach leads to human-like driving policies.",0
"Many automobile companies and research labs are currently focused on the technological and scientific challenges of developing autonomous vehicles (AVs). However, it is expected that human-controlled vehicles will continue to be present on the roads for several decades, possibly sharing traffic environments with AVs in the future. This makes it crucial for AVs to adopt human-like driving policies and negotiation skills to facilitate smooth traffic flow in mixed environments. To achieve this, we propose a model-free, deep reinforcement learning approach that imitates the behavior of an experienced human driver. Using a simulation (Unity), we study a static obstacle avoidance task on a two-lane highway road. Our control algorithm receives feedback from two sources: a model-driven component that encodes simple driving rules such as lane-keeping and speed control, and a stochastic, data-driven part that incorporates human expert knowledge from driving data. We evaluate the similarity between machine and human driving by modeling distributions of track position and speed as Gaussian processes. Our results show that our approach generates human-like driving policies.",1
"This paper introduces a new scalable multi-objective deep reinforcement learning (MODRL) framework based on deep Q-networks. We develop a high-performance MODRL framework that supports both single-policy and multi-policy strategies, as well as both linear and non-linear approaches to action selection. The experimental results on two benchmark problems (two-objective deep sea treasure environment and three-objective Mountain Car problem) indicate that the proposed framework is able to find the Pareto-optimal solutions effectively. The proposed framework is generic and highly modularized, which allows the integration of different deep reinforcement learning algorithms in different complex problem domains. This therefore overcomes many disadvantages involved with standard multi-objective reinforcement learning methods in the current literature. The proposed framework acts as a testbed platform that accelerates the development of MODRL for solving increasingly complicated multi-objective problems.",0
"In this paper, a novel scalable framework for multi-objective deep reinforcement learning (MODRL) is introduced. The framework is based on deep Q-networks and supports both single-policy and multi-policy strategies, as well as linear and non-linear approaches to action selection. Experimental results on two benchmark problems (two-objective deep sea treasure environment and three-objective Mountain Car problem) demonstrate the effectiveness of the proposed framework in finding Pareto-optimal solutions. The framework is highly modularized and generic, enabling the integration of different deep reinforcement learning algorithms in complex problem domains. This addresses the limitations of standard multi-objective reinforcement learning methods in existing literature. Therefore, the proposed framework serves as a platform for faster development of MODRL to resolve increasingly complex multi-objective problems.",1
"Nowadays, liquid rocket engines use closed-loop control at most near steady operating conditions. The control of the transient phases is traditionally performed in open-loop due to highly nonlinear system dynamics. This situation is unsatisfactory, in particular for reusable engines. The open-loop control system cannot provide optimal engine performance due to external disturbances or the degeneration of engine components over time. In this paper, we study a deep reinforcement learning approach for optimal control of a generic gas-generator engine's continuous start-up phase. It is shown that the learned policy can reach different steady-state operating points and convincingly adapt to changing system parameters. A quantitative comparison with carefully tuned open-loop sequences and PID controllers is included. The deep reinforcement learning controller achieves the highest performance and requires only minimal computational effort to calculate the control action, which is a big advantage over approaches that require online optimization, such as model predictive control. control.",0
"Currently, liquid rocket engines primarily use closed-loop control for steady operating conditions, but transient phases are controlled in open-loop due to the complex system dynamics. This approach is inadequate for reusable engines as external disturbances or component degeneration over time can lead to suboptimal engine performance. This study explores a deep reinforcement learning technique for optimal control of a generic gas-generator engine's continuous start-up phase. Results indicate that the learned policy can adapt to changing system parameters and achieve different steady-state operating points with higher performance than traditional open-loop sequences and PID controllers. Furthermore, the deep reinforcement learning approach requires minimal computational effort compared to online optimization techniques like model predictive control.",1
"Deep reinforcement learning has been applied more and more widely nowadays, especially in various complex control tasks. Effective exploration for noisy networks is one of the most important issues in deep reinforcement learning. Noisy networks tend to produce stable outputs for agents. However, this tendency is not always enough to find a stable policy for an agent, which decreases efficiency and stability during the learning process. Based on NoisyNets, this paper proposes an algorithm called NROWAN-DQN, i.e., Noise Reduction and Online Weight Adjustment NoisyNet-DQN. Firstly, we develop a novel noise reduction method for NoisyNet-DQN to make the agent perform stable actions. Secondly, we design an online weight adjustment strategy for noise reduction, which improves stable performance and gets higher scores for the agent. Finally, we evaluate this algorithm in four standard domains and analyze properties of hyper-parameters. Our results show that NROWAN-DQN outperforms prior algorithms in all these domains. In addition, NROWAN-DQN also shows better stability. The variance of the NROWAN-DQN score is significantly reduced, especially in some action-sensitive environments. This means that in some environments where high stability is required, NROWAN-DQN will be more appropriate than NoisyNets-DQN.",0
"Nowadays, deep reinforcement learning is being widely used in various complex control tasks. An essential issue in this field is the effective exploration for noisy networks, which tend to produce stable outputs for agents. However, this stability is not always enough to find a stable policy, leading to decreased efficiency and stability in the learning process. To address this problem, this paper presents an algorithm called NROWAN-DQN (Noise Reduction and Online Weight Adjustment NoisyNet-DQN) based on NoisyNets. Firstly, a novel noise reduction method is proposed to ensure stable actions. Secondly, an online weight adjustment strategy is designed to further improve performance and obtain higher scores for the agent. The algorithm is evaluated in four standard domains, and the hyper-parameters are analyzed. Our findings indicate that NROWAN-DQN outperforms prior algorithms in all domains and exhibits better stability. In some action-sensitive environments, NROWAN-DQN significantly reduces variance in scores, making it a more suitable option than NoisyNets-DQN.",1
"With the advent of ride-sharing services, there is a huge increase in the number of people who rely on them for various needs. Most of the earlier approaches tackling this issue required handcrafted functions for estimating travel times and passenger waiting times. Traditional Reinforcement Learning (RL) based methods attempting to solve the ridesharing problem are unable to accurately model the complex environment in which taxis operate. Prior Multi-Agent Deep RL based methods based on Independent DQN (IDQN) learn decentralized value functions prone to instability due to the concurrent learning and exploring of multiple agents. Our proposed method based on QMIX is able to achieve centralized training with decentralized execution. We show that our model performs better than the IDQN baseline on a fixed grid size and is able to generalize well to smaller or larger grid sizes. Also, our algorithm is able to outperform IDQN baseline in the scenario where we have a variable number of passengers and cars in each episode. Code for our paper is publicly available at: https://github.com/UMich-ML-Group/RL-Ridesharing.",0
"The popularity of ride-sharing services has led to a significant rise in the number of people who rely on them for various purposes. Previous methods addressing this issue involved creating custom functions to estimate travel and passenger waiting times. However, traditional Reinforcement Learning (RL) techniques used to solve the problem of ride-sharing have been unsuccessful in accurately modeling the intricate environment in which taxis operate. Prior Multi-Agent Deep RL methods relying on Independent DQN (IDQN) have decentralized value functions that are prone to instability due to the concurrent learning and exploration of multiple agents. Our proposed QMIX-based method can achieve centralized training and decentralized execution. We demonstrate that our model performs better than the IDQN baseline on a fixed grid size and has the ability to generalize well to smaller or larger grid sizes. Furthermore, our algorithm can outperform the IDQN baseline in scenarios where the number of passengers and cars varies in each episode. The code for our paper is publicly available at: https://github.com/UMich-ML-Group/RL-Ridesharing.",1
"We study reinforcement learning in continuous state and action spaces endowed with a metric. We provide a refined analysis of the algorithm of Sinclair, Banerjee, and Yu (2019) and show that its regret scales with the \emph{zooming dimension} of the instance. This parameter, which originates in the bandit literature, captures the size of the subsets of near optimal actions and is always smaller than the covering dimension used in previous analyses. As such, our results are the first provably adaptive guarantees for reinforcement learning in metric spaces.",0
"Our focus is on reinforcement learning, particularly in continuous state and action spaces that come with a metric. Our study involves a thorough examination of Sinclair, Banerjee, and Yu's (2019) algorithm. Our findings reveal that the algorithm's regret is proportional to the instance's zooming dimension. This dimension is a parameter derived from the bandit literature that measures the size of the sets of near-optimal actions, and it is consistently smaller than the covering dimension employed in previous analyses. Consequently, our results represent the initial set of reinforcement learning guarantees that are provably adaptive for metric spaces.",1
"Recently, reinforcement learning (RL) has been used as a tool for finding failures in autonomous systems. During execution, the RL agents often rely on some domain-specific heuristic reward to guide them towards finding failures, but constructing such a heuristic may be difficult or infeasible. Without a heuristic, the agent may only receive rewards at the time of failure, or even rewards that guide it away from failures. For example, some approaches give rewards for taking more-likely actions, because we want to find more-likely failures. However, the agent may then learn to only take likely actions, and may not be able to find a failure at all. Consequently, the problem becomes a hard-exploration problem, where rewards do not aid exploration. A new algorithm, go-explore (GE), has recently set new records on benchmarks from the hard-exploration field. We apply GE to adaptive stress testing (AST), one example of an RL-based falsification approach that provides a way to search for the most-likely failure scenario. We simulate a scenario where an autonomous vehicle drives while a pedestrian is crossing the road. We demonstrate that GE is able to find failures without domain-specific heuristics, such as the distance between the car and the pedestrian, on scenarios that other RL techniques are unable to solve. Furthermore, inspired by the robustification phase of GE, we demonstrate that the backwards algorithm (BA) improves the failures found by other RL techniques.",0
"The use of reinforcement learning (RL) to detect failures in autonomous systems has gained recent attention. RL agents often require domain-specific heuristic rewards to guide their search for failures, but creating such heuristics can be challenging. Without a heuristic, the agent may only receive rewards when a failure occurs, or even rewards that lead it away from failures. For instance, some methods give rewards for taking more probable actions, but this can cause the agent to only take likely actions and miss potential failures. This results in a hard-exploration problem where rewards do not assist in exploration. A novel algorithm, go-explore (GE), has achieved remarkable results on hard-exploration benchmarks. We apply GE to adaptive stress testing (AST), an RL-based falsification technique utilized to identify the most probable failure scenario. We simulate an autonomous vehicle driving while a pedestrian crosses the road and demonstrate that GE can detect failures without relying on domain-specific heuristics, such as the distance between the car and the pedestrian. Additionally, we show that the backwards algorithm (BA), inspired by the robustification phase of GE, improves the failures identified by other RL techniques.",1
"In this paper, we propose a distributed zeroth-order policy optimization method for Multi-Agent Reinforcement Learning (MARL). Existing MARL algorithms often assume that every agent can observe the states and actions of all the other agents in the network. This can be impractical in large-scale problems, where sharing the state and action information with multi-hop neighbors may incur significant communication overhead. The advantage of the proposed zeroth-order policy optimization method is that it allows the agents to compute the local policy gradients needed to update their local policy functions using local estimates of the global accumulated rewards that depend on partial state and action information only and can be obtained using consensus. Specifically, to calculate the local policy gradients, we develop a new distributed zeroth-order policy gradient estimator that relies on one-point residual-feedback which, compared to existing zeroth-order estimators that also rely on one-point feedback, significantly reduces the variance of the policy gradient estimates improving, in this way, the learning performance. We show that the proposed distributed zeroth-order policy optimization method with constant stepsize converges to a neighborhood of the global optimal policy that depends on the number of consensus steps used to calculate the local estimates of the global accumulated rewards. Moreover, we provide numerical experiments that demonstrate that our new zeroth-order policy gradient estimator is more sample-efficient compared to other existing one-point estimators.",0
"This paper introduces a method for Multi-Agent Reinforcement Learning (MARL) that focuses on distributed zeroth-order policy optimization. Existing MARL algorithms often assume that agents can observe the actions and states of all other agents in the network, which can be problematic in large-scale problems. The proposed method allows agents to compute local policy gradients using only partial state and action information, obtained through consensus. A new distributed zeroth-order policy gradient estimator is developed, which relies on one-point residual-feedback to reduce variance and improve learning performance. The proposed method converges to a neighborhood of the global optimal policy, and numerical experiments show that it is more sample-efficient than existing one-point estimators.",1
"While recent state-of-the-art results for adversarial imitation-learning algorithms are encouraging, recent works exploring the imitation learning from observation (ILO) setting, where trajectories \textit{only} contain expert observations, have not been met with the same success. Inspired by recent investigations of $f$-divergence manipulation for the standard imitation learning setting(Ke et al., 2019; Ghasemipour et al., 2019), we here examine the extent to which variations in the choice of probabilistic divergence may yield more performant ILO algorithms. We unfortunately find that $f$-divergence minimization through reinforcement learning is susceptible to numerical instabilities. We contribute a reparameterization trick for adversarial imitation learning to alleviate the optimization challenges of the promising $f$-divergence minimization framework. Empirically, we demonstrate that our design choices allow for ILO algorithms that outperform baseline approaches and more closely match expert performance in low-dimensional continuous-control tasks.",0
"Although recent adversarial imitation-learning algorithms have shown promising results, imitation learning from observation (ILO) has not been as successful. Unlike previous works, which focused on expert demonstrations, ILO only considers expert observations. To improve ILO, we investigate the effects of using different probabilistic divergences, building on recent research into $f$-divergence manipulation in standard imitation learning. However, we discover that using reinforcement learning to minimize $f$-divergence is vulnerable to numerical instabilities. To address this issue, we propose a reparameterization trick for adversarial imitation learning that overcomes the optimization challenges of $f$-divergence minimization. We demonstrate through experiments that our approach outperforms baseline methods and is closer to expert performance in low-dimensional continuous-control tasks.",1
"Visual attention serves as a means of feature selection mechanism in the perceptual system. Motivated by Broadbent's leaky filter model of selective attention, we evaluate how such mechanism could be implemented and affect the learning process of deep reinforcement learning. We visualize and analyze the feature maps of DQN on a toy problem Catch, and propose an approach to combine visual selective attention with deep reinforcement learning. We experiment with optical flow-based attention and A2C on Atari games. Experiment results show that visual selective attention could lead to improvements in terms of sample efficiency on tested games. An intriguing relation between attention and batch normalization is also discovered.",0
"In the perceptual system, visual attention functions as a mechanism for selecting features. Broadbent's leaky filter model of selective attention serves as a motivation to assess how this mechanism can impact the learning process of deep reinforcement learning. Our study involves visualizing and analyzing the feature maps of DQN in the Catch toy problem, and proposing an approach to integrate visual selective attention with deep reinforcement learning. We conducted experiments on Atari games using optical flow-based attention and A2C. The results demonstrate that visual selective attention can enhance sample efficiency in the tested games. Furthermore, an interesting correlation between attention and batch normalization was discovered.",1
"As humans, our goals and our environment are persistently changing throughout our lifetime based on our experiences, actions, and internal and external drives. In contrast, typical reinforcement learning problem set-ups consider decision processes that are stationary across episodes. Can we develop reinforcement learning algorithms that can cope with the persistent change in the former, more realistic problem settings? While on-policy algorithms such as policy gradients in principle can be extended to non-stationary settings, the same cannot be said for more efficient off-policy algorithms that replay past experiences when learning. In this work, we formalize this problem setting, and draw upon ideas from the online learning and probabilistic inference literature to derive an off-policy RL algorithm that can reason about and tackle such lifelong non-stationarity. Our method leverages latent variable models to learn a representation of the environment from current and past experiences, and performs off-policy RL with this representation. We further introduce several simulation environments that exhibit lifelong non-stationarity, and empirically find that our approach substantially outperforms approaches that do not reason about environment shift.",0
"Throughout our lifetime, our experiences, actions, and internal and external drives cause persistent change in our goals and environment. However, typical reinforcement learning problems assume decision processes remain stationary across episodes. Can we create reinforcement learning algorithms that adapt to these realistic problem settings? Although policy gradients can be extended to non-stationary settings, the more efficient off-policy algorithms that replay past experiences when learning cannot. In this study, we formalize the problem and draw from the online learning and probabilistic inference literature to develop an off-policy RL algorithm that handles lifelong non-stationarity. Our method employs latent variable models to learn an environment representation from current and past experiences and performs off-policy RL with this representation. We also introduce simulation environments that demonstrate lifelong non-stationarity. We find that our approach outperforms methods that do not consider environment shift.",1
"Policy optimization methods are one of the most widely used classes of Reinforcement Learning (RL) algorithms. Yet, so far, such methods have been mostly analyzed from an optimization perspective, without addressing the problem of exploration, or by making strong assumptions on the interaction with the environment. In this paper we consider model-based RL in the tabular finite-horizon MDP setting with unknown transitions and bandit feedback. For this setting, we propose an optimistic trust region policy optimization (TRPO) algorithm for which we establish $\tilde O(\sqrt{S^2 A H^4 K})$ regret for stochastic rewards. Furthermore, we prove $\tilde O( \sqrt{ S^2 A H^4 } K^{2/3} ) $ regret for adversarial rewards. Interestingly, this result matches previous bounds derived for the bandit feedback case, yet with known transitions. To the best of our knowledge, the two results are the first sub-linear regret bounds obtained for policy optimization algorithms with unknown transitions and bandit feedback.",0
"Although policy optimization methods are a commonly used category of Reinforcement Learning (RL) algorithms, they have primarily been analyzed from an optimization perspective, without addressing the problem of exploration or by making strong assumptions on interaction with the environment. This paper focuses on model-based RL in the tabular finite-horizon MDP setting, with unknown transitions and bandit feedback. The authors present an optimistic trust region policy optimization (TRPO) algorithm, which establishes $\tilde O(\sqrt{S^2 A H^4 K})$ regret for stochastic rewards and $\tilde O( \sqrt{ S^2 A H^4 } K^{2/3} ) $ regret for adversarial rewards. These findings match previous bounds derived for the bandit feedback case, with known transitions, and represent the first sub-linear regret bounds obtained for policy optimization algorithms with unknown transitions and bandit feedback.",1
"To facilitate research in the direction of sample efficient reinforcement learning, we held the MineRL Competition on Sample Efficient Reinforcement Learning Using Human Priors at the Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019). The primary goal of this competition was to promote the development of algorithms that use human demonstrations alongside reinforcement learning to reduce the number of samples needed to solve complex, hierarchical, and sparse environments. We describe the competition, outlining the primary challenge, the competition design, and the resources that we provided to the participants. We provide an overview of the top solutions, each of which use deep reinforcement learning and/or imitation learning. We also discuss the impact of our organizational decisions on the competition and future directions for improvement.",0
"In order to advance research in sample efficient reinforcement learning, the MineRL Competition on Sample Efficient Reinforcement Learning Using Human Priors was conducted at the Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019). The competition aimed to encourage the creation of algorithms that integrate human demonstrations with reinforcement learning to minimize the required number of samples for resolving intricate, hierarchical, and sparse environments. The competition's challenge, design, and resources provided to participants are described. The top solutions, which utilize deep reinforcement learning and/or imitation learning, are also summarized. Additionally, the effects of the organizers' decisions on the competition and future improvements are discussed.",1
"In most of the transfer learning approaches to reinforcement learning (RL) the distribution over the tasks is assumed to be stationary. Therefore, the target and source tasks are i.i.d. samples of the same distribution. In the context of this work, we consider the problem of transferring value functions through a variational method when the distribution that generates the tasks is time-variant, proposing a solution that leverages this temporal structure inherent in the task generating process. Furthermore, by means of a finite-sample analysis, the previously mentioned solution is theoretically compared to its time-invariant version. Finally, we will provide an experimental evaluation of the proposed technique with three distinct temporal dynamics in three different RL environments.",0
"Most approaches to transfer learning in reinforcement learning assume a stationary distribution of tasks, where the target and source tasks are independent and identically distributed samples of the same distribution. However, this work focuses on transferring value functions through a variational method in the presence of a time-variant task-generating distribution. To address this, we propose a solution that utilizes the inherent temporal structure of the task-generating process. Additionally, we conduct a finite-sample analysis to compare the proposed solution with its time-invariant counterpart. Finally, we evaluate the effectiveness of our technique in three different RL environments with distinct temporal dynamics.",1
"Developing mathematical models of dynamic systems is central to many disciplines of engineering and science. Models facilitate simulations, analysis of the system's behavior, decision making and design of automatic control algorithms. Even inherently model-free control techniques such as reinforcement learning (RL) have been shown to benefit from the use of models, typically learned online. Any model construction method must address the tradeoff between the accuracy of the model and its complexity, which is difficult to strike. In this paper, we propose to employ symbolic regression (SR) to construct parsimonious process models described by analytic equations. We have equipped our method with two different state-of-the-art SR algorithms which automatically search for equations that fit the measured data: Single Node Genetic Programming (SNGP) and Multi-Gene Genetic Programming (MGGP). In addition to the standard problem formulation in the state-space domain, we show how the method can also be applied to input-output models of the NARX (nonlinear autoregressive with exogenous input) type. We present the approach on three simulated examples with up to 14-dimensional state space: an inverted pendulum, a mobile robot, and a bipedal walking robot. A comparison with deep neural networks and local linear regression shows that SR in most cases outperforms these commonly used alternative methods. We demonstrate on a real pendulum system that the analytic model found enables a RL controller to successfully perform the swing-up task, based on a model constructed from only 100 data samples.",0
"The creation of mathematical models for dynamic systems is crucial in various fields of science and engineering. Models serve to aid in simulations, analysis of system behavior, decision-making processes, and the design of automatic control algorithms. Even control techniques that do not require models, such as reinforcement learning, have been found to benefit from models that are learned online. However, constructing a model that is both accurate and simple can be challenging. In this study, we propose using symbolic regression to develop parsimonious process models described by analytic equations. Our method utilizes two advanced symbolic regression algorithms, Single Node Genetic Programming and Multi-Gene Genetic Programming, to automatically search for equations that fit the measured data. We demonstrate the effectiveness of our approach on three simulated examples with up to 14-dimensional state space, including an inverted pendulum, a mobile robot, and a bipedal walking robot. Our results show that symbolic regression outperforms commonly used alternative methods such as deep neural networks and local linear regression. We also demonstrate that our analytic model can successfully perform the swing-up task on a real pendulum system, with a model constructed from just 100 data samples.",1
"We consider online learning for episodic stochastically constrained Markov decision processes (CMDP), which plays a central role in ensuring the safety of reinforcement learning. Here the loss function can vary arbitrarily across the episodes, whereas both the loss received and the budget consumption are revealed at the end of each episode. Previous works solve this problem under the restrictive assumption that the transition model of the Markov decision processes (MDP) is known a priori and establish regret bounds that depend polynomially on the cardinalities of the state space $\mathcal{S}$ and the action space $\mathcal{A}$. In this work, we propose a new \emph{upper confidence primal-dual} algorithm, which only requires the trajectories sampled from the transition model. In particular, we prove that the proposed algorithm achieves $\widetilde{\mathcal{O}}(L|\mathcal{S}|\sqrt{|\mathcal{A}|T})$ upper bounds of both the regret and the constraint violation, where $L$ is the length of each episode. Our analysis incorporates a new high-probability drift analysis of Lagrange multiplier processes into the celebrated regret analysis of upper confidence reinforcement learning, which demonstrates the power of ""optimism in the face of uncertainty"" in constrained online learning.",0
"The focus of our study is on online learning for episodic stochastically constrained Markov decision processes (CMDP), which is crucial in ensuring the safety of reinforcement learning. The loss function can vary across episodes, and both the loss received and the budget consumption are disclosed at the end of each episode. Previous approaches solve this problem under the restrictive assumption that the transition model of the Markov decision processes (MDP) is known in advance, and establish regret bounds that depend on the cardinalities of the state space $\mathcal{S}$ and the action space $\mathcal{A}$. In contrast, our proposed algorithm, the upper confidence primal-dual algorithm, only requires trajectories sampled from the transition model. Our analysis proves that our algorithm achieves $\widetilde{\mathcal{O}}(L|\mathcal{S}|\sqrt{|\mathcal{A}|T})$ upper bounds of both the regret and the constraint violation, where $L$ is the length of each episode. Our approach incorporates a new high-probability drift analysis of Lagrange multiplier processes into the celebrated regret analysis of upper confidence reinforcement learning, demonstrating the power of ""optimism in the face of uncertainty"" in constrained online learning.",1
"While the identification of nonlinear dynamical systems is a fundamental building block of model-based reinforcement learning and feedback control, its sample complexity is only understood for systems that either have discrete states and actions or for systems that can be identified from data generated by i.i.d. random inputs. Nonetheless, many interesting dynamical systems have continuous states and actions and can only be identified through a judicious choice of inputs. Motivated by practical settings, we study a class of nonlinear dynamical systems whose state transitions depend linearly on a known feature embedding of state-action pairs. To estimate such systems in finite time identification methods must explore all directions in feature space. We propose an active learning approach that achieves this by repeating three steps: trajectory planning, trajectory tracking, and re-estimation of the system from all available data. We show that our method estimates nonlinear dynamical systems at a parametric rate, similar to the statistical rate of standard linear regression.",0
"The identification of nonlinear dynamical systems is crucial for model-based reinforcement learning and feedback control. However, the understanding of its sample complexity is limited to systems with discrete states and actions or those that can be identified from i.i.d. random inputs. Nonetheless, many interesting systems have continuous states and actions and require a careful selection of inputs for identification. To address this practical challenge, we investigate a class of nonlinear dynamical systems that have linearly dependent state transitions on a feature embedding of state-action pairs. Identifying such systems in finite time requires exploring all directions in feature space. Our proposed active learning approach achieves this by iterating three steps: trajectory planning, trajectory tracking, and system re-estimation from available data. We demonstrate that our method estimates nonlinear dynamical systems at a parametric rate, similar to standard linear regression's statistical rate.",1
"The overestimation phenomenon caused by function approximation is a well-known issue in value-based reinforcement learning algorithms such as deep Q-networks and DDPG, which could lead to suboptimal policies. To address this issue, TD3 takes the minimum value between a pair of critics, which introduces underestimation bias. By unifying these two opposites, we propose a novel Weighted Delayed Deep Deterministic Policy Gradient algorithm, which can reduce the estimation error and further improve the performance by weighting a pair of critics. We compare the learning process of value function between DDPG, TD3, and our proposed algorithm, which verifies that our algorithm could indeed eliminate the estimation error of value function. We evaluate our algorithm in the OpenAI Gym continuous control tasks, outperforming the state-of-the-art algorithms on every environment tested.",0
"Value-based reinforcement learning algorithms like deep Q-networks and DDPG often suffer from the overestimation phenomenon caused by function approximation, resulting in suboptimal policies. To combat this, TD3 uses a pair of critics to take the minimum value, but this introduces underestimation bias. Our proposed Weighted Delayed Deep Deterministic Policy Gradient algorithm unifies these opposites by weighting a pair of critics to reduce estimation error and improve performance. We compared our algorithm to DDPG and TD3 and found that it successfully eliminates the estimation error in the value function. We tested our algorithm in OpenAI Gym's continuous control tasks and found that it outperforms state-of-the-art algorithms in every environment.",1
"We solve active target tracking, one of the essential tasks in autonomous systems, using a deep reinforcement learning (RL) approach. In this problem, an autonomous agent is tasked with acquiring information about targets of interests using its onboard sensors. The classical challenges in this problem are system model dependence and the difficulty of computing information-theoretic cost functions for a long planning horizon. RL provides solutions for these challenges as the length of its effective planning horizon does not affect the computational complexity, and it drops the strong dependency of an algorithm on system models. In particular, we introduce Active Tracking Target Network (ATTN), a unified RL policy that is capable of solving major sub-tasks of active target tracking -- in-sight tracking, navigation, and exploration. The policy shows robust behavior for tracking agile and anomalous targets with a partially known target model. Additionally, the same policy is able to navigate in obstacle environments to reach distant targets as well as explore the environment when targets are positioned in unexpected locations.",0
"Using a deep reinforcement learning (RL) approach, we address the crucial task of active target tracking in autonomous systems. The aim is for an autonomous agent to gather information about targets of interest using its onboard sensors. This problem poses challenges, including system model dependence and the difficulty of computing information-theoretic cost functions for a long planning horizon. RL offers solutions for these challenges by being unaffected by the effective planning horizon's length and reducing an algorithm's strong dependency on system models. To address the sub-tasks of active target tracking, including in-sight tracking, navigation, and exploration, we introduce the Active Tracking Target Network (ATTN), a unified RL policy. The policy displays robust behavior in tracking agile and anomalous targets, even with a partially known target model. Moreover, the same policy can navigate obstacle environments to reach distant targets and explore the environment when targets are located unexpectedly.",1
"Intelligent manipulation benefits from the capacity to flexibly control an end-effector with high degrees of freedom (DoF) and dynamically react to the environment. However, due to the challenges of collecting effective training data and learning efficiently, most grasping algorithms today are limited to top-down movements and open-loop execution. In this work, we propose a new low-cost hardware interface for collecting grasping demonstrations by people in diverse environments. Leveraging this data, we show that it is possible to train a robust end-to-end 6DoF closed-loop grasping model with reinforcement learning that transfers to real robots. A key aspect of our grasping model is that it uses ""action-view"" based rendering to simulate future states with respect to different possible actions. By evaluating these states using a learned value function (Q-function), our method is able to better select corresponding actions that maximize total rewards (i.e., grasping success). Our final grasping system is able to achieve reliable 6DoF closed-loop grasping of novel objects across various scene configurations, as well as dynamic scenes with moving objects.",0
"Having the ability to control an end-effector with high degrees of freedom and respond to the environment in a dynamic way is crucial for successful intelligent manipulation. However, due to the difficulties in obtaining effective training data and learning efficiently, most grasping algorithms are limited to open-loop execution and top-down movements. To address this issue, a low-cost hardware interface is proposed in this study to collect grasping demonstrations by individuals in diverse settings. By utilizing this data, a robust end-to-end 6DoF closed-loop grasping model is trained with reinforcement learning that can be transferred to real robots. The grasping model employs ""action-view"" based rendering to simulate possible future states and uses a learned value function to evaluate these states, selecting corresponding actions that maximize total rewards (i.e., grasping success). This grasping system is capable of achieving reliable 6DoF closed-loop grasping of novel objects in various scene configurations, including dynamic scenes with moving objects.",1
"We consider the recently proposed reinforcement learning (RL) framework of Contextual Markov Decision Processes (CMDP), where the agent interacts with a (potentially adversarial) sequence of episodic tabular MDPs. In addition, a context vector determining the MDP parameters is available to the agent at the start of each episode, thereby allowing it to learn a context-dependent near-optimal policy. In this paper, we propose a no-regret online RL algorithm in the setting where the MDP parameters are obtained from the context using generalized linear mappings (GLMs). We propose and analyze optimistic and randomized exploration methods which make (time and space) efficient online updates. The GLM based model subsumes previous work in this area and also improves previous known bounds in the special case where the contextual mapping is linear. In addition, we demonstrate a generic template to derive confidence sets using an online learning oracle and give a lower bound for the setting.",0
"The paper discusses the Contextual Markov Decision Processes (CMDP) model of reinforcement learning (RL), where the agent interacts with a series of potential adversarial tabular MDPs. At the beginning of each episode, a context vector is provided to the agent, allowing it to learn a context-dependent optimal policy. The study proposes an online RL algorithm that operates under the condition that the MDP parameters are obtained using generalized linear mappings (GLMs). The optimistic and randomized exploration methods presented enable efficient online updates, and the GLM-based model improves previously known bounds, including in the linear contextual mapping scenario. The study also provides a generic template for deriving confidence sets using an online learning oracle and a lower bound for the setting.",1
"Sequence generation models are commonly refined with reinforcement learning over user-defined metrics. However, high gradient variance hinders the practical use of this method. To stabilize this method, we adapt to contextual generation of categorical sequences a policy gradient estimator, which evaluates a set of correlated Monte Carlo (MC) rollouts for variance control. Due to the correlation, the number of unique rollouts is random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and hence are combined to effectively reduce gradient variance. We also demonstrate the use of correlated MC rollouts for binary-tree softmax models, which reduce the high generation cost in large vocabulary scenarios by decomposing each categorical action into a sequence of binary actions. We evaluate our methods on both neural program synthesis and image captioning. The proposed methods yield lower gradient variance and consistent improvement over related baselines.",0
"Reinforcement learning is a common method used to enhance sequence generation models, based on user-defined metrics. However, the high variance that occurs due to steep gradients can make this approach impractical. To address this issue, we propose a policy gradient estimator for contextual generation of categorical sequences, which uses correlated Monte Carlo rollouts to control variance. The number of unique rollouts is adaptable to the model's uncertainty, and since they are correlated, they act as baselines for one another and effectively reduce gradient variance. We also apply this technique to binary-tree softmax models, which can reduce generation costs in large vocabulary scenarios by breaking down categorical actions into a sequence of binary actions. We evaluate our approach on neural program synthesis and image captioning and observe consistently improved results with lower gradient variance compared to related baselines.",1
"Currently, deep reinforcement learning (RL) shows impressive results in complex gaming and robotic environments. Often these results are achieved at the expense of huge computational costs and require an incredible number of episodes of interaction between the agent and the environment. There are two main approaches to improving the sample efficiency of reinforcement learning methods - using hierarchical methods and expert demonstrations. In this paper, we propose a combination of these approaches that allow the agent to use low-quality demonstrations in complex vision-based environments with multiple related goals. Our forgetful experience replay (ForgER) algorithm effectively handles errors in expert data and reduces quality losses when adapting the action space and states representation to the agent's capabilities. Our proposed goal-oriented structuring of replay buffer allows the agent to automatically highlight sub-goals for solving complex hierarchical tasks in demonstrations. Our method is universal and can be integrated into various off-policy methods. It surpasses all known existing state-of-the-art RL methods using expert demonstrations on various model environments. The solution based on our algorithm beats all the solutions for the famous MineRL competition and allows the agent to mine a diamond in the Minecraft environment.",0
"At present, deep reinforcement learning (RL) exhibits remarkable outcomes in intricate gaming and robotic settings. However, these outcomes often come at a high computational cost and necessitate an enormous number of agent-environment interaction episodes. Two primary approaches to enhancing the sample efficiency of reinforcement learning techniques are hierarchical techniques and expert demonstrations. In this study, we suggest a fusion of these techniques that enables the agent to utilize low-quality demonstrations in complex vision-based environments with various related goals. Our algorithm, named forgetful experience replay (ForgER), handles errors in expert data and minimizes quality losses while adapting the action space and state representation to the agent's abilities. We structure the replay buffer in a goal-oriented manner, which aids the agent in identifying sub-goals for tackling complex hierarchical tasks in demonstrations. Our technique is versatile and can be integrated into different off-policy methods. It outperforms all known state-of-the-art RL methods that use expert demonstrations in various model environments. Our algorithm-based solution surpasses all other solutions in the renowned MineRL competition and empowers the agent to mine a diamond in the Minecraft environment.",1
"Biopharmaceutical manufacturing faces critical challenges, including complexity, high variability, lengthy lead time, and limited historical data and knowledge of the underlying system stochastic process. To address these challenges, we propose a green simulation assisted model-based reinforcement learning to support process online learning and guide dynamic decision making. Basically, the process model risk is quantified by the posterior distribution. At any given policy, we predict the expected system response with prediction risk accounting for both inherent stochastic uncertainty and model risk. Then, we propose green simulation assisted reinforcement learning and derive the mixture proposal distribution of decision process and likelihood ratio based metamodel for the policy gradient, which can selectively reuse process trajectory outputs collected from previous experiments to increase the simulation data-efficiency, improve the policy gradient estimation accuracy, and speed up the search for the optimal policy. Our numerical study indicates that the proposed approach demonstrates the promising performance.",0
"There are significant challenges that biopharmaceutical manufacturing faces, such as complexity, high variability, lengthy lead times, and limited historical data and knowledge of the underlying system stochastic process. To overcome these obstacles, we suggest applying a green simulation assisted model-based reinforcement learning approach that would aid in online learning of the process and support dynamic decision making. The model's risk is measured by the posterior distribution, and we estimate the expected system response and prediction risk at any given policy, taking into account both inherent stochastic uncertainty and model risk. We then propose a green simulation assisted reinforcement learning technique that employs a mixture proposal distribution of decision process and likelihood ratio-based metamodel for policy gradient. This approach selectively reuses process trajectory outputs from previous experiments to increase simulation data efficiency, enhance policy gradient estimation accuracy, and speed up the search for the optimal policy. Our numerical study shows that the proposed approach is highly effective.",1
"Off-policy reinforcement learning algorithms promise to be applicable in settings where only a fixed data-set (batch) of environment interactions is available and no new experience can be acquired. This property makes these algorithms appealing for real world problems such as robot control. In practice, however, standard off-policy algorithms fail in the batch setting for continuous control. In this paper, we propose a simple solution to this problem. It admits the use of data generated by arbitrary behavior policies and uses a learned prior -- the advantage-weighted behavior model (ABM) -- to bias the RL policy towards actions that have previously been executed and are likely to be successful on the new task. Our method can be seen as an extension of recent work on batch-RL that enables stable learning from conflicting data-sources. We find improvements on competitive baselines in a variety of RL tasks -- including standard continuous control benchmarks and multi-task learning for simulated and real-world robots.",0
"Reinforcement learning algorithms that operate off-policy are suitable for scenarios where there is a fixed data-set (batch) of environment interactions available, and no new experience can be gained. These algorithms offer an attractive solution for real-world problems, such as robot control. However, in continuous control, the standard off-policy algorithms fail in the batch setting. In this paper, we present a straightforward solution to this problem. Our proposed method allows the use of data generated by any behavior policy and utilizes a learned prior, the advantage-weighted behavior model (ABM), to influence the RL policy towards actions that have been executed previously and are likely to be successful in the new task. Our approach is an extension of recent batch-RL work, which enables stable learning from conflicting data-sources. We observed improvements on various RL tasks, including standard continuous control benchmarks and multi-task learning for both simulated and real-world robots, surpassing competitive baselines.",1
"Continually solving new, unsolved tasks is the key to learning diverse behaviors. Through reinforcement learning (RL), we have made massive strides towards solving tasks that have a single goal. However, in the multi-task domain, where an agent needs to reach multiple goals, the choice of training goals can largely affect sample efficiency. When biological agents learn, there is often an organized and meaningful order to which learning happens. Inspired by this, we propose setting up an automatic curriculum for goals that the agent needs to solve. Our key insight is that if we can sample goals at the frontier of the set of goals that an agent is able to reach, it will provide a significantly stronger learning signal compared to randomly sampled goals. To operationalize this idea, we introduce a goal proposal module that prioritizes goals that maximize the epistemic uncertainty of the Q-function of the policy. This simple technique samples goals that are neither too hard nor too easy for the agent to solve, hence enabling continual improvement. We evaluate our method across 13 multi-goal robotic tasks and 5 navigation tasks, and demonstrate performance gains over current state-of-the-art methods.",0
"Learning diverse behaviors is achieved by continuously solving new and unsolved tasks. Although reinforcement learning (RL) has made remarkable progress in solving tasks with a single goal, selecting training goals can significantly affect sample efficiency in the multi-task domain, where an agent must achieve multiple objectives. In biological agents, learning occurs in an organized and meaningful order. To emulate this, we propose establishing an automatic curriculum for the agent's goals. Our approach is to sample goals at the frontier of the set of goals that the agent can reach to provide a more robust learning signal than randomly sampled goals. We introduce a goal proposal module that prioritizes goals that maximize the epistemic uncertainty of the policy's Q-function. This technique samples goals that are neither too easy nor too challenging for the agent to solve, resulting in continual improvement. We tested our method on 13 multi-goal robotic tasks and 5 navigation tasks, and it outperformed current state-of-the-art methods.",1
"In e-commerce markets, on time delivery is of great importance to customer satisfaction. In this paper, we present a Deep Reinforcement Learning (DRL) approach for deciding how and when orders should be batched and picked in a warehouse to minimize the number of tardy orders. In particular, the technique facilitates making decisions on whether an order should be picked individually (pick-by-order) or picked in a batch with other orders (pick-by-batch), and if so with which other orders. We approach the problem by formulating it as a semi-Markov decision process and develop a vector-based state representation that includes the characteristics of the warehouse system. This allows us to create a deep reinforcement learning solution that learns a strategy by interacting with the environment and solve the problem with a proximal policy optimization algorithm. We evaluate the performance of the proposed DRL approach by comparing it with several batching and sequencing heuristics in different problem settings. The results show that the DRL approach is able to develop a strategy that produces consistent, good solutions and performs better than the proposed heuristics.",0
"The timely delivery of orders is crucial for customer satisfaction in e-commerce markets. This study introduces a Deep Reinforcement Learning (DRL) approach to determine the optimal time and method for batching and picking orders in a warehouse to minimize tardiness. Our method involves deciding whether to pick orders individually or in a batch with other orders, and if so, which orders to select. We formulate the problem as a semi-Markov decision process and use a vector-based state representation that considers warehouse system characteristics. With a proximal policy optimization algorithm, our DRL solution learns a strategy by interacting with the environment. We compare the performance of our DRL approach with various batching and sequencing heuristics in diverse problem settings. Results indicate that our DRL approach produces consistent, superior solutions and outperforms the proposed heuristics.",1
"Efficient exploration is one of the main challenges in reinforcement learning (RL). Most existing sample-efficient algorithms assume the existence of a single reward function during exploration. In many practical scenarios, however, there is not a single underlying reward function to guide the exploration, for instance, when an agent needs to learn many skills simultaneously, or multiple conflicting objectives need to be balanced. To address these challenges, we propose the \textit{task-agnostic RL} framework: In the exploration phase, the agent first collects trajectories by exploring the MDP without the guidance of a reward function. After exploration, it aims at finding near-optimal policies for $N$ tasks, given the collected trajectories augmented with \textit{sampled rewards} for each task. We present an efficient task-agnostic RL algorithm, \textsc{UCBZero}, that finds $\epsilon$-optimal policies for $N$ arbitrary tasks after at most $\tilde O(\log(N)H^5SA/\epsilon^2)$ exploration episodes. We also provide an $\Omega(\log (N)H^2SA/\epsilon^2)$ lower bound, showing that the $\log$ dependency on $N$ is unavoidable. Furthermore, we provide an $N$-independent sample complexity bound of \textsc{UCBZero} in the statistically easier setting when the ground truth reward functions are known.",0
"Reinforcement learning (RL) faces the challenge of efficient exploration, which is often tackled by assuming the existence of a single reward function. However, practical scenarios may involve learning multiple skills simultaneously or balancing conflicting objectives, making it difficult to rely on a single reward function for exploration. To address this issue, we propose the task-agnostic RL framework, wherein an agent explores the MDP without guidance from a reward function, collects trajectories, and then finds near-optimal policies for N tasks using sampled rewards. We introduce the UCBZero algorithm that efficiently finds epsilon-optimal policies for N tasks after exploration episodes. Our algorithm has a log(N) dependency, which is unavoidable, and an N-independent sample complexity bound in the statistically easier setting when the ground truth reward functions are known.",1
"Model-based reinforcement learning (MBRL) has shown its advantages in sample-efficiency over model-free reinforcement learning (MFRL). Despite the impressive results it achieves, it still faces a trade-off between the ease of data generation and model bias. In this paper, we propose a simple and elegant model-embedding model-based reinforcement learning (MEMB) algorithm in the framework of the probabilistic reinforcement learning. To balance the sample-efficiency and model bias, we exploit both real and imaginary data in the training. In particular, we embed the model in the policy update and learn $Q$ and $V$ functions from the real data set. We provide the theoretical analysis of MEMB with the Lipschitz continuity assumption on the model and policy. At last, we evaluate MEMB on several benchmarks and demonstrate our algorithm can achieve state-of-the-art performance.",0
"MBRL has demonstrated superior sample-efficiency compared to MFRL, but it still faces a challenge: finding a balance between ease of data generation and model bias. Our proposed MEMB algorithm, which is part of the probabilistic reinforcement learning framework, provides a simple and elegant solution to this problem. By incorporating both real and imaginary data in the training process and embedding the model in the policy update, we are able to balance sample-efficiency and model bias. The MEMB algorithm is theoretically analyzed with the Lipschitz continuity assumption on the model and policy. We evaluate the performance of MEMB on several benchmarks and demonstrate that it outperforms other algorithms.",1
"We study the problem of least squares linear regression where the data-points are dependent and are sampled from a Markov chain. We establish sharp information theoretic minimax lower bounds for this problem in terms of $\tau_{\mathsf{mix}}$, the mixing time of the underlying Markov chain, under different noise settings. Our results establish that in general, optimization with Markovian data is strictly harder than optimization with independent data and a trivial algorithm (SGD-DD) that works with only one in every $\tilde{\Theta}(\tau_{\mathsf{mix}})$ samples, which are approximately independent, is minimax optimal. In fact, it is strictly better than the popular Stochastic Gradient Descent (SGD) method with constant step-size which is otherwise minimax optimal in the regression with independent data setting.   Beyond a worst case analysis, we investigate whether structured datasets seen in practice such as Gaussian auto-regressive dynamics can admit more efficient optimization schemes. Surprisingly, even in this specific and natural setting, Stochastic Gradient Descent (SGD) with constant step-size is still no better than SGD-DD. Instead, we propose an algorithm based on experience replay--a popular reinforcement learning technique--that achieves a significantly better error rate. Our improved rate serves as one of the first results where an algorithm outperforms SGD-DD on an interesting Markov chain and also provides one of the first theoretical analyses to support the use of experience replay in practice.",0
"The problem of least squares linear regression is studied when the data-points are dependent and are sampled from a Markov chain. The research establishes lower bounds for this problem using information theory, with $\tau_{\mathsf{mix}}$, the mixing time of the underlying Markov chain, being a key factor under different noise settings. The results show that optimization with Markovian data is more challenging than optimization with independent data. A simple algorithm called SGD-DD, which operates with only one in every $\tilde{\Theta}(\tau_{\mathsf{mix}})$ samples, is demonstrated to be minimax optimal. In comparison, popular methods such as Stochastic Gradient Descent (SGD) with constant step-size, which is otherwise minimax optimal in the regression with independent data setting, perform worse. The study also investigates whether structured datasets such as Gaussian auto-regressive dynamics can lead to more efficient optimization schemes. The research reveals that even in this specific setting, SGD with constant step-size is no better than SGD-DD. However, an algorithm based on experience replay, a reinforcement learning technique, is proposed and found to achieve a significantly better error rate. This algorithm is among the first to outperform SGD-DD on an interesting Markov chain and provides theoretical support for the use of experience replay in practice.",1
"Temporal-Difference (TD) learning is a standard and very successful reinforcement learning approach, at the core of both algorithms that learn the value of a given policy, as well as algorithms which learn how to improve policies. TD-learning with eligibility traces provides a way to do temporal credit assignment, i.e. decide which portion of a reward should be assigned to predecessor states that occurred at different previous times, controlled by a parameter $\lambda$. However, tuning this parameter can be time-consuming, and not tuning it can lead to inefficient learning. To improve the sample efficiency of TD-learning, we propose a meta-learning method for adjusting the eligibility trace parameter, in a state-dependent manner. The adaptation is achieved with the help of auxiliary learners that learn distributional information about the update targets online, incurring roughly the same computational complexity per step as the usual value learner. Our approach can be used both in on-policy and off-policy learning. We prove that, under some assumptions, the proposed method improves the overall quality of the update targets, by minimizing the overall target error. This method can be viewed as a plugin which can also be used to assist prediction with function approximation by meta-learning feature (observation)-based $\lambda$ online, or even in the control case to assist policy improvement. Our empirical evaluation demonstrates significant performance improvements, as well as improved robustness of the proposed algorithm to learning rate variation.",0
"Temporal-Difference (TD) learning is a widely used and effective approach for reinforcement learning. It is used in algorithms that learn the value of a policy and those that improve policies. TD-learning with eligibility traces allows for temporal credit assignment, which assigns portions of a reward to predecessor states at different times using a parameter called $\lambda$. However, tuning this parameter can be time-consuming and not tuning it can lead to inefficient learning. To overcome this, we propose a meta-learning method that adjusts the eligibility trace parameter in a state-dependent manner, which improves sample efficiency. We use auxiliary learners to learn distributional information about the update targets online, with similar computational complexity as the value learner. Our method can be used in on-policy and off-policy learning, and we prove that it minimizes the overall target error and improves learning. This method can also be used for prediction with function approximation and even in control cases to assist policy improvement. Our experiments show significant performance improvements and increased robustness to learning rate variation.",1
"Reinforcement learning algorithms can acquire policies for complex tasks autonomously. However, the number of samples required to learn a diverse set of skills can be prohibitively large. While meta-reinforcement learning methods have enabled agents to leverage prior experience to adapt quickly to new tasks, their performance depends crucially on how close the new task is to the previously experienced tasks. Current approaches are either not able to extrapolate well, or can do so at the expense of requiring extremely large amounts of data for on-policy meta-training. In this work, we present model identification and experience relabeling (MIER), a meta-reinforcement learning algorithm that is both efficient and extrapolates well when faced with out-of-distribution tasks at test time. Our method is based on a simple insight: we recognize that dynamics models can be adapted efficiently and consistently with off-policy data, more easily than policies and value functions. These dynamics models can then be used to continue training policies and value functions for out-of-distribution tasks without using meta-reinforcement learning at all, by generating synthetic experience for the new task.",0
"Autonomous policies for complex tasks can be acquired through reinforcement learning algorithms. However, the considerable number of samples required for learning a diverse set of skills can be a hindrance. Although meta-reinforcement learning methods can help agents adapt quickly to new tasks by using prior experience, their effectiveness depends on the similarity of the new task to the previous ones. Current approaches either have poor extrapolation capabilities or require excessive amounts of data for on-policy meta-training. This study introduces model identification and experience relabeling (MIER), a meta-reinforcement learning algorithm that is efficient and capable of extrapolating well when dealing with out-of-distribution tasks during testing. Our approach is based on the idea that dynamics models can be adapted easily and consistently with off-policy data as compared to policies and value functions. These dynamics models can then be utilized to continue training policies and value functions for out-of-distribution tasks by producing synthetic experience for the new task without the need for meta-reinforcement learning.",1
"Actor-critic (AC) methods have exhibited great empirical success compared with other reinforcement learning algorithms, where the actor uses the policy gradient to improve the learning policy and the critic uses temporal difference learning to estimate the policy gradient. Under the two time-scale learning rate schedule, the asymptotic convergence of AC has been well studied in the literature. However, the non-asymptotic convergence and finite sample complexity of actor-critic methods are largely open. In this work, we provide a non-asymptotic analysis for two time-scale actor-critic methods under non-i.i.d. setting. We prove that the actor-critic method is guaranteed to find a first-order stationary point (i.e., $\|\nabla J(\boldsymbol{\theta})\|_2^2 \le \epsilon$) of the non-concave performance function $J(\boldsymbol{\theta})$, with $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ sample complexity. To the best of our knowledge, this is the first work providing finite-time analysis and sample complexity bound for two time-scale actor-critic methods.",0
"Compared to other reinforcement learning algorithms, Actor-critic (AC) methods have proven to be highly effective in practice. The actor improves the learning policy through policy gradient, while the critic uses temporal difference learning to estimate the policy gradient. While the asymptotic convergence of AC has been widely studied in the literature under the two time-scale learning rate schedule, the non-asymptotic convergence and finite sample complexity of actor-critic methods remain largely unexplored. This paper offers a non-asymptotic analysis of two time-scale actor-critic methods under non-i.i.d. settings. Our analysis demonstrates that the actor-critic method can find a first-order stationary point of the non-concave performance function with a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.5})$, guaranteeing $\|\nabla J(\boldsymbol{\theta})\|_2^2 \le \epsilon$. To our knowledge, this is the first research to provide a finite-time analysis and sample complexity bound for two time-scale actor-critic methods.",1
"The high sample complexity of reinforcement learning challenges its use in practice. A promising approach is to quickly adapt pre-trained policies to new environments. Existing methods for this policy adaptation problem typically rely on domain randomization and meta-learning, by sampling from some distribution of target environments during pre-training, and thus face difficulty on out-of-distribution target environments. We propose new model-based mechanisms that are able to make online adaptation in unseen target environments, by combining ideas from no-regret online learning and adaptive control. We prove that the approach learns policies in the target environment that can quickly recover trajectories from the source environment, and establish the rate of convergence in general settings. We demonstrate the benefits of our approach for policy adaptation in a diverse set of continuous control tasks, achieving the performance of state-of-the-art methods with much lower sample complexity.",0
"The practical use of reinforcement learning is hindered by its requirement for a large number of samples. To address this issue, a potential solution is to adapt pre-trained policies to new environments rapidly. However, current methods for this policy adaptation problem rely on domain randomization and meta-learning, which involve sampling from a distribution of target environments during pre-training and struggle with out-of-distribution target environments. To combat this, we propose new model-based mechanisms that can perform online adaptation in previously unseen target environments by incorporating concepts from no-regret online learning and adaptive control. Our approach is able to learn policies in the target environment that recover trajectories from the source environment quickly, and we establish the rate of convergence in general settings. Through a variety of continuous control tasks, we demonstrate the advantages of our approach for policy adaptation, achieving state-of-the-art results with a significantly lower sample complexity.",1
"When multiple agents learn in a decentralized manner, the environment appears non-stationary from the perspective of an individual agent due to the exploration and learning of the other agents. Recently proposed deep multi-agent reinforcement learning methods have tried to mitigate this non-stationarity by attempting to determine which samples are from other agent exploration or suboptimality and take them less into account during learning. Based on the same philosophy, this paper introduces a decentralized quantile estimator, which aims to improve performance by distinguishing non-stationary samples based on the likelihood of returns. In particular, each agent considers the likelihood that other agent exploration and policy changes are occurring, essentially utilizing the agent's own estimations to weigh the learning rate that should be applied towards the given samples. We introduce a formal method of calculating differences of our return distribution representations and methods for utilizing it to guide updates. We also explore the effect of risk-seeking strategies for adjusting learning over time and propose adaptive risk distortion functions which guides risk sensitivity. Our experiments, on traditional benchmarks and new domains, show our methods are more stable, sample efficient and more likely to converge to a joint optimal policy than previous methods.",0
"Decentralized learning by multiple agents results in a non-stationary environment from the perspective of individual agents due to exploration and learning by other agents. To address this issue, recent deep multi-agent reinforcement learning methods have attempted to identify samples from other agent exploration or suboptimality and reduce their impact during learning. Building on this approach, this study proposes a decentralized quantile estimator that distinguishes non-stationary samples based on the likelihood of returns to improve performance. Each agent assesses the likelihood of other agent exploration and policy changes, using its own estimations to determine the learning rate applied to the given samples. This study introduces a formal method of calculating differences in return distribution representations and methods for utilizing it to guide updates. Additionally, the study explores the effect of risk-seeking strategies in adjusting learning over time and proposes adaptive risk distortion functions to guide risk sensitivity. The experiments conducted on traditional benchmarks and new domains indicate that the proposed methods are more stable, sample efficient, and more likely to converge to a joint optimal policy than previous methods.",1
"Hard visual attention is a promising approach to reduce the computational burden of modern computer vision methodologies. Hard attention mechanisms are typically non-differentiable. They can be trained with reinforcement learning but the high-variance training this entails hinders more widespread application. We show how hard attention for image classification can be framed as a Bayesian optimal experimental design (BOED) problem. From this perspective, the optimal locations to attend to are those which provide the greatest expected reduction in the entropy of the classification distribution. We introduce methodology from the BOED literature to approximate this optimal behaviour, and use it to generate `near-optimal' sequences of attention locations. We then show how to use such sequences to partially supervise, and therefore speed up, the training of a hard attention mechanism. Although generating these sequences is computationally expensive, they can be reused by any other networks later trained on the same task.",0
"Utilizing hard visual attention is a promising approach to alleviate the computational load of modern computer vision methodologies. However, hard attention mechanisms are usually non-differentiable and can only be trained with reinforcement learning, which has a high-variance training process that limits its widespread application. In this study, we present a way to frame hard attention for image classification as a Bayesian optimal experimental design (BOED) problem. The optimal locations to focus on are those that can provide the greatest expected reduction in the entropy of the classification distribution. To achieve this, we employ methodology from the BOED literature to approximate optimal behavior and generate ""near-optimal"" sequences of attention locations. These sequences can be used to partially supervise and expedite the training of a hard attention mechanism. Although producing these sequences is computationally expensive, they can be reused by other networks trained on the same task later on.",1
"Adversarial attacks pose significant challenges for detecting adversarial attacks at an early stage. We propose attack-agnostic detection on reinforcement learning-based interactive recommendation systems. We first craft adversarial examples to show their diverse distributions and then augment recommendation systems by detecting potential attacks with a deep learning-based classifier based on the crafted data. Finally, we study the attack strength and frequency of adversarial examples and evaluate our model on standard datasets with multiple crafting methods. Our extensive experiments show that most adversarial attacks are effective, and both attack strength and attack frequency impact the attack performance. The strategically-timed attack achieves comparative attack performance with only 1/3 to 1/2 attack frequency. Besides, our black-box detector trained with one crafting method has the generalization ability over several crafting methods.",0
"Detecting adversarial attacks early on presents significant challenges. Our proposal involves attack-agnostic detection on interactive recommendation systems based on reinforcement learning. We demonstrate the diverse distributions of adversarial examples and then use a deep learning-based classifier to detect potential attacks. Our model is evaluated on standard datasets using multiple crafting methods, which reveal that most adversarial attacks are effective, with attack frequency and strength impacting their performance. A strategically-timed attack can achieve comparable results with less frequency. Additionally, our black-box detector can generalize across several crafting methods, even with just one method used for training.",1
"Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), as the widely employed policy based reinforcement learning (RL) methods, are prone to converge to a sub-optimal solution as they limit the policy representation to a particular parametric distribution class. To address this issue, we develop an innovative Optimistic Distributionally Robust Policy Optimization (ODRPO) algorithm, which effectively utilizes Optimistic Distributionally Robust Optimization (DRO) approach to solve the trust region constrained optimization problem without parameterizing the policies. Our algorithm improves TRPO and PPO with a higher sample efficiency and a better performance of the final policy while attaining the learning stability. Moreover, it achieves a globally optimal policy update that is not promised in the prevailing policy based RL algorithms. Experiments across tabular domains and robotic locomotion tasks demonstrate the effectiveness of our approach.",0
"The policy-based reinforcement learning methods, Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), have a tendency to reach sub-optimal solutions because they limit policy representation to a specific parametric distribution class. To overcome this limitation, we have created a new algorithm called Optimistic Distributionally Robust Policy Optimization (ODRPO) that employs the Optimistic Distributionally Robust Optimization (DRO) approach to solve the trust region constrained optimization problem. Unlike TRPO and PPO, ODRPO does not require policies to be parameterized, which leads to increased sample efficiency and improved final policy performance with learning stability. Additionally, our algorithm achieves globally optimal policy updates, which other policy-based RL methods cannot guarantee. We have demonstrated the effectiveness of our approach through experiments on tabular domains and robotic locomotion tasks.",1
"Reinforcement learning has achieved great success in various applications. To learn an effective policy for the agent, it usually requires a huge amount of data by interacting with the environment, which could be computational costly and time consuming. To overcome this challenge, the framework called Reinforcement Learning with Expert Demonstrations (RLED) was proposed to exploit the supervision from expert demonstrations. Although the RLED methods can reduce the number of learning iterations, they usually assume the demonstrations are perfect, and thus may be seriously misled by the noisy demonstrations in real applications. In this paper, we propose a novel framework to adaptively learn the policy by jointly interacting with the environment and exploiting the expert demonstrations. Specifically, for each step of the demonstration trajectory, we form an instance, and define a joint loss function to simultaneously maximize the expected reward and minimize the difference between agent behaviors and demonstrations. Most importantly, by calculating the expected gain of the value function, we assign each instance with a weight to estimate its potential utility, and thus can emphasize the more helpful demonstrations while filter out noisy ones. Experimental results in various environments with multiple popular reinforcement learning algorithms show that the proposed approach can learn robustly with noisy demonstrations, and achieve higher performance in fewer iterations.",0
"Reinforcement learning has been successful in various applications, but it often requires a large amount of data to learn an effective policy, which can be computationally expensive and time-consuming. To address this issue, the Reinforcement Learning with Expert Demonstrations (RLED) framework was proposed to leverage expert supervision. However, RLED methods assume perfect demonstrations, which can mislead the agent in real applications. In this study, we propose a novel framework that adaptively learns the policy by interacting with the environment and utilizing expert demonstrations. We create an instance for each step of the demonstration trajectory and define a joint loss function to maximize the expected reward and minimize the difference between agent behaviors and demonstrations. We assign each instance a weight based on the expected gain of the value function to estimate its potential utility and emphasize helpful demonstrations while filtering out noisy ones. Our experimental results demonstrate that our approach can learn robustly with noisy demonstrations and achieve higher performance in fewer iterations across various environments using popular reinforcement learning algorithms.",1
"Conventional Reinforcement Learning (RL) algorithms usually have one single agent learning to solve the task independently. As a result, the agent can only explore a limited part of the state-action space while the learned behavior is highly correlated to the agent's previous experience, making the training prone to a local minimum. In this work, we empower RL with the capability of teamwork and propose a novel non-local policy optimization framework called Diversity-regularized Collaborative Exploration (DiCE). DiCE utilizes a group of heterogeneous agents to explore the environment simultaneously and share the collected experiences. A regularization mechanism is further designed to maintain the diversity of the team and modulate the exploration. We implement the framework in both on-policy and off-policy settings and the experimental results show that DiCE can achieve substantial improvement over the baselines in the MuJoCo locomotion tasks.",0
"Typically, traditional Reinforcement Learning (RL) algorithms involve a single agent working independently to solve a task. This approach limits the agent's ability to explore the state-action space, leading to learned behavior that is highly correlated with the agent's past experience and making the training process susceptible to local minimums. Our research introduces a new approach to RL, called Diversity-regularized Collaborative Exploration (DiCE), which enables teamwork and utilizes a group of diverse agents to explore the environment together and share their experiences. Additionally, we have designed a regularization mechanism to ensure team diversity and modulate exploration. We have implemented DiCE in both on-policy and off-policy settings and have observed substantial improvements over the baselines in the MuJoCo locomotion tasks.",1
"Reinforcement learning (RL) is a general framework for adaptive control, which has proven to be efficient in many domains, e.g., board games, video games or autonomous vehicles. In such problems, an agent faces a sequential decision-making problem where, at every time step, it observes its state, performs an action, receives a reward and moves to a new state. An RL agent learns by trial and error a good policy (or controller) based on observations and numeric reward feedback on the previously performed action. In this chapter, we present the basic framework of RL and recall the two main families of approaches that have been developed to learn a good policy. The first one, which is value-based, consists in estimating the value of an optimal policy, value from which a policy can be recovered, while the other, called policy search, directly works in a policy space. Actor-critic methods can be seen as a policy search technique where the policy value that is learned guides the policy improvement. Besides, we give an overview of some extensions of the standard RL framework, notably when risk-averse behavior needs to be taken into account or when rewards are not available or not known.",0
"RL is a widely applicable adaptive control framework that has demonstrated its effectiveness in a variety of domains, including board games, video games, and autonomous vehicles. In these scenarios, an agent is faced with a sequential decision-making challenge, whereby it observes its state, takes action, receives a reward, and transitions to a new state at each time step. Through trial and error, an RL agent learns a good policy by leveraging feedback on the rewards obtained from previous actions. In this chapter, we outline the fundamental framework of RL and describe the two main approaches for learning a good policy. The first, referred to as value-based, involves estimating the optimal policy value, from which a policy can be derived. The second, known as policy search, operates directly in the policy space. Actor-critic methods can be viewed as a policy search technique, where the learned policy value guides policy improvement. Additionally, we provide a summary of some extensions to the standard RL framework, such as those that address risk-averse behavior or unknown or unavailable rewards.",1
"Several works have addressed the problem of incorporating constraints in the reinforcement learning (RL) framework, however majority of them can only guarantee the satisfaction of soft constraints. In this work, we address the problem of satisfying hard state constraints in a model-free RL setting with the deterministic system dynamics. The proposed algorithm is developed for the discrete state and action space and utilizes a multi-class support vector machine (SVM) to represent the policy. The state constraints are incorporated in the SVM optimization framework to derive an analytical solution for determining the policy parameters. This final policy converges to a solution which is guaranteed to satisfy the constraints. Additionally, the proposed formulation adheres to the Q-learning framework and thus, also guarantees convergence to the optimal solution. The algorithm is demonstrated with multiple example problems.",0
"Numerous studies have tackled the challenge of integrating constraints within the reinforcement learning framework. However, most of these studies can only ensure the fulfillment of soft constraints. This paper focuses on the attainment of hard state constraints in a model-free RL setup with deterministic system dynamics. The algorithm proposed herein is developed for a discrete state and action space and employs a multi-class support vector machine (SVM) to represent the policy. The SVM optimization framework accommodates the state constraints, resulting in an analytical solution for the policy parameters. The final policy converges to a solution that guarantees constraint satisfaction. Moreover, the suggested formulation adheres to the Q-learning framework, ensuring convergence to the optimal solution. The algorithm's effectiveness is demonstrated through various example problems.",1
"In reinforcement learning, an agent learns to reach a set of goals by means of an external reward signal. In the natural world, intelligent organisms learn from internal drives, bypassing the need for external signals, which is beneficial for a wide range of tasks. Motivated by this observation, we propose to formulate an intrinsic objective as the mutual information between the goal states and the controllable states. This objective encourages the agent to take control of its environment. Subsequently, we derive a surrogate objective of the proposed reward function, which can be optimized efficiently. Lastly, we evaluate the developed framework in different robotic manipulation and navigation tasks and demonstrate the efficacy of our approach. A video showing experimental results is available at https://youtu.be/CT4CKMWBYz0",0
"The concept of reinforcement learning involves an agent acquiring the ability to achieve a specific set of objectives through an external reward system. However, in nature, intelligent organisms are capable of learning from their own internal drives, thereby eliminating the need for external signals. This is useful for a multitude of tasks. Building on this notion, we suggest creating an intrinsic objective that measures the mutual information between the controllable states and the goal states. This objective encourages the agent to take control of its surroundings. We then establish a replacement objective for the proposed reward function, which can be optimized with ease. Finally, we evaluate our framework on various robotic navigation and manipulation scenarios and exhibit the effectiveness of our approach in a video available at https://youtu.be/CT4CKMWBYz0.",1
"We introduce an algorithm for model-based hierarchical reinforcement learning to acquire self-contained transition and reward models suitable for probabilistic planning at multiple levels of abstraction. We call this framework Planning with Abstract Learned Models (PALM). By representing subtasks symbolically using a new formal structure, the lifted abstract Markov decision process (L-AMDP), PALM learns models that are independent and modular. Through our experiments, we show how PALM integrates planning and execution, facilitating a rapid and efficient learning of abstract, hierarchical models. We also demonstrate the increased potential for learned models to be transferred to new and related tasks.",0
"Our article presents an algorithm that enables model-based hierarchical reinforcement learning. Its objective is to acquire independent and modular transition and reward models, which can be used for probabilistic planning at multiple levels of abstraction. This framework is called Planning with Abstract Learned Models (PALM). By utilizing a novel formal structure to represent subtasks, the lifted abstract Markov decision process (L-AMDP), PALM learns models that are suitable for planning and execution. Our experiments demonstrate that PALM facilitates fast and efficient learning of abstract, hierarchical models. Furthermore, we showcase the potential for learned models to be transferred to different and related tasks.",1
"Molecular geometry prediction of flexible molecules, or conformer search, is a long-standing challenge in computational chemistry. This task is of great importance for predicting structure-activity relationships for a wide variety of substances ranging from biomolecules to ubiquitous materials. Substantial computational resources are invested in Monte Carlo and Molecular Dynamics methods to generate diverse and representative conformer sets for medium to large molecules, which are yet intractable to chemoinformatic conformer search methods. We present TorsionNet, an efficient sequential conformer search technique based on reinforcement learning under the rigid rotor approximation. The model is trained via curriculum learning, whose theoretical benefit is explored in detail, to maximize a novel metric grounded in thermodynamics called the Gibbs Score. Our experimental results show that TorsionNet outperforms the highest scoring chemoinformatics method by 4x on large branched alkanes, and by several orders of magnitude on the previously unexplored biopolymer lignin, with applications in renewable energy.",0
"The computational chemistry field has long struggled with predicting molecular geometry for flexible molecules, also known as conformer search. This task is crucial in determining the structure-activity relationships of various substances, including biomolecules and common materials. Unfortunately, medium to large molecules require significant computational resources for Monte Carlo and Molecular Dynamics methods to generate diverse conformer sets. Chemoinformatic conformer search methods are not useful in these cases. To combat this issue, we introduce TorsionNet. This sequential conformer search technique uses reinforcement learning under the rigid rotor approximation and is efficient and effective. We use curriculum learning to train the model to optimize a novel metric called the Gibbs Score, which is based on thermodynamics. Our results show that TorsionNet performs better than the top-performing chemoinformatics method by four times for large branched alkanes and significantly better for lignin, a previously unexplored biopolymer with applications in renewable energy.",1
"In this paper, we propose SAMBA, a novel framework for safe reinforcement learning that combines aspects from probabilistic modelling, information theory, and statistics. Our method builds upon PILCO to enable active exploration using novel(semi-)metrics for out-of-sample Gaussian process evaluation optimised through a multi-objective problem that supports conditional-value-at-risk constraints. We evaluate our algorithm on a variety of safe dynamical system benchmarks involving both low and high-dimensional state representations. Our results show orders of magnitude reductions in samples and violations compared to state-of-the-art methods. Lastly, we provide intuition as to the effectiveness of the framework by a detailed analysis of our active metrics and safety constraints.",0
"SAMBA is a newly proposed framework for reinforcement learning that prioritizes safety. It incorporates elements from probabilistic modeling, information theory, and statistics. Our approach expands on PILCO by introducing unique metrics for out-of-sample Gaussian process evaluation that are optimized through a multi-objective problem. These metrics support conditional-value-at-risk constraints and facilitate active exploration. We tested our algorithm on different safe dynamical system benchmarks, with both low and high-dimensional state representations. Our findings indicate significant improvements in sample reduction and violations compared to other methods. Our study also includes an in-depth examination of our active metrics and safety constraints to explain the framework's effectiveness.",1
"In the real world, linguistic agents are also embodied agents: they perceive and act in the physical world. The notion of Language Grounding questions the interactions between language and embodiment: how do learning agents connect or ground linguistic representations to the physical world ? This question has recently been approached by the Reinforcement Learning community under the framework of instruction-following agents. In these agents, behavioral policies or reward functions are conditioned on the embedding of an instruction expressed in natural language. This paper proposes another approach: using language to condition goal generators. Given any goal-conditioned policy, one could train a language-conditioned goal generator to generate language-agnostic goals for the agent. This method allows to decouple sensorimotor learning from language acquisition and enable agents to demonstrate a diversity of behaviors for any given instruction. We propose a particular instantiation of this approach and demonstrate its benefits.",0
"In the physical world, linguistic agents are embodied and interact with their surroundings. The concept of Language Grounding explores the relationship between language and embodiment, specifically how learning agents connect linguistic representations to their physical environment. Recently, the Reinforcement Learning community has investigated this question through instruction-following agents, where language is used to condition behavioral policies or reward functions. However, this paper proposes an alternative approach: using language to condition goal generators. By training a language-conditioned goal generator for any goal-conditioned policy, agents can demonstrate a variety of behaviors for a given instruction, effectively separating sensorimotor learning from language acquisition. This paper presents a specific implementation of this approach and showcases its advantages.",1
"Deep reinforcement learning (RL) algorithms have achieved great success on a wide variety of sequential decision-making tasks. However, many of these algorithms suffer from high sample complexity when learning from scratch using environmental rewards, due to issues such as credit-assignment and high-variance gradients, among others. Transfer learning, in which knowledge gained on a source task is applied to more efficiently learn a different but related target task, is a promising approach to improve the sample complexity in RL. Prior work has considered using pre-trained teacher policies to enhance the learning of the student policy, albeit with the constraint that the teacher and the student MDPs share the state-space or the action-space. In this paper, we propose a new framework for transfer learning where the teacher and the student can have arbitrarily different state- and action-spaces. To handle this mismatch, we produce embeddings which can systematically extract knowledge from the teacher policy and value networks, and blend it into the student networks. To train the embeddings, we use a task-aligned loss and show that the representations could be enriched further by adding a mutual information loss. Using a set of challenging simulated robotic locomotion tasks involving many-legged centipedes, we demonstrate successful transfer learning in situations when the teacher and student have different state- and action-spaces.",0
"Sequential decision-making tasks have been successfully tackled by deep RL algorithms, but they often struggle with high sample complexity when learning from scratch due to issues such as credit-assignment and high-variance gradients. Transfer learning offers a promising solution by applying knowledge gained from a source task to a related target task. Previous work has used pre-trained teacher policies to enhance student policy learning, but with the restriction that the teacher and student MDPs share state and/or action spaces. This paper proposes a new framework for transfer learning where the teacher and student can have different state and action spaces, using embeddings to extract knowledge from the teacher policy and value networks and blend it into the student networks. The embeddings are trained using a task-aligned loss and can be further enriched with a mutual information loss. The success of this approach is demonstrated in challenging simulated robotic locomotion tasks involving many-legged centipedes.",1
"The real-time strategy game of StarCraft II has been posed as a challenge for reinforcement learning by Google's DeepMind. This study examines the use of an agent based on the Monte-Carlo Tree Search algorithm for optimizing the build order in StarCraft II, and discusses how its performance can be improved even further by combining it with a deep reinforcement learning neural network. The experimental results accomplished using Monte-Carlo Tree Search achieves a score similar to a novice human player by only using very limited time and computational resources, which paves the way to achieving scores comparable to those of a human expert by combining it with the use of deep reinforcement learning.",0
"Google's DeepMind has taken on the challenge of applying reinforcement learning to the real-time strategy game, StarCraft II. In this study, an agent that utilizes the Monte-Carlo Tree Search algorithm to optimize the game's build order is explored. The study also discusses the potential for further improvement by combining the agent with a deep reinforcement learning neural network. The results of the experiment demonstrate that Monte-Carlo Tree Search can achieve a score comparable to that of a novice human player with limited time and computational resources. By incorporating deep reinforcement learning, it may be possible to achieve scores on par with those of a human expert.",1
"Q-learning with value function approximation may have the poor performance because of overestimation bias and imprecise estimate. Specifically, overestimation bias is from the maximum operator over noise estimate, which is exaggerated using the estimate of a subsequent state. Inspired by the recent advance of deep reinforcement learning and Double Q-learning, we introduce the decorrelated double Q-learning (D2Q). Specifically, we introduce the decorrelated regularization item to reduce the correlation between value function approximators, which can lead to less biased estimation and low variance. The experimental results on a suite of MuJoCo continuous control tasks demonstrate that our decorrelated double Q-learning can effectively improve the performance.",0
"The performance of Q-learning with value function approximation may suffer due to issues such as overestimation bias and imprecise estimates. Overestimation bias arises from the maximum operator over noise estimate, which is further amplified by the estimate of a subsequent state. To tackle this problem, we have drawn inspiration from recent developments in deep reinforcement learning and Double Q-learning, and have introduced a new method called decorrelated double Q-learning (D2Q). Our approach involves adding a decorrelated regularization item to reduce the correlation between value function approximators, leading to less biased estimation and lower variance. Our experiments on a range of MuJoCo continuous control tasks demonstrate that our decorrelated double Q-learning is highly effective in improving performance.",1
"Optimization of parameterized policies for reinforcement learning (RL) is an important and challenging problem in artificial intelligence. Among the most common approaches are algorithms based on gradient ascent of a score function representing discounted return. In this paper, we examine the role of these policy gradient and actor-critic algorithms in partially-observable multiagent environments. We show several candidate policy update rules and relate them to a foundation of regret minimization and multiagent learning techniques for the one-shot and tabular cases, leading to previously unknown convergence guarantees. We apply our method to model-free multiagent reinforcement learning in adversarial sequential decision problems (zero-sum imperfect information games), using RL-style function approximation. We evaluate on commonly used benchmark Poker domains, showing performance against fixed policies and empirical convergence to approximate Nash equilibria in self-play with rates similar to or better than a baseline model-free algorithm for zero sum games, without any domain-specific state space reductions.",0
"The problem of optimizing parameterized policies for reinforcement learning (RL) is challenging and significant within the field of artificial intelligence. Gradient ascent algorithms based on score functions that represent discounted return are commonly employed in this area. In this study, we investigate the application of policy gradient and actor-critic algorithms in partially observable multiagent environments. We explore several policy update rules and their connection to regret minimization and multiagent learning techniques in the one-shot and tabular cases. As a result, we reveal previously unknown convergence guarantees. Our approach is used for model-free multiagent reinforcement learning in adversarial sequential decision problems, specifically zero-sum imperfect information games, using RL-style function approximation. We test our method on the popular Poker benchmark domains, comparing performance against fixed policies and demonstrating convergence to approximate Nash equilibria in self-play at rates equal to or better than a baseline model-free algorithm for zero-sum games. This is achieved without any domain-specific state space reductions.",1
"Deep Reinforcement Learning (DRL) has shown impressive performance on domains with visual inputs, in particular various games. However, the agent is usually trained on a fixed environment, e.g. a fixed number of levels. A growing mass of evidence suggests that these trained models fail to generalize to even slight variations of the environments they were trained on. This paper advances the hypothesis that the lack of generalization is partly due to the input representation, and explores how rotation, cropping and translation could increase generality. We show that a cropped, translated and rotated observation can get better generalization on unseen levels of two-dimensional arcade games from the GVGAI framework. The generality of the agents is evaluated on both human-designed and procedurally generated levels.",0
"DRL has demonstrated remarkable performance in games that require visual inputs. However, the agent is typically trained on a fixed environment, which may include a limited number of levels. Recent research indicates that the trained models struggle to adapt to even minor variations in the environment they were trained on. This study proposes that the inadequate generalization is partially due to the input representation and examines how rotation, cropping, and translation can enhance generality. Our results demonstrate that a cropped, translated, and rotated observation can improve generalization in two-dimensional arcade games from the GVGAI framework, even in novel levels created by humans or procedurally generated.",1
"In this paper, we consider the problem of actor-critic reinforcement learning. Firstly, we extend the actor-critic architecture to actor-critic-N architecture by introducing more critics beyond rewards. Secondly, we combine the reward-based critic with a potential-field-based critic to formulate the proposed potential field guided actor-critic reinforcement learning approach (actor-critic-2). This can be seen as a combination of the model-based gradients and the model-free gradients in policy improvement. State with large potential field often contains a strong prior information, such as pointing to the target at a long distance or avoiding collision by the side of an obstacle. In this situation, we should trust potential-field-based critic more as policy evaluation to accelerate policy improvement, where action policy tends to be guided. For example, in practical application, learning to avoid obstacles should be guided rather than learned by trial and error. State with small potential filed is often lack of information, for example, at the local minimum point or around the moving target. At this time, we should trust reward-based critic as policy evaluation more to evaluate the long-term return. In this case, action policy tends to explore. In addition, potential field evaluation can be combined with planning to estimate a better state value function. In this way, reward design can focus more on the final stage of reward, rather than reward shaping or phased reward. Furthermore, potential field evaluation can make up for the lack of communication in multi-agent cooperation problem, i.e., multi-agent each has a reward-based critic and a relative unified potential-field-based critic with prior information. Thirdly, simplified experiments on predator-prey game demonstrate the effectiveness of the proposed approach.",0
"The paper discusses the actor-critic reinforcement learning problem and proposes an extension to the actor-critic architecture by introducing additional critics beyond rewards. This new approach, called potential field guided actor-critic reinforcement learning (actor-critic-2), combines a reward-based critic with a potential-field-based critic to improve policy evaluation. The potential field-based critic is particularly useful in states with large potential fields, where it provides strong prior information to guide action policy. On the other hand, the reward-based critic is more useful in states with small potential fields, where it can better evaluate long-term returns and encourage exploration. The proposed approach also allows for better reward design and can be applied to multi-agent cooperation problems. Experiments on a predator-prey game demonstrate the effectiveness of the approach.",1
"Combinatorial optimization algorithms for graph problems are usually designed afresh for each new problem with careful attention by an expert to the problem structure. In this work, we develop a new framework to solve any combinatorial optimization problem over graphs that can be formulated as a single player game defined by states, actions, and rewards, including minimum spanning tree, shortest paths, traveling salesman problem, and vehicle routing problem, without expert knowledge. Our method trains a graph neural network using reinforcement learning on an unlabeled training set of graphs. The trained network then outputs approximate solutions to new graph instances in linear running time. In contrast, previous approximation algorithms or heuristics tailored to NP-hard problems on graphs generally have at least quadratic running time. We demonstrate the applicability of our approach on both polynomial and NP-hard problems with optimality gaps close to 1, and show that our method is able to generalize well: (i) from training on small graphs to testing on large graphs; (ii) from training on random graphs of one type to testing on random graphs of another type; and (iii) from training on random graphs to running on real world graphs.",0
"Traditionally, experts design combinatorial optimization algorithms for graph problems on a case-by-case basis, taking into account the unique structure of each problem. However, our research introduces a novel approach that can solve a variety of such problems, such as minimum spanning tree, shortest paths, traveling salesman problem, and vehicle routing problem, using a single player game framework. Our method employs a graph neural network trained via reinforcement learning on an unlabeled set of graphs, enabling it to produce quick and accurate solutions to new graph instances. This is in contrast to previous methods, which were tailored to specific NP-hard problems and had at least quadratic running time. Our approach is demonstrated to be effective in solving both polynomial and NP-hard problems, with optimality gaps close to 1, and it generalizes well across various scenarios, including training on small graphs and testing on large ones, training on one type of random graph and testing on another, and even training on random graphs and testing on real-world ones.",1
"In this paper, we demonstrate how to do automated theorem proving in the presence of a large knowledge base of potential premises without learning from human proofs. We suggest an exploration mechanism that mixes in additional premises selected by a tf-idf (term frequency-inverse document frequency) based lookup in a deep reinforcement learning scenario. This helps with exploring and learning which premises are relevant for proving a new theorem. Our experiments show that the theorem prover trained with this exploration mechanism outperforms provers that are trained only on human proofs. It approaches the performance of a prover trained by a combination of imitation and reinforcement learning. We perform multiple experiments to understand the importance of the underlying assumptions that make our exploration approach work, thus explaining our design choices.",0
This paper presents an approach to automated theorem proving that does not rely on human proofs and is capable of handling a large knowledge base of potential premises. The approach involves using a tf-idf based lookup to select additional premises in a deep reinforcement learning scenario. This helps the system learn and explore which premises are relevant for proving a new theorem. The experiments conducted indicate that this approach outperforms systems that rely solely on human proofs and is comparable to systems that use a combination of imitation and reinforcement learning. The paper also discusses the underlying assumptions that enable this approach to work and explains the design choices made.,1
"We consider the challenge of automated steering angle prediction for self driving cars using egocentric road images. In this work, we explore the use of feudal networks, used in hierarchical reinforcement learning (HRL), to devise a vehicle agent to predict steering angles from first person, dash-cam images of the Udacity driving dataset. Our method, Feudal Steering, is inspired by recent work in HRL consisting of a manager network and a worker network that operate on different temporal scales and have different goals. The manager works at a temporal scale that is relatively coarse compared to the worker and has a higher level, task-oriented goal space. Using feudal learning to divide the task into manager and worker sub-networks provides more accurate and robust prediction. Temporal abstraction in driving allows more complex primitives than the steering angle at a single time instance. Composite actions comprise a subroutine or skill that can be re-used throughout the driving sequence. The associated subroutine id is the manager network's goal, so that the manager seeks to succeed at the high level task (e.g. a sharp right turn, a slight right turn, moving straight in traffic, or moving straight unencumbered by traffic). The steering angle at a particular time instance is the worker network output which is regulated by the manager's high level task. We demonstrate state-of-the art steering angle prediction results on the Udacity dataset.",0
"The focus of our study is on automating the prediction of steering angles for self-driving cars by using images of the road from the perspective of the car. We investigate the applicability of feudal networks, which are typically used in hierarchical reinforcement learning, to create a vehicle agent that can predict steering angles based on dash-cam images from the Udacity driving dataset. Our approach, called Feudal Steering, is inspired by recent work in hierarchical reinforcement learning that involves a manager network and a worker network with different temporal scales and goals. By breaking down the task into sub-networks, we are able to achieve more accurate and reliable predictions. Our approach also allows for more complex primitives than just a single steering angle at a given time instance, as we can use composite actions that can be re-used throughout the driving sequence. The manager network sets high-level goals, while the worker network outputs the steering angle at a specific time instance. Our method outperforms other existing methods in predicting steering angles on the Udacity dataset.",1
"Generalization across environments is critical to the successful application of reinforcement learning algorithms to real-world challenges. In this paper, we consider the problem of learning abstractions that generalize in block MDPs, families of environments with a shared latent state space and dynamics structure over that latent space, but varying observations. We leverage tools from causal inference to propose a method of invariant prediction to learn model-irrelevance state abstractions (MISA) that generalize to novel observations in the multi-environment setting. We prove that for certain classes of environments, this approach outputs with high probability a state abstraction corresponding to the causal feature set with respect to the return. We further provide more general bounds on model error and generalization error in the multi-environment setting, in the process showing a connection between causal variable selection and the state abstraction framework for MDPs. We give empirical evidence that our methods work in both linear and nonlinear settings, attaining improved generalization over single- and multi-task baselines.",0
"For reinforcement learning algorithms to be effectively applied to real-world challenges, it is crucial to have generalization across environments. This paper focuses on the problem of learning abstractions that can generalize in block MDPs. These are families of environments that share a latent state space and dynamics structure but have varying observations. To achieve this, we use causal inference tools to propose a method of invariant prediction, which helps us learn model-irrelevance state abstractions (MISA) that can generalize to new observations in a multi-environment setting. Our approach outputs a state abstraction that corresponds to the causal feature set with respect to the return, with high probability for certain classes of environments. We also provide general bounds on model error and generalization error in the multi-environment setting, and show a connection between causal variable selection and the state abstraction framework for MDPs. Our methods work well in both linear and nonlinear settings, and we have empirical evidence of improved generalization over single- and multi-task baselines.",1
"Many cooperative multiagent reinforcement learning environments provide agents with a sparse team-based reward, as well as a dense agent-specific reward that incentivizes learning basic skills. Training policies solely on the team-based reward is often difficult due to its sparsity. Furthermore, relying solely on the agent-specific reward is sub-optimal because it usually does not capture the team coordination objective. A common approach is to use reward shaping to construct a proxy reward by combining the individual rewards. However, this requires manual tuning for each environment. We introduce Multiagent Evolutionary Reinforcement Learning (MERL), a split-level training platform that handles the two objectives separately through two optimization processes. An evolutionary algorithm maximizes the sparse team-based objective through neuroevolution on a population of teams. Concurrently, a gradient-based optimizer trains policies to only maximize the dense agent-specific rewards. The gradient-based policies are periodically added to the evolutionary population as a way of information transfer between the two optimization processes. This enables the evolutionary algorithm to use skills learned via the agent-specific rewards toward optimizing the global objective. Results demonstrate that MERL significantly outperforms state-of-the-art methods, such as MADDPG, on a number of difficult coordination benchmarks.",0
"In many cooperative multiagent reinforcement learning situations, agents are rewarded with a sparse team-based reward in addition to a dense agent-specific reward that encourages the learning of essential skills. However, relying solely on the team-based reward is challenging due to its scarcity, while relying solely on the agent-specific reward is not optimal since it does not always capture the team coordination objective. One common solution is to create a proxy reward by combining individual rewards using reward shaping, but this requires manual tuning for each environment. We propose Multiagent Evolutionary Reinforcement Learning (MERL), a training platform that separates the two objectives into two optimization processes. A neuroevolution algorithm maximizes the sparse team-based objective by evolving a population of teams, while a gradient-based optimizer trains policies to maximize the dense agent-specific rewards. The gradient-based policies are periodically added to the evolutionary population, facilitating information transfer between the two processes, allowing the evolutionary algorithm to optimize the global objective using skills acquired via the agent-specific rewards. Our results demonstrate that MERL outperforms existing approaches, such as MADDPG, on several challenging coordination benchmarks.",1
"In session-based or sequential recommendation, it is important to consider a number of factors like long-term user engagement, multiple types of user-item interactions such as clicks, purchases etc. The current state-of-the-art supervised approaches fail to model them appropriately. Casting sequential recommendation task as a reinforcement learning (RL) problem is a promising direction. A major component of RL approaches is to train the agent through interactions with the environment. However, it is often problematic to train a recommender in an on-line fashion due to the requirement to expose users to irrelevant recommendations. As a result, learning the policy from logged implicit feedback is of vital importance, which is challenging due to the pure off-policy setting and lack of negative rewards (feedback). In this paper, we propose self-supervised reinforcement learning for sequential recommendation tasks. Our approach augments standard recommendation models with two output layers: one for self-supervised learning and the other for RL. The RL part acts as a regularizer to drive the supervised layer focusing on specific rewards(e.g., recommending items which may lead to purchases rather than clicks) while the self-supervised layer with cross-entropy loss provides strong gradient signals for parameter updates. Based on such an approach, we propose two frameworks namely Self-Supervised Q-learning(SQN) and Self-Supervised Actor-Critic(SAC). We integrate the proposed frameworks with four state-of-the-art recommendation models. Experimental results on two real-world datasets demonstrate the effectiveness of our approach.",0
"When it comes to session-based or sequential recommendation, several factors must be taken into consideration, such as long-term user engagement and various types of user-item interactions. However, current supervised methods do not accurately model these factors. As a result, transforming sequential recommendation into a reinforcement learning problem is a promising direction. Nonetheless, training recommenders online can be problematic due to the need to expose users to irrelevant recommendations. Consequently, self-supervised reinforcement learning is proposed in this paper for sequential recommendation tasks. This approach involves augmenting standard recommendation models with two output layers: one for self-supervised learning and the other for RL. The RL portion serves as a regularizer to direct the supervised layer toward specific rewards, while the self-supervised layer, with cross-entropy loss, supplies strong gradient signals for parameter adjustments. As a result of this approach, two frameworks, Self-Supervised Q-learning(SQN) and Self-Supervised Actor-Critic(SAC), are proposed, which are integrated with four state-of-the-art recommendation models. The effectiveness of this approach is demonstrated in experimental results on two real-world datasets.",1
"Heretofore, neural networks with external memory are restricted to single memory with lossy representations of memory interactions. A rich representation of relationships between memory pieces urges a high-order and segregated relational memory. In this paper, we propose to separate the storage of individual experiences (item memory) and their occurring relationships (relational memory). The idea is implemented through a novel Self-attentive Associative Memory (SAM) operator. Found upon outer product, SAM forms a set of associative memories that represent the hypothetical high-order relationships between arbitrary pairs of memory elements, through which a relational memory is constructed from an item memory. The two memories are wired into a single sequential model capable of both memorization and relational reasoning. We achieve competitive results with our proposed two-memory model in a diversity of machine learning tasks, from challenging synthetic problems to practical testbeds such as geometry, graph, reinforcement learning, and question answering.",0
"Up until now, neural networks with external memory have been limited to using a single memory with imperfect representations of memory interactions. To improve the representation of relationships between memory pieces, a high-order and segregated relational memory is necessary. This paper proposes separating the storage of individual experiences (item memory) from the relationships that occur between them (relational memory), which is achieved through a Self-attentive Associative Memory (SAM) operator. SAM creates a set of associative memories that represent high-order relationships between any two memory elements, using an outer product. This allows for the construction of a relational memory from an item memory, both of which are integrated into a single sequential model capable of both memorization and relational reasoning. The proposed two-memory model is demonstrated to produce competitive results across a range of machine learning tasks, including synthetic problems, geometry, graph, reinforcement learning and question answering.",1
"In this paper, we present a new reinforcement learning (RL) algorithm called Distributional Soft Actor Critic (DSAC), which exploits the distributional information of accumulated rewards to achieve better performance. Seamlessly integrating SAC (which uses entropy to encourage exploration) with a principled distributional view of the underlying objective, DSAC takes into consideration the randomness in both action and rewards, and beats the state-of-the-art baselines in several continuous control benchmarks. Moreover, with the distributional information of rewards, we propose a unified framework for risk-sensitive learning, one that goes beyond maximizing only expected accumulated rewards. Under this framework we discuss three specific risk-related metrics: percentile, mean-variance and distorted expectation. Our extensive experiments demonstrate that with distribution modeling in RL, the agent performs better for both risk-averse and risk-seeking control tasks.",0
"The paper introduces a novel reinforcement learning algorithm, named Distributional Soft Actor Critic (DSAC), which enhances performance by utilizing the distributional data of rewards. DSAC combines the exploration encouragement of SAC with a principled distributional perspective of the objective, accounting for the randomness in both action and rewards, and surpasses current benchmarks in continuous control. Additionally, utilizing the distributional data of rewards, the paper proposes a comprehensive framework for risk-sensitive learning that surpasses the maximization of expected accumulated rewards alone. The framework presents three distinct risk-related metrics: percentile, mean-variance, and distorted expectation. The paper's experiments demonstrate that distribution modeling in RL leads to better performance in both risk-averse and risk-seeking control tasks for the agent.",1
"This paper focuses on inverse reinforcement learning (IRL) for autonomous robot navigation using semantic observations. The objective is to infer a cost function that explains demonstrated behavior while relying only on the expert's observations and state-control trajectory. We develop a map encoder, which infers semantic class probabilities from the observation sequence, and a cost encoder, defined as deep neural network over the semantic features. Since the expert cost is not directly observable, the representation parameters can only be optimized by differentiating the error between demonstrated controls and a control policy computed from the cost estimate. The error is optimized using a closed-form subgradient computed only over a subset of promising states via a motion planning algorithm. We show that our approach learns to follow traffic rules in the autonomous driving CARLA simulator by relying on semantic observations of cars, sidewalks and road lanes.",0
"The primary focus of this research is on utilizing inverse reinforcement learning (IRL) in combination with semantic observations for self-governing robot navigation. The aim is to deduce a cost function that can explain the demonstrated behavior while relying solely on the expert's observations and state-control trajectory. This is achieved through the development of a map encoder that can deduce semantic class probabilities from the observation sequence and a cost encoder that is defined as a deep neural network over the semantic features. As the expert cost is not directly observable, the representation parameters are optimized by differentiating the error between demonstrated controls and a control policy computed from the cost estimate. This is done using a closed-form subgradient, which is computed over a subset of promising states via a motion planning algorithm. The results show that our approach is capable of learning how to follow traffic rules in the autonomous driving CARLA simulator by relying on semantic observations of cars, sidewalks, and road lanes.",1
"We consider the question of learning $Q$-function in a sample efficient manner for reinforcement learning with continuous state and action spaces under a generative model. If $Q$-function is Lipschitz continuous, then the minimal sample complexity for estimating $\epsilon$-optimal $Q$-function is known to scale as ${\Omega}(\frac{1}{\epsilon^{d_1+d_2 +2}})$ per classical non-parametric learning theory, where $d_1$ and $d_2$ denote the dimensions of the state and action spaces respectively. The $Q$-function, when viewed as a kernel, induces a Hilbert-Schmidt operator and hence possesses square-summable spectrum. This motivates us to consider a parametric class of $Q$-functions parameterized by its ""rank"" $r$, which contains all Lipschitz $Q$-functions as $r \to \infty$. As our key contribution, we develop a simple, iterative learning algorithm that finds $\epsilon$-optimal $Q$-function with sample complexity of $\widetilde{O}(\frac{1}{\epsilon^{\max(d_1, d_2)+2}})$ when the optimal $Q$-function has low rank $r$ and the discounting factor $\gamma$ is below a certain threshold. Thus, this provides an exponential improvement in sample complexity. To enable our result, we develop a novel Matrix Estimation algorithm that faithfully estimates an unknown low-rank matrix in the $\ell_\infty$ sense even in the presence of arbitrary bounded noise, which might be of interest in its own right. Empirical results on several stochastic control tasks confirm the efficacy of our ""low-rank"" algorithms.",0
"In this paper, we explore the efficiency of learning the $Q$-function in reinforcement learning with continuous state and action spaces using a generative model. If the $Q$-function is Lipschitz continuous, then the minimum sample complexity required to estimate an $\epsilon$-optimal $Q$-function is known to follow the classical non-parametric learning theory and scale as ${\Omega}(\frac{1}{\epsilon^{d_1+d_2 +2}})$, where $d_1$ and $d_2$ represent the state and action space dimensions. The $Q$-function, viewed as a kernel, induces a Hilbert-Schmidt operator and possesses a square-summable spectrum. To address this, we introduce a parametric class of $Q$-functions that are parameterized by their ""rank"" $r$, which includes all Lipschitz $Q$-functions as $r \to \infty$. We propose a simple iterative learning algorithm that can find an $\epsilon$-optimal $Q$-function with a sample complexity of $\widetilde{O}(\frac{1}{\epsilon^{\max(d_1, d_2)+2}})$ when the optimal $Q$-function has a low rank $r$ and the discounting factor $\gamma$ is below a specific threshold. Our approach provides an exponential improvement in sample complexity. Additionally, we develop a Matrix Estimation algorithm that can accurately estimate an unknown low-rank matrix in the $\ell_\infty$ sense, even in the presence of arbitrary bounded noise, which may be of interest on its own. Our empirical results on various stochastic control tasks demonstrate the effectiveness of our ""low-rank"" algorithms.",1
"In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.",0
"Over the past few years, on-policy reinforcement learning (RL) has proven to be a successful approach for various continuous control tasks. Despite their simple concepts, RL algorithms require many low- and high-level design decisions that significantly impact the agents' performance. Unfortunately, these choices are not extensively discussed in literature, creating a disparity between described algorithms and their implementations. This issue has hindered progress in RL and slowed overall development. To address this gap, we have implemented over 50 choices within a unified on-policy RL framework, enabling us to investigate their effects in a large-scale empirical study. We have trained more than 250,000 agents across five continuous control environments with varying levels of complexity, and our findings provide practical recommendations and insights for on-policy RL agent training.",1
"Deep neural network (DNN) based approaches have been widely investigated and deployed in medical image analysis. For example, fully convolutional neural networks (FCN) achieve the state-of-the-art performance in several applications of 2D/3D medical image segmentation. Even the baseline neural network models (U-Net, V-Net, etc.) have been proven to be very effective and efficient when the training process is set up properly. Nevertheless, to fully exploit the potentials of neural networks, we propose an automated searching approach for the optimal training strategy with reinforcement learning. The proposed approach can be utilized for tuning hyper-parameters, and selecting necessary data augmentation with certain probabilities. The proposed approach is validated on several tasks of 3D medical image segmentation. The performance of the baseline model is boosted after searching, and it can achieve comparable accuracy to other manually-tuned state-of-the-art segmentation approaches.",0
"Medical image analysis has extensively researched and implemented DNN-based techniques. For instance, the FCN has demonstrated outstanding performance in various 2D/3D medical image segmentation applications. Even the basic neural network models like U-Net and V-Net have proven to be highly effective and efficient with proper training. Despite this, to maximize the potential of neural networks, we suggest using a reinforcement learning-based automated search approach for the optimal training strategy. This approach can configure hyper-parameters and select data augmentation with particular probabilities. We tested this approach on several 3D medical image segmentation tasks and found that it enhances the performance of the baseline model. After the search process, the model achieves accuracy comparable to other state-of-the-art segmentation techniques that have been manually tuned.",1
"Individuality is essential in human society, which induces the division of labor and thus improves the efficiency and productivity. Similarly, it should also be the key to multi-agent cooperation. Inspired by that individuality is of being an individual separate from others, we propose a simple yet efficient method for the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL). EOI learns a probabilistic classifier that predicts a probability distribution over agents given their observation and gives each agent an intrinsic reward of being correctly predicted by the classifier. The intrinsic reward encourages the agents to visit their own familiar observations, and learning the classifier by such observations makes the intrinsic reward signals stronger and the agents more identifiable. To further enhance the intrinsic reward and promote the emergence of individuality, two regularizers are proposed to increase the discriminability of the classifier. We implement EOI on top of popular MARL algorithms. Empirically, we show that EOI significantly outperforms existing methods in a variety of multi-agent cooperative scenarios.",0
"In human society, individuality plays a crucial role in promoting division of labor, which ultimately leads to increased efficiency and productivity. Similarly, in multi-agent cooperation, individuality should be a primary focus. To achieve this, we propose a straightforward yet effective approach called the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL). EOI involves training a probabilistic classifier that predicts the probability distribution of agents based on their observations. Each agent is then rewarded for being correctly predicted by the classifier, which encourages them to explore their own unique observations. This intrinsic reward signal strengthens as the classifier learns from these observations, promoting the emergence of individuality. To further enhance this process, two regularizers are introduced to increase the classifier's discriminability. We integrate EOI into popular MARL algorithms and demonstrate its superior performance in various multi-agent cooperative scenarios.",1
"In reinforcement learning, agents that consider the context, or current state, when selecting source policies for transfer have been shown to outperform context-free approaches. However, none of the existing approaches transfer knowledge contextually from model-based learners to a model-free learner. This could be useful, for instance, when source policies are intentionally learned on diverse simulations with plentiful data but transferred to a real-world setting with limited data. In this paper, we assume knowledge of estimated source task dynamics and policies, and common sub-goals but different dynamics. We introduce a novel deep mixture-of-experts formulation for learning state-dependent beliefs over source task dynamics that match the target dynamics using state trajectories collected from the target task. The mixture model is easy to interpret, demonstrates robustness to estimation errors in dynamics, and is compatible with most learning algorithms. We then show how this model can be incorporated into standard policy reuse frameworks, and demonstrate its effectiveness on benchmarks from OpenAI-Gym.",0
"The effectiveness of reinforcement learning agents in selecting source policies for transfer is improved when they take the current state, or context, into consideration. However, existing methods do not enable contextual knowledge transfer from model-based learners to model-free learners. This is particularly useful when source policies are learned on varied simulations with abundant data and applied to a real-world setting with limited data. In this research, we assume knowledge of source task dynamics and policies, and common sub-goals but different dynamics. We present a new deep mixture-of-experts formulation that learns state-dependent beliefs over source task dynamics that match the target dynamics, using state trajectories collected from the target task. The mixture model is straightforward to interpret, resilient to dynamics estimation errors, and compatible with most learning algorithms. We also demonstrate how this model can be incorporated into standard policy reuse frameworks and how effective it is on OpenAI-Gym benchmarks.",1
"We survey in this article the connections between Machine Learning and Control Theory. Control Theory provide useful concepts and tools for Machine Learning. Conversely Machine Learning can be used to solve large control problems. In the first part of the paper, we develop the connections between reinforcement learning and Markov Decision Processes, which are discrete time control problems. In the second part, we review the concept of supervised learning and the relation with static optimization. Deep learning which extends supervised learning, can be viewed as a control problem. In the third part, we present the links between stochastic gradient descent and mean-field theory. Conversely, in the fourth and fifth parts, we review machine learning approaches to stochastic control problems, and focus on the deterministic case, to explain, more easily, the numerical algorithms.",0
"This article explores the relationship between Machine Learning and Control Theory. Control Theory offers valuable concepts and tools that can be applied to Machine Learning, while Machine Learning can be utilized to address complex control problems. The first section of the paper discusses the connections between reinforcement learning and Markov Decision Processes, which are control problems in discrete time. The second section delves into supervised learning and its relationship to static optimization. Deep learning, an extension of supervised learning, can be regarded as a control problem. The third section examines the correlations between stochastic gradient descent and mean-field theory. The fourth and fifth sections focus on machine learning approaches to stochastic control problems, with an emphasis on the deterministic case to facilitate comprehension of the numerical algorithms.",1
"Discovering causal structure among a set of variables is a fundamental problem in many domains. However, state-of-the-art methods seldom consider the possibility that the observational data has missing values (incomplete data), which is ubiquitous in many real-world situations. The missing value will significantly impair the performance and even make the causal discovery algorithms fail. In this paper, we propose an approach to discover causal structures from incomplete data by using a novel encoder and reinforcement learning (RL). The encoder is designed for missing data imputation as well as feature extraction. In particular, it learns to encode the currently available information (with missing values) into a robust feature representation which is then used to determine where to search the best graph. The encoder is integrated into a RL framework that can be optimized using the actor-critic algorithm. Our method takes the incomplete observational data as input and generates a causal structure graph. Experimental results on synthetic and real data demonstrate that our method can robustly generate causal structures from incomplete data. Compared with the direct combination of data imputation and causal discovery methods, our method performs generally better and can even obtain a performance gain as much as 43.2%.",0
"The identification of causal relationships among variables is a fundamental challenge in various fields. However, contemporary methods rarely take into account the possibility of incomplete data, which is prevalent in real-world scenarios. The presence of missing values can significantly impact the performance of causal discovery algorithms and render them ineffective. Therefore, we propose a novel approach in this paper that employs an encoder and reinforcement learning to identify causal structures from incomplete data. The encoder is devised to perform missing data imputation and feature extraction simultaneously. It learns to encode the available information (with missing values) into a robust feature representation, which is then utilized to determine the optimal graph search location. The encoder is integrated into a reinforcement learning framework, which can be optimized using the actor-critic algorithm. Our method takes incomplete observational data as input and generates a causal structure graph. Results from experiments conducted on synthetic and real data demonstrate the robustness of our approach in generating causal structures from incomplete data. Our method outperforms direct combinations of data imputation and causal discovery methods and can even achieve a 43.2% performance improvement.",1
"We consider model selection in stochastic bandit and reinforcement learning problems. Given a set of base learning algorithms, an effective model selection strategy adapts to the best learning algorithm in an online fashion. We show that by estimating the regret of each algorithm and playing the algorithms such that all empirical regrets are ensured to be of the same order, the overall regret balancing strategy achieves a regret that is close to the regret of the optimal base algorithm. Our strategy requires an upper bound on the optimal base regret as input, and the performance of the strategy depends on the tightness of the upper bound. We show that having this prior knowledge is necessary in order to achieve a near-optimal regret. Further, we show that any near-optimal model selection strategy implicitly performs a form of regret balancing.",0
"The article discusses the selection of models in stochastic bandit and reinforcement learning problems. To choose the most effective learning algorithm in an online manner, a suitable model selection strategy is required. The authors propose a strategy that estimates the regret of each algorithm and ensures that all empirical regrets are of the same order. This approach achieves a regret that is similar to the optimal base algorithm by balancing the overall regret. The strategy requires an upper bound on the optimal base regret, and its success depends on the accuracy of this estimate. The authors argue that having this prior knowledge is essential for achieving near-optimal regret. Additionally, they suggest that any model selection strategy that performs well implicitly employs a form of regret balancing.",1
"Model-free deep reinforcement learning is sample inefficient. One hypothesis -- speculated, but not confirmed -- is that catastrophic interference within an environment inhibits learning. We test this hypothesis through a large-scale empirical study in the Arcade Learning Environment (ALE) and, indeed, find supporting evidence. We show that interference causes performance to plateau; the network cannot train on segments beyond the plateau without degrading the policy used to reach there. By synthetically controlling for interference, we demonstrate performance boosts across architectures, learning algorithms and environments. A more refined analysis shows that learning one segment of a game often increases prediction errors elsewhere. Our study provides a clear empirical link between catastrophic interference and sample efficiency in reinforcement learning.",0
"The efficiency of model-free deep reinforcement learning is limited due to its inability to effectively utilize samples. One possible explanation for this limitation is the occurrence of catastrophic interference within the learning environment, which hinders the learning process. To test this hypothesis, we conducted a comprehensive study in the Arcade Learning Environment (ALE) and discovered evidence to support this claim. Our findings indicate that interference causes performance to level off, preventing the network from learning beyond this point without sacrificing the quality of the policy used to reach that level. By artificially controlling for interference, we observed improved performance across various learning algorithms, architectures, and environments. Additionally, we discovered that learning one segment of a game can lead to prediction errors in other parts of the game. Our study establishes a concrete link between catastrophic interference and the sample efficiency of reinforcement learning.",1
"We consider a distributed system, consisting of a heterogeneous set of devices, ranging from low-end to high-end. These devices have different profiles, e.g., different energy budgets, or different hardware specifications, determining their capabilities on performing certain learning tasks. We propose the first approach that enables distributed learning in such a heterogeneous system. Applying our approach, each device employs a neural network (NN) with a topology that fits its capabilities; however, part of these NNs share the same topology, so that their parameters can be jointly learned. This differs from current approaches, such as federated learning, which require all devices to employ the same NN, enforcing a trade-off between achievable accuracy and computational overhead of training. We evaluate heterogeneous distributed learning for reinforcement learning (RL) and observe that it greatly improves the achievable reward on more powerful devices, compared to current approaches, while still maintaining a high reward on the weaker devices. We also explore supervised learning, observing similar gains.",0
"Our focus is on a distributed system comprising devices with varying capabilities, ranging from low-end to high-end. These devices possess different profiles, such as energy budgets and hardware specifications, which impact their capacity to complete specific learning tasks. Our novel approach allows for distributed learning in such a heterogeneous system. Each device employs a neural network (NN) that suits its capabilities, with some of these NNs sharing the same topology for joint learning of parameters. This differs from current approaches, such as federated learning, which mandate that all devices use the same NN, leading to a trade-off between accuracy and computational overhead. We tested our method for reinforcement learning (RL) and found that it significantly enhances rewards on more powerful devices compared to current approaches, while maintaining high rewards on weaker devices. We also explored supervised learning and observed similar benefits.",1
"With reinforcement learning, an agent could learn complex behaviors from high-level abstractions of the task. However, exploration and reward shaping remained challenging for existing methods, especially in scenarios where the extrinsic feedback was sparse. Expert demonstrations have been investigated to solve these difficulties, but a tremendous number of high-quality demonstrations were usually required. In this work, an integrated policy gradient algorithm was proposed to boost exploration and facilitate intrinsic reward learning from only limited number of demonstrations. We achieved this by reformulating the original reward function with two additional terms, where the first term measured the Jensen-Shannon divergence between current policy and the expert, and the second term estimated the agent's uncertainty about the environment. The presented algorithm was evaluated on a range of simulated tasks with sparse extrinsic reward signals where only one single demonstrated trajectory was provided to each task, superior exploration efficiency and high average return were demonstrated in all tasks. Furthermore, it was found that the agent could imitate the expert's behavior and meanwhile sustain high return.",0
"Reinforcement learning enables an agent to acquire sophisticated behaviors by abstracting high-level tasks. However, existing methods face challenges in exploration and reward shaping, particularly in situations where extrinsic feedback is limited. While expert demonstrations have been proposed to address these issues, a large number of high-quality demonstrations are typically necessary. This study introduces an integrated policy gradient algorithm that enhances exploration and facilitates intrinsic reward learning using a limited number of demonstrations. The original reward function is reformulated with two additional terms to accomplish this, including one that gauges the divergence between the current policy and the expert and another that assesses the agent's uncertainty about the environment. The algorithm is assessed on a range of simulated tasks with sparse extrinsic reward signals, requiring only one demonstrated trajectory per task. The results demonstrate superior exploration efficiency and high average return across all tasks, and the agent is capable of imitating the expert's behavior while maintaining a high return.",1
"Continual learning with neural networks is an important learning framework in AI that aims to learn a sequence of tasks well. However, it is often confronted with three challenges: (1) overcome the catastrophic forgetting problem, (2) adapt the current network to new tasks, and meanwhile (3) control its model complexity. To reach these goals, we propose a novel approach named as Continual Learning with Efficient Architecture Search, or CLEAS in short. CLEAS works closely with neural architecture search (NAS) which leverages reinforcement learning techniques to search for the best neural architecture that fits a new task. In particular, we design a neuron-level NAS controller that decides which old neurons from previous tasks should be reused (knowledge transfer), and which new neurons should be added (to learn new knowledge). Such a fine-grained controller allows one to find a very concise architecture that can fit each new task well. Meanwhile, since we do not alter the weights of the reused neurons, we perfectly memorize the knowledge learned from previous tasks. We evaluate CLEAS on numerous sequential classification tasks, and the results demonstrate that CLEAS outperforms other state-of-the-art alternative methods, achieving higher classification accuracy while using simpler neural architectures.",0
"In the field of AI, continual learning with neural networks is a significant framework that aims to learn a sequence of tasks effectively. However, this approach often encounters three challenges that need to be overcome, including the catastrophic forgetting problem, adapting the current network to new tasks, and controlling its model complexity. To address these challenges, we propose a new approach called Continual Learning with Efficient Architecture Search (CLEAS) that works closely with neural architecture search (NAS). Our approach uses a neuron-level NAS controller that decides which old neurons from previous tasks should be reused for knowledge transfer and which new neurons should be added to learn new knowledge. This fine-grained controller allows us to find a concise architecture that fits each new task well, while perfectly memorizing the knowledge learned from previous tasks. We evaluated CLEAS on various sequential classification tasks and found that it outperforms other state-of-the-art methods, achieving higher classification accuracy while using simpler neural architectures.",1
"Temporal-difference and Q-learning play a key role in deep reinforcement learning, where they are empowered by expressive nonlinear function approximators such as neural networks. At the core of their empirical successes is the learned feature representation, which embeds rich observations, e.g., images and texts, into the latent space that encodes semantic structures. Meanwhile, the evolution of such a feature representation is crucial to the convergence of temporal-difference and Q-learning.   In particular, temporal-difference learning converges when the function approximator is linear in a feature representation, which is fixed throughout learning, and possibly diverges otherwise. We aim to answer the following questions: When the function approximator is a neural network, how does the associated feature representation evolve? If it converges, does it converge to the optimal one?   We prove that, utilizing an overparameterized two-layer neural network, temporal-difference and Q-learning globally minimize the mean-squared projected Bellman error at a sublinear rate. Moreover, the associated feature representation converges to the optimal one, generalizing the previous analysis of Cai et al. (2019) in the neural tangent kernel regime, where the associated feature representation stabilizes at the initial one. The key to our analysis is a mean-field perspective, which connects the evolution of a finite-dimensional parameter to its limiting counterpart over an infinite-dimensional Wasserstein space. Our analysis generalizes to soft Q-learning, which is further connected to policy gradient.",0
"Deep reinforcement learning relies heavily on temporal-difference and Q-learning, which are made more effective through the use of expressive nonlinear function approximators, such as neural networks. A crucial aspect of their success is the feature representation, which is learned and encodes semantic structures from rich observations like images and texts. The evolution of this feature representation is critical to the convergence of temporal-difference and Q-learning. Specifically, when the function approximator is a neural network, questions arise about how the feature representation evolves and whether it converges to the optimal one. Our research shows, using an overparameterized two-layer neural network, that temporal-difference and Q-learning globally minimize the mean-squared projected Bellman error at a sublinear rate. Additionally, the feature representation converges to the optimal one, building upon previous analyses in the neural tangent kernel regime. Our approach uses a mean-field perspective connecting finite-dimensional parameters to their infinite-dimensional counterparts in a Wasserstein space. The analysis extends to soft Q-learning and connects to policy gradient.",1
"Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are usually less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows a flexible score function under the acyclicity constraint.",0
"In various empirical sciences, identifying the causal structure among a group of variables is a crucial task. Traditional methods for score-based causal discovery depend on local heuristics to find a Directed Acyclic Graph (DAG) based on a pre-defined score function. These methods, such as greedy equivalence search, may yield favorable results with infinite samples and specific model assumptions but are often unsatisfactory in practical scenarios due to limited data and potential assumption violations. Inspired by recent progress in neural combinatorial optimization, we suggest utilizing Reinforcement Learning (RL) to locate the DAG with the highest score. Our encoder-decoder model takes input from observable data and creates graph adjacency matrices that calculate rewards. The reward encompasses both the pre-defined score function and two penalty terms to ensure acyclicity. Unlike conventional RL applications that aim to learn a policy, we employ RL as a search strategy, and our ultimate output will be the graph that attains the best reward among all graphs generated during training. We conduct experiments on both artificial and real datasets, demonstrating that our proposed approach not only improves search capability but also enables flexibility in the score function while adhering to the acyclicity constraint.",1
"Dyna-style reinforcement learning (RL) agents improve sample efficiency over model-free RL agents by updating the value function with simulated experience generated by an environment model. However, it is often difficult to learn accurate models of environment dynamics, and even small errors may result in failure of Dyna agents. In this paper, we investigate one type of model error: hallucinated states. These are states generated by the model, but that are not real states of the environment. We present the Hallucinated Value Hypothesis (HVH): updating values of real states towards values of hallucinated states results in misleading state-action values which adversely affect the control policy. We discuss and evaluate four Dyna variants; three which update real states toward simulated -- and therefore potentially hallucinated -- states and one which does not. The experimental results provide evidence for the HVH thus suggesting a fruitful direction toward developing Dyna algorithms robust to model error.",0
"Dyna-style reinforcement learning (RL) agents are more efficient in terms of samples than model-free RL agents because they update the value function through simulated experience from an environment model. However, the accuracy of the model of environmental dynamics can be difficult to learn, and even minor errors can cause failure of the Dyna agents. In this article, we examine one type of model error, namely hallucinated states, which are generated by the model but are not real states of the environment. We present the Hallucinated Value Hypothesis (HVH), which asserts that updating the values of real states towards the values of hallucinated states results in misleading state-action values, which adversely affect the control policy. We investigate and assess four Dyna variants: three that update real states towards simulated states - which may include hallucinated states - and one that does not. Through experimental results, we provide evidence for the HVH and suggest a productive direction for developing Dyna algorithms that are resilient to model error.",1
"We consider the problem of reinforcement learning (RL) with unbounded state space motivated by the classical problem of scheduling in a queueing network. Traditional policies as well as error metric that are designed for finite, bounded or compact state space, require infinite samples for providing any meaningful performance guarantee (e.g. $\ell_\infty$ error) for unbounded state space. That is, we need a new notion of performance metric. As the main contribution of this work, inspired by the literature in queuing systems and control theory, we propose stability as the notion of ""goodness"": the state dynamics under the policy should remain in a bounded region with high probability. As a proof of concept, we propose an RL policy using Sparse-Sampling-based Monte Carlo Oracle and argue that it satisfies the stability property as long as the system dynamics under the optimal policy respects a Lyapunov function. The assumption of existence of a Lyapunov function is not restrictive as it is equivalent to the positive recurrence or stability property of any Markov chain, i.e., if there is any policy that can stabilize the system then it must possess a Lyapunov function. And, our policy does not utilize the knowledge of the specific Lyapunov function. To make our method sample efficient, we provide an improved, sample efficient Sparse-Sampling-based Monte Carlo Oracle with Lipschitz value function that may be of interest in its own right. Furthermore, we design an adaptive version of the algorithm, based on carefully constructed statistical tests, which finds the correct tuning parameter automatically.",0
"The focus of this study is reinforcement learning (RL) when dealing with an unbounded state space, inspired by the scheduling problem in queueing networks. Traditional RL policies and error metrics designed for finite or bounded state spaces require an infinite number of samples to provide any meaningful performance guarantee for unbounded state space. Therefore, an alternative performance metric is necessary. Our proposed metric is stability, inspired by literature in queuing systems and control theory. We argue that a policy that maintains state dynamics within a bounded region with high probability is considered ""good."" We present an RL policy that satisfies this stability property using Sparse-Sampling-based Monte Carlo Oracle, assuming that the system dynamics under the optimal policy respects a Lyapunov function. The existence of a Lyapunov function is not restrictive, as any policy that can stabilize the system must possess one. Our policy does not utilize knowledge of the specific Lyapunov function, and we provide an improved, sample-efficient Sparse-Sampling-based Monte Carlo Oracle with Lipschitz value function to make our method more efficient. Additionally, we design an adaptive version of the algorithm that finds the correct tuning parameter automatically using carefully constructed statistical tests.",1
"Recent work in the behavioural sciences has begun to overturn the long-held belief that human decision making is irrational, suboptimal and subject to biases. This turn to the rational suggests that human decision making may be a better source of ideas for constraining how machine learning problems are defined than would otherwise be the case. One promising idea concerns human decision making that is dependent on apparently irrelevant aspects of the choice context. Previous work has shown that by taking into account choice context and making relational observations, people can maximize expected value. Other work has shown that Partially observable Markov decision processes (POMDPs) are a useful way to formulate human-like decision problems. Here, we propose a novel POMDP model for contextual choice tasks and show that, despite the apparent irrationalities, a reinforcement learner can take advantage of the way that humans make decisions. We suggest that human irrationalities may offer a productive source of inspiration for improving the design of AI architectures and machine learning methods.",0
"The notion that human decision-making is irrational, suboptimal, and biased has been challenged by recent research in the behavioural sciences. This shift towards rational decision-making implies that it may be beneficial to use human decision-making as a guide for defining machine learning problems. One promising approach involves considering apparently irrelevant factors in the decision-making context, which previous studies have shown can lead to better outcomes. Additionally, Partially Observable Markov Decision Processes (POMDPs) have proven useful in formulating decision problems that mimic human behaviour. In this study, a novel POMDP model is proposed for contextual choice tasks, demonstrating that a reinforcement learner can leverage human irrationalities to make better decisions. Therefore, it is suggested that human irrationalities can serve as a valuable source of inspiration for enhancing AI architectures and machine learning methods.",1
"Policy distillation, which transfers a teacher policy to a student policy has achieved great success in challenging tasks of deep reinforcement learning. This teacher-student framework requires a well-trained teacher model which is computationally expensive. Moreover, the performance of the student model could be limited by the teacher model if the teacher model is not optimal. In the light of collaborative learning, we study the feasibility of involving joint intellectual efforts from diverse perspectives of student models. In this work, we introduce dual policy distillation(DPD), a student-student framework in which two learners operate on the same environment to explore different perspectives of the environment and extract knowledge from each other to enhance their learning. The key challenge in developing this dual learning framework is to identify the beneficial knowledge from the peer learner for contemporary learning-based reinforcement learning algorithms, since it is unclear whether the knowledge distilled from an imperfect and noisy peer learner would be helpful. To address the challenge, we theoretically justify that distilling knowledge from a peer learner will lead to policy improvement and propose a disadvantageous distillation strategy based on the theoretical results. The conducted experiments on several continuous control tasks show that the proposed framework achieves superior performance with a learning-based agent and function approximation without the use of expensive teacher models.",0
"The process of transferring a teacher policy to a student policy, known as policy distillation, has been successful in difficult deep reinforcement learning tasks. However, this framework relies on a well-trained teacher model, which can be computationally expensive, and may limit the performance of the student model if the teacher is not optimal. To address this, we propose dual policy distillation (DPD), a student-student framework where two learners operate on the same environment to extract knowledge from each other, enhancing their learning. The key challenge is identifying beneficial knowledge from a peer learner, given that it may be imperfect and noisy. We justify distilling knowledge from a peer learner, proposing a disadvantageous distillation strategy based on theoretical results. Experiments show that DPD achieves superior performance without expensive teacher models in several continuous control tasks.",1
"Driver distraction a significant risk to driving safety. Apart from spatial domain, research on temporal inattention is also necessary. This paper aims to figure out the pattern of drivers' temporal attention allocation. In this paper, we propose an actor-critic method - Attention-based Twin Delayed Deep Deterministic policy gradient (ATD3) algorithm to approximate a driver' s action according to observations and measure the driver' s attention allocation for consecutive time steps in car-following model. Considering reaction time, we construct the attention mechanism in the actor network to capture temporal dependencies of consecutive observations. In the critic network, we employ Twin Delayed Deep Deterministic policy gradient algorithm (TD3) to address overestimated value estimates persisting in the actor-critic algorithm. We conduct experiments on real-world vehicle trajectory datasets and show that the accuracy of our proposed approach outperforms seven baseline algorithms. Moreover, the results reveal that the attention of the drivers in smooth vehicles is uniformly distributed in previous observations while they keep their attention to recent observations when sudden decreases of relative speeds occur. This study is the first contribution to drivers' temporal attention and provides scientific support for safety measures in transportation systems from the perspective of data mining.",0
"The safety of driving is significantly jeopardized by driver distraction, and it is important to conduct research on temporal inattention in addition to spatial domain. The main objective of this study is to determine the temporal attention allocation pattern of drivers. To achieve this, we have proposed the Attention-based Twin Delayed Deep Deterministic policy gradient (ATD3) algorithm, which utilizes an actor-critic method to approximate a driver's actions based on observations and measure their attention allocation for consecutive time steps in a car-following model. Our attention mechanism in the actor network captures temporal dependencies of consecutive observations, while the critic network employs the Twin Delayed Deep Deterministic policy gradient algorithm (TD3) to address overestimated value estimates. We conducted experiments on real-world vehicle trajectory datasets and found that our proposed approach outperforms seven baseline algorithms in terms of accuracy. Our results also reveal that drivers in smooth vehicles distribute their attention uniformly in previous observations, whereas they focus on recent observations when sudden decreases of relative speeds occur. This study is the first to contribute to drivers' temporal attention and provides scientific support for safety measures in transportation systems using data mining.",1
"This paper describes the application of reinforcement learning (RL) to multi-product inventory management in supply chains. The problem description and solution are both adapted from a real-world business solution. The novelty of this problem with respect to supply chain literature is (i) we consider concurrent inventory management of a large number (50 to 1000) of products with shared capacity, (ii) we consider a multi-node supply chain consisting of a warehouse which supplies three stores, (iii) the warehouse, stores, and transportation from warehouse to stores have finite capacities, (iv) warehouse and store replenishment happen at different time scales and with realistic time lags, and (v) demand for products at the stores is stochastic. We describe a novel formulation in a multi-agent (hierarchical) reinforcement learning framework that can be used for parallelised decision-making, and use the advantage actor critic (A2C) algorithm with quantised action spaces to solve the problem. Experiments show that the proposed approach is able to handle a multi-objective reward comprised of maximising product sales and minimising wastage of perishable products.",0
"In this paper, the use of reinforcement learning (RL) in managing inventory for multiple products in supply chains is discussed. The problem and solution presented are based on a real-world business scenario. The unique aspect of this problem, in comparison to supply chain literature, is that it involves managing a large number of products (50 to 1000) with shared capacity, a multi-node supply chain consisting of a warehouse and three stores, finite capacities for the warehouse, stores, and transportation, warehouse and store replenishment at different time scales and with realistic time lags, and stochastic demand for products at the stores. A novel formulation is presented in a multi-agent reinforcement learning framework for parallelized decision-making, using the advantage actor critic (A2C) algorithm with quantized action spaces to solve the problem. The proposed approach is able to handle a multi-objective reward of maximizing product sales and minimizing waste of perishable products, as demonstrated in experiments.",1
"Policy evaluation algorithms are essential to reinforcement learning due to their ability to predict the performance of a policy. However, there are two long-standing issues lying in this prediction problem that need to be tackled: off-policy stability and on-policy efficiency. The conventional temporal difference (TD) algorithm is known to perform very well in the on-policy setting, yet is not off-policy stable. On the other hand, the gradient TD and emphatic TD algorithms are off-policy stable, but are not on-policy efficient. This paper introduces novel algorithms that are both off-policy stable and on-policy efficient by using the oblique projection method. The empirical experimental results on various domains validate the effectiveness of the proposed approach.",0
"Reinforcement learning relies heavily on policy evaluation algorithms to forecast the success of a policy. However, there exist two persistent challenges in this prediction process - on-policy efficiency and off-policy stability. While the conventional temporal difference (TD) algorithm performs well in the on-policy context, it lacks off-policy stability. Conversely, the gradient TD and emphatic TD algorithms exhibit stability in off-policy scenarios but are not efficient in on-policy settings. This research introduces innovative algorithms that leverage the oblique projection method to achieve both on-policy efficiency and off-policy stability. The study's empirical experimental results across various domains validate the efficacy of this approach.",1
"In this paper, we introduce proximal gradient temporal difference learning, which provides a principled way of designing and analyzing true stochastic gradient temporal difference learning algorithms. We show how gradient TD (GTD) reinforcement learning methods can be formally derived, not by starting from their original objective functions, as previously attempted, but rather from a primal-dual saddle-point objective function. We also conduct a saddle-point error analysis to obtain finite-sample bounds on their performance. Previous analyses of this class of algorithms use stochastic approximation techniques to prove asymptotic convergence, and do not provide any finite-sample analysis. We also propose an accelerated algorithm, called GTD2-MP, that uses proximal ``mirror maps'' to yield an improved convergence rate. The results of our theoretical analysis imply that the GTD family of algorithms are comparable and may indeed be preferred over existing least squares TD methods for off-policy learning, due to their linear complexity. We provide experimental results showing the improved performance of our accelerated gradient TD methods.",0
"The article presents the concept of proximal gradient temporal difference learning as a method for designing and analyzing true stochastic gradient temporal difference learning algorithms. Rather than starting from the original objective functions of gradient TD reinforcement learning methods, the article derives them from a primal-dual saddle-point objective function. The article also conducts a saddle-point error analysis to obtain finite-sample bounds on their performance. The article proposes an accelerated algorithm called GTD2-MP, which uses proximal ""mirror maps"" to improve convergence rate. The article suggests that the GTD family of algorithms are comparable and may be preferred over existing least squares TD methods for off-policy learning due to their linear complexity. The article presents experimental results demonstrating the improved performance of the accelerated gradient TD methods.",1
"Multi-Agent Reinforcement Learning (MARL) considers settings in which a set of coexisting agents interact with one another and their environment. The adaptation and learning of other agents induces non-stationarity in the environment dynamics. This poses a great challenge for value function-based algorithms whose convergence usually relies on the assumption of a stationary environment. Policy search algorithms also struggle in multi-agent settings as the partial observability resulting from an opponent's actions not being known introduces high variance to policy training. Modelling an agent's opponent(s) is often pursued as a means of resolving the issues arising from the coexistence of learning opponents. An opponent model provides an agent with some ability to reason about other agents to aid its own decision making. Most prior works learn an opponent model by assuming the opponent is employing a stationary policy or switching between a set of stationary policies. Such an approach can reduce the variance of training signals for policy search algorithms. However, in the multi-agent setting, agents have an incentive to continually adapt and learn. This means that the assumptions concerning opponent stationarity are unrealistic. In this work, we develop a novel approach to modelling an opponent's learning dynamics which we term Learning to Model Opponent Learning (LeMOL). We show our structured opponent model is more accurate and stable than naive behaviour cloning baselines. We further show that opponent modelling can improve the performance of algorithmic agents in multi-agent settings.",0
"The study of Multi-Agent Reinforcement Learning (MARL) involves scenarios where a group of agents coexist and interact with one another and their surroundings. The learning and adaptation of other agents create a non-stationary environment, which presents a considerable challenge for value function-based algorithms. These algorithms typically require a stationary environment to converge. Policy search algorithms also struggle in multi-agent settings because the partial observability resulting from an opponent's actions not being known introduces significant variance to policy training. To address this issue, many works aim to model an agent's opponent(s) to aid decision making. However, most prior works assume the opponent is employing a stationary or switching policy, which is unrealistic in multi-agent settings. This work proposes a novel approach called Learning to Model Opponent Learning (LeMOL) to model an opponent's learning dynamics. The structured opponent model developed in this study is more accurate and stable than naive behaviour cloning baselines. Additionally, the study shows that opponent modelling can improve the performance of algorithmic agents in multi-agent settings.",1
"Offline reinforcement learning, wherein one uses off-policy data logged by a fixed behavior policy to evaluate and learn new policies, is crucial in applications where experimentation is limited such as medicine. We study the estimation of policy value and gradient of a deterministic policy from off-policy data when actions are continuous. Targeting deterministic policies, for which action is a deterministic function of state, is crucial since optimal policies are always deterministic (up to ties). In this setting, standard importance sampling and doubly robust estimators for policy value and gradient fail because the density ratio does not exist. To circumvent this issue, we propose several new doubly robust estimators based on different kernelization approaches. We analyze the asymptotic mean-squared error of each of these under mild rate conditions for nuisance estimators. Specifically, we demonstrate how to obtain a rate that is independent of the horizon length.",0
"Offline reinforcement learning is essential in fields like medicine where experimentation is limited, and it involves using off-policy data collected by a fixed behavior policy to learn and evaluate new policies. Our research focuses on estimating the value and gradient of a deterministic policy from off-policy data when dealing with continuous actions. It is crucial to target deterministic policies, as optimal policies are always deterministic. However, standard importance sampling and doubly robust estimators fail in this setting due to the absence of a density ratio. To overcome this challenge, we propose several new doubly robust estimators based on different kernelization approaches. We analyze the asymptotic mean-squared error of each estimator under mild rate conditions for nuisance estimators and demonstrate how to obtain a rate that is independent of the horizon length.",1
"We study the reinforcement learning problem in the setting of finite-horizon episodic Markov Decision Processes (MDPs) with $S$ states, $A$ actions, and episode length $H$. We propose a model-free algorithm UCB-Advantage and prove that it achieves $\tilde{O}(\sqrt{H^2SAT})$ regret where $T = KH$ and $K$ is the number of episodes to play. Our regret bound improves upon the results of [Jin et al., 2018] and matches the best known model-based algorithms as well as the information theoretic lower bound up to logarithmic factors. We also show that UCB-Advantage achieves low local switching cost and applies to concurrent reinforcement learning, improving upon the recent results of [Bai et al., 2019].",0
"The focus of our research is on the reinforcement learning problem, which we investigate in the context of finite-horizon episodic Markov Decision Processes (MDPs). These MDPs have $S$ states, $A$ actions, and episode length $H$. In this study, we present a model-free algorithm called UCB-Advantage and demonstrate that it has a regret of $\tilde{O}(\sqrt{H^2SAT})$, where $T$ equals $KH$ and $K$ represents the number of episodes to be played. Our results show that UCB-Advantage performs better than the algorithm introduced by [Jin et al., 2018] and is comparable to the best known model-based algorithms, as well as the information theoretic lower bound, with logarithmic factors. Additionally, we prove that UCB-Advantage has low local switching cost and can be applied to concurrent reinforcement learning, surpassing the recent findings of [Bai et al., 2019].",1
"We study reinforcement learning in non-episodic factored Markov decision processes (FMDPs). We propose two near-optimal and oracle-efficient algorithms for FMDPs. Assuming oracle access to an FMDP planner, they enjoy a Bayesian and a frequentist regret bound respectively, both of which reduce to the near-optimal bound $\widetilde{O}(DS\sqrt{AT})$ for standard non-factored MDPs. We propose a tighter connectivity measure, factored span, for FMDPs and prove a lower bound that depends on the factored span rather than the diameter $D$. In order to decrease the gap between lower and upper bounds, we propose an adaptation of the REGAL.C algorithm whose regret bound depends on the factored span. Our oracle-efficient algorithms outperform previously proposed near-optimal algorithms on computer network administration simulations.",0
"Our focus is on reinforcement learning in non-episodic factored Markov decision processes (FMDPs). To this end, we present two algorithms that are both near-optimal and oracle-efficient for FMDPs. These algorithms have Bayesian and frequentist regret bounds, respectively, and both reduce to the near-optimal bound $\widetilde{O}(DS\sqrt{AT})$ for standard non-factored MDPs when assuming oracle access to an FMDP planner. We introduce a more precise measure of connectivity, called factored span, for FMDPs, and establish a lower bound that is based on factored span instead of diameter $D$. Additionally, we propose a modification of the REGAL.C algorithm that reduces the gap between lower and upper bounds, and whose regret bound is dependent on factored span. Our oracle-efficient algorithms are superior to previously proposed near-optimal algorithms in computer network administration simulations.",1
"Reinforcement Learning (RL) based methods have seen their paramount successes in solving serial decision-making and control problems in recent years. For conventional RL formulations, Markov Decision Process (MDP) and state-action-value function are the basis for the problem modeling and policy evaluation. However, several challenging issues still remain. Among most cited issues, the enormity of state/action space is an important factor that causes inefficiency in accurately approximating the state-action-value function. We observe that although actions directly define the agents' behaviors, for many problems the next state after a state transition matters more than the action taken, in determining the return of such a state transition. In this regard, we propose a new learning paradigm, State Action Separable Reinforcement Learning (sasRL), wherein the action space is decoupled from the value function learning process for higher efficiency. Then, a light-weight transition model is learned to assist the agent to determine the action that triggers the associated state transition. In addition, our convergence analysis reveals that under certain conditions, the convergence time of sasRL is $O(T^{1/k})$, where $T$ is the convergence time for updating the value function in the MDP-based formulation and $k$ is a weighting factor. Experiments on several gaming scenarios show that sasRL outperforms state-of-the-art MDP-based RL algorithms by up to $75\%$.",0
"In recent years, Reinforcement Learning (RL) methods have achieved remarkable success in solving control and decision-making problems. The conventional RL approach involves using Markov Decision Process (MDP) and state-action-value function to model and evaluate policies. However, challenges such as the vastness of state/action space still hinder its effectiveness in accurately approximating the state-action-value function. It is observed that for many problems, the subsequent state after a transition is more critical in determining the return of such a transition than the action taken. To address this issue, a novel learning paradigm, State Action Separable Reinforcement Learning (sasRL), is proposed to decouple the action space from the value function learning process for better efficiency. A lightweight transition model is learned to help the agent determine the action that triggers the relevant state transition. Furthermore, the convergence analysis shows that under specific conditions, sasRL's convergence time is $O(T^{1/k})$, where $T$ is the convergence time for updating the value function in the MDP-based formulation, and $k$ is a weighting factor. Experiments conducted on various gaming scenarios demonstrate that sasRL outperforms the state-of-the-art MDP-based RL algorithms by up to $75\%$.",1
"We discuss the problem of learning collaborative behaviour through communication in multi-agent systems using deep reinforcement learning. A connectivity-driven communication (CDC) algorithm is proposed to address three key aspects: what agents to involve in the communication, what information content to share, and how often to share it. The multi-agent system is modelled as a weighted graph with nodes representing agents. The unknown edge weights reflect the degree of communication between pairs of agents, which depends on a diffusion process on the graph - the heat kernel. An optimal communication strategy, tightly coupled with overall graph topology, is learned end-to-end concurrently with the agents' policy so as to maximise future expected returns. Empirical results show that CDC is capable of superior performance over alternative algorithms for a range of cooperative navigation tasks, and that the learned graph structures can be interpretable.",0
"The focus of our discussion is on the use of deep reinforcement learning to address the challenge of learning collaborative behavior in multi-agent systems via communication. To tackle this issue, we propose an algorithm called connectivity-driven communication (CDC) that deals with three main aspects: selecting which agents to include in the communication, determining what information should be shared, and deciding how often to share it. Our approach models the multi-agent system as a weighted graph consisting of nodes that represent agents. The edge weights are unknown and reflect the level of communication between agent pairs, which is determined by a diffusion process known as the heat kernel. By concurrently learning an optimal communication strategy and the agents' policy, we aim to maximize future expected returns, taking into account the overall graph topology. Our experimental results demonstrate that CDC outperforms alternative algorithms in various cooperative navigation tasks and that the learned graph structures are interpretable.",1
"Entropy augmented to reward is known to soften the greedy argmax policy to softmax policy. Entropy augmentation is reformulated and leads to a motivation to introduce an additional entropy term to the objective function in the form of KL-divergence to regularize optimization process. It results in a policy which monotonically improves while interpolating from the current policy to the softmax greedy policy. This policy is used to build a continuously parameterized algorithm which optimize policy and Q-function simultaneously and whose extreme limits correspond to policy gradient and Q-learning, respectively. Experiments show that there can be a performance gain using an intermediate algorithm.",0
"Adding entropy to the reward has been shown to transform the greedy argmax policy into a softmax policy that is more lenient. A new formulation of entropy augmentation has emerged, prompting the inclusion of an additional entropy term in the objective function through KL-divergence to regulate the optimization process. This leads to a policy that steadily improves while gradually transitioning from the current policy to the softmax greedy policy. This policy is employed in the creation of an algorithm that is continuously parameterized and optimizes both the policy and Q-function simultaneously, with its extremes corresponding to policy gradient and Q-learning, respectively. Experiments reveal that utilizing an intermediary algorithm can result in improved performance.",1
"Enterprise Wireless Local Area Networks (WLANs) consist of multiple Access Points (APs) covering a given area. Finding a suitable network configuration able to maximize the performance of enterprise WLANs is a challenging task given the complex dependencies between APs and stations. Recently, in wireless networking, the use of reinforcement learning techniques has emerged as an effective solution to efficiently explore the impact of different network configurations in the system performance, identifying those that provide better performance. In this paper, we study if Multi-Armed Bandits (MABs) are able to offer a feasible solution to the decentralized channel allocation and AP selection problems in Enterprise WLAN scenarios. To do so, we empower APs and stations with agents that, by means of implementing the Thompson sampling algorithm, explore and learn which is the best channel to use, and which is the best AP to associate, respectively. Our evaluation is performed over randomly generated scenarios, which enclose different network topologies and traffic loads. The presented results show that the proposed adaptive framework using MABs outperform the static approach (i.e., using always the initial default configuration, usually random) regardless of the network density and the traffic requirements. Moreover, we show that the use of the proposed framework reduces the performance variability between different scenarios. Results also show that we achieve the same performance (or better) than static strategies with less APs for the same number of stations. Finally, special attention is placed on how the agents interact. Even if the agents operate in a completely independent manner, their decisions have interrelated effects, as they take actions over the same set of channel resources.",0
"The challenge of maximizing the performance of Enterprise Wireless Local Area Networks (WLANs) lies in the complex dependencies between multiple Access Points (APs) and stations. A recent solution to this challenge is the use of reinforcement learning techniques in wireless networking, which efficiently explores different network configurations to identify those that provide better performance. In this study, we investigate if Multi-Armed Bandits (MABs) can solve the decentralized channel allocation and AP selection problems in Enterprise WLAN scenarios. We equip APs and stations with agents that use the Thompson sampling algorithm to determine the best channel and AP association. Our evaluation is performed on different network topologies and traffic loads, and the results show that our adaptive MAB framework outperforms the static approach, reduces performance variability, and achieves the same (or better) performance with fewer APs. We also examine how the agents interact and their interrelated effects on the channel resources.",1
"Current deep learning based autonomous driving approaches yield impressive results also leading to in-production deployment in certain controlled scenarios. One of the most popular and fascinating approaches relies on learning vehicle controls directly from data perceived by sensors. This end-to-end learning paradigm can be applied both in classical supervised settings and using reinforcement learning. Nonetheless the main drawback of this approach as also in other learning problems is the lack of explainability. Indeed, a deep network will act as a black-box outputting predictions depending on previously seen driving patterns without giving any feedback on why such decisions were taken. While to obtain optimal performance it is not critical to obtain explainable outputs from a learned agent, especially in such a safety critical field, it is of paramount importance to understand how the network behaves. This is particularly relevant to interpret failures of such systems. In this work we propose to train an imitation learning based agent equipped with an attention model. The attention model allows us to understand what part of the image has been deemed most important. Interestingly, the use of attention also leads to superior performance in a standard benchmark using the CARLA driving simulator.",0
"Autonomous driving techniques based on deep learning have shown impressive results and are being deployed in controlled scenarios. One approach that has gained popularity is learning vehicle controls directly from sensor data using an end-to-end learning paradigm, which can be applied in supervised and reinforcement learning settings. However, the main drawback of this approach, as with other learning problems, is the lack of explainability. Deep networks act as black boxes, making decisions based on previously seen driving patterns without providing any feedback on the reasoning behind them. While explainable outputs are not critical for optimal performance, it is important to understand how the network behaves, especially in safety-critical fields such as autonomous driving. To address this, we propose training an imitation learning based agent equipped with an attention model. This allows us to understand which parts of the image are most important and leads to superior performance in a standard benchmark using the CARLA driving simulator.",1
"Off-policy evaluation (OPE) in reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. We consider for the first time the semiparametric efficiency limits of OPE in Markov decision processes (MDPs), where actions, rewards, and states are memoryless. We show existing OPE estimators may fail to be efficient in this setting. We develop a new estimator based on cross-fold estimation of $q$-functions and marginalized density ratios, which we term double reinforcement learning (DRL). We show that DRL is efficient when both components are estimated at fourth-root rates and is also doubly robust when only one component is consistent. We investigate these properties empirically and demonstrate the performance benefits due to harnessing memorylessness.",0
"Reinforcement learning uses off-policy evaluation (OPE) to evaluate new decision policies without the need for costly exploration. In this study, we examine the efficiency limits of OPE in Markov decision processes (MDPs) where actions, rewards, and states are memoryless. Our findings reveal that existing OPE methods may not be efficient in this scenario. To address this, we introduce a new estimator called Double Reinforcement Learning (DRL), which utilizes cross-fold estimation of $q$-functions and marginalized density ratios. Our research demonstrates that DRL is efficient when both components are estimated at fourth-root rates and is doubly robust when only one component is consistent. We also present empirical evidence of the performance benefits of exploiting memorylessness.",1
"We introduce Wasserstein Adversarial Proximal Policy Optimization (WAPPO), a novel algorithm for visual transfer in Reinforcement Learning that explicitly learns to align the distributions of extracted features between a source and target task. WAPPO approximates and minimizes the Wasserstein-1 distance between the distributions of features from source and target domains via a novel Wasserstein Confusion objective. WAPPO outperforms the prior state-of-the-art in visual transfer and successfully transfers policies across Visual Cartpole and two instantiations of 16 OpenAI Procgen environments.",0
"A new algorithm called Wasserstein Adversarial Proximal Policy Optimization (WAPPO) has been introduced for Reinforcement Learning, which focuses on visual transfer. The algorithm aims to align the distributions of extracted features between a source and target task and utilizes a novel Wasserstein Confusion objective to approximate and minimize the Wasserstein-1 distance between the two domains. The algorithm has shown better performance than the previous state-of-the-art in visual transfer and has successfully transferred policies across Visual Cartpole and two instantiations of 16 OpenAI Procgen environments.",1
"In this paper, we propose enhancing actor-critic reinforcement learning agents by parameterising the final actor layer which produces the actions in order to accommodate the behaviour discrepancy of different actuators, under different load conditions during interaction with the environment. We propose branching the action producing layer in the actor to learn the tuning parameter controlling the activation layer (e.g. Tanh and Sigmoid). The learned parameters are then used to create tailored activation functions for each actuator. We ran experiments on three OpenAI Gym environments, i.e. Pendulum-v0, LunarLanderContinuous-v2 and BipedalWalker-v2. Results have shown an average of 23.15% and 33.80% increase in total episode reward of the LunarLanderContinuous-v2 and BipedalWalker-v2 environments, respectively. There was no significant improvement in Pendulum-v0 environment but the proposed method produces a more stable actuation signal compared to the state-of-the-art method. The proposed method allows the reinforcement learning actor to produce more robust actions that accommodate the discrepancy in the actuators' response functions. This is particularly useful for real life scenarios where actuators exhibit different response functions depending on the load and the interaction with the environment. This also simplifies the transfer learning problem by fine tuning the parameterised activation layers instead of retraining the entire policy every time an actuator is replaced. Finally, the proposed method would allow better accommodation to biological actuators (e.g. muscles) in biomechanical systems.",0
"The aim of this research is to improve actor-critic reinforcement learning agents by parameterising the final actor layer that produces actions. This is to account for the different behaviour discrepancies of various actuators under different load conditions when interacting with the environment. To achieve this, the action producing layer in the actor is branched, and the tuning parameter controlling the activation layer is learnt. The learnt parameters are then used to create tailored activation functions for each actuator. Experiments were conducted on three OpenAI Gym environments, namely Pendulum-v0, LunarLanderContinuous-v2 and BipedalWalker-v2, with results showing an average increase of 23.15% and 33.80% in total episode reward of the LunarLanderContinuous-v2 and BipedalWalker-v2 environments, respectively. Although there was no significant improvement in the Pendulum-v0 environment, the proposed method was able to produce a more stable actuation signal compared to the state-of-the-art method. This method enables the reinforcement learning actor to produce more robust actions that accommodate the discrepancy in the actuators' response functions. It also makes transfer learning easier by fine-tuning the parameterised activation layers instead of retraining the entire policy every time an actuator is replaced. Furthermore, this method can be applied to biological actuators, such as muscles, in biomechanical systems.",1
"Dynamic real-time optimization (DRTO) is a challenging task due to the fact that optimal operating conditions must be computed in real time. The main bottleneck in the industrial application of DRTO is the presence of uncertainty. Many stochastic systems present the following obstacles: 1) plant-model mismatch, 2) process disturbances, 3) risks in violation of process constraints. To accommodate these difficulties, we present a constrained reinforcement learning (RL) based approach. RL naturally handles the process uncertainty by computing an optimal feedback policy. However, no state constraints can be introduced intuitively. To address this problem, we present a chance-constrained RL methodology. We use chance constraints to guarantee the probabilistic satisfaction of process constraints, which is accomplished by introducing backoffs, such that the optimal policy and backoffs are computed simultaneously. Backoffs are adjusted using the empirical cumulative distribution function to guarantee the satisfaction of a joint chance constraint. The advantage and performance of this strategy are illustrated through a stochastic dynamic bioprocess optimization problem, to produce sustainable high-value bioproducts.",0
"Optimizing processes in real time is difficult in dynamic real-time optimization (DRTO) due to the need to compute optimal operating conditions in real-time and the presence of uncertainty. Uncertainty presents several obstacles in stochastic systems, such as plant-model mismatch, process disturbances, and risks of process constraint violations. To address these difficulties, a constrained reinforcement learning (RL) based approach is proposed. RL naturally handles process uncertainty by computing an optimal feedback policy, but state constraints cannot be introduced intuitively. To solve this problem, a chance-constrained RL methodology is presented, which uses chance constraints to guarantee the probabilistic satisfaction of process constraints. Backoffs are introduced, and the optimal policy and backoffs are computed simultaneously. The empirical cumulative distribution function is used to adjust backoffs, ensuring the satisfaction of a joint chance constraint. The effectiveness of this strategy is demonstrated through a stochastic dynamic bioprocess optimization problem aimed at producing sustainable high-value bioproducts.",1
"Graphs neural networks (GNNs) learn node features by aggregating and combining neighbor information, which have achieved promising performance on many graph tasks. However, GNNs are mostly treated as black-boxes and lack human intelligible explanations. Thus, they cannot be fully trusted and used in certain application domains if GNN models cannot be explained. In this work, we propose a novel approach, known as XGNN, to interpret GNNs at the model-level. Our approach can provide high-level insights and generic understanding of how GNNs work. In particular, we propose to explain GNNs by training a graph generator so that the generated graph patterns maximize a certain prediction of the model.We formulate the graph generation as a reinforcement learning task, where for each step, the graph generator predicts how to add an edge into the current graph. The graph generator is trained via a policy gradient method based on information from the trained GNNs. In addition, we incorporate several graph rules to encourage the generated graphs to be valid. Experimental results on both synthetic and real-world datasets show that our proposed methods help understand and verify the trained GNNs. Furthermore, our experimental results indicate that the generated graphs can provide guidance on how to improve the trained GNNs.",0
"Graph neural networks (GNNs) have shown great promise in achieving high performance on graph tasks by learning node features through the combination and aggregation of neighbor information. However, due to their black-box nature, GNNs lack human intelligible explanations, making them unreliable for use in certain applications. To address this issue, we present a novel approach called XGNN, which enables the interpretation of GNNs at the model-level. With XGNN, GNNs can be explained by training a graph generator to produce graph patterns that maximize a certain prediction of the model. We employ a reinforcement learning approach for graph generation, with the graph generator predicting how to add an edge into the current graph at each step. The generator is trained using a policy gradient method based on information from the trained GNNs, while incorporating graph rules to ensure the generated graphs are valid. Experimental results on both synthetic and real-world datasets demonstrate the efficacy of our approach for understanding and verifying trained GNNs, and suggest that the generated graphs can provide guidance on how to improve GNN performance.",1
"Reinforcement learning algorithms have had tremendous successes in online learning settings. However, these successes have relied on low-stakes interactions between the algorithmic agent and its environment. In many settings where RL could be of use, such as health care and autonomous driving, the mistakes made by most online RL algorithms during early training come with unacceptable costs. These settings require developing reinforcement learning algorithms that can operate in the so-called batch setting, where the algorithms must learn from set of data that is fixed, finite, and generated from some (possibly unknown) policy. Evaluating policies different from the one that collected the data is called off-policy evaluation, and naturally poses counter-factual questions. In this project we show how off-policy evaluation and the estimation of treatment effects in causal inference are two approaches to the same problem, and compare recent progress in these two areas.",0
"Online learning settings have seen remarkable achievements through reinforcement learning algorithms, but their efficacy has been limited to situations where the algorithmic agent and its environment engage in low-stakes interactions. In numerous contexts like autonomous driving and health care, however, the errors made by most online RL algorithms during early training are too costly. Thus, it becomes essential to develop reinforcement learning algorithms that can function in the batch setting, where the algorithms learn from a fixed, finite set of data generated from some unknown policy. This requires off-policy evaluation, which raises counterfactual questions. Our project demonstrates how off-policy evaluation and the estimation of treatment effects in causal inference are two methods to address this issue, and we compare the latest advancements in these two fields.",1
"Machine learning approaches have seen considerable applications in human movement modeling, but remain limited for motor learning. Motor learning requires accounting for motor variability, and poses new challenges as the algorithms need to be able to differentiate between new movements and variation of known ones. In this short review, we outline existing machine learning models for motor learning and their adaptation capabilities. We identify and describe three types of adaptation: Parameter adaptation in probabilistic models, Transfer and meta-learning in deep neural networks, and Planning adaptation by reinforcement learning. To conclude, we discuss challenges for applying these models in the domain of motor learning support systems.",0
"Although machine learning methods have been widely utilized in modeling human movement, their usefulness in motor learning is somewhat restricted. This is due to the need to factor in motor variability during motor learning, which presents fresh challenges as the algorithms must differentiate between novel movements and variations on established ones. In this brief overview, we examine the current machine learning models for motor learning and their flexibility in adaptation. We recognize and explain three types of adaptation: probabilistic models' parameter adaptation, deep neural networks' transfer and meta-learning, and reinforcement learning's planning adaptation. Finally, we address the difficulties associated with implementing these models in the field of motor learning support systems.",1
"Reinforcement learning can interact with the environment and is suitable for applications in decision control systems. Therefore, we used the reinforcement learning method to establish a foreign exchange transaction, avoiding the long-standing problem of unstable trends in deep learning predictions. In the system design, we optimized the Sure-Fire statistical arbitrage policy, set three different actions, encoded the continuous price over a period of time into a heat-map view of the Gramian Angular Field (GAF) and compared the Deep Q Learning (DQN) and Proximal Policy Optimization (PPO) algorithms. To test feasibility, we analyzed three currency pairs, namely EUR/USD, GBP/USD, and AUD/USD. We trained the data in units of four hours from 1 August 2018 to 30 November 2018 and tested model performance using data between 1 December 2018 and 31 December 2018. The test results of the various models indicated that favorable investment performance was achieved as long as the model was able to handle complex and random processes and the state was able to describe the environment, validating the feasibility of reinforcement learning in the development of trading strategies.",0
"We utilized reinforcement learning as it is capable of interacting with the environment and suitable for decision control systems. This allowed us to establish a foreign exchange transaction, addressing the issue of unstable trends in deep learning predictions. Our system design involved optimizing the Sure-Fire statistical arbitrage policy, creating a heat-map view of the Gramian Angular Field (GAF) to encode continuous prices, and comparing the Deep Q Learning (DQN) and Proximal Policy Optimization (PPO) algorithms. To assess feasibility, we analyzed the EUR/USD, GBP/USD, and AUD/USD currency pairs, training the data from August 1, 2018, to November 30, 2018, and testing the model using data from December 1, 2018, to December 31, 2018. The favorable investment performance of the various models indicated that reinforcement learning is a viable approach in developing trading strategies, provided the model can handle complex and random processes and the state can describe the environment.",1
"Recent work on exploration in reinforcement learning (RL) has led to a series of increasingly complex solutions to the problem. This increase in complexity often comes at the expense of generality. Recent empirical studies suggest that, when applied to a broader set of domains, some sophisticated exploration methods are outperformed by simpler counterparts, such as {\epsilon}-greedy. In this paper we propose an exploration algorithm that retains the simplicity of {\epsilon}-greedy while reducing dithering. We build on a simple hypothesis: the main limitation of {\epsilon}-greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. We propose a temporally extended form of {\epsilon}-greedy that simply repeats the sampled action for a random duration. It turns out that, for many duration distributions, this suffices to improve exploration on a large set of domains. Interestingly, a class of distributions inspired by ecological models of animal foraging behaviour yields particularly strong performance.",0
"Exploration in reinforcement learning (RL) has been the focus of recent research, resulting in increasingly complex solutions. However, these complex methods often lack generality. Empirical studies have shown that simpler methods, such as {\epsilon}-greedy, outperform sophisticated exploration methods when applied to a broader range of domains. This paper introduces a new exploration algorithm that reduces dithering while retaining the simplicity of {\epsilon}-greedy. The algorithm is based on the hypothesis that the main limitation of {\epsilon}-greedy exploration is its lack of temporal persistence, which hinders its ability to escape local optima. We propose a temporally extended version of {\epsilon}-greedy that repeats the sampled action for a random duration. This method significantly improves exploration on a wide range of domains, particularly when using duration distributions inspired by ecological models of animal foraging behavior.",1
"Goal-conditioned policies are used in order to break down complex reinforcement learning (RL) problems by using subgoals, which can be defined either in state space or in a latent feature space. This can increase the efficiency of learning by using a curriculum, and also enables simultaneous learning and generalization across goals. A crucial requirement of goal-conditioned policies is to be able to determine whether the goal has been achieved. Having a notion of distance to a goal is thus a crucial component of this approach. However, it is not straightforward to come up with an appropriate distance, and in some tasks, the goal space may not even be known a priori. In this work we learn a distance-to-goal estimate which is computed in terms of the number of actions that would need to be carried out in a self-supervised approach. Our method solves complex tasks without prior domain knowledge in the online setting in three different scenarios in the context of goal-conditioned policies a) the goal space is the same as the state space b) the goal space is given but an appropriate distance is unknown and c) the state space is accessible, but only a subset of the state space represents desired goals, and this subset is known a priori. We also propose a goal-generation mechanism as a secondary contribution.",0
"The use of goal-conditioned policies is employed to simplify complex reinforcement learning (RL) problems by breaking them down into subgoals, which can be defined in either state space or a latent feature space. This approach improves learning efficiency by using a curriculum and allows for simultaneous learning and generalization across goals. A critical aspect of goal-conditioned policies is the ability to determine if the goal has been achieved, which requires an appropriate distance measure. However, this can be challenging in some tasks where the goal space is unknown. In our study, we develop a self-supervised approach to estimate the distance-to-goal based on the number of actions required. Our approach successfully solves complex tasks in three scenarios, including cases where the goal space is the same as the state space, the appropriate distance is unknown, and only a subset of the state space represents desired goals. Additionally, we propose a goal-generation mechanism as a secondary contribution.",1
"This paper studies model-based reinforcement learning (RL) for regret minimization. We focus on finite-horizon episodic RL where the transition model $P$ belongs to a known family of models $\mathcal{P}$, a special case of which is when models in $\mathcal{P}$ take the form of linear mixtures: $P_{\theta} = \sum_{i=1}^{d} \theta_{i}P_{i}$. We propose a model based RL algorithm that is based on optimism principle: In each episode, the set of models that are `consistent' with the data collected is constructed. The criterion of consistency is based on the total squared error of that the model incurs on the task of predicting \emph{values} as determined by the last value estimate along the transitions. The next value function is then chosen by solving the optimistic planning problem with the constructed set of models. We derive a bound on the regret, which, in the special case of linear mixtures, the regret bound takes the form $\tilde{\mathcal{O}}(d\sqrt{H^{3}T})$, where $H$, $T$ and $d$ are the horizon, total number of steps and dimension of $\theta$, respectively. In particular, this regret bound is independent of the total number of states or actions, and is close to a lower bound $\Omega(\sqrt{HdT})$. For a general model family $\mathcal{P}$, the regret bound is derived using the notion of the so-called Eluder dimension proposed by Russo & Van Roy (2014).",0
"The aim of this study is to investigate model-based reinforcement learning for the purpose of minimizing regret. The focus is on finite-horizon episodic RL where the transition model $P$ is a member of a known family of models $\mathcal{P}$. One type of model in $\mathcal{P}$ is a linear mixture: $P_{\theta} = \sum_{i=1}^{d} \theta_{i}P_{i}$. The proposed RL algorithm follows the optimism principle and constructs a set of models that are ""consistent"" with the collected data in each episode. Consistency is determined based on the total squared error of the model's ability to predict values using the last value estimate along the transitions. The next value function is then chosen by solving the optimistic planning problem with the constructed set of models. A regret bound is derived, which is $\tilde{\mathcal{O}}(d\sqrt{H^{3}T})$ for the special case of linear mixtures. This bound is independent of the number of states or actions and is close to the lower bound $\Omega(\sqrt{HdT})$. For a general model family $\mathcal{P}$, the regret bound is determined using the Eluder dimension concept introduced by Russo & Van Roy (2014).",1
"Model-informed precision dosing (MIPD) using therapeutic drug/biomarker monitoring offers the opportunity to significantly improve the efficacy and safety of drug therapies. Current strategies comprise model-informed dosing tables or are based on maximum a-posteriori estimates. These approaches, however, lack a quantification of uncertainty and/or consider only part of the available patient-specific information. We propose three novel approaches for MIPD employing Bayesian data assimilation (DA) and/or reinforcement learning (RL) to control neutropenia, the major dose-limiting side effect in anticancer chemotherapy. These approaches have the potential to substantially reduce the incidence of life-threatening grade 4 and subtherapeutic grade 0 neutropenia compared to existing approaches. We further show that RL allows to gain further insights by identifying patient factors that drive dose decisions. Due to its flexibility, the proposed combined DA-RL approach can easily be extended to integrate multiple endpoints or patient-reported outcomes, thereby promising important benefits for future personalized therapies.",0
"The utilization of therapeutic drug/biomarker monitoring in Model-informed precision dosing (MIPD) presents an opportunity to enhance the effectiveness and safety of drug therapies. At present, MIPD strategies entail model-informed dosing tables or rely on maximum a-posteriori estimates. However, these methods do not take into account the complete patient-specific information or quantify uncertainty. We suggest three new MIPD approaches that use Bayesian data assimilation (DA) and/or reinforcement learning (RL) to regulate neutropenia, which is the primary dose-limiting side effect in anticancer chemotherapy. These approaches can considerably diminish the occurrence of life-threatening grade 4 and subtherapeutic grade 0 neutropenia compared to current methods. Additionally, we demonstrate that RL enables the identification of patient factors that influence dose decisions. The proposed DA-RL approach is highly adaptable and can readily incorporate multiple endpoints or patient-reported outcomes, offering significant potential for future personalized therapies.",1
"In this paper, a new reinforcement learning (RL) method known as the method of temporal differential is introduced. Compared to the traditional temporal-difference learning method, it plays a crucial role in developing novel RL techniques for continuous environments. In particular, the continuous-time least squares policy evaluation (CT-LSPE) and the continuous-time temporal-differential (CT-TD) learning methods are developed. Both theoretical and empirical evidences are provided to demonstrate the effectiveness of the proposed temporal-differential learning methodology.",0
"This paper presents a novel reinforcement learning (RL) approach called the temporal differential method. This technique is essential for developing new RL strategies for continuous environments, and is compared to the traditional temporal-difference learning method. The continuous-time least squares policy evaluation (CT-LSPE) and continuous-time temporal-differential (CT-TD) learning methods are created as a result. The effectiveness of the temporal-differential learning methodology is demonstrated through both theoretical and empirical evidence.",1
"Deep reinforcement learning has led to many recent-and groundbreaking-advancements. However, these advances have often come at the cost of both the scale and complexity of the underlying RL algorithms. Increases in complexity have in turn made it more difficult for researchers to reproduce published RL algorithms or rapidly prototype ideas. To address this, we introduce Acme, a tool to simplify the development of novel RL algorithms that is specifically designed to enable simple agent implementations that can be run at various scales of execution. Our aim is also to make the results of various RL algorithms developed in academia and industrial labs easier to reproduce and extend. To this end we are releasing baseline implementations of various algorithms, created using our framework. In this work we introduce the major design decisions behind Acme and show how these are used to construct these baselines. We also experiment with these agents at different scales of both complexity and computation-including distributed versions. Ultimately, we show that the design decisions behind Acme lead to agents that can be scaled both up and down and that, for the most part, greater levels of parallelization result in agents with equivalent performance, just faster.",0
"The current advancements in deep reinforcement learning are groundbreaking, but they often come with a trade-off between complexity and scale of the underlying algorithms. This increasing complexity poses a challenge for researchers to reproduce or quickly test new ideas. To tackle this issue, we introduce Acme, a tool that simplifies the development of novel RL algorithms. Acme enables simple agent implementations that can be executed at various scales, facilitating the reproducibility and extension of RL algorithms developed in industrial and academic settings. We provide baseline implementations of various algorithms, created using our framework, to support this goal. We explain the design decisions behind Acme and demonstrate how these are used to construct these baselines. We also experiment with these agents at different scales of complexity and computation, including distributed versions. Our results show that Acme's design decisions enable agents that can be scaled both up and down. Furthermore, parallelization improves agent performance without compromising their effectiveness.",1
"Robust Reinforcement Learning aims to find the optimal policy with some extent of robustness to environmental dynamics. Existing learning algorithms usually enable the robustness through disturbing the current state or simulating environmental parameters in a heuristic way, which lack quantified robustness to the system dynamics (i.e. transition probability). To overcome this issue, we leverage Wasserstein distance to measure the disturbance to the reference transition kernel. With Wasserstein distance, we are able to connect transition kernel disturbance to the state disturbance, i.e. reduce an infinite-dimensional optimization problem to a finite-dimensional risk-aware problem. Through the derived risk-aware optimal Bellman equation, we show the existence of optimal robust policies, provide a sensitivity analysis for the perturbations, and then design a novel robust learning algorithm--Wasserstein Robust Advantage Actor-Critic algorithm (WRAAC). The effectiveness of the proposed algorithm is verified in the Cart-Pole environment.",0
"The aim of Robust Reinforcement Learning is to find the best policy that is somewhat resistant to environmental changes. Current learning algorithms often achieve this by disrupting the current state or simulating environmental variables in a vague way, which does not adequately account for robustness to the system's dynamics. To address this shortcoming, we utilize the Wasserstein distance to quantify the disturbance to the reference transition kernel. By connecting the disturbance to the state through this metric, we simplify an infinite-dimensional optimization problem to a finite-dimensional risk-aware one. Using the risk-aware optimal Bellman equation, we prove that optimal robust policies exist, conduct a sensitivity analysis for perturbations, and create a new robust learning algorithm called the Wasserstein Robust Advantage Actor-Critic algorithm (WRAAC). We demonstrate the effectiveness of this algorithm in the Cart-Pole environment.",1
"Learning with sparse rewards remains a significant challenge in reinforcement learning (RL), especially when the aim is to train a policy capable of achieving multiple different goals. To date, the most successful approaches for dealing with multi-goal, sparse reward environments have been model-free RL algorithms. In this work we propose PlanGAN, a model-based algorithm specifically designed for solving multi-goal tasks in environments with sparse rewards. Our method builds on the fact that any trajectory of experience collected by an agent contains useful information about how to achieve the goals observed during that trajectory. We use this to train an ensemble of conditional generative models (GANs) to generate plausible trajectories that lead the agent from its current state towards a specified goal. We then combine these imagined trajectories into a novel planning algorithm in order to achieve the desired goal as efficiently as possible. The performance of PlanGAN has been tested on a number of robotic navigation/manipulation tasks in comparison with a range of model-free reinforcement learning baselines, including Hindsight Experience Replay. Our studies indicate that PlanGAN can achieve comparable performance whilst being around 4-8 times more sample efficient.",0
"Reinforcement learning (RL) faces a significant challenge in learning with sparse rewards, particularly in training a policy to achieve multiple goals. Model-free RL algorithms have been the most successful in addressing multi-goal, sparse reward environments so far. To tackle this issue, we present PlanGAN, a model-based algorithm specifically designed for solving multi-goal tasks in environments with sparse rewards. Our approach capitalizes on the information contained in any agent-collected experience trajectory regarding goal achievement. We train an ensemble of conditional generative models (GANs) to generate plausible trajectories that guide the agent towards a specified goal. We then use these trajectories to develop a new planning algorithm that achieves the desired goal as efficiently as possible. PlanGAN has been tested on various robotic navigation and manipulation tasks and compared to a range of model-free reinforcement learning baselines, including Hindsight Experience Replay. Our studies demonstrate that PlanGAN can achieve similar performance while requiring 4-8 times fewer samples.",1
"Adversarial examples are firstly investigated in the area of computer vision: by adding some carefully designed ''noise'' to the original input image, the perturbed image that cannot be distinguished from the original one by human, can fool a well-trained classifier easily. In recent years, researchers also demonstrated that adversarial examples can mislead deep reinforcement learning (DRL) agents on playing video games using image inputs with similar methods. However, although DRL has been more and more popular in the area of intelligent transportation systems, there is little research investigating the impacts of adversarial attacks on them, especially for algorithms that do not take images as inputs. In this work, we investigated several fast methods to generate adversarial examples to significantly degrade the performance of a well-trained DRL- based energy management system of an extended range electric delivery vehicle. The perturbed inputs are low-dimensional state representations and close to the original inputs quantified by different kinds of norms. Our work shows that, to apply DRL agents on real-world transportation systems, adversarial examples in the form of cyber-attack should be considered carefully, especially for applications that may lead to serious safety issues.",0
"Initially, adversarial examples were studied in computer vision. These examples involve adding purposeful ""noise"" to an image to create a perturbed image that is indistinguishable from the original to the human eye. This can easily deceive a well-trained classifier. Recently, researchers have found that using similar methods, adversarial examples can also mislead deep reinforcement learning (DRL) agents during video game play. Despite the growing popularity of DRL in intelligent transportation systems, there is limited research on the effects of adversarial attacks on these systems, particularly those that do not use images as inputs. In this study, we explored several fast methods to generate adversarial examples that significantly reduce the performance of a well-trained DRL-based energy management system in an extended range electric delivery vehicle. Our work demonstrates that the use of DRL agents in real-world transportation systems requires careful consideration of adversarial examples in the form of cyber-attacks, particularly in applications that could cause serious safety concerns. The perturbed inputs in our study were low-dimensional state representations that were quantified using different norm types and were close to the original inputs.",1
"When function approximation is deployed in reinforcement learning (RL), the same problem may be formulated in different ways, often by treating a pre-processing step as a part of the environment or as part of the agent. As a consequence, fundamental concepts in RL, such as (optimal) value functions, are not uniquely defined as they depend on where we draw this agent-environment boundary, causing problems in theoretical analyses that provide optimality guarantees. We address this issue via a simple and novel boundary-invariant analysis of Fitted Q-Iteration, a representative RL algorithm, where the assumptions and the guarantees are invariant to the choice of boundary. We also discuss closely related issues on state resetting and Monte-Carlo Tree Search, deterministic vs stochastic systems, imitation learning, and the verifiability of theoretical assumptions from data.",0
"In reinforcement learning (RL), function approximation can present the same problem in different ways, with a pre-processing step being treated as part of the environment or the agent. This leads to non-uniquely defined fundamental concepts in RL, like optimal value functions, that depend on where the agent-environment boundary is drawn, causing issues in theoretical analyses that guarantee optimality. Our solution is a novel boundary-invariant analysis of Fitted Q-Iteration, an RL algorithm, where the assumptions and guarantees remain invariant regardless of the boundary choice. We also explore other related issues, such as state resetting, Monte-Carlo Tree Search, deterministic versus stochastic systems, imitation learning, and the verifiability of theoretical assumptions from data.",1
"Coverage path planning is a well-studied problem in robotics in which a robot must plan a path that passes through every point in a given area repeatedly, usually with a uniform frequency. To address the scenario in which some points need to be visited more frequently than others, this problem has been extended to non-uniform coverage planning. This paper considers the variant of non-uniform coverage in which the robot does not know the distribution of relevant events beforehand and must nevertheless learn to maximize the rate of detecting events of interest. This continual area sweeping problem has been previously formalized in a way that makes strong assumptions about the environment, and to date only a greedy approach has been proposed. We generalize the continual area sweeping formulation to include fewer environmental constraints, and propose a novel approach based on reinforcement learning in a Semi-Markov Decision Process. This approach is evaluated in an abstract simulation and in a high fidelity Gazebo simulation. These evaluations show significant improvement upon the existing approach in general settings, which is especially relevant in the growing area of service robotics.",0
"The problem of coverage path planning is extensively researched in robotics, wherein a robot needs to plan a path that covers every point in a given area repeatedly at a uniform frequency. To cater to the situation where some points require more frequent visits than others, the issue has been extended to non-uniform coverage planning. This paper focuses on the non-uniform coverage variant where the robot is unaware of the distribution of relevant events and needs to learn to maximize the rate of detecting such events. The problem of continual area sweeping has been previously formalized with strong environment assumptions, and only a greedy approach has been suggested so far. To overcome this, we have generalized the continual area sweeping problem with fewer environmental constraints and proposed a new approach based on reinforcement learning in a Semi-Markov Decision Process. Our approach is evaluated in both an abstract and high fidelity Gazebo simulation, and the results demonstrate a significant improvement over the existing approach, particularly in the service robotics field.",1
"There has been an increasing surge of interest on development of advanced Reinforcement Learning (RL) systems as intelligent approaches to learn optimal control policies directly from smart agents' interactions with the environment. Objectives: In a model-free RL method with continuous state-space, typically, the value function of the states needs to be approximated. In this regard, Deep Neural Networks (DNNs) provide an attractive modeling mechanism to approximate the value function using sample transitions. DNN-based solutions, however, suffer from high sensitivity to parameter selection, are prone to overfitting, and are not very sample efficient. A Kalman-based methodology, on the other hand, could be used as an efficient alternative. Such an approach, however, commonly requires a-priori information about the system (such as noise statistics) to perform efficiently. The main objective of this paper is to address this issue. Methods: As a remedy to the aforementioned problems, this paper proposes an innovative Multiple Model Kalman Temporal Difference (MM-KTD) framework, which adapts the parameters of the filter using the observed states and rewards. Moreover, an active learning method is proposed to enhance the sampling efficiency of the system. More specifically, the estimated uncertainty of the value functions are exploited to form the behaviour policy leading to more visits to less certain values, therefore, improving the overall learning sample efficiency. As a result, the proposed MM-KTD framework can learn the optimal policy with significantly reduced number of samples as compared to its DNN-based counterparts. Results: To evaluate performance of the proposed MM-KTD framework, we have performed a comprehensive set of experiments based on three RL benchmarks. Experimental results show superiority of the MM-KTD framework in comparison to its state-of-the-art counterparts.",0
"The growing interest in developing advanced Reinforcement Learning (RL) systems has led to the creation of intelligent approaches that learn optimal control policies from smart agents' interactions with the environment. In model-free RL methods with continuous state-spaces, approximating the value function of the states is typically required. Deep Neural Networks (DNNs) provide an attractive modeling mechanism for this task, but they are sensitive to parameter selection, prone to overfitting, and not very sample efficient. Alternatively, a Kalman-based methodology could be used, but it requires a-priori information about the system to perform effectively. This paper proposes an innovative Multiple Model Kalman Temporal Difference (MM-KTD) framework that adapts the parameters of the filter using observed states and rewards, and an active learning method to enhance the sampling efficiency of the system. The proposed framework can learn the optimal policy with significantly fewer samples than its DNN-based counterparts, and experimental results show its superiority over state-of-the-art methods.",1
"With advances in reinforcement learning (RL), agents are now being developed in high-stakes application domains such as healthcare and transportation. Explaining the behavior of these agents is challenging, as the environments in which they act have large state spaces, and their decision-making can be affected by delayed rewards, making it difficult to analyze their behavior. To address this problem, several approaches have been developed. Some approaches attempt to convey the $\textit{global}$ behavior of the agent, describing the actions it takes in different states. Other approaches devised $\textit{local}$ explanations which provide information regarding the agent's decision-making in a particular state. In this paper, we combine global and local explanation methods, and evaluate their joint and separate contributions, providing (to the best of our knowledge) the first user study of combined local and global explanations for RL agents. Specifically, we augment strategy summaries that extract important trajectories of states from simulations of the agent with saliency maps which show what information the agent attends to. Our results show that the choice of what states to include in the summary (global information) strongly affects people's understanding of agents: participants shown summaries that included important states significantly outperformed participants who were presented with agent behavior in a randomly set of chosen world-states. We find mixed results with respect to augmenting demonstrations with saliency maps (local information), as the addition of saliency maps did not significantly improve performance in most cases. However, we do find some evidence that saliency maps can help users better understand what information the agent relies on in its decision making, suggesting avenues for future work that can further improve explanations of RL agents.",0
"Recent advancements in reinforcement learning (RL) have led to the development of agents operating in critical domains like healthcare and transportation. However, comprehending the behavior of these agents is challenging due to the vast state spaces in which they operate and their decision-making process influenced by delayed rewards. To tackle this issue, several approaches have been proposed, including conveying the agent's behavior globally by describing its actions in various states or providing local explanations that offer details regarding its decision-making in specific states. This paper combines both global and local explanation methods and evaluates their separate and joint contributions, presenting the first user study of combined local and global explanations for RL agents. The study employs strategy summaries to extract essential state trajectories from agent simulations and saliency maps to display the information the agent focuses on. The findings reveal that the selection of states to include in the summary (global information) significantly influences people's understanding of agents, with participants who viewed summaries containing crucial states performing better than those presented with agent behavior in randomly selected states. In contrast, the impact of saliency maps (local information) on performance was mixed, with no significant improvement in most cases. However, saliency maps were found to help users better understand the agent's decision-making process, indicating potential areas for further research to enhance explanations of RL agents.",1
"We explore the benefits of augmenting state-of-the-art model-free deep reinforcement algorithms with simple object representations. Following the Frostbite challenge posited by Lake et al. (2017), we identify object representations as a critical cognitive capacity lacking from current reinforcement learning agents. We discover that providing the Rainbow model (Hessel et al.,2018) with simple, feature-engineered object representations substantially boosts its performance on the Frostbite game from Atari 2600. We then analyze the relative contributions of the representations of different types of objects, identify environment states where these representations are most impactful, and examine how these representations aid in generalizing to novel situations.",0
"The advantages of incorporating basic object representations into advanced model-free deep reinforcement algorithms are investigated in this study. By taking inspiration from the Frostbite challenge proposed by Lake et al. (2017), we realize that reinforcement learning agents currently lack key cognitive abilities such as object representations. By equipping the Rainbow model (Hessel et al.,2018) with uncomplicated, engineered object representations, we observe a significant improvement in its performance on the Atari 2600 Frostbite game. We then assess the contributions of various object representations, identify the most influential environmental states, and explore how these representations aid in adapting to unfamiliar scenarios.",1
"Automatic Curriculum Learning (ACL) has become a cornerstone of recent successes in Deep Reinforcement Learning (DRL).These methods shape the learning trajectories of agents by challenging them with tasks adapted to their capacities. In recent years, they have been used to improve sample efficiency and asymptotic performance, to organize exploration, to encourage generalization or to solve sparse reward problems, among others. The ambition of this work is dual: 1) to present a compact and accessible introduction to the Automatic Curriculum Learning literature and 2) to draw a bigger picture of the current state of the art in ACL to encourage the cross-breeding of existing concepts and the emergence of new ideas.",0
"Deep Reinforcement Learning (DRL) has achieved great success with the implementation of Automatic Curriculum Learning (ACL). This approach challenges agents with tasks that are tailored to their abilities, shaping their learning trajectory. ACL has been utilized in various ways to enhance sample efficiency, asymptotic performance, exploration, generalization, and to solve sparse reward problems. This study aims to provide a concise and comprehensible overview of the Automatic Curriculum Learning literature while also highlighting the current state-of-the-art in ACL to foster the exchange of ideas and the evolution of new concepts.",1
"Model-free deep reinforcement learning (RL) agents can learn an effective policy directly from repeated interactions with a black-box environment. However in practice, the algorithms often require large amounts of training experience to learn and generalize well. In addition, classic model-free learning ignores the domain information contained in the state transition tuples. Model-based RL, on the other hand, attempts to learn a model of the environment from experience and is substantially more sample efficient, but suffers from significantly large asymptotic bias owing to the imperfect dynamics model. In this paper, we propose a gradient matching algorithm to improve sample efficiency by utilizing target slope information from the dynamics predictor to aid the model-free learner. We demonstrate this by presenting a technique for matching the gradient information from the model-based learner with the model-free component in an abstract low-dimensional space and validate the proposed technique through experimental results that demonstrate the efficacy of this approach.",0
"Deep reinforcement learning (RL) agents without models can acquire an efficient policy through direct interactions with a black-box environment. However, they often require extensive training experience to perform well in practical applications. Furthermore, classic model-free learning does not take into account the domain information contained in state transition tuples. In contrast, model-based RL aims to develop an environment model from experience and is more efficient in terms of sample usage, but suffers from significant asymptotic bias as a result of imperfect dynamic modeling. In this article, we suggest a gradient matching algorithm to enhance sample efficiency by utilizing target slope information from the dynamics predictor to support the model-free learner. We demonstrate this by developing a method for matching the gradient information from the model-based learner with the model-free component in a low-dimensional abstract space. Experimental results validate the proposed technique's effectiveness.",1
"We propose a model-free algorithm for learning efficient policies capable of returning table tennis balls by controlling robot joints at a rate of 100Hz. We demonstrate that evolutionary search (ES) methods acting on CNN-based policy architectures for non-visual inputs and convolving across time learn compact controllers leading to smooth motions. Furthermore, we show that with appropriately tuned curriculum learning on the task and rewards, policies are capable of developing multi-modal styles, specifically forehand and backhand stroke, whilst achieving 80\% return rate on a wide range of ball throws. We observe that multi-modality does not require any architectural priors, such as multi-head architectures or hierarchical policies.",0
"An algorithm that does not require a model is proposed for learning efficient policies for controlling robot joints to return table tennis balls at a rate of 100Hz. Through the use of evolutionary search (ES) methods on policy architectures based on convolutional neural networks (CNNs) for non-visual inputs and convolving across time, the algorithm learns to develop compact controllers that enable smooth motions. With curriculum learning appropriately tailored to the task and rewards, the policies demonstrate the ability to develop multi-modal styles, including forehand and backhand strokes, while achieving an 80% return rate on a wide range of ball throws. It is observed that multi-modality does not require any prior architectural knowledge, such as multi-head architectures or hierarchical policies.",1
"Recent work has studied the emergence of language among deep reinforcement learning agents that must collaborate to solve a task. Of particular interest are the factors that cause language to be compositional -- i.e., express meaning by combining words which themselves have meaning. Evolutionary linguists have found that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization.",0
"Recent research has focused on the development of language by collaborative deep reinforcement learning agents to solve tasks. Of particular interest is the identification of factors responsible for compositional language, where words with distinct meanings are combined to express a new meaning. Evolutionary linguists have discovered that, in addition to the structural priors studied in deep learning, the transmission of language across generations plays a crucial role in the emergence of compositional language. In this study, we introduce cultural evolutionary dynamics by periodically replacing agents in a population to create a knowledge gap. This encourages implicit cultural transmission of language and results in better compositional generalization.",1
"Orthogonal Monte Carlo (OMC) is a very effective sampling algorithm imposing structural geometric conditions (orthogonality) on samples for variance reduction. Due to its simplicity and superior performance as compared to its Quasi Monte Carlo counterparts, OMC is used in a wide spectrum of challenging machine learning applications ranging from scalable kernel methods to predictive recurrent neural networks, generative models and reinforcement learning. However theoretical understanding of the method remains very limited. In this paper we shed new light on the theoretical principles behind OMC, applying theory of negatively dependent random variables to obtain several new concentration results. We also propose a novel extensions of the method leveraging number theory techniques and particle algorithms, called Near-Orthogonal Monte Carlo (NOMC). We show that NOMC is the first algorithm consistently outperforming OMC in applications ranging from kernel methods to approximating distances in probabilistic metric spaces.",0
"The Orthogonal Monte Carlo (OMC) algorithm is an effective sampling technique that enforces geometric conditions (orthogonality) on samples to reduce variance. Despite its superior performance compared to Quasi Monte Carlo methods, its theoretical understanding remains limited. This paper aims to shed new light on the theoretical principles of OMC by utilizing the theory of negatively dependent random variables to obtain several new concentration results. Additionally, a novel extension of the method called Near-Orthogonal Monte Carlo (NOMC) is proposed, which leverages number theory techniques and particle algorithms. We demonstrate that NOMC consistently outperforms OMC in various machine learning applications, including kernel methods and approximating distances in probabilistic metric spaces.",1
"In this work we present a novel approach for transfer-guided exploration in reinforcement learning that is inspired by the human tendency to leverage experiences from similar encounters in the past while navigating a new task. Given an optimal policy in a related task-environment, we show that its bisimulation distance from the current task-environment gives a lower bound on the optimal advantage of state-action pairs in the current task-environment. Transfer-guided Exploration (ExTra) samples actions from a Softmax distribution over these lower bounds. In this way, actions with potentially higher optimum advantage are sampled more frequently. In our experiments on gridworld environments, we demonstrate that given access to an optimal policy in a related task-environment, ExTra can outperform popular domain-specific exploration strategies viz. epsilon greedy, Model-Based Interval Estimation - Exploration Bonus (MBIE-EB), Pursuit and Boltzmann in rate of convergence. We further show that ExTra is robust to choices of source task and shows a graceful degradation of performance as the dissimilarity of the source task increases. We also demonstrate that ExTra, when used alongside traditional exploration algorithms, improves their rate of convergence. Thus it is capable of complementing the efficacy of traditional exploration algorithms.",0
"This study presents a new method for transfer-guided exploration in reinforcement learning, which takes inspiration from how humans use past experiences to navigate new tasks. By utilizing an optimal policy from a related task-environment, the bisimulation distance between it and the current task-environment can provide a lower limit for the optimal advantage of state-action pairs. The Transfer-guided Exploration (ExTra) technique then samples actions from a Softmax distribution based on these lower bounds, increasing the frequency of potentially advantageous actions. The study's experiments on gridworld environments demonstrate that ExTra outperforms domain-specific exploration strategies such as epsilon greedy, Model-Based Interval Estimation - Exploration Bonus (MBIE-EB), Pursuit, and Boltzmann in convergence speed. Additionally, ExTra is shown to be robust to source task selection and performs well alongside traditional exploration algorithms. Therefore, it can effectively complement existing exploration methods.",1
"Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might require many samples to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, we show our method can also be used when the available expert trajectories do not contain the actions, which can leverage kinesthetic or third person demonstration. The code is available at https://sites.google.com/view/goalconditioned-il/.",0
"Coming up with effective rewards for Reinforcement Learning (RL) presents a challenge as it must convey the desired task, optimize efficiently and be easily computable. This is particularly problematic in robotics where detecting whether the desired configuration has been achieved requires a lot of supervision and instrumentation. Additionally, setting up a different reward for every configuration is impractical as a wide range of configurations must be reached. Recently, methods like Hindsight Experience Replay (HER) have shown potential in learning policies capable of reaching several goals without requiring rewards. However, HER may need many samples to discover how to reach certain areas of the state-space without resetting to points along the trajectory. In this study, we investigate various approaches to incorporate demonstrations to speed up policy convergence and surpass the performance of an agent trained with other Imitation Learning algorithms. Our method can also be used with kinesthetic or third person demonstration even if the expert trajectories do not contain the actions. The code is accessible at https://sites.google.com/view/goalconditioned-il/.",1
"Interventions are central to causal learning and reasoning. Yet ultimately an intervention is an abstraction: an agent embedded in a physical environment (perhaps modeled as a Markov decision process) does not typically come equipped with the notion of an intervention -- its action space is typically ego-centric, without actions of the form `intervene on X'. Such a correspondence between ego-centric actions and interventions would be challenging to hard-code. It would instead be better if an agent learnt which sequence of actions allow it to make targeted manipulations of the environment, and learnt corresponding representations that permitted learning from observation. Here we show how a meta-learning approach can be used to perform causal learning in this challenging setting, where the action-space is not a set of interventions and the observation space is a high-dimensional space with a latent causal structure. A meta-reinforcement learning algorithm is used to learn relationships that transfer on observational causal learning tasks. This work shows how advances in deep reinforcement learning and meta-learning can provide intervention-centric causal learning in high-dimensional environments with a latent causal structure.",0
"Causal learning and reasoning depend heavily on interventions. However, interventions themselves are abstract concepts. In a physical environment, an agent may not have the ability to intervene, as their actions are typically self-centered and do not include interventions. It would be difficult to program an agent to understand the connection between ego-centric actions and interventions. Instead, it would be more effective for an agent to learn which actions allow for targeted manipulation of the environment and develop corresponding representations for observational learning. Our study demonstrates how a meta-learning approach can overcome the challenge of learning causality in high-dimensional environments with latent causal structures, where the action-space does not include interventions. Utilizing a meta-reinforcement learning algorithm, we establish relationships that transfer to observational causal learning tasks. This research showcases the potential of deep reinforcement learning and meta-learning to enable intervention-centric causal learning in complex environments.",1
"In this paper, the design of an optimal trajectory for an energy-constrained drone operating in dynamic network environments is studied. In the considered model, a drone base station (DBS) is dispatched to provide uplink connectivity to ground users whose demand is dynamic and unpredictable. In this case, the DBS's trajectory must be adaptively adjusted to satisfy the dynamic user access requests. To this end, a meta-learning algorithm is proposed in order to adapt the DBS's trajectory when it encounters novel environments, by tuning a reinforcement learning (RL) solution. The meta-learning algorithm provides a solution that adapts the DBS in novel environments quickly based on limited former experiences. The meta-tuned RL is shown to yield a faster convergence to the optimal coverage in unseen environments with a considerably low computation complexity, compared to the baseline policy gradient algorithm. Simulation results show that, the proposed meta-learning solution yields a 25% improvement in the convergence speed, and about 10% improvement in the DBS' communication performance, compared to a baseline policy gradient algorithm. Meanwhile, the probability that the DBS serves over 50% of user requests increases about 27%, compared to the baseline policy gradient algorithm.",0
"This paper examines the design of an optimal trajectory for an energy-restricted drone operating in dynamic network environments. The model considers a drone base station (DBS) dispatched to provide uplink connectivity to ground users with unpredictable and dynamic demand. Therefore, the DBS's trajectory must be adaptively adjusted to satisfy the changing user access requests. To achieve this, a meta-learning algorithm is proposed to quickly adapt the DBS's trajectory when it faces new environments by modifying a reinforcement learning (RL) solution. The meta-learning algorithm offers a solution that adapts the DBS in new environments based on limited previous experiences. The meta-tuned RL yields faster convergence to optimal coverage in unfamiliar environments with significantly low computation complexity, compared to the baseline policy gradient algorithm. Simulation results show that the proposed meta-learning solution enhances the convergence speed by 25% and improves the DBS's communication performance by about 10% compared to the baseline policy gradient algorithm. Additionally, the probability that the DBS serves over 50% of user requests increases by approximately 27% compared to the baseline policy gradient algorithm.",1
"Training deep reinforcement learning agents on environments with multiple levels / scenes / conditions from the same task, has become essential for many applications aiming to achieve generalization and domain transfer from simulation to the real world. While such a strategy is helpful with generalization, the use of multiple scenes significantly increases the variance of samples collected for policy gradient computations. Current methods continue to view this collection of scenes as a single Markov Decision Process (MDP) with a common value function; however, we argue that it is better to treat the collection as a single environment with multiple underlying MDPs. To this end, we propose a dynamic value estimation (DVE) technique for these multiple-MDP environments, motivated by the clustering effect observed in the value function distribution across different scenes. The resulting agent is able to learn a more accurate and scene-specific value function estimate (and hence the advantage function), leading to a lower sample variance. Our proposed approach is simple to accommodate with several existing implementations (like PPO, A3C) and results in consistent improvements for a range of ProcGen environments and the AI2-THOR framework based visual navigation task.",0
"Many applications that aim to achieve generalization and domain transfer from simulation to the real world require training deep reinforcement learning agents on environments with multiple levels, scenes, or conditions from the same task. While this strategy is helpful with generalization, using multiple scenes significantly increases the variance of samples collected for policy gradient computations. Current methods treat this collection of scenes as a single Markov Decision Process (MDP) with a common value function; however, it is better to consider the collection as a single environment with multiple underlying MDPs. Therefore, we propose a dynamic value estimation (DVE) technique for these multiple-MDP environments, which is motivated by the clustering effect observed in the value function distribution across different scenes. This approach enables the agent to learn a more accurate and scene-specific value function estimate, resulting in a lower sample variance. Our proposed approach is easy to incorporate into existing implementations, such as PPO and A3C, and consistently improves performance in various ProcGen environments and the AI2-THOR framework-based visual navigation task.",1
"We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of ""code-level optimizations:"" algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO's gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty and importance of attributing performance gains in deep reinforcement learning. Code for reproducing our results is available at https://github.com/MadryLab/implementation-matters .",0
"Through a case study on Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO), we examine the origins of algorithmic progress in deep policy gradient algorithms. Our focus is on ""code-level optimizations,"" which are algorithm augmentations that are only found in implementations or described as auxiliary details to the core algorithm. Although they may seem insignificant, these optimizations have a significant impact on agent behavior. Our findings reveal that they are primarily responsible for PPO's gain in cumulative reward over TRPO and fundamentally alter how RL methods function. These insights demonstrate the challenge and significance of attributing performance gains in deep reinforcement learning. To replicate our results, visit https://github.com/MadryLab/implementation-matters.",1
"The slate re-ranking problem considers the mutual influences between items to improve user satisfaction in e-commerce, compared with the point-wise ranking. Previous works either directly rank items by an end to end model, or rank items by a score function that trades-off the point-wise score and the diversity between items. However, there are two main existing challenges that are not well studied: (1) the evaluation of the slate is hard due to the complex mutual influences between items of one slate; (2) even given the optimal evaluation, searching the optimal slate is challenging as the action space is exponentially large. In this paper, we present a novel Generator and Critic slate re-ranking approach, where the Critic evaluates the slate and the Generator ranks the items by the reinforcement learning approach. We propose a Full Slate Critic (FSC) model that considers the real impressed items and avoids the impressed bias of existing models. For the Generator, to tackle the problem of large action space, we propose a new exploration reinforcement learning algorithm, called PPO-Exploration. Experimental results show that the FSC model significantly outperforms the state of the art slate evaluation methods, and the PPO-Exploration algorithm outperforms the existing reinforcement learning methods substantially. The Generator and Critic approach improves both the slate efficiency(4% gmv and 5% number of orders) and diversity in live experiments on one of the largest e-commerce websites in the world.",0
"The problem of re-ranking slates aims to enhance user satisfaction in e-commerce by considering the interdependence between items, as opposed to point-wise ranking. Previous studies have either directly ranked items using an end-to-end model or a score function that balances point-wise scores and item diversity. However, two major challenges remain unaddressed: (1) assessing slate quality is difficult due to the complex interplay between items; (2) even with optimal evaluation, searching for the best slate is difficult due to the exponentially large action space. This paper proposes a new approach, where a Critic evaluates the slate while a Generator ranks items using reinforcement learning. The Full Slate Critic (FSC) model considers impressed items and avoids model bias, while the proposed PPO-Exploration algorithm tackles the issue of a large action space. Experimental results demonstrate the superiority of the FSC model and the PPO-Exploration algorithm over existing methods. The Generator and Critic approach improves slate efficiency and diversity in live experiments on one of the world's largest e-commerce sites.",1
"We present DRLViz, a visual analytics interface to interpret the internal memory of an agent (e.g. a robot) trained using deep reinforcement learning. This memory is composed of large temporal vectors updated when the agent moves in an environment and is not trivial to understand due to the number of dimensions, dependencies to past vectors, spatial/temporal correlations, and co-correlation between dimensions. It is often referred to as a black box as only inputs (images) and outputs (actions) are intelligible for humans. Using DRLViz, experts are assisted to interpret decisions using memory reduction interactions, and to investigate the role of parts of the memory when errors have been made (e.g. wrong direction). We report on DRLViz applied in the context of video games simulators (ViZDoom) for a navigation scenario with item gathering tasks. We also report on experts evaluation using DRLViz, and applicability of DRLViz to other scenarios and navigation problems beyond simulation games, as well as its contribution to black box models interpretability and explainability in the field of visual analytics.",0
"DRLViz is introduced as a visual analytics interface that aids in the interpretation of an agent's internal memory, which is made up of large temporal vectors that are updated as the agent moves in an environment. Due to the number of dimensions, dependencies to past vectors, spatial/temporal correlations, and co-correlation between dimensions, this memory is often considered a black box, with only inputs (images) and outputs (actions) being comprehensible to humans. With DRLViz, experts are assisted in interpreting decisions through memory reduction interactions and examining the role of memory parts in the event of errors. The article discusses DRLViz's application in video game simulators (ViZDoom) for a navigation scenario with item gathering tasks, as well as its usefulness in other scenarios and navigation problems beyond simulation games. Furthermore, it highlights DRLViz's potential contribution to the interpretability and explainability of black box models in the visual analytics field.",1
"This paper presents a novel neural network training approach for faster convergence and better generalization abilities in deep reinforcement learning. Particularly, we focus on the enhancement of training and evaluation performance in reinforcement learning algorithms by systematically reducing gradient's variance and thereby providing a more targeted learning process. The proposed method which we term as Gradient Monitoring(GM), is an approach to steer the learning in the weight parameters of a neural network based on the dynamic development and feedback from the training process itself. We propose different variants of the GM methodology which have been proven to increase the underlying performance of the model. The one of the proposed variant, Momentum with Gradient Monitoring (M-WGM), allows for a continuous adjustment of the quantum of back-propagated gradients in the network based on certain learning parameters. We further enhance the method with Adaptive Momentum with Gradient Monitoring (AM-WGM) method which allows for automatic adjustment between focused learning of certain weights versus a more dispersed learning depending on the feedback from the rewards collected. As a by-product, it also allows for automatic derivation of the required deep network sizes during training as the algorithm automatically freezes trained weights. The approach is applied to two discrete (Multi-Robot Co-ordination problem and Atari games) and one continuous control task (MuJoCo) using Advantage Actor-Critic (A2C) and Proximal Policy Optimization (PPO) respectively. The results obtained particularly underline the applicability and performance improvements of the methods in terms of generalization capability.",0
"In this article, a fresh neural network training technique is presented that promotes faster convergence and improved generalization abilities in deep reinforcement learning. The focus is on enhancing the training and evaluation performance in reinforcement learning algorithms by reducing the variance in the gradient, which leads to a more targeted learning process. The proposed method, called Gradient Monitoring (GM), steers the learning in the weight parameters of a neural network based on the dynamic development and feedback from the training process. Various versions of the GM methodology have been proposed and tested, including Momentum with Gradient Monitoring (M-WGM) and Adaptive Momentum with Gradient Monitoring (AM-WGM). The latter allows for automatic adjustment between focused learning of certain weights versus a more dispersed learning depending on feedback from rewards collected. The approach has been applied to three control tasks, including Multi-Robot Co-ordination, Atari games, and MuJoCo, using Advantage Actor-Critic (A2C) and Proximal Policy Optimization (PPO). The results demonstrate the superior generalization capability of the proposed methods. Additionally, the method allows for automatic derivation of the deep network sizes during training as the algorithm automatically freezes trained weights.",1
"One critical prerequisite for the deployment of reinforcement learning systems in the real world is the ability to reliably detect situations on which the agent was not trained. Such situations could lead to potential safety risks when wrong predictions lead to the execution of harmful actions. In this work, we propose PEOC, a new policy entropy based out-of-distribution classifier that reliably detects unencountered states in deep reinforcement learning. It is based on using the entropy of an agent's policy as the classification score of a one-class classifier. We evaluate our approach using a procedural environment generator. Results show that PEOC is highly competitive against state-of-the-art one-class classification algorithms on the evaluated environments. Furthermore, we present a structured process for benchmarking out-of-distribution classification in reinforcement learning.",0
"Detecting situations on which the agent was not trained is a crucial requirement for the successful implementation of reinforcement learning systems in real-world scenarios. This is because such situations can pose significant risks to safety if incorrect predictions lead to harmful actions. Our research proposes a new method called PEOC, which is a policy entropy based out-of-distribution classifier that can effectively detect unencountered states in deep reinforcement learning. This approach uses the agent's policy entropy as the classification score for a one-class classifier. We evaluated PEOC using a procedural environment generator and found that it outperformed state-of-the-art one-class classification algorithms on the tested environments. Additionally, we introduce a structured procedure for benchmarking out-of-distribution classification in reinforcement learning.",1
"Transfer Learning (TL) has shown great potential to accelerate Reinforcement Learning (RL) by leveraging prior knowledge from past learned policies of relevant tasks. Existing transfer approaches either explicitly computes the similarity between tasks or select appropriate source policies to provide guided explorations for the target task. However, how to directly optimize the target policy by alternatively utilizing knowledge from appropriate source policies without explicitly measuring the similarity is currently missing. In this paper, we propose a novel Policy Transfer Framework (PTF) to accelerate RL by taking advantage of this idea. Our framework learns when and which source policy is the best to reuse for the target policy and when to terminate it by modeling multi-policy transfer as the option learning problem. PTF can be easily combined with existing deep RL approaches. Experimental results show it significantly accelerates the learning process and surpasses state-of-the-art policy transfer methods in terms of learning efficiency and final performance in both discrete and continuous action spaces.",0
"The utilization of Transfer Learning (TL) has demonstrated its potential in hastening Reinforcement Learning (RL) by incorporating past knowledge from relevant tasks' learned policies. Current transfer techniques either explicitly calculate task similarity or choose suitable source policies to guide the exploration of the target task. However, there is currently a lack of knowledge on how to directly optimize the target policy by using knowledge from appropriate source policies without explicitly measuring similarity. To address this, we propose a Policy Transfer Framework (PTF) that leverages this concept to accelerate RL. Our framework determines the best source policy to reuse for the target policy and when to stop utilizing it by modeling multi-policy transfer as an option learning problem. PTF can be readily integrated with existing deep RL methods. Our experiments demonstrate that PTF significantly accelerates the learning process and outperforms existing policy transfer techniques in terms of learning efficiency and final performance in both discrete and continuous action spaces.",1
"Transferring as fast as possible the functioning of our brain to artificial intelligence is an ambitious goal that would help advance the state of the art in AI and robotics. It is in this perspective that we propose to start from hypotheses derived from an empirical study in a human-robot interaction and to verify if they are validated in the same way for children as for a basic reinforcement learning algorithm. Thus, we check whether receiving help from an expert when solving a simple close-ended task (the Towers of Hano\""i) allows to accelerate or not the learning of this task, depending on whether the intervention is canonical or requested by the player. Our experiences have allowed us to conclude that, whether requested or not, a Q-learning algorithm benefits in the same way from expert help as children do.",0
"The goal of transferring brain function to artificial intelligence as quickly as possible is an ambitious one that could push the boundaries of AI and robotics. Our approach is to begin with hypotheses derived from a study on human-robot interaction and test their validity on children and a basic reinforcement learning algorithm. The purpose is to determine if receiving assistance from an expert for a simple task, such as the Towers of Hanoi, can accelerate learning, depending on whether the intervention is initiated by the player or not. Our findings demonstrate that both children and a Q-learning algorithm benefit equally from expert help, whether requested or not.",1
"We propose a novel reinforcement learning-based approach for adaptive and iterative feature selection. Given a masked vector of input features, a reinforcement learning agent iteratively selects certain features to be unmasked, and uses them to predict an outcome when it is sufficiently confident. The algorithm makes use of a novel environment setting, corresponding to a non-stationary Markov Decision Process. A key component of our approach is a guesser network, trained to predict the outcome from the selected features and parametrizing the reward function. Applying our method to a national survey dataset, we show that it not only outperforms strong baselines when requiring the prediction to be made based on a small number of input features, but is also highly more interpretable. Our code is publicly available at \url{https://github.com/ushaham/adaptiveFS}.",0
"Our proposed method utilizes reinforcement learning to achieve adaptive and iterative feature selection. The approach involves an agent that chooses which features to unmask from a masked input vector, and uses them to make a prediction when it is confident enough. The method employs a unique environment setting, which corresponds to a non-stationary Markov Decision Process. A guesser network is a crucial element of our approach, and it is trained to predict outcomes from selected features, thereby parameterizing the reward function. We applied our method to a national survey dataset and discovered that it outperforms strong baselines in situations where predictions are based on a limited number of input features and is more interpretable. Our code is available for public use at \url{https://github.com/ushaham/adaptiveFS}.",1
"With widespread applications of artificial intelligence (AI), the capabilities of the perception, understanding, decision-making and control for autonomous systems have improved significantly in the past years. When autonomous systems consider the performance of accuracy and transferability, several AI methods, like adversarial learning, reinforcement learning (RL) and meta-learning, show their powerful performance. Here, we review the learning-based approaches in autonomous systems from the perspectives of accuracy and transferability. Accuracy means that a well-trained model shows good results during the testing phase, in which the testing set shares a same task or a data distribution with the training set. Transferability means that when a well-trained model is transferred to other testing domains, the accuracy is still good. Firstly, we introduce some basic concepts of transfer learning and then present some preliminaries of adversarial learning, RL and meta-learning. Secondly, we focus on reviewing the accuracy or transferability or both of them to show the advantages of adversarial learning, like generative adversarial networks (GANs), in typical computer vision tasks in autonomous systems, including image style transfer, image superresolution, image deblurring/dehazing/rain removal, semantic segmentation, depth estimation, pedestrian detection and person re-identification (re-ID). Then, we further review the performance of RL and meta-learning from the aspects of accuracy or transferability or both of them in autonomous systems, involving pedestrian tracking, robot navigation and robotic manipulation. Finally, we discuss several challenges and future topics for using adversarial learning, RL and meta-learning in autonomous systems.",0
"In recent years, the use of artificial intelligence (AI) has greatly improved the perception, understanding, decision-making, and control abilities of autonomous systems. Several AI methods, such as adversarial learning, reinforcement learning (RL), and meta-learning, have shown powerful performance in terms of accuracy and transferability. This article provides a review of learning-based approaches in autonomous systems, focusing on accuracy and transferability. Accuracy refers to the ability of a well-trained model to perform well during testing when the testing set shares the same task or data distribution as the training set. Transferability refers to the ability of a well-trained model to maintain accuracy when transferred to other testing domains. The article introduces basic concepts of transfer learning and preliminaries of adversarial learning, RL, and meta-learning. It then reviews the advantages of adversarial learning, including generative adversarial networks (GANs), in typical computer vision tasks in autonomous systems. The article also reviews the performance of RL and meta-learning in pedestrian tracking, robot navigation, and robotic manipulation. Finally, the article discusses the challenges and future topics for the use of adversarial learning, RL, and meta-learning in autonomous systems.",1
"Policy gradient is a generic and flexible reinforcement learning approach that generally enjoys simplicity in analysis, implementation, and deployment. In the last few decades, this approach has been extensively advanced for fully observable environments. In this paper, we generalize a variety of these advances to partially observable settings, and similar to the fully observable case, we keep our focus on the class of Markovian policies. We propose a series of technical tools, including a novel notion of advantage function, to develop policy gradient algorithms and study their convergence properties in such environments. Deploying these tools, we generalize a variety of existing theoretical guarantees, such as policy gradient and convergence theorems, to partially observable domains, those which also could be carried to more settings of interest. This study also sheds light on the understanding of policy gradient approaches in real-world applications which tend to be partially observable.",0
"Policy gradient is a flexible and widely used approach in reinforcement learning that is simple to analyze, implement, and deploy. While it has been extensively developed for fully observable environments, this paper aims to generalize these advances to partially observable settings, with a focus on Markovian policies. To achieve this, we propose technical tools, including a new type of advantage function, to design policy gradient algorithms and analyze their convergence in such environments. By doing so, we extend existing theoretical guarantees, such as policy gradient and convergence theorems, to partially observable domains, which can also be applied to other relevant settings. This study offers insights into the application of policy gradient approaches in real-world scenarios, which are often partially observable.",1
"In Multi-Goal Reinforcement Learning, an agent learns to achieve multiple goals with a goal-conditioned policy. During learning, the agent first collects the trajectories into a replay buffer, and later these trajectories are selected randomly for replay. However, the achieved goals in the replay buffer are often biased towards the behavior policies. From a Bayesian perspective, when there is no prior knowledge about the target goal distribution, the agent should learn uniformly from diverse achieved goals. Therefore, we first propose a novel multi-goal RL objective based on weighted entropy. This objective encourages the agent to maximize the expected return, as well as to achieve more diverse goals. Secondly, we developed a maximum entropy-based prioritization framework to optimize the proposed objective. For evaluation of this framework, we combine it with Deep Deterministic Policy Gradient, both with or without Hindsight Experience Replay. On a set of multi-goal robotic tasks of OpenAI Gym, we compare our method with other baselines and show promising improvements in both performance and sample-efficiency.",0
"The aim of Multi-Goal Reinforcement Learning is for an agent to learn how to achieve multiple objectives using a goal-conditioned policy. To achieve this, the agent gathers trajectories and stores them in a replay buffer. However, the goals achieved in the replay buffer may be biased towards the behavior policies. To address this, we propose a new multi-goal RL objective that focuses on weighted entropy. This objective encourages the agent to maximize the expected return while achieving diverse goals. We also introduce a maximum entropy-based prioritization framework to optimize this objective. Our evaluation combines this framework with Deep Deterministic Policy Gradient, with or without Hindsight Experience Replay. We compare our method to other baselines on OpenAI Gym's multi-goal robotic tasks and demonstrate promising improvements in both performance and sample-efficiency. This approach is based on a Bayesian perspective where the agent learns uniformly from diverse achieved goals in the absence of prior knowledge about the target goal distribution.",1
"In Reinforcement Learning (RL), an agent explores the environment and collects trajectories into the memory buffer for later learning. However, the collected trajectories can easily be imbalanced with respect to the achieved goal states. The problem of learning from imbalanced data is a well-known problem in supervised learning, but has not yet been thoroughly researched in RL. To address this problem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to encourage the agent to over-sample those trajectories that have rare achieved goal states. The CDP framework mimics the human learning process and focuses more on relatively uncommon events. We evaluate our methods using the robotic environment provided by OpenAI Gym. The environment contains six robot manipulation tasks. In our experiments, we combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER). The experimental results show that CDP improves both performance and sample-efficiency of reinforcement learning agents, compared to state-of-the-art methods.",0
"Reinforcement Learning (RL) involves an agent exploring and storing trajectories in a memory buffer for later learning. However, the collected trajectories may not be balanced in terms of goal states achieved. Although the problem of learning from imbalanced data is well-known in supervised learning, it has not been thoroughly researched in RL. To address this issue, we suggest a novel Curiosity-Driven Prioritization (CDP) framework that incentivizes the agent to oversample rare achieved goal states. The CDP framework emulates human learning processes and prioritizes relatively uncommon events. Our methods were tested in the robotic environment provided by OpenAI Gym, which includes six robot manipulation tasks. We combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER) in our experiments. The results indicate that CDP enhances both the performance and sample efficiency of RL agents when compared to state-of-the-art methods.",1
"Learning goal-oriented dialogues by means of deep reinforcement learning has recently become a popular research topic. However, commonly used policy-based dialogue agents often end up focusing on simple utterances and suboptimal policies. To mitigate this problem, we propose a class of novel temperature-based extensions for policy gradient methods, which are referred to as Tempered Policy Gradients (TPGs). On a recent AI-testbed, i.e., the GuessWhat?! game, we achieve significant improvements with two innovations. The first one is an extension of the state-of-the-art solutions with Seq2Seq and Memory Network structures that leads to an improvement of 7%. The second one is the application of our newly developed TPG methods, which improves the performance additionally by around 5% and, even more importantly, helps produce more convincing utterances.",0
"Recently, there has been a surge in research on utilizing deep reinforcement learning to enhance goal-oriented dialogues. However, conventional policy-based dialogue agents tend to concentrate on simplistic utterances and suboptimal policies, which poses a challenge. To address this issue, we propose a new category of temperature-based extensions called Tempered Policy Gradients (TPGs) for policy gradient methods. Using a recent AI-testbed, the GuessWhat?! game, we demonstrate two groundbreaking innovations. Firstly, we improve the state-of-the-art solutions with Seq2Seq and Memory Network structures, resulting in a 7% increase in performance. Secondly, we employ our newly developed TPG methods, which further enhance the performance by approximately 5% and, more importantly, generate more convincing utterances.",1
"In Hindsight Experience Replay (HER), a reinforcement learning agent is trained by treating whatever it has achieved as virtual goals. However, in previous work, the experience was replayed at random, without considering which episode might be the most valuable for learning. In this paper, we develop an energy-based framework for prioritizing hindsight experience in robotic manipulation tasks. Our approach is inspired by the work-energy principle in physics. We define a trajectory energy function as the sum of the transition energy of the target object over the trajectory. We hypothesize that replaying episodes that have high trajectory energy is more effective for reinforcement learning in robotics. To verify our hypothesis, we designed a framework for hindsight experience prioritization based on the trajectory energy of goal states. The trajectory energy function takes the potential, kinetic, and rotational energy into consideration. We evaluate our Energy-Based Prioritization (EBP) approach on four challenging robotic manipulation tasks in simulation. Our empirical results show that our proposed method surpasses state-of-the-art approaches in terms of both performance and sample-efficiency on all four tasks, without increasing computational time. A video showing experimental results is available at https://youtu.be/jtsF2tTeUGQ",0
"The Hindsight Experience Replay (HER) technique involves training a reinforcement learning agent by using its achieved objectives as virtual goals. However, previous research has randomly replayed experiences without considering which episodes are most valuable for learning. This study introduces an energy-based framework for prioritizing hindsight experience in robotic manipulation tasks, inspired by the work-energy principle in physics. A trajectory energy function is defined as the sum of the transition energy of the target object over the trajectory. The hypothesis is that replaying episodes with high trajectory energy is more effective for reinforcement learning in robotics. To test this hypothesis, a framework for hindsight experience prioritization based on the trajectory energy of goal states is designed. The trajectory energy function considers potential, kinetic, and rotational energy. The proposed Energy-Based Prioritization (EBP) approach is evaluated on four challenging robotic manipulation tasks in simulation. The results show that EBP outperforms state-of-the-art approaches in terms of both performance and sample-efficiency on all four tasks, without increasing computational time. An experimental results video is available at https://youtu.be/jtsF2tTeUGQ.",1
"Robotic insertion tasks are characterized by contact and friction mechanics, making them challenging for conventional feedback control methods due to unmodeled physical effects. Reinforcement learning (RL) is a promising approach for learning control policies in such settings. However, RL can be unsafe during exploration and might require a large amount of real-world training data, which is expensive to collect. In this paper, we study how to use meta-reinforcement learning to solve the bulk of the problem in simulation by solving a family of simulated industrial insertion tasks and then adapt policies quickly in the real world. We demonstrate our approach by training an agent to successfully perform challenging real-world insertion tasks using less than 20 trials of real-world experience. Videos and other material are available at https://pearl-insertion.github.io/",0
"The difficulty of conventional feedback control methods in robotic insertion tasks lies in their contact and friction mechanics, which entail unaccounted-for physical effects. To address this, reinforcement learning (RL) has shown promise as a viable alternative. However, its exploration phase can be hazardous and it requires a considerable amount of real-world data, which can be costly to gather. This study explores the use of meta-reinforcement learning to tackle the issue by solving a range of simulated industrial insertion tasks and subsequently quickly adapting policies in the real world. The approach was successfully demonstrated through less than 20 trials of real-world experience in performing challenging insertion tasks. Additional materials and videos can be accessed at https://pearl-insertion.github.io/.",1
"In the physical design of integrated circuits, global and detailed routing are critical stages involving the determination of the interconnected paths of each net on a circuit while satisfying the design constraints. Existing actual routers as well as routability predictors either have to resort to expensive approaches that lead to high computational times, or use heuristics that do not generalize well. Even though new, learning-based routing methods have been proposed to address this need, requirements on labelled data and difficulties in addressing complex design rule constraints have limited their adoption in advanced technology node physical design problems. In this work, we propose a new router: attention router, which is the first attempt to solve the track-assignment detailed routing problem using reinforcement learning. Complex design rule constraints are encoded into the routing algorithm and an attention-model-based REINFORCE algorithm is applied to solve the most critical step in routing: sequencing device pairs to be routed. The attention router and its baseline genetic router are applied to solve different commercial advanced technologies analog circuits problem sets. The attention router demonstrates generalization ability to unseen problems and is also able to achieve more than 100 times acceleration over the genetic router without significantly compromising the routing solution quality. We also discover a similarity between the attention router and the baseline genetic router in terms of positive correlations in cost and routing patterns, which demonstrate the attention router's ability to be utilized not only as a detailed router but also as a predictor for routability and congestion.",0
"Global and detailed routing are crucial stages in the physical design of integrated circuits, during which the interconnected paths of each net on a circuit must be determined while adhering to design constraints. The current routers and routability predictors available either require expensive approaches that lead to lengthy computational times or use heuristics that fail to generalize effectively. While learning-based routing methods have been proposed to address this issue, their adoption in advanced technology node physical design problems has been limited by labeled data requirements and complex design rule constraints. This study introduces a novel router, the attention router, which employs reinforcement learning to solve the track-assignment detailed routing problem. The attention router encodes complex design rule constraints into the routing algorithm and utilizes an attention-model-based REINFORCE algorithm to sequence device pairs for routing. The attention router is applied to various commercial advanced technologies analog circuits problem sets and demonstrates generalization ability to unseen problems. Furthermore, the attention router achieves over 100 times acceleration compared to the genetic router without significantly compromising the routing solution quality. Additionally, the attention router and baseline genetic router exhibit similarities in cost and routing patterns, indicating that the attention router can also function as a predictor for routability and congestion, in addition to a detailed router.",1
"The kind of closed-loop verification likely to be required for autonomous vehicle (AV) safety testing is beyond the reach of traditional test methodologies and discrete verification. Validation puts the autonomous vehicle system to the test in scenarios or situations that the system would likely encounter in everyday driving after its release. These scenarios can either be controlled directly in a physical (closed-course proving ground) or virtual (simulation of predefined scenarios) environment, or they can arise spontaneously during operation in the real world (open-road testing or simulation of randomly generated scenarios).   In AV testing, simulation serves primarily two purposes: to assist the development of a robust autonomous vehicle and to test and validate the AV before release. A challenge arises from the sheer number of scenario variations that can be constructed from each of the above sources due to the high number of variables involved (most of which are continuous). Even with continuous variables discretized, the possible number of combinations becomes practically infeasible to test. To overcome this challenge we propose using reinforcement learning (RL) to generate failure examples and unexpected traffic situations for the AV software implementation. Although reinforcement learning algorithms have achieved notable results in games and some robotic manipulations, this technique has not been widely scaled up to the more challenging real world applications like autonomous driving.",0
"Traditional testing methods and discrete verification are insufficient to meet the needs of closed-loop verification for safe testing of autonomous vehicles (AVs). Validation involves testing AV systems in situations that they would encounter in everyday driving after they are released. These scenarios can be controlled in a physical or virtual environment or may arise spontaneously during real-world operation. Simulation serves two main purposes in AV testing: assisting the development of a robust AV and testing and validating the AV before release. However, the large number of scenario variations that can be created due to the numerous continuous variables involved makes testing infeasible. To address this challenge, we propose using reinforcement learning (RL) to generate failure examples and unexpected traffic situations for the AV software implementation. Although RL algorithms have been successful in games and some robotic manipulations, applying them to more challenging real-world applications like autonomous driving has not been widely implemented.",1
"The combination of Monte-Carlo Tree Search (MCTS) and deep reinforcement learning is state-of-the-art in two-player perfect-information games. In this paper, we describe a search algorithm that uses a variant of MCTS which we enhanced by 1) a novel action value normalization mechanism for games with potentially unbounded rewards (which is the case in many optimization problems), 2) defining a virtual loss function that enables effective search parallelization, and 3) a policy network, trained by generations of self-play, to guide the search. We gauge the effectiveness of our method in ""SameGame""---a popular single-player test domain. Our experimental results indicate that our method outperforms baseline algorithms on several board sizes. Additionally, it is competitive with state-of-the-art search algorithms on a public set of positions.",0
"The most advanced technique for two-player perfect-information games is the combination of Monte-Carlo Tree Search (MCTS) and deep reinforcement learning. In this article, we present a search algorithm that utilizes a variant of MCTS, which we improved by implementing 1) a unique action value normalization mechanism that accommodates games with potentially infinite rewards (a common occurrence in optimization problems), 2) a virtual loss function that facilitates efficient parallel searching, and 3) a policy network that guides the search and has been trained through generations of self-play. We measure the efficacy of our approach in the popular single-player testing domain ""SameGame."" Our experimental results demonstrate that our technique surpasses baseline algorithms on various board sizes and is comparable to the most advanced search algorithms on a public set of positions.",1
"It is well known that ensemble methods often provide enhanced performance in reinforcement learning. In this paper, we explore this concept further by using group-aided training within the distributional reinforcement learning paradigm. Specifically, we propose an extension to categorical reinforcement learning, where distributional learning targets are implicitly based on the total information gathered by an ensemble. We empirically show that this may lead to much more robust initial learning, a stronger individual performance level, and good efficiency on a per-sample basis.",0
"The fact that ensemble methods can improve reinforcement learning is widely acknowledged. Our study delves deeper into this idea and introduces the concept of group-assisted training in the distributional reinforcement learning framework. Our approach expands on categorical reinforcement learning by incorporating distributional learning targets that take into account the collective information gathered by an ensemble. Through experimentation, we demonstrate that this can result in more resilient initial learning, higher individual performance, and improved efficiency per sample.",1
"Automatic search of neural architectures for various vision and natural language tasks is becoming a prominent tool as it allows to discover high-performing structures on any dataset of interest. Nevertheless, on more difficult domains, such as dense per-pixel classification, current automatic approaches are limited in their scope - due to their strong reliance on existing image classifiers they tend to search only for a handful of additional layers with discovered architectures still containing a large number of parameters. In contrast, in this work we propose a novel solution able to find light-weight and accurate segmentation architectures starting from only few blocks of a pre-trained classification network. To this end, we progressively build up a methodology that relies on templates of sets of operations, predicts which template and how many times should be applied at each step, while also generating the connectivity structure and downsampling factors. All these decisions are being made by a recurrent neural network that is rewarded based on the score of the emitted architecture on the holdout set and trained using reinforcement learning. One discovered architecture achieves 63.2% mean IoU on CamVid and 67.8% on CityScapes having only 270K parameters. Pre-trained models and the search code are available at https://github.com/DrSleep/nas-segm-pytorch.",0
"The use of automatic neural architecture search has become popular in discovering high-performing structures for vision and natural language tasks on any dataset. However, current automatic approaches have limitations in more difficult domains like dense per-pixel classification due to their reliance on existing image classifiers, resulting in the search for only a few additional layers with large numbers of parameters. This research proposes a new solution that can find accurate and lightweight segmentation architectures using only a few blocks of a pre-trained classification network. The methodology relies on templates of operation sets, predicting which template to apply and how many times, as well as generating connectivity structure and downsampling factors. A recurrent neural network makes these decisions and is trained through reinforcement learning based on the score of the architecture on the holdout set. The discovered architecture achieves 63.2% mean IoU on CamVid and 67.8% on CityScapes with only 270K parameters. The search code and pre-trained models are available at https://github.com/DrSleep/nas-segm-pytorch.",1
"In this paper, we propose the use of traditional animations, heuristic behavior and reinforcement learning in the creation of intelligent characters for computational media. The traditional animation and heuristic gives artistic control over the behavior while the reinforcement learning adds generalization. The use case presented is a dog character with a high-level controller in a 3D environment which is built around the desired behaviors to be learned, such as fetching an item. As the development of the environment is the key for learning, further analysis is conducted of how to build those learning environments, the effects of environment and agent modeling choices, training procedures and generalization of the learned behavior. This analysis builds insight of the aforementioned factors and may serve as guide in the development of environments in general.",0
"The introduction of intelligent characters for computational media is explored in this paper using a combination of traditional animations, heuristic behavior, and reinforcement learning. By utilizing traditional animation and heuristic behavior, the behavior of these characters can be artistically controlled, while reinforcement learning adds a level of generalization. The study presents a use case of a dog character with a high-level controller in a 3D environment, designed to teach specific behaviors such as fetching an item. The key to successful learning is the development of the environment, which is analyzed in more detail, including the effects of environment and agent modeling choices, training procedures, and generalization of the learned behavior. Through this analysis, valuable insight is gained, providing guidelines for the development of learning environments in general.",1
"Lane-change maneuvers are commonly executed by drivers to follow a certain routing plan, overtake a slower vehicle, adapt to a merging lane ahead, etc. However, improper lane change behaviors can be a major cause of traffic flow disruptions and even crashes. While many rule-based methods have been proposed to solve lane change problems for autonomous driving, they tend to exhibit limited performance due to the uncertainty and complexity of the driving environment. Machine learning-based methods offer an alternative approach, as Deep reinforcement learning (DRL) has shown promising success in many application domains including robotic manipulation, navigation, and playing video games. However, applying DRL to autonomous driving still faces many practical challenges in terms of slow learning rates, sample inefficiency, and safety concerns. In this study, we propose an automated lane change strategy using proximal policy optimization-based deep reinforcement learning, which shows great advantages in learning efficiency while still maintaining stable performance. The trained agent is able to learn a smooth, safe, and efficient driving policy to make lane-change decisions (i.e. when and how) in a challenging situation such as dense traffic scenarios. The effectiveness of the proposed policy is validated by using metrics of task success rate and collision rate. The simulation results demonstrate the lane change maneuvers can be efficiently learned and executed in a safe, smooth, and efficient manner.",0
"Drivers often perform lane-change maneuvers for various reasons, such as following a specific route, passing a slower vehicle, or merging into another lane. However, improper lane changes can cause traffic disruptions and accidents. While traditional rule-based methods have been proposed for autonomous driving, they have limited performance due to the uncertainty and complexity of the driving environment. Machine learning-based approaches, such as Deep reinforcement learning (DRL), have shown promise in other domains, but face practical challenges in autonomous driving. To address this, we propose an automated lane change strategy using proximal policy optimization-based deep reinforcement learning, which offers efficient learning and stable performance. Our approach enables the agent to learn safe and efficient lane-change decisions in challenging situations, such as dense traffic scenarios. We validate our strategy's effectiveness using metrics of task success rate and collision rate, showing that it can efficiently execute smooth and safe maneuvers.",1
"Greedy-GQ is an off-policy two timescale algorithm for optimal control in reinforcement learning. This paper develops the first finite-sample analysis for the Greedy-GQ algorithm with linear function approximation under Markovian noise. Our finite-sample analysis provides theoretical justification for choosing stepsizes for this two timescale algorithm for faster convergence in practice, and suggests a trade-off between the convergence rate and the quality of the obtained policy. Our paper extends the finite-sample analyses of two timescale reinforcement learning algorithms from policy evaluation to optimal control, which is of more practical interest. Specifically, in contrast to existing finite-sample analyses for two timescale methods, e.g., GTD, GTD2 and TDC, where their objective functions are convex, the objective function of the Greedy-GQ algorithm is non-convex. Moreover, the Greedy-GQ algorithm is also not a linear two-timescale stochastic approximation algorithm. Our techniques in this paper provide a general framework for finite-sample analysis of non-convex value-based reinforcement learning algorithms for optimal control.",0
"The Greedy-GQ algorithm is a two timescale algorithm used for optimal control in reinforcement learning. This paper presents the first analysis of the algorithm's performance with linear function approximation under Markovian noise. Our study provides a theoretical rationale for selecting stepsizes to achieve faster convergence in practice and highlights the trade-off between convergence rate and policy quality. Unlike previous analyses of convex two timescale methods such as GTD, GTD2, and TDC, our analysis extends to non-convex objective functions, which is crucial in optimal control. Additionally, the Greedy-GQ algorithm does not follow a linear two-timescale stochastic approximation algorithm. Our methods establish a comprehensive framework for analyzing non-convex value-based reinforcement learning algorithms for optimal control.",1
"Mixed integer linear programs are commonly solved by Branch and Bound algorithms. A key factor of the efficiency of the most successful commercial solvers is their fine-tuned heuristics. In this paper, we leverage patterns in real-world instances to learn from scratch a new branching strategy optimised for a given problem and compare it with a commercial solver. We propose FMSTS, a novel Reinforcement Learning approach specifically designed for this task. The strength of our method lies in the consistency between a local value function and a global metric of interest. In addition, we provide insights for adapting known RL techniques to the Branch and Bound setting, and present a new neural network architecture inspired from the literature. To our knowledge, it is the first time Reinforcement Learning has been used to fully optimise the branching strategy. Computational experiments show that our method is appropriate and able to generalise well to new instances.",0
"Branch and Bound algorithms are commonly used to solve Mixed Integer Linear Programs. The efficiency of successful commercial solvers is attributed to their well-tuned heuristics. This paper presents a new approach, FMSTS, which uses Reinforcement Learning to learn a new branching strategy from scratch, optimized for a specific problem. Our method is unique in its ability to maintain consistency between a local value function and a global metric of interest. We also provide insights on adapting Reinforcement Learning techniques to the Branch and Bound setting and present a neural network architecture inspired by existing literature. This is the first time that Reinforcement Learning has been used to optimize the branching strategy. Our computational experiments demonstrate that our method is suitable and can generalize well to new instances.",1
"Neural architecture search (NAS) remains a challenging problem, which is attributed to the indispensable and time-consuming component of performance estimation (PE). In this paper, we provide a novel yet systematic rethinking of PE in a resource constrained regime, termed budgeted PE (BPE), which precisely and effectively estimates the performance of an architecture sampled from an architecture space. Since searching an optimal BPE is extremely time-consuming as it requires to train a large number of networks for evaluation, we propose a Minimum Importance Pruning (MIP) approach. Given a dataset and a BPE search space, MIP estimates the importance of hyper-parameters using random forest and subsequently prunes the minimum one from the next iteration. In this way, MIP effectively prunes less important hyper-parameters to allocate more computational resource on more important ones, thus achieving an effective exploration. By combining BPE with various search algorithms including reinforcement learning, evolution algorithm, random search, and differentiable architecture search, we achieve 1, 000x of NAS speed up with a negligible performance drop comparing to the SOTA",0
"Designing neural architectures through Neural Architecture Search (NAS) is still a difficult task due to the time-consuming nature of Performance Estimation (PE). This paper introduces a new approach called Budgeted PE (BPE) that focuses on precise and effective performance estimation of an architecture sampled from an architecture space under resource constraints. However, BPE search is time-consuming and requires training numerous networks for evaluation. Therefore, this paper proposes a Minimum Importance Pruning (MIP) approach that estimates hyper-parameter importance using random forest and prunes the least important ones from the next iteration. MIP prunes less important hyper-parameters to allocate more computational resources to more important ones, leading to an effective exploration. By combining BPE with various search algorithms such as reinforcement learning, evolution algorithm, random search, and differentiable architecture search, we achieve 1, 000x of NAS speed up with negligible performance reduction compared to the SOTA.",1
"This paper introduced a reinforcement learning based decision support system in textile manufacturing process. A solution optimization problem of color fading ozonation is discussed and set up as a Markov Decision Process (MDP) in terms of tuple {S, A, P, R}. Q-learning is used to train an agent in the interaction with the setup environment by accumulating the reward R. According to the application result, it is found that the proposed MDP model has well expressed the optimization problem of textile manufacturing process discussed in this paper, therefore the use of reinforcement learning to support decision making in this sector is conducted and proven that is applicable with promising prospects.",0
"In this article, a decision support system for textile manufacturing process based on reinforcement learning was introduced. The problem of optimizing color fading ozonation was addressed and formulated as a Markov Decision Process (MDP) with the tuple {S, A, P, R}. An agent was trained using Q-learning by interacting with the environment and accumulating rewards. The results show that the MDP model effectively represents the optimization problem in textile manufacturing, supporting decision-making in this field. Therefore, the use of reinforcement learning in decision support for textile manufacturing is promising.",1
"Over the last decade, there has been significant progress in the field of machine learning for de novo drug design, particularly in deep generative models. However, current generative approaches exhibit a significant challenge as they do not ensure that the proposed molecular structures can be feasibly synthesized nor do they provide the synthesis routes of the proposed small molecules, thereby seriously limiting their practical applicability. In this work, we propose a novel forward synthesis framework powered by reinforcement learning (RL) for de novo drug design, Policy Gradient for Forward Synthesis (PGFS), that addresses this challenge by embedding the concept of synthetic accessibility directly into the de novo drug design system. In this setup, the agent learns to navigate through the immense synthetically accessible chemical space by subjecting commercially available small molecule building blocks to valid chemical reactions at every time step of the iterative virtual multi-step synthesis process. The proposed environment for drug discovery provides a highly challenging test-bed for RL algorithms owing to the large state space and high-dimensional continuous action space with hierarchical actions. PGFS achieves state-of-the-art performance in generating structures with high QED and penalized clogP. Moreover, we validate PGFS in an in-silico proof-of-concept associated with three HIV targets. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.",0
"Significant advancements have been made in the field of machine learning for de novo drug design, specifically in deep generative models, over the past decade. However, current generative methods have a significant limitation as they cannot guarantee the feasibility of synthesizing the proposed molecular structures or provide the synthesis routes of the proposed small molecules, limiting their practical applicability. This study proposes a novel forward synthesis framework called Policy Gradient for Forward Synthesis (PGFS) powered by reinforcement learning (RL) to overcome this challenge. Synthetic accessibility is embedded into the de novo drug design system, allowing the agent to navigate through the vast synthetically accessible chemical space by subjecting commercially available small molecule building blocks to valid chemical reactions during the iterative virtual multi-step synthesis process. The proposed environment for drug discovery presents a highly challenging test-bed for RL algorithms due to the large state space and high-dimensional continuous action space with hierarchical actions. PGFS achieves state-of-the-art performance in generating structures with high QED and penalized clogP. Additionally, PGFS is validated in an in-silico proof-of-concept associated with three HIV targets. Finally, the end-to-end training conceptualized in this study represents a paradigm that can radically expand the synthesizable chemical space and automate the drug discovery process.",1
"Exploration of the high-dimensional state action space is one of the biggest challenges in Reinforcement Learning (RL), especially in multi-agent domain. We present a novel technique called Experience Augmentation, which enables a time-efficient and boosted learning based on a fast, fair and thorough exploration to the environment. It can be combined with arbitrary off-policy MARL algorithms and is applicable to either homogeneous or heterogeneous environments. We demonstrate our approach by combining it with MADDPG and verifing the performance in two homogeneous and one heterogeneous environments. In the best performing scenario, the MADDPG with experience augmentation reaches to the convergence reward of vanilla MADDPG with 1/4 realistic time, and its convergence beats the original model by a significant margin. Our ablation studies show that experience augmentation is a crucial ingredient which accelerates the training process and boosts the convergence.",0
"Reinforcement Learning (RL), particularly in multi-agent domains, faces a significant challenge in exploring the high-dimensional state action space. To address this, we introduce a new technique called Experience Augmentation that enables a faster and more thorough exploration of the environment, leading to more efficient learning. This approach can be used with any off-policy MARL algorithms, and is effective for both homogeneous and heterogeneous environments. Our experiments show that when combined with MADDPG, Experience Augmentation significantly improves performance in two homogeneous and one heterogeneous environments. In the best performing scenario, the convergence reward of MADDPG with Experience Augmentation is achieved in just 1/4 of the time required by vanilla MADDPG, and outperforms the original model by a significant margin. Our ablation studies reveal that Experience Augmentation is a crucial component for accelerating the training process and enhancing the convergence of the model.",1
"The goal of this work is to provide a viable solution based on reinforcement learning for traffic signal control problems. Although the state-of-the-art reinforcement learning approaches have yielded great success in a variety of domains, directly applying it to alleviate traffic congestion can be challenging, considering the requirement of high sample efficiency and how training data is gathered. In this work, we address several challenges that we encountered when we attempted to mitigate serious traffic congestion occurring in a metropolitan area. Specifically, we are required to provide a solution that is able to (1) handle the traffic signal control when certain surveillance cameras that retrieve information for reinforcement learning are down, (2) learn from batch data without a traffic simulator, and (3) make control decisions without shared information across intersections. We present a two-stage framework to deal with the above-mentioned situations. The framework can be decomposed into an Evolution Strategies approach that gives a fixed-time traffic signal control schedule and a multi-agent off-policy reinforcement learning that is capable of learning from batch data with the aid of three proposed components, bounded action, batch augmentation, and surrogate reward clipping. Our experiments show that the proposed framework reduces traffic congestion by 36% in terms of waiting time compared with the currently used fixed-time traffic signal plan. Furthermore, the framework requires only 600 queries to a simulator to achieve the result.",0
"The aim of this study is to offer a feasible remedy for traffic signal control issues using reinforcement learning. Although reinforcement learning methods have achieved immense success in various fields, applying this approach to alleviate traffic congestion poses a challenge due to the need for high sample efficiency and data collection for training. This study addresses several challenges that arise when attempting to reduce severe traffic congestion in a metropolitan area. These challenges involve devising a solution that can handle traffic signal control when surveillance cameras are not functioning, learn from batch data without a traffic simulator, and make control decisions without shared information across intersections. To tackle these challenges, a two-stage framework is presented, consisting of an Evolution Strategies approach that generates a traffic signal control schedule and a multi-agent off-policy reinforcement learning that can learn from batch data using three proposed components: bounded action, batch augmentation, and surrogate reward clipping. The experiments conducted indicate that the proposed framework reduces waiting time by 36% compared to the current fixed-time traffic signal plan, and only 600 simulator queries are required to achieve this result.",1
"The successful application of general reinforcement learning algorithms to real-world robotics applications is often limited by their high data requirements. We introduce Regularized Hierarchical Policy Optimization (RHPO) to improve data-efficiency for domains with multiple dominant tasks and ultimately reduce required platform time. To this end, we employ compositional inductive biases on multiple levels and corresponding mechanisms for sharing off-policy transition data across low-level controllers and tasks as well as scheduling of tasks. The presented algorithm enables stable and fast learning for complex, real-world domains in the parallel multitask and sequential transfer case. We show that the investigated types of hierarchy enable positive transfer while partially mitigating negative interference and evaluate the benefits of additional incentives for efficient, compositional task solutions in single task domains. Finally, we demonstrate substantial data-efficiency and final performance gains over competitive baselines in a week-long, physical robot stacking experiment.",0
"The high amount of data required for general reinforcement learning algorithms to be successful in real-world robotics applications often limits their effectiveness. Our solution to this problem is the introduction of Regularized Hierarchical Policy Optimization (RHPO), which enhances data-efficiency in domains with multiple dominant tasks and reduces the required platform time. We achieve this by implementing compositional inductive biases on multiple levels and mechanisms for sharing off-policy transition data across low-level controllers and tasks, as well as scheduling of tasks. This algorithm facilitates stable and fast learning for complex, real-world domains in both parallel multitask and sequential transfer cases. Our research indicates that hierarchy types have a positive transfer effect while partially mitigating negative interference, and we evaluate the benefits of additional incentives for efficient, compositional task solutions in single task domains. Finally, we demonstrate a significant increase in data-efficiency and final performance gains over competitive baselines in a week-long physical robot stacking experiment.",1
"Video is an essential imaging modality for diagnostics, e.g. in ultrasound imaging, for endoscopy, or movement assessment. However, video hasn't received a lot of attention in the medical image analysis community. In the clinical practice, it is challenging to utilise raw diagnostic video data efficiently as video data takes a long time to process, annotate or audit. In this paper we introduce a novel, fully automatic video summarization method that is tailored to the needs of medical video data. Our approach is framed as reinforcement learning problem and produces agents focusing on the preservation of important diagnostic information. We evaluate our method on videos from fetal ultrasound screening, where commonly only a small amount of the recorded data is used diagnostically. We show that our method is superior to alternative video summarization methods and that it preserves essential information required by clinical diagnostic standards.",0
"Although video is a crucial tool in diagnostics, such as ultrasound imaging, endoscopy, and movement assessment, it has not received sufficient attention from the medical image analysis community. The medical field struggles to process, annotate, and audit raw diagnostic video data because it is time-consuming. In this study, we present a new, fully automated video summarization technique that caters specifically to medical video data. Our reinforcement learning-based approach creates agents that prioritize the preservation of crucial diagnostic information. We assess our method using videos from fetal ultrasound screening, where only a small portion of recorded data is typically utilized for diagnosis. Our results indicate that our approach outperforms other video summarization methods and preserves vital information necessary for clinical diagnostic standards.",1
"Reinforcement learning (RL) methods learn optimal decisions in the presence of a stationary environment. However, the stationary assumption on the environment is very restrictive. In many real world problems like traffic signal control, robotic applications, one often encounters situations with non-stationary environments and in these scenarios, RL methods yield sub-optimal decisions. In this paper, we thus consider the problem of developing RL methods that obtain optimal decisions in a non-stationary environment. The goal of this problem is to maximize the long-term discounted reward achieved when the underlying model of the environment changes over time. To achieve this, we first adapt a change point algorithm to detect change in the statistics of the environment and then develop an RL algorithm that maximizes the long-run reward accrued. We illustrate that our change point method detects change in the model of the environment effectively and thus facilitates the RL algorithm in maximizing the long-run reward. We further validate the effectiveness of the proposed solution on non-stationary random Markov decision processes, a sensor energy management problem and a traffic signal control problem.",0
"Reinforcement learning (RL) methods are effective in determining optimal decisions in a stationary environment. However, this assumption severely limits their applicability, particularly in real-world scenarios like robotic applications and traffic signal control, which often involve non-stationary environments. When confronted with these situations, RL methods produce sub-optimal decisions. In this paper, we focus on developing RL methods that can produce optimal decisions in non-stationary environments. The objective is to maximize the long-term discounted reward achieved as the underlying environment model changes over time. We achieve this by first adapting a change point algorithm that detects changes in the environment's statistics. We then develop an RL algorithm that maximizes the long-term reward earned. Our change point method effectively detects changes in the environment model and enables the RL algorithm to maximize long-term rewards. We validate our approach on non-stationary random Markov decision processes, a sensor energy management problem, and a traffic signal control problem.",1
"Reinforcement learning (RL) algorithms find applications in inventory control, recommender systems, vehicular traffic management, cloud computing and robotics. The real-world complications of many tasks arising in these domains makes them difficult to solve with the basic assumptions underlying classical RL algorithms. RL agents in these applications often need to react and adapt to changing operating conditions. A significant part of research on single-agent RL techniques focuses on developing algorithms when the underlying assumption of stationary environment model is relaxed. This paper provides a survey of RL methods developed for handling dynamically varying environment models. The goal of methods not limited by the stationarity assumption is to help autonomous agents adapt to varying operating conditions. This is possible either by minimizing the rewards lost during learning by RL agent or by finding a suitable policy for the RL agent which leads to efficient operation of the underlying system. A representative collection of these algorithms is discussed in detail in this work along with their categorization and their relative merits and demerits. Additionally we also review works which are tailored to application domains. Finally, we discuss future enhancements for this field.",0
"RL algorithms have diverse applications in domains such as inventory control, recommender systems, vehicular traffic management, cloud computing, and robotics. However, the non-linear and complex nature of real-world tasks in these domains poses a challenge for classical RL algorithms. In such scenarios, RL agents must be able to respond and adapt to changing environmental conditions. To address this, research has focused on developing RL methods that do not rely on the assumption of a stationary environment model. The aim of these methods is to enable autonomous agents to adapt to varying conditions by minimizing lost rewards or by finding a suitable policy for efficient system operation. This paper presents a survey of such RL techniques, categorized by their relative merits and demerits. Additionally, we review works tailored to specific application domains and discuss future directions for this field.",1
"Using privileged information during training can improve the sample efficiency and performance of machine learning systems. This paradigm has been applied to reinforcement learning (RL), primarily in the form of distillation or auxiliary tasks, and less commonly in the form of augmenting the inputs of agents. In this work, we investigate Privileged Information Dropout (\pid) for achieving the latter which can be applied equally to value-based and policy-based RL algorithms. Within a simple partially-observed environment, we demonstrate that \pid outperforms alternatives for leveraging privileged information, including distillation and auxiliary tasks, and can successfully utilise different types of privileged information. Finally, we analyse its effect on the learned representations.",0
"The utilization of confidential knowledge during training could enhance the efficiency of machine learning systems and their overall performance. This approach has been implemented in reinforcement learning (RL), mostly through distillation or auxiliary tasks, and less frequently through enhancing agents' inputs. Our study focuses on Privileged Information Dropout (\pid) as a means of achieving the latter, which is equally applicable to policy-based and value-based RL algorithms. Our research demonstrates that \pid is more effective than other methods of leveraging confidential knowledge, including distillation and auxiliary tasks, and can effectively apply various forms of privileged information in a simple partially-observed environment. Finally, we investigate its impact on the learned representations.",1
"Instance segmentation is one of the actively studied research topics in computer vision in which many objects of interest should be separated individually. While many feed-forward networks produce high-quality segmentation on different types of images, their results often suffer from topological errors (merging or splitting) for segmentation of many objects, requiring post-processing. Existing iterative methods, on the other hand, extract a single object at a time using discriminative knowledge-based properties (shapes, boundaries, etc.) without relying on post-processing, but they do not scale well. To exploit the advantages of conventional single-object-per-step segmentation methods without impairing the scalability, we propose a novel iterative deep reinforcement learning agent that learns how to differentiate multiple objects in parallel. Our reward function for the trainable agent is designed to favor grouping pixels belonging to the same object using a graph coloring algorithm. We demonstrate that the proposed method can efficiently perform instance segmentation of many objects without heavy post-processing.",0
"Computer vision researchers actively study instance segmentation, which involves separating many objects of interest individually. Although many feed-forward networks produce high-quality segmentation on various image types, they often suffer from topological errors such as merging or splitting when segmenting many objects, requiring post-processing. Existing iterative methods extract one object at a time using discriminative knowledge-based properties, such as shapes and boundaries, without relying on post-processing, but they do not scale well. To leverage the benefits of single-object-per-step segmentation methods without compromising scalability, we propose a novel iterative deep reinforcement learning agent that can differentiate multiple objects in parallel. Our trainable agent's reward function favors grouping pixels belonging to the same object using a graph coloring algorithm. We demonstrate that our proposed method can efficiently perform instance segmentation of many objects without requiring extensive post-processing.",1
"Although the challenge of the device connection is much relieved in 5G networks, the training latency is still an obstacle preventing Federated Learning (FL) from being largely adopted. One of the most fundamental problems that lead to large latency is the bad candidate-selection for FL. In the dynamic environment, the mobile devices selected by the existing reactive candidate-selection algorithms very possibly fail to complete the training and reporting phases of FL, because the FL parameter server only knows the currently-observed resources of all candidates. To this end, we study the proactive candidate-selection for FL in this paper. We first let each candidate device predict the qualities of both its training and reporting phases locally using LSTM. Then, the proposed candidateselection algorithm is implemented by the Deep Reinforcement Learning (DRL) framework. Finally, the real-world trace-driven experiments prove that the proposed approach outperforms the existing reactive algorithms",0
"Although 5G networks have made device connection less challenging, the adoption of Federated Learning (FL) is still hindered by training latency. This is primarily due to poor candidate-selection for FL which results in selected mobile devices failing to complete the training and reporting phases. Reactive candidate-selection algorithms rely on currently observed resources of all candidates, making them less effective in dynamic environments. In this paper, we propose a proactive candidate-selection approach that employs LSTM to enable each candidate device to predict the qualities of both its training and reporting phases locally. The Deep Reinforcement Learning (DRL) framework is then used to implement the proposed candidate-selection algorithm. Our real-world trace-driven experiments demonstrate that the proactive approach outperforms existing reactive algorithms.",1
"Intelligent agents need a physical understanding of the world to predict the impact of their actions in the future. While learning-based models of the environment dynamics have contributed to significant improvements in sample efficiency compared to model-free reinforcement learning algorithms, they typically fail to generalize to system states beyond the training data, while often grounding their predictions on non-interpretable latent variables.   We introduce Interactive Differentiable Simulation (IDS), a differentiable physics engine, that allows for efficient, accurate inference of physical properties of rigid-body systems. Integrated into deep learning architectures, our model is able to accomplish system identification using visual input, leading to an interpretable model of the world whose parameters have physical meaning. We present experiments showing automatic task-based robot design and parameter estimation for nonlinear dynamical systems by automatically calculating gradients in IDS. When integrated into an adaptive model-predictive control algorithm, our approach exhibits orders of magnitude improvements in sample efficiency over model-free reinforcement learning algorithms on challenging nonlinear control domains.",0
"For intelligent agents to predict the consequences of their actions, they must have a comprehensive understanding of the physical world. Learning-based models of environmental dynamics have improved sample efficiency, but they often struggle to generalize beyond the training data and rely on non-interpretable latent variables. To address this issue, we have developed Interactive Differentiable Simulation (IDS), a differentiable physics engine that enables accurate and efficient inference of rigid-body system properties. By integrating IDS into deep learning models, we can use visual input to identify system parameters in an interpretable manner. Our experiments demonstrate the effectiveness of IDS in automatic task-based robot design and parameter estimation for nonlinear dynamical systems. When combined with an adaptive model-predictive control algorithm, our approach significantly outperforms model-free reinforcement learning algorithms in terms of sample efficiency on challenging nonlinear control domains.",1
"We consider un-discounted reinforcement learning (RL) in Markov decision processes (MDPs) under temporal drifts, ie, both the reward and state transition distributions are allowed to evolve over time, as long as their respective total variations, quantified by suitable metrics, do not exceed certain variation budgets. This setting captures the endogeneity, exogeneity, uncertainty, and partial feedback in sequential decision-making scenarios, and finds applications in vehicle remarketing and real-time bidding. We first develop the Sliding Window Upper-Confidence bound for Reinforcement Learning with Confidence Widening (SWUCRL2-CW) algorithm, and establish its dynamic regret bound when the variation budgets are known. In addition, we propose the Bandit-over-Reinforcement Learning (BORL) algorithm to adaptively tune the SWUCRL2-CW algorithm to achieve the same dynamic regret bound, but in a parameter-free manner, ie, without knowing the variation budgets. Finally, we conduct numerical experiments to show that our proposed algorithms achieve superior empirical performance compared to existing algorithms.   Notably, the interplay between endogeneity and exogeneity presents a unique challenge, absent in existing (stationary and non-stationary) stochastic online learning settings, when we apply the conventional Optimism in Face of Uncertainty principle to design algorithms with provably low dynamic regret for RL in drifting MDPs. We overcome the challenge by a novel confidence widening technique that incorporates additional optimism into our learning algorithms to ensure low dynamic regret bounds. To extend our theoretical findings, we apply our framework to inventory control problems, and demonstrate how one can alternatively leverage special structures on the state transition distributions to bypass the difficulty in exploring time-varying environments.",0
"Our focus is on un-discounted reinforcement learning (RL) in Markov decision processes (MDPs), where both the reward and state transition distributions can evolve over time, within certain variation budgets. This captures the complexities of sequential decision-making scenarios, such as endogeneity, exogeneity, uncertainty, and partial feedback, and has applications in vehicle remarketing and real-time bidding. We introduce the Sliding Window Upper-Confidence bound for Reinforcement Learning with Confidence Widening (SWUCRL2-CW) algorithm, which has a dynamic regret bound when the variation budgets are known. We also propose the Bandit-over-Reinforcement Learning (BORL) algorithm, which can adaptively tune the SWUCRL2-CW algorithm to achieve the same dynamic regret bound without knowing the variation budgets. Our numerical experiments show that our algorithms outperform existing ones. The interplay between endogeneity and exogeneity poses a unique challenge that we overcome with a confidence widening technique that incorporates additional optimism into our learning algorithms to ensure low dynamic regret bounds. We apply our framework to inventory control problems, demonstrating how we can leverage special structures on the state transition distributions to explore time-varying environments.",1
"Extending the capabilities of robotics to real-world complex, unstructured environments requires the need of developing better perception systems while maintaining low sample complexity. When dealing with high-dimensional state spaces, current methods are either model-free or model-based based on reconstruction objectives. The sample inefficiency of the former constitutes a major barrier for applying them to the real-world. The later, while they present low sample complexity, they learn latent spaces that need to reconstruct every single detail of the scene. In real environments, the task typically just represents a small fraction of the scene. Reconstruction objectives suffer in such scenarios as they capture all the unnecessary components. In this work, we present MIRO, an information theoretic representational learning algorithm for model-based reinforcement learning. We design a latent space that maximizes the mutual information with the future information while being able to capture all the information needed for planning. We show that our approach is more robust than reconstruction objectives in the presence of distractors and cluttered scenes",0
"To equip robots with the ability to navigate complex, unstructured environments, it is essential to improve perception systems without increasing sample complexity. Currently, methods for dealing with high-dimensional state spaces are either model-free or model-based based on reconstruction objectives. However, the former is inefficient with samples, making it difficult to apply in the real world. The latter has low sample complexity, but it learns latent spaces that need to reconstruct every detail of the scene, including unnecessary components. In this paper, we introduce MIRO, an information-theoretic representational learning algorithm for model-based reinforcement learning. Our approach maximizes mutual information with future information while capturing only the necessary information for planning. We demonstrate that our approach is more robust than reconstruction objectives in cluttered environments with distractors.",1
"Current model-based reinforcement learning approaches use the model simply as a learned black-box simulator to augment the data for policy optimization or value function learning. In this paper, we show how to make more effective use of the model by exploiting its differentiability. We construct a policy optimization algorithm that uses the pathwise derivative of the learned model and policy across future timesteps. Instabilities of learning across many timesteps are prevented by using a terminal value function, learning the policy in an actor-critic fashion. Furthermore, we present a derivation on the monotonic improvement of our objective in terms of the gradient error in the model and value function. We show that our approach (i) is consistently more sample efficient than existing state-of-the-art model-based algorithms, (ii) matches the asymptotic performance of model-free algorithms, and (iii) scales to long horizons, a regime where typically past model-based approaches have struggled.",0
"At present, reinforcement learning methods based on models typically use the model as a black-box simulator to improve policy optimization or value function learning. This paper proposes a more effective approach to utilizing the model by taking advantage of its differentiability. The authors introduce a policy optimization algorithm that employs the pathwise derivative of the learned model and policy over future timesteps. To avoid instabilities in learning across many timesteps, the algorithm utilizes a terminal value function and learns the policy in an actor-critic manner. Additionally, the authors demonstrate that their method leads to a monotonic improvement of the objective in terms of the gradient error in the model and value function. They show that their approach is more sample efficient than existing state-of-the-art model-based algorithms, matches the asymptotic performance of model-free algorithms, and can scale to long horizons, an area where prior model-based approaches have encountered difficulties.",1
"In many settings (e.g., robotics) demonstrations provide a natural way to specify tasks; however, most methods for learning from demonstrations either do not provide guarantees that the artifacts learned for the tasks, such as rewards or policies, can be safely composed and/or do not explicitly capture history dependencies. Motivated by this deficit, recent works have proposed learning Boolean task specifications, a class of Boolean non-Markovian rewards which admit well-defined composition and explicitly handle historical dependencies. This work continues this line of research by adapting maximum causal entropy inverse reinforcement learning to estimate the posteriori probability of a specification given a multi-set of demonstrations. The key algorithmic insight is to leverage the extensive literature and tooling on reduced ordered binary decision diagrams to efficiently encode a time unrolled Markov Decision Process. This enables transforming a naive exponential time algorithm into a polynomial time algorithm.",0
"Demonstrations are a common way to define tasks in various settings, such as robotics. However, most methods of learning from demonstrations do not ensure that the learned artifacts for tasks, such as policies or rewards, can be safely combined, or incorporate historical dependencies. Recent studies have proposed using Boolean task specifications, which are a type of Boolean non-Markovian rewards that allow for well-defined composition and explicitly handle historical dependencies. This research builds on this idea by adapting maximum causal entropy inverse reinforcement learning to estimate the probability of a specification after observing multiple demonstrations. The algorithm uses reduced ordered binary decision diagrams to efficiently encode a time unrolled Markov Decision Process, which transforms an exponential time algorithm into a polynomial time algorithm.",1
"Temporal-Difference (TD) learning is a standard and very successful reinforcement learning approach, at the core of both algorithms that learn the value of a given policy, as well as algorithms which learn how to improve policies. TD-learning with eligibility traces provides a way to boost sample efficiency by temporal credit assignment, i.e. deciding which portion of a reward should be assigned to predecessor states that occurred at different previous times, controlled by a parameter $\lambda$. However, tuning this parameter can be time-consuming, and not tuning it can lead to inefficient learning. For better sample efficiency of TD-learning, we propose a meta-learning method for adjusting the eligibility trace parameter, in a state-dependent manner. The adaptation is achieved with the help of auxiliary learners that learn distributional information about the update targets online, incurring roughly the same computational complexity per step as the usual value learner. Our approach can be used both in on-policy and off-policy learning. We prove that, under some assumptions, the proposed method improves the overall quality of the update targets, by minimizing the overall target error. This method can be viewed as a plugin to assist prediction with function approximation by meta-learning feature (observation)-based $\lambda$ online, or even in the control case to assist policy improvement. Our empirical evaluation demonstrates significant performance improvements, as well as improved robustness of the proposed algorithm to learning rate variation.",0
"Temporal-Difference (TD) learning is a commonly used and highly effective reinforcement learning technique. It is utilized in algorithms that learn the value of a given policy, as well as those that learn how to improve policies. TD-learning with eligibility traces is a method that enhances sample efficiency by assigning credit to predecessor states that occurred at different previous times, controlled by a parameter $\lambda$. However, adjusting this parameter can be time-consuming and not adjusting it can lead to inefficient learning. To improve sample efficiency of TD-learning, we propose a meta-learning approach that adjusts the eligibility trace parameter in a state-dependent manner. This adaptation is accomplished using auxiliary learners that learn distributional information about the update targets online, with similar computational complexity per step as the usual value learner. Our approach can be used in both on-policy and off-policy learning and has been shown to minimize overall target error. This method can be used as a plugin to assist prediction with function approximation by meta-learning observation-based $\lambda$ online, or even in the control case to assist policy improvement. Our empirical evaluation has demonstrated significant performance improvements, as well as improved robustness of the proposed algorithm to learning rate variation.",1
"Fatigue is the most vital factor of road fatalities and one manifestation of fatigue during driving is drowsiness. In this paper, we propose using deep Q-learning to analyze an electroencephalogram (EEG) dataset captured during a simulated endurance driving test. By measuring the correlation between drowsiness and driving performance, this experiment represents an important brain-computer interface (BCI) paradigm especially from an application perspective. We adapt the terminologies in the driving test to fit the reinforcement learning framework, thus formulate the drowsiness estimation problem as an optimization of a Q-learning task. By referring to the latest deep Q-Learning technologies and attending to the characteristics of EEG data, we tailor a deep Q-network for action proposition that can indirectly estimate drowsiness. Our results show that the trained model can trace the variations of mind state in a satisfactory way against the testing EEG data, which demonstrates the feasibility and practicability of this new computation paradigm. We also show that our method outperforms the supervised learning counterpart and is superior for real applications. To the best of our knowledge, we are the first to introduce the deep reinforcement learning method to this BCI scenario, and our method can be potentially generalized to other BCI cases.",0
"The leading cause of road accidents is fatigue, and one sign of fatigue while driving is feeling sleepy. This study suggests using deep Q-learning to analyze an EEG dataset obtained during a simulated endurance driving test. By examining the relationship between drowsiness and driving performance, this experiment holds significant value for brain-computer interface (BCI) applications. The terminology used in the driving test has been adapted to suit the reinforcement learning framework, and the problem of estimating drowsiness has been formulated as a Q-learning task optimization. A deep Q-network has been created for action proposal that can indirectly estimate drowsiness, using the latest deep Q-learning technologies and EEG data characteristics. The results indicate that the trained model can effectively track changes in the state of mind against the testing EEG data, proving the feasibility and practicality of this new computation paradigm. It is also demonstrated that our method is superior to the supervised learning counterpart and is better suited for real-world applications. This study is the first to introduce the deep reinforcement learning method to this BCI scenario and has the potential to be applied to other BCI cases.",1
"Reinforcement learning (RL) for robotics is challenging due to the difficulty in hand-engineering a dense cost function, which can lead to unintended behavior, and dynamical uncertainty, which makes exploration and constraint satisfaction challenging. We address these issues with a new model-based reinforcement learning algorithm, Safety Augmented Value Estimation from Demonstrations (SAVED), which uses supervision that only identifies task completion and a modest set of suboptimal demonstrations to constrain exploration and learn efficiently while handling complex constraints. We then compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms on 6 standard simulation benchmarks involving navigation and manipulation and a physical knot-tying task on the da Vinci surgical robot. Results suggest that SAVED outperforms prior methods in terms of success rate, constraint satisfaction, and sample efficiency, making it feasible to safely learn a control policy directly on a real robot in less than an hour. For tasks on the robot, baselines succeed less than 5% of the time while SAVED has a success rate of over 75% in the first 50 training iterations. Code and supplementary material is available at https://tinyurl.com/saved-rl.",0
"Reinforcement learning in robotics is a complex task due to the challenges involved in creating a dense cost function that can lead to unintended behavior and the uncertainty of dynamics, which makes exploring and satisfying constraints difficult. To tackle these issues, a new model-based reinforcement learning algorithm called Safety Augmented Value Estimation from Demonstrations (SAVED) has been developed. This algorithm utilizes supervision that identifies only task completion along with a modest set of suboptimal demonstrations to facilitate exploration and efficient learning while managing complex constraints. The performance of SAVED was compared with three state-of-the-art model-based and model-free RL algorithms across six standard simulation benchmarks that involved navigation and manipulation, as well as a physical knot-tying task on the da Vinci surgical robot. The results indicate that SAVED outperforms previous methods in terms of success rate, constraint satisfaction, and sample efficiency, making it possible to safely learn a control policy directly on a real robot in less than an hour. In contrast, the baselines achieved less than 5% success rate, while SAVED achieved a success rate of over 75% in the first 50 training iterations. The code and supplementary materials for SAVED can be found at https://tinyurl.com/saved-rl.",1
"Existing on-policy imitation learning algorithms, such as DAgger, assume access to a fixed supervisor. However, there are many settings where the supervisor may evolve during policy learning, such as a human performing a novel task or an improving algorithmic controller. We formalize imitation learning from a ""converging supervisor"" and provide sublinear static and dynamic regret guarantees against the best policy in hindsight with labels from the converged supervisor, even when labels during learning are only from intermediate supervisors. We then show that this framework is closely connected to a class of reinforcement learning (RL) algorithms known as dual policy iteration (DPI), which alternate between training a reactive learner with imitation learning and a model-based supervisor with data from the learner. Experiments suggest that when this framework is applied with the state-of-the-art deep model-based RL algorithm PETS as an improving supervisor, it outperforms deep RL baselines on continuous control tasks and provides up to an 80-fold speedup in policy evaluation.",0
"The current imitation learning algorithms, like DAgger, assume that the supervisor remains constant. However, in some cases, the supervisor may change during the learning process, like when a human performs a new task or an algorithmic controller improves. We have developed a model for imitation learning from a ""converging supervisor"" that guarantees sublinear static and dynamic regret against the best policy in hindsight using labels from the converged supervisor, even if the labels during learning are only from intermediate supervisors. We have also discovered that this model is closely connected to a class of reinforcement learning algorithms called dual policy iteration (DPI), which alternate between teaching a reactive learner with imitation learning and a model-based supervisor with data from the learner. Our experiments show that when we apply this framework with the state-of-the-art deep model-based RL algorithm PETS as an improving supervisor, it outperforms deep RL baselines on continuous control tasks and provides up to an 80-fold speedup in policy evaluation.",1
"Modern reinforcement learning algorithms can learn solutions to increasingly difficult control problems while at the same time reduce the amount of prior knowledge needed for their application. One of the remaining challenges is the definition of reward schemes that appropriately facilitate exploration without biasing the solution in undesirable ways, and that can be implemented on real robotic systems without expensive instrumentation. In this paper we focus on a setting in which goal tasks are defined via simple sparse rewards, and exploration is facilitated via agent-internal auxiliary tasks. We introduce the idea of simple sensor intentions (SSIs) as a generic way to define auxiliary tasks. SSIs reduce the amount of prior knowledge that is required to define suitable rewards. They can further be computed directly from raw sensor streams and thus do not require expensive and possibly brittle state estimation on real systems. We demonstrate that a learning system based on these rewards can solve complex robotic tasks in simulation and in real world settings. In particular, we show that a real robotic arm can learn to grasp and lift and solve a Ball-in-a-Cup task from scratch, when only raw sensor streams are used for both controller input and in the auxiliary reward definition.",0
"Reinforcement learning algorithms in modern times have the ability to solve control problems that are increasingly difficult, while also decreasing the need for prior knowledge during application. Nonetheless, the challenge that remains is devising reward systems that enable exploration without causing any undue bias in the solution, and can be implemented on robotic systems without requiring expensive instrumentation. This paper concentrates on a scenario where simple sparse rewards are used to define goal tasks, and the exploration is encouraged by auxiliary tasks that are internal to the agent. Simple sensor intentions (SSIs) are proposed as a universal way to define auxiliary tasks, which helps decrease the requirement for prior knowledge to define appropriate rewards. Additionally, they can be directly computed from raw sensor streams, making state estimation redundant and costly for actual systems. We demonstrate that a learning system based on these rewards can successfully complete complicated robotic tasks in both simulation and real-world settings. Specifically, we prove that a real robotic arm can learn to grasp and lift, and solve the Ball-in-a-Cup task from the ground up, while employing raw sensor streams for both controller input and in the auxiliary reward definition.",1
"Many real-world problems require trading off multiple competing objectives. However, these objectives are often in different units and/or scales, which can make it challenging for practitioners to express numerical preferences over objectives in their native units. In this paper we propose a novel algorithm for multi-objective reinforcement learning that enables setting desired preferences for objectives in a scale-invariant way. We propose to learn an action distribution for each objective, and we use supervised learning to fit a parametric policy to a combination of these distributions. We demonstrate the effectiveness of our approach on challenging high-dimensional real and simulated robotics tasks, and show that setting different preferences in our framework allows us to trace out the space of nondominated solutions.",0
"Real-life problems frequently involve balancing multiple objectives that compete with each other. However, these objectives often vary in terms of units and scales, posing a challenge for professionals to express numerical preferences for objectives in their original units. This study introduces a new multi-objective reinforcement learning algorithm that enables individuals to set their preferred objectives in a scale-independent manner. The algorithm involves learning an action distribution for each objective and using supervised learning to fit a parametric policy that combines these distributions. The effectiveness of this approach is demonstrated in complex, high-dimensional robotics tasks, where different preferences can be set to explore the range of nondominated solutions.",1
"Traditional distributed deep reinforcement learning (RL) commonly relies on exchanging the experience replay memory (RM) of each agent. Since the RM contains all state observations and action policy history, it may incur huge communication overhead while violating the privacy of each agent. Alternatively, this article presents a communication-efficient and privacy-preserving distributed RL framework, coined federated reinforcement distillation (FRD). In FRD, each agent exchanges its proxy experience replay memory (ProxRM), in which policies are locally averaged with respect to proxy states clustering actual states. To provide FRD design insights, we present ablation studies on the impact of ProxRM structures, neural network architectures, and communication intervals. Furthermore, we propose an improved version of FRD, coined mixup augmented FRD (MixFRD), in which ProxRM is interpolated using the mixup data augmentation algorithm. Simulations in a Cartpole environment validate the effectiveness of MixFRD in reducing the variance of mission completion time and communication cost, compared to the benchmark schemes, vanilla FRD, federated reinforcement learning (FRL), and policy distillation (PD).",0
"The conventional method of distributed deep reinforcement learning involves exchanging the experience replay memory (RM) of each agent, which can result in high communication overhead and violate the privacy of individual agents. To address these issues, this article proposes a federated reinforcement distillation (FRD) framework that is both communication-efficient and privacy-preserving. In FRD, agents exchange their proxy experience replay memory (ProxRM), which averages policies locally with respect to proxy states that cluster actual states. The article also presents ablation studies on ProxRM structures, neural network architectures, and communication intervals to provide design insights for FRD. An improved version of FRD, called mixup augmented FRD (MixFRD), is proposed, where ProxRM is interpolated using the mixup data augmentation algorithm. Simulations in a Cartpole environment demonstrate that MixFRD is more effective in reducing the variance of mission completion time and communication cost compared to benchmark schemes, including vanilla FRD, federated reinforcement learning (FRL), and policy distillation (PD).",1
"We present a self-learning approach that combines deep reinforcement learning and Monte Carlo tree search to solve the traveling salesman problem. The proposed approach has two advantages. First, it adopts deep reinforcement learning to compute the value functions for decision, which removes the need of hand-crafted features and labelled data. Second, it uses Monte Carlo tree search to select the best policy by comparing different value functions, which increases its generalization ability. Experimental results show that the proposed method performs favorably against other methods in small-to-medium problem settings. And it shows comparable performance as state-of-the-art in large problem setting.",0
"Our study introduces a self-learning method that merges Monte Carlo tree search with deep reinforcement learning to tackle the traveling salesman problem. This method offers two benefits. Firstly, it leverages deep reinforcement learning to calculate decision value functions, thereby eliminating the necessity for hand-crafted features and labelled data. Secondly, it utilizes Monte Carlo tree search to choose the optimal policy by contrasting various value functions, which enhances its adaptability. Empirical findings demonstrate that our approach outperforms other approaches in small-to-medium scenarios and is on par with the state-of-the-art in large-scale problems.",1
"Hierarchical reinforcement learning is a promising approach to tackle long-horizon decision-making problems with sparse rewards. Unfortunately, most methods still decouple the lower-level skill acquisition process and the training of a higher level that controls the skills in a new task. Leaving the skills fixed can lead to significant sub-optimality in the transfer setting. In this work, we propose a novel algorithm to discover a set of skills, and continuously adapt them along with the higher level even when training on a new task. Our main contributions are two-fold. First, we derive a new hierarchical policy gradient with an unbiased latent-dependent baseline, and we introduce Hierarchical Proximal Policy Optimization (HiPPO), an on-policy method to efficiently train all levels of the hierarchy jointly. Second, we propose a method for training time-abstractions that improves the robustness of the obtained skills to environment changes. Code and results are available at sites.google.com/view/hippo-rl",0
"Long-horizon decision-making problems with sparse rewards can be addressed effectively by hierarchical reinforcement learning. However, most existing methods separate the acquisition of lower-level skills from the training of a higher level that governs the skills in a new task. This can result in suboptimal outcomes in the transfer setting if the skills are kept constant. To overcome this limitation, this study proposes a new algorithm that identifies a set of skills and continuously adapts them along with the higher level during training on a new task. The study proposes two main contributions: first, a new hierarchical policy gradient with an unbiased latent-dependent baseline is derived, and an on-policy method called Hierarchical Proximal Policy Optimization (HiPPO) is introduced to efficiently train all levels of the hierarchy jointly. Second, a method for training time-abstractions is proposed to enhance the resilience of the acquired skills to changes in the environment. Access to the code and results is available at sites.google.com/view/hippo-rl.",1
"Explainable Artificial Intelligence (XAI), i.e., the development of more transparent and interpretable AI models, has gained increased traction over the last few years. This is due to the fact that, in conjunction with their growth into powerful and ubiquitous tools, AI models exhibit one detrimential characteristic: a performance-transparency trade-off. This describes the fact that the more complex a model's inner workings, the less clear it is how its predictions or decisions were achieved. But, especially considering Machine Learning (ML) methods like Reinforcement Learning (RL) where the system learns autonomously, the necessity to understand the underlying reasoning for their decisions becomes apparent. Since, to the best of our knowledge, there exists no single work offering an overview of Explainable Reinforcement Learning (XRL) methods, this survey attempts to address this gap. We give a short summary of the problem, a definition of important terms, and offer a classification and assessment of current XRL methods. We found that a) the majority of XRL methods function by mimicking and simplifying a complex model instead of designing an inherently simple one, and b) XRL (and XAI) methods often neglect to consider the human side of the equation, not taking into account research from related fields like psychology or philosophy. Thus, an interdisciplinary effort is needed to adapt the generated explanations to a (non-expert) human user in order to effectively progress in the field of XRL and XAI in general.",0
"Explainable Artificial Intelligence (XAI) has gained momentum in recent years as a result of the development of more transparent and interpretable AI models. As AI models become more powerful and ubiquitous, they exhibit a performance-transparency trade-off, where the complexity of the model's inner workings makes it less evident how predictions or decisions are reached. This is particularly relevant in Machine Learning (ML) methods such as Reinforcement Learning (RL), where the system learns autonomously and the need to comprehend the underlying reasoning behind decisions is crucial. Despite the absence of a comprehensive overview of Explainable Reinforcement Learning (XRL) methods, this survey aims to address this gap by providing a concise summary of the issue, important terms definition, and a classification and evaluation of current XRL approaches. Our findings revealed that most XRL methods mimic and simplify complex models rather than designing inherently simple ones, and that XRL and XAI methods often overlook the human side of the equation by ignoring research from related fields like psychology or philosophy. Therefore, an interdisciplinary effort is required to adapt the generated explanations to a non-expert human user for effective progress in the field of XRL and XAI overall.",1
"Neurons in the brain communicate with each other through discrete action spikes as opposed to continuous signal transmission in artificial neural networks. Therefore, the traditional techniques for optimization of parameters in neural networks which rely on the assumption of differentiability of activation functions are no longer applicable to modeling the learning processes in the brain. In this project, we propose biologically-plausible alternatives to backpropagation to facilitate the training of spiking neural networks. We primarily focus on investigating the candidacy of reinforcement learning (RL) rules in solving the spatial and temporal credit assignment problems to enable decision-making in complex tasks. In one approach, we consider each neuron in a multi-layer neural network as an independent RL agent forming a different representation of the feature space while the network as a whole forms the representation of the complex policy to solve the task at hand. In other approach, we apply the reparameterization trick to enable differentiation through stochastic transformations in spiking neural networks. We compare and contrast the two approaches by applying them to traditional RL domains such as gridworld, cartpole and mountain car. Further we also suggest variations and enhancements to enable future research in this area.",0
"Communication between neurons in the brain occurs via discrete action spikes, unlike the continuous signal transmission in artificial neural networks. Consequently, conventional parameter optimization techniques for neural networks that rely on the differentiability of activation functions are unsuitable for modeling learning processes in the brain. To address this issue, we propose biologically-plausible alternatives to backpropagation for training spiking neural networks. Our primary focus is on exploring reinforcement learning (RL) rules as a potential solution to the spatial and temporal credit assignment challenges for decision-making in complex tasks. In one approach, each neuron in a multi-layer neural network functions as an independent RL agent, creating a distinct representation of the feature space. The network as a whole, however, represents the complex policy required to solve the given task. In another approach, we use the reparameterization trick to facilitate differentiation through stochastic transformations in spiking neural networks. To compare and contrast the two methods, we apply them to classic RL domains such as gridworld, cartpole, and mountain car. Additionally, we suggest variations and enhancements to enable further research in this field.",1
"The control of nonlinear dynamical systems remains a major challenge for autonomous agents. Current trends in reinforcement learning (RL) focus on complex representations of dynamics and policies, which have yielded impressive results in solving a variety of hard control tasks. However, this new sophistication and extremely over-parameterized models have come with the cost of an overall reduction in our ability to interpret the resulting policies. In this paper, we take inspiration from the control community and apply the principles of hybrid switching systems in order to break down complex dynamics into simpler components. We exploit the rich representational power of probabilistic graphical models and derive an expectation-maximization (EM) algorithm for learning a sequence model to capture the temporal structure of the data and automatically decompose nonlinear dynamics into stochastic switching linear dynamical systems. Moreover, we show how this framework of switching models enables extracting hierarchies of Markovian and auto-regressive locally linear controllers from nonlinear experts in an imitation learning scenario.",0
"Autonomous agents face a significant challenge in controlling nonlinear dynamical systems. Reinforcement learning (RL) has advanced with a focus on intricate representations of dynamics and policies, resulting in remarkable achievements in solving difficult control tasks. However, this increased sophistication and excessive over-parameterized models have led to a decrease in our ability to interpret policies. The present study is inspired by the control community and employs the principles of hybrid switching systems to simplify complex dynamics into more straightforward components. The study utilizes the powerful representational capabilities of probabilistic graphical models and develops an expectation-maximization (EM) algorithm to learn a sequence model that captures the data's temporal structure and automatically decomposes nonlinear dynamics into stochastic switching linear dynamical systems. Additionally, the study demonstrates how the switching models framework facilitates the extraction of hierarchies of Markovian and auto-regressive locally linear controllers from nonlinear experts in an imitation learning scenario.",1
"Our brain receives a dynamically changing stream of sensorimotor data. Yet, we perceive a rather organized world, which we segment into and perceive as events. Computational theories of cognitive science on event-predictive cognition suggest that our brain forms generative, event-predictive models by segmenting sensorimotor data into suitable chunks of contextual experiences. Here, we introduce a hierarchical, surprise-gated recurrent neural network architecture, which models this process and develops compact compressions of distinct event-like contexts. The architecture contains a contextual LSTM layer, which develops generative compressions of ongoing and subsequent contexts. These compressions are passed into a GRU-like layer, which uses surprise signals to update its recurrent latent state. The latent state is passed forward into another LSTM layer, which processes actual dynamic sensory flow in the light of the provided latent, contextual compression signals. Our model shows to develop distinct event compressions and achieves the best performance on multiple event processing tasks. The architecture may be very useful for the further development of resource-efficient learning, hierarchical model-based reinforcement learning, as well as the development of artificial event-predictive cognition and intelligence.",0
"Although our brain is constantly receiving sensorimotor data that is constantly changing, we are able to perceive an organized world that we break down into events. Computational theories of cognitive science suggest that this is achieved by our brain creating generative models that predict events by segmenting the sensorimotor data into contextual experiences. To model this process, we have developed a hierarchical, surprise-gated recurrent neural network architecture that creates compact compressions of distinct event-like contexts. This architecture includes a contextual LSTM layer that generates compressions of ongoing and subsequent contexts, which are fed into a GRU-like layer that uses surprise signals to update its recurrent latent state. The resulting latent state is then passed into another LSTM layer that processes dynamic sensory flow in the context of the provided compression signals. Our model effectively develops event compressions and performs well on multiple event processing tasks, making it a valuable tool for resource-efficient learning, hierarchical model-based reinforcement learning, and the development of artificial event-predictive cognition and intelligence.",1
"In recent years deep neural networks have been successfully applied to the domains of reinforcement learning \cite{bengio2009learning,krizhevsky2012imagenet,hinton2006reducing}. Deep reinforcement learning \cite{mnih2015human} is reported to have the advantage of learning effective policies directly from high-dimensional sensory inputs over traditional agents. However, within the scope of the literature, there is no fundamental change or improvement on the existing training framework. Here we propose a novel training framework that is conceptually comprehensible and potentially easy to be generalized to all feasible algorithms for reinforcement learning. We employ Monte-carlo sampling to achieve raw data inputs, and train them in batch to achieve Markov decision process sequences and synchronously update the network parameters instead of experience replay. This training framework proves to optimize the unbiased approximation of loss function whose estimation exactly matches the real probability distribution data inputs follow, and thus have overwhelming advantages of sample efficiency and convergence rate over existing deep reinforcement learning after evaluating it on both discrete action spaces and continuous control problems. Besides, we propose several algorithms embedded with our new framework to deal with typical discrete and continuous scenarios. These algorithms prove to be far more efficient than their original versions under the framework of deep reinforcement learning, and provide examples for existing and future algorithms to generalize to our new framework.",0
"Recent years have seen successful applications of deep neural networks in reinforcement learning domains \cite{bengio2009learning,krizhevsky2012imagenet,hinton2006reducing}. Deep reinforcement learning \cite{mnih2015human} has an advantage over traditional agents as it can learn effective policies directly from high-dimensional sensory inputs. However, although no fundamental change or improvement has been made to the existing training framework in the literature, we propose a novel training framework that is conceptually comprehensible and easily generalizable to all feasible algorithms for reinforcement learning. We use Monte-Carlo sampling to achieve raw data inputs and train them in batch to achieve Markov decision process sequences, synchronously updating the network parameters instead of experience replay. This training framework optimizes the unbiased approximation of the loss function whose estimation matches the real probability distribution data inputs follow. Thus, it has overwhelming advantages of sample efficiency and convergence rate over existing deep reinforcement learning, as evaluated on both discrete action spaces and continuous control problems. Furthermore, we propose several algorithms embedded with our new framework to deal with typical discrete and continuous scenarios. These algorithms prove to be far more efficient than their original versions under the framework of deep reinforcement learning, and provide examples for existing and future algorithms to generalize to our new framework.",1
Action delays degrade the performance of reinforcement learning in many real-world systems. This paper proposes a formal definition of delay-aware Markov Decision Process and proves it can be transformed into standard MDP with augmented states using the Markov reward process. We develop a delay-aware model-based reinforcement learning framework that can incorporate the multi-step delay into the learned system models without learning effort. Experiments with the Gym and MuJoCo platforms show that the proposed delay-aware model-based algorithm is more efficient in training and transferable between systems with various durations of delay compared with off-policy model-free reinforcement learning methods. Codes available at: https://github.com/baimingc/dambrl.,0
"Many real-world systems suffer from decreased performance in reinforcement learning due to delays in action. This paper presents a formal definition of a Markov Decision Process that is aware of delays and demonstrates that it can be transformed into a standard MDP with augmented states using the Markov reward process. We have developed a framework for delay-aware model-based reinforcement learning that allows for the incorporation of multi-step delays into learned system models without additional learning effort. Experimental results using the Gym and MuJoCo platforms indicate that our proposed delay-aware model-based algorithm is more efficient in training and can be transferred between systems with varying delay durations, as compared to off-policy model-free reinforcement learning methods. Our code is available on https://github.com/baimingc/dambrl.",1
"Fine-grained sketch-based image retrieval (FG-SBIR) addresses the problem of retrieving a particular photo instance given a user's query sketch. Its widespread applicability is however hindered by the fact that drawing a sketch takes time, and most people struggle to draw a complete and faithful sketch. In this paper, we reformulate the conventional FG-SBIR framework to tackle these challenges, with the ultimate goal of retrieving the target photo with the least number of strokes possible. We further propose an on-the-fly design that starts retrieving as soon as the user starts drawing. To accomplish this, we devise a reinforcement learning-based cross-modal retrieval framework that directly optimizes rank of the ground-truth photo over a complete sketch drawing episode. Additionally, we introduce a novel reward scheme that circumvents the problems related to irrelevant sketch strokes, and thus provides us with a more consistent rank list during the retrieval. We achieve superior early-retrieval efficiency over state-of-the-art methods and alternative baselines on two publicly available fine-grained sketch retrieval datasets.",0
"The issue with Fine-grained sketch-based image retrieval (FG-SBIR) is that it relies on users drawing an accurate sketch to retrieve a specific photo. However, drawing a complete and precise sketch takes time and can be challenging for most people. Therefore, this paper proposes a revised FG-SBIR framework that aims to retrieve the target photo with the fewest strokes possible. To achieve this, an on-the-fly design is introduced, which starts retrieving as soon as the user begins drawing. This is accomplished through a reinforcement learning-based cross-modal retrieval framework that optimizes the rank of the ground-truth photo during a complete sketch drawing episode. Additionally, a new reward scheme is introduced to address issues with irrelevant sketch strokes and provide a more consistent rank list during retrieval. The proposed method outperforms state-of-the-art techniques and alternative baselines on two publicly available fine-grained sketch retrieval datasets, demonstrating superior early-retrieval efficiency.",1
"The aim of the project is to investigate and assess opportunities for applying reinforcement learning (RL) for power system control. As a proof of concept (PoC), voltage control of thermostatically controlled loads (TCLs) for power consumption regulation was developed using Modelica-based pipeline. The Q-learning RL algorithm has been validated for deterministic and stochastic initialization of TCLs. The latter modelling is closer to real grid behaviour, which challenges the control development, considering the stochastic nature of load switching. In addition, the paper shows the influence of Q-learning parameters, including discretization of state-action space, on the controller performance.",0
"The project's objective is to explore and evaluate possibilities for utilizing reinforcement learning (RL) in power system management. To demonstrate this concept, a voltage control system for thermostatically controlled loads (TCLs) was created using a Modelica-based pipeline. The Q-learning RL algorithm was tested and verified for both deterministic and stochastic initialization of TCLs. The latter model more closely simulates actual grid behavior, which poses a challenge for control development due to the unpredictable nature of load switching. The study also examines the impact of various Q-learning parameters, such as state-action space discretization, on controller effectiveness.",1
"Human players in professional team sports achieve high level coordination by dynamically choosing complementary skills and executing primitive actions to perform these skills. As a step toward creating intelligent agents with this capability for fully cooperative multi-agent settings, we propose a two-level hierarchical multi-agent reinforcement learning (MARL) algorithm with unsupervised skill discovery. Agents learn useful and distinct skills at the low level via independent Q-learning, while they learn to select complementary latent skill variables at the high level via centralized multi-agent training with an extrinsic team reward. The set of low-level skills emerges from an intrinsic reward that solely promotes the decodability of latent skill variables from the trajectory of a low-level skill, without the need for hand-crafted rewards for each skill. For scalable decentralized execution, each agent independently chooses latent skill variables and primitive actions based on local observations. Our overall method enables the use of general cooperative MARL algorithms for training high level policies and single-agent RL for training low level skills. Experiments on a stochastic high dimensional team game show the emergence of useful skills and cooperative team play. The interpretability of the learned skills show the promise of the proposed method for achieving human-AI cooperation in team sports games.",0
"In professional team sports, players achieve coordination by selecting complementary skills and executing primitive actions. To create intelligent agents capable of fully cooperative multi-agent settings, we suggest a two-level hierarchical multi-agent reinforcement learning (MARL) algorithm with unsupervised skill discovery. At the low level, agents use independent Q-learning to learn useful and distinct skills, while at the high level, they learn to select complementary latent skill variables through centralized multi-agent training. The set of low-level skills emerges from an intrinsic reward, which promotes the decodability of latent skill variables. For scalable decentralized execution, each agent independently chooses latent skill variables and primitive actions based on local observations. Our method allows for the use of general cooperative MARL algorithms for training high level policies and single-agent RL for training low level skills. Experiments show the emergence of useful skills and cooperative team play, demonstrating the potential for human-AI cooperation in team sports games.",1
"As an important type of reinforcement learning algorithms, actor-critic (AC) and natural actor-critic (NAC) algorithms are often executed in two ways for finding optimal policies. In the first nested-loop design, actor's one update of policy is followed by an entire loop of critic's updates of the value function, and the finite-sample analysis of such AC and NAC algorithms have been recently well established. The second two time-scale design, in which actor and critic update simultaneously but with different learning rates, has much fewer tuning parameters than the nested-loop design and is hence substantially easier to implement. Although two time-scale AC and NAC have been shown to converge in the literature, the finite-sample convergence rate has not been established. In this paper, we provide the first such non-asymptotic convergence rate for two time-scale AC and NAC under Markovian sampling and with actor having general policy class approximation. We show that two time-scale AC requires the overall sample complexity at the order of $\mathcal{O}(\epsilon^{-2.5}\log^3(\epsilon^{-1}))$ to attain an $\epsilon$-accurate stationary point, and two time-scale NAC requires the overall sample complexity at the order of $\mathcal{O}(\epsilon^{-4}\log^2(\epsilon^{-1}))$ to attain an $\epsilon$-accurate global optimal point. We develop novel techniques for bounding the bias error of the actor due to dynamically changing Markovian sampling and for analyzing the convergence rate of the linear critic with dynamically changing base functions and transition kernel.",0
"Actor-critic (AC) and natural actor-critic (NAC) algorithms are important types of reinforcement learning algorithms that are commonly used to find optimal policies. There are two ways in which these algorithms are executed. The first is a nested-loop design, where the actor's update of policy is followed by a full loop of the critic's value function updates. The finite-sample analysis of this approach has been well established. The second approach is a two time-scale design, where the actor and critic update simultaneously but with different learning rates. This approach has fewer tuning parameters and is easier to implement. However, the finite-sample convergence rate of two time-scale AC and NAC has not been established. In this paper, we provide the first non-asymptotic convergence rate for two time-scale AC and NAC under Markovian sampling and with actor utilizing general policy class approximation. We show that two time-scale AC requires an overall sample complexity of $\mathcal{O}(\epsilon^{-2.5}\log^3(\epsilon^{-1}))$ to attain an $\epsilon$-accurate stationary point, while two time-scale NAC requires an overall sample complexity of $\mathcal{O}(\epsilon^{-4}\log^2(\epsilon^{-1}))$ to attain an $\epsilon$-accurate global optimal point. We also develop novel techniques for bounding the bias error of the actor due to dynamically changing Markovian sampling and for analyzing the convergence rate of the linear critic with dynamically changing base functions and transition kernel.",1
"We study episodic reinforcement learning in Markov decision processes when the agent receives additional feedback per step in the form of several transition observations. Such additional observations are available in a range of tasks through extended sensors or prior knowledge about the environment (e.g., when certain actions yield similar outcome). We formalize this setting using a feedback graph over state-action pairs and show that model-based algorithms can leverage the additional feedback for more sample-efficient learning. We give a regret bound that, ignoring logarithmic factors and lower-order terms, depends only on the size of the maximum acyclic subgraph of the feedback graph, in contrast with a polynomial dependency on the number of states and actions in the absence of a feedback graph. Finally, we highlight challenges when leveraging a small dominating set of the feedback graph as compared to the bandit setting and propose a new algorithm that can use knowledge of such a dominating set for more sample-efficient learning of a near-optimal policy.",0
"Our focus is on studying episodic reinforcement learning in Markov decision processes, where the agent gains additional feedback through various transition observations at each step. These observations can be obtained from extended sensors or prior knowledge of the environment. To formalize this, we utilize a feedback graph over state-action pairs and demonstrate that model-based algorithms can utilize these observations to improve the efficiency of learning. Our regret bound is solely dependent on the size of the maximum acyclic subgraph of the feedback graph, disregarding logarithmic factors and lower-order terms, as opposed to the polynomial dependency on the number of states and actions when there is no feedback graph. We also examine the challenges of utilizing a small dominating set of the feedback graph, as compared to the bandit setting, and propose a novel algorithm to leverage this knowledge for more efficient learning of a near-optimal policy.",1
"In this paper we introduce plan2vec, an unsupervised representation learning approach that is inspired by reinforcement learning. Plan2vec constructs a weighted graph on an image dataset using near-neighbor distances, and then extrapolates this local metric to a global embedding by distilling path-integral over planned path. When applied to control, plan2vec offers a way to learn goal-conditioned value estimates that are accurate over long horizons that is both compute and sample efficient. We demonstrate the effectiveness of plan2vec on one simulated and two challenging real-world image datasets. Experimental results show that plan2vec successfully amortizes the planning cost, enabling reactive planning that is linear in memory and computation complexity rather than exhaustive over the entire state space.",0
"The aim of this paper is to present plan2vec, an unsupervised representation learning technique that draws inspiration from reinforcement learning. Plan2vec creates a graph on an image dataset using near-neighbor distances and then extends this local metric to a global embedding by distilling a path-integral over a planned path. When used for control, plan2vec provides an efficient method to learn goal-conditioned value estimates that are accurate over long horizons. In this regard, plan2vec has demonstrated its effectiveness on one simulated and two challenging real-world image datasets. The experimental results have shown that plan2vec can amortize the planning cost, making it possible to employ reactive planning that is linear in memory and computation complexity, instead of being exhaustive over the entire state space.",1
"In this paper, we work on intra-variable handwriting, where the writing samples of an individual can vary significantly. Such within-writer variation throws a challenge for automatic writer inspection, where the state-of-the-art methods do not perform well. To deal with intra-variability, we analyze the idiosyncrasy in individual handwriting. We identify/verify the writer from highly idiosyncratic text-patches. Such patches are detected using a deep recurrent reinforcement learning-based architecture. An idiosyncratic score is assigned to every patch, which is predicted by employing deep regression analysis. For writer identification, we propose a deep neural architecture, which makes the final decision by the idiosyncratic score-induced weighted average of patch-based decisions. For writer verification, we propose two algorithms for patch-fed deep feature aggregation, which assist in authentication using a triplet network. The experiments were performed on two databases, where we obtained encouraging results.",0
"The objective of this study is to address the challenge of intra-variable handwriting, where there are significant variations in writing samples of an individual. Current state-of-the-art methods for automatic writer inspection are not effective in dealing with this within-writer variability. To overcome this, we have analyzed the individual handwriting idiosyncrasies and identified the writer from text-patches with high idiosyncratic scores. The score is calculated using a deep recurrent reinforcement learning-based architecture and deep regression analysis. For writer identification, we have introduced a deep neural architecture that makes the final decision based on the weighted average of patch-based decisions induced by the idiosyncratic score. For writer verification, we have proposed two algorithms for patch-fed deep feature aggregation that assist in authentication using a triplet network. Our experiments on two databases have yielded promising results.",1
"We focus on the challenge of finding a diverse collection of quality solutions on complex continuous domains. While quality diver-sity (QD) algorithms like Novelty Search with Local Competition (NSLC) and MAP-Elites are designed to generate a diverse range of solutions, these algorithms require a large number of evaluations for exploration of continuous spaces. Meanwhile, variants of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) are among the best-performing derivative-free optimizers in single-objective continuous domains. This paper proposes a new QD algorithm called Covariance Matrix Adaptation MAP-Elites (CMA-ME). Our new algorithm combines the self-adaptation techniques of CMA-ES with archiving and mapping techniques for maintaining diversity in QD. Results from experiments based on standard continuous optimization benchmarks show that CMA-ME finds better-quality solutions than MAP-Elites; similarly, results on the strategic game Hearthstone show that CMA-ME finds both a higher overall quality and broader diversity of strategies than both CMA-ES and MAP-Elites. Overall, CMA-ME more than doubles the performance of MAP-Elites using standard QD performance metrics. These results suggest that QD algorithms augmented by operators from state-of-the-art optimization algorithms can yield high-performing methods for simultaneously exploring and optimizing continuous search spaces, with significant applications to design, testing, and reinforcement learning among other domains.",0
"The main focus of our research is to tackle the challenge of discovering a varied set of high-quality solutions on intricate and continuous domains. Despite the effectiveness of Quality Diversity (QD) algorithms like Novelty Search with Local Competition (NSLC) and MAP-Elites in producing diverse solutions, these methods require a considerable number of evaluations to explore continuous spaces. In contrast, variants of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) are known for their exceptional performance in optimizing single-objective continuous domains without using derivatives. In this paper, we introduce a new QD algorithm called Covariance Matrix Adaptation MAP-Elites (CMA-ME) that combines the self-adaptation techniques of CMA-ES with archiving and mapping techniques to maintain diversity in QD. Our experiments on standard continuous optimization benchmarks and the strategic game Hearthstone demonstrate that CMA-ME outperforms MAP-Elites in terms of quality and diversity of solutions. The results indicate that QD algorithms augmented with advanced optimization operators can offer effective methods for exploring and optimizing continuous search spaces, with significant implications for various domains such as design, testing, and reinforcement learning. CMA-ME achieves more than double the performance of MAP-Elites in standard QD performance metrics.",1
"An important goal in reinforcement learning is to create agents that can quickly adapt to new goals while avoiding situations that might cause damage to themselves or their environments. One way agents learn is through exploration mechanisms, which are needed to discover new policies. However, in deep reinforcement learning, exploration is normally done by injecting noise in the action space. While performing well in many domains, this setup has the inherent risk that the noisy actions performed by the agent lead to unsafe states in the environment. Here we introduce a novel approach called Meta-Learned Instinctual Networks (MLIN) that allows agents to safely learn during their lifetime while avoiding potentially hazardous states. At the core of the approach is a plastic network trained through reinforcement learning and an evolved ""instinctual"" network, which does not change during the agent's lifetime but can modulate the noisy output of the plastic network. We test our idea on a simple 2D navigation task with no-go zones, in which the agent has to learn to approach new targets during deployment. MLIN outperforms standard meta-trained networks and allows agents to learn to navigate to new targets without colliding with any of the no-go zones. These results suggest that meta-learning augmented with an instinctual network is a promising new approach for safe AI, which may enable progress in this area on a variety of different domains.",0
"An important objective in reinforcement learning is to develop agents that can swiftly adapt to new objectives while avoiding situations that could be harmful to themselves or their surroundings. Agents learn through exploration mechanisms, which are necessary to discover new policies. Nevertheless, in deep reinforcement learning, exploration is frequently accomplished by introducing noise into the action space. Although this method performs well in numerous domains, it carries the inherent risk of generating hazardous states in the environment due to the agent's noisy actions. Our proposed solution, Meta-Learned Instinctual Networks (MLIN), enables agents to learn securely while avoiding potentially hazardous states during their lifetime. The approach entails a plastic network trained through reinforcement learning and an evolved ""instinctual"" network that remains unchanged during the agent's lifetime but can adjust the noisy output of the plastic network. We conducted experiments on a basic 2D navigation task with no-go zones, where the agent must learn to approach new targets during deployment. MLIN outperforms conventional meta-trained networks and enables agents to navigate to new targets without colliding with any of the no-go zones. These findings suggest that combining meta-learning with an instinctual network is a promising strategy for developing safe AI, which may have a positive impact on a wide range of domains.",1
"This paper tests the hypothesis that modeling a scene in terms of entities and their local interactions, as opposed to modeling the scene globally, provides a significant benefit in generalizing to physical tasks in a combinatorial space the learner has not encountered before. We present object-centric perception, prediction, and planning (OP3), which to the best of our knowledge is the first fully probabilistic entity-centric dynamic latent variable framework for model-based reinforcement learning that acquires entity representations from raw visual observations without supervision and uses them to predict and plan. OP3 enforces entity-abstraction -- symmetric processing of each entity representation with the same locally-scoped function -- which enables it to scale to model different numbers and configurations of objects from those in training. Our approach to solving the key technical challenge of grounding these entity representations to actual objects in the environment is to frame this variable binding problem as an inference problem, and we develop an interactive inference algorithm that uses temporal continuity and interactive feedback to bind information about object properties to the entity variables. On block-stacking tasks, OP3 generalizes to novel block configurations and more objects than observed during training, outperforming an oracle model that assumes access to object supervision and achieving two to three times better accuracy than a state-of-the-art video prediction model that does not exhibit entity abstraction.",0
"The aim of this study is to examine whether modeling a scene by entities and their local interactions, rather than a global approach, is advantageous for generalizing to physical tasks in an unfamiliar environment. The paper introduces the object-centric perception, prediction, and planning (OP3) framework, which is the first probabilistic entity-centric dynamic latent variable framework for model-based reinforcement learning that acquires entity representations from raw visual observations without supervision. OP3 enforces entity-abstraction and can accommodate different numbers and configurations of objects from those in training. The challenge of grounding entity representations to actual objects is addressed by framing it as an inference problem, and an interactive inference algorithm is developed using temporal continuity and interactive feedback to bind information about object properties to entity variables. OP3 demonstrated superior generalization performance on block-stacking tasks, achieving two to three times better accuracy than a state-of-the-art video prediction model that lacks entity abstraction. It also outperformed an oracle model that assumes access to object supervision.",1
"Deep learning inference is increasingly run at the edge. As the programming and system stack support becomes mature, it enables acceleration opportunities within a mobile system, where the system performance envelope is scaled up with a plethora of programmable co-processors. Thus, intelligent services designed for mobile users can choose between running inference on the CPU or any of the co-processors on the mobile system, or exploiting connected systems, such as the cloud or a nearby, locally connected system. By doing so, the services can scale out the performance and increase the energy efficiency of edge mobile systems. This gives rise to a new challenge - deciding when inference should run where. Such execution scaling decision becomes more complicated with the stochastic nature of mobile-cloud execution, where signal strength variations of the wireless networks and resource interference can significantly affect real-time inference performance and system energy efficiency. To enable accurate, energy-efficient deep learning inference at the edge, this paper proposes AutoScale. AutoScale is an adaptive and light-weight execution scaling engine built upon the custom-designed reinforcement learning algorithm. It continuously learns and selects the most energy-efficient inference execution target by taking into account characteristics of neural networks and available systems in the collaborative cloud-edge execution environment while adapting to the stochastic runtime variance. Real system implementation and evaluation, considering realistic execution scenarios, demonstrate an average of 9.8 and 1.6 times energy efficiency improvement for DNN edge inference over the baseline mobile CPU and cloud offloading, while meeting the real-time performance and accuracy requirement.",0
"The use of deep learning inference at the edge is increasing, as programming and system support becomes more advanced. This allows for acceleration within mobile systems, which can now incorporate programmable co-processors, resulting in higher performance envelopes. Intelligent services designed for mobile users have the option to run inference on the CPU or any of the co-processors on the mobile system, or to use connected systems like the cloud or a nearby system. This flexibility enables services to scale performance and increase energy efficiency. The challenge now is to decide when and where to run inference, given the stochastic nature of mobile-cloud execution. To address this challenge, this paper proposes AutoScale, a reinforcement learning-based execution scaling engine that considers the characteristics of neural networks and available systems in the cloud-edge execution environment. Real system implementation and evaluation show significant energy efficiency improvements for DNN edge inference over the baseline mobile CPU and cloud offloading, while meeting performance and accuracy requirements.",1
"Model-free Reinforcement Learning (RL) works well when experience can be collected cheaply and model-based RL is effective when system dynamics can be modeled accurately. However, both assumptions can be violated in real world problems such as robotics, where querying the system can be expensive and real-world dynamics can be difficult to model. In contrast to RL, Model Predictive Control (MPC) algorithms use a simulator to optimize a simple policy class online, constructing a closed-loop controller that can effectively contend with real-world dynamics. MPC performance is usually limited by factors such as model bias and the limited horizon of optimization. In this work, we present a novel theoretical connection between information theoretic MPC and entropy regularized RL and develop a Q-learning algorithm that can leverage biased models. We validate the proposed algorithm on sim-to-sim control tasks to demonstrate the improvements over optimal control and reinforcement learning from scratch. Our approach paves the way for deploying reinforcement learning algorithms on real systems in a systematic manner.",0
"When it comes to real-world problems like robotics, the assumptions underlying Model-free Reinforcement Learning (RL) and model-based RL may not always hold true. Collecting experience can be costly and modeling the system dynamics can be challenging, making these approaches less effective. Model Predictive Control (MPC) algorithms, on the other hand, utilize a simulator to optimize a simple policy class online, which helps them construct a closed-loop controller that can better handle real-world dynamics. However, MPC performance may still be limited by factors such as model bias and optimization horizon. To address these issues, we introduce a novel theoretical connection between information theoretic MPC and entropy regularized RL and present a Q-learning algorithm that can make use of biased models. Our experiments on sim-to-sim control tasks show that our approach outperforms optimal control and reinforcement learning from scratch, which could pave the way for deploying reinforcement learning algorithms on real systems in a more systematic manner.",1
"Neural networks are effective function approximators, but hard to train in the reinforcement learning (RL) context mainly because samples are correlated. For years, scholars have got around this by employing experience replay or an asynchronous parallel-agent system. This paper proposes Discrete-to-Deep Supervised Policy Learning (D2D-SPL) for training neural networks in RL. D2D-SPL discretises the continuous state space into discrete states and uses actor-critic to learn a policy. It then selects from each discrete state an input value and the action with the highest numerical preference as an input/target pair. Finally it uses input/target pairs from all discrete states to train a classifier. D2D-SPL uses a single agent, needs no experience replay and learns much faster than state-of-the-art methods. We test our method with two RL environments, the Cartpole and an aircraft manoeuvring simulator.",0
"While neural networks have proven to be effective in approximating functions, they can be difficult to train for reinforcement learning due to the correlation of samples. To overcome this challenge, scholars have traditionally used experience replay or parallel-agent systems. This study introduces a new approach called Discrete-to-Deep Supervised Policy Learning (D2D-SPL) that uses actor-critic to learn a policy by discretizing the continuous state space into discrete states. From each discrete state, an input value and the action with the highest numerical preference are selected as input/target pairs, which are used to train a classifier. Unlike state-of-the-art methods, D2D-SPL requires only a single agent and no experience replay, and it learns much faster. The efficacy of this method is demonstrated through testing in two RL environments, the Cartpole and an aircraft manoeuvring simulator.",1
"We consider the problem of demand-side energy management, where each household is equipped with a smart meter that is able to schedule home appliances online. The goal is to minimise the overall cost under a real-time pricing scheme. While previous works have introduced centralised approaches, we formulate the smart grid environment as a Markov game, where each household is a decentralised agent, and the grid operator produces a price signal that adapts to the energy demand. The main challenge addressed in our approach is partial observability and perceived non-stationarity of the environment from the viewpoint of each agent. We propose a multi-agent extension of a deep actor-critic algorithm that shows success in learning in this environment. This algorithm learns a centralised critic that coordinates training of all agents. Our approach thus uses centralised learning but decentralised execution. Simulation results show that our online deep reinforcement learning method can reduce both the peak-to-average ratio of total energy consumed and the cost of electricity for all households based purely on instantaneous observations and a price signal.",0
"The focus of our work is on managing energy consumption from the demand side, where every household has a smart meter capable of scheduling their home appliances online. Our aim is to minimize costs through a real-time pricing scheme. Previous studies have used centralized methods, but we present a new approach that treats the smart grid environment as a Markov game, with each household acting as a decentralized agent. The grid operator provides a price signal that adjusts to energy demand. Our main challenge is dealing with partial observability and perceived non-stationarity of the environment from each agent's perspective. Our proposed solution is a multi-agent extension of a deep actor-critic algorithm that proves effective in learning in this environment. The algorithm learns a centralized critic that coordinates training for all agents, while decentralizing execution. Our simulation results demonstrate that our online deep reinforcement learning method can reduce both the peak-to-average ratio of energy consumption and the cost of electricity for all households based solely on instantaneous observations and a price signal.",1
"Combinatorial optimization is frequently used in computer vision. For instance, in applications like semantic segmentation, human pose estimation and action recognition, programs are formulated for solving inference in Conditional Random Fields (CRFs) to produce a structured output that is consistent with visual features of the image. However, solving inference in CRFs is in general intractable, and approximation methods are computationally demanding and limited to unary, pairwise and hand-crafted forms of higher order potentials. In this paper, we show that we can learn program heuristics, i.e., policies, for solving inference in higher order CRFs for the task of semantic segmentation, using reinforcement learning. Our method solves inference tasks efficiently without imposing any constraints on the form of the potentials. We show compelling results on the Pascal VOC and MOTS datasets.",0
"Computer vision often employs combinatorial optimization, such as in tasks like semantic segmentation, human pose estimation, and action recognition. Programs are designed to solve inference in Conditional Random Fields (CRFs), which generate structured outputs that conform to the image's visual features. However, CRF inference is generally unsolvable, and approximation methods are limited to unary, pairwise, and hand-crafted higher order potentials that are computationally demanding. This paper proposes a reinforcement learning approach to learn program heuristics or policies that can solve higher order CRFs for semantic segmentation tasks without imposing potential form constraints. Our method provides efficient inference solutions and produces impressive results on datasets like Pascal VOC and MOTS.",1
"Recurrent Neural Networks (RNNs) have been shown to be valuable for constructing Intrusion Detection Systems (IDSs) for network data. They allow determining if a flow is malicious or not already before it is over, making it possible to take action immediately. However, considering the large number of packets that has to be inspected, for example in cloud/fog and edge computing, the question of computational efficiency arises. We show that by using a novel Reinforcement Learning (RL)-based approach called SparseIDS, we can reduce the number of consumed packets by more than three fourths while keeping classification accuracy high. To minimize the computational expenses of the RL-based sampling we show that a shared neural network can be used for both the classifier and the RL logic. Thus, no additional resources are consumed by the sampling in deployment. Comparing to various other sampling techniques, SparseIDS consistently achieves higher classification accuracy by learning to sample only relevant packets. A major novelty of our RL-based approach is that it can not only skip up to a predefined maximum number of samples like other approaches proposed in the domain of Natural Language Processing but can even skip arbitrarily many packets in one step. This enables saving even more computational resources for long sequences. Inspecting SparseIDS's behavior of choosing packets shows that it adopts different sampling strategies for different attack types and network flows. Finally we build an automatic steering mechanism that can guide SparseIDS in deployment to achieve a desired level of sparsity.",0
"Recurrent Neural Networks (RNNs) are effective in creating Intrusion Detection Systems (IDSs) for network data by determining if a flow is malicious before it ends, allowing immediate action. However, the computational efficiency of these systems is a concern when inspecting a large number of packets, such as in cloud/fog and edge computing. Our novel Reinforcement Learning (RL)-based approach, SparseIDS, reduces packet consumption by over 75% while maintaining high classification accuracy. We use a shared neural network for both the classifier and RL logic to minimize computational expenses. SparseIDS achieves higher classification accuracy than other sampling techniques by learning to sample only relevant packets. The RL-based approach can skip an unlimited number of packets in one step, saving even more computational resources. SparseIDS adopts different sampling strategies for various attack types and network flows. Finally, we create an automatic steering mechanism to guide SparseIDS in achieving a desired level of sparsity during deployment.",1
"We present a generic and flexible Reinforcement Learning (RL) based meta-learning framework for the problem of few-shot learning. During training, it learns the best optimization algorithm to produce a learner (ranker/classifier, etc) by exploiting stable patterns in loss surfaces. Our method implicitly estimates the gradients of a scaled loss function while retaining the general properties intact for parameter updates. Besides providing improved performance on few-shot tasks, our framework could be easily extended to do network architecture search. We further propose a novel dual encoder, affinity-score based decoder topology that achieves additional improvements to performance. Experiments on an internal dataset, MQ2007, and AwA2 show our approach outperforms existing alternative approaches by 21%, 8%, and 4% respectively on accuracy and NDCG metrics. On Mini-ImageNet dataset our approach achieves comparable results with Prototypical Networks. Empirical evaluations demonstrate that our approach provides a unified and effective framework.",0
"Our research introduces a Reinforcement Learning (RL) meta-learning framework that is both flexible and applicable to few-shot learning. During the training process, the framework utilizes stable loss patterns to determine the optimal optimization algorithm for producing a learner, such as a ranker or classifier. This approach implicitly estimates the gradients of a scaled loss function while retaining essential properties for parameter updates. In addition to improving few-shot task performance, our framework can also be extended to network architecture search. We propose a new dual encoder, affinity-score based decoder topology that enhances performance further. Our experiments on internal datasets, MQ2007 and AwA2, show that our approach outperforms alternative methods by 21%, 8%, and 4% respectively on accuracy and NDCG metrics. On the Mini-ImageNet dataset, our approach achieves similar results to Prototypical Networks. Empirical evaluations demonstrate that our approach provides a unified and effective framework.",1
"Adversarial Imitation Learning (AIL) is a class of algorithms in Reinforcement learning (RL), which tries to imitate an expert without taking any reward from the environment and does not provide expert behavior directly to the policy training. Rather, an agent learns a policy distribution that minimizes the difference from expert behavior in an adversarial setting. Adversarial Inverse Reinforcement Learning (AIRL) leverages the idea of AIL, integrates a reward function approximation along with learning the policy, and shows the utility of IRL in the transfer learning setting. But the reward function approximator that enables transfer learning does not perform well in imitation tasks. We propose an Off-Policy Adversarial Inverse Reinforcement Learning (Off-policy-AIRL) algorithm which is sample efficient as well as gives good imitation performance compared to the state-of-the-art AIL algorithm in the continuous control tasks. For the same reward function approximator, we show the utility of learning our algorithm over AIL by using the learned reward function to retrain the policy over a task under significant variation where expert demonstrations are absent.",0
"AIL is a type of algorithm in RL that seeks to replicate the behavior of an expert, without relying on external rewards or providing direct guidance to policy training. Instead, the algorithm trains an agent to learn a policy distribution that closely matches expert behavior in an adversarial environment. AIRL builds on this approach by including a reward function approximation in the policy learning process, making it useful for transfer learning. However, the reward function approximator is not effective for imitation tasks. To address this issue, we propose an Off-Policy Adversarial Inverse Reinforcement Learning algorithm that is both efficient and produces better imitation performance than the current state-of-the-art AIL algorithm for continuous control tasks. We demonstrate the usefulness of our approach by training the algorithm to learn a reward function that can be used to retrain the policy for tasks with significant variation in the absence of expert demonstrations.",1
"Latest technological improvements increased the quality of transportation. New data-driven approaches bring out a new research direction for all control-based systems, e.g., in transportation, robotics, IoT and power systems. Combining data-driven applications with transportation systems plays a key role in recent transportation applications. In this paper, the latest deep reinforcement learning (RL) based traffic control applications are surveyed. Specifically, traffic signal control (TSC) applications based on (deep) RL, which have been studied extensively in the literature, are discussed in detail. Different problem formulations, RL parameters, and simulation environments for TSC are discussed comprehensively. In the literature, there are also several autonomous driving applications studied with deep RL models. Our survey extensively summarizes existing works in this field by categorizing them with respect to application types, control models and studied algorithms. In the end, we discuss the challenges and open questions regarding deep RL-based transportation applications.",0
"The quality of transportation has improved due to recent advancements in technology. These advancements have led to the emergence of new research directions for control-based systems, including transportation, robotics, IoT, and power systems, through data-driven approaches. The integration of data-driven applications into transportation systems has become increasingly important in recent times. This paper provides a survey of the latest traffic control applications based on deep reinforcement learning (RL). Specifically, the paper discusses traffic signal control applications that have been extensively studied in the literature, detailing various problem formulations, RL parameters, and simulation environments for TSC. Additionally, the literature also includes studies on autonomous driving applications with deep RL models. This survey categorizes existing works by application types, control models, and studied algorithms. Finally, the paper concludes by discussing the open questions and challenges in deep RL-based transportation applications.",1
"Deep reinforcement learning (DRL) on Markov decision processes (MDPs) with continuous action spaces is often approached by directly training parametric policies along the direction of estimated policy gradients (PGs). Previous research revealed that the performance of these PG algorithms depends heavily on the bias-variance tradeoffs involved in estimating and using PGs. A notable approach towards balancing this tradeoff is to merge both on-policy and off-policy gradient estimations. However existing PG merging methods can be sample inefficient and are not suitable to train deterministic policies directly. To address these issues, this paper introduces elite PGs and strengthens their variance reduction effect by adopting elitism and policy consolidation techniques to regularize policy training based on policy behavioral knowledge extracted from elite trajectories. Meanwhile, we propose a two-step method to merge elite PGs and conventional PGs as a new extension of the conventional interpolation merging method. At both the theoretical and experimental levels, we show that both two-step merging and interpolation merging can induce varied bias-variance tradeoffs during policy training. They enable us to effectively use elite PGs and mitigate their performance impact on trained policies. Our experiments also show that two-step merging can outperform interpolation merging and several state-of-the-art algorithms on six benchmark control tasks.",0
"Markov decision processes (MDPs) with continuous action spaces are often tackled using deep reinforcement learning (DRL), which involves training parametric policies according to estimated policy gradients (PGs). However, previous research has shown that using PGs effectively requires balancing bias and variance, and that existing methods for merging on-policy and off-policy gradient estimations can be inefficient and unsuitable for training deterministic policies. This paper introduces elite PGs, which are regularized based on policy behavioral knowledge extracted from elite trajectories, and proposes a two-step method for merging elite PGs and conventional PGs. Both merging methods enable effective use of elite PGs and mitigate their performance impact on trained policies, with two-step merging outperforming interpolation merging and several state-of-the-art algorithms on six benchmark control tasks. Theoretical and experimental analyses demonstrate that both methods induce varied bias-variance tradeoffs during policy training.",1
"We propose expected policy gradients (EPG), which unify stochastic policy gradients (SPG) and deterministic policy gradients (DPG) for reinforcement learning. Inspired by expected sarsa, EPG integrates (or sums) across actions when estimating the gradient, instead of relying only on the action in the sampled trajectory. For continuous action spaces, we first derive a practical result for Gaussian policies and quadratic critics and then extend it to a universal analytical method, covering a broad class of actors and critics, including Gaussian, exponential families, and policies with bounded support. For Gaussian policies, we introduce an exploration method that uses covariance proportional to the matrix exponential of the scaled Hessian of the critic with respect to the actions. For discrete action spaces, we derive a variant of EPG based on softmax policies. We also establish a new general policy gradient theorem, of which the stochastic and deterministic policy gradient theorems are special cases. Furthermore, we prove that EPG reduces the variance of the gradient estimates without requiring deterministic policies and with little computational overhead. Finally, we provide an extensive experimental evaluation of EPG and show that it outperforms existing approaches on multiple challenging control domains.",0
"The proposed expected policy gradients (EPG) technique combines stochastic policy gradients (SPG) and deterministic policy gradients (DPG) for reinforcement learning. EPG is inspired by expected sarsa and estimates the gradient by summing across actions instead of relying solely on the action in the sampled trajectory. For continuous action spaces, EPG provides a practical result for Gaussian policies and quadratic critics, which is then extended to a universal analytical method covering a wide range of actors and critics, including Gaussian, exponential families, and bounded support policies. Exploration for Gaussian policies is achieved by using a covariance proportional to the matrix exponential of the scaled Hessian of the critic with respect to the actions. For discrete action spaces, a variant of EPG is derived based on softmax policies. A new general policy gradient theorem is established, with EPG reducing variance in gradient estimates without requiring deterministic policies and with minimal computational overhead. Extensive experiments show that EPG outperforms existing approaches in multiple challenging control domains.",1
"One major obstacle that precludes the success of reinforcement learning in real-world applications is the lack of robustness, either to model uncertainties or external disturbances, of the trained policies. Robustness is critical when the policies are trained in simulations instead of real world environment. In this work, we propose a risk-aware algorithm to learn robust policies in order to bridge the gap between simulation training and real-world implementation. Our algorithm is based on recently discovered distributional RL framework. We incorporate CVaR risk measure in sample based distributional policy gradients (SDPG) for learning risk-averse policies to achieve robustness against a range of system disturbances. We validate the robustness of risk-aware SDPG on multiple environments.",0
"The lack of robustness in trained policies is a major challenge for reinforcement learning in real-world scenarios. This is particularly crucial when training policies in simulated environments. To address this issue, we introduce a risk-aware algorithm that leverages the distributional RL framework and the CVaR risk measure. Our approach incorporates sample-based distributional policy gradients (SDPG) to learn risk-averse policies that can withstand a variety of system disturbances. We assess the effectiveness of our approach on multiple environments and confirm the robustness of risk-aware SDPG. By bridging the gap between simulation training and real-world implementation, our approach offers a promising solution for achieving success in reinforcement learning for practical applications.",1
"The linear quadratic regulator (LQR) problem has reemerged as an important theoretical benchmark for reinforcement learning-based control of complex dynamical systems with continuous state and action spaces. In contrast with nearly all recent work in this area, we consider multiplicative noise models, which are increasingly relevant because they explicitly incorporate inherent uncertainty and variation in the system dynamics and thereby improve robustness properties of the controller. Robustness is a critical and poorly understood issue in reinforcement learning; existing methods which do not account for uncertainty can converge to fragile policies or fail to converge at all. Additionally, intentional injection of multiplicative noise into learning algorithms can enhance robustness of policies, as observed in ad hoc work on domain randomization. Although policy gradient algorithms require optimization of a non-convex cost function, we show that the multiplicative noise LQR cost has a special property called gradient domination, which is exploited to prove global convergence of policy gradient algorithms to the globally optimum control policy with polynomial dependence on problem parameters. Results are provided both in the model-known and model-unknown settings where samples of system trajectories are used to estimate policy gradients.",0
"The use of the linear quadratic regulator (LQR) problem as a benchmark for reinforcement learning-based control of complex dynamical systems with continuous state and action spaces has become increasingly important. However, most recent work in this area has not considered multiplicative noise models, which are essential in incorporating inherent uncertainty and variation in the system dynamics and improving the robustness of the controller. Robustness is a critical issue in reinforcement learning, and existing methods that do not account for uncertainty can lead to fragile policies or even failure to converge. Injecting multiplicative noise into learning algorithms can enhance the robustness of policies, as observed in ad hoc work on domain randomization. Despite the non-convex nature of the cost function, we demonstrate that the multiplicative noise LQR cost has a special property known as gradient domination, which is utilized to prove the global convergence of policy gradient algorithms to the globally optimum control policy with polynomial dependence on problem parameters. We provide results in both the model-known and model-unknown settings, where system trajectory samples are used to estimate policy gradients.",1
"Blockchain-enabled Federated Learning (BFL) enables mobile devices to collaboratively train neural network models required by a Machine Learning Model Owner (MLMO) while keeping data on the mobile devices. Then, the model updates are stored in the blockchain in a decentralized and reliable manner. However, the issue of BFL is that the mobile devices have energy and CPU constraints that may reduce the system lifetime and training efficiency. The other issue is that the training latency may increase due to the blockchain mining process. To address these issues, the MLMO needs to (i) decide how much data and energy that the mobile devices use for the training and (ii) determine the block generation rate to minimize the system latency, energy consumption, and incentive cost while achieving the target accuracy for the model. Under the uncertainty of the BFL environment, it is challenging for the MLMO to determine the optimal decisions. We propose to use the Deep Reinforcement Learning (DRL) to derive the optimal decisions for the MLMO.",0
"By utilizing Blockchain-enabled Federated Learning (BFL), mobile devices are able to work together in training neural network models necessary for a Machine Learning Model Owner (MLMO), all while keeping data stored locally on the devices. These model updates are then securely stored in the blockchain. However, BFL presents two challenges: the limited energy and CPU capacities of mobile devices, which may decrease training efficiency and system lifespan, and the potential for training latency to increase due to the blockchain mining process. To overcome these challenges, the MLMO must determine the amount of data and energy used during training and block generation rates in order to minimize system latency, energy consumption, and incentive costs while achieving target model accuracy. Due to the uncertain nature of the BFL environment, determining optimal decisions can be difficult for the MLMO. Our solution is to use Deep Reinforcement Learning (DRL) to derive optimal decisions for the MLMO.",1
"Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach. Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners. Our experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratch.",0
"Meta-learning algorithms utilize past experiences to quickly solve new tasks, particularly in reinforcement learning where these algorithms acquire reinforcement learning procedures to efficiently solve new problems by utilizing prior task experience. The success of meta-learning algorithms largely depends on the tasks available for meta-training, similar to how supervised learning performs best with test points drawn from the same distribution as the training points. Thus, meta-reinforcement learning shifts the design burden from algorithm design to task design, and further automation of the task design process could lead to a truly automated meta-learning algorithm. In this study, we propose a family of unsupervised meta-learning algorithms for reinforcement learning, which formulate the unsupervised meta-reinforcement learning problem and describe how task proposals based on mutual information can train optimal meta-learners. Our experimental results demonstrate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design, surpassing the performance of learning from scratch.",1
"Solving the Goal-Conditioned Reward Sparse (GCRS) task is a challenging reinforcement learning problem due to the sparsity of reward signals. In this work, we propose a new formulation of GCRS tasks from the perspective of the drifted random walk on the state space, and design a novel method called Evolutionary Stochastic Policy Distillation (ESPD) to solve them based on the insight of reducing the First Hitting Time of the stochastic process. As a self-imitate approach, ESPD enables a target policy to learn from a series of its stochastic variants through the technique of policy distillation (PD). The learning mechanism of ESPD can be considered as an Evolution Strategy (ES) that applies perturbations upon policy directly on the action space, with a SELECT function to check the superiority of stochastic variants and then use PD to update the policy. The experiments based on the MuJoCo robotics control suite show the high learning efficiency of the proposed method.",0
"The GCRS task is a difficult reinforcement learning challenge due to the limited reward signals. This study introduces a new approach to the GCRS task by examining the drifted random walk on the state space. The researchers develop a novel technique known as Evolutionary Stochastic Policy Distillation (ESPD) that addresses this problem by reducing the First Hitting Time of the stochastic process. ESPD employs a self-imitating approach to teach a target policy by learning from a series of its stochastic variations using Policy Distillation (PD). The ESPD learning mechanism uses Evolution Strategy (ES) to apply perturbations on the action space, and a SELECT function to determine the superiority of stochastic variants, followed by PD to update the policy. The experiments on the MuJoCo robotics control suite demonstrate the high learning efficiency of the proposed method.",1
"Learning a good representation is an essential component for deep reinforcement learning (RL). Representation learning is especially important in multitask and partially observable settings where building a representation of the unknown environment is crucial to solve the tasks. Here we introduce Prediction of Bootstrap Latents (PBL), a simple and flexible self-supervised representation learning algorithm for multitask deep RL. PBL builds on multistep predictive representations of future observations, and focuses on capturing structured information about environment dynamics. Specifically, PBL trains its representation by predicting latent embeddings of future observations. These latent embeddings are themselves trained to be predictive of the aforementioned representations. These predictions form a bootstrapping effect, allowing the agent to learn more about the key aspects of the environment dynamics. In addition, by defining prediction tasks completely in latent space, PBL provides the flexibility of using multimodal observations involving pixel images, language instructions, rewards and more. We show in our experiments that PBL delivers across-the-board improved performance over state of the art deep RL agents in the DMLab-30 and Atari-57 multitask setting.",0
"Deep reinforcement learning (RL) relies heavily on acquiring a good representation, particularly in scenarios involving multiple tasks and partially observable settings where understanding the unknown environment is critical for successful task completion. This paper introduces Prediction of Bootstrap Latents (PBL), a self-supervised representation learning algorithm that is adaptable and straightforward for multitask deep RL. PBL utilizes multistep predictive representations of future observations to capture structured information about environment dynamics. The algorithm trains its representation by predicting latent embeddings of future observations, which in turn are trained to predict the aforementioned representations. This bootstrapping effect enables the agent to gain a better understanding of the key aspects of the environment dynamics. Additionally, PBL offers the flexibility of using multimodal observations such as pixel images, language instructions, and rewards by defining prediction tasks purely in latent space. Experimental results demonstrate that PBL outperforms state-of-the-art deep RL agents in the DMLab-30 and Atari-57 multitask settings.",1
"In the last decade convolutional neural networks have become gargantuan. Pre-trained models, when used as initializers are able to fine-tune ever larger networks on small datasets. Consequently, not all the convolutional features that these fine-tuned models detect are requisite for the end-task. Several works of channel pruning have been proposed to prune away compute and memory from models that were trained already. Typically, these involve policies that decide which and how many channels to remove from each layer leading to channel-wise and/or layer-wise pruning profiles, respectively. In this paper, we conduct several baseline experiments and establish that profiles from random channel-wise pruning policies are as good as metric-based ones. We also establish that there may exist profiles from some layer-wise pruning policies that are measurably better than common baselines. We then demonstrate that the top layer-wise pruning profiles found using an exhaustive random search from one datatset are also among the top profiles for other datasets. This implies that we could identify out-of-the-box layer-wise pruning profiles using benchmark datasets and use these directly for new datasets. Furthermore, we develop a Reinforcement Learning (RL) policy-based search algorithm with a direct objective of finding transferable layer-wise pruning profiles using many models for the same architecture. We use a novel reward formulation that drives this RL search towards an expected compression while maximizing accuracy. Our results show that our transferred RL-based profiles are as good or better than best profiles found on the original dataset via exhaustive search. We then demonstrate that if we found the profiles using a mid-sized dataset such as Cifar10/100, we are able to transfer them to even a large dataset such as Imagenet.",0
"Convolutional neural networks have grown increasingly large over the past decade. Pre-trained models are now used as initializers to fine-tune even larger networks on small datasets, resulting in the detection of non-essential convolutional features. To address this issue, channel pruning techniques have been introduced to reduce memory and compute usage. These techniques involve the removal of channels from each layer, either through channel-wise or layer-wise pruning policies. Our paper presents baseline experiments that show that random channel-wise pruning policies perform as well as metric-based ones, and that certain layer-wise pruning policies can yield better results than common baselines. We also demonstrate that top layer-wise pruning profiles found through an exhaustive random search on one dataset are transferable to other datasets. To facilitate this transfer, we develop a Reinforcement Learning (RL) policy-based search algorithm that uses a novel reward formulation to maximize accuracy while driving towards expected compression. Our transferred RL-based profiles perform as well as, or better than, the best profiles found through exhaustive search on the original dataset. We also show that profiles found using a mid-sized dataset such as Cifar10/100 can be transferred to larger datasets like Imagenet.",1
"Robot control problems are often structured with a policy function that maps state values into control values, but in many dynamic problems the observed state can have a difficult to characterize relationship with useful policy actions. In this paper we present a new method for learning state embeddings from plans or other forms of demonstrations such that the embedding space has a specified geometric relationship with the demonstrations. We present a novel variational framework for learning these embeddings that attempts to optimize trajectory linearity in the learned embedding space. We show how these embedding spaces can then be used as an augmentation to the robot state in reinforcement learning problems. We use kinodynamic planning to generate training trajectories for some example environments, and then train embedding spaces for these environments. We show empirically that observing a system in the learned embedding space improves the performance of policy gradient reinforcement learning algorithms, particularly by reducing the variance between training runs. Our technique is limited to environments where demonstration data is available, but places no limits on how that data is collected. Our embedding technique provides a way to transfer domain knowledge from existing technologies such as planning and control algorithms, into more flexible policy learning algorithms, by creating an abstract representation of the robot state with meaningful geometry.",0
"The common approach to solving robot control problems involves a policy function that maps state values to control values. However, in dynamic problems, the relationship between observed state and useful policy actions can be challenging to define. This research introduces a new method for learning state embeddings from plans or other demonstrations, creating an embedding space that has a specified geometric relationship with the demonstrations. A novel variational framework optimizes trajectory linearity in the learned embedding space. These embedding spaces can then be used to augment the robot state in reinforcement learning problems. The training trajectories are generated using kinodynamic planning, and embedding spaces are trained for these environments. The technique improves the performance of policy gradient reinforcement learning algorithms and reduces the variance between training runs. While demonstration data is required, the technique has no limits on how that data is collected. The embedding technique transfers domain knowledge from existing technologies such as planning and control algorithms to more flexible policy learning algorithms, creating an abstract representation of the robot state with meaningful geometry.",1
"While most approaches to the problem of Inverse Reinforcement Learning (IRL) focus on estimating a reward function that best explains an expert agent's policy or demonstrated behavior on a control task, it is often the case that such behavior is more succinctly represented by a simple reward combined with a set of hard constraints. In this setting, the agent is attempting to maximize cumulative rewards subject to these given constraints on their behavior. We reformulate the problem of IRL on Markov Decision Processes (MDPs) such that, given a nominal model of the environment and a nominal reward function, we seek to estimate state, action, and feature constraints in the environment that motivate an agent's behavior. Our approach is based on the Maximum Entropy IRL framework, which allows us to reason about the likelihood of an expert agent's demonstrations given our knowledge of an MDP. Using our method, we can infer which constraints can be added to the MDP to most increase the likelihood of observing these demonstrations. We present an algorithm which iteratively infers the Maximum Likelihood Constraint to best explain observed behavior, and we evaluate its efficacy using both simulated behavior and recorded data of humans navigating around an obstacle.",0
"The majority of Inverse Reinforcement Learning (IRL) approaches concentrate on determining a reward function that best describes the behavior of an expert agent on a control task. However, it is often simpler to represent this behavior through a basic reward and a set of strict constraints. In this scenario, the agent aims to maximize cumulative rewards while adhering to these given behavior constraints. Our approach to the problem of IRL on Markov Decision Processes (MDPs) involves estimating state, action, and feature constraints in the environment that motivate an agent's behavior, given a nominal model of the environment and reward function. We utilize the Maximum Entropy IRL framework to determine the likelihood of expert agent demonstrations with our knowledge of an MDP. Our technique allows us to determine which constraints can be added to the MDP to increase the probability of observing these demonstrations. We present an algorithm that iteratively identifies the Maximum Likelihood Constraint to best explain observed behavior and evaluate its effectiveness by using both simulated behavior and recorded data of humans navigating around an obstacle.",1
"Embodied artificial intelligence (AI) tasks shift from tasks focusing on internet images to active settings involving embodied agents that perceive and act within 3D environments. In this paper, we investigate the target-driven visual navigation using deep reinforcement learning (DRL) in 3D indoor scenes, whose navigation task aims to train an agent that can intelligently make a series of decisions to arrive at a pre-specified target location from any possible starting positions only based on egocentric views. However, most navigation methods currently struggle against several challenging problems, such as data efficiency, automatic obstacle avoidance, and generalization. Generalization problem means that agent does not have the ability to transfer navigation skills learned from previous experience to unseen targets and scenes. To address these issues, we incorporate two designs into classic DRL framework: attention on 3D knowledge graph (KG) and target skill extension (TSE) module. On the one hand, our proposed method combines visual features and 3D spatial representations to learn navigation policy. On the other hand, TSE module is used to generate sub-targets which allow agent to learn from failures. Specifically, our 3D spatial relationships are encoded through recently popular graph convolutional network (GCN). Considering the real world settings, our work also considers open action and adds actionable targets into conventional navigation situations. Those more difficult settings are applied to test whether DRL agent really understand its task, navigating environment, and can carry out reasoning. Our experiments, performed in the AI2-THOR, show that our model outperforms the baselines in both SR and SPL metrics, and improves generalization ability across targets and scenes.",0
"The focus of artificial intelligence (AI) tasks has shifted from internet images to active embodied agents that perceive and act within 3D environments. This paper investigates target-driven visual navigation using deep reinforcement learning (DRL) in 3D indoor scenes. The navigation task aims to train an agent that can make intelligent decisions to reach a pre-specified target location from any starting position based on egocentric views. However, current navigation methods struggle with data efficiency, automatic obstacle avoidance, and generalization. The generalization problem means that the agent cannot transfer navigation skills to unseen targets and scenes. To address these issues, the paper proposes two designs: attention on 3D knowledge graph (KG) and target skill extension (TSE) module. The proposed method combines visual features and 3D spatial representation to learn navigation policy. Additionally, TSE module generates sub-targets to allow the agent to learn from failures. The 3D spatial relationships are encoded using graph convolutional network (GCN). The paper also considers open action and adds actionable targets into conventional navigation situations to test the agent's ability to understand the task, navigate the environment, and carry out reasoning. Experiments conducted in AI2-THOR show that the proposed model outperforms the baselines in both SR and SPL metrics and improves generalization across targets and scenes.",1
"Human-computer interactive systems that rely on machine learning are becoming paramount to the lives of millions of people who use digital assistants on a daily basis. Yet, further advances are limited by the availability of data and the cost of acquiring new samples. One way to address this problem is by improving the sample efficiency of current approaches. As a solution path, we present a model-based reinforcement learning algorithm for an interactive dialogue task. We build on commonly used actor-critic methods, adding an environment model and planner that augments a learning agent to learn the model of the environment dynamics. Our results show that, on a simulation that mimics the interactive task, our algorithm requires 70 times fewer samples, compared to the baseline of commonly used model-free algorithm, and demonstrates 2~times better performance asymptotically. Moreover, we introduce a novel contribution of computing a soft planner policy and further updating a model-free policy yielding a less computationally expensive model-free agent as good as the model-based one. This model-based architecture serves as a foundation that can be extended to other human-computer interactive tasks allowing further advances in this direction.",0
"The use of machine learning in human-computer interactive systems has become increasingly important for individuals who rely on digital assistants in their daily lives. However, progress in this field is hindered by limited data availability and the high cost of collecting new samples. To address this issue, we propose a model-based reinforcement learning algorithm for interactive dialogue tasks that improves sample efficiency. Our approach incorporates an environment model and planner into the commonly used actor-critic methods, enabling the learning agent to acquire a model of the environment dynamics. Our simulation results reveal that our algorithm requires significantly fewer samples than the traditional model-free algorithm and demonstrates better asymptotic performance. Additionally, we introduce a novel feature that involves computing a soft planner policy and updating a model-free policy, resulting in a less computationally expensive model-free agent that performs as well as the model-based one. This model-based framework can be extended to other human-computer interactive tasks, paving the way for further advancements in this area.",1
"The purpose of this paper is to outline a generalised model for representing hybrids of relational-categorical, symbolic, perceptual-sensory and perceptual-latent data, so as to embody, in the same architectural data layer, representations for the input, output and latent tensors. This variety of representation is currently used by various machine-learning models in computer vision, NLP/NLU, reinforcement learning which allows for direct application of cross-domain queries and functions. This is achieved by endowing a directed Tensor-Typed Multi-Graph with at least some edge attributes which represent the embeddings from various latent spaces, so as to define, construct and compute new similarity and distance relationships between and across tensorial forms, including visual, linguistic, auditory latent representations, thus stitching the logical-categorical view of the observed universe to the Bayesian/statistical view.",0
"The aim of this paper is to present a generalized framework that can represent hybrid data types such as relational-categorical, symbolic, perceptual-sensory and perceptual-latent data. The framework allows for the representation of input, output and latent tensors in a single architectural data layer. This diverse representation is currently utilized by machine-learning models in fields like computer vision, NLP/NLU, and reinforcement learning, facilitating cross-domain queries and functions. To achieve this, a directed Tensor-Typed Multi-Graph is endowed with edge attributes that represent embeddings from various latent spaces. This enables the creation of new similarity and distance relationships between and across tensorial forms, including visual, linguistic, and auditory latent representations. This approach blends the logical-categorical view of the observed universe with the Bayesian/statistical view.",1
"Standard gradient descent methods are susceptible to a range of issues that can impede training, such as high correlations and different scaling in parameter space.These difficulties can be addressed by second-order approaches that apply a pre-conditioning matrix to the gradient to improve convergence. Unfortunately, such algorithms typically struggle to scale to high-dimensional problems, in part because the calculation of specific preconditioners such as the inverse Hessian or Fisher information matrix is highly expensive. We introduce first-order preconditioning (FOP), a fast, scalable approach that generalizes previous work on hypergradient descent (Almeida et al., 1998; Maclaurin et al., 2015; Baydin et al.,2017) to learn a preconditioning matrix that only makes use of first-order information. Experiments show that FOP is able to improve the performance of standard deep learning optimizers on visual classification and reinforcement learning tasks with minimal computational overhead. We also investigate the properties of the learned preconditioning matrices and perform a preliminary theoretical analysis of the algorithm.",0
"The use of standard gradient descent methods in training may encounter issues such as parameter space scaling and high correlations. To overcome these difficulties, second-order approaches that involve a pre-conditioning matrix are utilized to enhance convergence. However, this approach has limitations when applied to high-dimensional problems due to the costly computation of specific preconditioners like the inverse Hessian or Fisher information matrix. In this study, we propose a fast and scalable method called first-order preconditioning (FOP) that is based on previous research on hypergradient descent. FOP learns a preconditioning matrix using only first-order information. Our experiments demonstrate that FOP can enhance the performance of standard deep learning optimizers in visual classification and reinforcement learning tasks with minimal computational overhead. We also examine the characteristics of the learned preconditioning matrices and conduct a preliminary theoretical analysis of the algorithm.",1
"The success of reinforcement learning for real world robotics has been, in many cases limited to instrumented laboratory scenarios, often requiring arduous human effort and oversight to enable continuous learning. In this work, we discuss the elements that are needed for a robotic learning system that can continually and autonomously improve with data collected in the real world. We propose a particular instantiation of such a system, using dexterous manipulation as our case study. Subsequently, we investigate a number of challenges that come up when learning without instrumentation. In such settings, learning must be feasible without manually designed resets, using only on-board perception, and without hand-engineered reward functions. We propose simple and scalable solutions to these challenges, and then demonstrate the efficacy of our proposed system on a set of dexterous robotic manipulation tasks, providing an in-depth analysis of the challenges associated with this learning paradigm. We demonstrate that our complete system can learn without any human intervention, acquiring a variety of vision-based skills with a real-world three-fingered hand. Results and videos can be found at https://sites.google.com/view/realworld-rl/",0
"In many cases, reinforcement learning has only been successful for real world robotics in controlled laboratory environments. These scenarios often require significant human effort and supervision for continuous learning. This article explores the necessary components for a robotic learning system that can improve autonomously with real world data. The study focuses on dexterous manipulation as a case study and identifies challenges that arise when learning without instrumentation. The proposed system provides simple and scalable solutions to these challenges and is demonstrated on a set of dexterous manipulation tasks. Results show that the complete system can learn without human intervention and acquire a range of vision-based skills with a real-world three-fingered hand. For more information, including results and videos, please visit https://sites.google.com/view/realworld-rl/.",1
"We study reinforcement learning in settings where sampling an action from the policy must be done concurrently with the time evolution of the controlled system, such as when a robot must decide on the next action while still performing the previous action. Much like a person or an animal, the robot must think and move at the same time, deciding on its next action before the previous one has completed. In order to develop an algorithmic framework for such concurrent control problems, we start with a continuous-time formulation of the Bellman equations, and then discretize them in a way that is aware of system delays. We instantiate this new class of approximate dynamic programming methods via a simple architectural extension to existing value-based deep reinforcement learning algorithms. We evaluate our methods on simulated benchmark tasks and a large-scale robotic grasping task where the robot must ""think while moving"".",0
"Our focus is on studying reinforcement learning in situations where there is a need to concurrently sample an action from the policy and experience the time evolution of the controlled system. This is particularly relevant in scenarios where a robot must determine its next action while still performing the previous one, similar to how humans or animals think and move simultaneously. To address this, we begin by formulating the Bellman equations as a continuous-time model and then discretize it while accounting for system delays. Our approach involves extending current value-based deep reinforcement learning algorithms, resulting in a new class of approximate dynamic programming methods. We evaluate our methodology by applying it to various simulated benchmark tasks and a large-scale robotic grasping task that requires the robot to ""think while moving"".",1
"In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant. The information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a ""privileged"" input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information.",0
"The extraction of relevant information from intricate input data is often necessary in various applications, requiring the determination of which input features are pertinent. The information bottleneck approach addresses this by optimizing compression, which involves discarding irrelevant input information, and predicting the target, while maintaining an optimal balance between the two. However, in certain scenarios, such as in reinforcement learning problems, it may be preferable to compress only part of the input, specifically the ""privileged"" input, which pertains to the task's goal, output from a costly planning algorithm, or communication with another agent. This can improve generalization with respect to goals or minimize access to expensive information. While variational inference-based implementations of the information bottleneck perform compression, they require unrestricted, lossless access to the privileged input to compute the bottleneck variable. To address this, we propose the variational bandwidth bottleneck, which estimates the value of the privileged information based solely on the standard input and chooses whether to access it stochastically. We provide a feasible approximation to this approach and demonstrate its effectiveness in enhancing generalization and reducing access to costly information in a range of reinforcement learning experiments.",1
"As a subfield of machine learning, reinforcement learning (RL) aims at empowering one's capabilities in behavioural decision making by using interaction experience with the world and an evaluative feedback. Unlike traditional supervised learning methods that usually rely on one-shot, exhaustive and supervised reward signals, RL tackles with sequential decision making problems with sampled, evaluative and delayed feedback simultaneously. Such distinctive features make RL technique a suitable candidate for developing powerful solutions in a variety of healthcare domains, where diagnosing decisions or treatment regimes are usually characterized by a prolonged and sequential procedure. This survey discusses the broad applications of RL techniques in healthcare domains, in order to provide the research community with systematic understanding of theoretical foundations, enabling methods and techniques, existing challenges, and new insights of this emerging paradigm. By first briefly examining theoretical foundations and key techniques in RL research from efficient and representational directions, we then provide an overview of RL applications in healthcare domains ranging from dynamic treatment regimes in chronic diseases and critical care, automated medical diagnosis from both unstructured and structured clinical data, as well as many other control or scheduling domains that have infiltrated many aspects of a healthcare system. Finally, we summarize the challenges and open issues in current research, and point out some potential solutions and directions for future research.",0
"Reinforcement learning (RL) is a machine learning subfield that enhances one's ability to make behavioral decisions through interaction with the environment and feedback evaluation. Compared to traditional supervised learning methods, which rely on singular, exhaustive, and supervised reward signals, RL deals with sequential decision-making challenges using simultaneous sampled, evaluative, and delayed feedback. RL techniques are well-suited for developing robust solutions in healthcare domains where diagnosing decisions or treatment regimes involve prolonged and sequential procedures. This survey provides a comprehensive review of the broad applications of RL techniques in healthcare domains, including theoretical foundations, enabling methods and techniques, existing challenges, and new insights. It examines key techniques in RL research, discusses RL applications in healthcare domains ranging from dynamic treatment regimes to automated medical diagnosis, and explores control or scheduling domains that have infiltrated many aspects of a healthcare system. Additionally, it summarizes the current challenges and open issues in the field and proposes potential solutions and directions for future research.",1
"We investigate the evolution of the Q values for the implementation of Deep Q Learning (DQL) in the Stable Baselines library. Stable Baselines incorporates the latest Reinforcement Learning techniques and achieves superhuman performance in many game environments. However, for some simple non-game environments, the DQL in Stable Baselines can struggle to find the correct actions. In this paper we aim to understand the types of environment where this suboptimal behavior can happen, and also investigate the corresponding evolution of the Q values for individual states.   We compare a smart TrafficLight environment (where performance is poor) with the AI Gym FrozenLake environment (where performance is perfect). We observe that DQL struggles with TrafficLight because actions are reversible and hence the Q values in a given state are closer than in FrozenLake. We then investigate the evolution of the Q values using a recent decomposition technique of Achiam et al.. We observe that for TrafficLight, the function approximation error and the complex relationships between the states lead to a situation where some Q values meander far from optimal.",0
"Our study delves into the development of Q values when using Deep Q Learning (DQL) in the Stable Baselines library. This library employs advanced Reinforcement Learning techniques and has proven to outperform humans in numerous gaming environments. However, in some non-gaming scenarios, the DQL in Stable Baselines may encounter difficulty in identifying the correct actions. Our objective is to determine the types of environments where such suboptimal behavior can occur and to examine the corresponding evolution of Q values for individual states. Specifically, we compare the performance of a smart TrafficLight environment, which yields poor results, with that of the AI Gym FrozenLake environment, which yields perfect results. Our findings reveal that the DQL struggles with the TrafficLight environment due to the reversibility of actions, resulting in Q values that are closer together than in FrozenLake. We then explore the evolution of Q values using a recent decomposition technique developed by Achiam et al. Our investigation exposes that the function approximation error and the intricate relationships between states in TrafficLight lead to some Q values straying far from optimal.",1
"Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference, especially when deploying to edge or IoT devices with limited computation capacity and power consumption budget. The uniform bit width quantization across all the layers is usually sub-optimal and the exploration of hybrid quantization for different layers is vital for efficient deep compression. In this paper, we employ the meta learning method to automatically realize low-bit hybrid quantization of neural networks. A MetaQuantNet, together with a Quantization function, are trained to generate the quantized weights for the target DNN. Then, we apply a genetic algorithm to search the best hybrid quantization policy that meets compression constraints. With the best searched quantization policy, we subsequently retrain or finetune to further improve the performance of the quantized target network. Extensive experiments demonstrate the performance of searched hybrid quantization scheme surpass that of uniform bitwidth counterpart. Compared to the existing reinforcement learning (RL) based hybrid quantization search approach that relies on tedious explorations, our meta learning approach is more efficient and effective for any compression requirements since the MetaQuantNet only needs be trained once.",0
"Model quantization is frequently used to speed up and compress deep neural network (DNN) inference, particularly when deploying to edge or IoT devices that have limited computation capacity and power consumption. However, uniform bit width quantization across all layers is not always optimal, and hybrid quantization exploration for different layers is essential for efficient deep compression. This paper proposes using meta learning to automatically achieve low-bit hybrid quantization of neural networks. A MetaQuantNet and Quantization function are trained to generate quantized weights for the target DNN. A genetic algorithm is applied to search for the best hybrid quantization policy that meets compression constraints. With the best searched quantization policy, the quantized target network is retrained or fine-tuned to further enhance performance. Extensive experiments demonstrate that the performance of the searched hybrid quantization scheme outperforms the uniform bitwidth counterpart. Compared to existing reinforcement learning (RL) based hybrid quantization search approaches that depend on tedious explorations, the proposed meta learning approach is more efficient and effective for any compression requirements since the MetaQuantNet only needs to be trained once.",1
"This paper considers multi-agent reinforcement learning (MARL) in networked system control. Specifically, each agent learns a decentralized control policy based on local observations and messages from connected neighbors. We formulate such a networked MARL (NMARL) problem as a spatiotemporal Markov decision process and introduce a spatial discount factor to stabilize the training of each local agent. Further, we propose a new differentiable communication protocol, called NeurComm, to reduce information loss and non-stationarity in NMARL. Based on experiments in realistic NMARL scenarios of adaptive traffic signal control and cooperative adaptive cruise control, an appropriate spatial discount factor effectively enhances the learning curves of non-communicative MARL algorithms, while NeurComm outperforms existing communication protocols in both learning efficiency and control performance.",0
"The objective of this paper is to explore multi-agent reinforcement learning (MARL) in networked system control. The approach involves each agent learning a decentralized control policy based on local observations and messages from connected neighbors. To address this networked MARL (NMARL) problem, the paper presents a spatiotemporal Markov decision process and introduces a spatial discount factor to stabilize the training of each local agent. Additionally, the paper proposes a new communication protocol, NeurComm, to minimize information loss and non-stationarity in NMARL. The paper conducts experiments in adaptive traffic signal control and cooperative adaptive cruise control to test the effectiveness of the spatial discount factor and NeurComm. The results show that the spatial discount factor enhances the learning curves of non-communicative MARL algorithms, while NeurComm outperforms existing communication protocols in both learning efficiency and control performance.",1
"This paper presents a new neural architecture that combines a modulated Hebbian network (MOHN) with DQN, which we call modulated Hebbian plus Q network architecture (MOHQA). The hypothesis is that such a combination allows MOHQA to solve difficult partially observable Markov decision process (POMDP) problems which impair temporal difference (TD)-based RL algorithms such as DQN, as the TD error cannot be easily derived from observations. The key idea is to use a Hebbian network with bio-inspired neural traces in order to bridge temporal delays between actions and rewards when confounding observations and sparse rewards result in inaccurate TD errors. In MOHQA, DQN learns low level features and control, while the MOHN contributes to the high-level decisions by associating rewards with past states and actions. Thus the proposed architecture combines two modules with significantly different learning algorithms, a Hebbian associative network and a classical DQN pipeline, exploiting the advantages of both. Simulations on a set of POMDPs and on the MALMO environment show that the proposed algorithm improved DQN's results and even outperformed control tests with A2C, QRDQN+LSTM and REINFORCE algorithms on some POMDPs with confounding stimuli and sparse rewards.",0
"The article introduces a novel neural structure, called MOHQA, that merges a modulated Hebbian network (MOHN) with DQN to tackle challenging partially observable Markov decision process (POMDP) issues that traditional TD-based RL algorithms like DQN struggle to solve due to inaccurate TD errors resulting from obscure observations and sparse rewards. The MOHN with bio-inspired neural traces functions as a Hebbian network to connect temporal delays between actions and rewards, while DQN focuses on low-level features and control. By uniting these two modules, MOHQA takes advantage of both learning algorithms and produces better results than A2C, QRDQN+LSTM, and REINFORCE algorithms on some POMDPs with confounding stimuli and sparse rewards according to simulations on a variety of POMDPs and the MALMO setting.",1
"Standard planners for sequential decision making (including Monte Carlo planning, tree search, dynamic programming, etc.) are constrained by an implicit sequential planning assumption: The order in which a plan is constructed is the same in which it is executed. We consider alternatives to this assumption for the class of goal-directed Reinforcement Learning (RL) problems. Instead of an environment transition model, we assume an imperfect, goal-directed policy. This low-level policy can be improved by a plan, consisting of an appropriate sequence of sub-goals that guide it from the start to the goal state. We propose a planning algorithm, Divide-and-Conquer Monte Carlo Tree Search (DC-MCTS), for approximating the optimal plan by means of proposing intermediate sub-goals which hierarchically partition the initial tasks into simpler ones that are then solved independently and recursively. The algorithm critically makes use of a learned sub-goal proposal for finding appropriate partitions trees of new tasks based on prior experience. Different strategies for learning sub-goal proposals give rise to different planning strategies that strictly generalize sequential planning. We show that this algorithmic flexibility over planning order leads to improved results in navigation tasks in grid-worlds as well as in challenging continuous control environments.",0
"Sequential decision-making planners such as Monte Carlo planning, tree search, and dynamic programming are limited by the assumption that the order in which a plan is created is the same as the order in which it is executed. In order to address this constraint, we explore alternative approaches for goal-directed Reinforcement Learning (RL) problems. Instead of relying on an environment transition model, we assume an imperfect, goal-directed policy that can be refined through a plan consisting of sub-goals that guide it from the start to the goal state. We propose an algorithm called Divide-and-Conquer Monte Carlo Tree Search (DC-MCTS) that approximates the optimal plan by proposing intermediate sub-goals that hierarchically partition initial tasks into simpler ones that can be solved independently and recursively. This approach utilizes a learned sub-goal proposal for finding appropriate partitions based on prior experience, which enables different planning strategies that generalize beyond sequential planning. Our experiments demonstrate that this algorithmic flexibility leads to improved performance in navigation tasks in grid-worlds and challenging continuous control environments.",1
"Deep reinforcement learning (RL) algorithms frequently require prohibitive interaction experience to ensure the quality of learned policies. The limitation is partly because the agent cannot learn much from the many low-quality trials in early learning phase, which results in low learning rate. Focusing on addressing this limitation, this paper makes a twofold contribution. First, we develop an algorithm, called Experience Grafting (EG), to enable RL agents to reorganize segments of the few high-quality trajectories from the experience pool to generate many synthetic trajectories while retaining the quality. Second, building on EG, we further develop an AutoEG agent that automatically learns to adjust the grafting-based learning strategy. Results collected from a set of six robotic control environments show that, in comparison to a standard deep RL algorithm (DDPG), AutoEG increases the speed of learning process by at least 30%.",0
"To ensure the quality of learned policies, deep reinforcement learning (RL) algorithms often require a significant amount of interaction experience. This is due to the agent's inability to learn from low-quality trials in the early phases of learning, resulting in a low learning rate. This paper addresses this limitation in two ways. Firstly, we introduce the Experience Grafting (EG) algorithm, which enables RL agents to reorganize segments of high-quality trajectories from the experience pool to synthesize many high-quality trajectories. Secondly, we develop an AutoEG agent that automatically adjusts the grafting-based learning strategy. Our results from six robotic control environments indicate that AutoEG speeds up the learning process by at least 30% compared to the standard deep RL algorithm (DDPG).",1
"This paper presents a preliminary study comparing different observation and action space representations for Deep Reinforcement Learning (DRL) in the context of Real-time Strategy (RTS) games. Specifically, we compare two representations: (1) a global representation where the observation represents the whole game state, and the RL agent needs to choose which unit to issue actions to, and which actions to execute; and (2) a local representation where the observation is represented from the point of view of an individual unit, and the RL agent picks actions for each unit independently. We evaluate these representations in $\mu$RTS showing that the local representation seems to outperform the global representation when training agents with the task of harvesting resources.",0
"In this paper, an initial study is presented that compares various representations of observation and action spaces for Deep Reinforcement Learning (DRL) in Real-time Strategy (RTS) games. Specifically, two representations are compared: (1) a global representation that encompasses the entire game state, requiring the RL agent to select which unit to issue actions to and which actions to execute; and (2) a local representation that is viewed from the perspective of an individual unit, and the RL agent selects actions for each unit independently. The effectiveness of these representations is assessed in $\mu$RTS, indicating that the local representation appears to be more effective than the global representation when training agents to harvest resources.",1
"In this work, we present a learning-based approach to chip placement, one of the most complex and time-consuming stages of the chip design process. Unlike prior methods, our approach has the ability to learn from past experience and improve over time. In particular, as we train over a greater number of chip blocks, our method becomes better at rapidly generating optimized placements for previously unseen chip blocks. To achieve these results, we pose placement as a Reinforcement Learning (RL) problem and train an agent to place the nodes of a chip netlist onto a chip canvas. To enable our RL policy to generalize to unseen blocks, we ground representation learning in the supervised task of predicting placement quality. By designing a neural architecture that can accurately predict reward across a wide variety of netlists and their placements, we are able to generate rich feature embeddings of the input netlists. We then use this architecture as the encoder of our policy and value networks to enable transfer learning. Our objective is to minimize PPA (power, performance, and area), and we show that, in under 6 hours, our method can generate placements that are superhuman or comparable on modern accelerator netlists, whereas existing baselines require human experts in the loop and take several weeks.",0
"Our work introduces a learning-based approach to chip placement, which is a complex and time-consuming stage of chip design. Unlike previous methods, our approach has the ability to improve over time by learning from past experience. As we train on more chip blocks, our method becomes better at rapidly generating optimized placements for new blocks. We frame placement as a Reinforcement Learning problem and train an agent to place nodes of a chip netlist onto a canvas. To enable our RL policy to generalize to new blocks, we use supervised learning to predict placement quality. We design a neural architecture that accurately predicts reward across a variety of netlists and placements, generating rich feature embeddings of input netlists. This architecture serves as the encoder of our policy and value networks, enabling transfer learning. Our objective is to minimize PPA, and our method generates superhuman or comparable placements on modern accelerator netlists in under 6 hours. Existing baselines require human experts in the loop and take several weeks.",1
"One of the most interesting application scenarios in anomaly detection is when sequential data are targeted. For example, in a safety-critical environment, it is crucial to have an automatic detection system to screen the streaming data gathered by monitoring sensors and to report abnormal observations if detected in real-time. Oftentimes, stakes are much higher when these potential anomalies are intentional or goal-oriented. We propose an end-to-end framework for sequential anomaly detection using inverse reinforcement learning (IRL), whose objective is to determine the decision-making agent's underlying function which triggers his/her behavior. The proposed method takes the sequence of actions of a target agent (and possibly other meta information) as input. The agent's normal behavior is then understood by the reward function which is inferred via IRL. We use a neural network to represent a reward function. Using a learned reward function, we evaluate whether a new observation from the target agent follows a normal pattern. In order to construct a reliable anomaly detection method and take into consideration the confidence of the predicted anomaly score, we adopt a Bayesian approach for IRL. The empirical study on publicly available real-world data shows that our proposed method is effective in identifying anomalies.",0
"Anomaly detection is particularly fascinating when dealing with sequential data. In critical safety settings, the ability to automatically detect abnormalities in real-time from a stream of data is essential. When these anomalies are intentional or goal-oriented, the stakes can be even higher. This is why we have developed an end-to-end framework for sequential anomaly detection, utilizing inverse reinforcement learning (IRL). Our method aims to determine the decision-making process behind an agent's behavior by analyzing their sequence of actions and other relevant information. We use a neural network to represent the agent's normal behavior, which is inferred through IRL. The reward function is then utilized to assess whether a new observation from the target agent follows a normal pattern. In order to ensure the reliability of the anomaly detection method and account for the confidence level of the predicted anomaly score, we employ a Bayesian approach for IRL. Our empirical study on real-world data publicly available demonstrates the effectiveness of our proposed method in identifying anomalies.",1
"Probabilistic Computation Tree Logic (PCTL) is frequently used to formally specify control objectives such as probabilistic reachability and safety. In this work, we focus on model checking PCTL specifications statistically on Markov Decision Processes (MDPs) by sampling, e.g., checking whether there exists a feasible policy such that the probability of reaching certain goal states is greater than a threshold. We use reinforcement learning to search for such a feasible policy for PCTL specifications, and then develop a statistical model checking (SMC) method with provable guarantees on its error. Specifically, we first use upper-confidence-bound (UCB) based Q-learning to design an SMC algorithm for bounded-time PCTL specifications, and then extend this algorithm to unbounded-time specifications by identifying a proper truncation time by checking the PCTL specification and its negation at the same time. Finally, we evaluate the proposed method on case studies.",0
"The application of Probabilistic Computation Tree Logic (PCTL) is widespread in the formal specification of control objectives, such as probabilistic reachability and safety. This research directs its attention towards statistically model checking PCTL specifications on Markov Decision Processes (MDPs) through sampling, with a focus on determining the feasibility of policies that achieve a threshold probability of reaching certain goal states. The study then employs reinforcement learning to search for feasible policies for PCTL specifications and subsequently develops a statistical model checking (SMC) method with error guarantees. The SMC algorithm is designed for bounded-time PCTL specifications using upper-confidence-bound (UCB) based Q-learning and is further extended to unbounded-time specifications by identifying an appropriate truncation time. The proposed method is evaluated using case studies.",1
"Multi-simulator training has contributed to the recent success of Deep Reinforcement Learning by stabilizing learning and allowing for higher training throughputs. We propose Gossip-based Actor-Learner Architectures (GALA) where several actor-learners (such as A2C agents) are organized in a peer-to-peer communication topology, and exchange information through asynchronous gossip in order to take advantage of a large number of distributed simulators. We prove that GALA agents remain within an epsilon-ball of one-another during training when using loosely coupled asynchronous communication. By reducing the amount of synchronization between agents, GALA is more computationally efficient and scalable compared to A2C, its fully-synchronous counterpart. GALA also outperforms A2C, being more robust and sample efficient. We show that we can run several loosely coupled GALA agents in parallel on a single GPU and achieve significantly higher hardware utilization and frame-rates than vanilla A2C at comparable power draws.",0
"The success of Deep Reinforcement Learning is attributed to the use of multi-simulator training, which stabilizes learning and increases training throughput. Our proposal, Gossip-based Actor-Learner Architectures (GALA), involves organizing several actor-learners, such as A2C agents, in a peer-to-peer communication topology. These agents exchange information through asynchronous gossip to take advantage of distributed simulators. We prove that GALA agents remain within an epsilon-ball of one another during training with loosely coupled asynchronous communication. GALA is more computationally efficient and scalable than A2C, its fully-synchronous counterpart, as it reduces the amount of synchronization between agents. Moreover, GALA outperforms A2C in terms of robustness and sample efficiency. We demonstrate that we can run several loosely coupled GALA agents on a single GPU in parallel, achieving significantly higher hardware utilization and frame-rates than vanilla A2C at comparable power draws.",1
"This paper investigates to what extent one can improve reinforcement learning algorithms. Our study is split in three parts. First, our analysis shows that the classical asymptotic convergence rate $O(1/\sqrt{N})$ is pessimistic and can be replaced by $O((\log(N)/N)^{\beta})$ with $\frac{1}{2}\leq \beta \leq 1$ and $N$ the number of iterations. Second, we propose a dynamic optimal policy for the choice of the learning rate $(\gamma_k)_{k\geq 0}$ used in stochastic approximation (SA). We decompose our policy into two interacting levels: the inner and the outer level. In the inner level, we present the \nameref{Alg:v_4_s} algorithm (for ""PAst Sign Search"") which, based on a predefined sequence $(\gamma^o_k)_{k\geq 0}$, constructs a new sequence $(\gamma^i_k)_{k\geq 0}$ whose error decreases faster. In the outer level, we propose an optimal methodology for the selection of the predefined sequence $(\gamma^o_k)_{k\geq 0}$. Third, we show empirically that our selection methodology of the learning rate outperforms significantly standard algorithms used in reinforcement learning (RL) in the three following applications: the estimation of a drift, the optimal placement of limit orders and the optimal execution of large number of shares.",0
"The objective of this study is to explore ways to enhance reinforcement learning algorithms. The investigation is divided into three sections. Firstly, our findings reveal that the conventional asymptotic convergence rate of $O(1/\sqrt{N})$ is overly negative and can be substituted with $O((\log(N)/N)^{\beta})$, where $N$ is the number of iterations and $\frac{1}{2}\leq \beta \leq 1$. Secondly, we suggest a dynamic optimal policy for selecting the learning rate $(\gamma_k)_{k\geq 0}$ employed in stochastic approximation (SA). Our policy has two interacting levels: the inner and the outer level. In the inner level, we introduce the \nameref{Alg:v_4_s} algorithm, which constructs a new sequence $(\gamma^i_k)_{k\geq 0}$ with quicker error reduction based on a predefined sequence $(\gamma^o_k)_{k\geq 0}$. In the outer level, we propose a superior methodology for selecting the predefined sequence $(\gamma^o_k)_{k\geq 0}$. Thirdly, we demonstrate through empirical evidence that our learning rate selection methodology outperforms standard reinforcement learning algorithms in three applications: drift estimation, optimal limit order placement, and optimal execution of large number of shares.",1
"High-dimensional always-changing environments constitute a hard challenge for current reinforcement learning techniques. Artificial agents, nowadays, are often trained off-line in very static and controlled conditions in simulation such that training observations can be thought as sampled i.i.d. from the entire observations space. However, in real world settings, the environment is often non-stationary and subject to unpredictable, frequent changes. In this paper we propose and openly release CRLMaze, a new benchmark for learning continually through reinforcement in a complex 3D non-stationary task based on ViZDoom and subject to several environmental changes. Then, we introduce an end-to-end model-free continual reinforcement learning strategy showing competitive results with respect to four different baselines and not requiring any access to additional supervised signals, previously encountered environmental conditions or observations.",0
"Current reinforcement learning techniques face a difficult challenge in high-dimensional and always-changing environments. Presently, artificial agents are trained offline in controlled and static conditions in simulations, where training observations are considered as independent and identically distributed samples from the complete observation space. However, real-world settings are often non-stationary and frequently subject to unpredictable changes. This paper introduces CRLMaze, an openly released benchmark for continual reinforcement learning in a complex 3D non-stationary task based on ViZDoom, which is subject to various environmental changes. Additionally, we present an end-to-end model-free continual reinforcement learning strategy that delivers competitive results compared to four different baselines without requiring any supervised signals or prior access to environmental conditions and observations.",1
"Path planning methods for the unmanned aerial vehicle (UAV) in goods delivery have drawn great attention from industry and academics because of its flexibility which is suitable for many situations in the ""Last Kilometer"" between customer and delivery nodes. However, the complicated situation is still a problem for traditional combinatorial optimization methods. Based on the state-of-the-art Reinforcement Learning (RL), this paper proposed an improved method to achieve path planning for UAVs in complex surroundings: multiple no-fly zones. The improved approach leverages the attention mechanism and includes the embedding mechanism as the encoder and three different widths of beam search (i.e.,~1, 5, and 10) as the decoders. Policy gradients are utilized to train the RL model for obtaining the optimal strategies during inference. The results show the feasibility and efficiency of the model applying in this kind of complicated situation. Comparing the model with the results obtained by the optimization solver OR-tools, it improves the reliability of the distribution system and has a guiding significance for the broad application of UAVs.",0
"The use of unmanned aerial vehicles (UAVs) for goods delivery has gained considerable attention from industry and academia due to their flexibility in navigating the ""Last Kilometer"" between delivery nodes and customers. However, traditional combinatorial optimization methods struggle with the complex situations encountered in this field. To address this, this paper proposes an improved path planning method using Reinforcement Learning (RL) to navigate UAVs through multiple no-fly zones. The method employs an attention mechanism and an encoder-embedding mechanism, with three decoders of varying widths. Policy gradients are used to train the RL model to obtain optimal strategies during inference. Results demonstrate the model's feasibility and efficiency in navigating complex situations, improving the reliability of distribution systems and guiding the wider application of UAVs. A comparison with the optimization solver OR-tools further validates the model's effectiveness.",1
"Surveillance camera networks are a useful infrastructure for various visual analytics applications, where high-level inferences and predictions could be made based on target tracking across the network. Most multi-camera tracking works focus on target re-identification and trajectory association problems to track the target. However, since camera networks can generate enormous amount of video data, inefficient schemes for making re-identification or trajectory association queries can incur prohibitively large computational requirements. In this paper, we address the problem of intelligent scheduling of re-identification queries in a multi-camera tracking setting. To this end, we formulate the target tracking problem in a camera network as an MDP and learn a reinforcement learning based policy that selects a camera for making a re-identification query. The proposed approach to camera selection does not assume the knowledge of the camera network topology but the resulting policy implicitly learns it. We have also shown that such a policy can be learnt directly from data. Using the NLPR MCT and the Duke MTMC multi-camera multi-target tracking benchmarks, we empirically show that the proposed approach substantially reduces the number of frames queried.",0
"Surveillance camera networks have numerous applications for visual analytics, allowing for inferences and predictions based on target tracking across the network. However, multi-camera tracking can present challenges such as inefficient re-identification and trajectory association queries due to the vast amount of video data generated. This paper proposes an intelligent scheduling approach for re-identification queries in multi-camera tracking, using reinforcement learning to select the best camera without needing prior knowledge of the network topology. Empirical testing on the NLPR MCT and Duke MTMC benchmarks shows that this approach significantly reduces the number of frames queried.",1
"While high resolution images contain semantically more useful information than their lower resolution counterparts, processing them is computationally more expensive, and in some applications, e.g. remote sensing, they can be much more expensive to acquire. For these reasons, it is desirable to develop an automatic method to selectively use high resolution data when necessary while maintaining accuracy and reducing acquisition/run-time cost. In this direction, we propose PatchDrop a reinforcement learning approach to dynamically identify when and where to use/acquire high resolution data conditioned on the paired, cheap, low resolution images. We conduct experiments on CIFAR10, CIFAR100, ImageNet and fMoW datasets where we use significantly less high resolution data while maintaining similar accuracy to models which use full high resolution images.",0
"Although high resolution images offer more semantically useful information compared to their lower resolution counterparts, they come with a higher computational cost and can be much more expensive to acquire in certain applications such as remote sensing. To address this issue, there is a need to develop an automatic method that can selectively use high resolution data when necessary while still maintaining accuracy and reducing acquisition and run-time costs. To this end, we introduce PatchDrop, a reinforcement learning approach that can dynamically identify when and where to use/acquire high resolution data based on the paired low resolution images, which are significantly cheaper. In our experiments on various datasets including CIFAR10, CIFAR100, ImageNet, and fMoW, we demonstrate that our approach can achieve similar accuracy to models that use full high resolution images while using significantly less high resolution data.",1
"A significant challenge in developing AI that can generalize well is designing agents that learn about their world without being told what to learn, and apply that learning to challenges with sparse rewards. Moreover, most traditional reinforcement learning approaches explicitly separate learning and decision making in a way that does not correspond to biological learning. We implement an architecture founded in principles of experimental neuroscience, by combining computationally efficient abstractions of biological algorithms. Our approach is inspired by research on spike-timing dependent plasticity, the transition between short and long term memory, and the role of various neurotransmitters in rewarding curiosity. The Neurons-in-a-Box architecture can learn in a wholly generalizable manner, and demonstrates an efficient way to build and apply representations without explicitly optimizing over a set of criteria or actions. We find it performs well in many environments including OpenAI Gym's Mountain Car, which has no reward besides touching a hard-to-reach flag on a hill, Inverted Pendulum, where it learns simple strategies to improve the time it holds a pendulum up, a video stream, where it spontaneously learns to distinguish an open and closed hand, as well as other environments like Google Chrome's Dinosaur Game.",0
"Developing AI that can generalize poses a significant challenge, as agents must learn about their environment without explicit instruction and apply this knowledge to tasks with limited rewards. Many traditional reinforcement learning techniques separate learning and decision making, which differs from biological learning. To overcome this, we have designed an architecture based on principles of experimental neuroscience, using biologically inspired algorithms, including spike-timing dependent plasticity, short and long term memory, and neurotransmitters that reward curiosity. Our Neurons-in-a-Box architecture can learn in a wholly generalizable way, without explicitly optimizing over criteria or actions. We have tested it in various environments, including Mountain Car, Inverted Pendulum, and a video stream, where it successfully distinguished between open and closed hands. Our approach is efficient and effective, outperforming other methods, even in challenging environments like Google Chrome's Dinosaur Game.",1
"Recent works in high-dimensional model-predictive control and model-based reinforcement learning with learned dynamics and reward models have resorted to population-based optimization methods, such as the Cross-Entropy Method (CEM), for planning a sequence of actions. To decide on an action to take, CEM conducts a search for the action sequence with the highest return according to the dynamics model and reward. Action sequences are typically randomly sampled from an unconditional Gaussian distribution and evaluated on the environment. This distribution is iteratively updated towards action sequences with higher returns. However, this planning method can be very inefficient, especially for high-dimensional action spaces. An alternative line of approaches optimize action sequences directly via gradient descent, but are prone to local optima. We propose a method to solve this planning problem by interleaving CEM and gradient descent steps in optimizing the action sequence. Our experiments show faster convergence of the proposed hybrid approach, even for high-dimensional action spaces, avoidance of local minima, and better or equal performance to CEM. Code accompanying the paper is available here https://github.com/homangab/gradcem.",0
"In recent times, population-based optimization techniques like the Cross-Entropy Method (CEM) have been utilized in high-dimensional model-predictive control and model-based reinforcement learning with learned dynamics and reward models to plan a sequence of actions. CEM searches for the action sequence with the highest return based on the dynamics model and reward to decide on the action to take. The action sequences are chosen randomly from an unconditional Gaussian distribution and evaluated on the environment. The distribution is updated iteratively towards action sequences with higher returns, but this planning method can be inefficient, especially for high-dimensional action spaces. An alternative approach is to optimize action sequences directly via gradient descent, but this is susceptible to local optima. We propose a hybrid approach that interleaves CEM and gradient descent steps in optimizing the action sequence to solve this problem. Our experiments demonstrate faster convergence of the proposed method, avoidance of local minima, and better or equal performance to CEM, even for high-dimensional action spaces. The code for our method is accessible at https://github.com/homangab/gradcem.",1
"Although recent model-free reinforcement learning algorithms have been shown to be capable of mastering complicated decision-making tasks, the sample complexity of these methods has remained a hurdle to utilizing them in many real-world applications. In this regard, model-based reinforcement learning proposes some remedies. Yet, inherently, model-based methods are more computationally expensive and susceptible to sub-optimality. One reason is that model-generated data are always less accurate than real data, and this often leads to inaccurate transition and reward function models. With the aim to mitigate this problem, this work presents the notion of survival by discussing cases in which the agent's goal is to survive and its analogy to maximizing the expected rewards. To that end, a substitute model for the reward function approximator is introduced that learns to avoid terminal states rather than to maximize accumulated rewards from safe states. Focusing on terminal states, as a small fraction of state-space, reduces the training effort drastically. Next, a model-based reinforcement learning method is proposed (Survive) to train an agent to avoid dangerous states through a safety map model built upon temporal credit assignment in the vicinity of terminal states. Finally, the performance of the presented algorithm is investigated, along with a comparison between the proposed and current methods.",0
"While recent model-free reinforcement learning algorithms have proven effective at mastering complex decision-making tasks, the challenge of sample complexity has hindered their application in real-world scenarios. Model-based reinforcement learning offers potential solutions to this issue, yet it is computationally expensive and prone to sub-optimal outcomes due to inaccuracies in transition and reward function models generated by less precise model-generated data. This paper addresses this problem by introducing the concept of survival, which emphasizes avoiding terminal states rather than maximizing rewards. A substitute model for the reward function approximator is presented, and a new model-based reinforcement learning method called Survive is proposed to train agents to evade dangerous situations through a safety map model built around temporal credit assignment near terminal states. The study concludes with an assessment of the algorithm's performance and a comparison to existing methods.",1
"In real-world multi-robot systems, performing high-quality, collaborative behaviors requires robots to asynchronously reason about high-level action selection at varying time durations. Macro-Action Decentralized Partially Observable Markov Decision Processes (MacDec-POMDPs) provide a general framework for asynchronous decision making under uncertainty in fully cooperative multi-agent tasks. However, multi-agent deep reinforcement learning methods have only been developed for (synchronous) primitive-action problems. This paper proposes two Deep Q-Network (DQN) based methods for learning decentralized and centralized macro-action-value functions with novel macro-action trajectory replay buffers introduced for each case. Evaluations on benchmark problems and a larger domain demonstrate the advantage of learning with macro-actions over primitive-actions and the scalability of our approaches.",0
"To achieve high-quality, collaborative behaviors in real-world multi-robot systems, robots must be able to reason asynchronously about high-level action selection at varying time durations. The Macro-Action Decentralized Partially Observable Markov Decision Processes (MacDec-POMDPs) provide a general framework for asynchronous decision making under uncertainty in fully cooperative multi-agent tasks. However, current multi-agent deep reinforcement learning methods are only designed for synchronous primitive-action problems. This paper proposes two Deep Q-Network (DQN) based methods for learning decentralized and centralized macro-action-value functions. The proposed methods introduce novel macro-action trajectory replay buffers for each case. The evaluations conducted on benchmark problems and a larger domain show that learning with macro-actions has an advantage over primitive-actions and that our approaches are scalable.",1
"Reinforcement learning (RL) allows to solve complex tasks such as Go often with a stronger performance than humans. However, the learned behaviors are usually fixed to specific tasks and unable to adapt to different contexts. Here we consider the case of adapting RL agents to different time restrictions, such as finishing a task with a given time limit that might change from one task execution to the next. We define such problems as Time Adaptive Markov Decision Processes and introduce two model-free, value-based algorithms: the Independent Gamma-Ensemble and the n-Step Ensemble. In difference to classical approaches, they allow a zero-shot adaptation between different time restrictions. The proposed approaches represent general mechanisms to handle time adaptive tasks making them compatible with many existing RL methods, algorithms, and scenarios.",0
"Reinforcement learning (RL) is capable of solving complex tasks, like Go, with higher proficiency than humans. However, the learned behaviors are typically limited to specific tasks and fail to adapt to varying contexts. To address this, we examine the ability of RL agents to adapt to different time constraints when completing tasks, which may fluctuate from one execution to the next. We refer to these problems as Time Adaptive Markov Decision Processes and present two value-based, model-free algorithms: the Independent Gamma-Ensemble and the n-Step Ensemble. Unlike traditional approaches, these algorithms facilitate seamless adaptation between dissimilar time constraints. Our proposed methods are broadly applicable and complement established RL techniques, algorithms, and scenarios.",1
"In this paper, we formulate the adaptive learning problem---the problem of how to find an individualized learning plan (called policy) that chooses the most appropriate learning materials based on learner's latent traits---faced in adaptive learning systems as a Markov decision process (MDP). We assume latent traits to be continuous with an unknown transition model. We apply a model-free deep reinforcement learning algorithm---the deep Q-learning algorithm---that can effectively find the optimal learning policy from data on learners' learning process without knowing the actual transition model of the learners' continuous latent traits. To efficiently utilize available data, we also develop a transition model estimator that emulates the learner's learning process using neural networks. The transition model estimator can be used in the deep Q-learning algorithm so that it can more efficiently discover the optimal learning policy for a learner. Numerical simulation studies verify that the proposed algorithm is very efficient in finding a good learning policy, especially with the aid of a transition model estimator, it can find the optimal learning policy after training using a small number of learners.",0
"The aim of this paper is to present a solution to the adaptive learning problem faced by adaptive learning systems. This problem involves finding an individualized learning plan, or policy, that selects the most suitable learning materials based on the learner's latent traits. We approach this problem by formulating it as a Markov decision process (MDP) and assuming that the latent traits are continuous and have an unknown transition model. To address this issue, we use a model-free deep reinforcement learning algorithm, called the deep Q-learning algorithm, which can identify the optimal learning policy from learner data. To further improve the algorithm's performance, we introduce a transition model estimator that uses neural networks to simulate the learner's learning process. This estimator can be integrated into the deep Q-learning algorithm to enhance its ability to identify the optimal learning policy. Our simulations demonstrate that this algorithm is effective in identifying a good learning policy, and that it can quickly identify the optimal policy with a small number of learners when used in conjunction with the transition model estimator.",1
"We propose a novel approach to learn goal-conditioned policies for locomotion in a batch RL setting. The batch data is collected by a policy that is not goal-conditioned. For the locomotion task, this translates to data collection using a policy learnt by the agent for walking straight in one direction, and using that data to learn a goal-conditioned policy that enables the agent to walk in any direction. The data collection policy used should be invariant to the direction the agent is facing i.e. regardless of its initial orientation, the agent should take the same actions to walk forward. We exploit this property to learn a goal-conditioned policy using two key ideas: (1) augmenting data by generating trajectories with the same actions in different directions, and (2) learning an encoder that enforces invariance between these rotated trajectories with a Siamese framework. We show that our approach outperforms existing RL algorithms on 3-D locomotion agents like Ant, Humanoid and Minitaur.",0
"In this study, we present an innovative method for acquiring goal-conditioned policies for locomotion in a batch RL environment. The batch data is gathered using a policy that lacks goal-conditioning. To achieve locomotion, we utilize the data collected by an agent trained to walk straight in one direction and use it to teach a goal-conditioned policy that allows the agent to move in any direction. The data collection strategy should be unaffected by the agent's orientation, meaning that the agent should perform the same actions to move forward, regardless of its starting position. We make use of this characteristic to develop a goal-conditioned policy by (1) expanding data by producing trajectories with identical actions in varying directions, and (2) teaching an encoder that enforces consistency between these rotated trajectories using a Siamese framework. We demonstrate that our technique outperforms existing RL algorithms on 3-D locomotion agents such as Ant, Humanoid, and Minitaur.",1
"Traditional centralized multi-agent reinforcement learning (MARL) algorithms are sometimes unpractical in complicated applications, due to non-interactivity between agents, curse of dimensionality and computation complexity. Hence, several decentralized MARL algorithms are motivated. However, existing decentralized methods only handle the fully cooperative setting where massive information needs to be transmitted in training. The block coordinate gradient descent scheme they used for successive independent actor and critic steps can simplify the calculation, but it causes serious bias. In this paper, we propose a flexible fully decentralized actor-critic MARL framework, which can combine most of actor-critic methods, and handle large-scale general cooperative multi-agent setting. A primal-dual hybrid gradient descent type algorithm framework is designed to learn individual agents separately for decentralization. From the perspective of each agent, policy improvement and value evaluation are jointly optimized, which can stabilize multi-agent policy learning. Furthermore, our framework can achieve scalability and stability for large-scale environment and reduce information transmission, by the parameter sharing mechanism and a novel modeling-other-agents methods based on theory-of-mind and online supervised learning. Sufficient experiments in cooperative Multi-agent Particle Environment and StarCraft II show that our decentralized MARL instantiation algorithms perform competitively against conventional centralized and decentralized methods.",0
"In complex applications, traditional centralized multi-agent reinforcement learning (MARL) algorithms are not always practical due to non-interactivity between agents, the curse of dimensionality, and computation complexity. Consequently, decentralized MARL algorithms have been developed. However, current decentralized approaches only handle fully cooperative settings, which require significant information transmission during training. Although the block coordinate gradient descent scheme simplifies calculation, it results in significant bias. To address these issues, we propose a flexible, fully decentralized actor-critic MARL framework that can handle multi-agent settings and combine different actor-critic methods. We use a primal-dual hybrid gradient descent type algorithm to learn individual agents separately for decentralization. Our framework optimizes policy improvement and value evaluation jointly, which stabilizes multi-agent policy learning. Additionally, our framework achieves scalability and stability in large-scale environments and reduces information transmission through parameter sharing and modeling-other-agents methods based on theory-of-mind and online supervised learning. Our experiments in Cooperative Multi-agent Particle Environment and StarCraft II demonstrate the competitive performance of our decentralized MARL algorithms compared to conventional centralized and decentralized methods.",1
"Progress in multiagent intelligence research is fundamentally limited by the number and quality of environments available for study. In recent years, simulated games have become a dominant research platform within reinforcement learning, in part due to their accessibility and interpretability. Previous works have targeted and demonstrated success on arcade, first person shooter (FPS), real-time strategy (RTS), and massive online battle arena (MOBA) games. Our work considers massively multiplayer online role-playing games (MMORPGs or MMOs), which capture several complexities of real-world learning that are not well modeled by any other game genre. We present Neural MMO, a massively multiagent game environment inspired by MMOs and discuss our progress on two more general challenges in multiagent systems engineering for AI research: distributed infrastructure and game IO. We further demonstrate that standard policy gradient methods and simple baseline models can learn interesting emergent exploration and specialization behaviors in this setting.",0
"The availability and quality of environments for research significantly impact the advancement of multiagent intelligence studies. Simulated games have become the primary research platform for reinforcement learning due to their accessibility and interpretability. Previous studies have focused on arcade, FPS, RTS, and MOBA games, demonstrating success in these areas. However, our research takes a different approach by examining the complex learning intricacies of MMORPGs or MMOs. These games offer a unique opportunity to model real-world learning complexities that other game genres cannot capture. Our study introduces Neural MMO, a multiagent game environment for AI research based on MMOs. We discuss our progress in addressing two significant multiagent system engineering challenges, namely distributed infrastructure and game IO. Additionally, we showcase how standard policy gradient methods and simple baseline models can learn emergent exploration and specialization behaviors in this environment.",1
"Predictive auxiliary tasks have been shown to improve performance in numerous reinforcement learning works, however, this effect is still not well understood. The primary purpose of the work presented here is to investigate the impact that an auxiliary task's prediction timescale has on the agent's policy performance. We consider auxiliary tasks which learn to make on-policy predictions using temporal difference learning. We test the impact of prediction timescale using a specific form of auxiliary task in which the input image is used as the prediction target, which we refer to as temporal difference autoencoders (TD-AE). We empirically evaluate the effect of TD-AE on the A2C algorithm in the VizDoom environment using different prediction timescales. While we do not observe a clear relationship between the prediction timescale on performance, we make the following observations: 1) using auxiliary tasks allows us to reduce the trajectory length of the A2C algorithm, 2) in some cases temporally extended TD-AE performs better than a straight autoencoder, 3) performance with auxiliary tasks is sensitive to the weight placed on the auxiliary loss, 4) despite this sensitivity, auxiliary tasks improved performance without extensive hyper-parameter tuning. Our overall conclusions are that TD-AE increases the robustness of the A2C algorithm to the trajectory length and while promising, further study is required to fully understand the relationship between auxiliary task prediction timescale and the agent's performance.",0
"Although predictive auxiliary tasks have been found to enhance performance in numerous reinforcement learning studies, their impact is not yet fully comprehended. This study aims to investigate how an auxiliary task's prediction timescale impacts an agent's policy performance. We examine auxiliary tasks that utilize temporal difference learning to generate on-policy predictions and assess the impact of prediction timescale using a particular type of auxiliary task called temporal difference autoencoders (TD-AE), which use the input image as the prediction target. We evaluate the effect of TD-AE on the A2C algorithm in the VizDoom environment using various prediction timescales. While we don't observe a clear relationship between prediction timescale and performance, we do make several observations, including the fact that using auxiliary tasks reduces the trajectory length of the A2C algorithm and that TD-AE may perform better than a straight autoencoder in some cases. Our findings also indicate that performance with auxiliary tasks is sensitive to the weight placed on the auxiliary loss. Nevertheless, auxiliary tasks can enhance performance without extensive hyper-parameter tuning. In conclusion, TD-AE enhances the A2C algorithm's robustness to trajectory length, but further research is necessary to fully comprehend the connection between auxiliary task prediction timescale and agent performance.",1
"Multi-Agent Reinforcement Learning (MARL) encompasses a powerful class of methodologies that have been applied in a wide range of fields. An effective way to further empower these methodologies is to develop libraries and tools that could expand their interpretability and explainability. In this work, we introduce MARLeME: a MARL model extraction library, designed to improve explainability of MARL systems by approximating them with symbolic models. Symbolic models offer a high degree of interpretability, well-defined properties, and verifiable behaviour. Consequently, they can be used to inspect and better understand the underlying MARL system and corresponding MARL agents, as well as to replace all/some of the agents that are particularly safety and security critical.",0
"MARL, a set of methodologies that have been widely used in various fields, can be further enhanced by developing libraries and tools that enhance interpretability and explainability. This research presents MARLeME, a MARL model extraction library that uses symbolic models to improve the explainability of MARL systems. Symbolic models are interpretable, have clear properties, and exhibit verifiable behavior, making them useful for inspecting and understanding the MARL system and agents, as well as replacing critical agents for safety and security purposes.",1
"Deep Neural Networks (DNNs) typically require massive amount of computation resource in inference tasks for computer vision applications. Quantization can significantly reduce DNN computation and storage by decreasing the bitwidth of network encodings. Recent research affirms that carefully selecting the quantization levels for each layer can preserve the accuracy while pushing the bitwidth below eight bits. However, without arduous manual effort, this deep quantization can lead to significant accuracy loss, leaving it in a position of questionable utility. As such, deep quantization opens a large hyper-parameter space (bitwidth of the layers), the exploration of which is a major challenge. We propose a systematic approach to tackle this problem, by automating the process of discovering the quantization levels through an end-to-end deep reinforcement learning framework (ReLeQ). We adapt policy optimization methods to the problem of quantization, and focus on finding the best design decisions in choosing the state and action spaces, network architecture and training framework, as well as the tuning of various hyperparamters. We show how ReLeQ can balance speed and quality, and provide an asymmetric general solution for quantization of a large variety of deep networks (AlexNet, CIFAR-10, LeNet, MobileNet-V1, ResNet-20, SVHN, and VGG-11) that virtually preserves the accuracy (=< 0.3% loss) while minimizing the computation and storage cost. With these DNNs, ReLeQ enables conventional hardware to achieve 2.2x speedup over 8-bit execution. Similarly, a custom DNN accelerator achieves 2.0x speedup and energy reduction compared to 8-bit runs. These encouraging results mark ReLeQ as the initial step towards automating the deep quantization of neural networks.",0
"For computer vision applications, Deep Neural Networks (DNNs) require a significant amount of computation resources in inference tasks. To reduce DNN computation and storage, Quantization can decrease the bitwidth of network encodings. Research suggests that carefully selecting quantization levels for each layer can maintain accuracy while pushing the bitwidth below eight bits. However, this deep quantization process can lead to significant accuracy loss without intense manual effort. Consequently, deep quantization opens a vast hyper-parameter space that presents a significant challenge. To tackle this problem, we propose an automated approach to discovering quantization levels through an end-to-end deep reinforcement learning framework called ReLeQ. By adapting policy optimization methods to the quantization problem, we focus on designing optimal decisions, including choosing the state and action spaces, network architecture and training framework, as well as tuning various hyperparameters. ReLeQ balances speed and quality and provides an asymmetric general solution for quantization of different deep networks, virtually preserving accuracy while minimizing computation and storage cost. With ReLeQ, conventional hardware achieves 2.2x speedup over 8-bit execution, while custom DNN accelerators achieve 2.0x speedup and energy reduction compared to 8-bit runs. These promising results indicate that ReLeQ is the first step in automating deep quantization of neural networks.",1
"We propose a novel method for analyzing and visualizing the complexity of standard reinforcement learning (RL) benchmarks based on score distributions. A large number of policy networks are generated by randomly guessing their parameters, and then evaluated on the benchmark task; the study of their aggregated results provide insights into the benchmark complexity. Our method guarantees objectivity of evaluation by sidestepping learning altogether: the policy network parameters are generated using Random Weight Guessing (RWG), making our method agnostic to (i) the classic RL setup, (ii) any learning algorithm, and (iii) hyperparameter tuning. We show that this approach isolates the environment complexity, highlights specific types of challenges, and provides a proper foundation for the statistical analysis of the task's difficulty. We test our approach on a variety of classic control benchmarks from the OpenAI Gym, where we show that small untrained networks can provide a robust baseline for a variety of tasks. The networks generated often show good performance even without gradual learning, incidentally highlighting the triviality of a few popular benchmarks.",0
"A unique technique is proposed to analyze and visualize the intricacy of standard reinforcement learning benchmarks through score distributions. The method involves generating numerous policy networks by randomly guessing their parameters and assessing them on the benchmark task. The aggregated results of these evaluations offer insights into the complexity of the benchmark. This approach guarantees impartial evaluation by bypassing the learning process, using Random Weight Guessing (RWG) to generate policy network parameters. As a result, our technique is independent of the classic RL setup, any learning algorithm, and hyperparameter tuning. Our approach facilitates the isolation of environment complexity, identification of specific types of challenges, and provides a robust foundation for statistical analysis of task difficulty. We evaluated our approach on a variety of OpenAI Gym's classic control benchmarks, where we demonstrate that small untrained networks can serve as a reliable baseline for several tasks. Our generated networks often exhibit strong performance even without gradual learning, thus highlighting the insignificance of some popular benchmarks.",1
"In this paper, the main task we aim to tackle is the multi-instance semi-supervised video object segmentation across a sequence of frames where only the first-frame box-level ground-truth is provided. Detection-based algorithms are widely adopted to handle this task, and the challenges lie in the selection of the matching method to predict the result as well as to decide whether to update the target template using the newly predicted result. The existing methods, however, make these selections in a rough and inflexible way, compromising their performance. To overcome this limitation, we propose a novel approach which utilizes reinforcement learning to make these two decisions at the same time. Specifically, the reinforcement learning agent learns to decide whether to update the target template according to the quality of the predicted result. The choice of the matching method will be determined at the same time, based on the action history of the reinforcement learning agent. Experiments show that our method is almost 10 times faster than the previous state-of-the-art method with even higher accuracy (region similarity of 69.1% on DAVIS 2017 dataset).",0
"The objective of this paper is to address the challenge of multi-instance semi-supervised video object segmentation across multiple frames with only first-frame box-level ground-truth provided. Detection-based algorithms are commonly used for this task, but selecting an appropriate matching method and deciding whether to update the target template using newly predicted results can be difficult. Existing methods tend to make these selections in a rigid and imprecise manner, which impacts their efficacy. To overcome this shortcoming, we introduce a new technique that leverages reinforcement learning to make these two decisions simultaneously. The reinforcement learning agent learns to determine whether the target template should be updated based on the quality of the predicted result, and the matching method is chosen based on the agent's action history. Our experiments demonstrate that our approach is significantly faster than the previous state-of-the-art method, with even higher accuracy (69.1% region similarity on the DAVIS 2017 dataset).",1
"In this paper, we propose a multi-timescale replay (MTR) buffer for improving continual learning in RL agents faced with environments that are changing continuously over time at timescales that are unknown to the agent. The basic MTR buffer comprises a cascade of sub-buffers that accumulate experiences at different timescales, enabling the agent to improve the trade-off between adaptation to new data and retention of old knowledge. We also combine the MTR framework with invariant risk minimization, with the idea of encouraging the agent to learn a policy that is robust across the various environments it encounters over time. The MTR methods are evaluated in three different continual learning settings on two continuous control tasks and, in many cases, show improvement over the baselines.",0
"This paper proposes the use of a multi-timescale replay (MTR) buffer to enhance the ability of RL agents to learn continually in environments that are constantly evolving at unknown timescales. The MTR buffer is composed of several sub-buffers that accumulate experiences at different timescales, allowing the agent to balance the acquisition of new information with the preservation of prior knowledge. Additionally, the MTR framework is combined with invariant risk minimization to encourage the agent to develop a policy that can withstand changes in the environment over time. The effectiveness of the MTR approach is evaluated in three different continual learning scenarios using two continuous control tasks, and in many cases, the results show improvement compared to the baseline methods.",1
this paper has been withdrawn,0
The paper has been retracted.,1
"Reinforcement learning (RL) has been demonstrated to have great potential in many applications of scientific discovery and design. Recent work includes, for example, the design of new structures and compositions of molecules for therapeutic drugs. Much of the existing work related to the application of RL to scientific domains, however, assumes that the available state representation obeys the Markov property. For reasons associated with time, cost, sensor accuracy, and gaps in scientific knowledge, many scientific design and discovery problems do not satisfy the Markov property. Thus, something other than a Markov decision process (MDP) should be used to plan / find the optimal policy. In this paper, we present a physics-inspired semi-Markov RL environment, namely the phase change environment. In addition, we evaluate the performance of value-based RL algorithms for both MDPs and partially observable MDPs (POMDPs) on the proposed environment. Our results demonstrate deep recurrent Q-networks (DRQN) significantly outperform deep Q-networks (DQN), and that DRQNs benefit from training with hindsight experience replay. Implications for the use of semi-Markovian RL and POMDPs for scientific laboratories are also discussed.",0
"The potential of reinforcement learning (RL) has been shown in various scientific discovery and design applications, such as the development of therapeutic drug compositions and structures. However, most current research on the use of RL in scientific domains assumes a state representation that adheres to the Markov property, which is not always the case due to factors like time, cost, sensor accuracy, and gaps in scientific knowledge. This means that a Markov decision process (MDP) may not be the best option for finding the optimal policy. To address this, we introduce a physics-inspired semi-Markov RL environment called the phase change environment, and evaluate the performance of value-based RL algorithms on MDPs and partially observable MDPs (POMDPs) in this environment. Our results indicate that deep recurrent Q-networks (DRQNs) outperform deep Q-networks (DQNs), and that training DRQNs with hindsight experience replay is beneficial. We also discuss the implications of using semi-Markovian RL and POMDPs in scientific laboratories.",1
"Despite success in many real-world tasks (e.g., robotics), reinforcement learning (RL) agents still learn from tabula rasa when facing new and dynamic scenarios. By contrast, humans can offload this burden through textual descriptions. Although recent works have shown the benefits of instructive texts in goal-conditioned RL, few have studied whether descriptive texts help agents to generalize across dynamic environments. To promote research in this direction, we introduce a new platform, BabyAI++, to generate various dynamic environments along with corresponding descriptive texts. Moreover, we benchmark several baselines inherited from the instruction following setting and develop a novel approach towards visually-grounded language learning on our platform. Extensive experiments show strong evidence that using descriptive texts improves the generalization of RL agents across environments with varied dynamics.",0
"Despite achieving success in numerous real-world applications such as robotics, reinforcement learning (RL) agents continue to learn from scratch when confronted with new and ever-changing scenarios. On the other hand, humans can alleviate this burden by relying on written descriptions. Although recent studies have demonstrated the advantages of instructive texts in goal-oriented RL, there have been limited investigations into whether descriptive texts can help agents generalize across dynamic environments. To encourage research in this area, we have developed a new platform called BabyAI++ that generates various dynamic environments along with corresponding descriptive texts. Additionally, we have established a benchmark for several baselines derived from the instruction-following paradigm and have created a novel approach to visually-grounded language learning on our platform. Extensive experiments have yielded compelling evidence that the use of descriptive texts enhances the generalization of RL agents across environments with diverse dynamics.",1
"Having access to a forward model enables the use of planning algorithms such as Monte Carlo Tree Search and Rolling Horizon Evolution. Where a model is unavailable, a natural aim is to learn a model that reflects accurately the dynamics of the environment. In many situations it might not be possible and minimal glitches in the model may lead to poor performance and failure. This paper explores the problem of model misspecification through uncertainty-aware reinforcement learning agents. We propose a bootstrapped multi-headed neural network that learns the distribution of future states and rewards. We experiment with a number of schemes to extract the most likely predictions. Moreover, we also introduce a global error correction filter that applies high-level constraints guided by the context provided through the predictive distribution. We illustrate our approach on Minipacman. The evaluation demonstrates that when dealing with imperfect models, our methods exhibit increased performance and stability, both in terms of model accuracy and in its use within a planning algorithm.",0
"When a forward model is available, planning algorithms like Monte Carlo Tree Search and Rolling Horizon Evolution can be utilized. If a model is not accessible, the objective is to obtain a model that accurately reflects the environment's dynamics. This may not be feasible in some situations, and even minor errors in the model can result in poor performance and failure. This study examines the issue of model misspecification through the use of uncertainty-aware reinforcement learning agents. We propose a multi-headed neural network that learns the distribution of future states and rewards with bootstrapping. We experimented with various schemes to extract the most probable predictions. Furthermore, we present a global error correction filter that applies high-level constraints guided by the context provided by the predictive distribution. We demonstrate our approach on Minipacman. Our techniques demonstrate improved performance and stability when dealing with imperfect models, both in terms of model accuracy and its use in a planning algorithm.",1
"Recently, the bidirectional encoder representations from transformers (BERT) model has attracted much attention in the field of natural language processing, owing to its high performance in language understanding-related tasks. The BERT model learns language representation that can be adapted to various tasks via pre-training using a large corpus in an unsupervised manner. This study proposes the language and action learning using multimodal BERT (lamBERT) model that enables the learning of language and actions by 1) extending the BERT model to multimodal representation and 2) integrating it with reinforcement learning. To verify the proposed model, an experiment is conducted in a grid environment that requires language understanding for the agent to act properly. As a result, the lamBERT model obtained higher rewards in multitask settings and transfer settings when compared to other models, such as the convolutional neural network-based model and the lamBERT model without pre-training.",0
"The BERT model has become a popular approach in natural language processing due to its strong performance in language-related tasks. This is because the model learns language representation that can be adapted to various tasks through pre-training on a large corpus in an unsupervised manner. In this study, the lamBERT model is proposed, which extends the BERT model to multimodal representation and integrates it with reinforcement learning to enable the learning of language and actions. An experiment is conducted in a grid environment to test the model's effectiveness, and the results show that the lamBERT model outperforms other models in multitask and transfer settings. This includes the convolutional neural network-based model and the lamBERT model without pre-training.",1
"Temporal-difference learning (TD), coupled with neural networks, is among the most fundamental building blocks of deep reinforcement learning. However, due to the nonlinearity in value function approximation, such a coupling leads to nonconvexity and even divergence in optimization. As a result, the global convergence of neural TD remains unclear. In this paper, we prove for the first time that neural TD converges at a sublinear rate to the global optimum of the mean-squared projected Bellman error for policy evaluation. In particular, we show how such global convergence is enabled by the overparametrization of neural networks, which also plays a vital role in the empirical success of neural TD. Beyond policy evaluation, we establish the global convergence of neural (soft) Q-learning, which is further connected to that of policy gradient algorithms.",0
"One of the key components of deep reinforcement learning is the combination of temporal-difference learning (TD) and neural networks. However, the nonlinearity in value function approximation means that this coupling can result in nonconvexity and divergence during optimization, making it unclear whether neural TD can achieve global convergence. This paper presents the first proof that neural TD can converge to the global optimum of the mean-squared projected Bellman error for policy evaluation, albeit at a sublinear rate. We attribute this global convergence to the overparametrization of neural networks, which also explains its success in practice. In addition, we establish the global convergence of neural (soft) Q-learning and link it to policy gradient algorithms.",1
"Advances in deep neural networks (DNN) greatly bolster real-time detection of anomalous IoT data. However, IoT devices can hardly afford complex DNN models, and offloading anomaly detection tasks to the cloud incurs long delay. In this paper, we propose and build a demo for an adaptive anomaly detection approach for distributed hierarchical edge computing (HEC) systems to solve this problem, for both univariate and multivariate IoT data. First, we construct multiple anomaly detection DNN models with increasing complexity, and associate each model with a layer in HEC from bottom to top. Then, we design an adaptive scheme to select one of these models on the fly, based on the contextual information extracted from each input data. The model selection is formulated as a contextual bandit problem characterized by a single-step Markov decision process, and is solved using a reinforcement learning policy network. We build an HEC testbed, implement our proposed approach, and evaluate it using real IoT datasets. The demo shows that our proposed approach significantly reduces detection delay (e.g., by 71.4% for univariate dataset) without sacrificing accuracy, as compared to offloading detection tasks to the cloud. We also compare it with other baseline schemes and demonstrate that it achieves the best accuracy-delay tradeoff. Our demo is also available online: https://rebrand.ly/91a71",0
"Real-time detection of anomalous IoT data can be greatly enhanced by advances in deep neural networks (DNN). However, due to their complexity, IoT devices are unable to support these models and offloading tasks to the cloud can result in long delays. To address this issue, we propose an adaptive anomaly detection approach for distributed hierarchical edge computing (HEC) systems. Our approach involves constructing multiple DNN models with varying complexities and associating each model with a layer in the HEC system. We then design an adaptive scheme to select the appropriate model on the fly, based on contextual information extracted from input data. This model selection is formulated as a contextual bandit problem characterized by a single-step Markov decision process and solved using a reinforcement learning policy network. We evaluate our proposed approach using real IoT datasets and demonstrate that it significantly reduces detection delay (e.g., by 71.4% for univariate dataset) without compromising accuracy. Our approach also achieves the best accuracy-delay tradeoff compared to other baseline schemes. A demo of our approach is available online at https://rebrand.ly/91a71.",1
"An online resource scheduling framework is proposed for minimizing the sum of weighted task latency for all the Internet of things (IoT) users, by optimizing offloading decision, transmission power and resource allocation in the large-scale mobile edge computing (MEC) system. Towards this end, a deep reinforcement learning (DRL) based solution is proposed, which includes the following components. Firstly, a related and regularized stacked auto encoder (2r-SAE) with unsupervised learning is applied to perform data compression and representation for high dimensional channel quality information (CQI) data, which can reduce the state space for DRL. Secondly, we present an adaptive simulated annealing based approach (ASA) as the action search method of DRL, in which an adaptive h-mutation is used to guide the search direction and an adaptive iteration is proposed to enhance the search efficiency during the DRL process. Thirdly, a preserved and prioritized experience replay (2p-ER) is introduced to assist the DRL to train the policy network and find the optimal offloading policy. Numerical results are provided to demonstrate that the proposed algorithm can achieve near-optimal performance while significantly decreasing the computational time compared with existing benchmarks.",0
"The proposed solution aims to minimize the total weighted task latency of all users in the Internet of things (IoT) by optimizing the mobile edge computing (MEC) system's offloading decision, resource allocation, and transmission power. A deep reinforcement learning (DRL) based approach is proposed, consisting of three parts. Firstly, a related and regularized stacked auto encoder (2r-SAE) is applied to compress and represent high-dimensional channel quality information (CQI) data. Secondly, an adaptive simulated annealing based approach (ASA) is used as the action search method of DRL, which includes an adaptive h-mutation and an adaptive iteration to enhance the search efficiency. Thirdly, a preserved and prioritized experience replay (2p-ER) is introduced to assist DRL in training the policy network and finding the optimal offloading policy. The proposed algorithm achieves near-optimal performance with significantly reduced computational time compared to existing benchmarks, as shown by numerical results.",1
"Extrapolation in reinforcement learning is the ability to generalize at test time given states that could never have occurred at training time. Here we consider four factors that lead to improved extrapolation in a simple Gridworld environment: (a) avoiding maximum Q-value (or other deterministic methods) for action choice at test time, (b) ego-centric representation of the Gridworld, (c) building rotational and mirror symmetry into the learning mechanism using rotational and mirror invariant convolution (rather than standard translation-invariant convolution), and (d) adding a maximum entropy term to the loss function to encourage equally good actions to be chosen equally often.",0
"The capacity to make inferences beyond the scope of training data is known as extrapolation in reinforcement learning. In this article, we examine four elements that enhance extrapolation in a basic Gridworld setting. These include: (a) refraining from using maximum Q-value or deterministic approaches for selecting actions during testing, (b) adopting an ego-centric representation of the Gridworld, (c) integrating rotational and mirror symmetry into the learning process using rotational and mirror invariant convolution instead of standard translation-invariant convolution, and (d) incorporating a maximum entropy factor into the loss function to promote the selection of equally good actions with equal frequency.",1
"Recent progress in the field of reinforcement learning has been accelerated by virtual learning environments such as video games, where novel algorithms and ideas can be quickly tested in a safe and reproducible manner. We introduce the Google Research Football Environment, a new reinforcement learning environment where agents are trained to play football in an advanced, physics-based 3D simulator. The resulting environment is challenging, easy to use and customize, and it is available under a permissive open-source license. In addition, it provides support for multiplayer and multi-agent experiments. We propose three full-game scenarios of varying difficulty with the Football Benchmarks and report baseline results for three commonly used reinforcement algorithms (IMPALA, PPO, and Ape-X DQN). We also provide a diverse set of simpler scenarios with the Football Academy and showcase several promising research directions.",0
"Virtual learning environments, such as video games, have significantly advanced progress in the field of reinforcement learning by allowing quick and safe testing of novel algorithms and ideas. Our contribution to this field is the Google Research Football Environment, an advanced 3D simulator that trains agents to play football using physics-based mechanics. This environment is customizable and challenging, with support for multiplayer and multi-agent experiments, and is available under a permissive open-source license. We present three varying difficulty levels of full-game scenarios with Football Benchmarks and baseline results for three commonly used reinforcement algorithms (IMPALA, PPO, and Ape-X DQN). Additionally, Football Academy provides simpler scenarios and promising research directions.",1
"Drawing an inspiration from behavioral studies of human decision making, we propose here a more general and flexible parametric framework for reinforcement learning that extends standard Q-learning to a two-stream model for processing positive and negative rewards, and allows to incorporate a wide range of reward-processing biases -- an important component of human decision making which can help us better understand a wide spectrum of multi-agent interactions in complex real-world socioeconomic systems, as well as various neuropsychiatric conditions associated with disruptions in normal reward processing. From the computational perspective, we observe that the proposed Split-QL model and its clinically inspired variants consistently outperform standard Q-Learning and SARSA methods, as well as recently proposed Double Q-Learning approaches, on simulated tasks with particular reward distributions, a real-world dataset capturing human decision-making in gambling tasks, and the Pac-Man game in a lifelong learning setting across different reward stationarities.",0
"We suggest a more versatile and comprehensive parametric framework for reinforcement learning, based on studies of human decision making. Our approach expands standard Q-learning by incorporating a two-stream model for processing both positive and negative rewards, and enables the inclusion of a broad range of reward-processing biases. These biases are critical in understanding multi-agent interactions in complex real-world socioeconomic systems and various neuropsychiatric disorders associated with abnormal reward processing. Our Split-QL model and its clinically inspired variants demonstrate superior performance in comparison to standard Q-learning, SARSA methods, and Double Q-Learning approaches, in simulated tasks with diverse reward distributions, a real-world dataset capturing human decision-making in gambling tasks, and the Pac-Man game in a lifelong learning setting with different reward stationarities.",1
"In the contemporary big data realm, Deep Neural Networks (DNNs) are evolving towards more complex architectures to achieve higher inference accuracy. Model compression techniques can be leveraged to efficiently deploy such compute-intensive architectures on resource-limited mobile devices. Such methods comprise various hyper-parameters that require per-layer customization to ensure high accuracy. Choosing such hyper-parameters is cumbersome as the pertinent search space grows exponentially with model layers. This paper introduces GeneCAI, a novel optimization method that automatically learns how to tune per-layer compression hyper-parameters. We devise a bijective translation scheme that encodes compressed DNNs to the genotype space. The optimality of each genotype is measured using a multi-objective score based on accuracy and number of floating point operations. We develop customized genetic operations to iteratively evolve the non-dominated solutions towards the optimal Pareto front, thus, capturing the optimal trade-off between model accuracy and complexity. GeneCAI optimization method is highly scalable and can achieve a near-linear performance boost on distributed multi-GPU platforms. Our extensive evaluations demonstrate that GeneCAI outperforms existing rule-based and reinforcement learning methods in DNN compression by finding models that lie on a better accuracy-complexity Pareto curve.",0
"DNNs in the big data industry are becoming more complex to attain higher inference accuracy. However, these resource-intensive designs can be challenging to deploy on mobile devices with limited resources. Model compression techniques can offer a solution but require customization of various hyper-parameters for each layer, which can be tedious due to the exponential growth in search space. This paper introduces GeneCAI, a novel optimization approach that automatically tunes per-layer compression hyper-parameters with a multi-objective score based on accuracy and floating point operations. The method encodes compressed DNNs to the genotype space and uses customized genetic operations to evolve non-dominated solutions towards the optimal Pareto front, striking a balance between model accuracy and complexity. GeneCAI is highly scalable, achieving a near-linear performance boost on distributed multi-GPU platforms. Evaluations show that GeneCAI outperforms existing rule-based and reinforcement learning methods by finding models that lie on a better accuracy-complexity Pareto curve.",1
"We report a previously unidentified issue with model-free, value-based approaches to multiobjective reinforcement learning in the context of environments with stochastic state transitions. An example multiobjective Markov Decision Process (MOMDP) is used to demonstrate that under such conditions these approaches may be unable to discover the policy which maximises the Scalarised Expected Return, and in fact may converge to a Pareto-dominated solution. We discuss several alternative methods which may be more suitable for maximising SER in MOMDPs with stochastic transitions.",0
"In the context of environments with stochastic state transitions, we have identified a new problem with model-free, value-based approaches to multiobjective reinforcement learning. Our demonstration using a multiobjective Markov Decision Process (MOMDP) shows that these approaches may fail to uncover the policy that maximizes the Scalarized Expected Return, and instead may converge to a Pareto-dominated solution. We present several alternative methods that could be better suited for maximizing SER in MOMDPs with stochastic transitions.",1
"We propose and address a novel few-shot RL problem, where a task is characterized by a subtask graph which describes a set of subtasks and their dependencies that are unknown to the agent. The agent needs to quickly adapt to the task over few episodes during adaptation phase to maximize the return in the test phase. Instead of directly learning a meta-policy, we develop a Meta-learner with Subtask Graph Inference(MSGI), which infers the latent parameter of the task by interacting with the environment and maximizes the return given the latent parameter. To facilitate learning, we adopt an intrinsic reward inspired by upper confidence bound (UCB) that encourages efficient exploration. Our experiment results on two grid-world domains and StarCraft II environments show that the proposed method is able to accurately infer the latent task parameter, and to adapt more efficiently than existing meta RL and hierarchical RL methods.",0
"Our proposal focuses on a new few-shot RL challenge in which a task is defined by a subtask graph, representing unknown dependencies between subtasks. The agent must quickly adapt to the task to maximize the test phase's return across few episodes. Rather than directly learning a meta-policy, we introduce a Meta-learner with Subtask Graph Inference (MSGI) that deduces the task's latent parameter by interacting with the environment and maximizing the return. To facilitate learning, we adopt an intrinsic reward inspired by upper confidence bound (UCB) that encourages efficient exploration. Our experiments on two grid-world domains and StarCraft II environments demonstrate that the proposed method accurately infers the task's latent parameter and adapts more efficiently than current meta RL and hierarchical RL approaches.",1
"Fashion is a complex social phenomenon. People follow fashion styles from demonstrations by experts or fashion icons. However, for machine agent, learning to imitate fashion experts from demonstrations can be challenging, especially for complex styles in environments with high-dimensional, multimodal observations. Most existing research regarding fashion outfit composition utilizes supervised learning methods to mimic the behaviors of style icons. These methods suffer from distribution shift: because the agent greedily imitates some given outfit demonstrations, it can drift away from one style to another styles given subtle differences. In this work, we propose an adversarial inverse reinforcement learning formulation to recover reward functions based on hierarchical multimodal representation (HM-AIRL) during the imitation process. The hierarchical joint representation can more comprehensively model the expert composited outfit demonstrations to recover the reward function. We demonstrate that the proposed HM-AIRL model is able to recover reward functions that are robust to changes in multimodal observations, enabling us to learn policies under significant variation between different styles.",0
"The concept of fashion is a complicated social phenomenon that people tend to emulate by observing professionals or renowned fashion icons. However, for machine agents, imitating the styles of fashion experts through observation can be challenging, particularly in complicated settings with high-dimensional, multimodal observations. Previous research on fashion outfit composition has employed supervised learning techniques to imitate the conduct of style icons. Nevertheless, these methods are prone to distribution shift, which means that the agent may move away from a particular style towards other styles due to small differences. In this study, we present an adversarial inverse reinforcement learning approach that utilizes hierarchical multimodal representation (HM-AIRL) to recover reward functions during the imitation process. By utilizing a hierarchical joint representation, we can more comprehensively model the expertly composed outfits to recover the reward function. Our findings show that the proposed HM-AIRL model can recover reward functions that are resistant to changes in multimodal observations, allowing us to learn policies that are adaptable to different styles.",1
"We consider finite and infinite horizon dynamic programming problems, where the control at each stage consists of several distinct decisions, each one made by one of several agents. We introduce an approach, whereby at every stage, each agent's decision is made by executing a local rollout algorithm that uses a base policy, together with some coordinating information from the other agents. The amount of local computation required at every stage by each agent is independent of the number of agents, while the amount of total computation (over all agents) grows linearly with the number of agents. By contrast, with the standard rollout algorithm, the amount of total computation grows exponentially with the number of agents. Despite the drastic reduction in required computation, we show that our algorithm has the fundamental cost improvement property of rollout: an improved performance relative to the base policy. We also discuss possibilities to improve further the method's computational efficiency through limited agent coordination and parallelization of the agents' computations. Finally, we explore related approximate policy iteration algorithms for infinite horizon problems, and we prove that the cost improvement property steers the algorithm towards convergence to an agent-by-agent optimal policy.",0
"Our focus is on dynamic programming problems with finite and infinite horizons, where multiple agents make distinct decisions at each stage. To tackle this, we propose an approach that involves executing a local rollout algorithm at every stage, which uses a base policy and coordinating information from the other agents. Notably, the amount of local computation required by each agent remains independent of the number of agents, while the total computation required grows linearly with the number of agents, unlike the standard rollout algorithm where the total computation grows exponentially with the number of agents. Despite requiring significantly less computation, our algorithm outperforms the base policy. We also explore opportunities to improve computational efficiency through limited agent coordination and parallelization. Additionally, we investigate approximate policy iteration algorithms for infinite horizon problems and prove that the cost improvement property guides the algorithm towards convergence to an agent-by-agent optimal policy.",1
"The Internet of Things (IoT) extends the Internet connectivity into billions of IoT devices around the world, where the IoT devices collect and share information to reflect status of the physical world. The Autonomous Control System (ACS), on the other hand, performs control functions on the physical systems without external intervention over an extended period of time. The integration of IoT and ACS results in a new concept - autonomous IoT (AIoT). The sensors collect information on the system status, based on which the intelligent agents in the IoT devices as well as the Edge/Fog/Cloud servers make control decisions for the actuators to react. In order to achieve autonomy, a promising method is for the intelligent agents to leverage the techniques in the field of artificial intelligence, especially reinforcement learning (RL) and deep reinforcement learning (DRL) for decision making. In this paper, we first provide a tutorial of DRL, and then propose a general model for the applications of RL/DRL in AIoT. Next, a comprehensive survey of the state-of-art research on DRL for AIoT is presented, where the existing works are classified and summarized under the umbrella of the proposed general DRL model. Finally, the challenges and open issues for future research are identified.",0
"The IoT connects billions of devices across the globe to gather and share information on the physical world. Meanwhile, the ACS controls physical systems independently for an extended period. Combining these two concepts creates the AIoT, where sensors collect data on system status and intelligent agents make decisions for the actuators to react. To achieve autonomy, intelligent agents utilize artificial intelligence techniques like reinforcement learning (RL) and deep reinforcement learning (DRL) for decision making. This paper offers a DRL tutorial and proposes a general model for RL/DRL applications in AIoT. It also includes a comprehensive survey of existing DRL research for AIoT, categorized under the proposed general DRL model, and identifies challenges and open issues for future research.",1
"With the growing attention on learning-to-learn new tasks using only a few examples, meta-learning has been widely used in numerous problems such as few-shot classification, reinforcement learning, and domain generalization. However, meta-learning models are prone to overfitting when there are no sufficient training tasks for the meta-learners to generalize. Although existing approaches such as Dropout are widely used to address the overfitting problem, these methods are typically designed for regularizing models of a single task in supervised training. In this paper, we introduce a simple yet effective method to alleviate the risk of overfitting for gradient-based meta-learning. Specifically, during the gradient-based adaptation stage, we randomly drop the gradient in the inner-loop optimization of each parameter in deep neural networks, such that the augmented gradients improve generalization to new tasks. We present a general form of the proposed gradient dropout regularization and show that this term can be sampled from either the Bernoulli or Gaussian distribution. To validate the proposed method, we conduct extensive experiments and analysis on numerous computer vision tasks, demonstrating that the gradient dropout regularization mitigates the overfitting problem and improves the performance upon various gradient-based meta-learning frameworks.",0
"Meta-learning has become increasingly popular in various problems such as reinforcement learning, few-shot classification, and domain generalization due to the focus on learning-to-learn new tasks with only a few examples. However, when meta-learners lack sufficient training tasks to generalize, they are susceptible to overfitting. Existing approaches such as Dropout are designed for single-task supervised training models, and may not be effective in addressing overfitting in meta-learning models. This paper proposes a method to mitigate overfitting in gradient-based meta-learning by randomly dropping gradients during the adaptation stage. This improves generalization to new tasks and can be sampled from either the Bernoulli or Gaussian distribution. Extensive experiments and analysis on computer vision tasks demonstrate that this method improves performance and mitigates overfitting in various gradient-based meta-learning frameworks.",1
"In distributed reinforcement learning, it is common to exchange the experience memory of each agent and thereby collectively train their local models. The experience memory, however, contains all the preceding state observations and their corresponding policies of the host agent, which may violate the privacy of the agent. To avoid this problem, in this work, we propose a privacy-preserving distributed reinforcement learning (RL) framework, termed federated reinforcement distillation (FRD). The key idea is to exchange a proxy experience memory comprising a pre-arranged set of states and time-averaged policies, thereby preserving the privacy of actual experiences. Based on an advantage actor-critic RL architecture, we numerically evaluate the effectiveness of FRD and investigate how the performance of FRD is affected by the proxy memory structure and different memory exchanging rules.",0
"The practice of exchanging experience memory among agents is widespread in distributed reinforcement learning, as it allows for collective training of local models. However, this poses a potential privacy violation as the experience memory contains the previous observations and policies of the host agent. To address this issue, we present the federated reinforcement distillation (FRD) framework, which ensures privacy preservation. We propose exchanging a proxy experience memory that consists of a predetermined set of states and time-averaged policies, instead of actual experiences. We evaluate the effectiveness of FRD using an advantage actor-critic RL architecture and explore how the proxy memory structure and memory exchanging rules affect its performance.",1
"Demonstration is an appealing way for humans to provide assistance to reinforcement-learning agents. Most approaches in this area view demonstrations primarily as sources of behavioral bias. But in sparse-reward tasks, humans seem to treat demonstrations more as sources of causal knowledge. This paper proposes a framework for agents that benefit from demonstration in this human-inspired way. In this framework, agents develop causal models through observation, and reason from this knowledge to decompose tasks for effective reinforcement learning. Experimental results show that a basic implementation of Reasoning from Demonstration (RfD) is effective in a range of sparse-reward tasks.",0
"The act of demonstrating is a desirable means for humans to aid reinforcement-learning agents. While many methods consider demonstrations as primarily influencing behavior, sparse-reward tasks suggest that humans tend to use demonstrations as a source of causal understanding. To emulate this, the paper presents a system where agents create causal models through observation, and use this knowledge to break down tasks for efficient reinforcement learning. The experimental findings demonstrate that a rudimentary version of Reasoning from Demonstration (RfD) is useful in various sparse-reward tasks.",1
"While deep neural networks (DNNs) and Gaussian Processes (GPs) are both popularly utilized to solve problems in reinforcement learning, both approaches feature undesirable drawbacks for challenging problems. DNNs learn complex nonlinear embeddings, but do not naturally quantify uncertainty and are often data-inefficient to train. GPs infer posterior distributions over functions, but popular kernels exhibit limited expressivity on complex and high-dimensional data. Fortunately, recently discovered conjugate and neural tangent kernel functions encode the behavior of overparameterized neural networks in the kernel domain. We demonstrate that these kernels can be efficiently applied to regression and reinforcement learning problems by analyzing a baseline case study. We apply GPs with neural network dual kernels to solve reinforcement learning tasks for the first time. We demonstrate, using the well-understood mountain-car problem, that GPs empowered with dual kernels perform at least as well as those using the conventional radial basis function kernel. We conjecture that by inheriting the probabilistic rigor of GPs and the powerful embedding properties of DNNs, GPs using NN dual kernels will empower future reinforcement learning models on difficult domains.",0
"Although both deep neural networks (DNNs) and Gaussian Processes (GPs) are commonly employed for solving reinforcement learning problems, each approach has its own limitations for tackling challenging problems. DNNs excel at learning complicated nonlinear embeddings but do not quantify uncertainty naturally and can require a lot of data to train. On the other hand, GPs can infer posterior distributions over functions, but their popular kernels are not expressive enough for analyzing high-dimensional and complex data. Nonetheless, we have recently discovered conjugate and neural tangent kernel functions that encode the behavior of overparameterized neural networks in the kernel domain. By analyzing a baseline case study, we have demonstrated that these kernels can be efficiently applied to regression and reinforcement learning problems. We have successfully employed GPs with neural network dual kernels to solve reinforcement learning tasks for the first time. Through our analysis of the well-known mountain-car problem, we have proven that GPs empowered with dual kernels perform at least as well as those using the conventional radial basis function kernel. We anticipate that GPs using NN dual kernels, which inherit the probabilistic rigor of GPs and the powerful embedding properties of DNNs, will enhance future reinforcement learning models in challenging domains.",1
"Convolutional Neural Networks (CNNs) have a major impact on our society because of the numerous services they provide. On the other hand, they require considerable computing power. To satisfy these requirements, it is possible to use graphic processing units (GPUs). However, high power consumption and limited external IOs constrain their usability and suitability in industrial and mission-critical scenarios. Recently, the number of researches that utilize FPGAs to implement CNNs are increasing rapidly. This is due to the lower power consumption and easy reconfigurability offered by these platforms. Because of the research efforts put into topics such as architecture, synthesis and optimization, some new challenges are arising to integrate such hardware solutions to high-level machine learning software libraries. This paper introduces an integrated framework (CNN2Gate) that supports compilation of a CNN model for an FPGA target. CNN2Gate exploits the OpenCL synthesis workflow for FPGAs offered by commercial vendors. CNN2Gate is capable of parsing CNN models from several popular high-level machine learning libraries such as Keras, Pytorch, Caffe2 etc. CNN2Gate extracts computation flow of layers, in addition to weights and biases and applies a ""given"" fixed-point quantization. Furthermore, it writes this information in the proper format for OpenCL synthesis tools that are then used to build and run the project on FPGA. CNN2Gate performs design-space exploration using a reinforcement learning agent and fits the design on different FPGAs with limited logic resources automatically. This paper reports results of automatic synthesis and design-space exploration of AlexNet and VGG-16 on various Intel FPGA platforms. CNN2Gate achieves a latency of 205 ms for VGG-16 and 18 ms for AlexNet on the FPGA.",0
"Due to their extensive range of services, Convolutional Neural Networks (CNNs) have had a significant impact on society, although their requirements for substantial computing power can be challenging to meet. While graphic processing units (GPUs) can satisfy these needs, their high power consumption and limited external IOs restrict their suitability for industrial and mission-critical scenarios. Consequently, researchers have increasingly turned to FPGAs due to their lower power consumption and easy reconfigurability, resulting in new challenges for integrating hardware solutions with high-level machine learning software libraries. This paper introduces an integrated framework called CNN2Gate that supports compiling a CNN model for an FPGA target, utilizing OpenCL synthesis workflow and capable of parsing CNN models from popular machine learning libraries. Additionally, CNN2Gate performs design-space exploration using a reinforcement learning agent and fits the design on different FPGAs automatically. The results of automatic synthesis and design-space exploration of AlexNet and VGG-16 on various Intel FPGA platforms show that CNN2Gate achieves a latency of 205 ms for VGG-16 and 18 ms for AlexNet on the FPGA.",1
"This paper considers policy search in continuous state-action reinforcement learning problems. Typically, one computes search directions using a classic expression for the policy gradient called the Policy Gradient Theorem, which decomposes the gradient of the value function into two factors: the score function and the Q-function. This paper presents four results:(i) an alternative policy gradient theorem using weak (measure-valued) derivatives instead of score-function is established; (ii) the stochastic gradient estimates thus derived are shown to be unbiased and to yield algorithms that converge almost surely to stationary points of the non-convex value function of the reinforcement learning problem; (iii) the sample complexity of the algorithm is derived and is shown to be $O(1/\sqrt(k))$; (iv) finally, the expected variance of the gradient estimates obtained using weak derivatives is shown to be lower than those obtained using the popular score-function approach. Experiments on OpenAI gym pendulum environment show superior performance of the proposed algorithm.",0
"In this paper, the focus is on policy search within continuous state-action reinforcement learning problems. The traditional method involves using the Policy Gradient Theorem to calculate search directions, which breaks down the value function's gradient into two components: the score function and the Q-function. This paper introduces four significant findings. Firstly, it establishes an alternative policy gradient theorem that utilizes weak derivatives instead of the score function. Secondly, it demonstrates that the stochastic gradient estimates produced by this method are unbiased and lead to algorithms that converge almost certainly to stationary points of the reinforcement learning problem's non-convex value function. Thirdly, the algorithm's sample complexity is calculated to be $O(1/\sqrt(k))$. Lastly, it is shown that the expected gradient estimate variance obtained through the weak derivative approach is lower than that of the commonly used score-function method. The proposed algorithm outperforms previous experiments on the OpenAI gym pendulum environment.",1
"We undertake a precise study of the asymptotic and non-asymptotic properties of stochastic approximation procedures with Polyak-Ruppert averaging for solving a linear system $\bar{A} \theta = \bar{b}$. When the matrix $\bar{A}$ is Hurwitz, we prove a central limit theorem (CLT) for the averaged iterates with fixed step size and number of iterations going to infinity. The CLT characterizes the exact asymptotic covariance matrix, which is the sum of the classical Polyak-Ruppert covariance and a correction term that scales with the step size. Under assumptions on the tail of the noise distribution, we prove a non-asymptotic concentration inequality whose main term matches the covariance in CLT in any direction, up to universal constants. When the matrix $\bar{A}$ is not Hurwitz but only has non-negative real parts in its eigenvalues, we prove that the averaged LSA procedure actually achieves an $O(1/T)$ rate in mean-squared error. Our results provide a more refined understanding of linear stochastic approximation in both the asymptotic and non-asymptotic settings. We also show various applications of the main results, including the study of momentum-based stochastic gradient methods as well as temporal difference algorithms in reinforcement learning.",0
"Our study focuses on the properties of stochastic approximation procedures with Polyak-Ruppert averaging for solving linear systems of the form $\bar{A} \theta = \bar{b}$, including both asymptotic and non-asymptotic analyses. Specifically, when the matrix $\bar{A}$ is Hurwitz, we establish a central limit theorem (CLT) for the averaged iterates with fixed step size and an increasing number of iterations. This CLT characterizes the exact asymptotic covariance matrix, which comprises the classical Polyak-Ruppert covariance and a correction term that scales with the step size. Moreover, we present a non-asymptotic concentration inequality under assumptions on the tail of the noise distribution, whose main term matches the covariance in CLT in any direction, up to universal constants. In the case where $\bar{A}$ is not Hurwitz but only has non-negative real parts in its eigenvalues, we prove that the averaged LSA procedure attains an $O(1/T)$ rate in mean-squared error. Our findings enhance our comprehension of linear stochastic approximation in both asymptotic and non-asymptotic contexts and have practical applications, such as in the study of momentum-based stochastic gradient methods and temporal difference algorithms in reinforcement learning.",1
"Location is key to spatialize internet-of-things (IoT) data. However, it is challenging to use low-cost IoT devices for robust unsupervised localization (i.e., localization without training data that have known location labels). Thus, this paper proposes a deep reinforcement learning (DRL) based unsupervised wireless-localization method. The main contributions are as follows. (1) This paper proposes an approach to model a continuous wireless-localization process as a Markov decision process (MDP) and process it within a DRL framework. (2) To alleviate the challenge of obtaining rewards when using unlabeled data (e.g., daily-life crowdsourced data), this paper presents a reward-setting mechanism, which extracts robust landmark data from unlabeled wireless received signal strengths (RSS). (3) To ease requirements for model re-training when using DRL for localization, this paper uses RSS measurements together with agent location to construct DRL inputs. The proposed method was tested by using field testing data from multiple Bluetooth 5 smart ear tags in a pasture. Meanwhile, the experimental verification process reflected the advantages and challenges for using DRL in wireless localization.",0
"The significance of location in the spatialization of internet-of-things (IoT) data cannot be overstated. However, using low-cost IoT devices for unsupervised localization has proven to be a challenging task. This refers to the difficulty of localization without the aid of training data with known location labels. To address this issue, this paper suggests an unsupervised wireless-localization method based on deep reinforcement learning (DRL). The paper's main contributions include modeling a continuous wireless-localization process as a Markov decision process (MDP) and processing it within a DRL framework. Additionally, the paper presents a reward-setting mechanism that extracts robust landmark data from unlabeled wireless received signal strengths (RSS) to tackle the challenge of obtaining rewards when using unlabeled data. Furthermore, RSS measurements together with agent location are employed to construct DRL inputs, thereby easing the model re-training requirements when using DRL for localization. The proposed method was tested using field testing data from multiple Bluetooth 5 smart ear tags in a pasture, and the experimental verification process highlighted the advantages and challenges of using DRL in wireless localization.",1
"The ability to learn in dynamic, nonstationary environments without forgetting previous knowledge, also known as Continual Learning (CL), is a key enabler for scalable and trustworthy deployments of adaptive solutions. While the importance of continual learning is largely acknowledged in machine vision and reinforcement learning problems, this is mostly under-documented for sequence processing tasks. This work proposes a Recurrent Neural Network (RNN) model for CL that is able to deal with concept drift in input distribution without forgetting previously acquired knowledge. We also implement and test a popular CL approach, Elastic Weight Consolidation (EWC), on top of two different types of RNNs. Finally, we compare the performances of our enhanced architecture against EWC and RNNs on a set of standard CL benchmarks, adapted to the sequential data processing scenario. Results show the superior performance of our architecture and highlight the need for special solutions designed to address CL in RNNs.",0
"Continual Learning (CL), which refers to the ability to learn in constantly changing environments without losing previously acquired knowledge, is crucial for the successful deployment of adaptable solutions that can be trusted at scale. While the importance of CL is widely recognized in machine vision and reinforcement learning, it has been largely overlooked in the context of sequence processing tasks. To address this gap, we propose a Recurrent Neural Network (RNN) model that can effectively handle concept drift in input distribution while retaining previous knowledge. Additionally, we evaluate the performance of Elastic Weight Consolidation (EWC), a popular CL method, on two types of RNNs. Our enhanced architecture outperforms both EWC and RNNs on a range of standard CL benchmarks adapted for sequential data processing. These results underscore the need for tailored solutions that specifically address CL in RNNs.",1
"In the past few years supervised and adversarial learning have been widely adopted in various complex computer vision tasks. It seems natural to wonder whether another branch of artificial intelligence, commonly known as Reinforcement Learning (RL) can benefit such complex vision tasks. In this study, we explore the plausible usage of RL in super resolution of remote sensing imagery. Guided by recent advances in super resolution, we propose a theoretical framework that leverages the benefits of supervised and reinforcement learning. We argue that a straightforward implementation of RL is not adequate to address ill-posed super resolution as the action variables are not fully known. To tackle this issue, we propose to parameterize action variables by matrices, and train our policy network using Monte-Carlo sampling. We study the implications of parametric action space in a model-free environment from theoretical and empirical perspective. Furthermore, we analyze the quantitative and qualitative results on both remote sensing and non-remote sensing datasets. Based on our experiments, we report considerable improvement over state-of-the-art methods by encapsulating supervised models in a reinforcement learning framework.",0
"Over the past few years, computer vision tasks have widely adopted supervised and adversarial learning. It is natural to question whether Reinforcement Learning (RL), another branch of artificial intelligence, can also benefit complex vision tasks. This study explores the possible use of RL in super resolution of remote sensing imagery. Building on recent advances in super resolution, we propose a theoretical framework that combines the advantages of supervised and reinforcement learning. However, a direct implementation of RL is inadequate for addressing ill-posed super resolution due to incomplete action variable knowledge. To address this, we suggest parameterizing action variables using matrices and training our policy network with Monte-Carlo sampling. The implications of a parametric action space in a model-free environment are explored both theoretically and empirically. Additionally, we analyze the quantitative and qualitative results on remote sensing and non-remote sensing datasets. Our experiments show significant improvement over state-of-the-art methods by incorporating supervised models into a reinforcement learning framework.",1
"We present for the first time an asymptotic convergence analysis of two time-scale stochastic approximation driven by ""controlled"" Markov noise. In particular, the faster and slower recursions have non-additive controlled Markov noise components in addition to martingale difference noise. We analyze the asymptotic behavior of our framework by relating it to limiting differential inclusions in both time scales that are defined in terms of the ergodic occupation measures associated with the controlled Markov processes. Using a special case of our results, we present a solution to the off-policy convergence problem for temporal-difference learning with linear function approximation. We compile several aspects of the dynamics of stochastic approximation algorithms with Markov iterate-dependent noise when the iterates are not known to be stable beforehand. We achieve the same by extending the lock-in probability (i.e. the probability of convergence to a specific attractor of the limiting o.d.e. given that the iterates are in its domain of attraction after a sufficiently large number of iterations (say) n_0) framework to such recursions. We use these results to prove almost sure convergence of the iterates to the specified attractor when the iterates satisfy an ""asymptotic tightness"" condition. This, in turn, is shown to be useful in analyzing the tracking ability of general ""adaptive"" algorithms. Finally, we obtain the first informative error bounds on function approximation for the policy evaluation algorithm proposed by Basu et al. when the aim is to find the risk-sensitive cost represented using exponential utility. We show that this happens due to the absence of difference term in the earlier bound which is always present in all our bounds when the state space is large.",0
"This article presents an analysis of the asymptotic convergence of two time-scale stochastic approximation algorithms driven by controlled Markov noise. The faster and slower recursions feature non-additive controlled Markov noise components and martingale difference noise. The analysis is based on limiting differential inclusions in both time scales, defined in terms of ergodic occupation measures associated with the controlled Markov processes. The article also provides a solution to the off-policy convergence problem for temporal-difference learning with linear function approximation. Additionally, the article extends the lock-in probability framework to analyze stochastic approximation algorithms with Markov iterate-dependent noise when the iterates are not known to be stable beforehand, and proves almost sure convergence of the iterates to the specified attractor under an asymptotic tightness condition. Finally, informative error bounds on function approximation for the policy evaluation algorithm proposed by Basu et al. are obtained, with the absence of the difference term in earlier bounds being attributed to a large state space.",1
Recent developments in Transformers have opened new interesting areas of research in partially observable reinforcement learning tasks. Results from late 2019 showed that Transformers are able to outperform LSTMs on both memory intense and reactive tasks. In this work we first partially replicate the results shown in Stabilizing Transformers in RL on both reactive and memory based environments. We then show performance improvement coupled with reduced computation when adding adaptive attention span to this Stable Transformer on a challenging DMLab30 environment. The code for all our experiments and models is available at https://github.com/jerrodparker20/adaptive-transformers-in-rl.,0
New research opportunities in partially observable reinforcement learning tasks have emerged due to recent advancements in Transformers. Late 2019 findings revealed that Transformers surpassed LSTMs in both memory-intensive and reactive tasks. This study replicates the Stabilizing Transformers in RL results for reactive and memory-based environments and demonstrates how adding adaptive attention span to the Stable Transformer can improve performance while reducing computation on a difficult DMLab30 environment. All experimental and model codes are accessible at https://github.com/jerrodparker20/adaptive-transformers-in-rl.,1
"We present Catalyst.RL, an open-source PyTorch framework for reproducible and sample efficient reinforcement learning (RL) research. Main features of Catalyst.RL include large-scale asynchronous distributed training, efficient implementations of various RL algorithms and auxiliary tricks, such as n-step returns, value distributions, hyperbolic reinforcement learning, etc. To demonstrate the effectiveness of Catalyst.RL, we applied it to a physics-based reinforcement learning challenge ""NeurIPS 2019: Learn to Move -- Walk Around"" with the objective to build a locomotion controller for a human musculoskeletal model. The environment is computationally expensive, has a high-dimensional continuous action space and is stochastic. Our team took the 2nd place, capitalizing on the ability of Catalyst.RL to train high-quality and sample-efficient RL agents in only a few hours of training time. The implementation along with experiments is open-sourced so results can be reproduced and novel ideas tried out.",0
"Catalyst.RL is a PyTorch framework that is open-source and designed to enable reproducible and sample-efficient reinforcement learning (RL) research. It boasts a range of features, including large-scale asynchronous distributed training, effective implementations of various RL algorithms, and supplementary techniques such as value distributions, hyperbolic reinforcement learning, and n-step returns. In an effort to demonstrate the efficacy of Catalyst.RL, we utilized it to tackle the ""NeurIPS 2019: Learn to Move -- Walk Around"" physics-based reinforcement learning challenge. The environment is computationally intensive, stochastic, and has a high-dimensional continuous action space. Despite these challenges, our team managed to secure 2nd place, leveraging the ability of Catalyst.RL to train high-quality and sample-efficient RL agents in a few hours of training time. The implementation and experiments are open-sourced, allowing others to reproduce the results and experiment with new ideas.",1
"Applying reinforcement learning to robotic systems poses a number of challenging problems. A key requirement is the ability to handle continuous state and action spaces while remaining within a limited time and resource budget. Additionally, for safe operation, the system must make robust decisions under hard constraints. To address these challenges, we propose a model based approach that combines Gaussian Process regression and Receding Horizon Control. Using sparse spectrum Gaussian Processes, we extend previous work by updating the dynamics model incrementally from a stream of sensory data. This results in an agent that can learn and plan in real-time under non-linear constraints. We test our approach on a cart pole swing-up environment and demonstrate the benefits of online learning on an autonomous racing task. The environment's dynamics are learned from limited training data and can be reused in new task instances without retraining.",0
"The application of reinforcement learning in robotics is a complex task that presents many difficulties. A crucial aspect to consider is the ability to handle continuous state and action spaces within a restricted time and resource budget while making robust decisions to ensure safe operation. To overcome these problems, we propose a novel model-based approach that integrates Gaussian Process regression with Receding Horizon Control. This innovative approach utilizes a sparse spectrum Gaussian Process to update the dynamics model incrementally using a stream of sensory data. This results in an agent that can perform real-time learning and planning while adhering to non-linear constraints. We evaluate our approach on a cart pole swing-up scenario and demonstrate the advantages of online learning in an autonomous racing task. The model's dynamics can be learned from limited training data and can be applied to new task instances without requiring retraining.",1
"The demand for more transparency of decision-making processes of deep reinforcement learning agents is greater than ever, due to their increased use in safety critical and ethically challenging domains such as autonomous driving. In this empirical study, we address this lack of transparency following an idea that is inspired by research in the field of neuroscience. We characterize the learned representations of an agent's policy network through its activation space and perform partial network ablations to compare the representations of the healthy and the intentionally damaged networks. We show that the healthy agent's behavior is characterized by a distinct correlation pattern between the network's layer activation and the performed actions during an episode and that network ablations, which cause a strong change of this pattern, lead to the agent failing its trained control task. Furthermore, the learned representation of the healthy agent is characterized by a distinct pattern in its activation space reflecting its different behavioral stages during an episode, which again, when distorted by network ablations, leads to the agent failing its trained control task. Concludingly, we argue in favor of a new perspective on artificial neural networks as objects of empirical investigations, just as biological neural systems in neuroscientific studies, paving the way towards a new standard of scientific falsifiability with respect to research on transparency and interpretability of artificial neural networks.",0
"Due to the increased usage of deep reinforcement learning agents in safety-critical and ethically challenging domains like autonomous driving, there is a higher demand for transparency in decision-making processes. To address this lack of transparency, we conducted an empirical study inspired by neuroscience research. Our study characterizes the learned representations of an agent's policy network through its activation space and performs partial network ablations to compare the healthy and intentionally damaged networks' representations. Our results show that the healthy agent's behavior is characterized by a particular correlation pattern between the network's layer activation and the performed actions during an episode. Network ablations causing a significant change in this pattern lead to the agent failing its trained control task. Additionally, the healthy agent's learned representation is characterized by a particular pattern in its activation space reflecting its different behavioral stages during an episode. Distortion of this pattern by network ablations also leads to the agent failing its trained control task. With the new perspective of artificial neural networks as objects of empirical investigations, similar to biological neural systems in neuroscientific studies, we argue in favor of a new scientific falsifiability standard for research on transparency and interpretability of artificial neural networks.",1
"By moving a depth sensor around a room, we compute a 3D CAD model of the environment, capturing the room shape and contents such as chairs, desks, sofas, and tables. Rather than reconstructing geometry, we match, place, and align each object in the scene to thousands of CAD models of objects. In addition to the fully automatic system, the key technical contribution is a novel approach for aligning CAD models to 3D scans, based on deep reinforcement learning. This approach, which we call Learning-based ICP, outperforms prior ICP methods in the literature, by learning the best points to match and conditioning on object viewpoint. LICP learns to align using only synthetic data and does not require ground truth annotation of object pose or keypoint pair matching in real scene scans. While LICP is trained on synthetic data and without 3D real scene annotations, it outperforms both learned local deep feature matching and geometric based alignment methods in real scenes. The proposed method is evaluated on real scenes datasets of SceneNN and ScanNet as well as synthetic scenes of SUNCG. High quality results are demonstrated on a range of real world scenes, with robustness to clutter, viewpoint, and occlusion.",0
"A 3D CAD model of a room and its furniture can be generated by moving a depth sensor around the area. Instead of reconstructing the geometry of each object, we utilize thousands of pre-existing CAD models for object matching, placement, and alignment. Our innovative approach, called Learning-based ICP, uses deep reinforcement learning to align CAD models to 3D scans. Unlike previous ICP methods, LICP learns the optimal points to match and considers the object's viewpoint. LICP is trained on synthetic data without the need for ground truth object pose annotations or keypoint pair matching in real scene scans. Despite being trained on synthetic data and without 3D real scene annotations, LICP outperforms both learned local deep feature matching and geometric based alignment methods in real scenes. We evaluate our method using real and synthetic scene datasets (SceneNN, ScanNet, and SUNCG) and demonstrate high-quality results in diverse real-world scenes, with robustness to clutter, viewpoint, and occlusion.",1
"In this paper we introduce the first reinforcement learning (RL) based robotic navigation method which utilizes ultrasound (US) images as an input. Our approach combines state-of-the-art RL techniques, specifically deep Q-networks (DQN) with memory buffers and a binary classifier for deciding when to terminate the task.   Our method is trained and evaluated on an in-house collected data-set of 34 volunteers and when compared to pure RL and supervised learning (SL) techniques, it performs substantially better, which highlights the suitability of RL navigation for US-guided procedures. When testing our proposed model, we obtained a 82.91% chance of navigating correctly to the sacrum from 165 different starting positions on 5 different unseen simulated environments.",0
"The first RL-based robotic navigation method using ultrasound (US) images as input is introduced in this paper. Our approach combines deep Q-networks (DQN) with memory buffers and a binary classifier to determine when to end the task. The method is trained and tested on a data-set of 34 volunteers and outperforms pure RL and supervised learning (SL) techniques, demonstrating the potential of RL navigation for US-guided procedures. Testing our proposed model in five different simulated environments from 165 starting positions yielded an 82.91% success rate in navigating correctly to the sacrum.",1
"Traditionally, an object detector is applied to every part of the scene of interest, and its accuracy and computational cost increases with higher resolution images. However, in some application domains such as remote sensing, purchasing high spatial resolution images is expensive. To reduce the large computational and monetary cost associated with using high spatial resolution images, we propose a reinforcement learning agent that adaptively selects the spatial resolution of each image that is provided to the detector. In particular, we train the agent in a dual reward setting to choose low spatial resolution images to be run through a coarse level detector when the image is dominated by large objects, and high spatial resolution images to be run through a fine level detector when it is dominated by small objects. This reduces the dependency on high spatial resolution images for building a robust detector and increases run-time efficiency. We perform experiments on the xView dataset, consisting of large images, where we increase run-time efficiency by 50% and use high resolution images only 30% of the time while maintaining similar accuracy as a detector that uses only high resolution images.",0
"Usually, an object detector is utilized to scan every part of the desired scene, and its precision and computational requirements become more significant with higher resolution images. However, in certain fields, such as remote sensing, acquiring high spatial resolution images is costly. To decrease the expense of using high spatial resolution images, we suggest a reinforcement learning agent that can intelligently select the spatial resolution of each image submitted to the detector. We teach the agent to choose low spatial resolution images for a coarse level detector when the image is dominated by large objects and high spatial resolution images for a fine level detector when it is dominated by small objects. This reduces the dependence on high spatial resolution images and increases run-time efficiency. We tested our approach on the xView dataset, which has large images, and we achieved similar accuracy as a detector that uses only high resolution images while increasing run-time efficiency by 50% and using high resolution images only 30% of the time.",1
"Potential Based Reward Shaping combined with a potential function based on appropriately defined abstract knowledge has been shown to significantly improve learning speed in Reinforcement Learning. MultiGrid Reinforcement Learning (MRL) has further shown that such abstract knowledge in the form of a potential function can be learned almost solely from agent interaction with the environment. However, we show that MRL faces the problem of not extending well to work with Deep Learning. In this paper we extend and improve MRL to take advantage of modern Deep Learning algorithms such as Deep Q-Networks (DQN). We show that DQN augmented with our approach perform significantly better on continuous control tasks than its Vanilla counterpart and DQN augmented with MRL.",0
"The combination of Potential Based Reward Shaping and a potential function based on appropriately defined abstract knowledge has been proven to enhance the speed of learning in Reinforcement Learning. MultiGrid Reinforcement Learning (MRL) has also demonstrated that such abstract knowledge in the form of a potential function can be learned primarily through agent interaction with the environment. However, we reveal that MRL struggles to integrate with Deep Learning. Therefore, our paper extends and improves MRL by utilizing modern Deep Learning algorithms like Deep Q-Networks (DQN). Our results demonstrate that DQN augmented with our approach perform considerably better on continuous control tasks than its Vanilla counterpart and DQN augmented with MRL.",1
"This paper proposes a framework for adaptively learning a feedback linearization-based tracking controller for an unknown system using discrete-time model-free policy-gradient parameter update rules. The primary advantage of the scheme over standard model-reference adaptive control techniques is that it does not require the learned inverse model to be invertible at all instances of time. This enables the use of general function approximators to approximate the linearizing controller for the system without having to worry about singularities. However, the discrete-time and stochastic nature of these algorithms precludes the direct application of standard machinery from the adaptive control literature to provide deterministic stability proofs for the system. Nevertheless, we leverage these techniques alongside tools from the stochastic approximation literature to demonstrate that with high probability the tracking and parameter errors concentrate near zero when a certain persistence of excitation condition is satisfied. A simulated example of a double pendulum demonstrates the utility of the proposed theory. 1",0
"This paper presents a framework that utilizes discrete-time model-free policy-gradient parameter update rules to adaptively learn a feedback linearization-based tracking controller for an unknown system. The proposed approach has an advantage over standard model-reference adaptive control techniques in that it does not require an invertible learned inverse model at all time instances. This allows the use of general function approximators to approximate the linearizing controller for the system without concern for singularities. However, the discrete-time and stochastic nature of the algorithms makes direct application of standard machinery from the adaptive control literature for deterministic stability proofs infeasible. Despite this, the paper leverages these techniques alongside tools from the stochastic approximation literature to show that the tracking and parameter errors concentrate near zero with high probability when a specific persistence of excitation condition is met. The efficacy of the proposed theory is demonstrated through a simulated example of a double pendulum.",1
"Deep Reinforcement Learning (DRL) has been successfully applied in several research domains such as robot navigation and automated video game playing. However, these methods require excessive computation and interaction with the environment, so enhancements on sample efficiency are required. The main reason for this requirement is that sparse and delayed rewards do not provide an effective supervision for representation learning of deep neural networks. In this study, Proximal Policy Optimization (PPO) algorithm is augmented with Generative Adversarial Networks (GANs) to increase the sample efficiency by enforcing the network to learn efficient representations without depending on sparse and delayed rewards as supervision. The results show that an increased performance can be obtained by jointly training a DRL agent with a GAN discriminator.   ----   Derin Pekistirmeli Ogrenme, robot navigasyonu ve otomatiklestirilmis video oyunu oynama gibi arastirma alanlarinda basariyla uygulanmaktadir. Ancak, kullanilan yontemler ortam ile fazla miktarda etkilesim ve hesaplama gerektirmekte ve bu nedenle de ornek verimliligi yonunden iyilestirmelere ihtiyac duyulmaktadir. Bu gereksinimin en onemli nedeni, gecikmeli ve seyrek odul sinyallerinin derin yapay sinir aglarinin etkili betimlemeler ogrenebilmesi icin yeterli bir denetim saglayamamasidir. Bu calismada, Proksimal Politika Optimizasyonu algoritmasi Uretici Cekismeli Aglar (UCA) ile desteklenerek derin yapay sinir aglarinin seyrek ve gecikmeli odul sinyallerine bagimli olmaksizin etkili betimlemeler ogrenmesi tesvik edilmektedir. Elde edilen sonuclar onerilen algoritmanin ornek verimliliginde artis elde ettigini gostermektedir.",0
"Deep Reinforcement Learning (DRL) has been successfully applied in various research areas, including robot navigation and automated video game playing. However, these methods require extensive computational resources and interactions with the environment, necessitating improvements in sample efficiency. The primary reason for such improvements is that sparse and delayed rewards are insufficient for effective supervision of deep neural network representation learning. This study uses the Generative Adversarial Networks (GANs) to augment the Proximal Policy Optimization (PPO) algorithm, promoting efficient representation learning without relying on sparse and delayed rewards as supervision. Jointly training a DRL agent with a GAN discriminator yields increased performance, as demonstrated by the study's results.",1
"Visual navigation is a task of training an embodied agent by intelligently navigating to a target object (e.g., television) using only visual observations. A key challenge for current deep reinforcement learning models lies in the requirements for a large amount of training data. It is exceedingly expensive to construct sufficient 3D synthetic environments annotated with the target object information. In this paper, we focus on visual navigation in the low-resource setting, where we have only a few training environments annotated with object information. We propose a novel unsupervised reinforcement learning approach to learn transferable meta-skills (e.g., bypass obstacles, go straight) from unannotated environments without any supervisory signals. The agent can then fast adapt to visual navigation through learning a high-level master policy to combine these meta-skills, when the visual-navigation-specified reward is provided. Evaluation in the AI2-THOR environments shows that our method significantly outperforms the baseline by 53.34% relatively on SPL, and further qualitative analysis demonstrates that our method learns transferable motor primitives for visual navigation.",0
"The objective of visual navigation is to train an embodied agent to effectively navigate to a specified object using only visual input. A major challenge for current deep reinforcement learning models is the need for a vast amount of training data. It is prohibitively expensive to create enough 3D synthetic environments that are annotated with the required object information. This study focuses on visual navigation in a low-resource environment, where there are only a few annotated training environments available. To address this problem, we present an innovative unsupervised reinforcement learning technique that allows for the acquisition of transferable meta-skills, such as navigating around obstacles and moving straight ahead, without any supervisory signals. The agent can then quickly adapt to visual navigation by learning a high-level master policy that combines these meta-skills when provided with visual-navigation-based rewards. We conducted an evaluation of our approach in the AI2-THOR environments, which revealed that our method outperformed the baseline by 53.34% on SPL, and further qualitative analysis demonstrated that our approach effectively learned transferable motor primitives required for visual navigation.",1
"Intrinsic motivation enables reinforcement learning (RL) agents to explore when rewards are very sparse, where traditional exploration heuristics such as Boltzmann or e-greedy would typically fail. However, intrinsic exploration is generally handled in an ad-hoc manner, where exploration is not treated as a core objective of the learning process; this weak formulation leads to sub-optimal exploration performance. To overcome this problem, we propose a framework based on multi-objective RL where both exploration and exploitation are being optimized as separate objectives. This formulation brings the balance between exploration and exploitation at a policy level, resulting in advantages over traditional methods. This also allows for controlling exploration while learning, at no extra cost. Such strategies achieve a degree of control over agent exploration that was previously unattainable with classic or intrinsic rewards. We demonstrate scalability to continuous state-action spaces by presenting a method (EMU-Q) based on our framework, guiding exploration towards regions of higher value-function uncertainty. EMU-Q is experimentally shown to outperform classic exploration techniques and other intrinsic RL methods on a continuous control benchmark and on a robotic manipulator.",0
"When rewards are scarce, traditional exploration techniques like Boltzmann or e-greedy may not work, but intrinsic motivation can help reinforcement learning (RL) agents explore. However, intrinsic exploration is often not a central objective of the learning process, leading to sub-optimal results. To address this, we propose a multi-objective RL framework that optimizes both exploration and exploitation as separate objectives. This balances the two at a policy level, resulting in better performance than traditional methods and allowing for control over exploration without extra cost. Our method, EMU-Q, uses this framework to guide exploration towards uncertain regions of the value-function in continuous state-action spaces. We demonstrate its effectiveness on a continuous control benchmark and a robotic manipulator.",1
"The trend is to implement intelligent agents capable of analyzing available information and utilize it efficiently. This work presents a number of reinforcement learning (RL) architectures; one of them is designed for intelligent agents. The proposed architectures are called selector-actor-critic (SAC), tuner-actor-critic (TAC), and estimator-selector-actor-critic (ESAC). These architectures are improved models of a well known architecture in RL called actor-critic (AC). In AC, an actor optimizes the used policy, while a critic estimates a value function and evaluate the optimized policy by the actor. SAC is an architecture equipped with an actor, a critic, and a selector. The selector determines the most promising action at the current state based on the last estimate from the critic. TAC consists of a tuner, a model-learner, an actor, and a critic. After receiving the approximated value of the current state-action pair from the critic and the learned model from the model-learner, the tuner uses the Bellman equation to tune the value of the current state-action pair. ESAC is proposed to implement intelligent agents based on two ideas, which are lookahead and intuition. Lookahead appears in estimating the values of the available actions at the next state, while the intuition appears in maximizing the probability of selecting the most promising action. The newly added elements are an underlying model learner, an estimator, and a selector. The model learner is used to approximate the underlying model. The estimator uses the approximated value function, the learned underlying model, and the Bellman equation to estimate the values of all actions at the next state. The selector is used to determine the most promising action at the next state, which will be used by the actor to optimize the used policy. Finally, the results show the superiority of ESAC compared with the other architectures.",0
"The current trend is to utilize intelligent agents to efficiently analyze available information. This study introduces several reinforcement learning (RL) architectures, including selector-actor-critic (SAC), tuner-actor-critic (TAC), and estimator-selector-actor-critic (ESAC), which are improved versions of the actor-critic (AC) architecture. In AC, an actor optimizes the policy while a critic estimates a value function to evaluate the optimized policy. SAC includes an actor, a critic, and a selector that determines the most promising action using the critic's last estimate. TAC includes a tuner, a model-learner, an actor, and a critic. The tuner uses the Bellman equation to adjust the value of the current state-action pair based on the approximated value and learned model. ESAC implements intelligent agents based on lookahead and intuition, using a model learner, an estimator, and a selector to estimate values and determine the best action. The results demonstrate the superiority of ESAC over the other architectures.",1
"The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning.",0
"Unsupervised learning is typically categorized into two types: imitation learning and reinforcement learning. Imitation learning involves the machine mimicking the actions of an expert system, while reinforcement learning involves the machine learning from direct feedback from the environment. However, traditional deep reinforcement learning can take a significant amount of time to converge to an optimal policy. To address this issue, this paper introduces Augmented Q-Imitation-Learning, a method that accelerates deep reinforcement learning convergence by utilizing Q-imitation-learning as the initial training process in traditional Deep Q-learning.",1
"This work considers the sample and computational complexity of obtaining an $\epsilon$-optimal policy in a discounted Markov Decision Process (MDP), given only access to a generative model. In this work, we study the effectiveness of the most natural plug-in approach to model-based planning: we build the maximum likelihood estimate of the transition model in the MDP from observations and then find an optimal policy in this empirical MDP. We ask arguably the most basic and unresolved question in model based planning: is the naive ""plug-in"" approach, non-asymptotically, minimax optimal in the quality of the policy it finds, given a fixed sample size? Here, the non-asymptotic regime refers to when the sample size is sublinear in the model size.   With access to a generative model, we resolve this question in the strongest possible sense: our main result shows that \emph{any} high accuracy solution in the plug-in model constructed with $N$ samples, provides an $\epsilon$-optimal policy in the true underlying MDP (where $\epsilon$ is the minimax accuracy with $N$ samples at every state, action pair). In comparison, all prior (non-asymptotically) minimax optimal results use model free approaches, such as the Variance Reduced Q-value iteration algorithm (Sidford et al 2018), while the best known model-based results (e.g. Azar et al 2013) require larger sample sizes in their dependence on the planning horizon or the state space. Notably, we show that the model-based approach allows the use of \emph{any} efficient planning algorithm in the empirical MDP, which simplifies algorithm design as this approach does not tie the algorithm to the sampling procedure. The core of our analysis is avnovel ""absorbing MDP"" construction to address the statistical dependency issues that arise in the analysis of model-based planning approaches, a construction which may be helpful more generally.",0
"This study examines the complexity of obtaining an $\epsilon$-optimal policy in a discounted Markov Decision Process (MDP) using a generative model. The most common approach to model-based planning is to create a maximum likelihood estimate of the transition model in the MDP and then determine the optimal policy in this empirical MDP. The question of whether this ""plug-in"" approach is non-asymptotically minimax optimal in terms of policy quality given a fixed sample size is unresolved. The non-asymptotic regime refers to when the sample size is sublinear in the model size. Using a generative model, this study resolves this question by demonstrating that any high accuracy solution in the plug-in model constructed with N samples provides an $\epsilon$-optimal policy in the true underlying MDP. This result is stronger than prior minimax optimal results, which use model-free approaches, and the best known model-based results, which require larger sample sizes. The model-based approach allows for the use of any efficient planning algorithm in the empirical MDP, simplifying algorithm design. The study also introduces a novel ""absorbing MDP"" construction to address the statistical dependency issues that arise in the analysis of model-based planning approaches.",1
"We consider a novel application of inverse reinforcement learning with behavioral economics constraints to model, learn and predict the commenting behavior of YouTube viewers. Each group of users is modeled as a rationally inattentive Bayesian agent which solves a contextual bandit problem. Our methodology integrates three key components. First, to identify distinct commenting patterns, we use deep embedded clustering to estimate framing information (essential extrinsic features) that clusters users into distinct groups.Second, we present an inverse reinforcement learning algorithm that uses Bayesian revealed preferences to test for rationality: does there exist a utility function that rationalizes the given data, and if yes, can it be used to predict commenting behavior? Finally, we impose behavioral economics constraints stemming from rational inattention to characterize the attention span of groups of users. The test imposes a R{\'e}nyi mutual information cost constraint which impacts how the agent can select attention strategies to maximize their expected utility. After a careful analysis of a massive YouTube dataset, our surprising result is that in most YouTube user groups, the commenting behavior is consistent with optimizing a Bayesian utility with rationally inattentive constraints. The paper also highlights how the rational inattention model can accurately predict commenting behavior. The massive YouTube dataset and analysis used in this paper are available on GitHub and completely reproducible.",0
"Our study explores a novel usage of inverse reinforcement learning with behavioral economics constraints to model, learn, and predict the commenting behavior of YouTube viewers. We model each user group as a rationally inattentive Bayesian agent that solves a contextual bandit problem. Our approach comprises three crucial components. Firstly, we use deep embedded clustering to identify distinct commenting patterns and estimate framing information, which are essential extrinsic features that cluster users into distinct groups. Secondly, we use an inverse reinforcement learning algorithm that employs Bayesian revealed preferences to test for rationality and predict commenting behavior. Finally, we impose behavioral economics constraints emanating from rational inattention to define the attention span of user groups. The test applies a R{\'e}nyi mutual information cost constraint that impacts how the agent selects attention strategies to maximize their expected utility. Our analysis of a vast YouTube dataset reveals a surprising result: the commenting behavior in most YouTube user groups is consistent with optimizing a Bayesian utility with rationally inattentive constraints. Additionally, we demonstrate how the rational inattention model can accurately predict commenting behavior. The GitHub repository contains the massive YouTube dataset and analysis, which is entirely reproducible.",1
"This paper introduces Meta-Q-Learning (MQL), a new off-policy algorithm for meta-Reinforcement Learning (meta-RL). MQL builds upon three simple ideas. First, we show that Q-learning is competitive with state-of-the-art meta-RL algorithms if given access to a context variable that is a representation of the past trajectory. Second, a multi-task objective to maximize the average reward across the training tasks is an effective method to meta-train RL policies. Third, past data from the meta-training replay buffer can be recycled to adapt the policy on a new task using off-policy updates. MQL draws upon ideas in propensity estimation to do so and thereby amplifies the amount of available data for adaptation. Experiments on standard continuous-control benchmarks suggest that MQL compares favorably with the state of the art in meta-RL.",0
"A new off-policy algorithm for meta-Reinforcement Learning (meta-RL) called Meta-Q-Learning (MQL) is presented in this paper. MQL is built upon three simple ideas. The first is that Q-learning can compete with state-of-the-art meta-RL algorithms when given a context variable that represents the past trajectory. Second, a multi-task objective that maximizes the average reward across training tasks is an effective method to meta-train RL policies. Third, past data from the meta-training replay buffer can be reused to adapt the policy on a new task using off-policy updates, drawing upon ideas in propensity estimation to amplify the amount of available data for adaptation. Standard continuous-control benchmarks experiments suggest that MQL is comparable to the state of the art in meta-RL.",1
"As deep reinforcement learning (RL) is applied to more tasks, there is a need to visualize and understand the behavior of learned agents. Saliency maps explain agent behavior by highlighting the features of the input state that are most relevant for the agent in taking an action. Existing perturbation-based approaches to compute saliency often highlight regions of the input that are not relevant to the action taken by the agent. Our proposed approach, SARFA (Specific and Relevant Feature Attribution), generates more focused saliency maps by balancing two aspects (specificity and relevance) that capture different desiderata of saliency. The first captures the impact of perturbation on the relative expected reward of the action to be explained. The second downweighs irrelevant features that alter the relative expected rewards of actions other than the action to be explained. We compare SARFA with existing approaches on agents trained to play board games (Chess and Go) and Atari games (Breakout, Pong and Space Invaders). We show through illustrative examples (Chess, Atari, Go), human studies (Chess), and automated evaluation methods (Chess) that SARFA generates saliency maps that are more interpretable for humans than existing approaches. For the code release and demo videos, see https://nikaashpuri.github.io/sarfa-saliency/.",0
"With the increasing application of deep reinforcement learning (RL) in various tasks, it has become necessary to gain insight into the behavior of learned agents. Saliency maps serve to explain the actions taken by an agent by identifying the input state features that are most relevant to it. However, current perturbation-based methods for computing saliency tend to highlight input regions that are not necessarily relevant to the agent's actions. To address this issue, we propose an alternative approach called SARFA (Specific and Relevant Feature Attribution), which balances two factors - specificity and relevance - that encompass different aspects of saliency. The former factor assesses the impact of perturbation on the expected reward of the action being explained, while the latter factor downplays irrelevant features that may affect the expected rewards of other actions. We evaluate our approach on agents trained to play various board games (Chess and Go) and Atari games (Breakout, Pong and Space Invaders), and demonstrate, through examples, human studies and automated evaluation methods, that SARFA generates saliency maps that are more human-interpretable than existing approaches. To access the code and demo videos, please visit https://nikaashpuri.github.io/sarfa-saliency/.",1
"This paper investigates how to efficiently transition and update policies, trained initially with demonstrations, using off-policy actor-critic reinforcement learning. It is well-known that techniques based on Learning from Demonstrations, for example behavior cloning, can lead to proficient policies given limited data. However, it is currently unclear how to efficiently update that policy using reinforcement learning as these approaches are inherently optimizing different objective functions. Previous works have used loss functions, which combine behavior cloning losses with reinforcement learning losses to enable this update. However, the components of these loss functions are often set anecdotally, and their individual contributions are not well understood. In this work, we propose the Cycle-of-Learning (CoL) framework that uses an actor-critic architecture with a loss function that combines behavior cloning and 1-step Q-learning losses with an off-policy pre-training step from human demonstrations. This enables transition from behavior cloning to reinforcement learning without performance degradation and improves reinforcement learning in terms of overall performance and training time. Additionally, we carefully study the composition of these combined losses and their impact on overall policy learning. We show that our approach outperforms state-of-the-art techniques for combining behavior cloning and reinforcement learning for both dense and sparse reward scenarios. Our results also suggest that directly including the behavior cloning loss on demonstration data helps to ensure stable learning and ground future policy updates.",0
"The aim of this study is to explore the efficient transition and updating of policies that were initially trained using demonstrations, using off-policy actor-critic reinforcement learning. While Learning from Demonstrations techniques such as behavior cloning can lead to skillful policies with limited data, it is currently unclear how to update these policies efficiently using reinforcement learning, as these approaches optimize different objective functions. Previous studies have used loss functions that combine behavior cloning losses with reinforcement learning losses to enable this update, but the individual contributions of these components are not well understood. To address these challenges, the authors propose the Cycle-of-Learning (CoL) framework, which uses an actor-critic architecture with a loss function that combines behavior cloning and 1-step Q-learning losses, along with an off-policy pre-training step from human demonstrations. This approach enables a smooth transition from behavior cloning to reinforcement learning without any performance degradation and improves reinforcement learning in terms of overall performance and training time. The authors also carefully examine the composition of these combined losses and their impact on overall policy learning, showing that their approach outperforms state-of-the-art techniques for combining behavior cloning and reinforcement learning for both dense and sparse reward scenarios. They suggest that directly including the behavior cloning loss on demonstration data helps to ensure stable learning and ground future policy updates.",1
"Air traffic control is an example of a highly challenging operational problem that is readily amenable to human expertise augmentation via decision support technologies. In this paper, we propose a new intelligent decision making framework that leverages multi-agent reinforcement learning (MARL) to dynamically suggest adjustments of aircraft speeds in real-time. The goal of the system is to enhance the ability of an air traffic controller to provide effective guidance to aircraft to avoid air traffic congestion, near-miss situations, and to improve arrival timeliness. We develop a novel deep ensemble MARL method that can concisely capture the complexity of the air traffic control problem by learning to efficiently arbitrate between the decisions of a local kernel-based RL model and a wider-reaching deep MARL model. The proposed method is trained and evaluated on an open-source air traffic management simulator developed by Eurocontrol. Extensive empirical results on a real-world dataset including thousands of aircraft demonstrate the feasibility of using multi-agent RL for the problem of en-route air traffic control and show that our proposed deep ensemble MARL method significantly outperforms three state-of-the-art benchmark approaches.",0
"The augmentation of human expertise through decision support technologies can greatly benefit highly challenging operational problems such as air traffic control. This paper presents a new intelligent decision making framework that utilizes multi-agent reinforcement learning (MARL) to suggest real-time adjustments to aircraft speeds. The main objective of the system is to improve the effectiveness of air traffic controllers in guiding aircraft to prevent air traffic congestion, near-miss situations, and enhance arrival timeliness. A deep ensemble MARL method has been developed to efficiently manage the complexity of air traffic control by combining a local kernel-based RL model with a wider-reaching deep MARL model. The proposed method has been trained and evaluated on an open-source air traffic management simulator developed by Eurocontrol. Empirical results on a real-world dataset of thousands of aircraft demonstrate that multi-agent RL can be effectively used for en-route air traffic control. The deep ensemble MARL method outperforms three state-of-the-art benchmark approaches.",1
"While maximizing expected return is the goal in most reinforcement learning approaches, risk-sensitive objectives such as conditional value at risk (CVaR) are more suitable for many high-stakes applications. However, relatively little is known about how to explore to quickly learn policies with good CVaR. In this paper, we present the first algorithm for sample-efficient learning of CVaR-optimal policies in Markov decision processes based on the optimism in the face of uncertainty principle. This method relies on a novel optimistic version of the distributional Bellman operator that moves probability mass from the lower to the upper tail of the return distribution. We prove asymptotic convergence and optimism of this operator for the tabular policy evaluation case. We further demonstrate that our algorithm finds CVaR-optimal policies substantially faster than existing baselines in several simulated environments with discrete and continuous state spaces.",0
"Most reinforcement learning approaches aim to maximize expected return, but conditional value at risk (CVaR) is more appropriate for high-stakes applications that require a risk-sensitive objective. However, there is limited knowledge on how to efficiently explore and learn policies with good CVaR. This paper introduces the first algorithm for sample-efficient learning of CVaR-optimal policies in Markov decision processes, using the optimism in the face of uncertainty principle. The algorithm utilizes an optimistic version of the distributional Bellman operator that redistributes probability mass from the lower to the upper tail of the return distribution. The paper demonstrates the asymptotic convergence and optimism of this operator for the tabular policy evaluation case and shows that the algorithm finds CVaR-optimal policies much faster than existing baselines in simulated environments with discrete and continuous state spaces.",1
"Although in recent years reinforcement learning has become very popular the number of successful applications to different kinds of operations research problems is rather scarce. Reinforcement learning is based on the well-studied dynamic programming technique and thus also aims at finding the best stationary policy for a given Markov Decision Process, but in contrast does not require any model knowledge. The policy is assessed solely on consecutive states (or state-action pairs), which are observed while an agent explores the solution space. The contributions of this paper are manifold. First we provide deep theoretical insights to the widely applied standard discounted reinforcement learning framework, which give rise to the understanding of why these algorithms are inappropriate when permanently provided with non-zero rewards, such as costs or profit. Second, we establish a novel near-Blackwell-optimal reinforcement learning algorithm. In contrary to former method it assesses the average reward per step separately and thus prevents the incautious combination of different types of state values. Thereby, the Laurent Series expansion of the discounted state values forms the foundation for this development and also provides the connection between the two approaches. Finally, we prove the viability of our algorithm on a challenging problem set, which includes a well-studied M/M/1 admission control queuing system. In contrast to standard discounted reinforcement learning our algorithm infers the optimal policy on all tested problems. The insights are that in the operations research domain machine learning techniques have to be adapted and advanced to successfully apply these methods in our settings.",0
"Despite its recent popularity, reinforcement learning has not been widely successful in solving various operations research problems. This technique is based on dynamic programming and aims to find the best stationary policy for a given Markov Decision Process without requiring any model knowledge. The policy is evaluated based on consecutive states or state-action pairs observed during exploration. This paper presents multiple contributions. Firstly, it offers theoretical insights into why standard discounted reinforcement learning algorithms are unsuitable for problems with non-zero rewards. Secondly, it introduces a novel near-Blackwell-optimal reinforcement learning algorithm that assesses the average reward per step separately and avoids combining different types of state values. The algorithm is based on the Laurent Series expansion of the discounted state values and is shown to outperform standard discounted reinforcement learning on a challenging problem set that includes an M/M/1 admission control queuing system. These findings highlight the need to adapt and advance machine learning techniques for operations research applications.",1
"We demonstrate the first reinforcement-learning application for robots equipped with an event camera. Because of the considerably lower latency of the event camera, it is possible to achieve much faster control of robots compared with the existing vision-based reinforcement-learning applications using standard cameras. To handle a stream of events for reinforcement learning, we introduced an image-like feature and demonstrated the feasibility of training an agent in a simulator for two tasks: fast collision avoidance and obstacle tracking. Finally, we set up a robot with an event camera in the real world and then transferred the agent trained in the simulator, resulting in successful fast avoidance of randomly thrown objects. Incorporating event camera into reinforcement learning opens new possibilities for various robotics applications that require swift control, such as autonomous vehicles and drones, through end-to-end learning approaches.",0
"Our study showcases the initial employment of reinforcement learning in robots that are fitted with event cameras. These cameras have a significantly lower latency, enabling quicker control of robots in contrast to the conventional vision-based reinforcement-learning applications that make use of standard cameras. To manage the stream of events for reinforcement learning, we introduced an image-like feature and illustrated the possibility of training an agent in a simulator for two tasks - fast collision avoidance and obstacle tracking. Finally, we implemented an event camera-equipped robot in the actual world and transferred the agent trained in the simulator, which resulted in a successful fast avoidance of randomly thrown objects. The inclusion of event cameras in reinforcement learning presents new opportunities for various robotics applications like autonomous vehicles and drones through end-to-end learning approaches that demand swift control.",1
"Model-free deep reinforcement learning (RL) has demonstrated its superiority on many complex sequential decision-making problems. However, heavy dependence on dense rewards and high sample-complexity impedes the wide adoption of these methods in real-world scenarios. On the other hand, imitation learning (IL) learns effectively in sparse-rewarded tasks by leveraging the existing expert demonstrations. In practice, collecting a sufficient amount of expert demonstrations can be prohibitively expensive, and the quality of demonstrations typically limits the performance of the learning policy. In this work, we propose Self-Adaptive Imitation Learning (SAIL) that can achieve (near) optimal performance given only a limited number of sub-optimal demonstrations for highly challenging sparse reward tasks. SAIL bridges the advantages of IL and RL to reduce the sample complexity substantially, by effectively exploiting sup-optimal demonstrations and efficiently exploring the environment to surpass the demonstrated performance. Extensive empirical results show that not only does SAIL significantly improve the sample-efficiency but also leads to much better final performance across different continuous control tasks, comparing to the state-of-the-art.",0
"Although model-free deep reinforcement learning (RL) has proven effective in solving complex sequential decision-making problems, its reliance on dense rewards and high sample-complexity has hindered its widespread implementation in real-world situations. Conversely, imitation learning (IL) is proficient in learning sparse-rewarded tasks by utilizing expert demonstrations. However, obtaining a sufficient quantity of such demonstrations can be prohibitively expensive, and their quality may restrict the performance of the learning policy. This report introduces Self-Adaptive Imitation Learning (SAIL), which can achieve optimal or near-optimal performance in highly challenging sparse reward tasks with only a limited number of sub-optimal demonstrations. SAIL merges the advantages of IL and RL to significantly reduce sample complexity by efficiently utilizing sub-optimal demonstrations and exploring the environment to surpass the demonstrated performance. Extensive empirical results indicate that SAIL not only enhances sample-efficiency but also achieves much better final performance across various continuous control tasks compared to the state-of-the-art.",1
"Motivated by the many real-world applications of reinforcement learning (RL) that require safe-policy iterations, we consider the problem of off-policy evaluation (OPE) -- the problem of evaluating a new policy using the historical data obtained by different behavior policies -- under the model of nonstationary episodic Markov Decision Processes (MDP) with a long horizon and a large action space. Existing importance sampling (IS) methods often suffer from large variance that depends exponentially on the RL horizon $H$. To solve this problem, we consider a marginalized importance sampling (MIS) estimator that recursively estimates the state marginal distribution for the target policy at every step. MIS achieves a mean-squared error of $$ \frac{1}{n} \sum\nolimits_{t=1}^H\mathbb{E}_{\mu}\left[\frac{d_t^\pi(s_t)^2}{d_t^\mu(s_t)^2} \mathrm{Var}_{\mu}\left[\frac{\pi_t(a_t|s_t)}{\mu_t(a_t|s_t)}\big( V_{t+1}^\pi(s_{t+1}) + r_t\big) \middle| s_t\right]\right] + \tilde{O}(n^{-1.5}) $$ where $\mu$ and $\pi$ are the logging and target policies, $d_t^{\mu}(s_t)$ and $d_t^{\pi}(s_t)$ are the marginal distribution of the state at $t$th step, $H$ is the horizon, $n$ is the sample size and $V_{t+1}^\pi$ is the value function of the MDP under $\pi$. The result matches the Cramer-Rao lower bound in \citet{jiang2016doubly} up to a multiplicative factor of $H$. To the best of our knowledge, this is the first OPE estimation error bound with a polynomial dependence on $H$. Besides theory, we show empirical superiority of our method in time-varying, partially observable, and long-horizon RL environments.",0
"The problem of off-policy evaluation (OPE) is considered for nonstationary episodic Markov Decision Processes (MDP) with a long horizon and a large action space, motivated by the need for safe-policy iterations in real-world applications of reinforcement learning (RL). Existing importance sampling (IS) methods suffer from large variance that exponentially depends on the RL horizon H. To address this, a marginalized importance sampling (MIS) estimator is proposed that recursively estimates the state marginal distribution for the target policy at each step. The estimator achieves a mean-squared error that matches the Cramer-Rao lower bound up to a multiplicative factor of H and has a polynomial dependence on H, making it the first OPE estimation error bound with such a dependence. Empirical results demonstrate the superiority of the proposed method in time-varying, partially observable, and long-horizon RL environments. The logging and target policies are denoted as  and , respectively, and d_t^(s_t) and d_t^(s_t) represent the marginal distribution of the state at the tth step. The horizon is denoted as H, the sample size as n, and V_{t+1}^ is the value function of the MDP under . The reward at time t is denoted as r_t, and the action at time t as a_t.",1
"We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets. We have released our implementation at https://github.com/SNAS-Series/SNAS-Series.",0
"Our proposed solution, Stochastic Neural Architecture Search (SNAS), offers an efficient end-to-end approach to Neural Architecture Search (NAS). By training both neural operation parameters and architecture distribution parameters using the same round of back-propagation, SNAS ensures the completeness and differentiability of the NAS pipeline. We reframe NAS as an optimization problem on parameters of a joint distribution for the search space in a cell. To make use of gradient information in a generic differentiable loss for architecture search, we introduce a novel search gradient. We demonstrate that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but with more efficient credit assignment for structural decisions. Furthermore, we augment credit assignment with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS finds a cell architecture with state-of-the-art accuracy in fewer epochs than non-differentiable evolution-based and reinforcement-learning-based NAS. This approach is also transferable to ImageNet. We show that the child networks of SNAS can maintain the validation accuracy during searching, whereas attention-based NAS requires parameter retraining to compete. Our implementation is available at https://github.com/SNAS-Series/SNAS-Series.",1
"Maximizing utility with a budget constraint is the primary goal for advertisers in real-time bidding (RTB) systems. The policy maximizing the utility is referred to as the optimal bidding strategy. Earlier works on optimal bidding strategy apply model-based batch reinforcement learning methods which can not generalize to unknown budget and time constraint. Further, the advertiser observes a censored market price which makes direct evaluation infeasible on batch test datasets. Previous works ignore the losing auctions to alleviate the difficulty with censored states; thus significantly modifying the test distribution. We address the challenge of lacking a clear evaluation procedure as well as the error propagated through batch reinforcement learning methods in RTB systems. We exploit two conditional independence structures in the sequential bidding process that allow us to propose a novel practical framework using the maximum entropy principle to imitate the behavior of the true distribution observed in real-time traffic. Moreover, the framework allows us to train a model that can generalize to the unseen budget conditions than limit only to those observed in history. We compare our methods on two real-world RTB datasets with several baselines and demonstrate significantly improved performance under various budget settings.",0
"The primary objective for advertisers in real-time bidding (RTB) systems is to maximize utility while adhering to a budget constraint. This is accomplished through the implementation of an optimal bidding strategy. Previous approaches to this strategy have utilized model-based batch reinforcement learning methods, which are unable to generalize to unknown budget and time constraints. Additionally, due to the presence of censored market prices, direct evaluation on batch test datasets is not feasible. Prior works have ignored losing auctions to alleviate this issue, but this significantly alters the test distribution. Our approach addresses the challenge of lacking a clear evaluation procedure and the errors associated with batch reinforcement learning methods in RTB systems. We utilize two conditional independence structures in the sequential bidding process to propose a novel and practical framework that imitates the behavior of the true distribution observed in real-time traffic through the maximum entropy principle. Furthermore, this framework allows for the training of a model that can generalize to unseen budget conditions, rather than being limited to those observed in the past. We compare our approach to several baselines on two real-world RTB datasets and demonstrate significantly improved performance under various budget settings.",1
"We describe a method for 3D human pose estimation from transient images (i.e., a 3D spatio-temporal histogram of photons) acquired by an optical non-line-of-sight (NLOS) imaging system. Our method can perceive 3D human pose by `looking around corners' through the use of light indirectly reflected by the environment. We bring together a diverse set of technologies from NLOS imaging, human pose estimation and deep reinforcement learning to construct an end-to-end data processing pipeline that converts a raw stream of photon measurements into a full 3D human pose sequence estimate. Our contributions are the design of data representation process which includes (1) a learnable inverse point spread function (PSF) to convert raw transient images into a deep feature vector; (2) a neural humanoid control policy conditioned on the transient image feature and learned from interactions with a physics simulator; and (3) a data synthesis and augmentation strategy based on depth data that can be transferred to a real-world NLOS imaging system. Our preliminary experiments suggest that our method is able to generalize to real-world NLOS measurement to estimate physically-valid 3D human poses.",0
"A technique for determining the 3D posture of a human body from transient images obtained through an optical non-line-of-sight (NLOS) imaging system is presented. Our method uses light that is indirectly reflected by the environment to perceive 3D human posture by ""looking around corners."" We employ a variety of technologies, such as NLOS imaging, deep reinforcement learning, and human pose estimation, to create an end-to-end data processing pipeline that transforms a raw stream of photon measurements into a complete 3D human pose sequence estimate. Our contributions include the design of a data representation process that consists of a learnable inverse point spread function (PSF) for converting raw transient images into a deep feature vector, a neural humanoid control policy conditioned on the transient image feature and learned from interactions with a physics simulator, and a data synthesis and augmentation strategy based on depth data that can be transferred to a real-world NLOS imaging system. Our initial experiments indicate that our approach can generalize to real-world NLOS measurements and estimate physically valid 3D human postures.",1
"Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used methods, despite all their benefits, suffer from extreme data inefficiency, especially in the rich visual domains like Atari. To circumvent this problem, novel approaches were introduced that often claim to be much more efficient than popular variations of the state-of-the-art DQN algorithm. In this paper, however, we demonstrate that the newly proposed techniques simply used unfair baselines in their experiments. Namely, we show that the actual improvement in the efficiency came from allowing the algorithm for more training updates for each data sample, and not from employing the new methods. By allowing DQN to execute network updates more frequently we manage to reach similar or better results than the recently proposed advancement, often at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.",0
"Over the last few years, Reinforcement Learning (RL) has made significant progress. However, the RL community agrees that current methods have a major flaw - they are extremely data inefficient, especially in rich visual domains like Atari. To address this issue, novel techniques were introduced, claiming to be much more efficient than the state-of-the-art DQN algorithm. Conversely, this paper demonstrates that these new approaches used unfair benchmarks in their experiments. Our findings reveal that the real efficiency improvement came from allowing the algorithm to have more training updates for each data sample, not from the new techniques. By enabling DQN to execute network updates more frequently, we achieved comparable or better results than the recent advancements, at a fraction of the complexity and computational costs. Additionally, based on our study's outcomes, we suggest that the modified DQN agent we present should serve as a baseline for any future work aimed at enhancing the sample efficiency of deep reinforcement learning.",1
"Many medical decision-making tasks can be framed as partially observed Markov decision processes (POMDPs). However, prevailing two-stage approaches that first learn a POMDP and then solve it often fail because the model that best fits the data may not be well suited for planning. We introduce a new optimization objective that (a) produces both high-performing policies and high-quality generative models, even when some observations are irrelevant for planning, and (b) does so in batch off-policy settings that are typical in healthcare, when only retrospective data is available. We demonstrate our approach on synthetic examples and a challenging medical decision-making problem.",0
"Partially observed Markov decision processes (POMDPs) can be utilized for various medical decision-making tasks. However, the traditional two-stage approach of initially learning a POMDP and then solving it often results in failure since the model that fits the data accurately may not be suitable for planning purposes. To tackle this issue, we propose a new optimization objective that produces high-performing policies and high-quality generative models, even when some observations are irrelevant for planning. Additionally, our approach is effective in batch off-policy settings that are common in healthcare where only retrospective data is available. We demonstrate the effectiveness of our approach on synthetic examples and a challenging medical decision-making problem.",1
"We adapt the optimization's concept of momentum to reinforcement learning. Seeing the state-action value functions as an analog to the gradients in optimization, we interpret momentum as an average of consecutive $q$-functions. We derive Momentum Value Iteration (MoVI), a variation of Value Iteration that incorporates this momentum idea. Our analysis shows that this allows MoVI to average errors over successive iterations. We show that the proposed approach can be readily extended to deep learning. Specifically, we propose a simple improvement on DQN based on MoVI, and experiment it on Atari games.",0
"Incorporating the optimization principle of momentum into reinforcement learning, we view the state-action value functions as equivalents to optimization gradients, and consider momentum as the mean value of consecutive $q$-functions. This leads us to introduce Momentum Value Iteration (MoVI), a modified version of Value Iteration that integrates the momentum concept. Our study demonstrates that MoVI can smooth out errors across iterations. Furthermore, we demonstrate that this technique can be effortlessly extended to deep learning, and we suggest a straightforward enhancement to DQN based on MoVI, which we evaluate through experimentation on Atari games.",1
"A common challenge in reinforcement learning is how to convert the agent's interactions with an environment into fast and robust learning. For instance, earlier work makes use of domain knowledge to improve existing reinforcement learning algorithms in complex tasks. While promising, previously acquired knowledge is often costly and challenging to scale up. Instead, we decide to consider problem knowledge with signals from quantities relevant to solve any task, e.g., self-performance assessment and accurate expectations. $\mathcal{V}^{ex}$ is such a quantity. It is the fraction of variance explained by the value function $V$ and measures the discrepancy between $V$ and the returns. Taking advantage of $\mathcal{V}^{ex}$, we propose MERL, a general framework for structuring reinforcement learning by injecting problem knowledge into policy gradient updates. As a result, the agent is not only optimized for a reward but learns using problem-focused quantities provided by MERL, applicable out-of-the-box to any task. In this paper: (a) We introduce and define MERL, the multi-head reinforcement learning framework we use throughout this work. (b) We conduct experiments across a variety of standard benchmark environments, including 9 continuous control tasks, where results show improved performance. (c) We demonstrate that MERL also improves transfer learning on a set of challenging pixel-based tasks. (d) We ponder how MERL tackles the problem of reward sparsity and better conditions the feature space of reinforcement learning agents.",0
"One of the main obstacles in reinforcement learning is finding ways to enable the agent to learn quickly and effectively from its interactions with the environment. While previous research has used domain knowledge to enhance reinforcement learning algorithms in complex tasks, this approach can be costly and difficult to scale up. To address this issue, we propose using problem knowledge along with signals from relevant quantities like self-performance assessment and accurate expectations. The quantity $\mathcal{V}^{ex}$, which measures the discrepancy between the value function and the returns, is one such quantity that we leverage. Our approach, called MERL, injects problem knowledge into policy gradient updates, enabling the agent to learn not just from rewards but also from problem-focused quantities provided by MERL. In this paper, we introduce and define MERL, conduct experiments on standard benchmark environments, including 9 continuous control tasks, and demonstrate how it improves transfer learning on challenging pixel-based tasks. We also discuss how MERL addresses the issue of reward sparsity and conditions the feature space of reinforcement learning agents.",1
"Parameter space exploration methods with black-box optimization have recently been shown to outperform state-of-the-art approaches in continuous control reinforcement learning domains. In this paper, we examine reasons why these methods work better and the situations in which they are worse than traditional action space exploration methods. Through a simple theoretical analysis, we show that when the parametric complexity required to solve the reinforcement learning problem is greater than the product of action space dimensionality and horizon length, exploration in action space is preferred. This is also shown empirically by comparing simple exploration methods on several toy problems.",0
Recent studies have demonstrated that black-box optimization-based parameter space exploration techniques surpass the latest approaches in continuous control reinforcement learning fields. This paper aims to investigate why these methods are more successful and when they are less effective compared to traditional action space exploration methods. Our theoretical analysis indicates that exploration in action space is preferable when the reinforcement learning problem necessitates greater parametric complexity than the product of horizon length and action space dimensionality. This finding is reinforced by our empirical analysis of various toy problems using simple exploration methods.,1
"We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of $\tilde{O}(DS\sqrt{AT})$ for any communicating MDP with $S$ states, $A$ actions and diameter $D$. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon $T$. This result closely matches the known lower bound of $\Omega(\sqrt{DSAT})$. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest.",0
"An algorithm that utilizes posterior sampling, also known as Thompson sampling, is introduced in this paper. The algorithm is capable of achieving nearly optimal regret bounds in the worst-case scenario when the Markov Decision Process (MDP) is communicating with an unknown, yet finite diameter. The primary outcome of this study is a high probability regret upper bound of $\tilde{O}(DS\sqrt{AT})$ for any communicating MDP with $S$ states, $A$ actions, and diameter $D$. Regret is defined as the comparison between the total reward obtained by the algorithm and the total expected reward of an optimal infinite-horizon undiscounted average reward policy within the time horizon of $T$. The result obtained in this study is consistent with the previously established lower bound of $\Omega(\sqrt{DSAT})$. The techniques employed in this study involve establishing novel findings regarding the anti-concentration of Dirichlet distribution, which may be of independent interest.",1
"Epidemics of infectious diseases are an important threat to public health and global economies. Yet, the development of prevention strategies remains a challenging process, as epidemics are non-linear and complex processes. For this reason, we investigate a deep reinforcement learning approach to automatically learn prevention strategies in the context of pandemic influenza. Firstly, we construct a new epidemiological meta-population model, with 379 patches (one for each administrative district in Great Britain), that adequately captures the infection process of pandemic influenza. Our model balances complexity and computational efficiency such that the use of reinforcement learning techniques becomes attainable. Secondly, we set up a ground truth such that we can evaluate the performance of the 'Proximal Policy Optimization' algorithm to learn in a single district of this epidemiological model. Finally, we consider a large-scale problem, by conducting an experiment where we aim to learn a joint policy to control the districts in a community of 11 tightly coupled districts, for which no ground truth can be established. This experiment shows that deep reinforcement learning can be used to learn mitigation policies in complex epidemiological models with a large state space. Moreover, through this experiment, we demonstrate that there can be an advantage to consider collaboration between districts when designing prevention strategies.",0
"Infectious disease outbreaks pose a significant risk to public health and the global economy. However, developing prevention strategies is a difficult task due to the non-linear and complex nature of these epidemics. To address this issue in the context of pandemic influenza, we have explored a deep reinforcement learning approach. We started by creating a new epidemiological meta-population model with 379 patches, one for each administrative district in Great Britain, that accurately captures the infection process. This model strikes a balance between complexity and computational efficiency, making it suitable for reinforcement learning techniques. We then established a ground truth to evaluate the performance of the 'Proximal Policy Optimization' algorithm in a single district. Finally, we conducted an experiment in an 11-district community to learn a joint policy for controlling the districts. This experiment demonstrates that deep reinforcement learning can effectively learn mitigation policies in complex epidemiological models with a large state space. Additionally, we have shown that collaboration between districts can be advantageous when designing prevention strategies.",1
"We present a new class of stochastic, geometrically-driven optimization algorithms on the orthogonal group $O(d)$ and naturally reductive homogeneous manifolds obtained from the action of the rotation group $SO(d)$. We theoretically and experimentally demonstrate that our methods can be applied in various fields of machine learning including deep, convolutional and recurrent neural networks, reinforcement learning, normalizing flows and metric learning. We show an intriguing connection between efficient stochastic optimization on the orthogonal group and graph theory (e.g. matching problem, partition functions over graphs, graph-coloring). We leverage the theory of Lie groups and provide theoretical results for the designed class of algorithms. We demonstrate broad applicability of our methods by showing strong performance on the seemingly unrelated tasks of learning world models to obtain stable policies for the most difficult $\mathrm{Humanoid}$ agent from $\mathrm{OpenAI}$ $\mathrm{Gym}$ and improving convolutional neural networks.",0
"A new category of optimization algorithms, which are driven by stochastic and geometric approaches on the orthogonal group $O(d)$ and reductive homogeneous manifolds that are naturally derived from the action of the rotation group $SO(d), are presented in this study. Through theoretical and experimental evidence, we demonstrate that these approaches can be applied to different areas of machine learning such as deep, recurrent, and convolutional neural networks, metric learning, normalizing flows, and reinforcement learning. Furthermore, we establish an interesting relationship between efficient stochastic optimization on the orthogonal group and graph theory, including partition functions over graphs, graph-coloring, and matching problems. Using Lie group theory, we provide theoretical results for the proposed class of algorithms. Finally, we demonstrate the wide applicability of our methods by exhibiting their successful performance on seemingly unrelated tasks such as enhancing convolutional neural networks and training humanoid agents from OpenAI Gym to obtain stable policies.",1
"Overestimation of the maximum action-value is a well-known problem that hinders Q-Learning performance, leading to suboptimal policies and unstable learning. Among several Q-Learning variants proposed to address this issue, Weighted Q-Learning (WQL) effectively reduces the bias and shows remarkable results in stochastic environments. WQL uses a weighted sum of the estimated action-values, where the weights correspond to the probability of each action-value being the maximum; however, the computation of these probabilities is only practical in the tabular settings. In this work, we provide the methodological advances to benefit from the WQL properties in Deep Reinforcement Learning (DRL), by using neural networks with Dropout Variational Inference as an effective approximation of deep Gaussian processes. In particular, we adopt the Concrete Dropout variant to obtain calibrated estimates of epistemic uncertainty in DRL. We show that model uncertainty in DRL can be useful not only for action selection, but also action evaluation. We analyze how the novel Weighted Deep Q-Learning algorithm reduces the bias w.r.t. relevant baselines and provide empirical evidence of its advantages on several representative benchmarks.",0
"Q-Learning performance can suffer due to the problem of overestimating the maximum action-value, leading to suboptimal policies and unstable learning. Weighted Q-Learning (WQL) is one of the Q-Learning variants that effectively addresses this issue by reducing bias and showcasing remarkable results in stochastic environments. WQL makes use of a weighted sum of estimated action-values, with weights corresponding to the probability of each action-value being the maximum. However, computing these probabilities is only practical in tabular settings. This work introduces methodological advances to apply WQL properties in Deep Reinforcement Learning (DRL) by using neural networks with Dropout Variational Inference as an effective approximation of deep Gaussian processes. The Concrete Dropout variant is adopted to obtain calibrated estimates of epistemic uncertainty in DRL. The study shows that model uncertainty in DRL is useful not just for action selection, but also action evaluation. The Weighted Deep Q-Learning algorithm is analyzed for its ability to reduce the bias compared to relevant baselines, and empirical evidence of its advantages is presented through several representative benchmarks.",1
"Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.",0
"For over a decade, Atari games have been a prominent reference point in the reinforcement learning (RL) community, serving as a means to evaluate the general competence of RL algorithms. While previous work has achieved good average performance by excelling in many of the games in the set, they have struggled to perform well in challenging games. In contrast, we present Agent57, the first deep RL agent to surpass the standard human benchmark in all 57 Atari games. Our approach involves training a neural network that encompasses a range of policies, from exploratory to purely exploitative, and implementing an adaptive mechanism to prioritize policies during the training process. Additionally, we utilize a unique parameterization of the architecture to facilitate consistent and stable learning.",1
"As Computer Vision moves from a passive analysis of pixels to active analysis of semantics, the breadth of information algorithms need to reason over has expanded significantly. One of the key challenges in this vein is the ability to identify the information required to make a decision, and select an action that will recover it. We propose a reinforcement-learning approach that maintains a distribution over its internal information, thus explicitly representing the ambiguity in what it knows, and needs to know, towards achieving its goal. Potential actions are then generated according to this distribution. For each potential action a distribution of the expected outcomes is calculated, and the value of the potential information gain assessed. The action taken is that which maximizes the potential information gain. We demonstrate this approach applied to two vision-and-language problems that have attracted significant recent interest, visual dialog and visual query generation. In both cases, the method actively selects actions that will best reduce its internal uncertainty and outperforms its competitors in achieving the goal of the challenge.",0
"With the shift of Computer Vision from passive pixel analysis to active semantic analysis, algorithms are required to reason over a broader range of information. A crucial obstacle in this regard is recognizing the information necessary for decision-making and choosing an appropriate action to retrieve it. To address this, we suggest a reinforcement-learning technique that maintains a distribution over internal information to account for ambiguity in knowledge and requirements towards the goal. This distribution generates potential actions, and each action's expected outcomes are evaluated to determine its potential information gain. The action that maximizes this gain is selected. We demonstrate the effectiveness of this approach on two vision-and-language problems, visual dialog and visual query generation, showing that it outperforms other methods by actively reducing internal uncertainty.",1
"$ $Let $F$ be a multivariate function from a product set $\Sigma^n$ to an Abelian group $G$. A $k$-partition of $F$ with cost $\delta$ is a partition of the set of variables $\mathbf{V}$ into $k$ non-empty subsets $(\mathbf{X}_1, \dots, \mathbf{X}_k)$ such that $F(\mathbf{V})$ is $\delta$-close to $F_1(\mathbf{X}_1)+\dots+F_k(\mathbf{X}_k)$ for some $F_1, \dots, F_k$ with respect to a given error metric. We study algorithms for agnostically learning $k$ partitions and testing $k$-partitionability over various groups and error metrics given query access to $F$. In particular we show that   $1.$ Given a function that has a $k$-partition of cost $\delta$, a partition of cost $\mathcal{O}(k n^2)(\delta + \epsilon)$ can be learned in time $\tilde{\mathcal{O}}(n^2 \mathrm{poly} (1/\epsilon))$ for any $\epsilon > 0$. In contrast, for $k = 2$ and $n = 3$ learning a partition of cost $\delta + \epsilon$ is NP-hard.   $2.$ When $F$ is real-valued and the error metric is the 2-norm, a 2-partition of cost $\sqrt{\delta^2 + \epsilon}$ can be learned in time $\tilde{\mathcal{O}}(n^5/\epsilon^2)$.   $3.$ When $F$ is $\mathbb{Z}_q$-valued and the error metric is Hamming weight, $k$-partitionability is testable with one-sided error and $\mathcal{O}(kn^3/\epsilon)$ non-adaptive queries. We also show that even two-sided testers require $\Omega(n)$ queries when $k = 2$.   This work was motivated by reinforcement learning control tasks in which the set of control variables can be partitioned. The partitioning reduces the task into multiple lower-dimensional ones that are relatively easier to learn. Our second algorithm empirically increases the scores attained over previous heuristic partitioning methods applied in this context.",0
"A multivariate function $F$ maps a product set $\Sigma^n$ to an Abelian group $G$. A $k$-partition of $F$ is a partition of the set of variables $\mathbf{V}$ into $k$ non-empty subsets $(\mathbf{X}_1, \dots, \mathbf{X}_k)$ such that $F(\mathbf{V})$ is $\delta$-close to $F_1(\mathbf{X}_1)+\dots+F_k(\mathbf{X}_k)$ for some $F_1, \dots, F_k$ with respect to a given error metric. We investigate algorithms for agnostically learning $k$ partitions and testing $k$-partitionability over various groups and error metrics using query access to $F$. Our results are as follows. Firstly, we demonstrate that a function with a $k$-partition of cost $\delta$ can have a partition of cost $\mathcal{O}(k n^2)(\delta + \epsilon)$ learned in time $\tilde{\mathcal{O}}(n^2 \mathrm{poly} (1/\epsilon))$ for any $\epsilon > 0$. This is in contrast to the NP-hardness of learning a partition of cost $\delta + \epsilon$ for $k = 2$ and $n = 3$. Secondly, when $F$ is real-valued and the error metric is the 2-norm, a 2-partition of cost $\sqrt{\delta^2 + \epsilon}$ can be learned in time $\tilde{\mathcal{O}}(n^5/\epsilon^2)$. Finally, when $F$ is $\mathbb{Z}_q$-valued and the error metric is Hamming weight, $k$-partitionability is testable with one-sided error and $\mathcal{O}(kn^3/\epsilon)$ non-adaptive queries. We also prove that even two-sided testers require $\Omega(n)$ queries when $k = 2$. Our research was motivated by reinforcement learning control tasks that can be partitioned into lower-dimensional ones for easier learning. Our second algorithm improves the scores attained over previous heuristic partitioning methods applied in this context.",1
"A typical domain adaptation approach is to adapt models trained on the annotated data in a source domain (e.g., sunny weather) for achieving high performance on the test data in a target domain (e.g., rainy weather). Whether the target contains a single homogeneous domain or multiple heterogeneous domains, existing works always assume that there exist clear distinctions between the domains, which is often not true in practice (e.g., changes in weather). We study an open compound domain adaptation (OCDA) problem, in which the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach based on two technical insights into OCDA: 1) a curriculum domain adaptation strategy to bootstrap generalization across domains in a data-driven self-organizing fashion and 2) a memory module to increase the model's agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.",0
"To achieve high performance on test data in a target domain (e.g., rainy weather), a common domain adaptation method is to adapt models trained on annotated data in a source domain (e.g., sunny weather). However, existing works assume clear distinctions between domains, which is often not the case in practice (e.g., changes in weather). Our study focuses on an open compound domain adaptation (OCDA) problem where the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach that leverages two technical insights into OCDA: 1) a curriculum domain adaptation strategy for data-driven self-organizing generalization across domains and 2) a memory module to increase the model's agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.",1
"In batch reinforcement learning (RL), one often constrains a learned policy to be close to the behavior (data-generating) policy, e.g., by constraining the learned action distribution to differ from the behavior policy by some maximum degree that is the same at each state. This can cause batch RL to be overly conservative, unable to exploit large policy changes at frequently-visited, high-confidence states without risking poor performance at sparsely-visited states. To remedy this, we propose residual policies, where the allowable deviation of the learned policy is state-action-dependent. We derive a new for RL method, BRPO, which learns both the policy and allowable deviation that jointly maximize a lower bound on policy performance. We show that BRPO achieves the state-of-the-art performance in a number of tasks.",0
"Batch reinforcement learning (RL) often limits a learned policy to closely resemble the behavior policy by restricting the learned action distribution to have a maximum difference from the behavior policy at every state. However, this can lead to an overly cautious approach that misses out on significant policy changes at highly frequented and confident states while risking poor performance at infrequently visited states. To address this, we suggest residual policies that have state-action-dependent allowances for learned policy deviations. We introduce a new RL method, BRPO, that learns both the policy and allowable deviation, optimizing a lower bound on policy performance. Our results demonstrate that BRPO performs exceptionally well in various tasks, surpassing the current state-of-the-art performance.",1
"To effect behavior change a successful algorithm must make high-quality decisions in real-time. For example, a mobile health (mHealth) application designed to increase physical activity must make contextually relevant suggestions to motivate users. While machine learning offers solutions for certain stylized settings, such as when batch data can be processed offline, there is a dearth of approaches which can deliver high-quality solutions under the specific constraints of mHealth. We propose an algorithm which provides users with contextualized and personalized physical activity suggestions. This algorithm is able to overcome a challenge critical to mHealth that complex models be trained efficiently. We propose a tractable streamlined empirical Bayes procedure which fits linear mixed effects models in large-data settings. Our procedure takes advantage of sparsity introduced by hierarchical random effects to efficiently learn the posterior distribution of a linear mixed effects model. A key contribution of this work is that we provide explicit updates in order to learn both fixed effects, random effects and hyper-parameter values. We demonstrate the success of this approach in a mobile health (mHealth) reinforcement learning application, a domain in which fast computations are crucial for real time interventions. Not only is our approach computationally efficient, it is also easily implemented with closed form matrix algebraic updates and we show improvements over state of the art approaches both in speed and accuracy of up to 99% and 56% respectively.",0
"In order for behavior change to occur, a successful algorithm must be able to make high-quality decisions in real-time. For instance, a mobile health application that aims to increase physical activity must provide relevant suggestions to encourage users. While machine learning can provide solutions in certain settings, such as when data can be processed offline, there is a lack of approaches that can offer high-quality solutions under the specific constraints of mobile health. Our proposed algorithm offers contextualized and personalized physical activity suggestions that are able to overcome the challenge of efficiently training complex models critical to mobile health. We introduce a streamlined empirical Bayes procedure that fits linear mixed effects models in large-data settings, taking advantage of sparsity introduced by hierarchical random effects to efficiently learn the posterior distribution of a linear mixed effects model. Our approach updates fixed effects, random effects, and hyper-parameter values explicitly. We demonstrate the success of our approach in a mobile health reinforcement learning application where fast computations are essential for real-time interventions. Our approach is both computationally efficient and easy to implement, with closed-form matrix algebraic updates, and we show improvements over state-of-the-art approaches in terms of speed and accuracy, up to 99% and 56% respectively.",1
"Learning visual similarity requires to learn relations, typically between triplets of images. Albeit triplet approaches being powerful, their computational complexity mostly limits training to only a subset of all possible training triplets. Thus, sampling strategies that decide when to use which training sample during learning are crucial. Currently, the prominent paradigm are fixed or curriculum sampling strategies that are predefined before training starts. However, the problem truly calls for a sampling process that adjusts based on the actual state of the similarity representation during training. We, therefore, employ reinforcement learning and have a teacher network adjust the sampling distribution based on the current state of the learner network, which represents visual similarity. Experiments on benchmark datasets using standard triplet-based losses show that our adaptive sampling strategy significantly outperforms fixed sampling strategies. Moreover, although our adaptive sampling is only applied on top of basic triplet-learning frameworks, we reach competitive results to state-of-the-art approaches that employ diverse additional learning signals or strong ensemble architectures. Code can be found under https://github.com/Confusezius/CVPR2020_PADS.",0
"To learn visual similarity, it is necessary to learn relationships between groups of three images, known as triplets. However, the computational complexity associated with triplet approaches means that training is often limited to a subset of possible training triplets. Therefore, it is crucial to have sampling strategies that determine when to use which training sample during learning. Currently, fixed or curriculum sampling strategies are the predominant paradigms, but a sampling process that adapts to the current state of the similarity representation during training is required. To address this, we have employed reinforcement learning, utilizing a teacher network to adjust the sampling distribution based on the learner network's current state, which represents visual similarity. Our experiments on benchmark datasets using standard triplet-based losses demonstrate that our adaptive sampling strategy outperforms fixed sampling strategies. Moreover, despite our adaptive sampling only being applied on top of basic triplet-learning frameworks, we achieve competitive results compared to state-of-the-art approaches that utilize diverse additional learning signals or strong ensemble architectures. Our code can be found at https://github.com/Confusezius/CVPR2020_PADS.",1
"We present a distributional approach to theoretical analyses of reinforcement learning algorithms for constant step-sizes. We demonstrate its effectiveness by presenting simple and unified proofs of convergence for a variety of commonly-used methods. We show that value-based methods such as TD($\lambda$) and $Q$-Learning have update rules which are contractive in the space of distributions of functions, thus establishing their exponentially fast convergence to a stationary distribution. We demonstrate that the stationary distribution obtained by any algorithm whose target is an expected Bellman update has a mean which is equal to the true value function. Furthermore, we establish that the distributions concentrate around their mean as the step-size shrinks. We further analyse the optimistic policy iteration algorithm, for which the contraction property does not hold, and formulate a probabilistic policy improvement property which entails the convergence of the algorithm.",0
"Our paper proposes a distributional method for analyzing reinforcement learning algorithms with constant step-sizes. We prove the effectiveness of this approach by providing simple and unified proofs of convergence for several commonly used methods. Our analysis reveals that value-based techniques, such as TD($\lambda$) and $Q$-Learning, utilize update rules that contract within the distribution of functions. Therefore, these methods achieve exponentially fast convergence to a stationary distribution. We demonstrate that any algorithm targeting an expected Bellman update produces a stationary distribution whose mean equals the true value function, and that the distributions concentrate around their mean as the step-size decreases. We also examine the optimistic policy iteration algorithm, which lacks the contraction property, and establish a probabilistic policy improvement property that guarantees convergence.",1
"The neural architecture search (NAS) algorithm with reinforcement learning can be a powerful and novel framework for the automatic discovering process of neural architectures. However, its application is restricted by noncontinuous and high-dimensional search spaces, which result in difficulty in optimization. To resolve these problems, we proposed NAS in embedding space (NASES), which is a novel framework. Unlike other NAS with reinforcement learning approaches that search over a discrete and high-dimensional architecture space, this approach enables reinforcement learning to search in an embedding space by using architecture encoders and decoders. The current experiment demonstrated that the performance of the final architecture network using the NASES procedure is comparable with that of other popular NAS approaches for the image classification task on CIFAR-10. The results of the experiment were efficient and indicated that NASES was highly efficient to discover final architecture only in $<$3.5 GPU hours. The beneficial-performance and effectiveness of NASES was impressive when the architecture-embedding searching and weight initialization were applied.",0
"The automatic discovery process of neural architectures can be facilitated by the neural architecture search (NAS) algorithm using reinforcement learning. However, the optimization of this process is challenging due to noncontinuous and high-dimensional search spaces. To address these issues, a new framework called NAS in embedding space (NASES) has been proposed. Unlike other NAS with reinforcement learning algorithms, NASES enables reinforcement learning to search in an embedding space by using architecture encoders and decoders. The current experiment demonstrated that NASES is comparable in performance to other popular NAS approaches for the image classification task on CIFAR-10. Moreover, the results were efficient, with NASES discovering a final architecture in less than 3.5 GPU hours. The effectiveness of NASES was further improved by applying architecture-embedding searching and weight initialization.",1
"Black-box adversarial attacks on video recognition models have been explored. Considering the temporal interactions between frames, a few methods try to select some key frames, and then perform attacks on them. Unfortunately, their selecting strategy is independent with the attacking step, resulting in the limited performance. Instead, we argue the frame selection phase is closely relevant with the attacking phase. The key frames should be adjusted according to the attacking results. For that, we formulate the black-box video attacks into Reinforcement Learning (RL) framework. Specifically, the environment in RL is set as the threat model, and the agent in RL plays the role of frame selecting. By continuously querying the threat models and receiving the attacking feedback, the agent gradually adjusts its frame selection strategy and adversarial perturbations become smaller and smaller. A series of experiments demonstrate that our method can significantly reduce the adversarial perturbations with efficient query times.",0
"Various methods have been employed to investigate black-box adversarial attacks on video recognition models. Some approaches have attempted to select crucial frames while factoring in the temporal interactions between frames, followed by attacks on those selected frames. However, these methods have limitations due to their independent selection strategy from the attacking step. Instead, we propose that the frame selection phase should be closely linked to the attacking phase, with key frames adjusted according to the attacking results. To achieve this, we adopt a Reinforcement Learning (RL) framework to formulate black-box video attacks. The RL environment is set as the threat model, with the agent playing the role of frame selection. By continually querying the threat models and receiving feedback on the attacks, the agent adjusts its frame selection strategy, resulting in gradually decreasing adversarial perturbations. Our experimental results show that our approach effectively reduces adversarial perturbations with efficient query times.",1
"Recent advances in machine learning are consistently enabled by increasing amounts of computation. Reinforcement learning (RL) and population-based methods in particular pose unique challenges for efficiency and flexibility to the underlying distributed computing frameworks. These challenges include frequent interaction with simulations, the need for dynamic scaling, and the need for a user interface with low adoption cost and consistency across different backends. In this paper we address these challenges while still retaining development efficiency and flexibility for both research and practical applications by introducing Fiber, a scalable distributed computing framework for RL and population-based methods. Fiber aims to significantly expand the accessibility of large-scale parallel computation to users of otherwise complicated RL and population-based approaches without the need to for specialized computational expertise.",0
"Increasing amounts of computation consistently drive recent advancements in machine learning. However, the efficiency and flexibility of underlying distributed computing frameworks face unique challenges when dealing with reinforcement learning (RL) and population-based methods. These challenges involve dynamic scaling, frequent simulation interactions, and the need for a user interface that is consistent across various backends and has low adoption cost. This paper introduces Fiber, a scalable distributed computing framework for RL and population-based methods that overcomes these challenges while maintaining development efficiency and flexibility for both research and practical applications. Fiber aims to provide large-scale parallel computation accessibility to users of RL and population-based approaches without the need for specialized computational expertise.",1
"Off-policy estimation for long-horizon problems is important in many real-life applications such as healthcare and robotics, where high-fidelity simulators may not be available and on-policy evaluation is expensive or impossible. Recently, \cite{liu18breaking} proposed an approach that avoids the \emph{curse of horizon} suffered by typical importance-sampling-based methods. While showing promising results, this approach is limited in practice as it requires data be drawn from the \emph{stationary distribution} of a \emph{known} behavior policy. In this work, we propose a novel approach that eliminates such limitations. In particular, we formulate the problem as solving for the fixed point of a certain operator. Using tools from Reproducing Kernel Hilbert Spaces (RKHSs), we develop a new estimator that computes importance ratios of stationary distributions, without knowledge of how the off-policy data are collected. We analyze its asymptotic consistency and finite-sample generalization. Experiments on benchmarks verify the effectiveness of our approach.",0
"In various real-life scenarios, such as healthcare and robotics, off-policy estimation is crucial for long-horizon problems. On-policy evaluation can be expensive or even impossible, especially when high-fidelity simulators are not available. Although \cite{liu18breaking} proposed a new approach that avoids the curse of horizon, it has certain limitations as it requires the data to be drawn from the stationary distribution of a known behavior policy. In this study, we introduce a novel approach that overcomes such limitations by formulating the problem as solving for the fixed point of a particular operator. With the use of Reproducing Kernel Hilbert Spaces (RKHSs), we create a new estimator that can calculate importance ratios of stationary distributions without the need for knowledge regarding how the off-policy data were gathered. We examine its asymptotic consistency and finite-sample generalization, and our experiments on benchmarks confirm the efficacy of our approach.",1
"This work presents an application of Reinforcement Learning (RL) for the complete control of real soccer robots of the IEEE Very Small Size Soccer (VSSS), a traditional league in the Latin American Robotics Competition (LARC). In the VSSS league, two teams of three small robots play against each other. We propose a simulated environment in which continuous or discrete control policies can be trained, and a Sim-to-Real method to allow using the obtained policies to control a robot in the real world. The results show that the learned policies display a broad repertoire of behaviors that are difficult to specify by hand. This approach, called VSSS-RL, was able to beat the human-designed policy for the striker of the team ranked 3rd place in the 2018 LARC, in 1-vs-1 matches.",0
"The aim of this study is to demonstrate the effectiveness of Reinforcement Learning (RL) in controlling real soccer robots in the IEEE Very Small Size Soccer (VSSS) league, which is a well-known event in the Latin American Robotics Competition (LARC). The VSSS league involves six small robots playing against each other in two teams. To achieve our goal, we designed a simulated environment where continuous or discrete control policies can be trained and a Sim-to-Real method to apply the learned policies in the real world. The results of our study indicate that the learned policies are capable of exhibiting diverse behaviors that would be difficult to define manually. Our approach, which is called VSSS-RL, outperformed the human-designed policy for the striker of the 3rd place team in the 2018 LARC in 1-vs-1 matches.",1
"Adversarial learning has shown its advances in generating natural and diverse descriptions in image captioning. However, the learned reward of existing adversarial methods is vague and ill-defined due to the reward ambiguity problem. In this paper, we propose a refined Adversarial Inverse Reinforcement Learning (rAIRL) method to handle the reward ambiguity problem by disentangling reward for each word in a sentence, as well as achieve stable adversarial training by refining the loss function to shift the generator towards Nash equilibrium. In addition, we introduce a conditional term in the loss function to mitigate mode collapse and to increase the diversity of the generated descriptions. Our experiments on MS COCO and Flickr30K show that our method can learn compact reward for image captioning.",0
"The use of adversarial learning has demonstrated its ability to produce natural and varied descriptions in image captioning. Nevertheless, the inadequate and uncertain reward learned by current adversarial approaches is due to the reward ambiguity issue. In this study, we present an enhanced version of Adversarial Inverse Reinforcement Learning (rAIRL) to address the reward ambiguity by separating the reward for each word in a sentence. Furthermore, we refine the loss function to ensure stable adversarial training by shifting the generator towards the Nash equilibrium. Additionally, we introduce a conditional term in the loss function to reduce mode collapse and enhance the diversity of the generated descriptions. Our experiments on MS COCO and Flickr30K demonstrate that our approach can effectively learn a concise reward for image captioning.",1
"The last decade witnessed increasingly rapid progress in self-driving vehicle technology, mainly backed up by advances in the area of deep learning and artificial intelligence. The objective of this paper is to survey the current state-of-the-art on deep learning technologies used in autonomous driving. We start by presenting AI-based self-driving architectures, convolutional and recurrent neural networks, as well as the deep reinforcement learning paradigm. These methodologies form a base for the surveyed driving scene perception, path planning, behavior arbitration and motion control algorithms. We investigate both the modular perception-planning-action pipeline, where each module is built using deep learning methods, as well as End2End systems, which directly map sensory information to steering commands. Additionally, we tackle current challenges encountered in designing AI architectures for autonomous driving, such as their safety, training data sources and computational hardware. The comparison presented in this survey helps to gain insight into the strengths and limitations of deep learning and AI approaches for autonomous driving and assist with design choices",0
"Advancements in deep learning and artificial intelligence have led to significant progress in self-driving vehicle technology over the past decade. This paper aims to provide an overview of the current state-of-the-art in deep learning technologies used in autonomous driving. The presented AI-based self-driving architectures include convolutional and recurrent neural networks, as well as the deep reinforcement learning paradigm. These form the foundation for driving scene perception, path planning, behavior arbitration and motion control algorithms, which are examined in both the modular perception-planning-action pipeline and End2End systems. The paper also addresses challenges faced in designing AI architectures for autonomous driving such as safety, training data sources and computational hardware. By comparing the strengths and limitations of deep learning and AI approaches for autonomous driving, this survey offers insight to assist with design choices.",1
"Social Reinforcement Learning methods, which model agents in large networks, are useful for fake news mitigation, personalized teaching/healthcare, and viral marketing, but it is challenging to incorporate inter-agent dependencies into the models effectively due to network size and sparse interaction data. Previous social RL approaches either ignore agents dependencies or model them in a computationally intensive manner. In this work, we incorporate agent dependencies efficiently in a compact model by clustering users (based on their payoff and contribution to the goal) and combine this with a method to easily derive personalized agent-level policies from cluster-level policies. We also propose a dynamic clustering approach that captures changing user behavior. Experiments on real-world datasets illustrate that our proposed approach learns more accurate policy estimates and converges more quickly, compared to several baselines that do not use agent correlations or only use static clusters.",0
"Social Reinforcement Learning techniques are beneficial for addressing issues such as fake news, personalized healthcare and teaching, and viral marketing in extensive networks. However, creating effective models that account for inter-agent dependencies is challenging due to network size and sparse interaction data. Previous social RL methods have either ignored agent dependencies or employed complex models that are computationally intensive. This study presents a compact model that efficiently incorporates agent dependencies by clustering users based on their payoff and contribution to the goal. Moreover, we propose a method to derive personalized agent-level policies from cluster-level policies and a dynamic clustering approach that captures changing user behavior. Our approach outperformed several baselines that did not consider agent correlations or used static clusters in terms of policy accuracy and convergence speed in real-world experiments.",1
"In multi-agent games, the complexity of the environment can grow exponentially as the number of agents increases, so it is particularly challenging to learn good policies when the agent population is large. In this paper, we introduce Evolutionary Population Curriculum (EPC), a curriculum learning paradigm that scales up Multi-Agent Reinforcement Learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. Furthermore, EPC uses an evolutionary approach to fix an objective misalignment issue throughout the curriculum: agents successfully trained in an early stage with a small population are not necessarily the best candidates for adapting to later stages with scaled populations. Concretely, EPC maintains multiple sets of agents in each stage, performs mix-and-match and fine-tuning over these sets and promotes the sets of agents with the best adaptability to the next stage. We implement EPC on a popular MARL algorithm, MADDPG, and empirically show that our approach consistently outperforms baselines by a large margin as the number of agents grows exponentially.",0
"When dealing with multi-agent games, the complexity of the environment can become increasingly challenging as the number of agents involved grows. This makes it difficult to learn effective policies, especially when dealing with large populations of agents. To address this issue, we propose the Evolutionary Population Curriculum (EPC) as a learning paradigm for Multi-Agent Reinforcement Learning (MARL). EPC utilizes a stage-wise approach to gradually increase the population of agents being trained. Additionally, an evolutionary approach is used to prevent objective misalignment between early stage trained agents and later stage agents trained with bigger populations. EPC maintains multiple sets of agents, performs fine-tuning and mix-and-match techniques, and promotes the sets of agents with the best adaptability to the next stage. We implement EPC on MADDPG, a popular MARL algorithm, and the results show that our approach consistently outperforms baseline methods as the number of agents increases exponentially.",1
"Relational Reinforcement Learning (RRL) can offers various desirable features. Most importantly, it allows for incorporating expert knowledge into the learning, and hence leading to much faster learning and better generalization compared to the standard deep reinforcement learning. However, most of the existing RRL approaches are either incapable of incorporating expert background knowledge (e.g., in the form of explicit predicate language) or are not able to learn directly from non-relational data such as image. In this paper, we propose a novel deep RRL based on a differentiable Inductive Logic Programming (ILP) that can effectively learn relational information from image and present the state of the environment as first order logic predicates. Additionally, it can take the expert background knowledge and incorporate it into the learning problem using appropriate predicates. The differentiable ILP allows an end to end optimization of the entire framework for learning the policy in RRL. We show the efficacy of this novel RRL framework using environments such as BoxWorld, GridWorld as well as relational reasoning for the Sort-of-CLEVR dataset.",0
"Various desirable features can be offered by Relational Reinforcement Learning (RRL). The most significant advantage is its capability to include expert knowledge into the learning process, which leads to faster learning and better generalization than standard deep reinforcement learning. However, most existing RRL approaches cannot incorporate expert background knowledge in the form of explicit predicate language, or learn directly from non-relational data such as images. This paper presents a novel deep RRL that uses differentiable Inductive Logic Programming (ILP) to effectively learn relational information from images and represent the environment state as first-order logic predicates. The proposed framework can incorporate expert background knowledge using appropriate predicates and optimize the entire learning policy in RRL using differentiable ILP. The effectiveness of this novel RRL framework is demonstrated in various environments, including BoxWorld, GridWorld, and relational reasoning for the Sort-of-CLEVR dataset.",1
"Unmanned Surface Vehicles technology (USVs) is an exciting topic that essentially deploys an algorithm to safely and efficiently performs a mission. Although reinforcement learning is a well-known approach to modeling such a task, instability and divergence may occur when combining off-policy and function approximation. In this work, we used deep reinforcement learning combining Q-learning with a neural representation to avoid instability. Our methodology uses deep q-learning and combines it with a rolling wave planning approach on agile methodology. Our method contains two critical parts in order to perform missions in an unknown environment. The first is a path planner that is responsible for generating a potential effective path to a destination without considering the details of the root. The latter is a decision-making module that is responsible for short-term decisions on avoiding obstacles during the near future steps of USV exploitation within the context of the value function. Simulations were performed using two algorithms: a basic vanilla vessel navigator (VVN) as a baseline and an improved one for the vessel navigator with a planner and local view (VNPLV). Experimental results show that the proposed method enhanced the performance of VVN by 55.31 on average for long-distance missions. Our model successfully demonstrated obstacle avoidance by means of deep reinforcement learning using planning adaptive paths in unknown environments.",0
"The use of Unmanned Surface Vehicles (USVs) technology is an interesting subject which involves the implementation of an algorithm to carry out missions in a safe and efficient manner. While reinforcement learning is a commonly used approach for this task, issues such as instability and divergence can arise when combining off-policy and function approximation. This study proposes a methodology that utilizes deep q-learning and a rolling wave planning approach on agile methodology to overcome such challenges. The approach involves two critical components: a path planner that generates an effective path to a destination and a decision-making module responsible for avoiding obstacles during the mission. The proposed method was tested using two algorithms, a vanilla vessel navigator (VVN) as a baseline and an improved version with a planner and local view (VNPLV). Results from simulations showed that the proposed method improved the performance of VVN by 55.31 on average for long-distance missions. The study successfully demonstrated the use of deep reinforcement learning to adaptively plan paths and avoid obstacles in unknown environments.",1
"Deep reinforcement learning has proven to be successful for learning tasks in simulated environments, but applying same techniques for robots in real-world domain is more challenging, as they require hours of training. To address this, transfer learning can be used to train the policy first in a simulated environment and then transfer it to physical agent. As the simulation never matches reality perfectly, the physics, visuals and action spaces by necessity differ between these environments to some degree. In this work, we study how general video games can be directly used instead of fine-tuned simulations for the sim-to-real transfer. Especially, we study how the agent can learn the new action space autonomously, when the game actions do not match the robot actions. Our results show that the different action space can be learned by re-training only part of neural network and we obtain above 90% mean success rate in simulation and robot experiments.",0
"Although deep reinforcement learning has been successful in teaching tasks in simulated environments, it poses a greater challenge when applied to robots in the real world due to the lengthy training time required. To overcome this, transfer learning is a viable solution, where the policy is initially trained in a simulated environment and then transferred to a physical agent. However, since simulations cannot perfectly replicate reality, there are differences in the physics, visuals, and action spaces between the two environments. In this study, we explore the possibility of utilizing general video games instead of customized simulations for the sim-to-real transfer and investigate how the agent can learn a new action space independently, even when the game actions do not match the robot actions. Our findings indicate that by re-training only a portion of the neural network, the agent can effectively learn the different action space, achieving a mean success rate of over 90% in both simulation and robot experiments.",1
"In recent years, Multifactorial Optimization (MFO) has gained a notable momentum in the research community. MFO is known for its inherent capability to efficiently address multiple optimization tasks at the same time, while transferring information among such tasks to improve their convergence speed. On the other hand, the quantum leap made by Deep Q Learning (DQL) in the Machine Learning field has allowed facing Reinforcement Learning (RL) problems of unprecedented complexity. Unfortunately, complex DQL models usually find it difficult to converge to optimal policies due to the lack of exploration or sparse rewards. In order to overcome these drawbacks, pre-trained models are widely harnessed via Transfer Learning, extrapolating knowledge acquired in a source task to the target task. Besides, meta-heuristic optimization has been shown to reduce the lack of exploration of DQL models. This work proposes a MFO framework capable of simultaneously evolving several DQL models towards solving interrelated RL tasks. Specifically, our proposed framework blends together the benefits of meta-heuristic optimization, Transfer Learning and DQL to automate the process of knowledge transfer and policy learning of distributed RL agents. A thorough experimentation is presented and discussed so as to assess the performance of the framework, its comparison to the traditional methodology for Transfer Learning in terms of convergence, speed and policy quality , and the intertask relationships found and exploited over the search process.",0
"Multifactorial Optimization (MFO) has gained significant momentum in recent years for its ability to efficiently address multiple optimization tasks simultaneously and transfer information among them to improve convergence speed. However, while Deep Q Learning (DQL) has made a quantum leap in the Machine Learning field, complex DQL models often struggle to converge to optimal policies due to sparse rewards or lack of exploration. To overcome these challenges, pre-trained models are commonly used through Transfer Learning, which extrapolates knowledge acquired in a source task to the target task. Additionally, meta-heuristic optimization has been shown to reduce the lack of exploration of DQL models. This work proposes a MFO framework that blends meta-heuristic optimization, Transfer Learning, and DQL to automate the process of knowledge transfer and policy learning for distributed RL agents. We present thorough experimentation to assess the framework's performance, comparing it to traditional Transfer Learning methods in terms of convergence, speed, and policy quality, and highlighting the intertask relationships found and exploited during the search process.",1
"This paper presents the concept of an adaptive safe padding that forces Reinforcement Learning (RL) to synthesise optimal control policies while ensuring safety during the learning process. Policies are synthesised to satisfy a goal, expressed as a temporal logic formula, with maximal probability. Enforcing the RL agent to stay safe during learning might limit the exploration, however we show that the proposed architecture is able to automatically handle the trade-off between efficient progress in exploration (towards goal satisfaction) and ensuring safety. Theoretical guarantees are available on the optimality of the synthesised policies and on the convergence of the learning algorithm. Experimental results are provided to showcase the performance of the proposed method.",0
"The aim of this article is to introduce an adaptive safe padding that enables Reinforcement Learning (RL) to create optimal control policies while maintaining safety during the learning process. These policies are designed to meet a goal stated as a temporal logic formula with maximum probability. It may restrict exploration if the RL agent must remain safe during learning, but our proposed structure can balance effective progress in exploration (toward goal satisfaction) and safety. The article offers theoretical guarantees regarding the optimality of the synthesized policies and the convergence of the learning algorithm. Additionally, experimental results are provided to demonstrate the effectiveness of this approach.",1
"In this paper, we propose the problem of collaborative perception, where robots can combine their local observations with those of neighboring agents in a learnable way to improve accuracy on a perception task. Unlike existing work in robotics and multi-agent reinforcement learning, we formulate the problem as one where learned information must be shared across a set of agents in a bandwidth-sensitive manner to optimize for scene understanding tasks such as semantic segmentation. Inspired by networking communication protocols, we propose a multi-stage handshake communication mechanism where the neural network can learn to compress relevant information needed for each stage. Specifically, a target agent with degraded sensor data sends a compressed request, the other agents respond with matching scores, and the target agent determines who to connect with (i.e., receive information from). We additionally develop the AirSim-CP dataset and metrics based on the AirSim simulator where a group of aerial robots perceive diverse landscapes, such as roads, grasslands, buildings, etc. We show that for the semantic segmentation task, our handshake communication method significantly improves accuracy by approximately 20% over decentralized baselines, and is comparable to centralized ones using a quarter of the bandwidth.",0
"The aim of this paper is to introduce the concept of collaborative perception as a means for robots to enhance their accuracy in perception tasks by combining their individual observations with those of their neighboring agents. In contrast to previous work in robotics and multi-agent reinforcement learning, we propose a bandwidth-sensitive approach to the problem, where learned information must be shared between agents in an optimized manner for scene understanding tasks like semantic segmentation. Drawing inspiration from networking communication protocols, we suggest a multi-stage handshake communication mechanism that allows the neural network to learn how to compress relevant information for each stage. This involves a target agent with impaired sensor data sending a compressed request, to which other agents respond with matching scores, enabling the target agent to determine who to connect with for information. To test our approach, we created the AirSim-CP dataset and associated metrics based on the AirSim simulator, in which a group of aerial robots observe diverse landscapes. Our results show that our communication method significantly improves accuracy for the semantic segmentation task by approximately 20% over decentralized baselines, while using only a quarter of the bandwidth compared to centralized methods.",1
"This paper focuses on finding reinforcement learning policies for control systems with hard state and action constraints. Despite its success in many domains, reinforcement learning is challenging to apply to problems with hard constraints, especially if both the state variables and actions are constrained. Previous works seeking to ensure constraint satisfaction, or safety, have focused on adding a projection step to a learned policy. Yet, this approach requires solving an optimization problem at every policy execution step, which can lead to significant computational costs.   To tackle this problem, this paper proposes a new approach, termed Vertex Networks (VNs), with guarantees on safety during exploration and on learned control policies by incorporating the safety constraints into the policy network architecture. Leveraging the geometric property that all points within a convex set can be represented as the convex combination of its vertices, the proposed algorithm first learns the convex combination weights and then uses these weights along with the pre-calculated vertices to output an action. The output action is guaranteed to be safe by construction. Numerical examples illustrate that the proposed VN algorithm outperforms vanilla reinforcement learning in a variety of benchmark control tasks.",0
"The main focus of this paper is to discover reinforcement learning policies for control systems that possess strict state and action constraints. Reinforcement learning has shown success in many areas, but it is challenging to apply to problems with hard constraints, especially when both state variables and actions are limited. Prior research aimed at ensuring constraint satisfaction has involved adding a projection step to a learned policy. However, this method requires solving an optimization problem at each policy execution, leading to high computational costs. To address this issue, this paper introduces a new approach called Vertex Networks (VNs), which guarantees safety during exploration and learned control policies by incorporating safety constraints into the policy network architecture. By utilizing the geometric property that all points within a convex set can be expressed as the convex combination of its vertices, the proposed algorithm first learns the convex combination weights and then employs these weights with the pre-calculated vertices to generate an action that is safe by design. Numerical examples demonstrate that the VN algorithm surpasses vanilla reinforcement learning in various benchmark control tasks.",1
"This paper investigates the idea of encoding object-centered representations in the design of the reward function and policy architectures of a language-guided reinforcement learning agent. This is done using a combination of object-wise permutation invariant networks inspired from Deep Sets and gated-attention mechanisms. In a 2D procedurally-generated world where agents targeting goals in natural language navigate and interact with objects, we show that these architectures demonstrate strong generalization capacities to out-of-distribution goals. We study the generalization to varying numbers of objects at test time and further extend the object-centered architectures to goals involving relational reasoning.",0
"The aim of this study is to explore the concept of incorporating object-centered representations in the reward function and policy architectures of a language-guided reinforcement learning agent. To achieve this, the study employs a combination of object-wise permutation invariant networks inspired by Deep Sets and gated-attention mechanisms. The study is conducted in a 2D procedurally-generated world where agents navigate and interact with objects to achieve goals stated in natural language. The study demonstrates that these architectures display strong generalization capabilities for goals that are not part of the distribution. The study also examines the ability of the architectures to generalize to different numbers of objects during testing and extends the object-centered architectures to goals that involve relational reasoning.",1
"We address a core problem of computer vision: Detection and description of 2D feature points for image matching. For a long time, hand-crafted designs, like the seminal SIFT algorithm, were unsurpassed in accuracy and efficiency. Recently, learned feature detectors emerged that implement detection and description using neural networks. Training these networks usually resorts to optimizing low-level matching scores, often pre-defining sets of image patches which should or should not match, or which should or should not contain key points. Unfortunately, increased accuracy for these low-level matching scores does not necessarily translate to better performance in high-level vision tasks. We propose a new training methodology which embeds the feature detector in a complete vision pipeline, and where the learnable parameters are trained in an end-to-end fashion. We overcome the discrete nature of key point selection and descriptor matching using principles from reinforcement learning. As an example, we address the task of relative pose estimation between a pair of images. We demonstrate that the accuracy of a state-of-the-art learning-based feature detector can be increased when trained for the task it is supposed to solve at test time. Our training methodology poses little restrictions on the task to learn, and works for any architecture which predicts key point heat maps, and descriptors for key point locations.",0
"The main concern of computer vision is identifying and describing 2D feature points for image comparison. Traditional methods, such as the SIFT algorithm, have been reliable for accuracy and efficiency. However, modern learned feature detectors have emerged that use neural networks for detection and description. These networks are trained using low-level matching scores, which can limit their performance in high-level vision tasks. To improve this, we suggest a new training approach that incorporates the feature detector into an entire vision pipeline, and trains the learnable parameters end-to-end. We use reinforcement learning to overcome the challenges of discrete key point selection and descriptor matching. Our methodology can be applied to any architecture that predicts key point heat maps and descriptors, with no limitations on the task to be learned. We demonstrate this by improving the accuracy of a state-of-the-art learning-based feature detector for the task of relative pose estimation between two images.",1
"Poor sample efficiency is a major limitation of deep reinforcement learning in many domains. This work presents an attention-based method to project neural network inputs into an efficient representation space that is invariant under changes to input ordering. We show that our proposed representation results in an input space that is a factor of $m!$ smaller for inputs of $m$ objects. We also show that our method is able to represent inputs over variable numbers of objects. Our experiments demonstrate improvements in sample efficiency for policy gradient methods on a variety of tasks. We show that our representation allows us to solve problems that are otherwise intractable when using na\""ive approaches.",0
"Deep reinforcement learning faces a significant drawback in numerous fields due to its inadequate sample efficiency. In this study, we introduce an attention-based approach that transforms the inputs of neural networks into a productive representation space that remains constant, regardless of any changes in input sequencing. Our proposed representation results in an input space that is $m!$ times smaller for $m$ objects. Furthermore, we prove that our method can handle inputs with varying numbers of objects. Our tests indicate that our approach enhances the sample efficiency of policy gradient techniques across various tasks. We demonstrate that our representation allows us to resolve problems that would otherwise be challenging to solve using unsophisticated methods.",1
"It is well known that quantifying uncertainty in the action-value estimates is crucial for efficient exploration in reinforcement learning. Ensemble sampling offers a relatively computationally tractable way of doing this using randomized value functions. However, it still requires a huge amount of computational resources for complex problems. In this paper, we present an alternative, computationally efficient way to induce exploration using index sampling. We use an indexed value function to represent uncertainty in our action-value estimates. We first present an algorithm to learn parameterized indexed value function through a distributional version of temporal difference in a tabular setting and prove its regret bound. Then, in a computational point of view, we propose a dual-network architecture, Parameterized Indexed Networks (PINs), comprising one mean network and one uncertainty network to learn the indexed value function. Finally, we show the efficacy of PINs through computational experiments.",0
"The importance of quantifying uncertainty in action-value estimates for effective exploration in reinforcement learning is widely recognized. Ensemble sampling is a feasible method for achieving this by employing randomized value functions, but it is still computationally demanding for intricate problems. This paper presents an alternative approach that is computationally efficient, utilizing index sampling to induce exploration. An indexed value function is employed to represent uncertainty in action-value estimates, and a parameterized indexed value function is learned through a distributional version of temporal difference in a tabular setting. A dual-network architecture called Parameterized Indexed Networks (PINs) is proposed for computational purposes, comprising one mean network and one uncertainty network to learn the indexed value function. The efficacy of PINs is demonstrated through computational experiments.",1
"An agent with an inaccurate model of its environment faces a difficult choice: it can ignore the errors in its model and act in the real world in whatever way it determines is optimal with respect to its model. Alternatively, it can take a more conservative stance and eschew its model in favor of optimizing its behavior solely via real-world interaction. This latter approach can be exceedingly slow to learn from experience, while the former can lead to ""planner overfitting"" - aspects of the agent's behavior are optimized to exploit errors in its model. This paper explores an intermediate position in which the planner seeks to avoid overfitting through a kind of regularization of the plans it considers. We present three different approaches that demonstrably mitigate planner overfitting in reinforcement-learning environments.",0
"When an agent has an incorrect understanding of its surroundings, it must decide whether to disregard the inaccuracies and act in the physical world using its flawed model or take a more cautious approach and focus solely on real-world interactions to optimize its behavior. However, the latter method can be a slow learning process, while the former can result in ""planner overfitting,"" where the agent's actions exploit errors in its model. This research proposes a middle ground, where the planner avoids overfitting by regulating the plans it considers. The paper outlines three distinct approaches that effectively reduce planner overfitting in reinforcement-learning environments.",1
"The problem at the heart of this tutorial consists in modeling the path choice behavior of network users. This problem has been extensively studied in transportation science, where it is known as the route choice problem. In this literature, individuals' choice of paths are typically predicted using discrete choice models. This article is a tutorial on a specific category of discrete choice models called recursive, and it makes three main contributions: First, for the purpose of assisting future research on route choice, we provide a comprehensive background on the problem, linking it to different fields including inverse optimization and inverse reinforcement learning. Second, we formally introduce the problem and the recursive modeling idea along with an overview of existing models, their properties and applications. Third, we extensively analyze illustrative examples from different angles so that a novice reader can gain intuition on the problem and the advantages provided by recursive models in comparison to path-based ones.",0
"The tutorial focuses on modeling the behavior of network users in choosing paths, also known as the route choice problem in transportation science. Discrete choice models are commonly used in this field to predict individuals' path selection. This article specifically examines recursive discrete choice models and presents three main contributions. Firstly, it provides a thorough background on the problem and its connection to other fields like inverse optimization and inverse reinforcement learning to assist future research. Secondly, it introduces the problem and the idea behind recursive modeling, including an overview of existing models, their properties and applications. Lastly, it offers an in-depth analysis of illustrative examples to help novice readers understand the problem and the benefits of recursive models in comparison to path-based ones.",1
"Building home assistant robots has long been a pursuit for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufficient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simplification and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set for articulated objects. Our SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We evaluate state-of-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that our SAPIEN can open a lot of research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment.",0
"For a long time, researchers in vision and robotics have been striving to create home assistant robots. In order to accomplish this, it is crucial to have a simulated environment that is physically realistic, has enough articulated objects, and can be transferred to a real robot. There are already existing environments that meet these requirements, but they vary in their level of simplification and focus. Our team has taken it a step further by creating an environment that specifically supports household tasks for training robot learning algorithms. Our simulated environment, called SAPIEN, is realistic and physics-rich, and contains a large-scale set of articulated objects. With SAPIEN, we are able to conduct various robotic vision and interaction tasks that require a detailed understanding of individual parts. We have tested our environment by evaluating state-of-the-art vision algorithms for part detection and motion attribute recognition, and by demonstrating robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We believe that SAPIEN has the potential to pave the way for many new research directions, such as learning cognition through interaction, part motion discovery, and the creation of a simulated game environment that is ready for robotics.",1
"Our aim is to establish a framework where reinforcement learning (RL) of optimizing interventions retrospectively allows us a regulatory compliant pathway to prospective clinical testing of the learned policies in a clinical deployment. We focus on infections in intensive care units which are one of the major causes of death and difficult to treat because of the complex and opaque patient dynamics, and the clinically debated, highly-divergent set of intervention policies required by each individual patient, yet intensive care units are naturally data rich. In our work, we build on RL approaches in healthcare (""AI Clinicians""), and learn off-policy continuous dosing policy of pharmaceuticals for sepsis treatment using historical intensive care data under partially observable MDPs (POMDPs). POMPDs capture uncertainty in patient state better by taking in all historical information, yielding an efficient representation, which we investigate through ablations. We compensate for the lack of exploration in our retrospective data by evaluating each encountered state with a best-first tree search. We mitigate state distributional shift by optimizing our policy in the vicinity of the clinicians' compound policy. Crucially, we evaluate our model recommendations using not only conventional policy evaluations but a novel framework that incorporates human experts: a model-agnostic pre-clinical evaluation method to estimate the accuracy and uncertainty of clinician's decisions versus our system recommendations when confronted with the same individual patient history (""shadow mode"").",0
"Our goal is to establish a structure in which reinforcement learning (RL) can be used to optimize interventions retrospectively. This will provide us with a compliant pathway for prospective clinical testing of the learned policies in a clinical deployment. We are focusing on infections in intensive care units as they are one of the major causes of death and are difficult to treat due to the complex and opaque patient dynamics. Intensive care units are naturally data rich, but the clinically debated and highly-divergent set of intervention policies required for each individual patient makes treatment challenging. In our research, we are using RL approaches in healthcare (""AI Clinicians"") to learn off-policy continuous dosing pharmaceutical policies for sepsis treatment. We are using historical intensive care data under partially observable MDPs (POMDPs) to capture uncertainty in patient states better by taking in all historical information. We are compensating for the lack of exploration in our retrospective data by evaluating each encountered state with a best-first tree search. We are mitigating state distributional shift by optimizing our policy in the vicinity of the clinicians' compound policy. We are also evaluating our model recommendations using a novel framework that incorporates human experts. This framework is a model-agnostic pre-clinical evaluation method that estimates the accuracy and uncertainty of clinician's decisions versus our system recommendations when confronted with the same individual patient history (""shadow mode"").",1
"Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.",0
"To aid in learning intricate behaviors, learned world models condense an agent's experiences. Although deep learning is making it increasingly possible to learn such models from sensory inputs with high dimensions, there are several potential approaches to extracting behaviors from them. This paper showcases Dreamer, a reinforcement learning agent that can tackle long-horizon tasks based solely on latent imagination from images. Using analytic gradients of learned state values, we efficiently learn behaviors by propagating them back along imagined trajectories within the concise state space of a learned world model. Across 20 challenging visual control tasks, Dreamer surpasses prevailing methods in terms of computation time, data-efficiency, and final performance.",1
"We present an adversarial active exploration for inverse dynamics model learning, a simple yet effective learning scheme that incentivizes exploration in an environment without any human intervention. Our framework consists of a deep reinforcement learning (DRL) agent and an inverse dynamics model contesting with each other. The former collects training samples for the latter, with an objective to maximize the error of the latter. The latter is trained with samples collected by the former, and generates rewards for the former when it fails to predict the actual action taken by the former. In such a competitive setting, the DRL agent learns to generate samples that the inverse dynamics model fails to predict correctly, while the inverse dynamics model learns to adapt to the challenging samples. We further propose a reward structure that ensures the DRL agent to collect only moderately hard samples but not overly hard ones that prevent the inverse model from predicting effectively. We evaluate the effectiveness of our method on several robotic arm and hand manipulation tasks against multiple baseline models. Experimental results show that our method is comparable to those directly trained with expert demonstrations, and superior to the other baselines even without any human priors.",0
"Our study introduces an adversarial active exploration method to learn an inverse dynamics model without any human intervention. This approach employs a deep reinforcement learning (DRL) agent and an inverse dynamics model to compete with one another. The DRL agent gathers training data for the inverse dynamics model while attempting to maximize its error. The inverse dynamics model, on the other hand, is trained with the data collected by the DRL agent and rewards the agent when it fails to predict the actual action. This competitive environment motivates the DRL agent to generate challenging samples that the inverse dynamics model cannot predict accurately, while the inverse dynamics model adapts to these samples. We also suggest a reward system that ensures the DRL agent only gathers moderately difficult samples, preventing the inverse dynamics model from being ineffective. We assess the effectiveness of our method on several robotic arm and hand manipulation tasks and compare it with multiple baseline models. The results indicate that our technique is comparable to those trained directly with expert demonstrations and superior to other baselines without any human priors.",1
"Reinforcement learning systems require good representations to work well. For decades practical success in reinforcement learning was limited to small domains. Deep reinforcement learning systems, on the other hand, are scalable, not dependent on domain specific prior knowledge and have been successfully used to play Atari, in 3D navigation from pixels, and to control high degree of freedom robots. Unfortunately, the performance of deep reinforcement learning systems is sensitive to hyper-parameter settings and architecture choices. Even well tuned systems exhibit significant instability both within a trial and across experiment replications. In practice, significant expertise and trial and error are usually required to achieve good performance. One potential source of the problem is known as catastrophic interference: when later training decreases performance by overriding previous learning. Interestingly, the powerful generalization that makes Neural Networks (NN) so effective in batch supervised learning might explain the challenges when applying them in reinforcement learning tasks. In this paper, we explore how online NN training and interference interact in reinforcement learning. We find that simply re-mapping the input observations to a high-dimensional space improves learning speed and parameter sensitivity. We also show this preprocessing reduces interference in prediction tasks. More practically, we provide a simple approach to NN training that is easy to implement, and requires little additional computation. We demonstrate that our approach improves performance in both prediction and control with an extensive batch of experiments in classic control domains.",0
"Good representations are essential for reinforcement learning systems to function effectively. Although reinforcement learning was previously limited to small domains, deep reinforcement learning systems have proven to be scalable and successful in playing Atari, navigating 3D environments, and controlling complex robots without relying on domain-specific prior knowledge. However, the performance of these systems is highly sensitive to hyper-parameter settings and architecture choices, and even well-tuned systems exhibit significant instability and require extensive trial and error. This may be due to catastrophic interference, where later training overrides previous learning and decreases performance. Online neural network training and interference may exacerbate these challenges. In this study, we investigate the interaction between online neural network training and interference in reinforcement learning and demonstrate that mapping input observations to a high-dimensional space can improve learning speed and reduce interference in prediction tasks. Our approach is easy to implement and requires minimal additional computation, and we show improved performance in both prediction and control tasks through a series of experiments in classic control domains.",1
"Deep reinforcement learning can learn effective policies for a wide range of tasks, but is notoriously difficult to use due to instability and sensitivity to hyperparameters. The reasons for this remain unclear. When using standard supervised methods (e.g., for bandits), on-policy data collection provides ""hard negatives"" that correct the model in precisely those states and actions that the policy is likely to visit. We call this phenomenon ""corrective feedback."" We show that bootstrapping-based Q-learning algorithms do not necessarily benefit from this corrective feedback, and training on the experience collected by the algorithm is not sufficient to correct errors in the Q-function. In fact, Q-learning and related methods can exhibit pathological interactions between the distribution of experience collected by the agent and the policy induced by training on that experience, leading to potential instability, sub-optimal convergence, and poor results when learning from noisy, sparse or delayed rewards. We demonstrate the existence of this problem, both theoretically and empirically. We then show that a specific correction to the data distribution can mitigate this issue. Based on these observations, we propose a new algorithm, DisCor, which computes an approximation to this optimal distribution and uses it to re-weight the transitions used for training, resulting in substantial improvements in a range of challenging RL settings, such as multi-task learning and learning from noisy reward signals. Blog post presenting a summary of this work is available at: https://bair.berkeley.edu/blog/2020/03/16/discor/.",0
"Although deep reinforcement learning can effectively learn policies for various tasks, it is difficult to use due to instability and sensitivity to hyperparameters, and the reasons for this are unclear. In contrast to standard supervised methods, on-policy data collection provides ""corrective feedback,"" which helps correct the model in states and actions that the policy is likely to visit. However, bootstrapping-based Q-learning algorithms do not necessarily benefit from this feedback, and training on the collected experience does not correct Q-function errors. Q-learning and related methods can even have problematic interactions between the distribution of experience and the induced policy, leading to potential instability, sub-optimal convergence, and poor results in noisy, sparse, or delayed reward scenarios. This issue can be mitigated by correcting the data distribution, which is the basis for the DisCor algorithm proposed in this work. DisCor approximates an optimal distribution to re-weight the transitions used for training, resulting in significant improvements in challenging RL settings such as multi-task learning and learning from noisy reward signals. A summary of this work is available at https://bair.berkeley.edu/blog/2020/03/16/discor/.",1
"Reinforcement Learning (RL) aims at learning an optimal behavior policy from its own experiments and not rule-based control methods. However, there is no RL algorithm yet capable of handling a task as difficult as urban driving. We present a novel technique, coined implicit affordances, to effectively leverage RL for urban driving thus including lane keeping, pedestrians and vehicles avoidance, and traffic light detection. To our knowledge we are the first to present a successful RL agent handling such a complex task especially regarding the traffic light detection. Furthermore, we have demonstrated the effectiveness of our method by winning the Camera Only track of the CARLA challenge.",0
"The goal of Reinforcement Learning (RL) is to acquire the best behavior policy through self-experimentation, rather than relying on rule-based control techniques. However, no RL algorithm has yet been developed that can tackle the challenge of urban driving. Our approach, which we call implicit affordances, is a new method that effectively employs RL for urban driving, encompassing tasks such as lane keeping, pedestrian and vehicle avoidance, and traffic light detection. To our knowledge, we are the first to successfully implement an RL agent that can handle such a complex task, particularly with regard to traffic light detection. Additionally, our method's efficacy has been demonstrated by our victory in the Camera Only track of the CARLA challenge.",1
"For effective matching of resources (e.g., taxis, food, bikes, shopping items) to customer demand, aggregation systems have been extremely successful. In aggregation systems, a central entity (e.g., Uber, Food Panda, Ofo) aggregates supply (e.g., drivers, delivery personnel) and matches demand to supply on a continuous basis (sequential decisions). Due to the objective of the central entity to maximize its profits, individual suppliers get sacrificed thereby creating incentive for individuals to leave the system. In this paper, we consider the problem of learning approximate equilibrium solutions (win-win solutions) in aggregation systems, so that individuals have an incentive to remain in the aggregation system.   Unfortunately, such systems have thousands of agents and have to consider demand uncertainty and the underlying problem is a (Partially Observable) Stochastic Game. Given the significant complexity of learning or planning in a stochastic game, we make three key contributions: (a) To exploit infinitesimally small contribution of each agent and anonymity (reward and transitions between agents are dependent on agent counts) in interactions, we represent this as a Multi-Agent Reinforcement Learning (MARL) problem that builds on insights from non-atomic congestion games model; (b) We provide a novel variance reduction mechanism for moving joint solution towards Nash Equilibrium that exploits the infinitesimally small contribution of each agent; and finally (c) We provide detailed results on three different domains to demonstrate the utility of our approach in comparison to state-of-the-art methods.",0
"Aggregation systems have proven to be effective in matching resources such as taxis, food, bikes, and shopping items to customer demand. These systems are run by a central entity like Uber, Food Panda, or Ofo, which aggregates supply from drivers and delivery personnel to match demand on a continuous basis. However, the central entity's goal of maximizing profits often results in individual suppliers being sacrificed, leading to a lack of incentive for them to remain in the system. To address this problem, we aim to learn approximate equilibrium solutions that benefit both the central entity and individual suppliers. Unfortunately, the complexity of such systems, involving thousands of agents and demand uncertainty, makes it a challenging (Partially Observable) Stochastic Game. In this paper, we present three key contributions to overcome these challenges: (a) We represent the problem as a Multi-Agent Reinforcement Learning (MARL) problem that builds on non-atomic congestion games model; (b) We provide a novel variance reduction mechanism for moving joint solution towards Nash Equilibrium that exploits the infinitesimally small contribution of each agent; (c) We demonstrate the effectiveness of our approach in three different domains compared to state-of-the-art methods.",1
"Model-based reinforcement learning (MBRL) aims to learn a dynamic model to reduce the number of interactions with real-world environments. However, due to estimation error, rollouts in the learned model, especially those of long horizons, fail to match the ones in real-world environments. This mismatching has seriously impacted the sample complexity of MBRL. The phenomenon can be attributed to the fact that previous works employ supervised learning to learn the one-step transition models, which has inherent difficulty ensuring the matching of distributions from multi-step rollouts. Based on the claim, we propose to learn the transition model by matching the distributions of multi-step rollouts sampled from the transition model and the real ones via WGAN. We theoretically show that matching the two can minimize the difference of cumulative rewards between the real transition and the learned one. Our experiments also show that the proposed Model Imitation method can compete or outperform the state-of-the-art in terms of sample complexity and average return.",0
"The objective of Model-based reinforcement learning (MBRL) is to minimize interactions with real-world environments by learning a dynamic model. However, the rollouts in the learned model, especially for those with long horizons, fail to correspond to those in real-world environments due to estimation error. This mismatch severely affects the sample complexity of MBRL. The reason behind this issue is that previous studies have used supervised learning to learn the one-step transition models, which makes it difficult to ensure the matching of distributions from multi-step rollouts. Consequently, we propose a Model Imitation method that learns the transition model by matching the distributions of multi-step rollouts sampled from the transition model and real ones via WGAN. We theoretically prove that by matching the two, the difference in cumulative rewards between real and learned transitions can be minimized. Our experiments indicate that the proposed method can either outperform or compete with the state-of-the-art in terms of sample complexity and average return.",1
"The vast majority of visual animals actively control their eyes, heads, and/or bodies to direct their gaze toward different parts of their environment. In contrast, recent applications of reinforcement learning in robotic manipulation employ cameras as passive sensors. These are carefully placed to view a scene from a fixed pose. Active perception allows animals to gather the most relevant information about the world and focus their computational resources where needed. It also enables them to view objects from different distances and viewpoints, providing a rich visual experience from which to learn abstract representations of the environment. Inspired by the primate visual-motor system, we present a framework that leverages the benefits of active perception to accomplish manipulation tasks. Our agent uses viewpoint changes to localize objects, to learn state representations in a self-supervised manner, and to perform goal-directed actions. We apply our model to a simulated grasping task with a 6-DoF action space. Compared to its passive, fixed-camera counterpart, the active model achieves 8% better performance in targeted grasping. Compared to vanilla deep Q-learning algorithms, our model is at least four times more sample-efficient, highlighting the benefits of both active perception and representation learning.",0
"The majority of animals that rely on vision actively control their eyes, heads, or bodies to direct their gaze towards different parts of their surroundings. Conversely, in recent applications of reinforcement learning in robotic manipulation, cameras are used as passive sensors that are placed in a fixed position to observe a scene. Active perception allows animals to gather the most relevant information about their environment and focus their computational resources where needed. Additionally, it enables them to view objects from varying distances and perspectives, leading to a comprehensive visual experience that facilitates learning abstract representations of their surroundings. Our framework is inspired by the primate visual-motor system and leverages the advantages of active perception to perform manipulation tasks. Our agent changes viewpoints to locate objects, self-supervisedly learn state representations, and execute goal-oriented actions. We apply our model to a simulated grasping task using a 6-DoF action space. Compared to a passive, fixed-camera model, the active model achieves an 8% better performance in targeted grasping. Our model is at least four times more sample-efficient than vanilla deep Q-learning algorithms, demonstrating the advantages of active perception and representation learning.",1
"We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.",0
"Our research delves into the use of deep reinforcement learning techniques for multi-agent environments. Initially, we examine the obstacles traditional algorithms face in such scenarios: Q-learning struggles due to the environment's inherent non-stationarity, whereas policy gradient experiences a rise in variance as the number of agents increases. We then propose an actor-critic method adaptation that accounts for the action policies of other agents, enabling successful learning of complex multi-agent coordination policies. Furthermore, we introduce a training approach that employs a set of policies for each agent, resulting in more resilient multi-agent policies. Our method demonstrates superior performance compared to existing approaches in both cooperative and competitive settings, with agent populations discovering diverse physical and informational coordination strategies.",1
"In this work, we first describe a framework for the application of Reinforcement Learning (RL) control to a radar system that operates in a congested spectral setting. We then compare the utility of several RL algorithms through a discussion of experiments performed on Commercial off-the-shelf (COTS) hardware. Each RL technique is evaluated in terms of convergence, radar detection performance achieved in a congested spectral environment, and the ability to share 100MHz spectrum with an uncooperative communications system. We examine policy iteration, which solves an environment posed as a Markov Decision Process (MDP) by directly solving for a stochastic mapping between environmental states and radar waveforms, as well as Deep RL techniques, which utilize a form of Q-Learning to approximate a parameterized function that is used by the radar to select optimal actions. We show that RL techniques are beneficial over a Sense-and-Avoid (SAA) scheme and discuss the conditions under which each approach is most effective.",0
"This study outlines a framework for implementing Reinforcement Learning (RL) control in a radar system operating in a congested spectral environment. The effectiveness of various RL algorithms is compared through experiments carried out on readily available Commercial off-the-shelf (COTS) equipment. The evaluation of each algorithm is based on its ability to converge, the radar detection performance achieved in a congested spectral environment, and its capability to share 100MHz spectrum with an uncooperative communications system. Two types of RL techniques are explored: policy iteration, which directly solves for a stochastic mapping between environmental states and radar waveforms, and Deep RL techniques, which use a form of Q-Learning to approximate a parameterized function for optimal action selection. The study concludes that RL techniques are superior to a Sense-and-Avoid (SAA) scheme, and the most effective approach depends on specific conditions.",1
"In this work, we investigate the application of Taylor expansions in reinforcement learning. In particular, we propose Taylor expansion policy optimization, a policy optimization formalism that generalizes prior work (e.g., TRPO) as a first-order special case. We also show that Taylor expansions intimately relate to off-policy evaluation. Finally, we show that this new formulation entails modifications which improve the performance of several state-of-the-art distributed algorithms.",0
"The focus of our research is to examine how Taylor expansions can be utilized in reinforcement learning. We introduce a policy optimization framework called Taylor expansion policy optimization, which extends previous methods such as TRPO by incorporating first-order special cases. Additionally, we demonstrate the close connection between Taylor expansions and off-policy evaluation. To conclude, we illustrate how our new approach involves adjustments that enhance the effectiveness of various state-of-the-art distributed algorithms.",1
"Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.",0
"Numerous domains have demonstrated the effectiveness of deep learning, such as acoustics, images, and natural language processing. Nonetheless, applying deep learning to graph data is not straightforward due to graphs' unique characteristics. Nonetheless, considerable research has been dedicated to implementing deep learning techniques on graphs, resulting in significant advancements in graph analysis. This survey offers a comprehensive review of the various types of deep learning methods utilized on graphs. The existing methods are categorized into five groups based on their architecture and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. The survey provides a systematic overview of these methods, mainly following their development history, including an analysis of their differences and compositions. Finally, the survey briefly outlines the applications where these methods have been employed and suggests potential future research directions.",1
"In this paper, we introduce the TrojAI software framework, an open source set of Python tools capable of generating triggered (poisoned) datasets and associated deep learning (DL) models with trojans at scale. We utilize the developed framework to generate a large set of trojaned MNIST classifiers, as well as demonstrate the capability to produce a trojaned reinforcement-learning model using vector observations. Results on MNIST show that the nature of the trigger, training batch size, and dataset poisoning percentage all affect successful embedding of trojans. We test Neural Cleanse against the trojaned MNIST models and successfully detect anomalies in the trained models approximately $18\%$ of the time. Our experiments and workflow indicate that the TrojAI software framework will enable researchers to easily understand the effects of various configurations of the dataset and training hyperparameters on the generated trojaned deep learning model, and can be used to rapidly and comprehensively test new trojan detection methods.",0
"The TrojAI software framework is introduced in this paper as a Python toolset that is open source and can produce triggered datasets and deep learning models with trojans at scale. The framework is utilized to create numerous trojaned MNIST classifiers, as well as to demonstrate its ability to generate a trojaned reinforcement-learning model that uses vector observations. Our research shows that the successful embedding of trojans is affected by the type of trigger, training batch size, and percentage of dataset poisoning. We tested Neural Cleanse on the trojaned MNIST models and successfully detected anomalies in the trained models about 18% of the time. Our experiments and workflow highlight the usefulness of the TrojAI software framework for researchers to easily comprehend the effects of various dataset and training hyperparameters on the generated trojaned deep learning models. Additionally, the framework can be used to rapidly and comprehensively test new trojan detection methods.",1
"This paper presents a general mean-field game (GMFG) framework for simultaneous learning and decision-making in stochastic games with a large population. It first establishes the existence of a unique Nash Equilibrium to this GMFG, and demonstrates that naively combining Q-learning with the fixed-point approach in classical MFGs yields unstable algorithms. It then proposes value-based and policy-based reinforcement learning algorithms (GMF-P and GMF-P respectively) with smoothed policies, with analysis of convergence property and computational complexity. The experiments on repeated Ad auction problems demonstrate that GMF-V-Q, a specific GMF-V algorithm based on Q-learning, is efficient and robust in terms of convergence and learning accuracy. Moreover, its performance is superior in convergence, stability, and learning ability, when compared with existing algorithms for multi-agent reinforcement learning.",0
"In this paper, a framework called general mean-field game (GMFG) is presented for both learning and decision-making in stochastic games involving a large number of players. The paper establishes that there is a single Nash Equilibrium for this GMFG and highlights the instability of combining Q-learning with the fixed-point approach used in classical MFGs. The paper then proposes two reinforcement learning algorithms (GMF-P and GMF-V) that use smoothed policies, with an analysis of their convergence and computational complexity. The experiments conducted on repeated Ad auction problems demonstrate that the GMF-V-Q algorithm, based on Q-learning, is both efficient and robust in terms of convergence and learning accuracy. Additionally, it outperforms other existing algorithms for multi-agent reinforcement learning in terms of convergence, stability, and learning ability.",1
"Sample inefficiency of deep reinforcement learning methods is a major obstacle for their use in real-world applications. In this work, we show how human demonstrations can improve final performance of agents on the Minecraft minigame ObtainDiamond with only 8M frames of environment interaction. We propose a training procedure where policy networks are first trained on human data and later fine-tuned by reinforcement learning. Using a policy exploitation mechanism, experience replay and an additional loss against catastrophic forgetting, our best agent was able to achieve a mean score of 48. Our proposed solution placed 3rd in the NeurIPS MineRL Competition for Sample-Efficient Reinforcement Learning.",0
"The inefficiency of deep reinforcement learning methods poses a significant challenge to their practical application. Our study demonstrates how employing human demonstrations can enhance agent performance on the ObtainDiamond minigame in Minecraft, with a mere 8M frames of environment interaction. We introduce a training technique that trains policy networks on human data, followed by reinforcement learning fine-tuning. By adopting a strategy of policy exploitation, experience replay, and an additional loss to combat catastrophic forgetting, we achieved a mean score of 48, and our approach was ranked 3rd in the NeurIPS MineRL Competition for Sample-Efficient Reinforcement Learning.",1
"Path-based relational reasoning over knowledge graphs has become increasingly popular due to a variety of downstream applications such as question answering in dialogue systems, fact prediction, and recommender systems. In recent years, reinforcement learning (RL) has provided solutions that are more interpretable and explainable than other deep learning models. However, these solutions still face several challenges, including large action space for the RL agent and accurate representation of entity neighborhood structure. We address these problems by introducing a type-enhanced RL agent that uses the local neighborhood information for efficient path-based reasoning over knowledge graphs. Our solution uses graph neural network (GNN) for encoding the neighborhood information and utilizes entity types to prune the action space. Experiments on real-world dataset show that our method outperforms state-of-the-art RL methods and discovers more novel paths during the training procedure.",0
"The use of path-based relational reasoning on knowledge graphs has gained popularity because of its usefulness in various applications such as dialogue system question answering, fact prediction, and recommender systems. While reinforcement learning (RL) has been successful in providing more interpretable models than other deep learning methods, it still faces challenges such as a large action space and difficulty representing entity neighborhoods. Our proposed solution is a type-enhanced RL agent that utilizes local neighborhood information for efficient path-based reasoning on knowledge graphs. We use graph neural networks (GNN) to encode neighborhood information and entity types to reduce the action space. Our experiments on real-world data demonstrate that our method outperforms the current state-of-the-art RL methods and discovers more unique paths during the training process.",1
"Recent advances in deep reinforcement learning require a large amount of training data and generally result in representations that are often over specialized to the target task. In this work, we present a methodology to study the underlying potential causes for this specialization. We use the recently proposed projection weighted Canonical Correlation Analysis (PWCCA) to measure the similarity of visual representations learned in the same environment by performing different tasks.   We then leverage our proposed methodology to examine the task dependence of visual representations learned on related but distinct embodied navigation tasks. Surprisingly, we find that slight differences in task have no measurable effect on the visual representation for both SqueezeNet and ResNet architectures. We then empirically demonstrate that visual representations learned on one task can be effectively transferred to a different task.",0
"Advanced developments in deep reinforcement learning necessitate a significant amount of data for training and typically lead to specialized representations that are tailored to the target task. In this study, we present a technique to investigate the underlying reasons for this specialization. By utilizing projection weighted Canonical Correlation Analysis (PWCCA), which is a recent proposal, we measure the similarity of visual representations acquired from performing different tasks in the same environment. Our proposed methodology is used to scrutinize the task dependence of visual representations obtained from related yet distinct embodied navigation tasks. Surprisingly, we discover that even small differences in tasks do not noticeably influence the visual representation for both SqueezeNet and ResNet designs. Furthermore, we demonstrate through empirical evidence that visual representations acquired from one task can be efficiently transferred to a different one.",1
"The landmark achievements of AlphaGo Zero have created great research interest into self-play in reinforcement learning. In self-play, Monte Carlo Tree Search is used to train a deep neural network, that is then used in tree searches. Training itself is governed by many hyperparameters.There has been surprisingly little research on design choices for hyper-parameter values and loss-functions, presumably because of the prohibitive computational cost to explore the parameter space. In this paper, we investigate 12 hyper-parameters in an AlphaZero-like self-play algorithm and evaluate how these parameters contribute to training. We use small games, to achieve meaningful exploration with moderate computational effort. The experimental results show that training is highly sensitive to hyper-parameter choices. Through multi-objective analysis we identify 4 important hyper-parameters to further assess. To start, we find surprising results where too much training can sometimes lead to lower performance. Our main result is that the number of self-play iterations subsumes MCTS-search simulations, game-episodes, and training epochs. The intuition is that these three increase together as self-play iterations increase, and that increasing them individually is sub-optimal. A consequence of our experiments is a direct recommendation for setting hyper-parameter values in self-play: the overarching outer-loop of self-play iterations should be maximized, in favor of the three inner-loop hyper-parameters, which should be set at lower values. A secondary result of our experiments concerns the choice of optimization goals, for which we also provide recommendations.",0
"The remarkable accomplishments of AlphaGo Zero have sparked a keen interest in research on self-play in reinforcement learning. In order to train a deep neural network using self-play, Monte Carlo Tree Search is utilized during tree searches. The training process is governed by numerous hyperparameters, however, there has been minimal research on selecting hyperparameter values and loss functions, possibly due to the high computational cost of exploring the parameter space. In this study, we investigate 12 hyperparameters in an AlphaZero-like self-play algorithm to assess the impact of these parameters on training. To achieve meaningful exploration with moderate computational effort, we utilize small games. Our results indicate that hyperparameter selection is critical to training, and we identify four key hyperparameters to further evaluate through multi-objective analysis. We discover that excessive training can sometimes result in lower performance, and that the number of self-play iterations is more important than MCTS-search simulations, game-episodes, and training epochs. Our experiments lead us to recommend maximizing the overarching outer-loop of self-play iterations and setting the three inner-loop hyperparameters at lower values. We also provide recommendations for selecting optimization goals based on our findings.",1
"A common technique to improve learning performance in deep reinforcement learning (DRL) and many other machine learning algorithms is to run multiple learning agents in parallel. A neglected component in the development of these algorithms has been how best to arrange the learning agents involved to improve distributed search. Here we draw upon results from the networked optimization literatures suggesting that arranging learning agents in communication networks other than fully connected topologies (the implicit way agents are commonly arranged in) can improve learning. We explore the relative performance of four popular families of graphs and observe that one such family (Erdos-Renyi random graphs) empirically outperforms the de facto fully-connected communication topology across several DRL benchmark tasks. Additionally, we observe that 1000 learning agents arranged in an Erdos-Renyi graph can perform as well as 3000 agents arranged in the standard fully-connected topology, showing the large learning improvement possible when carefully designing the topology over which agents communicate. We complement these empirical results with a theoretical investigation of why our alternate topologies perform better. Overall, our work suggests that distributed machine learning algorithms could be made more effective if the communication topology between learning agents was optimized.",0
"To enhance learning performance in deep reinforcement learning (DRL) and other machine learning algorithms, it is common to run multiple learning agents simultaneously. However, the arrangement of these agents has been overlooked in algorithm development, despite its potential to improve distributed search. Drawing on networked optimization literature, we propose that learning agents should be arranged in communication networks other than fully connected topologies. We compare the performance of four graph families and find that Erdos-Renyi random graphs outperform fully-connected communication topologies across several DRL benchmark tasks. Furthermore, we observe that 1000 agents arranged in an Erdos-Renyi graph can perform as well as 3000 agents arranged in a standard fully-connected topology. Our study also provides a theoretical explanation for the improved performance of alternate topologies. Overall, our results suggest that optimizing the communication topology between learning agents can significantly improve distributed machine learning algorithms.",1
"We hypothesize that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent's life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. We formulate the problem of generating curious behavior as one of meta-learning: an outer loop will search over a space of curiosity mechanisms that dynamically adapt the agent's reward signal, and an inner loop will perform standard reinforcement learning using the adapted reward signal. However, current meta-RL methods based on transferring neural network weights have only generalized between very similar tasks. To broaden the generalization, we instead propose to meta-learn algorithms: pieces of code similar to those designed by humans in ML papers. Our rich language of programs combines neural networks with other building blocks such as buffers, nearest-neighbor modules and custom loss functions. We demonstrate the effectiveness of the approach empirically, finding two novel curiosity algorithms that perform on par or better than human-designed published curiosity algorithms in domains as disparate as grid navigation with image inputs, acrobot, lunar lander, ant and hopper.",0
"Our theory suggests that curiosity is an evolutionary mechanism that drives valuable exploration in an agent's early life, exposing it to experiences that lead to high rewards throughout its lifespan. We approach the challenge of generating curious behavior using meta-learning, where an outer loop searches for curiosity mechanisms that dynamically adapt the agent's reward signal, while an inner loop applies standard reinforcement learning with the adapted reward signal. However, current meta-RL methods have only been effective in similar tasks, so we propose to meta-learn algorithms rather than neural network weights. Our language of programs includes neural networks, buffers, nearest-neighbor modules, and custom loss functions. Through empirical testing, we discovered two novel curiosity algorithms that perform on par or better than existing human-designed curiosity algorithms in diverse domains such as grid navigation, acrobot, lunar lander, ant, and hopper.",1
"This paper proposes the first-ever algorithmic framework for tuning hyper-parameters of stochastic optimization algorithm based on reinforcement learning. Hyper-parameters impose significant influences on the performance of stochastic optimization algorithms, such as evolutionary algorithms (EAs) and meta-heuristics. Yet, it is very time-consuming to determine optimal hyper-parameters due to the stochastic nature of these algorithms. We propose to model the tuning procedure as a Markov decision process, and resort the policy gradient algorithm to tune the hyper-parameters. Experiments on tuning stochastic algorithms with different kinds of hyper-parameters (continuous and discrete) for different optimization problems (continuous and discrete) show that the proposed hyper-parameter tuning algorithms do not require much less running times of the stochastic algorithms than bayesian optimization method. The proposed framework can be used as a standard tool for hyper-parameter tuning in stochastic algorithms.",0
"This paper introduces an algorithmic framework for fine-tuning hyper-parameters in stochastic optimization algorithms through reinforcement learning. Hyper-parameters significantly impact the performance of algorithms such as evolutionary algorithms and meta-heuristics. However, as these algorithms are stochastic in nature, determining optimal hyper-parameters is a time-consuming process. We suggest modeling the tuning procedure as a Markov decision process and using the policy gradient algorithm to adjust the hyper-parameters. Our experiments on different types of hyper-parameters (discrete and continuous) for various optimization problems (discrete and continuous) demonstrate that our proposed algorithm requires significantly less time than the Bayesian optimization method. This framework can be used as a standardized tool for hyper-parameter tuning in stochastic algorithms.",1
"The advancement of artificial intelligence has cast a new light on the development of optimization algorithm. This paper proposes to learn a two-phase (including a minimization phase and an escaping phase) global optimization algorithm for smooth non-convex functions. For the minimization phase, a model-driven deep learning method is developed to learn the update rule of descent direction, which is formalized as a nonlinear combination of historical information, for convex functions. We prove that the resultant algorithm with the proposed adaptive direction guarantees convergence for convex functions. Empirical study shows that the learned algorithm significantly outperforms some well-known classical optimization algorithms, such as gradient descent, conjugate descent and BFGS, and performs well on ill-posed functions. The escaping phase from local optimum is modeled as a Markov decision process with a fixed escaping policy. We further propose to learn an optimal escaping policy by reinforcement learning. The effectiveness of the escaping policies is verified by optimizing synthesized functions and training a deep neural network for CIFAR image classification. The learned two-phase global optimization algorithm demonstrates a promising global search capability on some benchmark functions and machine learning tasks.",0
"The development of optimization algorithms has been given a new perspective due to the progress in artificial intelligence. This research proposes a global optimization algorithm for smooth non-convex functions consisting of two phases: minimization and escaping. The minimization phase employs a model-driven deep learning approach to learn the descent direction update rule, which combines historical information to obtain a nonlinear combination for convex functions, ensuring convergence with the proposed adaptive direction. Results show that the learned algorithm surpasses classical optimization algorithms, including gradient descent, conjugate descent, and BFGS, and performs well on ill-posed functions. For the escaping phase, a fixed policy Markov decision process is employed, and an optimal escaping policy is learned through reinforcement learning. The effectiveness of the escaping policies is verified by optimizing synthesized functions and training a deep neural network for CIFAR image classification. The learned two-phase global optimization algorithm presents a promising global search capability in benchmark functions and machine learning tasks.",1
"In this paper, we study a transfer reinforcement learning problem where the state transitions and rewards are affected by the environmental context. Specifically, we consider a demonstrator agent that has access to a context-aware policy and can generate transition and reward data based on that policy. These data constitute the experience of the demonstrator. Then, the goal is to transfer this experience, excluding the underlying contextual information, to a learner agent that does not have access to the environmental context, so that they can learn a control policy using fewer samples. It is well known that, disregarding the causal effect of the contextual information, can introduce bias in the transition and reward models estimated by the learner, resulting in a learned suboptimal policy. To address this challenge, in this paper, we develop a method to obtain causal bounds on the transition and reward functions using the demonstrator's data, which we then use to obtain causal bounds on the value functions. Using these value function bounds, we propose new Q learning and UCB-Q learning algorithms that converge to the true value function without bias. We provide numerical experiments for robot motion planning problems that validate the proposed value function bounds and demonstrate that the proposed algorithms can effectively make use of the data from the demonstrator to accelerate the learning process of the learner.",0
"This paper examines a transfer reinforcement learning problem that involves state transitions and rewards being influenced by the environmental context. We focus on a scenario whereby a demonstrator agent with context-aware policies generates transition and reward data. Our aim is to transfer this experience to a learner agent that lacks access to environmental context, which would enable them to learn a control policy with fewer samples. It is important to note that ignoring the causal effect of contextual information can result in bias in the transition and reward models, which may lead to suboptimal policy learning. To overcome this challenge, we introduce a method to obtain causal bounds on the transition and reward functions using the demonstrator's data. We then use these bounds to obtain causal bounds on the value functions, which we use to develop new unbiased Q learning and UCB-Q learning algorithms that converge to the true value function. We conducted numerical experiments on robot motion planning problems, which validated our proposed value function bounds and demonstrated the effectiveness of our algorithms in accelerating the learner's learning process using the demonstrator's data.",1
"Reinforcement Learning (RL) in various decision-making tasks of machine learning provides effective results with an agent learning from a stand-alone reward function. However, it presents unique challenges with large amounts of environment states and action spaces, as well as in the determination of rewards. This complexity, coming from high dimensionality and continuousness of the environments considered herein, calls for a large number of learning trials to learn about the environment through Reinforcement Learning. Imitation Learning (IL) offers a promising solution for those challenges using a teacher. In IL, the learning process can take advantage of human-sourced assistance and/or control over the agent and environment. A human teacher and an agent learner are considered in this study. The teacher takes part in the agent training towards dealing with the environment, tackling a specific objective, and achieving a predefined goal. Within that paradigm, however, existing IL approaches have the drawback of expecting extensive demonstration information in long-horizon problems. This paper proposes a novel approach combining IL with different types of RL methods, namely state action reward state action (SARSA) and asynchronous advantage actor-critic (A3C) agents, to overcome the problems of both stand-alone systems. It is addressed how to effectively leverage the teacher feedback, be it direct binary or indirect detailed for the agent learner to learn sequential decision-making policies. The results of this study on various OpenAI Gym environments show that this algorithmic method can be incorporated with different combinations, significantly decreases both human endeavor and tedious exploration process.",0
"Reinforcement Learning (RL) has proven to provide effective results in various decision-making tasks of machine learning by allowing an agent to learn from a stand-alone reward function. However, the complexity of large amounts of environment states and action spaces, as well as the determination of rewards, presents unique challenges, especially in environments with high dimensionality and continuousness. This calls for a large number of learning trials to learn about the environment through RL. Imitation Learning (IL), on the other hand, offers a promising solution to these challenges by using a teacher to provide human-sourced assistance and control over the agent and environment. In this study, a human teacher and an agent learner are considered to tackle a specific objective and achieve a predefined goal. However, existing IL approaches have the drawback of requiring extensive demonstration information in long-horizon problems. To overcome this, this paper proposes a novel approach that combines IL with different types of RL methods, namely SARSA and A3C agents, to effectively leverage teacher feedback, be it direct binary or indirect detailed, for the agent learner to learn sequential decision-making policies. The results of this study show that this algorithmic method can significantly decrease both human endeavor and tedious exploration processes and can be incorporated with different combinations in various OpenAI Gym environments.",1
"Despite the wealth of research into provably efficient reinforcement learning algorithms, most works focus on tabular representation and thus struggle to handle exponentially or infinitely large state-action spaces. In this paper, we consider episodic reinforcement learning with a continuous state-action space which is assumed to be equipped with a natural metric that characterizes the proximity between different states and actions. We propose ZoomRL, an online algorithm that leverages ideas from continuous bandits to learn an adaptive discretization of the joint space by zooming in more promising and frequently visited regions while carefully balancing the exploitation-exploration trade-off. We show that ZoomRL achieves a worst-case regret $\tilde{O}(H^{\frac{5}{2}} K^{\frac{d+1}{d+2}})$ where $H$ is the planning horizon, $K$ is the number of episodes and $d$ is the covering dimension of the space with respect to the metric. Moreover, our algorithm enjoys improved metric-dependent guarantees that reflect the geometry of the underlying space. Finally, we show that our algorithm is robust to small misspecification errors.",0
"Although many studies have explored reinforcement learning algorithms that are provably efficient, most of them concentrate on tabular representation which makes it challenging to handle state-action spaces that are exponentially or infinitely large. This paper, however, tackles episodic reinforcement learning with a continuous state-action space that comes with a natural metric to indicate the proximity between different actions and states. The authors propose an online algorithm called ZoomRL that employs concepts from continuous bandits to learn an adaptive discretization of the joint space. ZoomRL zooms in on regions that are more promising and frequently visited while balancing the trade-off between exploitation and exploration. The paper shows that ZoomRL achieves a worst-case regret of $\tilde{O}(H^{\frac{5}{2}} K^{\frac{d+1}{d+2}})$, where $H$ is the planning horizon, $K$ is the number of episodes, and $d$ is the covering dimension of the space with respect to the metric. Furthermore, the algorithm provides improved metric-dependent guarantees that reflect the geometry of the underlying space. Finally, the paper demonstrates that the algorithm is robust to small misspecification errors.",1
"The in-memory computing paradigm with emerging memory devices has been recently shown to be a promising way to accelerate deep learning. Resistive processing unit (RPU) has been proposed to enable the vector-vector outer product in a crossbar array using a stochastic train of identical pulses to enable one-shot weight update, promising intense speed-up in matrix multiplication operations, which form the bulk of training neural networks. However, the performance of the system suffers if the device does not satisfy the condition of linear conductance change over around 1,000 conductance levels. This is a challenge for nanoscale memories. Recently, Charge Trap Flash (CTF) memory was shown to have a large number of levels before saturation, but variable non-linearity. In this paper, we explore the trade-off between the range of conductance change and linearity. We show, through simulations, that at an optimum choice of the range, our system performs nearly as well as the models trained using exact floating point operations, with less than 1% reduction in the performance. Our system reaches an accuracy of 97.9% on MNIST dataset, 89.1% and 70.5% accuracy on CIFAR-10 and CIFAR-100 datasets (using pre-extracted features). We also show its use in reinforcement learning, where it is used for value function approximation in Q-Learning, and learns to complete an episode the mountain car control problem in around 146 steps. Benchmarked to state-of-the-art, the CTF based RPU shows best in class performance to enable software equivalent performance.",0
"Recently, the in-memory computing approach utilizing emerging memory devices has been deemed a promising method for accelerating deep learning. To enable one-shot weight update and speed up matrix multiplication operations, a Resistive Processing Unit (RPU) has been suggested, which uses a stochastic train of identical pulses to allow the vector-vector outer product in a crossbar array. However, if the device does not meet the condition of linear conductance change over approximately 1,000 conductance levels, the system's performance suffers, posing a challenge for nanoscale memories. Recently, Charge Trap Flash (CTF) memory was found to have a high number of levels before saturation but variable non-linearity. In this study, we examine the trade-off between range of conductance change and linearity. We demonstrate through simulations that our system performs almost as well as models trained using exact floating point operations with less than a 1% decrease in performance by selecting an optimal range. Our system achieves an accuracy of 97.9% on the MNIST dataset and 89.1% and 70.5% accuracy on the CIFAR-10 and CIFAR-100 datasets, respectively (using pre-extracted features). Our system's use in reinforcement learning, where it is used for value function approximation in Q-Learning, is also demonstrated, and it learns to complete an episode of the mountain car control problem in approximately 146 steps. Compared to state-of-the-art, the CTF-based RPU exhibits the best performance to allow software-equivalent performance.",1
"Machines are a long way from robustly solving open-world perception-control tasks, such as first-person view (FPV) aerial navigation. While recent advances in end-to-end Machine Learning, especially Imitation and Reinforcement Learning appear promising, they are constrained by the need of large amounts of difficult-to-collect labeled real-world data. Simulated data, on the other hand, is easy to generate, but generally does not render safe behaviors in diverse real-life scenarios. In this work we propose a novel method for learning robust visuomotor policies for real-world deployment which can be trained purely with simulated data. We develop rich state representations that combine supervised and unsupervised environment data. Our approach takes a cross-modal perspective, where separate modalities correspond to the raw camera data and the system states relevant to the task, such as the relative pose of gates to the drone in the case of drone racing. We feed both data modalities into a novel factored architecture, which learns a joint low-dimensional embedding via Variational Auto Encoders. This compact representation is then fed into a control policy, which we trained using imitation learning with expert trajectories in a simulator. We analyze the rich latent spaces learned with our proposed representations, and show that the use of our cross-modal architecture significantly improves control policy performance as compared to end-to-end learning or purely unsupervised feature extractors. We also present real-world results for drone navigation through gates in different track configurations and environmental conditions. Our proposed method, which runs fully onboard, can successfully generalize the learned representations and policies across simulation and reality, significantly outperforming baseline approaches.   Supplementary video: https://youtu.be/VKc3A5HlUU8",0
"Although recent advances in end-to-end Machine Learning, particularly Imitation and Reinforcement Learning, show potential for solving open-world perception-control tasks like first-person view (FPV) aerial navigation, machines still have a long way to go in achieving robustness in this area. One of the primary challenges is the need for large amounts of labeled real-world data, which is difficult to collect. While simulated data is easy to generate, it does not typically produce safe behaviors in diverse real-life scenarios. To address this issue, we propose a new method for learning robust visuomotor policies for real-world deployment that can be trained solely with simulated data. Our approach involves developing rich state representations that combine supervised and unsupervised environment data. We take a cross-modal perspective, where separate modalities correspond to the raw camera data and the system states relevant to the task. We then feed both data modalities into a novel factored architecture that learns a joint low-dimensional embedding via Variational Auto Encoders. This compact representation is then fed into a control policy, which we trained using imitation learning with expert trajectories in a simulator. Our approach significantly improves control policy performance compared to end-to-end learning or purely unsupervised feature extractors. We also present real-world results for drone navigation through gates in different track configurations and environmental conditions. Our proposed method, which runs fully onboard, can successfully generalize the learned representations and policies across simulation and reality and significantly outperforms baseline approaches. A supplementary video demonstrating our approach is available at https://youtu.be/VKc3A5HlUU8.",1
"In cooperative multi-agent reinforcement learning (c-MARL), agents learn to cooperatively take actions as a team to maximize a total team reward. We analyze the robustness of c-MARL to adversaries capable of attacking one of the agents on a team. Through the ability to manipulate this agent's observations, the adversary seeks to decrease the total team reward.   Attacking c-MARL is challenging for three reasons: first, it is difficult to estimate team rewards or how they are impacted by an agent mispredicting; second, models are non-differentiable; and third, the feature space is low-dimensional. Thus, we introduce a novel attack. The attacker first trains a policy network with reinforcement learning to find a wrong action it should encourage the victim agent to take. Then, the adversary uses targeted adversarial examples to force the victim to take this action.   Our results on the StartCraft II multi-agent benchmark demonstrate that c-MARL teams are highly vulnerable to perturbations applied to one of their agent's observations. By attacking a single agent, our attack method has highly negative impact on the overall team reward, reducing it from 20 to 9.4. This results in the team's winning rate to go down from 98.9% to 0%.",0
"The goal of cooperative multi-agent reinforcement learning (c-MARL) is to teach agents how to work together to gain the highest possible team reward. We wanted to see how c-MARL would hold up against attackers who target one of the agents on the team. These attackers can manipulate the agent's observations to lower the overall team reward. Unfortunately, attacking c-MARL is not easy due to the difficulty of estimating team rewards, non-differentiable models, and low-dimensional feature spaces. As a result, we developed a new method for attacking c-MARL. First, we trained a policy network using reinforcement learning to find an action that would harm the victim agent. Then, we used targeted adversarial examples to force the victim agent to take this action. Our experiments on the StartCraft II multi-agent benchmark showed that c-MARL teams are highly susceptible to attacks on individual agents. By attacking just one agent, our method reduced the team's overall reward from 20 to 9.4, resulting in a 98.9% decrease in their winning rate.",1
"There have been increasing challenges to solve combinatorial optimization problems by machine learning. Khalil et al. proposed an end-to-end reinforcement learning framework, S2V-DQN, which automatically learns graph embeddings to construct solutions to a wide range of problems. To improve the generalization ability of their Q-learning method, we propose a novel learning strategy based on AlphaGo Zero which is a Go engine that achieved a superhuman level without the domain knowledge of the game. Our framework is redesigned for combinatorial problems, where the final reward might take any real number instead of a binary response, win/lose. In experiments conducted for five kinds of NP-hard problems including {\sc MinimumVertexCover} and {\sc MaxCut}, our method is shown to generalize better to various graphs than S2V-DQN. Furthermore, our method can be combined with recently-developed graph neural network (GNN) models such as the \emph{Graph Isomorphism Network}, resulting in even better performance. This experiment also gives an interesting insight into a suitable choice of GNN models for each task.",0
"Machine learning has faced increasing challenges in solving combinatorial optimization problems. One proposed solution is the S2V-DQN framework by Khalil et al. This framework utilizes reinforcement learning and graph embeddings to generate solutions for a wide range of problems. In order to enhance the generalizability of their Q-learning method, we suggest a new learning strategy based on AlphaGo Zero, which achieved superhuman proficiency in Go without prior knowledge of the game. Our approach is specifically designed for combinatorial problems that may not yield binary responses. We tested our method on five NP-hard problems, including {\sc MinimumVertexCover} and {\sc MaxCut}, and found that it outperformed S2V-DQN in terms of generalization to various graphs. We also discovered that combining our method with the Graph Isomorphism Network, a recently-developed graph neural network model, leads to even better performance. Overall, our experiment provides valuable insights into the suitability of different GNN models for various tasks.",1
"Many current behavior generation methods struggle to handle real-world traffic situations as they do not scale well with complexity. However, behaviors can be learned off-line using data-driven approaches. Especially, reinforcement learning is promising as it implicitly learns how to behave utilizing collected experiences. In this work, we combine policy-based reinforcement learning with local optimization to foster and synthesize the best of the two methodologies. The policy-based reinforcement learning algorithm provides an initial solution and guiding reference for the post-optimization. Therefore, the optimizer only has to compute a single homotopy class, e.g.\ drive behind or in front of the other vehicle. By storing the state-history during reinforcement learning, it can be used for constraint checking and the optimizer can account for interactions. The post-optimization additionally acts as a safety-layer and the novel method, thus, can be applied in safety-critical applications. We evaluate the proposed method using lane-change scenarios with a varying number of vehicles.",0
"Current methods for generating behavior struggle to handle complex real-world traffic situations. However, offline learning using data-driven approaches can be used to learn behaviors. Reinforcement learning is particularly promising as it learns how to behave using collected experiences. This work combines policy-based reinforcement learning with local optimization to synthesize the best of both methodologies. The initial solution and guiding reference are provided by the policy-based reinforcement learning algorithm, which allows the optimizer to compute a single homotopy class. The state-history stored during reinforcement learning can be used for constraint checking, and the optimizer can account for interactions. The post-optimization serves as a safety layer, making the new method applicable to safety-critical applications. The proposed method is evaluated using lane-change scenarios with varying numbers of vehicles.",1
"Producing agents that can generalize to a wide range of visually different environments is a significant challenge in reinforcement learning. One method for overcoming this issue is visual domain randomization, whereby at the start of each training episode some visual aspects of the environment are randomized so that the agent is exposed to many possible variations. However, domain randomization is highly inefficient and may lead to policies with high variance across domains. Instead, we propose a regularization method whereby the agent is only trained on one variation of the environment, and its learned state representations are regularized during training to be invariant across domains. We conduct experiments that demonstrate that our technique leads to more efficient and robust learning than standard domain randomization, while achieving equal generalization scores.",0
"Reinforcement learning faces a significant challenge in producing agents that can adapt to a wide variety of visually distinct environments. One approach to address this challenge is visual domain randomization. This technique involves randomizing certain visual aspects of the environment before each training episode, allowing the agent to encounter multiple variations. However, this approach is inefficient and may result in policies that perform poorly across different domains. Instead, we propose a regularization method. This technique involves training the agent on a single variation of the environment and then regularizing its learned state representations to remain invariant across domains. Our experiments demonstrate that this approach leads to more efficient and robust learning than domain randomization, while achieving similar generalization scores.",1
"Portfolio Selection is an important real-world financial task and has attracted extensive attention in artificial intelligence communities. This task, however, has two main difficulties: (i) the non-stationary price series and complex asset correlations make the learning of feature representation very hard; (ii) the practicality principle in financial markets requires controlling both transaction and risk costs. Most existing methods adopt handcraft features and/or consider no constraints for the costs, which may make them perform unsatisfactorily and fail to control both costs in practice. In this paper, we propose a cost-sensitive portfolio selection method with deep reinforcement learning. Specifically, a novel two-stream portfolio policy network is devised to extract both price series patterns and asset correlations, while a new cost-sensitive reward function is developed to maximize the accumulated return and constrain both costs via reinforcement learning. We theoretically analyze the near-optimality of the proposed reward, which shows that the growth rate of the policy regarding this reward function can approach the theoretical optimum. We also empirically evaluate the proposed method on real-world datasets. Promising results demonstrate the effectiveness and superiority of the proposed method in terms of profitability, cost-sensitivity and representation abilities.",0
"The task of Portfolio Selection is crucial in the financial world and has garnered significant attention from artificial intelligence communities. However, this task presents two main challenges: (i) the non-stationary price series and complex asset correlations make it difficult to learn feature representation, and (ii) practicality in financial markets requires controlling transaction and risk costs. Many existing methods use handcrafted features and overlook cost constraints, leading to unsatisfactory performance and cost control issues. This paper proposes a cost-sensitive portfolio selection approach using deep reinforcement learning. The method uses a novel two-stream portfolio policy network to extract price series patterns and asset correlations, and a new cost-sensitive reward function to maximize returns and constrain costs through reinforcement learning. The proposed reward function is theoretically analyzed, demonstrating near-optimality, and empirical evaluations on real-world datasets confirm the method's effectiveness, profitability, cost-sensitivity, and representation abilities.",1
"Efficient and effective learning is one of the ultimate goals of the deep reinforcement learning (DRL), although the compromise has been made in most of the time, especially for the application of robot manipulations. Learning is always expensive for robot manipulation tasks and the learning effectiveness could be affected by the system uncertainty. In order to solve above challenges, in this study, we proposed a simple but powerful reward shaping method, namely Dense2Sparse. It combines the advantage of fast convergence of dense reward and the noise isolation of the sparse reward, to achieve a balance between learning efficiency and effectiveness, which makes it suitable for robot manipulation tasks. We evaluated our Dense2Sparse method with a series of ablation experiments using the state representation model with system uncertainty. The experiment results show that the Dense2Sparse method obtained higher expected reward compared with the ones using standalone dense reward or sparse reward, and it also has a superior tolerance of system uncertainty.",0
"Deep reinforcement learning (DRL) aims to achieve efficient and effective learning, which is a major objective. However, this can be challenging, particularly in robot manipulation applications, due to the high cost of learning and the impact of system uncertainty on learning effectiveness. To address these issues, we propose a reward shaping method called Dense2Sparse, which balances learning efficiency and effectiveness. This method combines the quick convergence of dense reward with the noise isolation of sparse reward, making it suitable for robot manipulation tasks. Our evaluation of the Dense2Sparse method using state representation model with system uncertainty shows that it outperforms standalone dense and sparse rewards, and exhibits superior tolerance of system uncertainty.",1
"Measuring the quality of a generated sequence against a set of references is a central problem in many learning frameworks, be it to compute a score, to assign a reward, or to perform discrimination. Despite great advances in model architectures, metrics that scale independently of the number of references are still based on n-gram estimates. We show that the underlying operations, counting words and comparing counts, can be lifted to embedding words and comparing embeddings. An in-depth analysis of BERT embeddings shows empirically that contextual embeddings can be employed to capture the required dependencies while maintaining the necessary scalability through appropriate pruning and smoothing techniques. We cast unconditional generation as a reinforcement learning problem and show that our reward function indeed provides a more effective learning signal than n-gram reward in this challenging setting.",0
"The evaluation of a generated sequence against a reference set is a crucial issue in various learning frameworks, whether to calculate a score, assign a reward, or carry out discrimination. Despite significant advancements in model architectures, metrics that can scale independently of the number of references still rely on n-gram estimates. We demonstrate that the fundamental operations of counting words and comparing counts can be elevated to embedding words and comparing embeddings. After conducting a thorough analysis of BERT embeddings, we find that contextual embeddings can capture the necessary dependencies while preserving scalability through suitable smoothing and pruning techniques. We frame unconditional generation as a reinforcement learning problem and establish that our reward function offers a more effective learning signal than an n-gram reward under challenging circumstances.",1
"The advances in deep reinforcement learning recently revived interest in data-driven learning based approaches to navigation. In this paper we propose to learn viewpoint invariant and target invariant visual servoing for local mobile robot navigation; given an initial view and the goal view or an image of a target, we train deep convolutional network controller to reach the desired goal. We present a new architecture for this task which rests on the ability of establishing correspondences between the initial and goal view and novel reward structure motivated by the traditional feedback control error. The advantage of the proposed model is that it does not require calibration and depth information and achieves robust visual servoing in a variety of environments and targets without any parameter fine tuning. We present comprehensive evaluation of the approach and comparison with other deep learning architectures as well as classical visual servoing methods in visually realistic simulation environment. The presented model overcomes the brittleness of classical visual servoing based methods and achieves significantly higher generalization capability compared to the previous learning approaches.",0
The recent advancements in deep reinforcement learning have reignited interest in navigation based on data-driven learning approaches. Our paper proposes a method for local mobile robot navigation that learns viewpoint invariant and target invariant visual servoing. This involves training a deep convolutional network controller to reach a desired goal based on an initial view and a goal view or image of a target. Our approach uses a new architecture that establishes correspondences between the initial and goal view and a reward structure based on traditional feedback control error. The proposed model is advantageous as it does not require calibration or depth information and achieves robust visual servoing in various environments and targets without parameter fine-tuning. We provide a comprehensive evaluation of our approach and compare it with other deep learning architectures and classical visual servoing methods in a visually realistic simulation environment. Our model overcomes the brittleness of classical visual servoing methods and achieves significantly higher generalization capability compared to previous learning approaches.,1
"We propose a reinforcement learning framework for discrete environments in which an agent makes both strategic and tactical decisions. The former manifests itself through the use of value function, while the latter is powered by a tree search planner. These tools complement each other. The planning module performs a local \textit{what-if} analysis, which allows to avoid tactical pitfalls and boost backups of the value function. The value function, being global in nature, compensates for inherent locality of the planner. In order to further solidify this synergy, we introduce an exploration mechanism with two distinctive components: uncertainty modelling and risk measurement. To model the uncertainty we use value function ensembles, and to reflect risk we use propose several functionals that summarize the implied by the ensemble. We show that our method performs well on hard exploration environments: Deep-sea, toy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up in learning and boost in performance.",0
"Our proposal involves a reinforcement learning structure designed for environments with discrete variables, where an agent must make strategic and tactical decisions. To achieve this, we utilize a value function for the former and a tree search planner for the latter. These two tools work in unison, with the planner preventing tactical errors and enhancing the backups of the value function. Meanwhile, the value function offsets the planner's inherent locality due to its global nature. To enhance this collaboration, we introduce an exploration mechanism comprising uncertainty modeling and risk measurement. For the former, we utilize value function ensembles, while we propose multiple functionals to summarize the risk implied by the ensemble. Our approach performs well in challenging exploration environments, such as Deep-sea, toy Montezuma's Revenge, and Sokoban, achieving faster learning and improved performance.",1
"We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes RGB images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.   Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort.   In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.",0
"To control a real-world vehicle, we employ reinforcement learning in simulation. The driving policy utilizes input from a single camera in the form of RGB images and their semantic segmentation. Synthetic data is primarily used, with labeled real-world data only being used in the segmentation network training. The use of reinforcement learning in simulation and synthetic data is driven by cost and engineering effort reduction. Our successful sim-to-real policy transfer is confirmed through real-world experiments. Extensive evaluation is conducted to analyze the impact of design decisions regarding perception, control, and training on real-world performance.",1
"The emergence of structured databases for Question Answering (QA) systems has led to developing methods, in which the problem of learning the correct answer efficiently is based on a linking task between the constituents of the question and the corresponding entries in the database. As a result, parsing the questions in order to determine their main elements, which are required for answer retrieval, becomes crucial. However, most datasets for QA systems lack gold annotations for parsing, i.e., labels are only available in the form of (question, formal-query, answer). In this paper, we propose a distantly supervised learning framework based on reinforcement learning to learn the mentions of entities and relations in questions. We leverage the provided formal queries to characterize delayed rewards for optimizing a policy gradient objective for the parsing model. An empirical evaluation of our approach shows a significant improvement in the performance of entity and relation linking compared to the state of the art. We also demonstrate that a more accurate parsing component enhances the overall performance of QA systems.",0
"Structured databases for Question Answering (QA) systems have given rise to methods that rely on linking the question constituents to corresponding entries in the database to efficiently learn the correct answer. Thus, accurately identifying the main elements of the question necessary for answer retrieval through parsing becomes essential. However, most QA system datasets lack gold annotations for parsing and only provide labels in the form of (question, formal-query, answer). In this study, we propose a reinforcement learning-based distantly supervised learning framework for identifying entities and relations mentioned in questions. We utilize formal queries to define delayed rewards and optimize a policy gradient objective for the parsing model. Our approach demonstrates a significant improvement in entity and relation linking performance compared to the state of the art and highlights the importance of accurate parsing for improving overall QA system performance.",1
"Most meta reinforcement learning (meta-RL) methods learn to adapt to new tasks by directly optimizing the parameters of policies over primitive action space. Such algorithms work well in tasks with relatively slight difference. However, when the task distribution becomes wider, it would be quite inefficient to directly learn such a meta-policy. In this paper, we propose a new meta-RL algorithm called Meta Goal-generation for Hierarchical RL (MGHRL). Instead of directly generating policies over primitive action space for new tasks, MGHRL learns to generate high-level meta strategies over subgoals given past experience and leaves the rest of how to achieve subgoals as independent RL subtasks. Our empirical results on several challenging simulated robotics environments show that our method enables more efficient and generalized meta-learning from past experience.",0
"The majority of meta reinforcement learning (meta-RL) techniques learn to adapt to novel tasks by optimizing policy parameters over primitive action space. These algorithms perform well in tasks with minor distinctions, but are inefficient when the task distribution is broad. In this study, we introduce a new meta-RL algorithm, Meta Goal-generation for Hierarchical RL (MGHRL), which does not produce policies directly over primitive action space for new tasks. Instead, MGHRL generates high-level meta strategies over subgoals using past experience and allows the remaining RL subtasks to be independent. Our experimental results on challenging simulated robotics environments demonstrate that our approach results in more efficient and generalized meta-learning from past experience.",1
"We introduce a new approach for comparing reinforcement learning policies, using Wasserstein distances (WDs) in a newly defined latent behavioral space. We show that by utilizing the dual formulation of the WD, we can learn score functions over policy behaviors that can in turn be used to lead policy optimization towards (or away from) (un)desired behaviors. Combined with smoothed WDs, the dual formulation allows us to devise efficient algorithms that take stochastic gradient descent steps through WD regularizers. We incorporate these regularizers into two novel on-policy algorithms, Behavior-Guided Policy Gradient and Behavior-Guided Evolution Strategies, which we demonstrate can outperform existing methods in a variety of challenging environments. We also provide an open source demo.",0
"A new method for comparing reinforcement learning policies is presented in this study. The method employs Wasserstein distances (WDs) within a new latent behavioral space. The study demonstrates that the dual formulation of WDs can be used to develop score functions that guide policy optimization towards or away from undesired behaviors. By utilizing smoothed WDs, the study develops efficient algorithms that integrate stochastic gradient descent steps through WD regularizers. Two on-policy algorithms, Behavior-Guided Policy Gradient and Behavior-Guided Evolution Strategies, are introduced, which outperform existing methods in challenging environments. An open source demo is also provided.",1
"The most data-efficient algorithms for reinforcement learning in robotics are model-based policy search algorithms, which alternate between learning a dynamical model of the robot and optimizing a policy to maximize the expected return given the model and its uncertainties. However, the current algorithms lack an effective exploration strategy to deal with sparse or misleading reward scenarios: if they do not experience any state with a positive reward during the initial random exploration, it is very unlikely to solve the problem. Here, we propose a novel model-based policy search algorithm, Multi-DEX, that leverages a learned dynamical model to efficiently explore the task space and solve tasks with sparse rewards in a few episodes. To achieve this, we frame the policy search problem as a multi-objective, model-based policy optimization problem with three objectives: (1) generate maximally novel state trajectories, (2) maximize the expected return and (3) keep the system in state-space regions for which the model is as accurate as possible. We then optimize these objectives using a Pareto-based multi-objective optimization algorithm. The experiments show that Multi-DEX is able to solve sparse reward scenarios (with a simulated robotic arm) in much lower interaction time than VIME, TRPO, GEP-PG, CMA-ES and Black-DROPS.",0
"The most efficient algorithms for reinforcement learning in robotics that utilize minimal data are those that involve model-based policy search. These algorithms alternate between learning a robot's dynamical model and optimizing a policy to maximize the expected return based on the model and its uncertainties. However, current algorithms lack an effective exploration strategy to handle sparse or misleading rewards. If positive rewards are not experienced during the initial random exploration, then the problem is unlikely to be solved. To address this issue, a new model-based policy search algorithm called Multi-DEX is proposed. It leverages a learned dynamical model to efficiently explore the task space and solve problems with sparse rewards in just a few episodes. The algorithm frames the policy search problem as a multi-objective, model-based policy optimization problem with three objectives: (1) generate maximally novel state trajectories, (2) maximize the expected return, and (3) keep the system in state-space regions where the model is as accurate as possible. These objectives are optimized using a Pareto-based multi-objective optimization algorithm. Experiments show that Multi-DEX is capable of solving sparse reward scenarios with a simulated robotic arm in much less interaction time than other algorithms such as VIME, TRPO, GEP-PG, CMA-ES, and Black-DROPS.",1
"In this paper, we propose a reinforcement learning-based algorithm for trajectory optimization for constrained dynamical systems. This problem is motivated by the fact that for most robotic systems, the dynamics may not always be known. Generating smooth, dynamically feasible trajectories could be difficult for such systems. Using sampling-based algorithms for motion planning may result in trajectories that are prone to undesirable control jumps. However, they can usually provide a good reference trajectory which a model-free reinforcement learning algorithm can then exploit by limiting the search domain and quickly finding a dynamically smooth trajectory. We use this idea to train a reinforcement learning agent to learn a dynamically smooth trajectory in a curriculum learning setting. Furthermore, for generalization, we parameterize the policies with goal locations, so that the agent can be trained for multiple goals simultaneously. We show result in both simulated environments as well as real experiments, for a $6$-DoF manipulator arm operated in position-controlled mode to validate the proposed idea. We compare the proposed ideas against a PID controller which is used to track a designed trajectory in configuration space. Our experiments show that our RL agent trained with a reference path outperformed a model-free PID controller of the type commonly used on many robotic platforms for trajectory tracking.",0
"The aim of this study is to suggest an algorithm based on reinforcement learning for optimizing trajectories in constrained dynamical systems. The challenge arises from the lack of knowledge about the dynamics of most robotic systems, making it difficult to generate feasible trajectories. Although sampling-based algorithms can be used for motion planning, they can produce trajectories with undesirable control jumps. However, these algorithms can still provide a reference trajectory that can be exploited through a model-free reinforcement learning algorithm to quickly find a dynamically smooth trajectory. We use this concept to train a reinforcement learning agent in a curriculum learning setting to learn a dynamically smooth trajectory, which can be generalized by parameterizing the policies with goal locations. We conducted experiments on a $6$-DoF manipulator arm in position-controlled mode to validate our idea and compare it with a PID controller. The results demonstrate that our RL agent outperforms the model-free PID controller typically used for trajectory tracking on many robotic platforms.",1
"Q-learning with neural network function approximation (neural Q-learning for short) is among the most prevalent deep reinforcement learning algorithms. Despite its empirical success, the non-asymptotic convergence rate of neural Q-learning remains virtually unknown. In this paper, we present a finite-time analysis of a neural Q-learning algorithm, where the data are generated from a Markov decision process and the action-value function is approximated by a deep ReLU neural network. We prove that neural Q-learning finds the optimal policy with $O(1/\sqrt{T})$ convergence rate if the neural function approximator is sufficiently overparameterized, where $T$ is the number of iterations. To our best knowledge, our result is the first finite-time analysis of neural Q-learning under non-i.i.d. data assumption.",0
"Neural Q-learning, a popular deep reinforcement learning algorithm, has achieved great practical success. However, its non-asymptotic convergence rate remains largely unknown. This study conducts a finite-time analysis of neural Q-learning using data from a Markov decision process and a deep ReLU neural network to approximate the action-value function. The results show that if the neural function approximator is overparameterized enough, neural Q-learning can achieve optimal policy with a convergence rate of $O(1/\sqrt{T})$ after a finite number of iterations. This is the first finite-time analysis of neural Q-learning under non-i.i.d. data assumption.",1
"Deep reinforcement learning (DRL) is capable of learning high-performing policies on a variety of complex high-dimensional tasks, ranging from video games to robotic manipulation. However, standard DRL methods often suffer from poor sample efficiency, partially because they aim to be entirely problem-agnostic. In this work, we introduce a novel approach to exploration and hierarchical skill learning that derives its sample efficiency from intuitive assumptions it makes about the behavior of objects both in the physical world and simulations which mimic physics. Specifically, we propose the Hypothesis Proposal and Evaluation (HyPE) algorithm, which discovers objects from raw pixel data, generates hypotheses about the controllability of observed changes in object state, and learns a hierarchy of skills to test these hypotheses. We demonstrate that HyPE can dramatically improve the sample efficiency of policy learning in two different domains: a simulated robotic block-pushing domain, and a popular benchmark task: Breakout. In these domains, HyPE learns high-scoring policies an order of magnitude faster than several state-of-the-art reinforcement learning methods.",0
"DRL has proven to be effective in acquiring top-performing policies for complex high-dimensional tasks, including video games and robotic manipulation. However, the traditional DRL approaches struggle with inadequate sample efficiency as they aim to be problem-agnostic. To overcome this issue, we present a fresh approach to exploration and hierarchical skill learning that derives its sample efficiency from intuitive assumptions about the behavior of objects in the physical world and physics simulations. Our proposed algorithm, HyPE, detects objects from raw pixel data, generates hypotheses about the controllability of observed changes in object state, and learns a hierarchy of skills to test these hypotheses. Our research shows that HyPE vastly improves the sample efficiency of policy learning in two domains: a simulated robotic block-pushing domain, and the popular Breakout benchmark task. HyPE learns high-scoring policies ten times faster than several cutting-edge reinforcement learning methods in these domains.",1
"A common approach for defining a reward function for Multi-objective Reinforcement Learning (MORL) problems is the weighted sum of the multiple objectives. The weights are then treated as design parameters dependent on the expertise (and preference) of the person performing the learning, with the typical result that a new solution is required for any change in these settings. This paper investigates the relationship between the reward function and the optimal value function for MORL; specifically addressing the question of how to approximate the optimal value function well beyond the set of weights for which the optimization problem was actually solved, thereby avoiding the need to recompute for any particular choice. We prove that the value function transforms smoothly given a transformation of weights of the reward function (and thus a smooth interpolation in the policy space). A Gaussian process is used to obtain a smooth interpolation over the reward function weights of the optimal value function for three well-known examples: GridWorld, Objectworld and Pendulum. The results show that the interpolation can provide very robust values for sample states and action space in discrete and continuous domain problems. Significant advantages arise from utilizing this interpolation technique in the domain of autonomous vehicles: easy, instant adaptation of user preferences while driving and true randomization of obstacle vehicle behavior preferences during training.",0
"In Multi-objective Reinforcement Learning (MORL), a prevalent method for defining a reward function is to use a weighted sum of objectives, where the weights are considered design parameters that depend on the expertise and preference of the learner. However, changing these settings often entails finding a new solution. This study explores how to approximate the optimal value function of MORL beyond the weights for which the optimization problem was solved. The goal is to avoid recomputing for each choice and provide a smooth interpolation in the policy space. The paper demonstrates that the value function transforms smoothly with a transformation of reward function weights, and a Gaussian process is employed to obtain a smooth interpolation for three familiar examples. The interpolation technique proves beneficial for autonomous vehicles, allowing instant adaptation of user preferences while driving and randomization of obstacle vehicle behavior preferences during training. The results demonstrate that the interpolation can produce robust values for sample states and action space in discrete and continuous domain problems.",1
"In this paper, we consider the problem of building learning agents that can efficiently learn to navigate in constrained environments. The main goal is to design agents that can efficiently learn to understand and generalize to different environments using high-dimensional inputs (a 2D map), while following feasible paths that avoid obstacles in obstacle-cluttered environment. To achieve this, we make use of traditional path planning algorithms, supervised learning, and reinforcement learning algorithms in a synergistic way. The key idea is to decouple the navigation problem into planning and control, the former of which is achieved by supervised learning whereas the latter is done by reinforcement learning. Specifically, we train a deep convolutional network that can predict collision-free paths based on a map of the environment-- this is then used by a reinforcement learning algorithm to learn to closely follow the path. This allows the trained agent to achieve good generalization while learning faster. We test our proposed method in the recently proposed Safety Gym suite that allows testing of safety-constraints during training of learning agents. We compare our proposed method with existing work and show that our method consistently improves the sample efficiency and generalization capability to novel environments.",0
"The focus of this paper is to address the challenge of developing effective learning agents that can navigate in constrained environments. Our primary objective is to create agents that can comprehend and adapt to various environments using high-dimensional inputs, while avoiding obstacles in complex environments. To accomplish this, we employ a combination of traditional path planning algorithms, reinforcement learning, and supervised learning. Our approach involves separating the navigation problem into planning and control, where we train a deep convolutional network to predict collision-free paths using a map of the environment. Subsequently, a reinforcement learning algorithm is used to closely follow the predicted path. This approach enhances the agent's generalization and accelerates learning. We evaluate our method using the Safety Gym suite and compare its performance with existing approaches. Our results demonstrate that our approach significantly enhances sample efficiency and generalization to new environments.",1
"Group activities usually involve spatiotemporal dynamics among many interactive individuals, while only a few participants at several key frames essentially define the activity. Therefore, effectively modeling the group-relevant and suppressing the irrelevant actions (and interactions) are vital for group activity recognition. In this paper, we propose a novel method based on deep reinforcement learning to progressively refine the low-level features and high-level relations of group activities. Firstly, we construct a semantic relation graph (SRG) to explicitly model the relations among persons. Then, two agents adopting policy according to two Markov decision processes are applied to progressively refine the SRG. Specifically, one feature-distilling (FD) agent in the discrete action space refines the low-level spatio-temporal features by distilling the most informative frames. Another relation-gating (RG) agent in continuous action space adjusts the high-level semantic graph to pay more attention to group-relevant relations. The SRG, FD agent, and RG agent are optimized alternately to mutually boost the performance of each other. Extensive experiments on two widely used benchmarks demonstrate the effectiveness and superiority of the proposed approach.",0
"The dynamics of group activities involve interactions among multiple individuals, but only a few participants at key moments determine the activity. Therefore, it is crucial to model relevant group actions and suppress irrelevant ones for successful group activity recognition. This research proposes a new approach based on deep reinforcement learning to progressively enhance low-level features and high-level relations of group activities. Initially, a semantic relation graph (SRG) is constructed to model the connections among individuals explicitly. Two agents, each following a Markov decision process, are then employed to refine the SRG. The feature-distilling (FD) agent refines low-level spatio-temporal features by selecting the most informative frames, while the relation-gating (RG) agent adjusts high-level semantic graphs to focus on group-relevant relationships. The SRG, FD agent, and RG agent are optimized alternately to enhance each other's performance. Extensive experiments on two commonly used benchmarks demonstrate the effectiveness and superiority of this approach.",1
"Despite recent success of deep network-based Reinforcement Learning (RL), it remains elusive to achieve human-level efficiency in learning novel tasks. While previous efforts attempt to address this challenge using meta-learning strategies, they typically suffer from sampling inefficiency with on-policy RL algorithms or meta-overfitting with off-policy learning. In this work, we propose a novel meta-RL strategy to address those limitations. In particular, we decompose the meta-RL problem into three sub-tasks, task-exploration, task-inference and task-fulfillment, instantiated with two deep network agents and a task encoder. During meta-training, our method learns a task-conditioned actor network for task-fulfillment, an explorer network with a self-supervised reward shaping that encourages task-informative experiences in task-exploration, and a context-aware graph-based task encoder for task inference. We validate our approach with extensive experiments on several public benchmarks and the results show that our algorithm effectively performs exploration for task inference, improves sample efficiency during both training and testing, and mitigates the meta-overfitting problem.",0
"Although deep network-based Reinforcement Learning (RL) has experienced success lately, achieving human-level proficiency in learning new tasks remains challenging. Previous attempts to address this challenge using meta-learning strategies have led to sampling inefficiency with on-policy RL algorithms or meta-overfitting with off-policy learning. This study introduces a novel meta-RL strategy to overcome these limitations. The meta-RL problem is decomposed into three sub-tasks, namely task-exploration, task-inference, and task-fulfillment, which are implemented with two deep network agents and a task encoder. During meta-training, the proposed approach learns a task-conditioned actor network for task-fulfillment, an explorer network with a self-supervised reward shaping that encourages task-informative experiences in task-exploration, and a context-aware graph-based task encoder for task inference. Extensive experiments on several public benchmarks validate the effectiveness of the algorithm in performing exploration for task inference, improving sample efficiency during both training and testing, and mitigating the meta-overfitting problem.",1
"Machine learning algorithms aim to find patterns from observations, which may include some noise, especially in robotics domain. To perform well even with such noise, we expect them to be able to detect outliers and discard them when needed. We therefore propose a new stochastic gradient optimization method, whose robustness is directly built in the algorithm, using the robust student-t distribution as its core idea. Adam, the popular optimization method, is modified with our method and the resultant optimizer, so-called TAdam, is shown to effectively outperform Adam in terms of robustness against noise on diverse task, ranging from regression and classification to reinforcement learning problems. The implementation of our algorithm can be found at https://github.com/Mahoumaru/TAdam.git",0
"The goal of machine learning algorithms is to identify patterns from observations, which may contain noise, particularly in the field of robotics. To ensure their reliability in the presence of such noise, it's crucial that they recognize and eliminate outliers. Our team has developed a new stochastic gradient optimization technique, which incorporates robustness as a fundamental aspect of the algorithm by relying on the robust student-t distribution. We've modified the popular optimization method, Adam, with our approach, resulting in a new optimizer called TAdam. Our experiments demonstrate that TAdam outperforms Adam in terms of its resilience to noise across a range of tasks, including regression, classification, and reinforcement learning. Our algorithm can be accessed at https://github.com/Mahoumaru/TAdam.git.",1
"Reinforcement learning (RL) is attracting increasing interests in autonomous driving due to its potential to solve complex classification and control problems. However, existing RL algorithms are rarely applied to real vehicles for two predominant problems: behaviours are unexplainable, and they cannot guarantee safety under new scenarios. This paper presents a safe RL algorithm, called Parallel Constrained Policy Optimization (PCPO), for two autonomous driving tasks. PCPO extends today's common actor-critic architecture to a three-component learning framework, in which three neural networks are used to approximate the policy function, value function and a newly added risk function, respectively. Meanwhile, a trust region constraint is added to allow large update steps without breaking the monotonic improvement condition. To ensure the feasibility of safety constrained problems, synchronized parallel learners are employed to explore different state spaces, which accelerates learning and policy-update. The simulations of two scenarios for autonomous vehicles confirm we can ensure safety while achieving fast learning.",0
"Autonomous driving has seen a surge in interest in Reinforcement Learning (RL) due to its potential in solving complex control and classification problems. However, current RL algorithms are not often used in real vehicles due to two major issues. Firstly, their behaviors cannot be explained, and secondly, they cannot guarantee safety in new situations. This research presents a safe RL algorithm, PCPO, for two autonomous driving tasks. PCPO extends the standard actor-critic architecture to a three-component learning framework, adding a risk function and a trust region constraint to allow for larger update steps. Synchronized parallel learners are used to explore different state spaces, which helps accelerate learning and policy-updates. Simulations of two autonomous vehicle scenarios demonstrate that safety is ensured while achieving fast learning.",1
"We propose a novel actor-critic, model-free reinforcement learning algorithm which employs a Bayesian method of parameter space exploration to solve environments. A Gaussian process is used to learn the expected return of a policy given the policy's parameters. The system is trained by updating the parameters using gradient descent on a new surrogate loss function consisting of the Proximal Policy Optimization 'Clipped' loss function and a bonus term representing the expected improvement acquisition function given by the Gaussian process. This new method is shown to be comparable to and at times empirically outperform current algorithms on environments that simulate robotic locomotion using the MuJoCo physics engine.",0
"Our proposal is for an innovative algorithm for reinforcement learning, utilizing the actor-critic model without any pre-determined models. It incorporates a Bayesian approach to explore parameter space for solving problems. For determining the expected return of a policy with given parameters, we employ a Gaussian process. The system is trained by updating parameters using gradient descent on a new surrogate loss function that combines the Proximal Policy Optimization 'Clipped' loss function and an expected improvement acquisition function represented by the Gaussian process. Our method has been tested on environments that simulate robotic locomotion through the MuJoCo physics engine and has been found to be comparable and sometimes even outperform current algorithms.",1
"In this study, we investigate the use of global information to speed up the learning process and increase the cumulative rewards of reinforcement learning (RL) in competition tasks. Within the actor-critic RL, we introduce multiple cooperative critics from two levels of the hierarchy and propose a reinforcement learning from hierarchical critics (RLHC) algorithm. In our approach, each agent receives value information from local and global critics regarding a competition task and accesses multiple cooperative critics in a top-down hierarchy. Thus, each agent not only receives low-level details but also considers coordination from higher levels, thereby obtaining global information to improve the training performance. Then, we test the proposed RLHC algorithm against the benchmark algorithm, proximal policy optimisation (PPO), for two experimental scenarios performed in a Unity environment consisting of tennis and soccer agents' competitions. The results showed that RLHC outperforms the benchmark on both competition tasks.",0
"The aim of our research is to explore how global information can enhance the learning process and increase cumulative rewards in reinforcement learning (RL) during competition tasks. Our approach involves introducing multiple cooperative critics from two levels of the hierarchy within the actor-critic RL, which we call the reinforcement learning from hierarchical critics (RLHC) algorithm. This algorithm allows each agent to receive value information from both local and global critics, and access multiple cooperative critics in a top-down hierarchy. By doing so, agents can not only obtain low-level details but also consider coordination from higher levels, leading to improved training performance. We then compared the RLHC algorithm with the benchmark algorithm, proximal policy optimisation (PPO), in two experimental scenarios involving tennis and soccer competitions within a Unity environment. Our results demonstrated that the RLHC algorithm outperformed the benchmark in both competition tasks.",1
"In the present paper, we propose an extension of the Deep Planning Network (PlaNet), also referred to as PlaNet of the Bayesians (PlaNet-Bayes). There has been a growing demand in model predictive control (MPC) in partially observable environments in which complete information is unavailable because of, for example, lack of expensive sensors. PlaNet is a promising solution to realize such latent MPC, as it is used to train state-space models via model-based reinforcement learning (MBRL) and to conduct planning in the latent space. However, recent state-of-the-art strategies mentioned in MBRR literature, such as involving uncertainty into training and planning, have not been considered, significantly suppressing the training performance. The proposed extension is to make PlaNet uncertainty-aware on the basis of Bayesian inference, in which both model and action uncertainty are incorporated. Uncertainty in latent models is represented using a neural network ensemble to approximately infer model posteriors. The ensemble of optimal action candidates is also employed to capture multimodal uncertainty in the optimality. The concept of the action ensemble relies on a general variational inference MPC (VI-MPC) framework and its instance, probabilistic action ensemble with trajectory sampling (PaETS). In this paper, we extend VI-MPC and PaETS, which have been originally introduced in previous literature, to address partially observable cases. We experimentally compare the performances on continuous control tasks, and conclude that our method can consistently improve the asymptotic performance compared with PlaNet.",0
"The present study introduces PlaNet-Bayes, an extension of the Deep Planning Network, to address the increasing demand for model predictive control in partially observable environments. PlaNet utilizes model-based reinforcement learning to train state-space models and conduct planning in the latent space. However, recent advances in MBRL literature that incorporate uncertainty in training and planning have not been considered, leading to reduced training performance. To address this issue, we propose making PlaNet uncertainty-aware through Bayesian inference, incorporating both model and action uncertainty. To represent uncertainty, a neural network ensemble is used to approximate model posteriors, and an ensemble of optimal action candidates captures multimodal uncertainty in optimality. We extend the general variational inference MPC framework and probabilistic action ensemble with trajectory sampling to address partially observable cases. Our experimental results on continuous control tasks show that PlaNet-Bayes consistently improves asymptotic performance compared to PlaNet.",1
"Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes in its learned state representation. We evaluate our method on multiple challenging procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. Our experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedurally-generated MiniGrid environments. Furthermore, we analyze the learned behavior as well as the intrinsic reward received by our agent. In contrast to previous approaches, our intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control.",0
"One of the main obstacles of model-free reinforcement learning is exploring in environments with sparse rewards. Instead of relying solely on external rewards from the environment, modern methods use internal rewards to encourage exploration. However, these methods are insufficient in procedurally-generated environments where the agent is unlikely to revisit a state. In this paper, we propose a new kind of internal reward that motivates the agent to take actions resulting in significant changes in its learned state representation. We evaluate our approach on challenging procedurally-generated tasks in MiniGrid and high-dimensional observation tasks from previous research. Our experiments show that our method is more efficient than existing exploration techniques, particularly in procedurally-generated MiniGrid environments. Additionally, we analyze the agent's learned behavior and intrinsic reward, which doesn't decrease during training and greatly rewards the agent for interacting with controllable objects, unlike previous methods.",1
"Conventional video summarization approaches based on reinforcement learning have the problem that the reward can only be received after the whole summary is generated. Such kind of reward is sparse and it makes reinforcement learning hard to converge. Another problem is that labelling each frame is tedious and costly, which usually prohibits the construction of large-scale datasets. To solve these problems, we propose a weakly supervised hierarchical reinforcement learning framework, which decomposes the whole task into several subtasks to enhance the summarization quality. This framework consists of a manager network and a worker network. For each subtask, the manager is trained to set a subgoal only by a task-level binary label, which requires much fewer labels than conventional approaches. With the guide of the subgoal, the worker predicts the importance scores for video frames in the subtask by policy gradient according to both global reward and innovative defined sub-rewards to overcome the sparse problem. Experiments on two benchmark datasets show that our proposal has achieved the best performance, even better than supervised approaches.",0
"Reinforcement learning-based video summarization methods face the challenge of receiving rewards only after the entire summary has been generated, which is sparse and makes convergence difficult. Additionally, labeling each frame is a tedious and expensive process, impeding the creation of large datasets. To address these issues, we propose a weakly supervised hierarchical reinforcement learning framework that breaks down the task into subtasks to improve the quality of summarization. The framework includes a manager and worker network for each subtask. The manager network is trained to set subgoals based on a task-level binary label, requiring fewer labels than conventional approaches. The worker network predicts the importance scores for video frames in the subtask through policy gradient, leveraging both global rewards and sub-rewards to overcome the sparse problem. Our proposal outperforms supervised approaches and achieves the best performance on two benchmark datasets.",1
"Value-based reinforcement learning (RL) methods like Q-learning have shown success in a variety of domains. One challenge in applying Q-learning to continuous-action RL problems, however, is the continuous action maximization (max-Q) required for optimal Bellman backup. In this work, we develop CAQL, a (class of) algorithm(s) for continuous-action Q-learning that can use several plug-and-play optimizers for the max-Q problem. Leveraging recent optimization results for deep neural networks, we show that max-Q can be solved optimally using mixed-integer programming (MIP). When the Q-function representation has sufficient power, MIP-based optimization gives rise to better policies and is more robust than approximate methods (e.g., gradient ascent, cross-entropy search). We further develop several techniques to accelerate inference in CAQL, which despite their approximate nature, perform well. We compare CAQL with state-of-the-art RL algorithms on benchmark continuous-control problems that have different degrees of action constraints and show that CAQL outperforms policy-based methods in heavily constrained environments, often dramatically.",0
"Q-learning, a value-based reinforcement learning method, has proven successful in various domains. However, it faces a challenge in continuous-action RL problems, where optimal Bellman backup requires continuous action maximization (max-Q). To address this, we introduce CAQL, a class of algorithms for continuous-action Q-learning that can utilize various plug-and-play optimizers for the max-Q problem. Recent optimization findings for deep neural networks reveal that max-Q can be optimally solved through mixed-integer programming (MIP), which yields better policies and greater robustness than gradient ascent or cross-entropy search. We also develop several techniques to speed up inference in CAQL, which perform well despite their approximate nature. We compare CAQL to state-of-the-art RL algorithms on benchmark continuous-control problems with different levels of action constraints and demonstrate that CAQL surpasses policy-based methods in heavily constrained environments, often with significant margins.",1
"Reinforcement learning has shown much success in games such as chess, backgammon and Go. However, in most of these games, agents have full knowledge of the environment at all times. In this paper, we describe a deep learning model that successfully optimizes its score using reinforcement learning in a game with incomplete and imperfect information. We apply our model to FlipIt, a two-player game in which both players, the attacker and the defender, compete for ownership of a shared resource and only receive information on the current state (such as the current owner of the resource, or the time since the opponent last moved, etc.) upon making a move. Our model is a deep neural network combined with Q-learning and is trained to maximize the defender's time of ownership of the resource. Despite the imperfect observations, our model successfully learns an optimal cost-effective counter-strategy and shows the advantages of the use of deep reinforcement learning in game theoretic scenarios. Our results show that it outperforms the Greedy strategy against distributions such as periodic and exponential distributions without any prior knowledge of the opponent's strategy, and we generalize the model to $n$-player games.",0
"Games like chess, backgammon, and Go have seen success with reinforcement learning, but these games feature agents with comprehensive knowledge of the environment. This research presents a deep learning model that utilizes reinforcement learning to optimize its score in a game with incomplete and imperfect information. The model is applied to FlipIt, a two-player game where the attacker and defender compete for a shared resource and receive information only after making a move. The model, a deep neural network combined with Q-learning, is trained to maximize the defender's time of ownership of the resource. Despite the imperfect observations, the model effectively learns a cost-effective counter-strategy and highlights the benefits of deep reinforcement learning in game theory scenarios. The results demonstrate the model's outperformance of the Greedy strategy against periodic and exponential distributions without prior knowledge of the opponent's strategy, and the model is generalized to $n$-player games.",1
"Exploration is critical to a reinforcement learning agent's performance in its given environment. Prior exploration methods are often based on using heuristic auxiliary predictions to guide policy behavior, lacking a mathematically-grounded objective with clear properties. In contrast, we recast exploration as a problem of State Marginal Matching (SMM), where we aim to learn a policy for which the state marginal distribution matches a given target state distribution. The target distribution is a uniform distribution in most cases, but can incorporate prior knowledge if available. In effect, SMM amortizes the cost of learning to explore in a given environment. The SMM objective can be viewed as a two-player, zero-sum game between a state density model and a parametric policy, an idea that we use to build an algorithm for optimizing the SMM objective. Using this formalism, we further demonstrate that prior work approximately maximizes the SMM objective, offering an explanation for the success of these methods. On both simulated and real-world tasks, we demonstrate that agents that directly optimize the SMM objective explore faster and adapt more quickly to new tasks as compared to prior exploration methods.",0
"Effective performance of a reinforcement learning agent in its environment relies heavily on exploration. Traditional exploration methods are often based on heuristic auxiliary predictions that guide policy behavior and lack an objective with clear properties grounded in mathematics. In contrast, exploration can be viewed as a State Marginal Matching (SMM) problem where the objective is to learn a policy that matches the state marginal distribution with a given target state distribution, which is generally a uniform distribution but can also incorporate prior knowledge. This approach reduces the cost of learning to explore in a specific environment. The SMM objective is a two-player, zero-sum game between a state density model and a parametric policy, and we use this concept to develop an algorithm for optimizing the SMM objective. This framework explains why prior methods have achieved success in approximately maximizing the SMM objective. Our experiments on simulated and real-world tasks demonstrate that direct optimization of the SMM objective leads to faster exploration and better adaptation to new tasks than traditional exploration methods.",1
"With advances in deep learning, exponential data growth and increasing model complexity, developing efficient optimization methods are attracting much research attention. Several implementations favor the use of Conjugate Gradient (CG) and Stochastic Gradient Descent (SGD) as being practical and elegant solutions to achieve quick convergence, however, these optimization processes also present many limitations in learning across deep learning applications. Recent research is exploring higher-order optimization functions as better approaches, but these present very complex computational challenges for practical use. Comparing first and higher-order optimization functions, in this paper, our experiments reveal that Levemberg-Marquardt (LM) significantly supersedes optimal convergence but suffers from very large processing time increasing the training complexity of both, classification and reinforcement learning problems. Our experiments compare off-the-shelf optimization functions(CG, SGD, LM and L-BFGS) in standard CIFAR, MNIST, CartPole and FlappyBird experiments.The paper presents arguments on which optimization functions to use and further, which functions would benefit from parallelization efforts to improve pretraining time and learning rate convergence.",0
"Efficient optimization methods are receiving much attention due to the exponential growth of data, increasing model complexity, and advances in deep learning. While Conjugate Gradient (CG) and Stochastic Gradient Descent (SGD) are practical and elegant solutions for quick convergence, they have limitations in deep learning applications. Higher-order optimization functions are being explored as better approaches, but they present complex computational challenges. In this paper, we compare first and higher-order optimization functions and our experiments show that Levemberg-Marquardt (LM) has optimal convergence but suffers from large processing time, increasing the complexity of classification and reinforcement learning problems. Our experiments compare off-the-shelf optimization functions(CG, SGD, LM and L-BFGS) in standard CIFAR, MNIST, CartPole, and FlappyBird experiments. We argue which optimization functions to use and suggest parallelization efforts to improve pretraining time and learning rate convergence.",1
"The central tenet of reinforcement learning (RL) is that agents seek to maximize the sum of cumulative rewards. In contrast, active inference, an emerging framework within cognitive and computational neuroscience, proposes that agents act to maximize the evidence for a biased generative model. Here, we illustrate how ideas from active inference can augment traditional RL approaches by (i) furnishing an inherent balance of exploration and exploitation, and (ii) providing a more flexible conceptualization of reward. Inspired by active inference, we develop and implement a novel objective for decision making, which we term the free energy of the expected future. We demonstrate that the resulting algorithm successfully balances exploration and exploitation, simultaneously achieving robust performance on several challenging RL benchmarks with sparse, well-shaped, and no rewards.",0
"The fundamental principle of reinforcement learning (RL) is that agents aim to achieve the highest sum of cumulative rewards. Conversely, active inference, a growing framework in cognitive and computational neuroscience, suggests that agents strive to maximize the evidence for a biased generative model. This article showcases how active inference concepts can enhance conventional RL methods by (i) ensuring a natural balance of exploration and exploitation, and (ii) delivering a more adaptable interpretation of reward. Drawing inspiration from active inference, we devise and implement a fresh decision-making objective, which we call the free energy of the expected future. We prove that this algorithm can effectively balance exploration and exploitation, displaying robust outcomes on various demanding RL benchmarks with sparse, well-shaped, and no rewards.",1
"Modern deep learning methods provide effective means to learn good representations. However, is a good representation itself sufficient for sample efficient reinforcement learning? This question has largely been studied only with respect to (worst-case) approximation error, in the more classical approximate dynamic programming literature. With regards to the statistical viewpoint, this question is largely unexplored, and the extant body of literature mainly focuses on conditions which permit sample efficient reinforcement learning with little understanding of what are necessary conditions for efficient reinforcement learning.   This work shows that, from the statistical viewpoint, the situation is far subtler than suggested by the more traditional approximation viewpoint, where the requirements on the representation that suffice for sample efficient RL are even more stringent. Our main results provide sharp thresholds for reinforcement learning methods, showing that there are hard limitations on what constitutes good function approximation (in terms of the dimensionality of the representation), where we focus on natural representational conditions relevant to value-based, model-based, and policy-based learning. These lower bounds highlight that having a good (value-based, model-based, or policy-based) representation in and of itself is insufficient for efficient reinforcement learning, unless the quality of this approximation passes certain hard thresholds. Furthermore, our lower bounds also imply exponential separations on the sample complexity between 1) value-based learning with perfect representation and value-based learning with a good-but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning.",0
"Although modern deep learning methods are effective for learning good representations, the question remains whether a good representation alone is enough for sample-efficient reinforcement learning. Previous research has focused on worst-case approximation error in classical approximate dynamic programming literature, leaving the statistical viewpoint largely unexplored. Little is known about the necessary conditions for efficient reinforcement learning, despite existing literature on conditions that permit sample-efficient reinforcement learning. This study reveals that the situation is more complex than previously thought, and the requirements for good function approximation are even more stringent than traditionally believed. Our results provide precise thresholds for reinforcement learning methods, demonstrating that having a good representation is not sufficient for efficient reinforcement learning unless certain hard thresholds are met. Our lower bounds also yield exponential separations in sample complexity between various learning methods, including value-based, policy-based, supervised, and imitation learning.",1
"One crucial aspect of partial domain adaptation (PDA) is how to select the relevant source samples in the shared classes for knowledge transfer. Previous PDA methods tackle this problem by re-weighting the source samples based on their high-level information (deep features). However, since the domain shift between source and target domains, only using the deep features for sample selection is defective. We argue that it is more reasonable to additionally exploit the pixel-level information for PDA problem, as the appearance difference between outlier source classes and target classes is significantly large. In this paper, we propose a reinforced transfer network (RTNet), which utilizes both high-level and pixel-level information for PDA problem. Our RTNet is composed of a reinforced data selector (RDS) based on reinforcement learning (RL), which filters out the outlier source samples, and a domain adaptation model which minimizes the domain discrepancy in the shared label space. Specifically, in the RDS, we design a novel reward based on the reconstruct errors of selected source samples on the target generator, which introduces the pixel-level information to guide the learning of RDS. Besides, we develope a state containing high-level information, which used by the RDS for sample selection. The proposed RDS is a general module, which can be easily integrated into existing DA models to make them fit the PDA situation. Extensive experiments indicate that RTNet can achieve state-of-the-art performance for PDA tasks on several benchmark datasets.",0
"Partial domain adaptation (PDA) requires the selection of relevant source samples in the shared classes for knowledge transfer. Previous PDA approaches have re-weighted source samples based on their deep features, but this is insufficient due to the domain shift between source and target domains. Therefore, it is more reasonable to use pixel-level information for PDA, as the appearance difference between outlier source classes and target classes is significant. To address this, we propose a reinforced transfer network (RTNet) that uses both high-level and pixel-level information for PDA. RTNet comprises a reinforced data selector (RDS) based on reinforcement learning (RL), which filters out the outlier source samples, and a domain adaptation model that reduces the domain discrepancy in the shared label space. The RDS uses a novel reward that considers the reconstruct errors of selected source samples on the target generator, thus introducing pixel-level information. Additionally, we develop a state containing high-level information, which the RDS uses for sample selection. The proposed RDS is a general module that can be easily integrated into existing DA models for PDA tasks. Extensive experiments demonstrate that RTNet outperforms other PDA methods on several benchmark datasets.",1
"We study the estimation of risk-sensitive policies in reinforcement learning problems defined by a Markov Decision Process (MDPs) whose state and action spaces are countably finite. Prior efforts are predominately afflicted by computational challenges associated with the fact that risk-sensitive MDPs are time-inconsistent. To ameliorate this issue, we propose a new definition of risk, which we call caution, as a penalty function added to the dual objective of the linear programming (LP) formulation of reinforcement learning. The caution measures the distributional risk of a policy, which is a function of the policy's long-term state occupancy distribution. To solve this problem in an online model-free manner, we propose a stochastic variant of primal-dual method that uses Kullback-Lieber (KL) divergence as its proximal term. We establish that the number of iterations/samples required to attain approximately optimal solutions of this scheme matches tight dependencies on the cardinality of the state and action spaces, but differs in its dependence on the infinity norm of the gradient of the risk measure. Experiments demonstrate the merits of this approach for improving the reliability of reward accumulation without additional computational burdens.",0
"Our focus is on risk-sensitive policies in reinforcement learning problems, specifically those defined by a Markov Decision Process with countably finite state and action spaces. Past attempts to estimate such policies have struggled due to computational difficulties caused by the time-inconsistency of risk-sensitive MDPs. In order to address this problem, we introduce a new definition of risk called ""caution,"" which is a penalty function added to the dual objective of the LP formulation of reinforcement learning. This measure evaluates the distributional risk of a policy based on its long-term state occupancy distribution. To address this issue in an online model-free way, we suggest a stochastic version of the primal-dual method that incorporates Kullback-Lieber divergence as its proximal term. We demonstrate that the number of iterations/samples required for nearly optimal solutions corresponds closely to the cardinality of the state and action spaces, but differs in its dependence on the infinity norm of the gradient of the risk measure. Our experiments illustrate the effectiveness of our approach in improving reward accumulation reliability without additional computational demands.",1
"Value functions derived from Markov decision processes arise as a central component of algorithms as well as performance metrics in many statistics and engineering applications of machine learning techniques. Computation of the solution to the associated Bellman equations is challenging in most practical cases of interest. A popular class of approximation techniques, known as Temporal Difference (TD) learning algorithms, are an important sub-class of general reinforcement learning methods. The algorithms introduced in this paper are intended to resolve two well-known difficulties of TD-learning approaches: Their slow convergence due to very high variance, and the fact that, for the problem of computing the relative value function, consistent algorithms exist only in special cases. First we show that the gradients of these value functions admit a representation that lends itself to algorithm design. Based on this result, a new class of differential TD-learning algorithms is introduced. For Markovian models on Euclidean space with smooth dynamics, the algorithms are shown to be consistent under general conditions. Numerical results show dramatic variance reduction when compared to standard methods.",0
"Markov decision processes produce value functions that are essential elements of machine learning techniques in various statistics and engineering applications. However, computing the solution to the corresponding Bellman equations is challenging in most practical cases. To tackle this issue, Temporal Difference (TD) learning algorithms have been introduced as a popular approximation technique in the field of general reinforcement learning methods. Nonetheless, TD-learning approaches are known to suffer from slow convergence and inconsistent algorithms for computing the relative value function. In this paper, we propose a new class of differential TD-learning algorithms that resolve these challenges by introducing a representation of the value function gradients that facilitates algorithm design. Under general conditions, the proposed algorithms are consistent for Markovian models with smooth dynamics on Euclidean space. Numerical results demonstrate a significant reduction in variance compared to standard methods.",1
"Off-policy reinforcement learning (RL) is concerned with learning a rewarding policy by executing another policy that gathers samples of experience. While the former policy (i.e. target policy) is rewarding but in-expressive (in most cases, deterministic), doing well in the latter task, in contrast, requires an expressive policy (i.e. behavior policy) that offers guided and effective exploration. Contrary to most methods that make a trade-off between optimality and expressiveness, disentangled frameworks explicitly decouple the two objectives, which each is dealt with by a distinct separate policy. Although being able to freely design and optimize the two policies with respect to their own objectives, naively disentangling them can lead to inefficient learning or stability issues. To mitigate this problem, our proposed method Analogous Disentangled Actor-Critic (ADAC) designs analogous pairs of actors and critics. Specifically, ADAC leverages a key property about Stein variational gradient descent (SVGD) to constraint the expressive energy-based behavior policy with respect to the target one for effective exploration. Additionally, an analogous critic pair is introduced to incorporate intrinsic rewards in a principled manner, with theoretical guarantees on the overall learning stability and effectiveness. We empirically evaluate environment-reward-only ADAC on 14 continuous-control tasks and report the state-of-the-art on 10 of them. We further demonstrate ADAC, when paired with intrinsic rewards, outperform alternatives in exploration-challenging tasks.",0
"The focus of off-policy reinforcement learning (RL) is to acquire a rewarding policy by implementing a different policy that collects experience samples. The target policy is rewarding but lacks expression, whereas the behavior policy requires expression for successful exploration. Many methods typically compromise between optimality and expressiveness, but disentangled frameworks separate the objectives and handle them with distinct policies. However, naive disentanglement can result in ineffective learning or instability issues. To address this, our proposed method, Analogous Disentangled Actor-Critic (ADAC), creates analogous pairs of actors and critics. ADAC constrains the expressive behavior policy using Stein variational gradient descent (SVGD) for effective exploration and introduces an analogous critic pair to include intrinsic rewards. The overall learning stability and effectiveness are guaranteed theoretically. We evaluated ADAC on 14 continuous-control tasks and achieved state-of-the-art results in 10 of them. Furthermore, ADAC paired with intrinsic rewards outperformed other methods in exploration-challenging tasks.",1
"We present a modular neural network architecture Main that learns algorithms given a set of input-output examples. Main consists of a neural controller that interacts with a variable-length input tape and learns to compose modules together with their corresponding argument choices. Unlike previous approaches, Main uses a general domain-agnostic mechanism for selection of modules and their arguments. It uses a general input tape layout together with a parallel history tape to indicate most recently used locations. Finally, it uses a memoryless controller with a length-invariant self-attention based input tape encoding to allow for random access to tape locations. The Main architecture is trained end-to-end using reinforcement learning from a set of input-output examples. We evaluate Main on five algorithmic tasks and show that it can learn policies that generalizes perfectly to inputs of much longer lengths than the ones used for training.",0
"Introducing the Main modular neural network architecture, which has the ability to learn algorithms through a given set of input-output examples. Main consists of a neural controller that interacts with a variable-length input tape and has the capability to compose modules together with their corresponding argument choices. Unlike previous approaches, Main implements a domain-agnostic mechanism for selecting modules and their arguments. A general input tape layout is utilized along with a parallel history tape to indicate the most recently used locations. Furthermore, a memoryless controller with a length-invariant self-attention based input tape encoding is employed to allow for random access to tape locations. The Main architecture is trained end-to-end using reinforcement learning from a set of input-output examples. To evaluate Main's performance, we conducted experiments on five algorithmic tasks and demonstrated that it can learn policies that generalize effectively to inputs of much longer lengths than those used for training.",1
"Deep learning has become an increasingly common technique for various control problems, such as robotic arm manipulation, robot navigation, and autonomous vehicles. However, the downside of using deep neural networks to learn control policies is their opaque nature and the difficulties of validating their safety. As the networks used to obtain state-of-the-art results become increasingly deep and complex, the rules they have learned and how they operate become more challenging to understand. This presents an issue, since in safety-critical applications the safety of the control policy must be ensured to a high confidence level. In this paper, we propose an automated black box testing framework based on adversarial reinforcement learning. The technique uses an adversarial agent, whose goal is to degrade the performance of the target model under test. We test the approach on an autonomous vehicle problem, by training an adversarial reinforcement learning agent, which aims to cause a deep neural network-driven autonomous vehicle to collide. Two neural networks trained for autonomous driving are compared, and the results from the testing are used to compare the robustness of their learned control policies. We show that the proposed framework is able to find weaknesses in both control policies that were not evident during online testing and therefore, demonstrate a significant benefit over manual testing methods.",0
"The use of deep learning is increasingly popular for control problems such as robot navigation, robotic arm manipulation, and autonomous vehicles. However, the drawback of using deep neural networks to learn control policies lies in their complex and opaque nature, which makes validating their safety challenging. This is especially problematic for safety-critical applications, where the control policy's safety must be ensured with a high level of confidence. To address this issue, we propose an automated black box testing framework based on adversarial reinforcement learning. The framework employs an adversarial agent whose objective is to degrade the performance of the target model being tested. To demonstrate the effectiveness of the approach, we apply it to an autonomous vehicle problem and train an adversarial reinforcement learning agent aimed at causing a deep neural network-driven autonomous vehicle to collide. We compare two neural networks trained for autonomous driving and use the testing results to compare the robustness of their learned control policies. Our results show that the proposed framework successfully identifies weaknesses in both control policies that were not apparent during online testing, highlighting the significant benefit of this approach over manual testing methods.",1
"Current Deep Reinforcement Learning algorithms still heavily rely on handcrafted neural network architectures. We propose a novel approach to automatically find strong topologies for continuous control tasks while only adding a minor overhead in terms of interactions in the environment. To achieve this, we combine Neuroevolution techniques with off-policy training and propose a novel architecture mutation operator. Experiments on five continuous control benchmarks show that the proposed Actor-Critic Neuroevolution algorithm often outperforms the strong Actor-Critic baseline and is capable of automatically finding topologies in a sample-efficient manner which would otherwise have to be found by expensive architecture search.",0
"The reliance on handcrafted neural network architectures remains high in current Deep Reinforcement Learning algorithms. However, we suggest a new method that can discover effective topologies for continuous control tasks without significantly increasing the interactions with the environment. Our approach involves merging Neuroevolution techniques with off-policy training and introducing a novel architecture mutation operator. Through experiments on five continuous control benchmarks, we show that the Actor-Critic Neuroevolution algorithm we propose often surpasses the strong Actor-Critic baseline. Furthermore, it can efficiently locate topologies that would otherwise require costly architecture searches.",1
"This work exploits action equivariance for representation learning in reinforcement learning. Equivariance under actions states that transitions in the input space are mirrored by equivalent transitions in latent space, while the map and transition functions should also commute. We introduce a contrastive loss function that enforces action equivariance on the learned representations. We prove that when our loss is zero, we have a homomorphism of a deterministic Markov Decision Process (MDP). Learning equivariant maps leads to structured latent spaces, allowing us to build a model on which we plan through value iteration. We show experimentally that for deterministic MDPs, the optimal policy in the abstract MDP can be successfully lifted to the original MDP. Moreover, the approach easily adapts to changes in the goal states. Empirically, we show that in such MDPs, we obtain better representations in fewer epochs compared to representation learning approaches using reconstructions, while generalizing better to new goals than model-free approaches.",0
"In this study, we utilize action equivariance as a means of representation learning in reinforcement learning. Action equivariance refers to the reflection of transitions in the input space through equivalent transitions in the latent space, with the additional requirement that the map and transition functions should also commute. Our approach involves the implementation of a contrastive loss function that enforces action equivariance in the learned representations. We have demonstrated that when the loss is zero, we achieve a homomorphism of a deterministic Markov Decision Process (MDP). The use of equivariant maps leads to structured latent spaces, allowing for the creation of a model that can be utilized for value iteration planning. Our experimental results show that for deterministic MDPs, the optimal policy in the abstract MDP can be successfully transferred to the original MDP. Additionally, our approach is easily adaptable to changes in the goal states. Our empirical findings demonstrate that in such MDPs, we obtain better representations in fewer epochs compared to representation learning methods using reconstructions, and our approach generalizes better to new goals than model-free techniques.",1
"Molecular graph generation is a fundamental problem for drug discovery and has been attracting growing attention. The problem is challenging since it requires not only generating chemically valid molecular structures but also optimizing their chemical properties in the meantime. Inspired by the recent progress in deep generative models, in this paper we propose a flow-based autoregressive model for graph generation called GraphAF. GraphAF combines the advantages of both autoregressive and flow-based approaches and enjoys: (1) high model flexibility for data density estimation; (2) efficient parallel computation for training; (3) an iterative sampling process, which allows leveraging chemical domain knowledge for valency checking. Experimental results show that GraphAF is able to generate 68% chemically valid molecules even without chemical knowledge rules and 100% valid molecules with chemical rules. The training process of GraphAF is two times faster than the existing state-of-the-art approach GCPN. After fine-tuning the model for goal-directed property optimization with reinforcement learning, GraphAF achieves state-of-the-art performance on both chemical property optimization and constrained property optimization.",0
"Generating molecular graphs is a crucial task in drug discovery and has been gaining increasing attention. The challenge lies in not only producing valid molecular structures, but also optimizing their chemical properties. Building on advancements in deep generative models, our paper introduces a graph generation model called GraphAF. This model combines the strengths of both autoregressive and flow-based techniques, offering high flexibility, parallel computation, and iterative sampling with chemical domain knowledge for valency checking. Our experiments demonstrate that GraphAF can generate 68% chemically valid molecules without chemical knowledge rules, and 100% valid molecules with such rules. Furthermore, GraphAF's training process is twice as fast as the current state-of-the-art method GCPN. After fine-tuning for goal-directed property optimization with reinforcement learning, GraphAF achieves superior performance in both chemical and constrained property optimization.",1
"For a robotic grasping task in which diverse unseen target objects exist in a cluttered environment, some deep learning-based methods have achieved state-of-the-art results using visual input directly. In contrast, actor-critic deep reinforcement learning (RL) methods typically perform very poorly when grasping diverse objects, especially when learning from raw images and sparse rewards. To make these RL techniques feasible for vision-based grasping tasks, we employ state representation learning (SRL), where we encode essential information first for subsequent use in RL. However, typical representation learning procedures are unsuitable for extracting pertinent information for learning the grasping skill, because the visual inputs for representation learning, where a robot attempts to grasp a target object in clutter, are extremely complex. We found that preprocessing based on the disentanglement of a raw input image is the key to effectively capturing a compact representation. This enables deep RL to learn robotic grasping skills from highly varied and diverse visual inputs. We demonstrate the effectiveness of this approach with varying levels of disentanglement in a realistic simulated environment.",0
"In a cluttered environment with various unseen target objects, some deep learning techniques have excelled in robotic grasping tasks using direct visual input. However, actor-critic deep reinforcement learning methods struggle to grasp diverse objects, especially when learning from sparse rewards and raw images. To address this, we incorporate state representation learning to encode necessary information for reinforcement learning. Nonetheless, traditional representation learning methods are not ideal for grasping skills as the visual inputs are complex. Disentangling the raw input image through preprocessing is crucial in capturing a concise representation that allows for effective deep RL learning of robotic grasping skills from diverse visual inputs. We demonstrate the efficacy of this approach with varying levels of disentanglement in a simulated environment.",1
"Many reinforcement learning algorithms use value functions to guide the search for better policies. These methods estimate the value of a single policy while generalizing across many states. The core idea of this paper is to flip this convention and estimate the value of many policies, for a single set of states. This approach opens up the possibility of performing direct gradient ascent in policy space without seeing any new data. The main challenge for this approach is finding a way to represent complex policies that facilitates learning and generalization. To address this problem, we introduce a scalable, differentiable fingerprinting mechanism that retains essential policy information in a concise embedding. Our empirical results demonstrate that combining these three elements (learned Policy Evaluation Network, policy fingerprints, gradient ascent) can produce policies that outperform those that generated the training data, in zero-shot manner.",0
"Value functions are frequently employed by reinforcement learning algorithms to enhance policy optimization. These algorithms estimate the value of one policy, while generalizing across several states. The paper's key idea is to estimate the value of multiple policies for a single set of states, enabling direct gradient ascent in policy space without requiring new data. The challenge is to find a way to represent complex policies that facilitates learning and generalization. To overcome this, a scalable, differentiable fingerprinting mechanism is introduced, which retains essential policy information in a concise embedding. Empirical results reveal that the combination of a learned Policy Evaluation Network, policy fingerprints, and gradient ascent can produce policies that outperform those that generated the training data, in zero-shot manner.",1
"Signalized intersections are managed by controllers that assign right of way (green, yellow, and red lights) to non-conflicting directions. Optimizing the actuation policy of such controllers is expected to alleviate traffic congestion and its adverse impact. Given such a safety-critical domain, the affiliated actuation policy is required to be interpretable in a way that can be understood and regulated by a human. This paper presents and analyzes several on-line optimization techniques for tuning interpretable control functions. Although these techniques are defined in a general way, this paper assumes a specific class of interpretable control functions (polynomial functions) for analysis purposes. We show that such an interpretable policy function can be as effective as a deep neural network for approximating an optimized signal actuation policy. We present empirical evidence that supports the use of value-based reinforcement learning for on-line training of the control function. Specifically, we present and study three variants of the Deep Q-learning algorithm that allow the training of an interpretable policy function. Our Deep Regulatable Hardmax Q-learning variant is shown to be particularly effective in optimizing our interpretable actuation policy, resulting in up to 19.4% reduced vehicles delay compared to commonly deployed actuated signal controllers.",0
"Controllers manage signalized intersections by assigning right of way to non-conflicting directions through green, yellow, and red lights. Improving the actuation policy of these controllers can help alleviate traffic congestion and its negative effects. However, the policy must be interpretable for humans to understand and regulate due to the safety-critical nature of the domain. This paper explores several online optimization techniques for tuning interpretable control functions, specifically focusing on polynomial functions. Results show that an interpretable policy function can be as effective as a deep neural network for approximating optimized signal actuation policies. The study demonstrates the effectiveness of using value-based reinforcement learning for on-line training of the control function and presents three variants of the Deep Q-learning algorithm. The Deep Regulatable Hardmax Q-learning variant is particularly effective in optimizing the interpretable actuation policy, resulting in up to 19.4% reduced vehicle delay compared to commonly deployed signal controllers.",1
"One of the key reasons for the high sample complexity in reinforcement learning (RL) is the inability to transfer knowledge from one task to another. In standard multi-task RL settings, low-reward data collected while trying to solve one task provides little to no signal for solving that particular task and is hence effectively wasted. However, we argue that this data, which is uninformative for one task, is likely a rich source of information for other tasks. To leverage this insight and efficiently reuse data, we present Generalized Hindsight: an approximate inverse reinforcement learning technique for relabeling behaviors with the right tasks. Intuitively, given a behavior generated under one task, Generalized Hindsight returns a different task that the behavior is better suited for. Then, the behavior is relabeled with this new task before being used by an off-policy RL optimizer. Compared to standard relabeling techniques, Generalized Hindsight provides a substantially more efficient reuse of samples, which we empirically demonstrate on a suite of multi-task navigation and manipulation tasks. Videos and code can be accessed here: https://sites.google.com/view/generalized-hindsight.",0
"Reinforcement learning (RL) has a high sample complexity due to the challenge of transferring knowledge between tasks. In multi-task RL settings, low-reward data collected in attempting to solve one task is often unhelpful for that specific task and is therefore wasted. However, we propose that this data can be valuable for other tasks. To make the most of this, we introduce Generalized Hindsight, an approximate inverse reinforcement learning technique that relabels behaviors with the appropriate tasks. Essentially, Generalized Hindsight identifies the task a behavior is better suited for and relabels it accordingly, making it more efficient for off-policy RL optimization. Compared to other relabeling techniques, Generalized Hindsight significantly improves sample reuse, as demonstrated through a range of navigation and manipulation tasks. More information, including videos and code, can be found at https://sites.google.com/view/generalized-hindsight.",1
"Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.",0
"Reinforcement learning (RL) benefits from optimistic initialization, a strategy that promotes efficient exploration. For efficient model-free algorithms in the tabular case, this approach is essential. However, model-free deep RL algorithms, despite drawing inspiration from these algorithms, do not employ optimistic initialization. Pessimistic initialization is often used, with Q-values initialized at their lowest values in scenarios with only positive rewards. Merely initializing a network to produce optimistic Q-values is insufficient since it is not guaranteed that they will remain optimistic for novel state-action pairs, which is essential for exploration. Our proposed solution is to augment pessimistically initialized Q-values with count-based bonuses to ensure that they remain optimistic. We demonstrate that this method is provably efficient in the tabular setting and extend it to the deep RL setting with our OPIQ algorithm. OPIQ uses count-derived bonuses to enhance the Q-value estimates of a DQN-based agent, ensuring optimism during both action selection and bootstrapping. We show that OPIQ outperforms pseudocount-based intrinsic motivation in hard exploration tasks and predicts optimistic estimates for novel state-action pairs.",1
"This paper focuses on inverse reinforcement learning (IRL) to enable safe and efficient autonomous navigation in unknown partially observable environments. The objective is to infer a cost function that explains expert-demonstrated navigation behavior while relying only on the observations and state-control trajectory used by the expert. We develop a cost function representation composed of two parts: a probabilistic occupancy encoder, with recurrent dependence on the observation sequence, and a cost encoder, defined over the occupancy features. The representation parameters are optimized by differentiating the error between demonstrated controls and a control policy computed from the cost encoder. Such differentiation is typically computed by dynamic programming through the value function over the whole state space. We observe that this is inefficient in large partially observable environments because most states are unexplored. Instead, we rely on a closed-form subgradient of the cost-to-go obtained only over a subset of promising states via an efficient motion-planning algorithm such as A* or RRT. Our experiments show that our model exceeds the accuracy of baseline IRL algorithms in robot navigation tasks, while substantially improving the efficiency of training and test-time inference.",0
"The main focus of this paper is on using inverse reinforcement learning (IRL) to ensure safe and efficient autonomous navigation in partially observable environments that are unknown. The aim is to deduce a cost function that explains the navigation behavior of an expert, using only the expert's observations and state-control trajectory. The cost function representation is made up of two parts: a probabilistic occupancy encoder with recurrent dependence on the observation sequence, and a cost encoder that is defined over the occupancy features. The representation parameters are optimized by minimizing the error between the demonstrated controls and a control policy computed from the cost encoder. However, computing the differentiation of the error is inefficient in large partially observable environments because most states have not been explored. Instead, a closed-form subgradient of the cost-to-go is used, obtained only over a subset of promising states via an efficient motion-planning algorithm like A* or RRT. The experiments demonstrate that this model surpasses baseline IRL algorithms in robot navigation tasks and also improves the efficiency of training and test-time inference.",1
"Policy gradient and actor-critic algorithms form the basis of many commonly used training techniques in deep reinforcement learning. Using these algorithms in multiagent environments poses problems such as nonstationarity and instability. In this paper, we first demonstrate that standard softmax-based policy gradient can be prone to poor performance in the presence of even the most benign nonstationarity. By contrast, it is known that the replicator dynamics, a well-studied model from evolutionary game theory, eliminates dominated strategies and exhibits convergence of the time-averaged trajectories to interior Nash equilibria in zero-sum games. Thus, using the replicator dynamics as a foundation, we derive an elegant one-line change to policy gradient methods that simply bypasses the gradient step through the softmax, yielding a new algorithm titled Neural Replicator Dynamics (NeuRD). NeuRD reduces to the exponential weights/Hedge algorithm in the single-state all-actions case. Additionally, NeuRD has formal equivalence to softmax counterfactual regret minimization, which guarantees convergence in the sequential tabular case. Importantly, our algorithm provides a straightforward way of extending the replicator dynamics to the function approximation setting. Empirical results show that NeuRD quickly adapts to nonstationarities, outperforming policy gradient significantly in both tabular and function approximation settings, when evaluated on the standard imperfect information benchmarks of Kuhn Poker, Leduc Poker, and Goofspiel.",0
"Deep reinforcement learning commonly uses policy gradient and actor-critic algorithms for training. However, these algorithms face challenges like nonstationarity and instability when used in multiagent environments. This paper highlights how even benign nonstationarity can negatively impact the standard softmax-based policy gradient's performance. On the other hand, replicator dynamics, which is a well-established model in evolutionary game theory, eliminates dominated strategies and converges to interior Nash equilibria in zero-sum games. Using replicator dynamics as a foundation, the authors propose a one-line change to policy gradient methods that bypasses the gradient step through the softmax, resulting in a new algorithm called Neural Replicator Dynamics (NeuRD). NeuRD reduces to the exponential weights/Hedge algorithm in the single-state all-actions case and has formal equivalence to softmax counterfactual regret minimization. NeuRD also provides a straightforward approach to extending the replicator dynamics to the function approximation setting. Empirical results demonstrate that NeuRD quickly adapts to nonstationarities and outperforms policy gradient significantly in both tabular and function approximation settings when evaluated on the standard imperfect information benchmarks of Kuhn Poker, Leduc Poker, and Goofspiel.",1
"Thompson Sampling is a well established approach to bandit and reinforcement learning problems. However its use in continuum armed bandit problems has received relatively little attention. We provide the first bounds on the regret of Thompson Sampling for continuum armed bandits under weak conditions on the function class containing the true function and sub-exponential observation noise. Our bounds are realised by analysis of the eluder dimension, a recently proposed measure of the complexity of a function class, which has been demonstrated to be useful in bounding the Bayesian regret of Thompson Sampling for simpler bandit problems under sub-Gaussian observation noise. We derive a new bound on the eluder dimension for classes of functions with Lipschitz derivatives, and generalise previous analyses in multiple regards.",0
"Although Thompson Sampling is a widely used technique for solving bandit and reinforcement learning problems, it has not been extensively studied for continuum armed bandit problems. Our study introduces the first set of bounds on the regret of Thompson Sampling for continuum armed bandits, with weak conditions imposed on the function class that contains the true function and sub-exponential observation noise. Using the eluder dimension, a recently proposed measure of function class complexity, we analyze and derive new bounds for classes of functions with Lipschitz derivatives. Our analysis builds on previous studies and provides a more comprehensive understanding of the performance of Thompson Sampling under sub-Gaussian observation noise.",1
"Practitioners often rely on compute-intensive domain randomization to ensure reinforcement learning policies trained in simulation can robustly transfer to the real world. Due to unmodeled nonlinearities in the real system, however, even such simulated policies can still fail to perform stably enough to acquire experience in real environments. In this paper we propose a novel method that guarantees a stable region of attraction for the output of a policy trained in simulation, even for highly nonlinear systems. Our core technique is to use ""bias-shifted"" neural networks for constructing the controller and training the network in the simulator. The modified neural networks not only capture the nonlinearities of the system but also provably preserve linearity in a certain region of the state space and thus can be tuned to resemble a linear quadratic regulator that is known to be stable for the real system. We have tested our new method by transferring simulated policies for a swing-up inverted pendulum to real systems and demonstrated its efficacy.",0
"To ensure that reinforcement learning policies trained in simulation can transfer to the real world, domain randomization that is compute-intensive is often used by practitioners. However, unmodeled nonlinearities in the real system can cause even simulated policies to perform unstably when gaining experience in real environments. In this paper, we present a novel approach that guarantees a stable region of attraction for the output of a policy trained in simulation, even for highly nonlinear systems. Our technique involves using ""bias-shifted"" neural networks to construct the controller and train the network in the simulator. These modified neural networks not only capture system nonlinearities but also preserve linearity in a specific state space region, which can be adjusted to resemble a linear quadratic regulator that is known to be stable for the real system. Our new method was tested by transferring simulated policies for a swing-up inverted pendulum to real systems, and its efficacy was demonstrated.",1
"The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a ""lucky"" sub-network initialization being present rather than by helping the optimization process (Frankle & Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether ""winning ticket"" initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL).For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with workin supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs.",0
"The lottery ticket hypothesis states that deep neural networks (DNNs) can be trained more effectively by increasing the likelihood of a ""lucky"" sub-network initialization, rather than by enhancing the optimization process itself (Frankle & Carbin, 2019). This theory implies that initialization strategies for DNNs could be substantially improved, but it has only been previously examined in the context of supervised learning for natural image tasks. The purpose of this study is to investigate whether ""winning ticket"" initializations also exist in two different domains, namely natural language processing (NLP) and reinforcement learning (RL). Our research examined recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017) for NLP, and a variety of discrete-action space tasks for RL, including classic control and pixel control. Our findings confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates, for both NLP and RL. Additionally, we discovered winning ticket initializations for Transformers, enabling models one-third the size to achieve nearly equivalent performance. These results suggest that the lottery ticket hypothesis is not limited to supervised learning of natural images, but rather represents a widespread phenomenon in DNNs.",1
"Multi-task reinforcement learning (RL) aims to simultaneously learn policies for solving many tasks. Several prior works have found that relabeling past experience with different reward functions can improve sample efficiency. Relabeling methods typically ask: if, in hindsight, we assume that our experience was optimal for some task, for what task was it optimal? In this paper, we show that hindsight relabeling is inverse RL, an observation that suggests that we can use inverse RL in tandem for RL algorithms to efficiently solve many tasks. We use this idea to generalize goal-relabeling techniques from prior work to arbitrary classes of tasks. Our experiments confirm that relabeling data using inverse RL accelerates learning in general multi-task settings, including goal-reaching, domains with discrete sets of rewards, and those with linear reward functions.",0
"The objective of multi-task reinforcement learning is to learn different policies to solve multiple tasks at once. Previous studies have concluded that changing the reward function in past experiences can enhance the efficiency of the learning process. Typically, this involves asking which task the experience was optimal for, assuming it was optimal for a particular task in hindsight. In this study, we propose that hindsight relabeling is a form of inverse RL, which can be used in conjunction with RL algorithms to efficiently solve multiple tasks. We apply this concept to generalize goal-relabeling methods for various classes of tasks. Our experiments show that using inverse RL to relabel data speeds up the learning process in general multi-task settings, including those with goal-reaching, discrete rewards, and linear reward functions.",1
"Model-free reinforcement learning is known to be memory and computation efficient and more amendable to large scale problems. In this paper, two model-free algorithms are introduced for learning infinite-horizon average-reward Markov Decision Processes (MDPs). The first algorithm reduces the problem to the discounted-reward version and achieves $\mathcal{O}(T^{2/3})$ regret after $T$ steps, under the minimal assumption of weakly communicating MDPs. To our knowledge, this is the first model-free algorithm for general MDPs in this setting. The second algorithm makes use of recent advances in adaptive algorithms for adversarial multi-armed bandits and improves the regret to $\mathcal{O}(\sqrt{T})$, albeit with a stronger ergodic assumption. This result significantly improves over the $\mathcal{O}(T^{3/4})$ regret achieved by the only existing model-free algorithm by Abbasi-Yadkori et al. (2019a) for ergodic MDPs in the infinite-horizon average-reward setting.",0
"The efficiency of model-free reinforcement learning is well-known, particularly in relation to its memory and computation capabilities, making it more suitable for dealing with large-scale problems. This study presents two model-free algorithms that enable learning of infinite-horizon average-reward Markov Decision Processes (MDPs). The first algorithm simplifies the problem to the discounted-reward version and can achieve regret of $\mathcal{O}(T^{2/3})$ after $T$ steps, with only the assumption of weakly communicating MDPs. This marks the first model-free algorithm of its kind for general MDPs in this context. The second algorithm builds on recent advancements in adaptive algorithms for adversarial multi-armed bandits, improving the regret to $\mathcal{O}(\sqrt{T})$, albeit with a stronger ergodic assumption. This result is a significant improvement over the $\mathcal{O}(T^{3/4})$ regret of the only existing model-free algorithm for ergodic MDPs in the infinite-horizon average-reward setting, as developed by Abbasi-Yadkori et al. (2019a).",1
"The success of deep neural networks relies on significant architecture engineering. Recently neural architecture search (NAS) has emerged as a promise to greatly reduce manual effort in network design by automatically searching for optimal architectures, although typically such algorithms need an excessive amount of computational resources, e.g., a few thousand GPU-days. To date, on challenging vision tasks such as object detection, NAS, especially fast versions of NAS, is less studied. Here we propose to search for the decoder structure of object detectors with search efficiency being taken into consideration. To be more specific, we aim to efficiently search for the feature pyramid network (FPN) as well as the prediction head of a simple anchor-free object detector, namely FCOS, using a tailored reinforcement learning paradigm. With carefully designed search space, search algorithms and strategies for evaluating network quality, we are able to efficiently search a top-performing detection architecture within 4 days using 8 V100 GPUs. The discovered architecture surpasses state-of-the-art object detection models (such as Faster R-CNN, RetinaNet and FCOS) by 1.5 to 3.5 points in AP on the COCO dataset, with comparable computation complexity and memory footprint, demonstrating the efficacy of the proposed NAS for object detection.",0
"Deep neural networks' success depends heavily on carefully engineered architectures. Neural architecture search (NAS) has recently emerged as a promising solution to minimize manual effort in designing networks by automatically searching for optimal architectures. However, these algorithms typically require significant computational resources, such as a few thousand GPU-days. Current research on challenging vision tasks, such as object detection, has yet to extensively explore NAS, particularly with fast versions. This study focuses on efficiently searching for the decoder structure of object detectors while prioritizing search efficiency. Specifically, we aim to search for the feature pyramid network (FPN) and prediction head of a simple anchor-free object detector, FCOS, using a tailored reinforcement learning model. Our carefully designed search space, algorithms, and network quality evaluation strategies enable us to efficiently search for the top-performing detection architecture within four days using eight V100 GPUs. The discovered architecture outperforms state-of-the-art object detection models (e.g., Faster R-CNN, RetinaNet, and FCOS) by 1.5 to 3.5 points in AP on the COCO dataset, with comparable computation complexity and memory footprint. This demonstrates the effectiveness of our proposed NAS approach for object detection.",1
"The disparate experimental conditions in recent off-policy policy evaluation (OPE) literature make it difficult both for practitioners to choose a reliable estimator for their application domain, as well as for researchers to identify fruitful research directions. In this work, we present the first detailed empirical study of a broad suite of OPE methods. Based on thousands of experiments and empirical analysis, we offer a summarized set of guidelines to advance the understanding of OPE performance in practice, and suggest directions for future research. Along the way, our empirical findings challenge several commonly held beliefs about which class of approaches tends to perform well. Our accompanying software implementation serves as a first comprehensive benchmark for OPE.",0
"The variety of experimental conditions used in recent off-policy policy evaluation (OPE) literature poses challenges for practitioners in selecting a reliable estimator for their specific needs, as well as for researchers in determining productive research directions. This study presents the initial comprehensive empirical examination of a wide range of OPE methods. Through an analysis of thousands of experiments, we present a condensed set of recommendations to enhance the comprehension of OPE performance in practical applications, and suggest potential avenues for future research. Our empirical discoveries challenge multiple commonly accepted beliefs about the efficacy of certain approaches. Additionally, our associated software implementation serves as an extensive OPE benchmark.",1
"In this paper, we propose a derivative-free model learning framework for Reinforcement Learning (RL) algorithms based on Gaussian Process Regression (GPR). In many mechanical systems, only positions can be measured by the sensing instruments. Then, instead of representing the system state as suggested by the physics with a collection of positions, velocities, and accelerations, we define the state as the set of past position measurements. However, the equation of motions derived by physical first principles cannot be directly applied in this framework, being functions of velocities and accelerations. For this reason, we introduce a novel derivative-free physically-inspired kernel, which can be easily combined with nonparametric derivative-free Gaussian Process models. Tests performed on two real platforms show that the considered state definition combined with the proposed model improves estimation performance and data-efficiency w.r.t. traditional models based on GPR. Finally, we validate the proposed framework by solving two RL control problems for two real robotic systems.",0
"The purpose of this paper is to suggest a framework for Reinforcement Learning (RL) algorithms that uses Gaussian Process Regression (GPR) for model learning, without relying on derivatives. In situations where only positions can be measured by sensors in mechanical systems, the state is defined as a set of past position measurements rather than the traditional method of including velocities and accelerations. However, the equation of motion based on physical principles cannot be used directly in this framework since it involves velocities and accelerations. Thus, a new physically-inspired kernel is introduced, which can be combined with nonparametric derivative-free Gaussian Process models. The framework is tested on two real platforms, and the results show that the proposed model outperforms traditional models based on GPR in terms of data-efficiency and estimation performance. Additionally, two RL control problems for two real robotic systems are solved using the proposed framework, validating its effectiveness.",1
"We consider the problem of finding Nash equilibrium for two-player turn-based zero-sum games. Inspired by the AlphaGo Zero (AGZ) algorithm, we develop a Reinforcement Learning based approach. Specifically, we propose Explore-Improve-Supervise (EIS) method that combines ""exploration"", ""policy improvement""' and ""supervised learning"" to find the value function and policy associated with Nash equilibrium. We identify sufficient conditions for convergence and correctness for such an approach. For a concrete instance of EIS where random policy is used for ""exploration"", Monte-Carlo Tree Search is used for ""policy improvement"" and Nearest Neighbors is used for ""supervised learning"", we establish that this method finds an $\varepsilon$-approximate value function of Nash equilibrium in $\widetilde{O}(\varepsilon^{-(d+4)})$ steps when the underlying state-space of the game is continuous and $d$-dimensional. This is nearly optimal as we establish a lower bound of $\widetilde{\Omega}(\varepsilon^{-(d+2)})$ for any policy.",0
"Our focus is on the challenge of discovering Nash equilibrium in turn-based zero-sum games played by two players. Drawing inspiration from the AlphaGo Zero algorithm, we have devised a Reinforcement Learning-based technique called Explore-Improve-Supervise (EIS), which blends ""exploration"", ""policy improvement"", and ""supervised learning"" to determine the value function and policy linked to Nash equilibrium. We have also identified sufficient conditions for the convergence and accuracy of this approach. In one example of EIS that employs a random policy for ""exploration"", Monte-Carlo Tree Search for ""policy improvement"", and Nearest Neighbors for ""supervised learning"", we have proven that the method can determine an $\varepsilon$-approximate value function of Nash equilibrium in $\widetilde{O}(\varepsilon^{-(d+4)})$ steps for continuous $d$-dimensional state-spaces. This is almost optimal, as we have discovered a lower bound of $\widetilde{\Omega}(\varepsilon^{-(d+2)})$ for any policy.",1
"Bayesian optimization has recently emerged as a popular method for the sample-efficient optimization of expensive black-box functions. However, the application to high-dimensional problems with several thousand observations remains challenging, and on difficult problems Bayesian optimization is often not competitive with other paradigms. In this paper we take the view that this is due to the implicit homogeneity of the global probabilistic models and an overemphasized exploration that results from global acquisition. This motivates the design of a local probabilistic approach for global optimization of large-scale high-dimensional problems. We propose the $\texttt{TuRBO}$ algorithm that fits a collection of local models and performs a principled global allocation of samples across these models via an implicit bandit approach. A comprehensive evaluation demonstrates that $\texttt{TuRBO}$ outperforms state-of-the-art methods from machine learning and operations research on problems spanning reinforcement learning, robotics, and the natural sciences.",0
"The optimization of expensive black-box functions can be achieved using Bayesian optimization, which has gained popularity due to its sample-efficient approach. However, the method faces challenges when applied to high-dimensional problems with thousands of observations, and in some cases, it is not as effective as other paradigms. This is due to the homogeneity of the global probabilistic models and excessive exploration resulting from global acquisition. To address this, we propose a local probabilistic approach for global optimization of large-scale high-dimensional problems, which is implemented through the $\texttt{TuRBO}$ algorithm. This algorithm fits a collection of local models and employs an implicit bandit approach to perform a principled global allocation of samples across these models. Comprehensive evaluations show that $\texttt{TuRBO}$ outperforms state-of-the-art methods from machine learning and operations research, and is effective in solving problems in reinforcement learning, robotics, and the natural sciences.",1
"Model-based reinforcement learning algorithms make decisions by building and utilizing a model of the environment. However, none of the existing algorithms attempts to infer the dynamics of any state-action pair from known state-action pairs before meeting it for sufficient times. We propose a new model-based method called Greedy Inference Model (GIM) that infers the unknown dynamics from known dynamics based on the internal spectral properties of the environment. In other words, GIM can ""learn by analogy"". We further introduce a new exploration strategy which ensures that the agent rapidly and evenly visits unknown state-action pairs. GIM is much more computationally efficient than state-of-the-art model-based algorithms, as the number of dynamic programming operations is independent of the environment size. Lower sample complexity could also be achieved under mild conditions compared against methods without inferring. Experimental results demonstrate the effectiveness and efficiency of GIM in a variety of real-world tasks.",0
"The decisions made by model-based reinforcement learning algorithms rely on an understanding of the environment. However, current algorithms do not attempt to deduce the dynamics of state-action pairs until they have encountered them multiple times. To address this issue, we propose the Greedy Inference Model (GIM), a new method that can infer unknown dynamics from known dynamics using the internal spectral properties of the environment. This allows GIM to learn by analogy. Additionally, we introduce an exploration strategy that ensures the agent visits unknown state-action pairs quickly and evenly. Compared to other model-based algorithms, GIM is more computationally efficient since the number of dynamic programming operations is independent of the environment size. It also achieves lower sample complexity under mild conditions when compared to methods that do not infer. Our experimental results demonstrate the effectiveness and efficiency of GIM in various real-world tasks.",1
"Model-free reinforcement learning (RL) is a powerful, general tool for learning complex behaviors. However, its sample efficiency is often impractically large for solving challenging real-world problems, even with off-policy algorithms such as Q-learning. A limiting factor in classic model-free RL is that the learning signal consists only of scalar rewards, ignoring much of the rich information contained in state transition tuples. Model-based RL uses this information, by training a predictive model, but often does not achieve the same asymptotic performance as model-free RL due to model bias. We introduce temporal difference models (TDMs), a family of goal-conditioned value functions that can be trained with model-free learning and used for model-based control. TDMs combine the benefits of model-free and model-based RL: they leverage the rich information in state transitions to learn very efficiently, while still attaining asymptotic performance that exceeds that of direct model-based RL methods. Our experimental results show that, on a range of continuous control tasks, TDMs provide a substantial improvement in efficiency compared to state-of-the-art model-based and model-free methods.",0
"Although model-free reinforcement learning (RL) is a useful tool for acquiring complex behaviors, it often requires impractical amounts of sampling to solve difficult real-world problems, even with off-policy algorithms such as Q-learning. Classic model-free RL is restricted by the fact that the learning signal is limited to scalar rewards, disregarding the valuable information contained in state transition tuples. Model-based RL addresses this issue by utilizing predictive models; however, it typically does not achieve the same asymptotic performance as model-free RL due to model bias. The solution to this problem is temporal difference models (TDMs), a family of goal-conditioned value functions that can be trained using model-free learning and applied to model-based control. TDMs provide the advantages of both model-free and model-based RL by utilizing the abundant information in state transitions for efficient learning, while still achieving asymptotic performance that surpasses that of direct model-based RL methods. Our experimental results reveal that TDMs significantly improve efficiency in a range of continuous control tasks compared to state-of-the-art model-based and model-free methods.",1
"Autonomous systems possess the features of inferring their own ego-motion, autonomously understanding their surroundings, and planning trajectories. With the applications of deep learning and reinforcement learning, the perception and decision-making abilities of autonomous systems are being efficiently addressed, and many new learning-based algorithms have surfaced with respect to autonomous perception and decision-making. In this review, we focus on the applications of learning-based approaches in perception and decision-making in autonomous systems, which is different from previous reviews that discussed traditional methods. First, we delineate the existing classical simultaneous localization and mapping (SLAM) solutions and review the environmental perception and understanding methods based on deep learning, including deep learning-based monocular depth estimation, ego-motion prediction, image enhancement, object detection, semantic segmentation, and their combinations with traditional SLAM frameworks. Second, we briefly summarize the existing motion planning techniques, such as path planning and trajectory planning methods, and discuss the navigation methods based on reinforcement learning. Finally, we examine the several challenges and promising directions discussed and concluded in related research for future works in the era of computer science, automatic control, and robotics.",0
"Autonomous systems have the ability to deduce their own motion, comprehend their surroundings, and plan routes independent of human intervention. Thanks to deep learning and reinforcement learning, autonomous systems can now perceive and make decisions more efficiently, resulting in the emergence of numerous new learning-based algorithms. This review focuses on the use of learning-based approaches in autonomous systems' perception and decision-making, which is distinct from earlier reviews that focused on traditional methods. The review first examines existing classical simultaneous localization and mapping (SLAM) solutions and reviews environmental perception and comprehension methods based on deep learning, including deep learning-based monocular depth estimation, ego-motion prediction, image enhancement, object detection, semantic segmentation, and their integration with traditional SLAM frameworks. The second part briefly summarizes existing motion planning techniques, such as path planning and trajectory planning methods, and discusses navigation methods based on reinforcement learning. Finally, the review examines the various challenges and promising directions discussed in related research and concludes with future works in the fields of computer science, automatic control, and robotics.",1
"Despite the great empirical success of deep reinforcement learning, its theoretical foundation is less well understood. In this work, we make the first attempt to theoretically understand the deep Q-network (DQN) algorithm (Mnih et al., 2015) from both algorithmic and statistical perspectives. In specific, we focus on a slight simplification of DQN that fully captures its key features. Under mild assumptions, we establish the algorithmic and statistical rates of convergence for the action-value functions of the iterative policy sequence obtained by DQN. In particular, the statistical error characterizes the bias and variance that arise from approximating the action-value function using deep neural network, while the algorithmic error converges to zero at a geometric rate. As a byproduct, our analysis provides justifications for the techniques of experience replay and target network, which are crucial to the empirical success of DQN. Furthermore, as a simple extension of DQN, we propose the Minimax-DQN algorithm for zero-sum Markov game with two players. Borrowing the analysis of DQN, we also quantify the difference between the policies obtained by Minimax-DQN and the Nash equilibrium of the Markov game in terms of both the algorithmic and statistical rates of convergence.",0
"While deep reinforcement learning has been successful empirically, its theoretical basis is not well understood. This paper aims to provide a theoretical understanding of the deep Q-network (DQN) algorithm (Mnih et al., 2015) from both algorithmic and statistical perspectives. We focus on a simplified version of DQN that captures its key features and establish the algorithmic and statistical rates of convergence for the action-value functions of the iterative policy sequence obtained by DQN, assuming mild assumptions. The statistical error measures the bias and variance that arise from approximating the action-value function using a deep neural network, while the algorithmic error converges to zero at a geometric rate. Our analysis also justifies the techniques of experience replay and target network, which are crucial to DQN's empirical success. Additionally, we propose the Minimax-DQN algorithm as a simple extension of DQN for zero-sum Markov games with two players. Borrowing from the analysis of DQN, we quantify the difference between the policies obtained by Minimax-DQN and the Nash equilibrium of the Markov game in terms of both algorithmic and statistical rates of convergence.",1
"In this paper, we consider the source of Deep Reinforcement Learning (DRL)'s sample complexity, asking how much derives from the requirement of learning useful representations of environment states and how much is due to the sample complexity of learning a policy. While for DRL agents, the distinction between representation and policy may not be clear, we seek new insight through a set of transfer learning experiments. In each experiment, we retain some fraction of layers trained on either the same game or a related game, comparing the benefits of transfer learning to learning a policy from scratch. Interestingly, we find that benefits due to transfer are highly variable in general and non-symmetric across pairs of tasks. Our experiments suggest that perhaps transfer from simpler environments can boost performance on more complex downstream tasks and that the requirements of learning a useful representation can range from negligible to the majority of the sample complexity, based on the environment. Furthermore, we find that fine-tuning generally outperforms training with the transferred layers frozen, confirming an insight first noted in the classification setting.",0
"The focus of this paper is on understanding the factors contributing to the sample complexity of Deep Reinforcement Learning (DRL), specifically the extent to which it is influenced by learning useful representations of environment states versus learning a policy. Although the distinction between representation and policy may not be clear for DRL agents, a series of transfer learning experiments are conducted to gain new insights. These experiments involve retaining some layers from either the same or a related game, and comparing the benefits of transfer learning to learning a policy from scratch. Results reveal that transfer benefits are highly variable and non-symmetric across tasks. It is suggested that transfer from simpler environments may improve performance on more complex tasks, and the significance of learning a useful representation can vary depending on the environment. Additionally, fine-tuning is found to outperform training with frozen transferred layers, as noted in the classification setting.",1
"In mobile health (mHealth), reinforcement learning algorithms that adapt to one's context without learning personalized policies might fail to distinguish between the needs of individuals. Yet the high amount of noise due to the in situ delivery of mHealth interventions can cripple the ability of an algorithm to learn when given access to only a single user's data, making personalization challenging. We present IntelligentPooling, which learns personalized policies via an adaptive, principled use of other users' data. We show that IntelligentPooling achieves an average of 26% lower regret than state-of-the-art across all generative models. Additionally, we inspect the behavior of this approach in a live clinical trial, demonstrating its ability to learn from even a small group of users.",0
"When it comes to mobile health (mHealth), reinforcement learning algorithms that don't learn personalized policies can fail to differentiate between individuals' needs. However, the high level of noise caused by in situ delivery of mHealth interventions can hinder an algorithm's ability to learn when given access to only one user's data, making personalization a challenge. To address this, we introduce IntelligentPooling, which uses an adaptive and principled approach to leverage other users' data to learn personalized policies. Our research shows that IntelligentPooling outperforms state-of-the-art models by an average of 26% across all generative models. We also demonstrate its effectiveness in a real-world clinical trial, proving its ability to learn from a small group of users.",1
"Deep reinforcement learning is successful in decision making for sophisticated games, such as Atari, Go, etc. However, real-world decision making often requires reasoning with partial information extracted from complex visual observations. This paper presents Discriminative Particle Filter Reinforcement Learning (DPFRL), a new reinforcement learning framework for complex partial observations. DPFRL encodes a differentiable particle filter in the neural network policy for explicit reasoning with partial observations over time. The particle filter maintains a belief using learned discriminative update, which is trained end-to-end for decision making. We show that using the discriminative update instead of standard generative models results in significantly improved performance, especially for tasks with complex visual observations, because they circumvent the difficulty of modeling complex observations that are irrelevant to decision making. In addition, to extract features from the particle belief, we propose a new type of belief feature based on the moment generating function. DPFRL outperforms state-of-the-art POMDP RL models in Flickering Atari Games, an existing POMDP RL benchmark, and in Natural Flickering Atari Games, a new, more challenging POMDP RL benchmark introduced in this paper. Further, DPFRL performs well for visual navigation with real-world data in the Habitat environment.",0
"While deep reinforcement learning has been successful in decision making for advanced games like Atari and Go, real-world decision making often involves reasoning with partial information extracted from complex visual observations. This study introduces a new reinforcement learning framework called Discriminative Particle Filter Reinforcement Learning (DPFRL) that addresses complex partial observations. DPFRL incorporates a differentiable particle filter into the neural network policy for explicit reasoning with partial observations overtime. The particle filter utilizes a learned discriminative update to maintain a belief that is trained end-to-end for decision making. The study demonstrates that using the discriminative update in place of standard generative models considerably enhances performance, particularly for tasks with complex visual observations, by avoiding the challenge of modeling irrelevant complex observations. Additionally, the study proposes a new belief feature based on the moment generating function to extract features from the particle belief. DPFRL outperforms current POMDP RL models in the Flickering Atari Games benchmark and in the new, more challenging POMDP RL benchmark called Natural Flickering Atari Games proposed in this study. Furthermore, DPFRL performs well for visual navigation with real-world data in the Habitat environment.",1
"Stochastic shortest path (SSP) is a well-known problem in planning and control, in which an agent has to reach a goal state in minimum total expected cost. In the learning formulation of the problem, the agent is unaware of the environment dynamics (i.e., the transition function) and has to repeatedly play for a given number of episodes while reasoning about the problem's optimal solution. Unlike other well-studied models in reinforcement learning (RL), the length of an episode is not predetermined (or bounded) and is influenced by the agent's actions. Recently, Tarbouriech et al. (2019) studied this problem in the context of regret minimization and provided an algorithm whose regret bound is inversely proportional to the square root of the minimum instantaneous cost. In this work we remove this dependence on the minimum cost---we give an algorithm that guarantees a regret bound of $\widetilde{O}(B_\star |S| \sqrt{|A| K})$, where $B_\star$ is an upper bound on the expected cost of the optimal policy, $S$ is the set of states, $A$ is the set of actions and $K$ is the number of episodes. We additionally show that any learning algorithm must have at least $\Omega(B_\star \sqrt{|S| |A| K})$ regret in the worst case.",0
"The problem of stochastic shortest path (SSP) is a familiar challenge in planning and control, requiring an agent to reach a goal state with the minimum total expected cost. In the learning aspect of the problem, the agent is unaware of the environment dynamics, meaning the transition function, and must repeatedly play for a certain number of episodes while considering the optimal solution. Unlike other reinforcement learning models, the length of an episode in SSP is not predetermined and can be influenced by the agent's actions. Recently, Tarbouriech et al. (2019) approached this problem in the context of regret minimization and created an algorithm whose regret bound is inversely proportional to the square root of the minimum instantaneous cost. In our work, we remove this dependence on the minimum cost by presenting an algorithm that guarantees a regret bound of $\widetilde{O}(B_\star |S| \sqrt{|A| K})$. Here, $B_\star$ is an upper bound on the expected cost of the optimal policy, $S$ is the set of states, $A$ is the set of actions, and $K$ is the number of episodes. Furthermore, we prove that any learning algorithm must have at least $\Omega(B_\star \sqrt{|S| |A| K})$ regret in the worst case.",1
"The use of target networks is a common practice in deep reinforcement learning for stabilizing the training; however, theoretical understanding of this technique is still limited. In this paper, we study the so-called periodic Q-learning algorithm (PQ-learning for short), which resembles the technique used in deep Q-learning for solving infinite-horizon discounted Markov decision processes (DMDP) in the tabular setting. PQ-learning maintains two separate Q-value estimates - the online estimate and target estimate. The online estimate follows the standard Q-learning update, while the target estimate is updated periodically. In contrast to the standard Q-learning, PQ-learning enjoys a simple finite time analysis and achieves better sample complexity for finding an epsilon-optimal policy. Our result provides a preliminary justification of the effectiveness of utilizing target estimates or networks in Q-learning algorithms.",0
"The practice of using target networks is commonplace in deep reinforcement learning to ensure stability during training. However, the theoretical understanding of this approach remains limited. This study delves into the periodic Q-learning algorithm (PQ-learning), which is similar to deep Q-learning in solving infinite-horizon discounted Markov decision processes (DMDP) in a tabular setting. PQ-learning maintains two Q-value estimates: the online and target estimates. The online estimate follows standard Q-learning updates, while the target estimate is updated periodically. Compared to standard Q-learning, PQ-learning has a straightforward finite time analysis and achieves better sample complexity in finding an epsilon-optimal policy. These results justify the use of target estimates or networks in Q-learning algorithms.",1
"We propose a targeted communication architecture for multi-agent reinforcement learning, where agents learn both what messages to send and whom to address them to while performing cooperative tasks in partially-observable environments. This targeting behavior is learnt solely from downstream task-specific reward without any communication supervision. We additionally augment this with a multi-round communication approach where agents coordinate via multiple rounds of communication before taking actions in the environment. We evaluate our approach on a diverse set of cooperative multi-agent tasks, of varying difficulties, with varying number of agents, in a variety of environments ranging from 2D grid layouts of shapes and simulated traffic junctions to 3D indoor environments, and demonstrate the benefits of targeted and multi-round communication. Moreover, we show that the targeted communication strategies learned by agents are interpretable and intuitive. Finally, we show that our architecture can be easily extended to mixed and competitive environments, leading to improved performance and sample complexity over recent state-of-the-art approaches.",0
"Our proposed communication architecture for multi-agent reinforcement learning involves agents learning how to effectively communicate with each other while working together on tasks in environments that are partially observable. This behavior is learned solely based on the rewards received from completing the task, without any supervision. We also incorporate a multi-round communication approach where agents communicate multiple times before taking action. We test our approach on various cooperative multi-agent tasks in different environments and show the advantages of targeted and multi-round communication. Additionally, we demonstrate that the communication strategies learned by the agents are understandable and logical. Finally, we extend our architecture to mixed and competitive environments, resulting in improved performance and less need for samples compared to recent state-of-the-art methods.",1
"Inspired by the adaptation phenomenon of neuronal firing, we propose the regularity normalization (RN) as an unsupervised attention mechanism (UAM) which computes the statistical regularity in the implicit space of neural networks under the Minimum Description Length (MDL) principle. Treating the neural network optimization process as a partially observable model selection problem, UAM constrains the implicit space by a normalization factor, the universal code length. We compute this universal code incrementally across neural network layers and demonstrated the flexibility to include data priors such as top-down attention and other oracle information. Empirically, our approach outperforms existing normalization methods in tackling limited, imbalanced and non-stationary input distribution in image classification, classic control, procedurally-generated reinforcement learning, generative modeling, handwriting generation and question answering tasks with various neural network architectures. Lastly, UAM tracks dependency and critical learning stages across layers and recurrent time steps of deep networks.",0
"Our proposed regularity normalization (RN) technique is based on the phenomenon of neuronal firing adaptation and serves as an unsupervised attention mechanism (UAM). It utilizes the Minimum Description Length (MDL) principle to calculate the statistical regularity in the implicit space of neural networks. As a partially observable model selection problem, UAM imposes constraints on the implicit space through a normalization factor, the universal code length, which is computed incrementally across neural network layers. Our approach is flexible enough to incorporate data priors such as top-down attention and other oracle information. Empirically, our method has shown superior performance compared to existing normalization techniques in image classification, classic control, procedurally-generated reinforcement learning, generative modeling, handwriting generation, and question answering tasks using various neural network architectures. Additionally, UAM can track the dependency and critical learning stages across layers and recurrent time steps of deep networks.",1
"This paper studies the statistical theory of batch data reinforcement learning with function approximation. Consider the off-policy evaluation problem, which is to estimate the cumulative value of a new target policy from logged history generated by unknown behavioral policies. We study a regression-based fitted Q iteration method, and show that it is equivalent to a model-based method that estimates a conditional mean embedding of the transition operator. We prove that this method is information-theoretically optimal and has nearly minimal estimation error. In particular, by leveraging contraction property of Markov processes and martingale concentration, we establish a finite-sample instance-dependent error upper bound and a nearly-matching minimax lower bound. The policy evaluation error depends sharply on a restricted $\chi^2$-divergence over the function class between the long-term distribution of the target policy and the distribution of past data. This restricted $\chi^2$-divergence is both instance-dependent and function-class-dependent. It characterizes the statistical limit of off-policy evaluation. Further, we provide an easily computable confidence bound for the policy evaluator, which may be useful for optimistic planning and safe policy improvement.",0
"The focus of this paper is on the statistical principles behind batch data reinforcement learning with function approximation. Specifically, we address the challenge of estimating the cumulative value of a new target policy based on previously recorded data from unknown behavioral policies. Our approach involves using a regression-based fitted Q iteration method to estimate the conditional mean embedding of the transition operator. We demonstrate that this method is both information-theoretically optimal and has low estimation error. In fact, using the contraction property of Markov processes and martingale concentration, we establish both an upper bound and matching lower bound on the finite-sample instance-dependent error. The key factor in determining the policy evaluation error is the restricted $\chi^2$-divergence between the long-term distribution of the target policy and the distribution of past data. This divergence is both instance and function-class dependent, and is critical to understanding the statistical limits of off-policy evaluation. Finally, we provide a practical confidence bound for the policy evaluator, which can be useful for optimistic planning and safe policy improvement.",1
"In order to drive safely and efficiently under merging scenarios, autonomous vehicles should be aware of their surroundings and make decisions by interacting with other road participants. Moreover, different strategies should be made when the autonomous vehicle is interacting with drivers having different level of cooperativeness. Whether the vehicle is on the merge-lane or main-lane will also influence the driving maneuvers since drivers will behave differently when they have the right-of-way than otherwise. Many traditional methods have been proposed to solve decision making problems under merging scenarios. However, these works either are incapable of modeling complicated interactions or require implementing hand-designed rules which cannot properly handle the uncertainties in real-world scenarios. In this paper, we proposed an interaction-aware decision making with adaptive strategies (IDAS) approach that can let the autonomous vehicle negotiate the road with other drivers by leveraging their cooperativeness under merging scenarios. A single policy is learned under the multi-agent reinforcement learning (MARL) setting via the curriculum learning strategy, which enables the agent to automatically infer other drivers' various behaviors and make decisions strategically. A masking mechanism is also proposed to prevent the agent from exploring states that violate common sense of human judgment and increase the learning efficiency. An exemplar merging scenario was used to implement and examine the proposed method.",0
"To ensure safe and efficient driving in merging scenarios, autonomous vehicles must be aware of their surroundings and interact with other drivers to make decisions. The vehicle's approach may differ depending on the level of cooperation of the other drivers and whether it is on the merge-lane or main-lane. While traditional methods have been proposed, they are not capable of modeling complex interactions or handling uncertainties in real-world scenarios. Therefore, we introduce the Interaction-Aware Decision Making with Adaptive Strategies (IDAS) approach, which uses a single policy learned through Multi-Agent Reinforcement Learning (MARL) and curriculum learning to infer other drivers' behaviors and make strategic decisions. We also propose a masking mechanism to increase learning efficiency and prevent the agent from exploring states that violate human judgment. Finally, we implemented and tested the proposed method on an exemplar merging scenario.",1
"Reinforcement learning (RL) in discrete action space is ubiquitous in real-world applications, but its complexity grows exponentially with the action-space dimension, making it challenging to apply existing on-policy gradient based deep RL algorithms efficiently. To effectively operate in multidimensional discrete action spaces, we construct a critic to estimate action-value functions, apply it on correlated actions, and combine these critic estimated action values to control the variance of gradient estimation. We follow rigorous statistical analysis to design how to generate and combine these correlated actions, and how to sparsify the gradients by shutting down the contributions from certain dimensions. These efforts result in a new discrete action on-policy RL algorithm that empirically outperforms related on-policy algorithms relying on variance control techniques. We demonstrate these properties on OpenAI Gym benchmark tasks, and illustrate how discretizing the action space could benefit the exploration phase and hence facilitate convergence to a better local optimal solution thanks to the flexibility of discrete policy.",0
"Discrete action space reinforcement learning (RL) is widely used in practical applications, but its complexity increases exponentially with the action-space dimension, making it difficult to efficiently apply existing on-policy gradient-based deep RL algorithms. To address this challenge, we developed a new on-policy RL algorithm for multidimensional discrete action spaces. This algorithm constructs a critic to estimate action-value functions, applies it on correlated actions, and combines critic-estimated action values to control gradient estimation variance. We conducted a rigorous statistical analysis to design the generation and combination of these correlated actions and sparsify gradients by excluding certain dimensions. Our algorithm outperforms related on-policy algorithms relying on variance control techniques, as demonstrated by OpenAI Gym benchmark tasks. Discretizing the action space can benefit the exploration phase and facilitate convergence to a better local optimal solution, thanks to the flexibility of discrete policy.",1
"Parameter-transfer is a well-known and versatile approach for meta-learning, with applications including few-shot learning, federated learning, and reinforcement learning. However, parameter-transfer algorithms often require sharing models that have been trained on the samples from specific tasks, thus leaving the task-owners susceptible to breaches of privacy. We conduct the first formal study of privacy in this setting and formalize the notion of task-global differential privacy as a practical relaxation of more commonly studied threat models. We then propose a new differentially private algorithm for gradient-based parameter transfer that not only satisfies this privacy requirement but also retains provable transfer learning guarantees in convex settings. Empirically, we apply our analysis to the problems of federated learning with personalization and few-shot classification, showing that allowing the relaxation to task-global privacy from the more commonly studied notion of local privacy leads to dramatically increased performance in recurrent neural language modeling and image classification.",0
"Parameter-transfer is a widely applicable method for meta-learning that can be used for tasks such as reinforcement learning, few-shot learning, and federated learning. However, these algorithms can pose a risk to privacy by requiring the sharing of models trained on specific tasks. We have conducted the first formal examination of privacy concerns in this context and introduced the concept of task-global differential privacy as a practical alternative to commonly studied threats. Our new differentially private algorithm for gradient-based parameter transfer provides both privacy protection and provable transfer learning guarantees for convex settings. Through experimentation with recurrent neural language modeling and image classification, we demonstrate that relaxing the privacy requirement to task-global privacy significantly improves performance compared to the more commonly studied local privacy.",1
"In many vision-based reinforcement learning (RL) problems, the agent controls a movable object in its visual field, e.g., the player's avatar in video games and the robotic arm in visual grasping and manipulation. Leveraging action-conditioned video prediction, we propose an end-to-end learning framework to disentangle the controllable object from the observation signal. The disentangled representation is shown to be useful for RL as additional observation channels to the agent. Experiments on a set of Atari games with the popular Double DQN algorithm demonstrate improved sample efficiency and game performance (from 222.8% to 261.4% measured in normalized game scores, with prediction bonus reward).",0
"The agent in vision-based reinforcement learning problems often manages a movable object within its visual field, such as a video game character or robotic arm. Our proposed approach utilizes action-conditioned video prediction to create an end-to-end learning framework that separates the controllable object from the observation signal. This new representation is beneficial for RL as it provides additional observation channels to the agent. Through experiments with the Double DQN algorithm on Atari games, our framework demonstrated increased sample efficiency and game performance, measured by normalized game scores, with prediction bonus reward improving from 222.8% to 261.4%.",1
"Evolution strategy (ES) has been shown great promise in many challenging reinforcement learning (RL) tasks, rivaling other state-of-the-art deep RL methods. Yet, there are two limitations in the current ES practice that may hinder its otherwise further capabilities. First, most current methods rely on Monte Carlo type gradient estimators to suggest search direction, where the policy parameter is, in general, randomly sampled. Due to the low accuracy of such estimators, the RL training may suffer from slow convergence and require more iterations to reach optimal solution. Secondly, the landscape of reward functions can be deceptive and contains many local maxima, causing ES algorithms to prematurely converge and be unable to explore other parts of the parameter space with potentially greater rewards. In this work, we employ a Directional Gaussian Smoothing Evolutionary Strategy (DGS-ES) to accelerate RL training, which is well-suited to address these two challenges with its ability to i) provide gradient estimates with high accuracy, and ii) find nonlocal search direction which lays stress on large-scale variation of the reward function and disregards local fluctuation. Through several benchmark RL tasks demonstrated herein, we show that DGS-ES is highly scalable, possesses superior wall-clock time, and achieves competitive reward scores to other popular policy gradient and ES approaches.",0
"The Evolution Strategy (ES) has proven to be a highly effective method for reinforcement learning (RL) tasks, even surpassing other advanced deep RL techniques. However, there are two limitations that currently prevent ES from reaching its full potential. Firstly, most ES methods use Monte Carlo gradient estimators to suggest search direction, which leads to slow convergence and more iterations needed to reach an optimal solution. Secondly, the landscape of reward functions can be misleading and contain many local maxima, causing ES algorithms to converge prematurely and not explore other parts of the parameter space that may offer greater rewards. To overcome these challenges, we introduce a Directional Gaussian Smoothing Evolutionary Strategy (DGS-ES) that can provide highly accurate gradient estimates and find nonlocal search directions that focus on large-scale reward function variations and disregard local fluctuations. Our DGS-ES approach is highly scalable, has superior wall-clock time, and achieves competitive reward scores compared to other popular policy gradient and ES approaches, as demonstrated through several benchmark RL tasks.",1
"An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this challenging scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove its consistency under general conditions, provide an error analysis, and demonstrate strong empirical performance on benchmark problems, including off-line PageRank and off-policy policy evaluation.",0
"When using reinforcement learning and Monte Carlo methods, it can be difficult to estimate quantities based on the stationary distribution of a Markov chain. This is especially true in real-world scenarios where access to the transition operator is limited to pre-collected data, with no further interaction with the environment. However, our study shows that it is still possible to consistently estimate these quantities in such challenging situations. Our method involves estimating a ratio that accounts for the difference between the stationary and empirical distributions, using fundamental properties of the stationary distribution. We also use constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is both effective and straightforward. We have proven its consistency under general conditions, analyzed its error rate, and demonstrated its strong performance on benchmark problems, including off-line PageRank and off-policy policy evaluation.",1
"Explicit engineering of reward functions for given environments has been a major hindrance to reinforcement learning methods. While Inverse Reinforcement Learning (IRL) is a solution to recover reward functions from demonstrations only, these learned rewards are generally heavily \textit{entangled} with the dynamics of the environment and therefore not portable or \emph{robust} to changing environments. Modern adversarial methods have yielded some success in reducing reward entanglement in the IRL setting. In this work, we leverage one such method, Adversarial Inverse Reinforcement Learning (AIRL), to propose an algorithm that learns hierarchical disentangled rewards with a policy over options. We show that this method has the ability to learn \emph{generalizable} policies and reward functions in complex transfer learning tasks, while yielding results in continuous control benchmarks that are comparable to those of the state-of-the-art methods.",0
"Reinforcement learning methods have struggled with the challenge of explicitly engineering reward functions for specific environments. Inverse Reinforcement Learning (IRL) helps by recovering reward functions from demonstrations, but the learned rewards are usually closely linked with the environment's dynamics and not adaptable to different environments. Adversarial methods have made progress in reducing reward entanglement in IRL. This study utilizes Adversarial Inverse Reinforcement Learning (AIRL) to develop an algorithm that can learn hierarchical disentangled rewards with a policy over options. The results demonstrate that this method can learn generalizable policies and reward functions in complex transfer learning tasks and achieve outcomes in continuous control benchmarks comparable to those of the state-of-the-art methods.",1
"Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.",0
"Saliency maps are commonly employed to explain the behavior of deep reinforcement learning (RL) agents, but their practical usage reveals that the explanations derived from them are frequently subjective and difficult to prove or disprove. To address this issue, we propose an empirical method based on counterfactual reasoning to test the hypotheses generated from saliency maps and evaluate their alignment with the semantics of RL environments. We evaluate three types of saliency maps using Atari games, a popular benchmark for deep RL. Our findings demonstrate the degree to which existing claims about Atari games can be examined and suggest that saliency maps are better suited as a tool for exploration rather than explanation.",1
"Recent deep neural networks based techniques, especially those equipped with the ability of self-adaptation in the system level such as deep reinforcement learning (DRL), are shown to possess many advantages of optimizing robot learning systems (e.g., autonomous navigation and continuous robot arm control.) However, the learning-based systems and the associated models may be threatened by the risks of intentionally adaptive (e.g., noisy sensor confusion) and adversarial perturbations from real-world scenarios. In this paper, we introduce timing-based adversarial strategies against a DRL-based navigation system by jamming in physical noise patterns on the selected time frames. To study the vulnerability of learning-based navigation systems, we propose two adversarial agent models: one refers to online learning; another one is based on evolutionary learning. Besides, three open-source robot learning and navigation control environments are employed to study the vulnerability under adversarial timing attacks. Our experimental results show that the adversarial timing attacks can lead to a significant performance drop, and also suggest the necessity of enhancing the robustness of robot learning systems.",0
"Advanced techniques using deep neural networks, particularly those incorporating self-adaptation at the system level like deep reinforcement learning (DRL), have demonstrated numerous advantages in optimizing robot learning systems such as autonomous navigation and continuous robot arm control. However, these learning-based systems and their models are susceptible to intentional adaptation, such as noisy sensor confusion, and adversarial perturbations from real-world scenarios. This study introduces timing-based adversarial strategies targeting a DRL-based navigation system by introducing physical noise patterns on selected time frames. To assess the vulnerability of learning-based navigation systems, two adversarial agent models are proposed: one for online learning and the other based on evolutionary learning. Three open-source robot learning and navigation control environments are utilized to examine vulnerability under adversarial timing attacks. Results indicate that these attacks can cause a significant decline in performance, emphasizing the need to enhance the robustness of robot learning systems.",1
"We formulate a general framework for competitive gradient-based learning that encompasses a wide breadth of multi-agent learning algorithms, and analyze the limiting behavior of competitive gradient-based learning algorithms using dynamical systems theory. For both general-sum and potential games, we characterize a non-negligible subset of the local Nash equilibria that will be avoided if each agent employs a gradient-based learning algorithm. We also shed light on the issue of convergence to non-Nash strategies in general- and zero-sum games, which may have no relevance to the underlying game, and arise solely due to the choice of algorithm. The existence and frequency of such strategies may explain some of the difficulties encountered when using gradient descent in zero-sum games as, e.g., in the training of generative adversarial networks. To reinforce the theoretical contributions, we provide empirical results that highlight the frequency of linear quadratic dynamic games (a benchmark for multi-agent reinforcement learning) that admit global Nash equilibria that are almost surely avoided by policy gradient.",0
"We have developed a comprehensive framework for competitive gradient-based learning, which covers a wide range of multi-agent learning algorithms. We have investigated the behavior of competitive gradient-based learning algorithms using dynamical systems theory, for both general-sum and potential games. Our findings reveal that there is a significant subset of the local Nash equilibria that will not be reached if each agent uses a gradient-based learning algorithm. Additionally, we have identified the issue of convergence to non-Nash strategies in general- and zero-sum games, which is unrelated to the underlying game and is solely due to the choice of algorithm. The frequency of such strategies may help explain some of the challenges encountered in using gradient descent in zero-sum games, such as in the training of generative adversarial networks. Our theoretical contributions are supported by empirical results that demonstrate how frequently linear quadratic dynamic games, a benchmark for multi-agent reinforcement learning, have global Nash equilibria that are almost always avoided by policy gradient.",1
"Policy gradient methods in reinforcement learning update policy parameters by taking steps in the direction of an estimated gradient of policy value. In this paper, we consider the statistically efficient estimation of policy gradients from off-policy data, where the estimation is particularly non-trivial. We derive the asymptotic lower bound on the feasible mean-squared error in both Markov and non-Markov decision processes and show that existing estimators fail to achieve it in general settings. We propose a meta-algorithm that achieves the lower bound without any parametric assumptions and exhibits a unique 3-way double robustness property. We discuss how to estimate nuisances that the algorithm relies on. Finally, we establish guarantees on the rate at which we approach a stationary point when we take steps in the direction of our new estimated policy gradient.",0
"The update of policy parameters in reinforcement learning is accomplished by policy gradient methods, which involve taking steps in the direction of an estimated gradient of policy value. This paper focuses on the estimation of policy gradients from off-policy data, which is a particularly challenging task. We examine the statistically efficient estimation of policy gradients in both Markov and non-Markov decision processes, and establish an asymptotic lower bound on the feasible mean-squared error. We find that existing estimators generally fail to achieve this bound, and thus propose a meta-algorithm that surpasses it without relying on any parametric assumptions. This meta-algorithm also possesses a unique 3-way double robustness property, and we discuss how to estimate the nuisances that the algorithm relies on. Lastly, we establish the rate at which we approach a stationary point when we take steps in the direction of our newly estimated policy gradient.",1
"We propose a Reinforcement Learning based approach to approximately solve the Tree Decomposition (TD) problem. TD is a combinatorial problem, which is central to the analysis of graph minor structure and computational complexity, as well as in the algorithms of probabilistic inference, register allocation, and other practical tasks. Recently, it has been shown that combinatorial problems can be successively solved by learned heuristics. However, the majority of existing works do not address the question of the generalization of learning-based solutions. Our model is based on the graph convolution neural network (GCN) for learning graph representations. We show that the agent builton GCN and trained on a single graph using an Actor-Critic method can efficiently generalize to real-world TD problem instances. We establish that our method successfully generalizes from small graphs, where TD can be found by exact algorithms, to large instances of practical interest, while still having very low time-to-solution. On the other hand, the agent-based approach surpasses all greedy heuristics by the quality of the solution.",0
"Our proposed solution to the Tree Decomposition (TD) problem involves using Reinforcement Learning. TD is a vital combinatorial problem that plays a significant role in graph minor structure analysis, computational complexity, probabilistic inference algorithms, register allocation, and other practical tasks. While some studies have demonstrated that learned heuristics can solve combinatorial problems successfully, most do not address the generalization of these solutions. Our model, which relies on the graph convolution neural network (GCN) for learning graph representations, is different. We demonstrate that an agent built on GCN and trained on a single graph using an Actor-Critic method can generalize efficiently to real-world TD problems. Notably, our approach can solve large instances of practical interest while maintaining a low time-to-solution and outperforming all greedy heuristics in terms of solution quality.",1
"Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.",0
"Learning effective policies for complex tasks, such as Atari games, from image observations using model-free reinforcement learning (RL) usually requires a significant amount of interaction, more than what a human would need to learn the same games. Humans can learn quickly by understanding how the game works and predicting which actions will lead to desirable outcomes. This paper explores how video prediction models can help agents solve Atari games with fewer interactions than model-free methods. Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models, is described, and a comparison of several model architectures is presented. The experiments evaluate SimPLe on a range of Atari games with 100k interactions, equivalent to two hours of real-time play. In most games, SimPLe performs better than state-of-the-art model-free algorithms, in some games by over an order of magnitude.",1
"Text-based games -- in which an agent interacts with the world through textual natural language -- present us with the problem of combinatorially-sized action-spaces. Most current reinforcement learning algorithms are not capable of effectively handling such a large number of possible actions per turn. Poor sample efficiency, consequently, results in agents that are unable to pass bottleneck states, where they are unable to proceed because they do not see the right action sequence to pass the bottleneck enough times to be sufficiently reinforced. Building on prior work using knowledge graphs in reinforcement learning, we introduce two new game state exploration strategies. We compare our exploration strategies against strong baselines on the classic text-adventure game, Zork1, where prior agent have been unable to get past a bottleneck where the agent is eaten by a Grue.",0
"The issue with text-based games is the vast array of possible actions an agent can take, which creates a combinatorial issue. This poses a challenge for reinforcement learning algorithms as they struggle to handle the multitude of actions per turn, leading to poor sample efficiency. Consequently, agents can get stuck at bottleneck states where they cannot progress due to insufficient reinforcement. To address this problem, we build upon previous research that utilized knowledge graphs in reinforcement learning and propose two new game state exploration strategies. We evaluate our strategies against strong baselines on Zork1, a classic text-adventure game notorious for a bottleneck where agents are eaten by a Grue.",1
"Experience replay is widely used in deep reinforcement learning algorithms and allows agents to remember and learn from experiences from the past. In an effort to learn more efficiently, researchers proposed prioritized experience replay (PER) which samples important transitions more frequently. In this paper, we propose Prioritized Sequence Experience Replay (PSER) a framework for prioritizing sequences of experience in an attempt to both learn more efficiently and to obtain better performance. We compare the performance of PER and PSER sampling techniques in a tabular Q-learning environment and in DQN on the Atari 2600 benchmark. We prove theoretically that PSER is guaranteed to converge faster than PER and empirically show PSER substantially improves upon PER.",0
"The utilization of experience replay is prevalent in deep reinforcement learning algorithms to enable agents to recollect and learn from past experiences. To enhance the learning process, researchers have introduced prioritized experience replay (PER), which favors significant transitions for more frequent sampling. This paper introduces Prioritized Sequence Experience Replay (PSER), which prioritizes sequences of experience to improve learning efficiency and performance. We evaluate the effectiveness of PER and PSER sampling techniques in tabular Q-learning environments and on the Atari 2600 benchmark. Our theoretical analysis confirms that PSER guarantees faster convergence than PER, and our empirical results demonstrate that PSER outperforms PER significantly.",1
"The construction by Du et al. (2019) implies that even if a learner is given linear features in $\mathbb R^d$ that approximate the rewards in a bandit with a uniform error of $\epsilon$, then searching for an action that is optimal up to $O(\epsilon)$ requires examining essentially all actions. We use the Kiefer-Wolfowitz theorem to prove a positive result that by checking only a few actions, a learner can always find an action that is suboptimal with an error of at most $O(\epsilon \sqrt{d})$. Thus, features are useful when the approximation error is small relative to the dimensionality of the features. The idea is applied to stochastic bandits and reinforcement learning with a generative model where the learner has access to $d$-dimensional linear features that approximate the action-value functions for all policies to an accuracy of $\epsilon$. For linear bandits, we prove a bound on the regret of order $\sqrt{dn \log(k)} + \epsilon n \sqrt{d} \log(n)$ with $k$ the number of actions and $n$ the horizon. For RL we show that approximate policy iteration can learn a policy that is optimal up to an additive error of order $\epsilon \sqrt{d}/(1 - \gamma)^2$ and using $d/(\epsilon^2(1 - \gamma)^4)$ samples from a generative model. These bounds are independent of the finer details of the features. We also investigate how the structure of the feature set impacts the tradeoff between sample complexity and estimation error.",0
"Du et al. (2019) discovered that even if a learner is given linear features in $\mathbb R^d$ that approximate the rewards in a bandit with an error of $\epsilon$ that is uniform, finding an action that is optimal up to $O(\epsilon)$ necessitates examining almost all possible actions. We employ the Kiefer-Wolfowitz theorem to prove that by inspecting only a few actions, a learner can always discover an action that is suboptimal with an error of at most $O(\epsilon \sqrt{d})$. This indicates that features are advantageous when the approximation error is small compared to the feature dimensionality. This idea is applied to stochastic bandits and reinforcement learning with a generative model, where the learner has access to $d$-dimensional linear features that approximate the action-value functions for all policies to an accuracy of $\epsilon$. We establish a bound on the regret of order $\sqrt{dn \log(k)} + \epsilon n \sqrt{d} \log(n)$ for linear bandits, with $k$ denoting the number of actions and $n$ representing the horizon. For RL, we demonstrate that approximate policy iteration can learn a policy that is optimal up to an additive error of order $\epsilon \sqrt{d}/(1 - \gamma)^2$, using $d/(\epsilon^2(1 - \gamma)^4)$ samples from a generative model. These bounds are independent of the details of the features. We also explore how the structure of the feature set affects the tradeoff between sample complexity and estimation error.",1
"We establish that an optimistic variant of Q-learning applied to a fixed-horizon episodic Markov decision process with an aggregated state representation incurs regret $\tilde{\mathcal{O}}(\sqrt{H^5 M K} + \epsilon HK)$, where $H$ is the horizon, $M$ is the number of aggregate states, $K$ is the number of episodes, and $\epsilon$ is the largest difference between any pair of optimal state-action values associated with a common aggregate state. Notably, this regret bound does not depend on the number of states or actions and indicates that asymptotic per-period regret is no greater than $\epsilon$, independent of horizon. To our knowledge, this is the first such result that applies to reinforcement learning with nontrivial value function approximation without any restrictions on transition probabilities.",0
"It has been determined that utilizing an optimistic variation of Q-learning on a Markov decision process with an aggregated state representation that has a fixed-horizon and is episodic, results in regret of approximately $\tilde{\mathcal{O}}(\sqrt{H^5 M K} + \epsilon HK)$. The horizon is denoted by $H$, the number of aggregate states by $M$, the number of episodes by $K$, and $\epsilon$ represents the largest disparity between optimal state-action values that are associated with a shared aggregate state. It is significant to note that this regret boundary is not dependent on the quantity of states or actions, and it implies that the per-period regret in the asymptotic is not greater than $\epsilon, regardless of the horizon. To our knowledge, this is the first such outcome that pertains to reinforcement learning using nontrivial value function approximation, without any limitations on transition probabilities.",1
"Data-driven methods have made great progress in fault diagnosis, especially deep learning method. Deep learning is suitable for processing big data, and has a strong feature extraction ability to realize end-to-end fault diagnosis systems. However, designing neural network architecture requires rich professional knowledge and debugging experience, and a lot of experiments are needed to screen models and hyperparameters, increasing the difficulty of developing deep learning models. Frortunately, neural architecture search (NAS) is developing rapidly, and is becoming one of the next directions for deep learning. In this paper, we proposed a NAS method for fault diagnosis using reinforcement learning. A recurrent neural network is used as an agent to generate network architecture. The accuracy of the generated network on the validation dataset is fed back to the agent as a reward, and the parameters of the agent are updated through the strategy gradient algorithm. We use PHM 2009 Data Challenge gearbox dataset to prove the effectiveness of proposed method, and obtain state-of-the-art results compared with other artificial designed network structures. To author's best knowledge, it's the first time that NAS has been applied in fault diagnosis.",0
"Significant advancements in fault diagnosis have been achieved through data-driven approaches, particularly utilizing deep learning methods. The strength of deep learning lies in its ability to handle large data sets and perform feature extraction, enabling the development of end-to-end fault diagnosis systems. However, creating neural network architecture requires extensive professional knowledge and debugging experience, and the process of screening models and hyperparameters through experimentation adds to the complexity of developing deep learning models. Fortunately, neural architecture search (NAS) is a rapidly developing field and is seen as the next step in deep learning. In this study, a NAS method for fault diagnosis using reinforcement learning is proposed. A recurrent neural network serves as an agent to generate network architecture, with the accuracy of the generated network on the validation dataset serving as a reward. The agent's parameters are updated through the strategy gradient algorithm. The effectiveness of the proposed method was demonstrated using the PHM 2009 Data Challenge gearbox dataset, with the resulting state-of-the-art results compared to other artificially designed network structures. This study marks the first known application of NAS in fault diagnosis.",1
"Gradient-based meta-learners such as Model-Agnostic Meta-Learning (MAML) have shown strong few-shot performance in supervised and reinforcement learning settings. However, specifically in the case of meta-reinforcement learning (meta-RL), we can show that gradient-based meta-learners are sensitive to task distributions. With the wrong curriculum, agents suffer the effects of meta-overfitting, shallow adaptation, and adaptation instability. In this work, we begin by highlighting intriguing failure cases of gradient-based meta-RL and show that task distributions can wildly affect algorithmic outputs, stability, and performance. To address this problem, we leverage insights from recent literature on domain randomization and propose meta Active Domain Randomization (meta-ADR), which learns a curriculum of tasks for gradient-based meta-RL in a similar as ADR does for sim2real transfer. We show that this approach induces more stable policies on a variety of simulated locomotion and navigation tasks. We assess in- and out-of-distribution generalization and find that the learned task distributions, even in an unstructured task space, greatly improve the adaptation performance of MAML. Finally, we motivate the need for better benchmarking in meta-RL that prioritizes \textit{generalization} over single-task adaption performance.",0
"Gradient-based meta-learning techniques, including Model-Agnostic Meta-Learning (MAML), have achieved impressive few-shot performance in supervised and reinforcement learning contexts. However, when it comes to meta-reinforcement learning (meta-RL), these methods can be vulnerable to variations in task distributions, leading to issues such as meta-overfitting, shallow adaptation, and adaptation instability. This paper highlights cases where gradient-based meta-RL fails and demonstrates how task distributions can have a significant impact on algorithmic outputs, stability, and performance. To address this issue, the authors propose meta Active Domain Randomization (meta-ADR), which uses insights from domain randomization literature to develop a task curriculum for gradient-based meta-RL. They show that this approach leads to more stable policies in a range of simulated locomotion and navigation tasks, and greatly improves the adaptation performance of MAML, even in an unstructured task space. The authors also argue for more comprehensive benchmarking in meta-RL that prioritizes generalization over single-task adaptation performance.",1
"Memory is an important aspect of intelligence and plays a role in many deep reinforcement learning models. However, little progress has been made in understanding when specific memory systems help more than others and how well they generalize. The field also has yet to see a prevalent consistent and rigorous approach for evaluating agent performance on holdout data. In this paper, we aim to develop a comprehensive methodology to test different kinds of memory in an agent and assess how well the agent can apply what it learns in training to a holdout set that differs from the training set along dimensions that we suggest are relevant for evaluating memory-specific generalization. To that end, we first construct a diverse set of memory tasks that allow us to evaluate test-time generalization across multiple dimensions. Second, we develop and perform multiple ablations on an agent architecture that combines multiple memory systems, observe its baseline models, and investigate its performance against the task suite.",0
"The role of memory in intelligence is crucial and has been incorporated into many deep reinforcement learning models. However, there is a lack of progress in comprehending which memory systems are more effective and how well they can generalize. Furthermore, there is no consistent method for evaluating agent performance on holdout data. This paper proposes a comprehensive methodology to assess different memory types in agents and their ability to apply what they learn to a holdout set with relevant dimensions for memory-specific generalization. To achieve this, a set of diverse memory tasks is constructed to evaluate test-time generalization across multiple dimensions. Additionally, multiple ablations are performed on an agent architecture that combines several memory systems, and its performance against the task suite is investigated.",1
"We devise and analyze algorithms for the empirical policy evaluation problem in reinforcement learning. Our algorithms explore backward from high-cost states to find high-value ones, in contrast to forward approaches that work forward from all states. While several papers have demonstrated the utility of backward exploration empirically, we conduct rigorous analyses which show that our algorithms can reduce average-case sample complexity from $O(S \log S)$ to as low as $O(\log S)$.",0
"Our focus is on solving the empirical policy evaluation problem in reinforcement learning through algorithm development and analysis. Our approach involves exploring high-value states by working backwards from high-cost states, which differs from forward approaches that explore from all states. Previous studies have demonstrated the effectiveness of backward exploration, but we provide rigorous analysis that shows our algorithms can significantly reduce the average-case sample complexity from $O(S \log S)$ to $O(\log S)$.",1
