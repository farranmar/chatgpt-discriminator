"In this paper we propose a unified framework for structured prediction with latent variables which includes hidden conditional random fields and latent structured support vector machines as special cases. We describe a local entropy approximation for this general formulation using duality, and derive an efficient message passing algorithm that is guaranteed to converge. We demonstrate its effectiveness in the tasks of image segmentation as well as 3D indoor scene understanding from single images, showing that our approach is superior to latent structured support vector machines and hidden conditional random fields.",0
"A unified framework for structured prediction with latent variables is put forward in this study, encompassing hidden conditional random fields and latent structured support vector machines as specific instances. By utilizing duality, we expound on a local entropy approximation for this comprehensive formulation, and establish a proficient message passing algorithm that ensures convergence. Our methodology proves to be more effective than latent structured support vector machines and hidden conditional random fields in image segmentation and 3D indoor scene comprehension from single images.",1
"Fuzzy systems concern fundamental methodology to represent and process uncertainty and imprecision in the linguistic information. The fuzzy systems that use fuzzy rules to represent the domain knowledge of the problem are known as Fuzzy Rule Base Systems (FRBS). On the other hand image segmentation and subsequent extraction from a noise-affected background, with the help of various soft computing methods, are relatively new and quite popular due to various reasons. These methods include various Artificial Neural Network (ANN) models (primarily supervised in nature), Genetic Algorithm (GA) based techniques, intensity histogram based methods etc. providing an extraction solution working in unsupervised mode happens to be even more interesting problem. Literature suggests that effort in this respect appears to be quite rudimentary. In the present article, we propose a fuzzy rule guided novel technique that is functional devoid of any external intervention during execution. Experimental results suggest that this approach is an efficient one in comparison to different other techniques extensively addressed in literature. In order to justify the supremacy of performance of our proposed technique in respect of its competitors, we take recourse to effective metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), Peak Signal to Noise Ratio (PSNR).",0
"The concept of fuzzy systems involves the fundamental methodology of processing and representing uncertainty and imprecision in linguistic information. Fuzzy Rule Base Systems (FRBS) refer to fuzzy systems that utilize fuzzy rules to represent domain knowledge in problem-solving. In contrast, the use of soft computing techniques such as Artificial Neural Network (ANN) models (primarily supervised), Genetic Algorithm (GA) based techniques, and intensity histogram-based methods to extract images from a noise-affected background is a relatively new and popular approach. However, providing an extraction solution working in an unsupervised mode remains an interesting problem, with literature suggesting that efforts in this area are still at a rudimentary stage. In this article, we propose a novel technique that utilizes fuzzy rules to guide image segmentation without requiring external intervention during execution. Our experimental results demonstrate that this approach is more efficient than other extensively addressed techniques in the literature. To further demonstrate the superiority of our proposed technique, we use metrics such as Mean Squared Error (MSE), Mean Absolute Error (MAE), and Peak Signal to Noise Ratio (PSNR).",1
"We describe a nonparametric topic model for labeled data. The model uses a mixture of random measures (MRM) as a base distribution of the Dirichlet process (DP) of the HDP framework, so we call it the DP-MRM. To model labeled data, we define a DP distributed random measure for each label, and the resulting model generates an unbounded number of topics for each label. We apply DP-MRM on single-labeled and multi-labeled corpora of documents and compare the performance on label prediction with MedLDA, LDA-SVM, and Labeled-LDA. We further enhance the model by incorporating ddCRP and modeling multi-labeled images for image segmentation and object labeling, comparing the performance with nCuts and rddCRP.",0
"Our nonparametric topic model for labeled data utilizes a mixture of random measures (MRM) as the base distribution of the Dirichlet process (DP) within the HDP framework, known as the DP-MRM. To account for labeled data, we create a DP distributed random measure for each label, resulting in an unlimited number of topics for each label. We test the DP-MRM on both single-labeled and multi-labeled document corpora, comparing its label prediction performance to MedLDA, LDA-SVM, and Labeled-LDA. Additionally, we improve upon the model by incorporating ddCRP and applying it to multi-labeled images for object labeling and image segmentation, comparing its performance to nCuts and rddCRP.",1
"In the recent advancement of multimedia technologies, it becomes a major concern of detecting visual attention regions in the field of image processing. The popularity of the terminal devices in a heterogeneous environment of the multimedia technology gives us enough scope for the betterment of image visualization. Although there exist numerous methods, feature based image extraction becomes a popular one in the field of image processing. The objective of image segmentation is the domain-independent partition of the image into a set of regions, which are visually distinct and uniform with respect to some property, such as grey level, texture or colour. Segmentation and subsequent extraction can be considered the first step and key issue in object recognition, scene understanding and image analysis. Its application area encompasses mobile devices, industrial quality control, medical appliances, robot navigation, geophysical exploration, military applications, etc. In all these areas, the quality of the final results depends largely on the quality of the preprocessing work. Most of the times, acquiring spurious-free preprocessing data requires a lot of application cum mathematical intensive background works. We propose a feature based fuzzy rule guided novel technique that is functionally devoid of any external intervention during execution. Experimental results suggest that this approach is an efficient one in comparison to different other techniques extensively addressed in literature. In order to justify the supremacy of performance of our proposed technique in respect of its competitors, we take recourse to effective metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE) and Peak Signal to Noise Ratio (PSNR).",0
"The detection of visual attention regions in image processing has become a major concern due to advancements in multimedia technologies. The use of terminal devices in a heterogeneous environment provides ample opportunity for improved image visualization. Feature-based image extraction has become a popular method for image processing, although there are numerous other approaches. Image segmentation aims to divide an image into visually distinct and uniform regions based on properties such as grey level, texture, or color. Segmentation and extraction are essential for object recognition, scene understanding, and image analysis. These techniques have applications in various fields, including mobile devices, medical appliances, industrial quality control, military applications, and geophysical exploration. High-quality preprocessing is crucial for obtaining accurate results, and this often requires extensive mathematical work. We propose a feature-based fuzzy rule-guided technique that does not require external intervention during execution. Experimental results show that our approach outperforms other extensively addressed techniques in the literature, as measured by metrics such as Mean Squared Error, Mean Absolute Error, and Peak Signal to Noise Ratio.",1
"Hierarchical image segmentation provides region-oriented scalespace, i.e., a set of image segmentations at different detail levels in which the segmentations at finer levels are nested with respect to those at coarser levels. Most image segmentation algorithms, such as region merging algorithms, rely on a criterion for merging that does not lead to a hierarchy, and for which the tuning of the parameters can be difficult. In this work, we propose a hierarchical graph based image segmentation relying on a criterion popularized by Felzenzwalb and Huttenlocher. We illustrate with both real and synthetic images, showing efficiency, ease of use, and robustness of our method.",0
"The method of hierarchical image segmentation creates a region-focused scalespace that consists of various image segmentations with different levels of detail. The finer segmentations are nested within the coarser ones. However, most image segmentation algorithms, such as those using region merging techniques, do not produce a hierarchy, making it challenging to fine-tune the parameters. To address this, we introduce a hierarchical graph-based image segmentation approach that employs Felzenzwalb and Huttenlocher's popularized criterion. Our method is demonstrated using both real and synthetic images, highlighting its effectiveness, user-friendliness, and robustness.",1
"This paper proposes a Genetic Algorithm based segmentation method that can automatically segment gray-scale images. The proposed method mainly consists of spatial unsupervised grayscale image segmentation that divides an image into regions. The aim of this algorithm is to produce precise segmentation of images using intensity information along with neighborhood relationships. In this paper, Fuzzy Hopfield Neural Network (FHNN) clustering helps in generating the population of Genetic algorithm which there by automatically segments the image. This technique is a powerful method for image segmentation and works for both single and multiple-feature data with spatial information. Validity index has been utilized for introducing a robust technique for finding the optimum number of components in an image. Experimental results shown that the algorithm generates good quality segmented image.",0
"A Genetic Algorithm-based method for automatically segmenting gray-scale images is proposed in this paper. The method primarily involves spatial unsupervised grayscale image segmentation that divides an image into regions. The objective of this algorithm is to achieve accurate segmentation of images by utilizing intensity information and neighborhood relationships. The Fuzzy Hopfield Neural Network (FHNN) clustering is incorporated in this paper to generate the population of Genetic algorithm, which automatically segments the image. This technique is a potent approach for image segmentation and is effective for both single and multiple-feature data with spatial information. To determine the optimal number of components in an image, a validity index is utilized, which introduces a robust technique. The experimental results demonstrate that the algorithm produces high-quality segmented images.",1
"Sampling from distributions of implicitly defined shapes enables analysis of various energy functionals used for image segmentation. Recent work describes a computationally efficient Metropolis-Hastings method for accomplishing this task. Here, we extend that framework so that samples are accepted at every iteration of the sampler, achieving an order of magnitude speed up in convergence. Additionally, we show how to incorporate topological constraints.",0
"The analysis of energy functionals utilized in image segmentation can be conducted by sampling from distributions with implicitly defined shapes. A Metropolis-Hastings method that is computationally efficient has been described in recent studies for this purpose. Our enhancement of this framework allows for the acceptance of samples at each iteration of the sampler, resulting in a tenfold acceleration in convergence. Furthermore, we demonstrate how to include topological constraints.",1
"This paper introduces a Bayesian image segmentation algorithm based on finite mixtures. An EM algorithm is developed to estimate parameters of the Gaussian mixtures. The finite mixture is a flexible and powerful probabilistic modeling tool. It can be used to provide a model-based clustering in the field of pattern recognition. However, the application of finite mixtures to image segmentation presents some difficulties; especially it's sensible to noise. In this paper we propose a variant of this method which aims to resolve this problem. Our approach proceeds by the characterization of pixels by two features: the first one describes the intrinsic properties of the pixel and the second characterizes the neighborhood of pixel. Then the classification is made on the base on adaptive distance which privileges the one or the other features according to the spatial position of the pixel in the image. The obtained results have shown a significant improvement of our approach compared to the standard version of EM algorithm.",0
"In this paper, a Bayesian image segmentation algorithm that is based on finite mixtures is introduced. The algorithm employs an EM algorithm to estimate parameters of the Gaussian mixtures. Finite mixture is a powerful and flexible probabilistic modeling tool that can be utilized for model-based clustering in pattern recognition. However, applying finite mixtures to image segmentation can be challenging, particularly in the presence of noise. To address this issue, this paper proposes a modified approach that characterizes pixels using two features: the intrinsic properties of the pixel and its neighborhood. Classification is then performed based on an adaptive distance approach, which favors one or the other feature depending on the pixel's spatial position in the image. The results indicate a significant improvement compared to the standard EM algorithm.",1
"Color image segmentation is an important topic in the image processing field. MRF-MAP is often adopted in the unsupervised segmentation methods, but their performance are far behind recent interactive segmentation tools supervised by user inputs. Furthermore, the existing related unsupervised methods also suffer from the low efficiency, and high risk of being trapped in the local optima, because MRF-MAP is currently solved by iterative frameworks with inaccurate initial color distribution models. To address these problems, the letter designs an efficient method to calculate the energy functions approximately in the non-iteration style, and proposes a new binary segmentation algorithm based on the slightly tuned Lanczos eigensolver. The experiments demonstrate that the new algorithm achieves competitive performance compared with two state-of-art segmentation methods.",0
"The image processing field considers color image segmentation an important topic. Unsupervised segmentation methods often utilize MRF-MAP, but their performance lags behind recent interactive segmentation tools that require user inputs. Additionally, existing unsupervised methods suffer from low efficiency and high risk of being trapped in local optima due to the inaccurate initial color distribution models used in iterative frameworks to solve MRF-MAP. To tackle these issues, a more efficient non-iterative approach to calculate energy functions is proposed, along with a binary segmentation algorithm based on the slightly tuned Lanczos eigensolver. Experimental results demonstrate that the new algorithm achieves competitive performance compared to two state-of-the-art segmentation methods.",1
"Context-dependence in human cognition process is a well-established fact. Following this, we introduced the image segmentation method that can use context to classify a pixel on the basis of its membership to a particular object-class of the concerned image. In the broad methodological steps, each pixel was defined by its context window (CW) surrounding it the size of which was fixed heuristically. CW texture defined by the intensities of its pixels was convoluted with weights optimized through a non-parametric function supported by a backpropagation network. Result of convolution was used to classify them. The training data points (i.e., pixels) were carefully chosen to include all variety of contexts of types, i) points within the object, ii) points near the edge but inside the objects, iii) points at the border of the objects, iv) points near the edge but outside the objects, v) points near or at the edge of the image frame. Moreover the training data points were selected from all the images within image-dataset. CW texture information for 1000 pixels from face area and background area of images were captured, out of which 700 CWs were used as training input data, and remaining 300 for testing. Our work gives the first time foundation of quantitative enumeration of efficiency of image-segmentation which is extendable to segment out more than 2 objects within an image.",0
"It is a widely accepted fact that human cognitive processes are context-dependent. In light of this, we have introduced a method for image segmentation that utilizes context to classify pixels based on their membership in a specific object class within an image. The first step in our methodology involved defining each pixel by its surrounding context window (CW), which was a fixed size determined heuristically. The CW texture, defined by the intensities of its pixels, was then convoluted with optimized weights through a non-parametric function supported by a backpropagation network. The result of this convolution was used to classify the pixels. To ensure comprehensive training, data points (i.e., pixels) were carefully selected to include all varieties of contexts, including points within the object, near the edge but inside the object, at the border of the object, near the edge but outside the object, and near or at the edge of the image frame. These training data points were selected from all images within the image dataset. We captured CW texture information for 1000 pixels from both the face and background areas of the images, using 700 CWs for training input data and 300 for testing. Our work provides the first foundation for quantitatively enumerating the efficiency of image segmentation, which can be extended to segment out more than two objects within an image.",1
"The problem of segmenting a given image into coherent regions is important in Computer Vision and many industrial applications require segmenting a known object into its components. Examples include identifying individual parts of a component for process control work in a manufacturing plant and identifying parts of a car from a photo for automatic damage detection. Unfortunately most of an object's parts of interest in such applications share the same pixel characteristics, having similar colour and texture. This makes segmenting the object into its components a non-trivial task for conventional image segmentation algorithms. In this paper, we propose a ""Model Assisted Segmentation"" method to tackle this problem. A 3D model of the object is registered over the given image by optimising a novel gradient based loss function. This registration obtains the full 3D pose from an image of the object. The image can have an arbitrary view of the object and is not limited to a particular set of views. The segmentation is subsequently performed using a level-set based method, using the projected contours of the registered 3D model as initialisation curves. The method is fully automatic and requires no user interaction. Also, the system does not require any prior training. We present our results on photographs of a real car.",0
"Segmenting images into coherent regions is a crucial task in Computer Vision, particularly in industrial applications where identifying parts of known objects is required. For instance, manufacturing plants need to identify individual components for process control, while automatic damage detection in cars relies on identifying parts from photos. Unfortunately, conventional image segmentation algorithms find it challenging to segment objects into components with similar colour and texture. To address this issue, we propose a ""Model Assisted Segmentation"" method that registers a 3D model of the object over the given image using a novel gradient-based loss function. This method allows for arbitrary views of the object and requires no user interaction or prior training. We demonstrate the effectiveness of our approach on real car photographs.",1
"The watershed is one of the most used tools in image segmentation. We present how its concept is born and developed over time. Its implementation as an algorithm or a hardwired device evolved together with the technology which allowed it. We present also how it is used in practice, first together with markers, and later introduced in a multiscale framework, in order to produce not a unique partition but a complete hierarchy.",0
"The concept of the watershed has become a popular tool in image segmentation, and its evolution is discussed in this presentation. As technology progressed, it was implemented as an algorithm or device. It is commonly used with markers and has been introduced into a multiscale framework to create a hierarchy rather than a single partition.",1
"Solving the Maximum a Posteriori on Markov Random Field, MRF-MAP, is a prevailing method in recent interactive image segmentation tools. Although mathematically explicit in its computational targets, and impressive for the segmentation quality, MRF-MAP is hard to accomplish without the interactive information from users. So it is rarely adopted in the automatic style up to today. In this paper, we present an automatic image segmentation algorithm, NegCut, based on the approximation to MRF-MAP. First we prove MRF-MAP is NP-hard when the probabilistic models are unknown, and then present an approximation function in the form of minimum cuts on graphs with negative weights. Finally, the binary segmentation is taken from the largest eigenvector of the target matrix, with a tuned version of the Lanczos eigensolver. It is shown competitive at the segmentation quality in our experiments.",0
"Interactive image segmentation tools commonly use the Maximum a Posteriori on Markov Random Field (MRF-MAP) method, which is effective in achieving high-quality segmentation but requires user interaction. Consequently, it has not been widely adopted for automatic segmentation. In this study, we propose an automatic image segmentation algorithm called NegCut, which approximates the MRF-MAP method using minimum cuts on graphs with negative weights. We demonstrate that MRF-MAP is NP-hard when the probabilistic models are unknown. To obtain the binary segmentation, we use the largest eigenvector of the target matrix with a modified Lanczos eigensolver. Our experiments demonstrate that NegCut achieves competitive segmentation quality.",1
"This paper presents a new method for automatic quantification of ellipse-like cells in images, an important and challenging problem that has been studied by the computer vision community. The proposed method can be described by two main steps. Initially, image segmentation based on the k-means algorithm is performed to separate different types of cells from the background. Then, a robust and efficient strategy is performed on the blob contour for touching cells splitting. Due to the contour processing, the method achieves excellent results of detection compared to manual detection performed by specialists.",0
"In this paper, a novel approach is introduced for the automatic measurement of cells with an elliptical shape in images. This is a complex and significant issue that has been investigated by the computer vision field. The technique comprises of two primary stages. Firstly, k-means algorithm-based image segmentation is utilized to differentiate between various kinds of cells and the background. Following this, a strong and efficient method is employed to split touching cells by processing the blob contour. With the assistance of contour processing, the method attains outstanding detection outcomes when compared to the manual detection performed by experts.",1
"Maximum flow (and minimum cut) algorithms have had a strong impact on computer vision. In particular, graph cuts algorithms provide a mechanism for the discrete optimization of an energy functional which has been used in a variety of applications such as image segmentation, stereo, image stitching and texture synthesis. Algorithms based on the classical formulation of max-flow defined on a graph are known to exhibit metrication artefacts in the solution. Therefore, a recent trend has been to instead employ a spatially continuous maximum flow (or the dual min-cut problem) in these same applications to produce solutions with no metrication errors. However, known fast continuous max-flow algorithms have no stopping criteria or have not been proved to converge. In this work, we revisit the continuous max-flow problem and show that the analogous discrete formulation is different from the classical max-flow problem. We then apply an appropriate combinatorial optimization technique to this combinatorial continuous max-flow CCMF problem to find a null-divergence solution that exhibits no metrication artefacts and may be solved exactly by a fast, efficient algorithm with provable convergence. Finally, by exhibiting the dual problem of our CCMF formulation, we clarify the fact, already proved by Nozawa in the continuous setting, that the max-flow and the total variation problems are not always equivalent.",0
"Computer vision has been greatly influenced by maximum flow (and minimum cut) algorithms. Graph cuts algorithms have been utilized for discrete optimization of energy functional in various applications such as image segmentation, stereo, image stitching, and texture synthesis. However, the classical formulation of max-flow on a graph can result in metrication artefacts in the solution. Therefore, the latest trend is to use spatially continuous maximum flow (or the dual min-cut problem) in these applications to obtain solutions with no metrication errors. Unfortunately, known fast continuous max-flow algorithms have no stopping criteria or have not been proven to converge. This study revisits the continuous max-flow problem and reveals that the analogous discrete formulation differs from the classical max-flow problem. An appropriate combinatorial optimization technique is then applied to solve the combinatorial continuous max-flow (CCMF) problem and produce a null-divergence solution that exhibits no metrication artefacts. This efficient algorithm is provably convergent. Lastly, the dual problem of the CCMF formulation is presented to demonstrate that the max-flow and total variation problems are not equivalent, a fact already proven by Nozawa in the continuous setting.",1
In this paper a vision-based vehicles recognition method is presented. Proposed method uses fuzzy description of image segments for automatic recognition of vehicles recorded in image data. The description takes into account selected geometrical properties and shape coefficients determined for segments of reference image (vehicle model). The proposed method was implemented using reasoning system with fuzzy rules. A vehicles recognition algorithm was developed based on the fuzzy rules describing shape and arrangement of the image segments that correspond to visible parts of a vehicle. An extension of the algorithm with set of fuzzy rules defined for different reference images (and various vehicle shapes) enables vehicles classification in traffic scenes. The devised method is suitable for application in video sensors for road traffic control and surveillance systems.,0
"This paper presents a method for recognizing vehicles based on vision. The method involves using fuzzy descriptions of image segments to automatically identify vehicles in image data. The descriptions consider specific geometrical properties and shape coefficients that are determined for segments of a reference image, which represents the vehicle model. The method utilizes a reasoning system with fuzzy rules, and an algorithm that recognizes vehicles based on the shape and arrangement of image segments that correspond to visible parts of the vehicle. The algorithm has been extended to include a set of fuzzy rules defined for different reference images and various vehicle shapes, allowing for vehicles classification in traffic scenes. This method is appropriate for use in video sensors for road traffic control and surveillance systems.",1
"Image segmentation is one of the principal approaches of image processing. The choice of the most appropriate Binarization algorithm for each case proved to be a very interesting procedure itself. In this paper, we have done the comparison study between the various algorithms based on Binarization algorithms and propose a methodologies for the validation of Binarization algorithms. In this work we have developed two novel algorithms to determine threshold values for the pixels value of the gray scale image. The performance estimation of the algorithm utilizes test images with, the evaluation metrics for Binarization of textual and synthetic images. We have achieved better resolution of the image by using the Binarization method of optimum thresholding techniques.",0
"Image processing involves various techniques, with image segmentation being one of the most important. However, selecting the most suitable Binarization algorithm for a given situation can be a complex task. This study aims to compare different Binarization algorithms and proposes new methodologies to validate them. The research introduces two innovative algorithms that determine threshold values for gray scale image pixels. The evaluation of the algorithm's performance includes test images and metrics for Binarization of textual and synthetic images. Through the use of optimum thresholding techniques, the Binarization method enhances image resolution.",1
"Most image labeling problems such as segmentation and image reconstruction are fundamentally ill-posed and suffer from ambiguities and noise. Higher order image priors encode high level structural dependencies between pixels and are key to overcoming these problems. However, these priors in general lead to computationally intractable models. This paper addresses the problem of discovering compact representations of higher order priors which allow efficient inference. We propose a framework for solving this problem which uses a recently proposed representation of higher order functions where they are encoded as lower envelopes of linear functions. Maximum a Posterior inference on our learned models reduces to minimizing a pairwise function of discrete variables, which can be done approximately using standard methods. Although this is a primarily theoretical paper, we also demonstrate the practical effectiveness of our framework on the problem of learning a shape prior for image segmentation and reconstruction. We show that our framework can learn a compact representation that approximates a prior that encourages low curvature shapes. We evaluate the approximation accuracy, discuss properties of the trained model, and show various results for shape inpainting and image segmentation.",0
"Image labeling problems such as segmentation and image reconstruction are inherently problematic due to noise and ambiguities. Overcoming these issues requires higher order image priors that encode structural dependencies between pixels. However, these priors often result in complex and computationally intractable models. This paper presents a framework that addresses this problem by discovering compact representations of higher order priors for efficient inference. The proposed approach uses a novel representation of higher order functions as lower envelopes of linear functions. This allows for maximum a Posterior inference to be reduced to minimizing a pairwise function of discrete variables. The framework is demonstrated on the problem of learning a shape prior for image segmentation and reconstruction, where it effectively learns a compact representation that approximates a prior that encourages low curvature shapes. The paper evaluates the accuracy of the approximation, discusses the properties of the trained model, and presents results for shape inpainting and image segmentation.",1
"In this paper we develop a new technique to model joint distributions of signals. Our technique is based on quantum mechanical conjugate variables. We show that the transition probability of quantum states leads to a distance function on the signals. This distance function obeys the triangle inequality on all quantum states and becomes a metric on pure quantum states. Treating signals as conjugate variables allows us to create a new approach to segment them.   Keywords: Quantum information, transition probability, Euclidean distance, Fubini-study metric, Bhattacharyya coefficients, conjugate variable, signal/sensor fusion, signal and image segmentation.",0
"A novel method for modeling joint signal distributions is presented in this paper, utilizing quantum mechanical conjugate variables. The transition probability of quantum states is demonstrated to produce a distance function on signals, which adheres to the triangle inequality across all quantum states and becomes a metric for pure quantum states. The utilization of signals as conjugate variables enables the development of a fresh approach to segment them. Keywords related to this study include quantum information, transition probability, Euclidean distance, Fubini-study metric, Bhattacharyya coefficients, conjugate variable, signal/sensor fusion, and signal and image segmentation.",1
"Topological alignments and snakes are used in image processing, particularly in locating object boundaries. Both of them have their own advantages and limitations. To improve the overall image boundary detection system, we focused on developing a novel algorithm for image processing. The algorithm we propose to develop will based on the active contour method in conjunction with topological alignments method to enhance the image detection approach. The algorithm presents novel technique to incorporate the advantages of both Topological Alignments and snakes. Where the initial segmentation by Topological Alignments is firstly transformed into the input of the snake model and begins its evolvement to the interested object boundary. The results show that the algorithm can deal with low contrast images and shape cells, demonstrate the segmentation accuracy under weak image boundaries, which responsible for lacking accuracy in image detecting techniques. We have achieved better segmentation and boundary detecting for the image, also the ability of the system to improve the low contrast and deal with over and under segmentation.",0
"In image processing, Topological Alignments and snakes are utilized to locate object boundaries. While both methods have their own benefits and drawbacks, we sought to enhance the image boundary detection system by creating a new algorithm. Our proposed algorithm combines the active contour method with the topological alignments method to improve the image detection approach. This algorithm incorporates the advantages of both Topological Alignments and snakes by utilizing the initial segmentation of Topological Alignments as input for the snake model, allowing it to evolve towards the object boundary. The algorithm can handle low contrast images and shape cells, as well as weak image boundaries that typically affect the accuracy of image detection techniques. We have achieved better segmentation and boundary detection for the image and improved the system's ability to deal with over and under segmentation, as well as low contrast images.",1
"We present the first method to handle curvature regularity in region-based image segmentation and inpainting that is independent of initialization.   To this end we start from a new formulation of length-based optimization schemes, based on surface continuation constraints, and discuss the connections to existing schemes. The formulation is based on a \emph{cell complex} and considers basic regions and boundary elements. The corresponding optimization problem is cast as an integer linear program.   We then show how the method can be extended to include curvature regularity, again cast as an integer linear program. Here, we are considering pairs of boundary elements to reflect curvature. Moreover, a constraint set is derived to ensure that the boundary variables indeed reflect the boundary of the regions described by the region variables.   We show that by solving the linear programming relaxation one gets quite close to the global optimum, and that curvature regularity is indeed much better suited in the presence of long and thin objects compared to standard length regularity.",0
"Our approach addresses the issue of curvature regularity in region-based image segmentation and inpainting without the need for initialization. We propose a novel formulation of length-based optimization schemes that incorporates surface continuation constraints and investigate their relationship with existing schemes. Our formulation employs a \emph{cell complex} that distinguishes between basic regions and boundary elements, with the corresponding optimization problem formulated as an integer linear program. We extend this method to incorporate curvature regularity, where pairs of boundary elements are considered to reflect curvature. We also derive a constraint set to ensure that the boundary variables accurately represent the boundary of the described regions. By solving the linear programming relaxation, we achieve results that are close to the global optimum, and our approach is particularly effective for long and narrow objects compared to standard length regularity.",1
"This paper addresses the automatic image segmentation problem in a region merging style. With an initially over-segmented image, in which the many regions (or super-pixels) with homogeneous color are detected, image segmentation is performed by iteratively merging the regions according to a statistical test. There are two essential issues in a region merging algorithm: order of merging and the stopping criterion. In the proposed algorithm, these two issues are solved by a novel predicate, which is defined by the sequential probability ratio test (SPRT) and the maximum likelihood criterion. Starting from an over-segmented image, neighboring regions are progressively merged if there is an evidence for merging according to this predicate. We show that the merging order follows the principle of dynamic programming. This formulates image segmentation as an inference problem, where the final segmentation is established based on the observed image. We also prove that the produced segmentation satisfies certain global properties. In addition, a faster algorithm is developed to accelerate the region merging process, which maintains a nearest neighbor graph in each iteration. Experiments on real natural images are conducted to demonstrate the performance of the proposed dynamic region merging algorithm.",0
"The aim of this paper is to tackle the issue of automatic image segmentation through a region merging approach. The method involves using an initially over-segmented image where multiple regions with similar colors are detected. These regions are then merged iteratively based on a statistical test. The two main challenges faced in this algorithm are the order in which the regions are merged and the stopping criterion. To address these issues, a novel predicate is proposed that combines the sequential probability ratio test and maximum likelihood criterion. This predicate is used to determine whether neighboring regions should be merged. The algorithm follows the principle of dynamic programming and treats image segmentation as an inference problem. The resulting segmentation satisfies certain global properties, and a faster algorithm is developed to speed up the region merging process. Real natural images are used in experiments to demonstrate the effectiveness of this dynamic region merging algorithm.",1
"Combining the properties of monovariate internal functions as proposed in Kolmogorov superimposition theorem, in tandem with the bounds wielded by the multivariate formulation of Chebyshev inequality, a hybrid model is presented, that decomposes images into homogeneous probabilistically bounded multivariate surfaces. Given an image, the model shows a novel way of working on reduced image representation while processing and capturing the interaction among the multidimensional information that describes the content of the same. Further, it tackles the practical issues of preventing leakage by bounding the growth of surface and reducing the problem sample size. The model if used, also sheds light on how the Chebyshev parameter relates to the number of pixels and the dimensionality of the feature space that associates with a pixel. Initial segmentation results on the Berkeley image segmentation benchmark indicate the effectiveness of the proposed decomposition algorithm.",0
"A new hybrid model has been developed by combining the features of monovariate internal functions from Kolmogorov superimposition theorem and the bounds of multivariate formulation of Chebyshev inequality. This model decomposes images into probabilistically bounded multivariate surfaces that are homogeneous. The model provides a unique way of working on reduced image representation while capturing the interaction among multidimensional information that describes the image content. Additionally, it solves practical issues of preventing leakage by bounding surface growth and reducing the sample size problem. By using this model, the relationship between the Chebyshev parameter, pixel count, and feature space dimensionality can be identified. The algorithm's effectiveness was tested on the Berkeley image segmentation benchmark with promising initial segmentation results.",1
"In this paper we propose a vision system that performs image Super Resolution (SR) with selectivity. Conventional SR techniques, either by multi-image fusion or example-based construction, have failed to capitalize on the intrinsic structural and semantic context in the image, and performed ""blind"" resolution recovery to the entire image area. By comparison, we advocate example-based selective SR whereby selectivity is exemplified in three aspects: region selectivity (SR only at object regions), source selectivity (object SR with trained object dictionaries), and refinement selectivity (object boundaries refinement using matting). The proposed system takes over-segmented low-resolution images as inputs, assimilates recent learning techniques of sparse coding (SC) and grouped multi-task lasso (GMTL), and leads eventually to a framework for joint figure-ground separation and interest object SR. The efficiency of our framework is manifested in our experiments with subsets of the VOC2009 and MSRC datasets. We also demonstrate several interesting vision applications that can build on our system.",0
"The paper presents a new vision system for image Super Resolution (SR) that incorporates selectivity. Traditional SR techniques have not effectively utilized the intrinsic structural and semantic context of images, resulting in blind resolution recovery across the entire image. In contrast, the proposed system advocates for example-based selective SR through region selectivity, source selectivity, and refinement selectivity. The system takes in over-segmented low-resolution images and integrates sparse coding and grouped multi-task lasso learning techniques to enable joint figure-ground separation and interest object SR. The efficiency of the framework is demonstrated through experiments with subsets of the VOC2009 and MSRC datasets, and various vision applications that can be built upon the system are also showcased.",1
"Image segmentation has been a very active research topic in image analysis area. Currently, most of the image segmentation algorithms are designed based on the idea that images are partitioned into a set of regions preserving homogeneous intra-regions and inhomogeneous inter-regions. However, human visual intuition does not always follow this pattern. A new image segmentation method named Visual-Hint Boundary to Segment (VHBS) is introduced, which is more consistent with human perceptions. VHBS abides by two visual hint rules based on human perceptions: (i) the global scale boundaries tend to be the real boundaries of the objects; (ii) two adjacent regions with quite different colors or textures tend to result in the real boundaries between them. It has been demonstrated by experiments that, compared with traditional image segmentation method, VHBS has better performance and also preserves higher computational efficiency.",0
"The field of image analysis has seen a great deal of research activity devoted to image segmentation. Currently, most image segmentation algorithms operate under the assumption that images can be divided into regions that maintain homogeneity within them, and heterogeneity between them. However, human visual perception does not always conform to this pattern. To address this issue, a new image segmentation method called Visual-Hint Boundary to Segment (VHBS) has been developed, which aligns more closely with human visual intuition. VHBS adheres to two visual hint rules based on human perception: (i) global scale boundaries are likely to correspond to the actual boundaries of objects, and (ii) when two adjacent regions have significantly different colors or textures, this usually indicates the presence of a boundary between them. Experimental results have shown that VHBS outperforms traditional image segmentation methods in terms of accuracy while simultaneously maintaining higher computational efficiency.",1
"We propose a mid-level image segmentation framework that combines multiple figure-ground hypothesis (FG) constrained at different locations and scales, into interpretations that tile the entire image. The problem is cast as optimization over sets of maximal cliques sampled from the graph connecting non-overlapping, putative figure-ground segment hypotheses. Potential functions over cliques combine unary Gestalt-based figure quality scores and pairwise compatibilities among spatially neighboring segments, constrained by T-junctions and the boundary interface statistics resulting from projections of real 3d scenes. Learning the model parameters is formulated as rank optimization, alternating between sampling image tilings and optimizing their potential function parameters. State of the art results are reported on both the Berkeley and the VOC2009 segmentation dataset, where a 28% improvement was achieved.",0
"Our proposed framework for mid-level image segmentation utilizes multiple figure-ground hypotheses, which are constrained at various locations and scales to create interpretations that cover the entirety of the image. The problem is approached through optimization over sets of maximal cliques that are sampled from a graph connecting non-overlapping, potential figure-ground segment hypotheses. The potential functions over these cliques combine unary Gestalt-based figure quality scores and pairwise compatibilities among neighboring segments, while also being constrained by T-junctions and boundary interface statistics resulting from real 3d scene projections. To learn the model parameters, rank optimization is used, alternating between sampling image tilings and optimizing potential function parameters. We have achieved state-of-the-art results on both the Berkeley and VOC2009 segmentation datasets, with a 28% improvement.",1
"Object detection has been a focus of research in human-computer interaction. Skin area detection has been a key to different recognitions like face recognition, human motion detection, pornographic and nude image prediction, etc. Most of the research done in the fields of skin detection has been trained and tested on human images of African, Mongolian and Anglo-Saxon ethnic origins. Although there are several intensity invariant approaches to skin detection, the skin color of Indian sub-continentals have not been focused separately. The approach of this research is to make a comparative study between three image segmentation approaches using Indian sub-continental human images, to optimize the detection criteria, and to find some efficient parameters to detect the skin area from these images. The experiments observed that HSV color model based approach to Indian sub-continental skin detection is more suitable with considerable success rate of 91.1% true positives and 88.1% true negatives.",0
"Research in human-computer interaction has placed emphasis on object detection. Skin area detection has played a crucial role in various recognitions such as face recognition, human motion detection, pornographic and nude image prediction, among others. Past studies on skin detection have predominantly focused on human images of African, Mongolian, and Anglo-Saxon ethnic origins. Despite the existence of several intensity invariant approaches for skin detection, the skin color of Indian sub-continentals has not been studied separately. This research aims to compare three image segmentation approaches using Indian sub-continental human images, optimize detection criteria, and identify efficient parameters for skin area detection in these images. The experiments revealed that the HSV color model-based approach for Indian sub-continental skin detection was more suitable, with a success rate of 91.1% true positives and 88.1% true negatives.",1
"The nematode Caenorhabditis elegans is a well-known model organism used to investigate fundamental questions in biology. Motility assays of this small roundworm are designed to study the relationships between genes and behavior. Commonly, motility analysis is used to classify nematode movements and characterize them quantitatively. Over the past years, C. elegans' motility has been studied across a wide range of environments, including crawling on substrates, swimming in fluids, and locomoting through microfluidic substrates. However, each environment often requires customized image processing tools relying on heuristic parameter tuning. In the present study, we propose a novel Multi-Environment Model Estimation (MEME) framework for automated image segmentation that is versatile across various environments. The MEME platform is constructed around the concept of Mixture of Gaussian (MOG) models, where statistical models for both the background environment and the nematode appearance are explicitly learned and used to accurately segment a target nematode. Our method is designed to simplify the burden often imposed on users; here, only a single image which includes a nematode in its environment must be provided for model learning. In addition, our platform enables the extraction of nematode `skeletons' for straightforward motility quantification. We test our algorithm on various locomotive environments and compare performances with an intensity-based thresholding method. Overall, MEME outperforms the threshold-based approach for the overwhelming majority of cases examined. Ultimately, MEME provides researchers with an attractive platform for C. elegans' segmentation and `skeletonizing' across a wide range of motility assays.",0
"The nematode Caenorhabditis elegans is a popular model organism utilized to explore fundamental biological questions. Researchers commonly conduct motility assays on this small roundworm to analyze the relationship between genes and behavior. These assays enable the classification and quantitative characterization of nematode movements. In recent years, C. elegans' motility has been studied in various environments, such as crawling on substrates, swimming in fluids, and moving through microfluidic substrates. However, the task of developing customized image processing tools tailored to each environment can be burdensome and rely on heuristic parameter tuning. This study proposes a Multi-Environment Model Estimation (MEME) framework for automated image segmentation that is versatile and can be used across different environments. MEME utilizes a Mixture of Gaussian (MOG) models that learn statistical models for the background environment and the nematode's appearance to accurately segment the target nematode. Our approach simplifies the user's burden by only requiring a single image that includes a nematode in its environment for model learning. Furthermore, our platform enables easy extraction of nematode ""skeletons"" for straightforward motility quantification. We tested our algorithm on various locomotive environments and compared its performance with an intensity-based thresholding method. Overall, MEME outperforms the threshold-based approach for most cases examined, providing researchers with an attractive platform for C. elegans' segmentation and ""skeletonizing"" across a wide range of motility assays.",1
"Minimization of boundary curvature is a classic regularization technique for image segmentation in the presence of noisy image data. Techniques for minimizing curvature have historically been derived from descent methods which could be trapped in a local minimum and therefore required a good initialization. Recently, combinatorial optimization techniques have been applied to the optimization of curvature which provide a solution that achieves nearly a global optimum. However, when applied to image segmentation these methods required a meaningful data term. Unfortunately, for many images, particularly medical images, it is difficult to find a meaningful data term. Therefore, we propose to remove the data term completely and instead weight the curvature locally, while still achieving a global optimum.",0
"The classic regularization technique for image segmentation in noisy image data involves minimizing boundary curvature. In the past, this has been accomplished through descent methods that risk getting stuck in a local minimum and require a good initialization. Combinatorial optimization techniques have recently been used to optimize curvature and achieve a near-global optimum. However, applying these methods to image segmentation necessitates a meaningful data term, which is often challenging to find in medical images. To address this, we suggest eliminating the data term and instead weighting the curvature locally, while still obtaining a global optimum.",1
"The problem of image segmentation is known to become particularly challenging in the case of partial occlusion of the object(s) of interest, background clutter, and the presence of strong noise. To overcome this problem, the present paper introduces a novel approach segmentation through the use of ""weak"" shape priors. Specifically, in the proposed method, an segmenting active contour is constrained to converge to a configuration at which its geometric parameters attain their empirical probability densities closely matching the corresponding model densities that are learned based on training samples. It is shown through numerical experiments that the proposed shape modeling can be regarded as ""weak"" in the sense that it minimally influences the segmentation, which is allowed to be dominated by data-related forces. On the other hand, the priors provide sufficient constraints to regularize the convergence of segmentation, while requiring substantially smaller training sets to yield less biased results as compared to the case of PCA-based regularization methods. The main advantages of the proposed technique over some existing alternatives is demonstrated in a series of experiments.",0
"When dealing with partial occlusion, background clutter, and strong noise, image segmentation can be particularly difficult. In this paper, a new approach to segmentation is introduced using ""weak"" shape priors. The method involves constraining an active contour to converge to a configuration where its geometric parameters match closely with corresponding model densities learned from training samples. Numerical experiments show that this shape modeling is weakly influential on segmentation, allowing data-related forces to dominate. However, the priors still provide enough constraints to regularize the segmentation and require smaller training sets than PCA-based methods, resulting in less biased results. The proposed technique is compared to existing alternatives in a series of experiments, demonstrating its main advantages.",1
"This paper attempts to undertake the study of segmentation image techniques by using five threshold methods as Mean method, P-tile method, Histogram Dependent Technique (HDT), Edge Maximization Technique (EMT) and visual Technique and they are compared with one another so as to choose the best technique for threshold segmentation techniques image. These techniques applied on three satellite images to choose base guesses for threshold segmentation image.",0
"The objective of this paper is to explore segmentation image techniques by utilizing five threshold methods, namely Mean method, P-tile method, Histogram Dependent Technique (HDT), Edge Maximization Technique (EMT), and visual Technique. A comparison is made among these methods to determine the most effective technique for threshold segmentation image. The study involves the application of these techniques on three satellite images to select the optimal guesses for threshold segmentation image.",1
"Image segmentation is a vital part of image processing. Segmentation has its application widespread in the field of medical images in order to diagnose curious diseases. The same medical images can be segmented manually. But the accuracy of image segmentation using the segmentation algorithms is more when compared with the manual segmentation. In the field of medical diagnosis an extensive diversity of imaging techniques is presently available, such as radiography, computed tomography (CT) and magnetic resonance imaging (MRI). Medical image segmentation is an essential step for most consequent image analysis tasks. Although the original FCM algorithm yields good results for segmenting noise free images, it fails to segment images corrupted by noise, outliers and other imaging artifact. This paper presents an image segmentation approach using Modified Fuzzy C-Means (FCM) algorithm and Fuzzy Possibilistic c-means algorithm (FPCM). This approach is a generalized version of standard Fuzzy CMeans Clustering (FCM) algorithm. The limitation of the conventional FCM technique is eliminated in modifying the standard technique. The Modified FCM algorithm is formulated by modifying the distance measurement of the standard FCM algorithm to permit the labeling of a pixel to be influenced by other pixels and to restrain the noise effect during segmentation. Instead of having one term in the objective function, a second term is included, forcing the membership to be as high as possible without a maximum limit constraint of one. Experiments are conducted on real images to investigate the performance of the proposed modified FCM technique in segmenting the medical images. Standard FCM, Modified FCM, Fuzzy Possibilistic CMeans algorithm (FPCM) are compared to explore the accuracy of our proposed approach.",0
"Image processing relies heavily on the process of image segmentation, which has a wide range of applications in the field of medical imaging for the purpose of detecting various diseases. While manual segmentation can be done on medical images, segmentation algorithms tend to offer greater accuracy. With a variety of imaging techniques available for medical diagnosis, such as radiography, computed tomography, and magnetic resonance imaging, segmenting medical images is an essential step for subsequent analysis. However, the original FCM algorithm struggles with images that are affected by noise, outliers, and other artifacts. This paper introduces a modified approach to image segmentation using the Fuzzy C-Means algorithm and Fuzzy Possibilistic C-Means algorithm, which eliminates the limitations of the conventional FCM technique by modifying the standard approach's distance measurement to account for the influence of other pixels and reduce noise during segmentation. The proposed modified FCM technique is compared to Standard FCM and FPCM using real images to assess its segmentation accuracy.",1
Medical image segmentation demands an efficient and robust segmentation algorithm against noise. The conventional fuzzy c-means algorithm is an efficient clustering algorithm that is used in medical image segmentation. But FCM is highly vulnerable to noise since it uses only intensity values for clustering the images. This paper aims to develop a novel and efficient fuzzy spatial c-means clustering algorithm which is robust to noise. The proposed clustering algorithm uses fuzzy spatial information to calculate membership value. The input image is clustered using proposed ISFCM algorithm. A comparative study has been made between the conventional FCM and proposed ISFCM. The proposed approach is found to be outperforming the conventional FCM.,0
"Efficient and robust segmentation algorithms are necessary for medical image segmentation, particularly when dealing with noise. While the conventional fuzzy c-means (FCM) algorithm is a widely used clustering algorithm in medical image segmentation, it is susceptible to noise due to its reliance on intensity values for clustering. This study introduces a novel fuzzy spatial c-means clustering algorithm that is more resilient to noise. By incorporating fuzzy spatial information, the proposed algorithm calculates membership values and clusters the input image. A comparison between the conventional FCM and the proposed ISFCM algorithm was conducted, and the results demonstrate that the ISFCM algorithm outperforms the conventional FCM algorithm.",1
In this article we give our contribution to the problem of segmentation with plug-in procedures. We give general sufficient conditions under which plug in procedure are efficient. We also give an algorithm that satisfy these conditions. We give an application of the used algorithm to hyperspectral images segmentation. Hyperspectral images are images that have both spatial and spectral coherence with thousands of spectral bands on each pixel. In the proposed procedure we combine a reduction dimension technique and a spatial regularisation technique. This regularisation is based on the mixlet modelisation of Kolaczyck and Al.,0
"Our article presents our contribution to the segmentation problem through the use of plug-in procedures. We provide comprehensive and practical sufficient conditions for the effectiveness of these procedures, along with an algorithm that meets these conditions. To demonstrate the algorithm's capabilities, we apply it to hyperspectral image segmentation, which involves thousands of spectral bands per pixel, as well as spatial and spectral coherence. Our procedure employs a combination of dimension reduction and spatial regularization techniques, with the latter based on Kolaczyck and Al's mixlet modeling.",1
"Acquisition-to-acquisition signal intensity variations (non-standardness) are inherent in MR images. Standardization is a post processing method for correcting inter-subject intensity variations through transforming all images from the given image gray scale into a standard gray scale wherein similar intensities achieve similar tissue meanings. The lack of a standard image intensity scale in MRI leads to many difficulties in tissue characterizability, image display, and analysis, including image segmentation. This phenomenon has been documented well; however, effects of standardization on medical image registration have not been studied yet. In this paper, we investigate the influence of intensity standardization in registration tasks with systematic and analytic evaluations involving clinical MR images. We conducted nearly 20,000 clinical MR image registration experiments and evaluated the quality of registrations both quantitatively and qualitatively. The evaluations show that intensity variations between images degrades the accuracy of registration performance. The results imply that the accuracy of image registration not only depends on spatial and geometric similarity but also on the similarity of the intensity values for the same tissues in different images.",0
"MR images naturally exhibit variations in signal intensity between acquisitions, which can result in non-standardness. To address this issue, standardization is a post-processing technique used to correct inter-subject intensity variations by transforming all images to a standard gray scale. The absence of a standard intensity scale in MRI poses several challenges regarding tissue identification, image display, and analysis, including image segmentation. While the impact of standardization on medical image registration remains unexplored, this paper investigates its influence through systematic and analytic evaluations involving clinical MR images. Our study involved nearly 20,000 clinical MR image registration experiments, with quantitative and qualitative assessments of registration quality. The evaluations revealed that intensity variations between images adversely affect registration accuracy, suggesting that image registration accuracy depends not only on spatial and geometric similarity but also on the similarity of intensity values for the same tissues across different images.",1
"One of the important evidence in a crime scene that is normally overlooked but very important evidence is shoe print as the criminal is normally unaware of the mask for this. In this paper we use image processing technique to process reference shoe images to make it index-able for a search from the database the shoe print impressions available in the commercial market. This is achieved first by converting the commercially available image through the process of converting them to gray scale then apply image enhancement and restoration techniques and finally do image segmentation to store the segmented parameter as index in the database storage. We use histogram method for image enhancement, inverse filtering for image restoration and threshold method for indexing. We use global threshold as index of the shoe print. The paper describes this method and simulation results are included to validate the method.",0
"Shoe prints are often overlooked as important evidence in a crime scene, as criminals are typically unaware of leaving them behind. This paper proposes the use of image processing techniques to index shoe print impressions available in the commercial market by processing reference shoe images. The process involves converting the available images to grayscale, enhancing and restoring them using histogram and inverse filtering methods, and segmenting them to store the segmented parameter as an index in the database storage. The global threshold is used as an index for the shoe print. The paper explains the method and includes simulation results to validate it.",1
"Segmentation of medical images using seeded region growing technique is increasingly becoming a popular method because of its ability to involve high-level knowledge of anatomical structures in seed selection process. Region based segmentation of medical images are widely used in varied clinical applications like visualization, bone detection, tumor detection and unsupervised image retrieval in clinical databases. As medical images are mostly fuzzy in nature, segmenting regions based intensity is the most challenging task. In this paper, we discuss about popular seeded region grow methodology used for segmenting anatomical structures in CT Angiography images. We have proposed a gradient based homogeneity criteria to control the region grow process while segmenting CTA images.",0
"The seeded region growing technique is increasingly popular for segmenting medical images due to its ability to incorporate high-level knowledge of anatomical structures during seed selection. Medical images are often fuzzy, making region-based segmentation based on intensity a challenging task. This technique is widely used in various clinical applications, including visualization, bone detection, tumor detection, and unsupervised image retrieval in clinical databases. This paper focuses on the popular seeded region growing methodology for segmenting anatomical structures in CT Angiography images. A gradient-based homogeneity criterion is proposed to regulate the region growing process during CTA image segmentation.",1
"The need of sign language is increasing radically especially to hearing impaired community. Only few research groups try to automatically recognize sign language from video, colored gloves and etc. Their approach requires a valid segmentation of the data that is used for training and of the data that is used to be recognized. Recognition of a sign language image sequence is challenging because of the variety of hand shapes and hand motions. Here, this paper proposes to apply a combination of image segmentation with restoration using topological derivatives for achieving high recognition accuracy. Image quality measures are conceded here to differentiate the methods both subjectively as well as objectively. Experiments show that the additional use of the restoration before segmenting the postures significantly improves the correct rate of hand detection, and that the discrete derivatives yields a high rate of discrimination between different static hand postures as well as between hand postures and the scene background. Eventually, the research is to contribute to the implementation of automated sign language recognition system mainly established for the welfare purpose.",0
"The demand for sign language is rapidly increasing, particularly within the hearing impaired community. Despite this, only a small number of research groups are working on automatic recognition of sign language using techniques such as video and colored gloves. These methods require accurate segmentation of training and recognition data, which can be challenging due to the diversity of hand shapes and motions. This paper proposes a novel approach that combines image segmentation with restoration using topological derivatives to achieve higher recognition accuracy. The effectiveness of the proposed method is evaluated using subjective and objective image quality measures. Results show that restoration before segmentation significantly improves hand detection accuracy, while discrete derivatives enable discrimination between different hand postures and the background scene. Ultimately, this research aims to contribute to the development of an automated sign language recognition system for the benefit of the community.",1
"We presents in this paper a novel fish classification methodology based on a combination between robust feature selection, image segmentation and geometrical parameter techniques using Artificial Neural Network and Decision Tree. Unlike existing works for fish classification, which propose descriptors and do not analyze their individual impacts in the whole classification task and do not make the combination between the feature selection, image segmentation and geometrical parameter, we propose a general set of features extraction using robust feature selection, image segmentation and geometrical parameter and their correspondent weights that should be used as a priori information by the classifier. In this sense, instead of studying techniques for improving the classifiers structure itself, we consider it as a black box and focus our research in the determination of which input information must bring a robust fish discrimination.The main contribution of this paper is enhancement recognize and classify fishes based on digital image and To develop and implement a novel fish recognition prototype using global feature extraction, image segmentation and geometrical parameters, it have the ability to Categorize the given fish into its cluster and Categorize the clustered fish into poison or non-poison fish, and categorizes the non-poison fish into its family .",0
"In this paper, we introduce a new approach to classifying fish that utilizes a combination of robust feature selection, image segmentation, and geometrical parameter techniques. Our methodology employs Artificial Neural Network and Decision Tree to achieve accurate fish classification. Unlike previous works, our approach does not rely on descriptors and instead analyzes the individual impact of each feature selection, image segmentation, and geometrical parameter technique. We propose a set of features extraction along with corresponding weights that should be used as a priori information by the classifier. Rather than solely focusing on improving the classifier's structure, we treat it as a black box and concentrate on determining the input information that ensures robust fish discrimination. Our research aims to enhance fish recognition and classification based on digital images and to develop a fish recognition prototype capable of categorizing fish into clusters, separating them into poison or non-poison, and categorizing non-poison fish into their respective families.",1
"Images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation. Machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates. However, this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph. We present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the Rand index, a well known segmentation performance measure. The Rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation. By using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph, we are able to train an affinity classifier to directly minimize the Rand index of segmentations resulting from the graph partitioning. Our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs, which are predictive of the pixel-pair connectivity.",0
"To segment images, the first step is to use a classifier to predict an affinity graph that groups image pixels together. This graph is then partitioned to create a segmentation. Machine learning techniques have been applied to improve the affinity classifier, but the error rate alone does not necessarily guarantee high-quality segmentations. Our new machine learning algorithm trains a classifier to produce affinity graphs that directly minimize the Rand index, a well-known measure of segmentation performance. By finding the connected components of the thresholded affinity graph, our algorithm trains the affinity classifier to minimize the Rand index of resulting segmentations. Our learning algorithm focuses on the maximin affinities between image pixel pairs, which predict pixel-pair connectivity.",1
"By a ""covering"" we mean a Gaussian mixture model fit to observed data. Approximations of the Bayes factor can be availed of to judge model fit to the data within a given Gaussian mixture model. Between families of Gaussian mixture models, we propose the R\'enyi quadratic entropy as an excellent and tractable model comparison framework. We exemplify this using the segmentation of an MRI image volume, based (1) on a direct Gaussian mixture model applied to the marginal distribution function, and (2) Gaussian model fit through k-means applied to the 4D multivalued image volume furnished by the wavelet transform. Visual preference for one model over another is not immediate. The R\'enyi quadratic entropy allows us to show clearly that one of these modelings is superior to the other.",0
"A ""covering"" refers to a Gaussian mixture model that is fitted to observed data. To assess the model fit within a given Gaussian mixture model, approximations of the Bayes factor can be used. For comparing different families of Gaussian mixture models, we suggest using the R\'enyi quadratic entropy as it is an effective and manageable framework. We demonstrate this by performing segmentation on an MRI image volume using two approaches: (1) direct application of a Gaussian mixture model to the marginal distribution function, and (2) fitting a Gaussian model through k-means to the 4D multivalued image volume obtained from the wavelet transform. It is not immediately apparent which model is better from a visual standpoint. However, the R\'enyi quadratic entropy enables us to clearly identify which modeling approach is superior.",1
"We exam various geometric active contour methods for radar image segmentation. Due to special properties of radar images, we propose our new model based on modified Chan-Vese functional. Our method is efficient in separating non-meteorological noises from meteorological images.",0
"We evaluated multiple geometric active contour techniques to segment radar images. Considering the unique characteristics of radar images, we developed a novel model using a modified Chan-Vese functional. Our approach effectively distinguishes meteorological images from non-meteorological noises.",1
"Physical modeling method, represented by simulation and visualization of the principles in physics, is introduced in the shape extraction of the active contours. The objectives of adopting this concept are to address the several major difficulties in the application of Active Contours. Primarily, a technique is developed to realize the topological changes of Parametric Active Contours (Snakes). The key strategy is to imitate the process of a balloon expanding and filling in a closed space with several objects. After removing the touched balloon surfaces, the objects can be identified by surrounded remaining balloon surfaces. A burned region swept by Snakes is utilized to trace the contour and to give a criterion for stopping the movement of Snake curve. When the Snakes terminates evolution totally, through ignoring this criterion, it can form a connected area by evolving the Snakes again and continuing the region burning. The contours extracted from the boundaries of the burned area can represent the child snake of each object respectively. Secondly, a novel scheme is designed to solve the problems of leakage of the contour from the large gaps, and the segmentation error in Geometric Active Contours (GAC). It divides the segmentation procedure into two processing stages. By simulating the wave propagating in the isotropic substance at the final stage, it can significantly enhance the effect of image force in GAC based on Level Set and give the satisfied solutions to the two problems. Thirdly, to support the physical models for active contours above, we introduce a general image force field created on a template plane over the image plane. This force is more adaptable to noisy images with complicated geometric shapes.",0
"The utilization of the physical modeling method, which involves simulating and visualizing physics principles, is implemented in the extraction process of active contours. This approach aims to overcome the major challenges that arise with the use of Active Contours. In order to achieve this, a technique is developed to enable the topological changes of Parametric Active Contours (Snakes), which involves the simulation of a balloon expanding and filling a closed space with multiple objects. The objects are then identified by the remaining surfaces of the balloon after removing the touched surfaces. The contours are traced using a burned region swept by the Snakes and a stopping criterion is used to halt the movement of the Snake curve. If the Snakes fail to terminate evolution, a connected area is formed by evolving them again and continuing the region burning. The contours extracted represent the child snake of each object respectively. To address the issues of leakage of the contour from large gaps and segmentation errors in Geometric Active Contours (GAC), a new scheme is designed that divides the segmentation process into two stages. At the final stage, the simulation of wave propagation in an isotropic substance is used to enhance the image force in GAC based on Level Set, resulting in satisfactory solutions to the two problems. Additionally, a general image force field is introduced to support the physical models for active contours, created on a template plane over the image plane, which is more adaptable to noisy images with complicated geometric shapes.",1
"Image segmentation techniques are predominately based on parameter-laden optimization. The objective function typically involves weights for balancing competing image fidelity and segmentation regularization cost terms. Setting these weights suitably has been a painstaking, empirical process. Even if such ideal weights are found for a novel image, most current approaches fix the weight across the whole image domain, ignoring the spatially-varying properties of object shape and image appearance. We propose a novel technique that autonomously balances these terms in a spatially-adaptive manner through the incorporation of image reliability in a graph-based segmentation framework. We validate on synthetic data achieving a reduction in mean error of 47% (p-value << 0.05) when compared to the best fixed parameter segmentation. We also present results on medical images (including segmentations of the corpus callosum and brain tissue in MRI data) and on natural images.",0
"The current methods for image segmentation rely heavily on optimization that involves parameters. The objective function involves balancing image fidelity and segmentation regularization cost using weights, which are difficult to set appropriately through an empirical process. Moreover, these weights are usually fixed across the entire image, disregarding the varying object shape and image appearance. Our proposed technique utilizes image reliability in a graph-based segmentation framework to autonomously balance these terms in a spatially-adaptive manner. We have conducted experiments on synthetic data, which resulted in a significant reduction in mean error of 47% (with p-value << 0.05) compared to the best fixed parameter segmentation. Additionally, we have demonstrated the effectiveness of our approach on medical images, such as segmentations of the corpus callosum and brain tissue in MRI data, as well as natural images.",1
"We describe a fully-automatic 3D-segmentation technique for brain MR images. Using Markov random fields the segmentation algorithm captures three important MR features, i.e. non-parametric distributions of tissue intensities, neighborhood correlations and signal inhomogeneities. Detailed simulations and real MR images demonstrate the performance of the segmentation algorithm. The impact of noise, inhomogeneity, smoothing and structure thickness is analyzed quantitatively. Even single echo MR images are well classified into gray matter, white matter, cerebrospinal fluid, scalp-bone and background. A simulated annealing and an iterated conditional modes implementation are presented.   Keywords: Magnetic Resonance Imaging, Segmentation, Markov Random Fields",0
"A fully-automatic segmentation technique for brain MR images is presented, utilizing Markov random fields to capture non-parametric distributions of tissue intensities, neighborhood correlations, and signal inhomogeneities. The segmentation algorithm's performance is demonstrated through detailed simulations and real MR images, taking into account the impact of noise, inhomogeneity, smoothing, and structure thickness. Gray matter, white matter, cerebrospinal fluid, scalp-bone, and background can all be classified accurately, even with single echo MR images. The implementation of the segmentation algorithm includes a simulated annealing and an iterated conditional modes. Keywords include Magnetic Resonance Imaging, Segmentation, and Markov Random Fields.",1
"Min-cut clustering, based on minimizing one of two heuristic cost-functions proposed by Shi and Malik, has spawned tremendous research, both analytic and algorithmic, in the graph partitioning and image segmentation communities over the last decade. It is however unclear if these heuristics can be derived from a more general principle facilitating generalization to new problem settings. Motivated by an existing graph partitioning framework, we derive relationships between optimizing relevance information, as defined in the Information Bottleneck method, and the regularized cut in a K-partitioned graph. For fast mixing graphs, we show that the cost functions introduced by Shi and Malik can be well approximated as the rate of loss of predictive information about the location of random walkers on the graph. For graphs generated from a stochastic algorithm designed to model community structure, the optimal information theoretic partition and the optimal min-cut partition are shown to be the same with high probability.",0
"Over the past decade, Min-cut clustering, which involves minimizing one of two cost-functions proposed by Shi and Malik, has generated significant research in both the graph partitioning and image segmentation communities. However, it remains unclear if these heuristics can be applied to other problem settings. To address this, we have investigated the relationship between optimizing relevance information and regularized cut in a K-partitioned graph, building on an existing graph partitioning framework. Our results show that, for fast mixing graphs, the cost functions proposed by Shi and Malik can be approximated by the rate of loss of predictive information about random walkers' location on the graph. Furthermore, for graphs generated using a stochastic algorithm to model community structure, we demonstrate that the optimal information theoretic partition and the optimal min-cut partition are highly likely to be the same.",1
"Distributed synchronization is known to occur at several scales in the brain, and has been suggested as playing a key functional role in perceptual grouping. State-of-the-art visual grouping algorithms, however, seem to give comparatively little attention to neural synchronization analogies. Based on the framework of concurrent synchronization of dynamic systems, simple networks of neural oscillators coupled with diffusive connections are proposed to solve visual grouping problems. Multi-layer algorithms and feedback mechanisms are also studied. The same algorithm is shown to achieve promising results on several classical visual grouping problems, including point clustering, contour integration and image segmentation.",0
"The brain exhibits distributed synchronization at various levels, which is believed to have a significant functional impact on perceptual grouping. However, modern visual grouping techniques do not seem to place much emphasis on neural synchronization comparisons. To address this, a solution to visual grouping problems is proposed through simple networks of neural oscillators with diffusive connections, based on the concurrent synchronization of dynamic systems framework. Additionally, multi-layer algorithms and feedback mechanisms are explored. This same approach is applied to a range of classic visual grouping tasks, such as point clustering, contour integration, and image segmentation, and produces encouraging outcomes.",1
"This paper proposes a novel algorithm for the problem of structural image segmentation through an interactive model-based approach. Interaction is expressed in the model creation, which is done according to user traces drawn over a given input image. Both model and input are then represented by means of attributed relational graphs derived on the fly. Appearance features are taken into account as object attributes and structural properties are expressed as relational attributes. To cope with possible topological differences between both graphs, a new structure called the deformation graph is introduced. The segmentation process corresponds to finding a labelling of the input graph that minimizes the deformations introduced in the model when it is updated with input information. This approach has shown to be faster than other segmentation methods, with competitive output quality. Therefore, the method solves the problem of multiple label segmentation in an efficient way. Encouraging results on both natural and target-specific color images, as well as examples showing the reusability of the model, are presented and discussed.",0
"A new technique for structural image segmentation is suggested in this article, using an interactive model-based method. User input is utilized in model creation, with traces drawn over a given input image. Both the model and input are represented through attributed relational graphs, with object attributes accounting for appearance features and relational attributes expressing structural properties. A deformation graph is introduced to handle any topological differences between the two graphs. The segmentation process entails finding a labelling for the input graph that minimizes deformations in the model when updated with input information. This approach is faster than other segmentation methods, with competitive output quality, and efficiently addresses the issue of multiple label segmentation. The article includes examples of encouraging results for both natural and target-specific color images, as well as the model's reusability.",1
"Image segmentation is to separate an image into distinct homogeneous regions belonging to different objects. It is an essential step in image analysis and computer vision. This paper compares some segmentation technologies and attempts to find an automated way to better determine the parameters for image segmentation, especially the connectivity value of $\lambda$ in $\lambda$-connected segmentation.   Based on the theories on the maximum entropy method and Otsu's minimum variance method, we propose:(1)maximum entropy connectedness determination: a method that uses maximum entropy to determine the best $\lambda$ value in $\lambda$-connected segmentation, and (2) minimum variance connectedness determination: a method that uses the principle of minimum variance to determine $\lambda$ value. Applying these optimization techniques in real images, the experimental results have shown great promise in the development of the new methods. In the end, we extend the above method to more general case in order to compare it with the famous Mumford-Shah method that uses variational principle and geometric measure.",0
"The process of image segmentation involves dividing an image into different regions that pertain to various objects within it. This action is pivotal in both image analysis and computer vision. This research paper aims to compare several segmentation technologies while seeking out an automated approach to improve the process of determining segmentation parameters. In particular, we focus on the connectivity value of $\lambda$ in $\lambda$-connected segmentation. Our proposed methods are the maximum entropy connectedness determination, which utilizes maximum entropy to determine the optimal $\lambda$ value, and the minimum variance connectedness determination, which employs the principle of minimum variance to determine $\lambda$ value. By implementing these optimization techniques in real-life images, our experiments have shown significant potential in developing new methods. Finally, we extend these methods to more general cases and compare them to the Mumford-Shah method, which employs both variational principle and geometric measure.",1
"Partitioning and grouping of similar objects plays a fundamental role in image segmentation and in clustering problems. In such problems a typical goal is to group together similar objects, or pixels in the case of image processing. At the same time another goal is to have each group distinctly dissimilar from the rest and possibly to have the group size fairly large. These goals are often combined as a ratio optimization problem. One example of such problem is the normalized cut problem, another is the ratio regions problem. We devise here the first polynomial time algorithms solving these problems optimally. The algorithms are efficient and combinatorial. This contrasts with the heuristic approaches used in the image segmentation literature that formulate those problems as nonlinear optimization problems, which are then relaxed and solved with spectral techniques in real numbers. These approaches not only fail to deliver an optimal solution, but they are also computationally expensive. The algorithms presented here use as a subroutine a minimum $s,t-cut procedure on a related graph which is of polynomial size. The output consists of the optimal solution to the respective ratio problem, as well as a sequence of nested solution with respect to any relative weighting of the objectives of the numerator and denominator.   An extension of the results here to bi-criteria and multi-criteria objective functions is presented in part II.",0
"Image segmentation and clustering problems rely heavily on partitioning and grouping similar objects. The main aim is to group similar objects or pixels together, while ensuring that each group is distinct from the rest, and preferably of a considerable size. Achieving these goals requires a ratio optimization problem, such as the normalized cut problem or the ratio regions problem. We have created polynomial time algorithms that can solve these problems optimally, without the need for heuristic methods. In contrast, existing literature on image segmentation often uses nonlinear optimization problems, which are then relaxed and solved using spectral techniques in real numbers. These methods are not only computationally expensive but also fail to deliver an optimal solution. Our algorithms utilize a minimum $s,t-cut procedure on a related graph, which has a polynomial size, and can provide the optimal solution to the respective ratio problem. Additionally, they offer a sequence of nested solutions based on any relative weighting of the numerator and denominator objectives. Part II of this work extends these results to bi-criteria and multi-criteria objective functions.",1
"This paper proposes a novel method for segmentation of images by hierarchical multilevel thresholding. The method is global, agglomerative in nature and disregards pixel locations. It involves the optimization of the ratio of the unbiased estimators of within class to between class variances. We obtain a recursive relation at each step for the variances which expedites the process. The efficacy of the method is shown in a comparison with some well-known methods.",0
"The proposed approach introduced in this paper suggests a new technique for image segmentation using hierarchical multilevel thresholding. The method is agglomerative and global, and it does not take into account pixel positioning. The optimization of the ratio between within class and between class variances is central to the approach. A recursive relation for the variances at each stage expedites the process. The effectiveness of this method is demonstrated by comparing it with several established methods.",1
"We propose a new method for the numerical solution of a PDE-driven model for colour image segmentation and give numerical examples of the results. The method combines the vector-valued Allen-Cahn phase field equation with initial data fitting terms. This method is known to be closely related to the Mumford-Shah problem and the level set segmentation by Chan and Vese. Our numerical solution is performed using a multigrid splitting of a finite element space, thereby producing an efficient and robust method for the segmentation of large images.",0
"Our proposed approach for colour image segmentation involves utilizing the vector-valued Allen-Cahn phase field equation alongside initial data fitting terms. This method shares similarities with both the Mumford-Shah problem and the level set segmentation approach developed by Chan and Vese. To demonstrate the effectiveness of our approach, we provide numerical examples of the results achieved. The numerical solution is carried out through a multigrid splitting of a finite element space, which offers an efficient and reliable method for segmenting large images.",1
"Kernel estimation techniques, such as mean shift, suffer from one major drawback: the kernel bandwidth selection. The bandwidth can be fixed for all the data set or can vary at each points. Automatic bandwidth selection becomes a real challenge in case of multidimensional heterogeneous features. This paper presents a solution to this problem. It is an extension of \cite{Comaniciu03a} which was based on the fundamental property of normal distributions regarding the bias of the normalized density gradient. The selection is done iteratively for each type of features, by looking for the stability of local bandwidth estimates across a predefined range of bandwidths. A pseudo balloon mean shift filtering and partitioning are introduced. The validity of the method is demonstrated in the context of color image segmentation based on a 5-dimensional space.",0
"The main issue with kernel estimation techniques, particularly mean shift, is the selection of the kernel bandwidth. It can either be constant for the entire dataset or vary for each point. For datasets with diverse, multidimensional features, automatic bandwidth selection is a difficult task. This paper proposes a solution to address this problem, building upon the approach presented in \cite{Comaniciu03a}, which uses the normal distribution's fundamental property to normalize density gradients. The proposed method involves iterative selection of bandwidths for each feature type, by assessing the stability of local bandwidth estimates across a preset range of bandwidths. The method introduces a pseudo balloon mean shift filtering and partitioning. The validity of the approach is shown in the context of color image segmentation in a 5-dimensional space.",1
"This paper presents deformable templates as a tool for segmentation and localization of biological structures in medical images. Structures are represented by a prototype template, combined with a parametric warp mapping used to deform the original shape. The localization procedure is achieved using a multi-stage, multi-resolution algorithm de-signed to reduce computational complexity and time. The algorithm initially identifies regions in the image most likely to contain the desired objects and then examines these regions at progressively increasing resolutions. The final stage of the algorithm involves warping the prototype template to match the localized objects. The algorithm is presented along with the results of four example applications using MRI, x-ray and ultrasound images.",0
"In this paper, the use of deformable templates for segmenting and localizing biological structures in medical images is introduced. The prototype template represents the structure and is combined with a parametric warp mapping to modify the original shape. To minimize computation time, a multi-stage, multi-resolution algorithm is utilized. The algorithm begins by identifying the most probable regions in the image where the object is located and then examines these regions at progressively higher resolutions. In the final stage, the prototype template is warped to match the localized object. The paper also includes four examples of the algorithm applied to MRI, x-ray and ultrasound images.",1
"A novel algorithm is proposed for segmenting an image into multiple levels using its mean and variance. Starting from the extreme pixel values at both ends of the histogram plot, the algorithm is applied recursively on sub-ranges computed from the previous step, so as to find a threshold level and a new sub-range for the next step, until no significant improvement in image quality can be achieved. The method makes use of the fact that a number of distributions tend towards Dirac delta function, peaking at the mean, in the limiting condition of vanishing variance. The procedure naturally provides for variable size segmentation with bigger blocks near the extreme pixel values and finer divisions around the mean or other chosen value for better visualization. Experiments on a variety of images show that the new algorithm effectively segments the image in computationally very less time.",0
"A new algorithm has been introduced to segment an image into multiple levels based on its mean and variance. The algorithm begins by analyzing the extreme pixel values at both ends of the histogram plot and then proceeds recursively on sub-ranges derived from the previous step. This enables the algorithm to identify a threshold level and a new sub-range for the next step until no further improvement in image quality can be achieved. The algorithm leverages the fact that several distributions tend towards Dirac delta function, which peaks at the mean, when their variance approaches zero. The algorithm's natural property is to provide variable size segmentation with larger blocks around extreme pixel values and finer divisions around the mean or any other selected value for improved visualization. The algorithm's effectiveness has been tested on various images, demonstrating that it can successfully segment the image in considerably less computational time.",1
"In this paper we present an unconventional image segmentation approach which is devised to meet the requirements of image understanding and pattern recognition tasks. Generally image understanding assumes interplay of two sub-processes: image information content discovery and image information content interpretation. Despite of its widespread use, the notion of ""image information content"" is still ill defined, intuitive, and ambiguous. Most often, it is used in the Shannon's sense, which means information content assessment averaged over the whole signal ensemble. Humans, however,rarely resort to such estimates. They are very effective in decomposing images into their meaningful constituents and focusing attention to the perceptually relevant image parts. We posit that following the latest findings in human attention vision studies and the concepts of Kolmogorov's complexity theory an unorthodox segmentation approach can be proposed that provides effective image decomposition to information preserving image fragments well suited for subsequent image interpretation. We provide some illustrative examples, demonstrating effectiveness of this approach.",0
"This paper introduces a novel technique for image segmentation that is specifically designed to meet the demands of image understanding and pattern recognition tasks. Image understanding entails two sub-processes: discovering the information content of an image and interpreting that content. Despite its widespread use, the term ""image information content"" is still not clearly defined and can be ambiguous. Usually, it is assessed in a Shannon sense, which averages information content over the entire signal ensemble. Humans, on the other hand, tend to decompose images into meaningful components and focus their attention on the relevant parts. Using recent research on human vision and Kolmogorov's complexity theory, we propose an unconventional segmentation approach that can effectively decompose images into information-preserving fragments that are suitable for further interpretation. We provide illustrative examples to demonstrate the effectiveness of this approach.",1
"The Gradient Vector Flow (GVF) is a vector diffusion approach based on Partial Differential Equations (PDEs). This method has been applied together with snake models for boundary extraction medical images segmentation. The key idea is to use a diffusion-reaction PDE to generate a new external force field that makes snake models less sensitivity to initialization as well as improves the snake's ability to move into boundary concavities. In this paper, we firstly review basic results about convergence and numerical analysis of usual GVF schemes. We point out that GVF presents numerical problems due to discontinuities image intensity. This point is considered from a practical viewpoint from which the GVF parameters must follow a relationship in order to improve numerical convergence. Besides, we present an analytical analysis of the GVF dependency from the parameters values. Also, we observe that the method can be used for multiply connected domains by just imposing the suitable boundary condition. In the experimental results we verify these theoretical points and demonstrate the utility of GVF on a segmentation approach that we have developed based on snakes.",0
"The Gradient Vector Flow (GVF) utilizes Partial Differential Equations (PDEs) for vector diffusion and is frequently paired with snake models in medical image segmentation. By implementing a diffusion-reaction PDE, GVF generates a new external force field that improves the snake models' ability to move into boundary concavities and reduces sensitivity to initialization. This paper initially reviews the convergence and numerical analysis of standard GVF schemes, highlighting the numerical issues due to image intensity discontinuities. From a practical perspective, the GVF parameters must follow a relationship for improved numerical convergence. Additionally, an analytical analysis of GVF's parameter values dependency is presented. The paper also notes that GVF can be used for multiply connected domains with the appropriate boundary condition. The experimental results demonstrate the utility of GVF in a segmentation approach based on snakes and validate the theoretical points discussed.",1
"We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. This work is more of a test of the whole system, rather than of any one part of the system. However, beyond the concept of the system itself, the uncommon map (despite its simplicity) is the main innovative part of the system. The uncommon map helps to determine interest-points in a context-free manner. Overall, the hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: dealing with structural shadow and microtexture, and also, controlling the camera's zoom lens in an intelligent manner. Nonetheless, despite these and other technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner. We use these capabilities for autonomous guidance towards geological points-of-interest.",0
"Our study presents the initial geological field tests of the `Cyborg Astrobiologist'. This wearable computer and video camcorder system is being utilized to train and test a computer-vision system to possess some of the decision-making abilities of a field-geologist. Our platform has been primarily utilized to examine and develop algorithms and systems such as robotic image acquisition, real-time image segmentation, and determination of interesting points in image mosaics. We aimed to test the entire system instead of any individual part. However, the innovative aspect of the system is the uncommon map that aids in identifying interest-points without any context. The hardware and software systems functioned without any issues, and the computer-vision algorithms were adequate for the initial field tests. We also identified some areas of improvement such as managing structural shadows, microtexture, and controlling the camera's zoom lens. Nonetheless, the Cyborg Astrobiologist system demonstrated its ability to identify interesting points in real-time and gather more information in an automated manner, which can aid in autonomous guidance towards geological points-of-interest.",1
"Pattern recognition is generally assumed as an interaction of two inversely directed image-processing streams: the bottom-up information details gathering and localization (segmentation) stream, and the top-down information features aggregation, association and interpretation (recognition) stream. Inspired by recent evidence from biological vision research and by the insights of Kolmogorov Complexity theory, we propose a new, just top-down evolving, procedure of initial image segmentation. We claim that traditional top-down cognitive reasoning, which is supposed to guide the segmentation process to its final result, is not at all a part of the image information content evaluation. And that initial image segmentation is certainly an unsupervised process. We present some illustrative examples, which support our claims.",0
"The process of pattern recognition is commonly believed to involve two streams of image processing that work in opposite directions. The first stream, known as the bottom-up stream, focuses on collecting details and localizing them through segmentation. The second stream, called the top-down stream, involves aggregating, associating, and interpreting features to recognize patterns. Drawing on recent research in biological vision and Kolmogorov Complexity theory, we propose a new method of initial image segmentation that relies solely on top-down processing. Our assertion is that the traditional top-down cognitive reasoning that guides segmentation is not involved in evaluating image information content, and that the process is unsupervised. We provide examples to support our argument.",1
"We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist and field-astrobiologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. The hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: first, detection and accounting for shadows caused by 3D jagged edges in the outcrop; second, reincorporation of more sophisticated texture-analysis algorithms into the system; third, creation of hardware and software capabilities to control the camera's zoom lens in an intelligent manner; and fourth, development of algorithms for interpretation of complex geological scenery. Nonetheless, despite these technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner.",0
"Our study details the initial geological field tests of the `Cyborg Astrobiologist'. This wearable computer and video camcorder system aims to train a computer-vision system to possess some of the independent decision-making abilities of a field-geologist and field-astrobiologist. To date, the Cyborg Astrobiologist has been utilized to test and refine robotic image acquisition, real-time image segmentation, and identification of interesting points within image mosaics. The hardware and software components operate dependably, and the computer-vision algorithms are satisfactory for the preliminary field tests. Furthermore, the field tests highlight areas for future improvement, including detection and accounting for 3D jagged edge shadows, incorporation of more advanced texture-analysis algorithms, intelligent control of the camera's zoom lens, and interpretation of intricate geological scenes. Despite these technical limitations, the Cyborg Astrobiologist system, comprising a camera-equipped wearable-computer and its computer-vision algorithms, displays the ability to identify genuinely interesting points in geological scenes and collect further information on these points in an automated manner.",1
"We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. The hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: dealing with structural shadow and microtexture, and also, controlling the camera's zoom lens in an intelligent manner. Nonetheless, despite these and other technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner.",0
"We have conducted the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system. Our aim is to test and train a computer-vision system to have some of the autonomous decision-making capabilities of a field-geologist. The Cyborg Astrobiologist platform has been used for testing and developing algorithms and systems, including robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. The hardware and software systems are reliable, and the computer-vision algorithms are adequate for the first field tests. The field tests have identified issues for improvement, such as dealing with structural shadow and microtexture, and controlling the camera's zoom lens intelligently. Nonetheless, despite technical inadequacies, the Cyborg Astrobiologist system has demonstrated its ability to find genuinely interesting points in real-time in the geological scenery and gather more information about them in an automated manner. The field tests serve as a proof-of-concept and highlight areas for future development.",1
"Coordinate-based Multilayer Perceptron (MLP) networks, despite being capable of learning neural implicit representations, are not performant for internal image synthesis applications. Convolutional Neural Networks (CNNs) are typically used instead for a variety of internal generative tasks, at the cost of a larger model. We propose Neural Knitwork, an architecture for neural implicit representation learning of natural images that achieves image synthesis by optimizing the distribution of image patches in an adversarial manner and by enforcing consistency between the patch predictions. To the best of our knowledge, this is the first implementation of a coordinate-based MLP tailored for synthesis tasks such as image inpainting, super-resolution, and denoising. We demonstrate the utility of the proposed technique by training on these three tasks. The results show that modeling natural images using patches, rather than pixels, produces results of higher fidelity. The resulting model requires 80% fewer parameters than alternative CNN-based solutions while achieving comparable performance and training time.",0
"Although Coordinate-based Multilayer Perceptron (MLP) networks can learn neural implicit representations, they are not well-suited for internal image synthesis applications. Convolutional Neural Networks (CNNs) are more commonly used for various internal generative tasks, but they require a larger model. To address this issue, we introduce Neural Knitwork, an architecture that employs an adversarial approach to optimize the distribution of image patches and enforce consistency between patch predictions for natural image synthesis. Our implementation of a coordinate-based MLP is specifically designed for synthesis tasks such as image inpainting, super-resolution, and denoising. By training on these three tasks, we demonstrate the effectiveness of our technique, which produces higher-quality results than pixel-based models. Additionally, our approach requires 80% less parameters than CNN-based solutions while achieving similar performance and training time.",1
"Graph-level representation learning is the pivotal step for downstream tasks that operate on the whole graph. The most common approach to this problem heretofore is graph pooling, where node features are typically averaged or summed to obtain the graph representations. However, pooling operations like averaging or summing inevitably cause massive information missing, which may severely downgrade the final performance. In this paper, we argue what is crucial to graph-level downstream tasks includes not only the topological structure but also the distribution from which nodes are sampled. Therefore, powered by existing Graph Neural Networks (GNN), we propose a new plug-and-play pooling module, termed as Distribution Knowledge Embedding (DKEPool), where graphs are rephrased as distributions on top of GNNs and the pooling goal is to summarize the entire distribution information instead of retaining a certain feature vector by simple predefined pooling operations. A DKEPool network de facto disassembles representation learning into two stages, structure learning and distribution learning. Structure learning follows a recursive neighborhood aggregation scheme to update node features where structure information is obtained. Distribution learning, on the other hand, omits node interconnections and focuses more on the distribution depicted by all the nodes. Extensive experiments demonstrate that the proposed DKEPool significantly and consistently outperforms the state-of-the-art methods.",0
"The key step for downstream tasks that operate on the entire graph is graph-level representation learning. The traditional approach to this has been graph pooling, where node features are averaged or summed to create graph representations. However, this method often results in significant information loss, which can negatively impact performance. This paper argues that for graph-level downstream tasks, it is essential to consider not only the topological structure but also the distribution from which nodes are sampled. To address this, the authors propose a new pooling module called Distribution Knowledge Embedding (DKEPool), which rephrases graphs as distributions on top of Graph Neural Networks (GNN). DKEPool summarizes the entire distribution information instead of retaining a certain feature vector by predefined pooling operations. The DKEPool network divides representation learning into two stages: structure learning and distribution learning. Structure learning updates node features following a recursive neighborhood aggregation scheme to obtain structure information, while distribution learning focuses on the distribution depicted by all the nodes. Extensive experiments show that DKEPool outperforms state-of-the-art methods consistently.",1
"We propose flexgrid2vec, a novel approach for image representation learning. Existing visual representation methods suffer from several issues, including the need for highly intensive computation, the risk of losing in-depth structural information and the specificity of the method to certain shapes or objects. flexgrid2vec converts an image to a low-dimensional feature vector. We represent each image with a graph of flexible, unique node locations and edge distances. flexgrid2vec is a multi-channel GCN that learns features of the most representative image patches. We have investigated both spectral and non-spectral implementations of the GCN node-embedding. Specifically, we have implemented flexgrid2vec based on different node-aggregation methods, such as vector summation, concatenation and normalisation with eigenvector centrality. We compare the performance of flexgrid2vec with a set of state-of-the-art visual representation learning models on binary and multi-class image classification tasks. Although we utilise imbalanced, low-size and low-resolution datasets, flexgrid2vec shows stable and outstanding results against well-known base classifiers. flexgrid2vec achieves 96.23% on CIFAR-10, 83.05% on CIFAR-100, 94.50% on STL-10, 98.8% on ASIRRA and 89.69% on the COCO dataset.",0
"Our proposed method for learning image representation is called flexgrid2vec. The current visual representation techniques have limitations such as high computational requirements, loss of structural information, and the inability to adapt to different shapes or objects. To overcome these issues, we use a graph-based approach to convert each image into a low-dimensional feature vector. The graph consists of nodes with unique locations and edge distances, which allows for flexibility in representation. Our multi-channel GCN learns features from the most representative image patches, through various node-aggregation methods such as vector summation, concatenation and eigenvector centrality. We evaluate the performance of flexgrid2vec against state-of-the-art visual representation models on binary and multi-class image classification tasks, using small, low-resolution and unbalanced datasets. Despite the challenging dataset characteristics, flexgrid2vec outperforms well-known classifiers with stable and impressive results. For instance, on CIFAR-10, CIFAR-100, STL-10, ASIRRA, and COCO datasets, flexgrid2vec achieves accuracies of 96.23%, 83.05%, 94.50%, 98.8%, and 89.69%, respectively.",1
"The microstructure is an essential part of materials, storing the genes of materials and having a decisive influence on materials' physical and chemical properties. The material genetic engineering program aims to establish the relationship between material composition/process, organization, and performance to realize the reverse design of materials, thereby accelerating the research and development of new materials. However, tissue analysis methods of materials science, such as metallographic analysis, XRD analysis, and EBSD analysis, cannot directly establish a complete quantitative relationship between tissue structure and performance. Therefore, this paper proposes a novel data-knowledge-driven organization representation and performance prediction method to obtain a quantitative structure-performance relationship. First, a knowledge graph based on EBSD is constructed to describe the material's mesoscopic microstructure. Then a graph representation learning network based on graph attention is constructed, and the EBSD organizational knowledge graph is input into the network to obtain graph-level feature embedding. Finally, the graph-level feature embedding is input to a graph feature mapping network to obtain the material's mechanical properties. The experimental results show that our method is superior to traditional machine learning and machine vision methods.",0
"The microstructure of materials plays a crucial role in determining their physical and chemical properties, as it stores their genetic information. To accelerate the development of new materials, the material genetic engineering program seeks to establish a relationship between material composition, organization, and performance, leading to the reverse design of materials. However, traditional tissue analysis methods, such as metallographic analysis, XRD analysis, and EBSD analysis, fail to provide a complete quantitative relationship between tissue structure and performance. As a solution, this paper proposes a novel data-knowledge-driven organization representation and performance prediction method. First, a knowledge graph is constructed based on EBSD to describe the material's microstructure. Then, a graph representation learning network based on graph attention is built to obtain graph-level feature embedding. Finally, a graph feature mapping network is used to predict the material's mechanical properties. The experimental results indicate that our method outperforms traditional machine learning and machine vision methods.",1
"In multi-agent deep reinforcement learning, extracting sufficient and compact information of other agents is critical to attain efficient convergence and scalability of an algorithm. In canonical frameworks, distilling of such information is often done in an implicit and uninterpretable manner, or explicitly with cost functions not able to reflect the relationship between information compression and utility in representation. In this paper, we present Information-Bottleneck-based Other agents' behavior Representation learning for Multi-agent reinforcement learning (IBORM) to explicitly seek low-dimensional mapping encoder through which a compact and informative representation relevant to other agents' behaviors is established. IBORM leverages the information bottleneck principle to compress observation information, while retaining sufficient information relevant to other agents' behaviors used for cooperation decision. Empirical results have demonstrated that IBORM delivers the fastest convergence rate and the best performance of the learned policies, as compared with implicit behavior representation learning and explicit behavior representation learning without explicitly considering information compression and utility.",0
"The ability to extract concise and informative information about other agents is crucial for achieving efficient convergence and scalability in multi-agent deep reinforcement learning. However, traditional frameworks often distill this information in an unclear and implicit manner, or use cost functions that fail to accurately reflect the relationship between information compression and representation utility. To address this issue, we introduce the Information-Bottleneck-based Other agents' behavior Representation learning for Multi-agent reinforcement learning (IBORM) approach. IBORM seeks a low-dimensional mapping encoder that establishes a compact yet informative representation of other agents' behaviors. By leveraging the information bottleneck principle, IBORM compresses observation information while retaining sufficient data relevant to cooperation decision-making. Empirical results demonstrate that IBORM outperforms both implicit and explicit behavior representation learning methods that do not explicitly consider information compression and utility, achieving the fastest convergence rate and best performance of learned policies.",1
"Not having access to compact and meaningful representations is known to significantly increase the complexity of reinforcement learning (RL). For this reason, it can be useful to perform state representation learning (SRL) before tackling RL tasks. However, obtaining a good state representation can only be done if a large diversity of transitions is observed, which can require a difficult exploration, especially if the environment is initially reward-free. To solve the problems of exploration and SRL in parallel, we propose a new approach called XSRL (eXploratory State Representation Learning). On one hand, it jointly learns compact state representations and a state transition estimator which is used to remove unexploitable information from the representations. On the other hand, it continuously trains an inverse model, and adds to the prediction error of this model a $k$-step learning progress bonus to form the maximization objective of a discovery policy. This results in a policy that seeks complex transitions from which the trained models can effectively learn. Our experimental results show that the approach leads to efficient exploration in challenging environments with image observations, and to state representations that significantly accelerate learning in RL tasks.",0
"Reinforcement learning (RL) can become more complex without access to concise and meaningful representations. To address this, state representation learning (SRL) can be performed before tackling RL tasks. However, obtaining a good state representation requires diverse transitions, which can be a challenge, especially in initially reward-free environments. To solve the problems of exploration and SRL simultaneously, a new approach called XSRL (eXploratory State Representation Learning) is proposed. XSRL learns compact state representations and a state transition estimator while removing unexploitable information. It also trains an inverse model and adds a $k$-step learning progress bonus to form the maximization objective of a discovery policy, resulting in a policy that seeks complex transitions. Experimental results show XSRL's effectiveness in exploring challenging environments with image observations and accelerating learning in RL tasks.",1
"Vision-based reinforcement learning (RL) is a promising technique to solve control tasks involving images as the main observation. State-of-the-art RL algorithms still struggle in terms of sample efficiency, especially when using image observations. This has led to an increased attention on integrating state representation learning (SRL) techniques into the RL pipeline. Work in this field demonstrates a substantial improvement in sample efficiency among other benefits. However, to take full advantage of this paradigm, the quality of samples used for training plays a crucial role. More importantly, the diversity of these samples could affect the sample efficiency of vision-based RL, but also its generalization capability. In this work, we present an approach to improve the sample diversity. Our method enhances the exploration capability of the RL algorithms by taking advantage of the SRL setup. Our experiments show that the presented approach outperforms the baseline for all tested environments. These results are most apparent for environments where the baseline method struggles. Even in simple environments, our method stabilizes the training, reduces the reward variance and boosts sample efficiency.",0
"The use of vision-based reinforcement learning (RL) is a promising solution for control tasks that heavily rely on image observations. Despite the advances in RL algorithms, they still face challenges in terms of sample efficiency, especially when using image observations. Thus, researchers are focusing on integrating state representation learning (SRL) techniques into the RL pipeline to improve sample efficiency and other benefits. However, the quality and diversity of samples used for training are crucial factors that can impact the sample efficiency and generalization capability of vision-based RL. To address this issue, we propose an approach that leverages the SRL setup to enhance the exploration capability of RL algorithms and improve sample diversity. Our experiments demonstrate that our approach outperforms the baseline for all tested environments, particularly in environments where the baseline method struggles. Even in simple environments, our method stabilizes training, reduces reward variance, and boosts sample efficiency.",1
"Heterogeneous multi-typed, multimodal relational data is increasingly available in many domains and their exploratory analysis poses several challenges. We advance the state-of-the-art in neural unsupervised learning to analyze such data. We design the first neural method for collective matrix tri-factorization of arbitrary collections of matrices to perform spectral clustering of all constituent entities and learn cluster associations. Experiments on benchmark datasets demonstrate its efficacy over previous non-neural approaches. Leveraging signals from multi-way clustering and collective matrix completion we design a unique technique, called Discordance Analysis, to reveal information discrepancies across subsets of matrices in a collection with respect to two entities. We illustrate its utility in quality assessment of knowledge bases and in improving representation learning.",0
"The prevalence of heterogeneous multi-typed, multimodal relational data in various domains presents a range of challenges for exploratory analysis. To address this issue, we have developed a novel approach to neural unsupervised learning that facilitates the analysis of such data. Our method involves the use of collective matrix tri-factorization to perform spectral clustering of all constituent entities and learn cluster associations. This approach outperforms previous non-neural methods, as demonstrated through experiments on benchmark datasets. Additionally, we have introduced a new technique known as Discordance Analysis, which leverages signals from multi-way clustering and collective matrix completion to reveal discrepancies across subsets of matrices in a collection with respect to two entities. This technique is useful for quality assessment of knowledge bases and improving representation learning.",1
"This paper addresses a fundamental question: how good are our current self-supervised visual representation learning algorithms relative to humans? More concretely, how much ""human-like"", natural visual experience would these algorithms need in order to reach human-level performance in a complex, realistic visual object recognition task such as ImageNet? Using a scaling experiment, here we estimate that the answer is on the order of a million years of natural visual experience, in other words several orders of magnitude longer than a human lifetime. However, this estimate is quite sensitive to some underlying assumptions, underscoring the need to run carefully controlled human experiments. We discuss the main caveats surrounding our estimate and the implications of this rather surprising result.",0
"The main focus of this paper is to determine how effective our current self-supervised visual representation learning algorithms are in comparison to humans. Specifically, the paper seeks to determine how much natural visual experience these algorithms require to achieve human-level performance in a complex and realistic visual object recognition task like ImageNet. Through a scaling experiment, the paper concludes that these algorithms would require approximately a million years of natural visual experience, which is significantly longer than a human's lifespan. However, the accuracy of this estimate is subject to certain assumptions, highlighting the need for carefully controlled human experiments. The paper explores the limitations of the estimate and the surprising implications of the findings.",1
"This paper proposes a few-shot method based on Faster R-CNN and representation learning for object detection in aerial images. The two classification branches of Faster R-CNN are replaced by prototypical networks for online adaptation to new classes. These networks produce embeddings vectors for each generated box, which are then compared with class prototypes. The distance between an embedding and a prototype determines the corresponding classification score. The resulting networks are trained in an episodic manner. A new detection task is randomly sampled at each epoch, consisting in detecting only a subset of the classes annotated in the dataset. This training strategy encourages the network to adapt to new classes as it would at test time. In addition, several ideas are explored to improve the proposed method such as a hard negative examples mining strategy and self-supervised clustering for background objects. The performance of our method is assessed on DOTA, a large-scale remote sensing images dataset. The experiments conducted provide a broader understanding of the capabilities of representation learning. It highlights in particular some intrinsic weaknesses for the few-shot object detection task. Finally, some suggestions and perspectives are formulated according to these insights.",0
"In this paper, a novel few-shot approach for object detection in aerial images is presented, which utilizes Faster R-CNN and representation learning. The traditional classification branches of Faster R-CNN are replaced with prototypical networks to enable online adaptation to new classes. These networks generate embedding vectors for each box, which are compared to class prototypes to determine the classification score. The networks are trained episodically with a new detection task randomly sampled at each epoch, focusing on a subset of annotated classes to encourage adaptation to new classes. The paper further explores various strategies to enhance the proposed approach, including hard negative examples mining and self-supervised clustering for background objects. The method's performance is evaluated on DOTA, a remote sensing images dataset, revealing the limitations of representation learning for few-shot object detection. Lastly, the paper concludes with insights, suggestions, and perspectives for future research.",1
"With the remarkable success of representation learning for prediction problems, we have witnessed a rapid expansion of the use of machine learning and deep learning for the analysis of digital pathology and biopsy image patches. However, learning over patch-wise features using convolutional neural networks limits the ability of the model to capture global contextual information and comprehensively model tissue composition. The phenotypical and topological distribution of constituent histological entities play a critical role in tissue diagnosis. As such, graph data representations and deep learning have attracted significant attention for encoding tissue representations, and capturing intra- and inter- entity level interactions. In this review, we provide a conceptual grounding for graph analytics in digital pathology, including entity-graph construction and graph architectures, and present their current success for tumor localization and classification, tumor invasion and staging, image retrieval, and survival prediction. We provide an overview of these methods in a systematic manner organized by the graph representation of the input image, scale, and organ on which they operate. We also outline the limitations of existing techniques, and suggest potential future research directions in this domain.",0
"The use of machine learning and deep learning for the analysis of digital pathology and biopsy image patches has rapidly expanded due to the success of representation learning for prediction problems. However, limitations arise when learning over patch-wise features using convolutional neural networks, as the model's ability to capture global contextual information and comprehensively model tissue composition is restricted. It is crucial to consider the phenotypical and topological distribution of constituent histological entities in tissue diagnosis. Graph data representations and deep learning have gained attention in encoding tissue representations and capturing intra- and inter-entity level interactions. This review provides a conceptual grounding for graph analytics in digital pathology, including entity-graph construction and graph architectures, and highlights their success for tumor localization and classification, tumor invasion and staging, image retrieval, and survival prediction. The methods are systematically organized by the graph representation of the input image, scale, and organ on which they operate. The review also outlines the limitations of existing techniques and suggests potential future research directions in this domain.",1
"Unsupervised disentangled representation learning is a long-standing problem in computer vision. This work proposes a novel framework for performing image clustering from deep embeddings by combining instance-level contrastive learning with a deep embedding based cluster center predictor. Our approach jointly learns representations and predicts cluster centers in an end-to-end manner. This is accomplished via a three-pronged approach that combines a clustering loss, an instance-wise contrastive loss, and an anchor loss. Our fundamental intuition is that using an ensemble loss that incorporates instance-level features and a clustering procedure focusing on semantic similarity reinforces learning better representations in the latent space. We observe that our method performs exceptionally well on popular vision datasets when evaluated using standard clustering metrics such as Normalized Mutual Information (NMI), in addition to producing geometrically well-separated cluster embeddings as defined by the Euclidean distance. Our framework performs on par with widely accepted clustering methods and outperforms the state-of-the-art contrastive learning method on the CIFAR-10 dataset with an NMI score of 0.772, a 7-8% improvement on the strong baseline.",0
"The problem of unsupervised disentangled representation learning has been a persistent challenge in the field of computer vision. In this study, we introduce an innovative framework that utilizes instance-level contrastive learning and a deep embedding based cluster center predictor to perform image clustering from deep embeddings. By jointly learning representations and predicting cluster centers in an end-to-end manner, our approach employs a three-pronged strategy that incorporates a clustering loss, an instance-wise contrastive loss, and an anchor loss. Our key insight is that by combining instance-level features and a clustering procedure that focuses on semantic similarity, we can reinforce the learning of better representations in the latent space. Our method performs exceptionally well on widely used vision datasets, as evidenced by standard clustering metrics such as Normalized Mutual Information (NMI). Furthermore, our framework produces cluster embeddings that are geometrically well-separated according to Euclidean distance. We find that our method is comparable to established clustering techniques and outperforms the state-of-the-art contrastive learning method on the CIFAR-10 dataset, improving the NMI score by 7-8% over the strong baseline.",1
"Robots could learn their own state and world representation from perception and experience without supervision. This desirable goal is the main focus of our field of interest, state representation learning (SRL). Indeed, a compact representation of such a state is beneficial to help robots grasp onto their environment for interacting. The properties of this representation have a strong impact on the adaptive capability of the agent. In this article we present an approach based on imitation learning. The idea is to train several policies that share the same representation to reproduce various demonstrations. To do so, we use a multi-head neural network with a shared state representation feeding a task-specific agent. If the demonstrations are diverse, the trained representation will eventually contain the information necessary for all tasks, while discarding irrelevant information. As such, it will potentially become a compact state representation useful for new tasks. We call this approach SRLfD (State Representation Learning from Demonstration). Our experiments confirm that when a controller takes SRLfD-based representations as input, it can achieve better performance than with other representation strategies and promote more efficient reinforcement learning (RL) than with an end-to-end RL strategy.",0
"The field of state representation learning (SRL) aims to enable robots to learn their own state and world representation through perception and experience without supervision. The focus of SRL is to create a compact representation of the state that can help robots interact with their environment. This representation has a significant impact on the adaptive capability of the robot. In this article, we introduce an approach based on imitation learning, where several policies share the same representation to reproduce various demonstrations. We use a multi-head neural network with a shared state representation feeding a task-specific agent. If the demonstrations are diverse, the trained representation will eventually contain the information necessary for all tasks while discarding irrelevant information, potentially becoming a useful compact state representation for new tasks. This approach is called SRLfD (State Representation Learning from Demonstration). Our experiments show that when a controller takes SRLfD-based representations as input, it achieves better performance than with other representation strategies and promotes more efficient reinforcement learning (RL) than with an end-to-end RL strategy.",1
"In cooperative multi-agent reinforcement learning (MARL), where agents only have access to partial observations, efficiently leveraging local information is critical. During long-time observations, agents can build \textit{awareness} for teammates to alleviate the problem of partial observability. However, previous MARL methods usually neglect this kind of utilization of local information. To address this problem, we propose a novel framework, multi-agent \textit{Local INformation Decomposition for Awareness of teammates} (LINDA), with which agents learn to decompose local information and build awareness for each teammate. We model the awareness as stochastic random variables and perform representation learning to ensure the informativeness of awareness representations by maximizing the mutual information between awareness and the actual trajectory of the corresponding agent. LINDA is agnostic to specific algorithms and can be flexibly integrated to different MARL methods. Sufficient experiments show that the proposed framework learns informative awareness from local partial observations for better collaboration and significantly improves the learning performance, especially on challenging tasks.",0
"Efficiently utilizing local information is crucial in cooperative multi-agent reinforcement learning (MARL) as agents have access to only partial observations. Building awareness for teammates during long-time observations can alleviate the problem of partial observability. However, previous MARL approaches have neglected this aspect of utilizing local information. To tackle this issue, we propose a novel framework called multi-agent Local Information Decomposition for Awareness of teammates (LINDA), where agents learn to decompose local information and build awareness for each teammate. We model awareness as stochastic random variables and perform representation learning to ensure informative awareness representations by maximizing the mutual information between awareness and the actual trajectory of the corresponding agent. LINDA is algorithm-agnostic and can be flexibly integrated with different MARL methods. Extensive experiments demonstrate that the proposed framework learns informative awareness from local partial observations, leading to better collaboration and significantly improved learning performance, particularly on challenging tasks.",1
"This paper introduces a novel self-supervised method that leverages incoherence detection for video representation learning. It roots from the observation that visual systems of human beings can easily identify video incoherence based on their comprehensive understanding of videos. Specifically, the training sample, denoted as the incoherent clip, is constructed by multiple sub-clips hierarchically sampled from the same raw video with various lengths of incoherence between each other. The network is trained to learn high-level representation by predicting the location and length of incoherence given the incoherent clip as input. Additionally, intra-video contrastive learning is introduced to maximize the mutual information between incoherent clips from the same raw video. We evaluate our proposed method through extensive experiments on action recognition and video retrieval utilizing various backbone networks. Experiments show that our proposed method achieves state-of-the-art performance across different backbone networks and different datasets compared with previous coherence-based methods.",0
"A new method for self-supervised video representation learning is presented in this paper, which utilizes incoherence detection. The method is based on the observation that humans can easily recognize incoherent video due to their deep understanding of the medium. To create the training sample, called the incoherent clip, multiple sub-clips are hierarchically sampled from the same raw video with varying lengths of incoherence. The network is trained to identify the location and length of incoherence in the incoherent clip. Intra-video contrastive learning is also introduced to maximize the mutual information between incoherent clips from the same raw video. Our proposed method is evaluated in action recognition and video retrieval experiments using various backbone networks. The results show that our method outperforms previous coherence-based methods across different datasets and backbone networks.",1
"The micro-segmentation of customers in the finance sector is a non-trivial task and has been an atypical omission from recent scientific literature. Where traditional segmentation classifies customers based on coarse features such as demographics, micro-segmentation depicts more nuanced differences between individuals, bringing forth several advantages including the potential for improved personalization in financial services. AI and representation learning offer a unique opportunity to solve the problem of micro-segmentation. Although ubiquitous in many industries, the proliferation of AI in sensitive industries such as finance has become contingent on the imperatives of responsible AI. We had previously solved the micro-segmentation problem by extracting temporal features from the state space of a recurrent neural network (RNN). However, due to the inherent opacity of RNNs our solution lacked an explanation - one of the imperatives of responsible AI. In this study, we address this issue by extracting an explanation for and providing an interpretation of our temporal features. We investigate the state space of our RNN and through a linear regression model reconstruct the trajectories in the state space with high fidelity. We show that our linear regression coefficients have not only learned the rules used to create the RNN's output data but have also learned the relationships that were not directly evident in the raw data.",0
"Micro-segmenting customers in the finance sector is a complex task that has not been well-documented in recent scientific literature. Unlike traditional segmentation methods that rely on broad demographic characteristics, micro-segmentation takes into account more subtle differences between individuals, allowing for improved personalization in financial services. AI and representation learning present a promising solution to this problem, but their implementation in sensitive industries like finance requires responsible use. In a previous study, we used a recurrent neural network (RNN) to extract temporal features for micro-segmentation but lacked an explanation for our results. In this study, we provide an interpretation of our temporal features by analyzing the state space of our RNN and using a linear regression model to reconstruct trajectories with high accuracy. Our regression coefficients not only learned the rules used to create the RNN's output data but also discovered relationships not immediately apparent in the raw data.",1
"Graph neural networks have emerged as a powerful model for graph representation learning to undertake graph-level prediction tasks. Various graph pooling methods have been developed to coarsen an input graph into a succinct graph-level representation through aggregating node embeddings obtained via graph convolution. However, most graph pooling methods are heavily node-centric and are unable to fully leverage the crucial information contained in global graph structure. This paper presents a cross-view graph pooling (Co-Pooling) method to better exploit crucial graph structure information. The proposed Co-Pooling fuses pooled representations learnt from both node view and edge view. Through cross-view interaction, edge-view pooling and node-view pooling seamlessly reinforce each other to learn more informative graph-level representations. Co-Pooling has the advantage of handling various graphs with different types of node attributes. Extensive experiments on a total of 15 graph benchmark datasets validate the effectiveness of our proposed method, demonstrating its superior performance over state-of-the-art pooling methods on both graph classification and graph regression tasks.",0
"Graph neural networks are a powerful tool for graph representation learning and can be used for graph-level prediction tasks. Many graph pooling methods have been developed to create a concise graph-level representation by combining node embeddings obtained through graph convolution. However, most of these methods focus heavily on nodes and do not use the important information contained in the overall graph structure. This study introduces a new approach, called cross-view graph pooling (Co-Pooling), which combines node and edge views to better utilize the graph structure information. Through cross-view interaction, Co-Pooling enhances edge-view pooling and node-view pooling, resulting in more informative graph-level representations. Co-Pooling can also handle various graphs with different node attributes. The proposed method is tested on 15 benchmark datasets, demonstrating superior performance over state-of-the-art pooling methods for both graph classification and regression tasks.",1
"Inspired by the success of BERT, several multimodal representation learning approaches have been proposed that jointly represent image and text. These approaches achieve superior performance by capturing high-level semantic information from large-scale multimodal pretraining. In particular, LXMERT and UNITER adopt visual region feature regression and label classification as pretext tasks. However, they tend to suffer from the problems of noisy labels and sparse semantic annotations, based on the visual features having been pretrained on a crowdsourced dataset with limited and inconsistent semantic labeling. To overcome these issues, we propose unbiased Dense Contrastive Visual-Linguistic Pretraining (DCVLP), which replaces the region regression and classification with cross-modality region contrastive learning that requires no annotations. Two data augmentation strategies (Mask Perturbation and Intra-/Inter-Adversarial Perturbation) are developed to improve the quality of negative samples used in contrastive learning. Overall, DCVLP allows cross-modality dense region contrastive learning in a self-supervised setting independent of any object annotations. We compare our method against prior visual-linguistic pretraining frameworks to validate the superiority of dense contrastive learning on multimodal representation learning.",0
"Several multimodal representation learning approaches have been introduced, inspired by the triumph of BERT, that jointly represent text and image. These approaches attain exceptional performance by capturing high-level semantic information through large-scale multimodal pretraining. LXMERT and UNITER utilize visual region feature regression and label classification as pretext tasks, but they encounter problems with noisy labels and sparse semantic annotations. This is due to the visual features being pretrained on a crowdsourced dataset with limited and inconsistent semantic labeling. To solve these issues, we propose an unbiased Dense Contrastive Visual-Linguistic Pretraining (DCVLP). DCVLP replaces region regression and classification with cross-modality region contrastive learning that does not require annotations. We develop two data augmentation strategies (Mask Perturbation and Intra-/Inter-Adversarial Perturbation) to enhance the quality of negative samples used in contrastive learning. DCVLP enables cross-modality dense region contrastive learning in a self-supervised setting independent of any object annotations. We compare our approach to prior visual-linguistic pretraining frameworks to demonstrate the superiority of dense contrastive learning on multimodal representation learning.",1
"To mitigate the radiologist's workload, computer-aided diagnosis with the capability to review and analyze medical images is gradually deployed. Deep learning-based region of interest segmentation is among the most exciting use cases. However, this paradigm is restricted in real-world clinical applications due to poor robustness and generalization. The issue is more sinister with a lack of training data. In this paper, we address the challenge from the representation learning point of view. We investigate that the collapsed representations, as one of the main reasons which caused poor robustness and generalization, could be avoided through transfer learning. Therefore, we propose a novel two-stage framework for robust generalized segmentation. In particular, an unsupervised Tile-wise AutoEncoder (T-AE) pretraining architecture is coined to learn meaningful representation for improving the generalization and robustness of the downstream tasks. Furthermore, the learned knowledge is transferred to the segmentation benchmark. Coupled with an image reconstruction network, the representation keeps to be decoded, encouraging the model to capture more semantic features. Experiments of lung segmentation on multi chest X-ray datasets are conducted. Empirically, the related experimental results demonstrate the superior generalization capability of the proposed framework on unseen domains in terms of high performance and robustness to corruption, especially under the scenario of the limited training data.",0
"In order to decrease the workload of radiologists, computer-aided diagnosis technology is being implemented gradually. One of the most exciting applications is deep learning-based region of interest segmentation. However, this approach has limitations in real-world clinical settings due to poor robustness and generalization, compounded by a lack of training data. This paper proposes a new approach to address these challenges by using transfer learning to avoid collapsed representations. A two-stage framework is introduced, including an unsupervised Tile-wise AutoEncoder pretraining architecture to improve generalization and robustness, and an image reconstruction network to capture more semantic features. Experiments on multi chest X-ray datasets demonstrate the superior generalization capability of the proposed framework, especially under limited training data.",1
"Recently, supervised methods, which often require substantial amounts of class labels, have achieved promising results for EEG representation learning. However, labeling EEG data is a challenging task. More recently, holistic semi-supervised learning approaches, which only require few output labels, have shown promising results in the field of computer vision. These methods, however, have not yet been adapted for EEG learning. In this paper, we adapt three state-of-the-art holistic semi-supervised approaches, namely MixMatch, FixMatch, and AdaMatch, as well as five classical semi-supervised methods for EEG learning. We perform rigorous experiments with all 8 methods on two public EEG-based emotion recognition datasets, namely SEED and SEED-IV. The experiments with different amounts of limited labeled samples show that the holistic approaches achieve strong results even when only 1 labeled sample is used per class. Further experiments show that in most cases, AdaMatch is the most effective method, followed by MixMatch and FixMatch.",0
"Recently, promising results have been achieved for EEG representation learning through supervised methods, which often require a significant number of class labels. However, labeling EEG data is a challenging task. In contrast, holistic semi-supervised learning approaches have demonstrated promising results in computer vision, requiring only a few output labels. These approaches have yet to be adapted for EEG learning. This paper explores the adaptation of three state-of-the-art holistic semi-supervised approaches, namely MixMatch, FixMatch, and AdaMatch, as well as five classical semi-supervised methods for EEG learning. Rigorous experiments were conducted on two public EEG-based emotion recognition datasets, SEED and SEED-IV, with varying amounts of limited labeled samples. The results show that the holistic approaches achieve strong results even when only 1 labeled sample is used per class. Furthermore, AdaMatch is found to be the most effective method in most cases, followed by MixMatch and FixMatch.",1
"Recently many efforts have been devoted to applying graph neural networks (GNNs) to molecular property prediction which is a fundamental task for computational drug and material discovery. One of major obstacles to hinder the successful prediction of molecule property by GNNs is the scarcity of labeled data. Though graph contrastive learning (GCL) methods have achieved extraordinary performance with insufficient labeled data, most focused on designing data augmentation schemes for general graphs. However, the fundamental property of a molecule could be altered with the augmentation method (like random perturbation) on molecular graphs. Whereas, the critical geometric information of molecules remains rarely explored under the current GNN and GCL architectures. To this end, we propose a novel graph contrastive learning method utilizing the geometry of the molecule across 2D and 3D views, which is named GeomGCL. Specifically, we first devise a dual-view geometric message passing network (GeomMPNN) to adaptively leverage the rich information of both 2D and 3D graphs of a molecule. The incorporation of geometric properties at different levels can greatly facilitate the molecular representation learning. Then a novel geometric graph contrastive scheme is designed to make both geometric views collaboratively supervise each other to improve the generalization ability of GeomMPNN. We evaluate GeomGCL on various downstream property prediction tasks via a finetune process. Experimental results on seven real-life molecular datasets demonstrate the effectiveness of our proposed GeomGCL against state-of-the-art baselines.",0
"In recent times, significant efforts have been made towards utilizing graph neural networks (GNNs) for predicting molecular properties, which is a crucial task for drug and material discovery. However, the limited availability of labeled data has hindered the success of GNNs in accurately predicting molecule properties. While graph contrastive learning (GCL) methods have shown impressive results with limited labeled data, most of them focus on designing data augmentation techniques for general graphs. Unfortunately, these augmentation methods, such as random perturbation, can alter the fundamental property of a molecule. Moreover, the critical geometric information of molecules is still underexplored by current GNN and GCL models. To address these issues, we propose a new graph contrastive learning method called GeomGCL, which utilizes the geometry of molecules across 2D and 3D views. GeomGCL employs a dual-view geometric message passing network (GeomMPNN) to leverage the rich information of both 2D and 3D graphs of a molecule. Additionally, we design a novel geometric graph contrastive scheme that collaboratively supervises both geometric views to improve the generalization ability of GeomMPNN. We evaluate GeomGCL on several downstream property prediction tasks and compare it against state-of-the-art baselines. Our experimental results on seven real-life molecular datasets demonstrate the effectiveness of the proposed GeomGCL.",1
"Recently, deep neural networks have made remarkable achievements in 3D point cloud classification. However, existing classification methods are mainly implemented on idealized point clouds and suffer heavy degradation of per-formance on non-idealized scenarios. To handle this prob-lem, a feature representation learning method, named Dual-Neighborhood Deep Fusion Network (DNDFN), is proposed to serve as an improved point cloud encoder for the task of non-idealized point cloud classification. DNDFN utilizes a trainable neighborhood learning method called TN-Learning to capture the global key neighborhood. Then, the global neighborhood is fused with the local neighbor-hood to help the network achieve more powerful reasoning ability. Besides, an Information Transfer Convolution (IT-Conv) is proposed for DNDFN to learn the edge infor-mation between point-pairs and benefits the feature transfer procedure. The transmission of information in IT-Conv is similar to the propagation of information in the graph which makes DNDFN closer to the human reasoning mode. Extensive experiments on existing benchmarks especially non-idealized datasets verify the effectiveness of DNDFN and DNDFN achieves the state of the arts.",0
"In recent times, 3D point cloud classification has witnessed impressive advancements courtesy of deep neural networks. However, the current classification techniques are primarily utilized on point clouds that are idealized and tend to suffer a significant drop in performance when applied to non-idealized scenarios. To address this challenge, a feature representation learning strategy called Dual-Neighborhood Deep Fusion Network (DNDFN) has been introduced to enhance the point cloud encoder for non-idealized point cloud classification. DNDFN incorporates a trainable neighborhood learning approach known as TN-Learning to capture the global key neighborhood. Furthermore, it fuses the global neighborhood with the local neighborhood to enhance the network's reasoning ability. Additionally, DNDFN employs an Information Transfer Convolution (IT-Conv) to learn the edge information between point-pairs and improve the feature transfer process. The propagation of information in IT-Conv is akin to the human reasoning mode, making DNDFN more relatable. The effectiveness of DNDFN has been verified through extensive experiments on existing benchmarks, especially non-idealized datasets, and has been found to outperform state-of-the-art methods.",1
"Self-supervised video representation methods typically focus on the representation of temporal attributes in videos. However, the role of stationary versus non-stationary attributes is less explored: Stationary features, which remain similar throughout the video, enable the prediction of video-level action classes. Non-stationary features, which represent temporally varying attributes, are more beneficial for downstream tasks involving more fine-grained temporal understanding, such as action segmentation. We argue that a single representation to capture both types of features is sub-optimal, and propose to decompose the representation space into stationary and non-stationary features via contrastive learning from long and short views, i.e. long video sequences and their shorter sub-sequences. Stationary features are shared between the short and long views, while non-stationary features aggregate the short views to match the corresponding long view. To empirically verify our approach, we demonstrate that our stationary features work particularly well on an action recognition downstream task, while our non-stationary features perform better on action segmentation. Furthermore, we analyse the learned representations and find that stationary features capture more temporally stable, static attributes, while non-stationary features encompass more temporally varying ones.",0
"Typically, self-supervised video representation methods focus on temporal attributes in videos. However, the role of stationary versus non-stationary attributes is not well-explored. Stationary features remain constant throughout the video, allowing for the prediction of video-level action classes. Non-stationary features, on the other hand, represent attributes that change over time and are more useful for tasks that require fine-grained temporal understanding, such as action segmentation. We believe that using a single representation to capture both types of features is not optimal. Therefore, we propose to split the representation space into stationary and non-stationary features using contrastive learning from long and short views. Stationary features are shared between the short and long views, while non-stationary features are derived from the short views and matched to the corresponding long view. Our empirical results show that our stationary features work well for action recognition, while our non-stationary features perform better for action segmentation. Furthermore, our analysis reveals that stationary features capture more static attributes, while non-stationary features encompass more dynamic attributes.",1
We propose and demonstrate a representation learning approach by maximizing the mutual information between local features of images and text. The goal of this approach is to learn useful image representations by taking advantage of the rich information contained in the free text that describes the findings in the image. Our method trains image and text encoders by encouraging the resulting representations to exhibit high local mutual information. We make use of recent advances in mutual information estimation with neural network discriminators. We argue that the sum of local mutual information is typically a lower bound on the global mutual information. Our experimental results in the downstream image classification tasks demonstrate the advantages of using local features for image-text representation learning.,0
Our proposed approach involves representation learning through maximizing the mutual information between local image features and text. The aim of this approach is to create beneficial image representations by utilizing the comprehensive information present in the accompanying text. We train image and text encoders to produce representations with high local mutual information using neural network discriminators to estimate mutual information. We believe that the sum of local mutual information is a reasonable approximation of global mutual information. Our experimental results show that utilizing local features for image-text representation learning has benefits in downstream image classification tasks.,1
"Automated characterization of spatial data is a kind of critical geographical intelligence. As an emerging technique for characterization, Spatial Representation Learning (SRL) uses deep neural networks (DNNs) to learn non-linear embedded features of spatial data for characterization. However, SRL extracts features by internal layers of DNNs, and thus suffers from lacking semantic labels. Texts of spatial entities, on the other hand, provide semantic understanding of latent feature labels, but is insensible to deep SRL models. How can we teach a SRL model to discover appropriate topic labels in texts and pair learned features with the labels? This paper formulates a new problem: feature-topic pairing, and proposes a novel Particle Swarm Optimization (PSO) based deep learning framework. Specifically, we formulate the feature-topic pairing problem into an automated alignment task between 1) a latent embedding feature space and 2) a textual semantic topic space. We decompose the alignment of the two spaces into: 1) point-wise alignment, denoting the correlation between a topic distribution and an embedding vector; 2) pair-wise alignment, denoting the consistency between a feature-feature similarity matrix and a topic-topic similarity matrix. We design a PSO based solver to simultaneously select an optimal set of topics and learn corresponding features based on the selected topics. We develop a closed loop algorithm to iterate between 1) minimizing losses of representation reconstruction and feature-topic alignment and 2) searching the best topics. Finally, we present extensive experiments to demonstrate the enhanced performance of our method.",0
"Spatial Representation Learning (SRL) is a method of characterizing spatial data through deep neural networks (DNNs). However, SRL lacks semantic labels, which can be provided by texts of spatial entities. This paper proposes a solution to this problem by formulating a new problem called feature-topic pairing and developing a Particle Swarm Optimization (PSO) based deep learning framework. The framework aligns a latent embedding feature space and a textual semantic topic space through point-wise and pair-wise alignment. The PSO based solver selects an optimal set of topics and learns corresponding features based on the selected topics. The closed loop algorithm iterates between minimizing losses and searching for the best topics. Extensive experiments demonstrate the enhanced performance of the proposed method.",1
"Score-based methods represented as stochastic differential equations on a continuous time domain have recently proven successful as a non-adversarial generative model. Training such models relies on denoising score matching, which can be seen as multi-scale denoising autoencoders. Here, we augment the denoising score-matching framework to enable representation learning without any supervised signal. GANs and VAEs learn representations by directly transforming latent codes to data samples. In contrast, the introduced diffusion based representation learning relies on a new formulation of the denoising score-matching objective and thus encodes information needed for denoising. We illustrate how this difference allows for manual control of the level of details encoded in the representation. Using the same approach, we propose to learn an infinite-dimensional latent code which achieves improvements of state-of-the-art models on semi-supervised image classification. As a side contribution, we show how adversarial training in score-based models can improve sample quality and improve sampling speed using a new approximation of the prior at smaller noise scales.",0
"Recently, stochastic differential equations on a continuous time domain, known as score-based methods, have been successful in generating models without adversarial training. These models rely on denoising score matching, which can be viewed as multi-scale denoising autoencoders. We expand on this framework to enable representation learning without supervision. Unlike GANs and VAEs, which learn representations by transforming latent codes to data samples, our diffusion-based representation learning encodes information needed for denoising. This allows for manual control of the level of details encoded in the representation. Our proposed infinite-dimensional latent code achieves state-of-the-art results on semi-supervised image classification. Additionally, we show how adversarial training can improve sample quality and speed using a new approximation of the prior at smaller noise scales.",1
"The recent success of Transformers in the language domain has motivated adapting it to a multimodal setting, where a new visual model is trained in tandem with an already pretrained language model. However, due to the excessive memory requirements from Transformers, existing work typically fixes the language model and train only the vision module, which limits its ability to learn cross-modal information in an end-to-end manner. In this work, we focus on reducing the parameters of multimodal Transformers in the context of audio-visual video representation learning. We alleviate the high memory requirement by sharing the parameters of Transformers across layers and modalities; we decompose the Transformer into modality-specific and modality-shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. We show that our approach reduces parameters of the Transformers up to 97$\%$, allowing us to train our model end-to-end from scratch. We also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers. To demonstrate our approach, we pretrain our model on 30-second clips (480 frames) from Kinetics-700 and transfer it to audio-visual classification tasks.",0
"The triumph of Transformers in language has inspired its application to a multimodal environment, in which a fresh visual model is trained alongside an already established language model. Nevertheless, due to Transformers' large memory requirements, prior work has only trained the vision module, leaving the language model fixed and limiting its ability to acquire cross-modal knowledge in an end-to-end fashion. In this study, we aim to decrease the parameters of multimodal Transformers in the area of audio-visual video representation learning. We overcome the high memory demand by sharing Transformer parameters across layers and modalities. We break down the Transformer into modality-specific and modality-shared components, enabling the model to learn the dynamics of each modality individually and collectively. Additionally, we propose a novel parameter sharing strategy founded on low-rank approximation. Our method reduces Transformer parameters by up to 97%, enabling us to train our model from scratch in an end-to-end manner. Furthermore, we propose a negative sampling technique based on instance similarity gauged on the CNN embedding space that our model learns concurrently with the Transformers. To illustrate our method, we pretrain our model on 30-second Kinetics-700 clips (480 frames) and apply it to audio-visual classification tasks.",1
"We propose a Deep Variational Clustering (DVC) framework for unsupervised representation learning and clustering of large-scale medical images. DVC simultaneously learns the multivariate Gaussian posterior through the probabilistic convolutional encoder and the likelihood distribution with the probabilistic convolutional decoder; and optimizes cluster labels assignment. Here, the learned multivariate Gaussian posterior captures the latent distribution of a large set of unlabeled images. Then, we perform unsupervised clustering on top of the variational latent space using a clustering loss. In this approach, the probabilistic decoder helps to prevent the distortion of data points in the latent space and to preserve the local structure of data generating distribution. The training process can be considered as a self-training process to refine the latent space and simultaneously optimizing cluster assignments iteratively. We evaluated our proposed framework on three public datasets that represented different medical imaging modalities. Our experimental results show that our proposed framework generalizes better across different datasets. It achieves compelling results on several medical imaging benchmarks. Thus, our approach offers potential advantages over conventional deep unsupervised learning in real-world applications. The source code of the method and all the experiments are available publicly at: https://github.com/csfarzin/DVC",0
"Our proposed framework for unsupervised representation learning and clustering of large-scale medical images is called Deep Variational Clustering (DVC). The framework simultaneously learns the multivariate Gaussian posterior through the probabilistic convolutional encoder and the likelihood distribution with the probabilistic convolutional decoder. It also optimizes cluster label assignment. The multivariate Gaussian posterior captures the latent distribution of a large set of unlabeled images, and we perform unsupervised clustering on top of the variational latent space using a clustering loss. The probabilistic decoder helps to prevent the distortion of data points in the latent space and preserve the local structure of data generating distribution. The training process is a self-training process that refines the latent space and optimizes cluster assignments iteratively. We evaluated our proposed framework on three public datasets representing different medical imaging modalities and achieved compelling results on several medical imaging benchmarks. Thus, our approach offers potential advantages over conventional deep unsupervised learning in real-world applications. The source code of the method and all the experiments are publicly available at: https://github.com/csfarzin/DVC",1
"In this work, we present a novel mask guided attention (MGA) method for fine-grained patchy image classification. The key challenge of fine-grained patchy image classification lies in two folds, ultra-fine-grained inter-category variances among objects and very few data available for training. This motivates us to consider employing more useful supervision signal to train a discriminative model within limited training samples. Specifically, the proposed MGA integrates a pre-trained semantic segmentation model that produces auxiliary supervision signal, i.e., patchy attention mask, enabling a discriminative representation learning. The patchy attention mask drives the classifier to filter out the insignificant parts of images (e.g., common features between different categories), which enhances the robustness of MGA for the fine-grained patchy image classification. We verify the effectiveness of our method on three publicly available patchy image datasets. Experimental results demonstrate that our MGA method achieves superior performance on three datasets compared with the state-of-the-art methods. In addition, our ablation study shows that MGA improves the accuracy by 2.25% and 2% on the SoyCultivarVein and BtfPIS datasets, indicating its practicality towards solving the fine-grained patchy image classification.",0
"Our work introduces a new approach, called mask guided attention (MGA), for precise classification of patchy images. Fine-grained patchy image classification poses two main challenges: inter-category variances among objects at the ultra-fine-grained level and limited training data availability. To address these challenges, we propose using more useful supervision signals to train a discriminative model. Our MGA method integrates a pre-trained semantic segmentation model to produce an auxiliary supervision signal, or patchy attention mask, which facilitates discriminative representation learning. By filtering out insignificant image parts, such as common features between different categories, the patchy attention mask enhances the robustness of MGA. We evaluate our method on three publicly available patchy image datasets and demonstrate its superiority over state-of-the-art methods. Furthermore, our ablation study shows that MGA improves accuracy by 2.25% and 2% on the SoyCultivarVein and BtfPIS datasets, respectively, indicating its practicality in solving fine-grained patchy image classification.",1
"Distances between data points are widely used in point cloud representation learning. Yet, it is no secret that under the effect of noise, these distances-and thus the models based upon them-may lose their usefulness in high dimensions. Indeed, the small marginal effects of the noise may then accumulate quickly, shifting empirical closest and furthest neighbors away from the ground truth. In this paper, we characterize such effects in high-dimensional data using an asymptotic probabilistic expression. Furthermore, while it has been previously argued that neighborhood queries become meaningless and unstable when there is a poor relative discrimination between the furthest and closest point, we conclude that this is not necessarily the case when explicitly separating the ground truth data from the noise. More specifically, we derive that under particular conditions, empirical neighborhood relations affected by noise are still likely to be true even when we observe this discrimination to be poor. We include thorough empirical verification of our results, as well as experiments that interestingly show our derived phase shift where neighbors become random or not is identical to the phase shift where common dimensionality reduction methods perform poorly or well for finding low-dimensional representations of high-dimensional data with dense noise.",0
"The use of distances between data points is common in representing point clouds, but it is well-known that in high dimensions, these distances and the models based on them may lose their effectiveness due to noise. The minor effects of noise can accumulate quickly, causing empirical closest and furthest neighbors to deviate from the true values. This paper presents an asymptotic probabilistic expression characterizing these effects in high-dimensional data. Although it has been argued that neighborhood queries are unreliable when there is poor relative discrimination between the furthest and closest point, the authors conclude that this is not necessarily the case when the ground truth data is distinguished from the noise. They show that under certain conditions, empirical neighborhood relations affected by noise are still likely to be accurate even when the discrimination is low. The authors provide empirical verification of their findings and experiments that demonstrate a phase shift where neighbors become random or not, which corresponds to the phase shift where dimensionality reduction methods perform poorly or well in finding low-dimensional representations of high-dimensional data with dense noise.",1
"Molecule representation learning (MRL) methods aim to embed molecules into a real vector space. However, existing SMILES-based (Simplified Molecular-Input Line-Entry System) or GNN-based (Graph Neural Networks) MRL methods either take SMILES strings as input that have difficulty in encoding molecule structure information, or over-emphasize the importance of GNN architectures but neglect their generalization ability. Here we propose using chemical reactions to assist learning molecule representation. The key idea of our approach is to preserve the equivalence of molecules with respect to chemical reactions in the embedding space, i.e., forcing the sum of reactant embeddings and the sum of product embeddings to be equal for each chemical equation. This constraint is proven effective to 1) keep the embedding space well-organized and 2) improve the generalization ability of molecule embeddings. Moreover, our model can use any GNN as the molecule encoder and is thus agnostic to GNN architectures. Experimental results demonstrate that our method achieves state-of-the-art performance in a variety of downstream tasks, e.g., 17.4% absolute Hit@1 gain in chemical reaction prediction, 2.3% absolute AUC gain in molecule property prediction, and 18.5% relative RMSE gain in graph-edit-distance prediction, respectively, over the best baseline method. The code is available at https://github.com/hwwang55/MolR.",0
"Molecule representation learning (MRL) methods aim to embed molecules into a real vector space. However, current MRL methods that are SMILES-based or GNN-based have limitations. SMILES-based methods have difficulty encoding molecule structure information while GNN-based methods over-emphasize the importance of GNN architectures at the expense of generalization ability. This paper proposes the use of chemical reactions to aid molecule representation learning. The proposed approach preserves the equivalence of molecules with respect to chemical reactions in the embedding space by ensuring that the sum of reactant embeddings and the sum of product embeddings are equal for each chemical equation. This constraint effectively organizes the embedding space and improves generalization ability. Additionally, any GNN can be used as the molecule encoder, making the model agnostic to GNN architectures. Experimental results show that the proposed method outperforms the best baseline method in various downstream tasks, including chemical reaction prediction, molecule property prediction, and graph-edit-distance prediction. The code is available at https://github.com/hwwang55/MolR.",1
"3D visual grounding aims at grounding a natural language description about a 3D scene, usually represented in the form of 3D point clouds, to the targeted object region. Point clouds are sparse, noisy, and contain limited semantic information compared with 2D images. These inherent limitations make the 3D visual grounding problem more challenging. In this study, we propose 2D Semantics Assisted Training (SAT) that utilizes 2D image semantics in the training stage to ease point-cloud-language joint representation learning and assist 3D visual grounding. The main idea is to learn auxiliary alignments between rich, clean 2D object representations and the corresponding objects or mentioned entities in 3D scenes. SAT takes 2D object semantics, i.e., object label, image feature, and 2D geometric feature, as the extra input in training but does not require such inputs during inference. By effectively utilizing 2D semantics in training, our approach boosts the accuracy on the Nr3D dataset from 37.7% to 49.2%, which significantly surpasses the non-SAT baseline with the identical network architecture and inference input. Our approach outperforms the state of the art by large margins on multiple 3D visual grounding datasets, i.e., +10.4% absolute accuracy on Nr3D, +9.9% on Sr3D, and +5.6% on ScanRef.",0
"The goal of 3D visual grounding is to match a natural language description of a 3D scene, typically consisting of 3D point clouds, to its designated object region. Compared to 2D images, point clouds are sparse, noisy, and have limited semantic information, which makes the 3D visual grounding task more difficult. To address this challenge, we propose the 2D Semantics Assisted Training (SAT) method, which uses 2D image semantics during the training phase to aid in joint representation learning of point-cloud-language. Our method learns auxiliary alignments between 2D object representations and their corresponding entities in 3D scenes, using object label, image feature, and 2D geometric feature as additional inputs during training. During inference, these inputs are not required. By leveraging 2D semantics during training, our approach improves the accuracy on the Nr3D dataset from 37.7% to 49.2%, surpassing the non-SAT baseline with the same network architecture and inference input. Our method also outperforms the state of the art on multiple 3D visual grounding datasets, with an increase in absolute accuracy of +10.4% on Nr3D, +9.9% on Sr3D, and +5.6% on ScanRef.",1
"Self-supervised vision-and-language pretraining (VLP) aims to learn transferable multi-modal representations from large-scale image-text data and to achieve strong performances on a broad scope of vision-language tasks after finetuning. Previous mainstream VLP approaches typically adopt a two-step strategy relying on external object detectors to encode images in a multi-modal Transformer framework, which suffer from restrictive object concept space, limited image context and inefficient computation. In this paper, we propose an object-aware end-to-end VLP framework, which directly feeds image grid features from CNNs into the Transformer and learns the multi-modal representations jointly. More importantly, we propose to perform object knowledge distillation to facilitate learning cross-modal alignment at different semantic levels. To achieve that, we design two novel pretext tasks by taking object features and their semantic labels from external detectors as supervision: 1.) Object-guided masked vision modeling task focuses on enforcing object-aware representation learning in the multi-modal Transformer; 2.) Phrase-region alignment task aims to improve cross-modal alignment by utilizing the similarities between noun phrases and object labels in the linguistic space. Extensive experiments on a wide range of vision-language tasks demonstrate the efficacy of our proposed framework, and we achieve competitive or superior performances over the existing pretraining strategies. The code is available in supplementary materials.",0
"The goal of self-supervised vision-and-language pretraining (VLP) is to develop versatile multi-modal representations using large-scale image-text data and attain strong outcomes on diverse vision-language tasks through finetuning. Prior VLP techniques utilized a two-step method that relied on external object detectors to encode images in a multi-modal Transformer framework. However, this approach had limitations such as a restricted object concept space, limited image context, and inefficient computation. Thus, we propose an object-aware end-to-end VLP framework in this study. This approach feeds image grid features directly from CNNs into the Transformer and learns multi-modal representations simultaneously. Additionally, we suggest performing object knowledge distillation to aid in learning cross-modal alignment at various semantic levels. To accomplish this, we create two new pretext tasks utilizing object features and their semantic labels from external detectors as supervision. The first task is the Object-guided Masked Vision Modeling task, which emphasizes object-aware representation learning in the multi-modal Transformer. The second task is the Phrase-Region Alignment task, which aims to improve cross-modal alignment by leveraging the similarities between noun phrases and object labels in the linguistic space. We demonstrate the efficacy of our proposed framework through extensive experiments on a wide range of vision-language tasks, achieving competitive or superior performances compared to existing pretraining strategies. The supplementary materials contain the code.",1
"Nowadays, many network representation learning algorithms and downstream network mining tasks have already paid attention to dynamic networks or temporal networks, which are more suitable for real-world complex scenarios by modeling evolving patterns and temporal dependencies between node interactions. Moreover, representing and mining temporal networks have a wide range of applications, such as fraud detection, social network analysis, and drug discovery. To contribute to the network representation learning and network mining research community, in this paper, we generate a new biological repository of dynamic protein-protein interaction network data (i.e., DPPIN), which consists of twelve dynamic network datasets describing protein-level interactions of yeast cells at different scales. We first introduce the generation process of DPPIN. To demonstrate the value of our published repository DPPIN, we then list the potential applications that would be benefited. Furthermore, we design dynamic local clustering, dynamic spectral clustering, dynamic subgraph matching, dynamic node classification, and dynamic graph classification experiments, where network datasets of DPPIN could indicate future research opportunities for some tasks by presenting challenges on state-of-the-art baseline algorithms. Finally, we identify future directions for improving the utility of this repository and welcome constructive inputs from the community. All resources of this work are deployed and publicly available at https://github.com/DongqiFu/DPPIN.",0
"In modern times, network representation learning algorithms and downstream network mining tasks have shifted their focus towards dynamic networks or temporal networks, which are better suited for complex real-world scenarios by modeling evolving patterns and temporal dependencies between node interactions. The representation and mining of temporal networks have a broad range of applications, including fraud detection, social network analysis, and drug discovery. This paper aims to contribute to the research community by creating a new biological repository of dynamic protein-protein interaction network data (DPPIN), comprising twelve dynamic network datasets that describe protein-level interactions of yeast cells at different scales. The generation process of DPPIN is discussed, and potential applications are listed to demonstrate the value of the published repository. Furthermore, dynamic local clustering, dynamic spectral clustering, dynamic subgraph matching, dynamic node classification, and dynamic graph classification experiments are designed, utilizing network datasets from DPPIN to present challenges for state-of-the-art baseline algorithms and indicate future research opportunities. The authors also identify future directions for improving the utility of this repository and welcome input from the community. All resources related to this work are publicly available on https://github.com/DongqiFu/DPPIN.",1
"Contrastive learning has been widely applied to graph representation learning, where the view generators play a vital role in generating effective contrastive samples. Most of the existing contrastive learning methods employ pre-defined view generation methods, e.g., node drop or edge perturbation, which usually cannot adapt to input data or preserve the original semantic structures well. To address this issue, we propose a novel framework named Automated Graph Contrastive Learning (AutoGCL) in this paper. Specifically, AutoGCL employs a set of learnable graph view generators orchestrated by an auto augmentation strategy, where every graph view generator learns a probability distribution of graphs conditioned by the input. While the graph view generators in AutoGCL preserve the most representative structures of the original graph in generation of every contrastive sample, the auto augmentation learns policies to introduce adequate augmentation variances in the whole contrastive learning procedure. Furthermore, AutoGCL adopts a joint training strategy to train the learnable view generators, the graph encoder, and the classifier in an end-to-end manner, resulting in topological heterogeneity yet semantic similarity in the generation of contrastive samples. Extensive experiments on semi-supervised learning, unsupervised learning, and transfer learning demonstrate the superiority of our AutoGCL framework over the state-of-the-arts in graph contrastive learning. In addition, the visualization results further confirm that the learnable view generators can deliver more compact and semantically meaningful contrastive samples compared against the existing view generation methods.",0
"The use of contrastive learning in graph representation learning is widespread, but current methods rely on pre-defined view generation methods that may not adapt well to input data or accurately maintain original semantic structures. To address this problem, we introduce a new framework called Automated Graph Contrastive Learning (AutoGCL). AutoGCL utilizes a set of learnable graph view generators that are coordinated by an auto augmentation strategy. Each generator learns a probability distribution of graphs conditioned by the input, and the auto augmentation learns policies to introduce adequate augmentation variances throughout the contrastive learning process. The view generators in AutoGCL maintain the most representative structures of the original graph while generating each contrastive sample. Additionally, AutoGCL uses a joint training strategy that trains the learnable view generators, graph encoder, and classifier in an end-to-end manner, resulting in topological heterogeneity yet semantic similarity in the generation of contrastive samples. Our experiments, including semi-supervised learning, unsupervised learning, and transfer learning, demonstrate that AutoGCL outperforms existing methods in graph contrastive learning. Furthermore, the visualization results confirm that the learnable view generators produce more concise and semantically significant contrastive samples compared to other view generation methods.",1
"Data is the key factor to drive the development of machine learning (ML) during the past decade. However, high-quality data, in particular labeled data, is often hard and expensive to collect. To leverage large-scale unlabeled data, self-supervised learning, represented by contrastive learning, is introduced. The objective of contrastive learning is to map different views derived from a training sample (e.g., through data augmentation) closer in their representation space, while different views derived from different samples more distant. In this way, a contrastive model learns to generate informative representations for data samples, which are then used to perform downstream ML tasks. Recent research has shown that machine learning models are vulnerable to various privacy attacks. However, most of the current efforts concentrate on models trained with supervised learning. Meanwhile, data samples' informative representations learned with contrastive learning may cause severe privacy risks as well.   In this paper, we perform the first privacy analysis of contrastive learning through the lens of membership inference and attribute inference. Our experimental results show that contrastive models trained on image datasets are less vulnerable to membership inference attacks but more vulnerable to attribute inference attacks compared to supervised models. The former is due to the fact that contrastive models are less prone to overfitting, while the latter is caused by contrastive models' capability of representing data samples expressively. To remedy this situation, we propose the first privacy-preserving contrastive learning mechanism, Talos, relying on adversarial training. Empirical results show that Talos can successfully mitigate attribute inference risks for contrastive models while maintaining their membership privacy and model utility.",0
"Over the past decade, the development of machine learning (ML) has been heavily influenced by data. However, collecting high-quality data, especially labeled data, can be challenging and costly. To address this issue, self-supervised learning, specifically contrastive learning, has been introduced to make use of large-scale unlabeled data. Contrastive learning aims to bring together different views of a training sample, obtained through data augmentation, in the representation space while keeping different views of different samples farther apart. This way, a contrastive model can generate informative representations of data samples that can be used for downstream ML tasks. Recent research has highlighted the vulnerability of machine learning models to privacy attacks, but most of the effort has focused on models trained with supervised learning. However, informative representations learned with contrastive learning may also pose severe privacy risks. This paper offers the first privacy analysis of contrastive learning, using membership inference and attribute inference. The results demonstrate that contrastive models trained on image datasets are less vulnerable to membership inference attacks but more vulnerable to attribute inference attacks than supervised models. The former is due to the lower overfitting risk of contrastive models, while the latter is due to their ability to represent data samples expressively. To address this issue, the authors propose Talos, the first privacy-preserving contrastive learning mechanism based on adversarial training. The experimental results show that Talos can effectively mitigate attribute inference risks while maintaining membership privacy and model utility.",1
"Vision-language pre-training has recently emerged as a promising alternative for representation learning. It shifts from the tradition of using images and discrete labels for learning a fixed set of weights, seen as visual concepts, to aligning images and raw text for two separate encoders. Such a paradigm benefits from a broader source of supervision and allows zero-shot transfer to downstream tasks since visual concepts can be diametrically generated from natural language, known as prompt. In this paper, we identify that a major challenge of deploying such models in practice is prompt engineering. This is because designing a proper prompt, especially for context words surrounding a class name, requires domain expertise and typically takes a significant amount of time for words tuning since a slight change in wording could have a huge impact on performance. Moreover, different downstream tasks require specific designs, further hampering the efficiency of deployment. To overcome this challenge, we propose a novel approach named context optimization (CoOp). The main idea is to model context in prompts using continuous representations and perform end-to-end learning from data while keeping the pre-trained parameters fixed. In this way, the design of task-relevant prompts can be fully automated. Experiments on 11 datasets show that CoOp effectively turns pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin and able to gain significant improvements when using more shots (e.g., at 16 shots the average gain is around 17% with the highest reaching over 50%). CoOp also exhibits strong robustness to distribution shift.",0
"The use of vision-language pre-training as a representation learning method has gained popularity as it allows for the alignment of images and raw text using two separate encoders instead of relying on discrete labels to learn fixed weights. This approach is advantageous as it benefits from a wider range of supervision and enables zero-shot transfer to downstream tasks. However, a major challenge in implementing these models in practice is prompt engineering. Designing an appropriate prompt, especially for context words surrounding a class name, requires domain expertise and a significant amount of time for tuning. Additionally, different downstream tasks necessitate specific prompt designs, further reducing efficiency. To tackle this challenge, we propose a new method called context optimization (CoOp) that models context in prompts using continuous representations and performs end-to-end learning from data while maintaining pre-trained parameters. This approach automates the design of task-relevant prompts. Our experiments on 11 datasets demonstrate that CoOp effectively transforms pre-trained vision-language models into data-efficient visual learners, requiring only one or two shots to outperform hand-crafted prompts and achieving significant improvements with more shots. CoOp also exhibits strong robustness to distribution shift.",1
"The prediction of future climate scenarios under anthropogenic forcing is critical to understand climate change and to assess the impact of potentially counter-acting technologies. Machine learning and hybrid techniques for this prediction rely on informative metrics that are sensitive to pertinent but often subtle influences. For atmospheric dynamics, a critical part of the climate system, the ""eyeball metric"", i.e. a visual inspection by an expert, is currently still the gold standard. However, it cannot be used as metric in machine learning systems where an algorithmic description is required. Motivated by the success of intermediate neural network activations as basis for learned metrics, e.g. in computer vision, we present a novel, self-supervised representation learning approach specifically designed for atmospheric dynamics. Our approach, called AtmoDist, trains a neural network on a simple, auxiliary task: predicting the temporal distance between elements of a shuffled sequence of atmospheric fields (e.g. the components of the wind field from a reanalysis or simulation). The task forces the network to learn important intrinsic aspects of the data as activations in its layers and from these hence a discriminative metric can be obtained. We demonstrate this by using AtmoDist to define a metric for GAN-based super resolution of vorticity and divergence. Our upscaled data matches closely the true statistics of a high resolution reference and it significantly outperform the state-of-the-art based on mean squared error. Since AtmoDist is unsupervised, only requires a temporal sequence of fields, and uses a simple auxiliary task, it can be used in a wide range of applications that aim to understand and mitigate climate change.",0
"It is crucial to predict future climate scenarios caused by human activity in order to comprehend climate change and evaluate the effectiveness of counteracting technologies. Machine learning and hybrid techniques rely on informative metrics that are sensitive to subtle but significant influences. The ""eyeball metric"" is currently the gold standard for atmospheric dynamics, but it cannot be used in machine learning systems that require an algorithmic description. We present a new approach called AtmoDist, which trains a neural network on a simple auxiliary task of predicting the temporal distance between elements of a shuffled sequence of atmospheric fields. This approach allows for the learning of intrinsic aspects of data, and a discriminative metric can be obtained. AtmoDist can be applied in a wide range of climate change mitigation applications as it is unsupervised, requires only a temporal sequence of fields, and uses a simple auxiliary task. We demonstrate its effectiveness by using AtmoDist to define a metric for GAN-based super resolution of vorticity and divergence, which outperforms the state-of-the-art based on mean squared error and closely matches the true statistics of a high-resolution reference.",1
"MLP-based architectures, which consist of a sequence of consecutive multi-layer perceptron blocks, have recently been found to reach comparable results to convolutional and transformer-based methods. However, most adopt spatial MLPs which take fixed dimension inputs, therefore making it difficult to apply them to downstream tasks, such as object detection and semantic segmentation. Moreover, single-stage designs further limit performance in other computer vision tasks and fully connected layers bear heavy computation. To tackle these problems, we propose ConvMLP: a hierarchical Convolutional MLP for visual recognition, which is a light-weight, stage-wise, co-design of convolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8% top-1 accuracy on ImageNet-1k with 9M parameters and 2.4G MACs (15% and 19% of MLP-Mixer-B/16, respectively). Experiments on object detection and semantic segmentation further show that visual representation learned by ConvMLP can be seamlessly transferred and achieve competitive results with fewer parameters. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Convolutional-MLPs.",0
"Recently, MLP-based architectures have been shown to produce comparable outcomes to convolutional and transformer-based methods. However, many of these architectures employ spatial MLPs that only accept fixed-dimension inputs, making them challenging to implement in downstream tasks like object detection and semantic segmentation. Additionally, single-stage designs hinder performance in other computer vision tasks, and fully connected layers require significant computation. To overcome these challenges, we have developed ConvMLP: a hierarchical Convolutional MLP for visual recognition that combines convolution layers and MLPs in a lightweight, stage-wise co-design. ConvMLP-S has achieved a top-1 accuracy of 76.8% on ImageNet-1k with 9M parameters and 2.4G MACs. Furthermore, experiments on object detection and semantic segmentation have demonstrated that ConvMLP's visual representation can be efficiently transferred with fewer parameters, resulting in highly competitive results. Our code and pre-trained models are available on GitHub at https://github.com/SHI-Labs/Convolutional-MLPs.",1
"User representation is essential for providing high-quality commercial services in industry. Universal user representation has received many interests recently, with which we can be free from the cumbersome work of training a specific model for each downstream application. In this paper, we attempt to improve universal user representation from two points of views. First, a contrastive self-supervised learning paradigm is presented to guide the representation model training. It provides a unified framework that allows for long-term or short-term interest representation learning in a data-driven manner. Moreover, a novel multi-interest extraction module is presented. The module introduces an interest dictionary to capture principal interests of the given user, and then generate his/her interest-oriented representations via behavior aggregation. Experimental results demonstrate the effectiveness and applicability of the learned user representations.",0
"To deliver top-notch commercial services in the industry, it is crucial to have user representation. Recently, there has been growing interest in universal user representation, which eliminates the need to train a specific model for each downstream application, saving time and effort. This paper aims to enhance universal user representation from two perspectives. Firstly, it proposes a contrastive self-supervised learning paradigm to guide the representation model training, facilitating long-term or short-term interest representation learning in a data-driven way. Secondly, the paper introduces a novel multi-interest extraction module that utilizes an interest dictionary to identify the user's key interests and generates interest-oriented representations via behavior aggregation. The results of the experiments demonstrate the effectiveness and practicality of the learned user representations.",1
"Molecular representation learning plays an essential role in cheminformatics. Recently, language model-based approaches have been popular as an alternative to traditional expert-designed features to encode molecules. However, these approaches only utilize a single modality for representing molecules. Driven by the fact that a given molecule can be described through different modalities such as Simplified Molecular Line Entry System (SMILES), The International Union of Pure and Applied Chemistry (IUPAC), and The IUPAC International Chemical Identifier (InChI), we propose a multimodal molecular embedding generation approach called MM-Deacon (multimodal molecular domain embedding analysis via contrastive learning). MM-Deacon is trained using SMILES and IUPAC molecule representations as two different modalities. First, SMILES and IUPAC strings are encoded by using two different transformer-based language models independently, then the contrastive loss is utilized to bring these encoded representations from different modalities closer to each other if they belong to the same molecule, and to push embeddings farther from each other if they belong to different molecules. We evaluate the robustness of our molecule embeddings on molecule clustering, cross-modal molecule search, drug similarity assessment and drug-drug interaction tasks.",0
"In cheminformatics, molecular representation learning is crucial. Instead of expert-designed features, language model-based approaches have become popular for encoding molecules. However, these approaches only use a single modality to represent molecules. Given that a molecule can be described in various modalities such as SMILES, IUPAC, and InChI, we introduce a multimodal molecular embedding generation approach called MM-Deacon (multimodal molecular domain embedding analysis via contrastive learning). MM-Deacon utilizes SMILES and IUPAC molecule representations as two different modalities and is trained using two different transformer-based language models. The contrastive loss is then used to bring encoded representations from different modalities closer if they belong to the same molecule and push them farther apart if they belong to different molecules. We assess the robustness of our molecule embeddings on tasks such as molecule clustering, cross-modal molecule search, drug similarity assessment, and drug-drug interaction.",1
"Semantic understanding of 3D point clouds is important for various robotics applications. Given that point-wise semantic annotation is expensive, in this paper, we address the challenge of learning models with extremely sparse labels. The core problem is how to leverage numerous unlabeled points. To this end, we propose a self-supervised 3D representation learning framework named viewpoint bottleneck. It optimizes a mutual-information based objective, which is applied on point clouds under different viewpoints. A principled analysis shows that viewpoint bottleneck leads to an elegant surrogate loss function that is suitable for large-scale point cloud data. Compared with former arts based upon contrastive learning, viewpoint bottleneck operates on the feature dimension instead of the sample dimension. This paradigm shift has several advantages: It is easy to implement and tune, does not need negative samples and performs better on our goal down-streaming task. We evaluate our method on the public benchmark ScanNet, under the pointly-supervised setting. We achieve the best quantitative results among comparable solutions. Meanwhile we provide an extensive qualitative inspection on various challenging scenes. They demonstrate that our models can produce fairly good scene parsing results for robotics applications. Our code, data and models will be made public.",0
"For robotics applications, understanding the semantics of 3D point clouds is crucial. However, annotating points with semantic information can be costly. In this paper, we aim to address this challenge by developing models that can learn from sparsely labeled data. Our approach focuses on utilizing unlabeled points, and we propose a self-supervised 3D representation learning framework called viewpoint bottleneck. This framework optimizes a mutual-information based objective by applying it to point clouds from different viewpoints. Our analysis shows that viewpoint bottleneck results in a suitable surrogate loss function for large-scale point cloud data. Unlike previous methods that utilize contrastive learning, our approach operates on the feature dimension rather than the sample dimension, which has several advantages such as ease of implementation and tuning, not needing negative samples, and better performance on downstream tasks. We evaluate our method on the public benchmark ScanNet, achieving the best quantitative results among comparable solutions and demonstrating good scene parsing results for robotics applications. We also provide extensive qualitative inspection on various challenging scenes. Our code, data, and models will be made publicly available.",1
"Face parsing infers a pixel-wise label to each facial component, which has drawn much attention recently. Previous methods have shown their success in face parsing, which however overlook the correlation among facial components. As a matter of fact, the component-wise relationship is a critical clue in discriminating ambiguous pixels in facial area. To address this issue, we propose adaptive graph representation learning and reasoning over facial components, aiming to learn representative vertices that describe each component, exploit the component-wise relationship and thereby produce accurate parsing results against ambiguity. In particular, we devise an adaptive and differentiable graph abstraction method to represent the components on a graph via pixel-to-vertex projection under the initial condition of a predicted parsing map, where pixel features within a certain facial region are aggregated onto a vertex. Further, we explicitly incorporate the image edge as a prior in the model, which helps to discriminate edge and non-edge pixels during the projection, thus leading to refined parsing results along the edges. Then, our model learns and reasons over the relations among components by propagating information across vertices on the graph. Finally, the refined vertex features are projected back to pixel grids for the prediction of the final parsing map. To train our model, we propose a discriminative loss to penalize small distances between vertices in the feature space, which leads to distinct vertices with strong semantics. Experimental results show the superior performance of the proposed model on multiple face parsing datasets, along with the validation on the human parsing task to demonstrate the generalizability of our model.",0
"Recently, there has been a lot of interest in inferring a pixel-wise label for each facial component through face parsing. While previous methods have been successful, they have overlooked the importance of the correlation among facial components. In fact, understanding the relationship between components is critical in discriminating ambiguous pixels in the facial area. To address this issue, we propose a new approach that utilizes adaptive graph representation learning and reasoning over facial components. Our approach aims to learn representative vertices that describe each component, exploit the component-wise relationship, and produce accurate parsing results against ambiguity. We accomplish this by using an adaptive and differentiable graph abstraction method to represent the components on a graph via pixel-to-vertex projection. We also incorporate the image edge as a prior in the model, which helps to refine parsing results along the edges. Finally, our model learns and reasons over the relations among components by propagating information across vertices on the graph. To train our model, we propose a discriminative loss that penalizes small distances between vertices in the feature space, leading to distinct vertices with strong semantics. Our experimental results demonstrate the superior performance of our proposed model on multiple face parsing datasets, as well as the generalizability of our model to human parsing tasks.",1
"We consider the problem of complementary fashion prediction. Existing approaches focus on learning an embedding space where fashion items from different categories that are visually compatible are closer to each other. However, creating such labeled outfits is intensive and also not feasible to generate all possible outfit combinations, especially with large fashion catalogs. In this work, we propose a semi-supervised learning approach where we leverage large unlabeled fashion corpus to create pseudo-positive and pseudo-negative outfits on the fly during training. For each labeled outfit in a training batch, we obtain a pseudo-outfit by matching each item in the labeled outfit with unlabeled items. Additionally, we introduce consistency regularization to ensure that representation of the original images and their transformations are consistent to implicitly incorporate colour and other important attributes through self-supervision. We conduct extensive experiments on Polyvore, Polyvore-D and our newly created large-scale Fashion Outfits datasets, and show that our approach with only a fraction of labeled examples performs on-par with completely supervised methods.",0
"The focus of our research is on predicting complementary fashion. Current methods aim to teach a space where visually compatible fashion items from different categories are closer together. However, creating labeled outfits is a difficult task and generating every possible outfit combination is not practical, especially with large fashion catalogs. To address this, we suggest a semi-supervised approach that utilizes a large unlabeled fashion dataset to create pseudo-positive and pseudo-negative outfits on the spot during training. For each labeled outfit in a training set, we create a pseudo-outfit by pairing each item with an unlabeled item. We also integrate consistency regularization to ensure that the representation of the original images and their transformations are consistent and include essential features like color through self-supervision. We performed extensive experiments on Polyvore, Polyvore-D, and a new large-scale Fashion Outfits dataset, demonstrating that our method with just a fraction of labeled examples performs as well as fully supervised methods.",1
"Disentangled representation learning has been proposed as an approach to learning general representations. This can be done in the absence of, or with limited, annotations. A good general representation can be readily fine-tuned for new target tasks using modest amounts of data, or even be used directly in unseen domains achieving remarkable performance in the corresponding task. This alleviation of the data and annotation requirements offers tantalising prospects for tractable and affordable applications in computer vision and healthcare. Finally, disentangled representations can offer model explainability and can help us understand the underlying causal relations of the factors of variation, increasing their suitability for real-world deployment. In this tutorial paper, we will offer an overview of the disentangled representation learning, its building blocks and criteria, and discuss applications in computer vision and medical imaging. We conclude our tutorial by presenting the identified opportunities for the integration of recent machine learning advances into disentanglement, as well as the remaining challenges.",0
"The concept of disentangled representation learning has been put forward as a means of acquiring general representations, with or without annotations. An effective general representation can be easily adjusted for new target tasks using minimal amounts of data, or even applied directly in new domains to achieve impressive results in the given task. This reduction in data and annotation requirements creates exciting possibilities for feasible and cost-effective applications in fields such as computer vision and healthcare. Additionally, disentangled representations can facilitate model explainability and help us comprehend the inherent causal relationships of the variables, thereby improving their suitability for real-world implementation. In this instructional article, we will provide an overview of disentangled representation learning, its fundamental components and standards, and explore its use in computer vision and medical imaging. We conclude the tutorial by presenting opportunities for incorporating recent machine learning advancements into disentanglement, as well as the challenges that remain.",1
"Self-supervised representation learning for visual pre-training has achieved remarkable success with sample (instance or pixel) discrimination and semantics discovery of instance, whereas there still exists a non-negligible gap between pre-trained model and downstream dense prediction tasks. Concretely, these downstream tasks require more accurate representation, in other words, the pixels from the same object must belong to a shared semantic category, which is lacking in the previous methods. In this work, we present Dense Semantic Contrast (DSC) for modeling semantic category decision boundaries at a dense level to meet the requirement of these tasks. Furthermore, we propose a dense cross-image semantic contrastive learning framework for multi-granularity representation learning. Specially, we explicitly explore the semantic structure of the dataset by mining relations among pixels from different perspectives. For intra-image relation modeling, we discover pixel neighbors from multiple views. And for inter-image relations, we enforce pixel representation from the same semantic class to be more similar than the representation from different classes in one mini-batch. Experimental results show that our DSC model outperforms state-of-the-art methods when transferring to downstream dense prediction tasks, including object detection, semantic segmentation, and instance segmentation. Code will be made available.",0
"Visual pre-training using self-supervised representation learning has been highly successful in achieving sample discrimination and discovering semantics of instances or pixels. However, there is still a considerable gap between the pre-trained model and downstream dense prediction tasks, which require more accurate representation. Specifically, these tasks demand that pixels from the same object share a semantic category, a feature that previous methods lack. To address this issue, we introduce Dense Semantic Contrast (DSC), a method that models semantic category decision boundaries at a dense level to fulfill the requirements of such tasks. Additionally, we propose a framework for multi-granularity representation learning called dense cross-image semantic contrastive learning, which explicitly explores the semantic structure of the dataset by mining relations among pixels from various perspectives. For intra-image relation modeling, we identify pixel neighbors from multiple views, and for inter-image relations, we enforce pixel representation from the same semantic class to be more similar than the representation from different classes in one mini-batch. Our experimental results demonstrate that the DSC model surpasses state-of-the-art methods in transferring to downstream dense prediction tasks, such as object detection, semantic segmentation, and instance segmentation. The code for our approach will be made available.",1
"When machine predictors can achieve higher performance than the human decision-makers they support, improving the performance of human decision-makers is often conflated with improving machine accuracy. Here we propose a framework to directly support human decision-making, in which the role of machines is to reframe problems rather than to prescribe actions through prediction. Inspired by the success of representation learning in improving performance of machine predictors, our framework learns human-facing representations optimized for human performance. This ""Mind Composed with Machine"" framework incorporates a human decision-making model directly into the representation learning paradigm and is trained with a novel human-in-the-loop training procedure. We empirically demonstrate the successful application of the framework to various tasks and representational forms.",0
"The improvement of machine accuracy is often seen as synonymous with enhancing the performance of human decision-makers when machine predictors outperform them. To address this, we propose a framework that supports human decision-making by reframing problems rather than predicting actions. We take inspiration from representation learning which has been successful in improving machine predictors and develop a framework that creates human-facing representations optimized for human performance. Our ""Mind Composed with Machine"" framework integrates a human decision-making model into the representation learning paradigm and uses a new human-in-the-loop training method. We demonstrate the effectiveness of our framework through empirical studies across different tasks and representational forms.",1
"Recent years have seen a rise in the development of representational learning methods for graph data. Most of these methods, however, focus on node-level representation learning at various scales (e.g., microscopic, mesoscopic, and macroscopic node embedding). In comparison, methods for representation learning on whole graphs are currently relatively sparse. In this paper, we propose a novel unsupervised whole graph embedding method. Our method uses spectral graph wavelets to capture topological similarities on each k-hop sub-graph between nodes and uses them to learn embeddings for the whole graph. We evaluate our method against 12 well-known baselines on 4 real-world datasets and show that our method achieves the best performance across all experiments, outperforming the current state-of-the-art by a considerable margin.",0
"In recent years, there has been an increase in the development of approaches for learning representations for graph data. However, most of these approaches concentrate on learning node-level representations at different scales, while there are fewer methods for learning representations for entire graphs. This paper introduces a new unsupervised technique for embedding entire graphs. The method proposed here applies spectral graph wavelets to capture topological similarities between nodes within k-hop sub-graphs and employs this information to learn embeddings for the entire graph. To demonstrate the efficacy of our approach, we evaluate it against 12 well-known baselines on four real-world datasets. Our results indicate that our method outperforms all other approaches, including the current state-of-the-art, by a significant margin.",1
"Many real-world graphs involve different types of nodes and relations between nodes, being heterogeneous by nature. The representation learning of heterogeneous graphs (HGs) embeds the rich structure and semantics of such graphs into a low-dimensional space and facilitates various data mining tasks, such as node classification, node clustering, and link prediction. In this paper, we propose a self-supervised method that learns HG representations by relying on knowledge exchange and discovery among different HG structural semantics (meta-paths). Specifically, by maximizing the mutual information of meta-path representations, we promote meta-path information fusion and consensus, and ensure that globally shared semantics are encoded. By extensive experiments on node classification, node clustering, and link prediction tasks, we show that the proposed self-supervision both outperforms and improves competing methods by 1% and up to 10% for all tasks.",0
"Real-life graphs often consist of various types of nodes and relationships between nodes, making them inherently heterogeneous. To better analyze such graphs and perform tasks like node classification, node clustering, and link prediction, we need to represent this rich structure and meaning in a low-dimensional space. In this study, we introduce a self-supervised approach that leverages the exchange and discovery of knowledge amongst different structural semantics (meta-paths) in heterogeneous graphs. By maximizing the mutual information of meta-path representations, we promote the fusion and consensus of meta-path information while encoding globally shared semantics. Through extensive experiments on node classification, node clustering, and link prediction, we demonstrate that our proposed self-supervision method outperforms other methods by 1% and up to 10% across all tasks.",1
"3D object detection and dense depth estimation are one of the most vital tasks in autonomous driving. Multiple sensor modalities can jointly attribute towards better robot perception, and to that end, we introduce a method for jointly training 3D object detection and monocular dense depth reconstruction neural networks. It takes as inputs, a LiDAR point-cloud, and a single RGB image during inference and produces object pose predictions as well as a densely reconstructed depth map. LiDAR point-cloud is converted into a set of voxels, and its features are extracted using 3D convolution layers, from which we regress object pose parameters. Corresponding RGB image features are extracted using another 2D convolutional neural network. We further use these combined features to predict a dense depth map. While our object detection is trained in a supervised manner, the depth prediction network is trained with both self-supervised and supervised loss functions. We also introduce a loss function, edge-preserving smooth loss, and show that this results in better depth estimation compared to the edge-aware smooth loss function, frequently used in depth prediction works.",0
"Autonomous driving relies heavily on tasks such as 3D object detection and dense depth estimation. Improved robot perception can be achieved by combining multiple sensor modalities. To this end, we propose a method for jointly training neural networks for 3D object detection and monocular dense depth reconstruction. During inference, our method takes a LiDAR point-cloud and a single RGB image as input and produces object pose predictions as well as a densely reconstructed depth map. We convert the LiDAR point-cloud into a set of voxels and use 3D convolution layers to extract features for object pose parameter regression. The RGB image features are extracted using another 2D convolutional neural network. We then use the combined features to predict a dense depth map. Our object detection is trained using a supervised approach, while the depth prediction network is trained using both self-supervised and supervised loss functions. We also introduce an edge-preserving smooth loss function, which results in better depth estimation than the commonly used edge-aware smooth loss function.",1
"We consider functional outlier detection from a geometric perspective, specifically: for functional data sets drawn from a functional manifold which is defined by the data's modes of variation in amplitude and phase. Based on this manifold, we develop a conceptualization of functional outlier detection that is more widely applicable and realistic than previously proposed. Our theoretical and experimental analyses demonstrate several important advantages of this perspective: It considerably improves theoretical understanding and allows to describe and analyse complex functional outlier scenarios consistently and in full generality, by differentiating between structurally anomalous outlier data that are off-manifold and distributionally outlying data that are on-manifold but at its margins. This improves practical feasibility of functional outlier detection: We show that simple manifold learning methods can be used to reliably infer and visualize the geometric structure of functional data sets. We also show that standard outlier detection methods requiring tabular data inputs can be applied to functional data very successfully by simply using their vector-valued representations learned from manifold learning methods as input features. Our experiments on synthetic and real data sets demonstrate that this approach leads to outlier detection performances at least on par with existing functional data-specific methods in a large variety of settings, without the highly specialized, complex methodology and narrow domain of application these methods often entail.",0
"From a geometric standpoint, our focus is on identifying functional outliers in datasets originating from a functional manifold. This manifold is defined by the amplitude and phase modes of variation present in the data. By utilizing this manifold, we offer a more realistic and broadly applicable conceptualization of functional outlier detection compared to previous approaches. Our theoretical and experimental analyses highlight the benefits of this perspective, including enhanced theoretical comprehension and the ability to consistently and comprehensively analyze complex functional outlier scenarios. This also improves practicality; we demonstrate that simple manifold learning techniques can be utilized to effectively infer and visualize the geometric structure of functional datasets. Additionally, standard outlier detection methods that typically require tabular data inputs can be successfully applied to functional data by using vector-valued representations learned through manifold learning techniques as input features. Through synthetic and real data set experiments, we show that this approach performs at least as well as existing functional-specific methods in a wide range of settings, without necessitating specialized, intricate methodologies or a limited domain of application.",1
"Real-world information networks are increasingly occurring across various disciplines including online social networks and citation networks. These network data are generally characterized by sparseness, nonlinearity and heterogeneity bringing different challenges to the network analytics task to capture inherent properties from network data. Artificial intelligence and machine learning have been recently leveraged as powerful systems to learn insights from network data and deal with presented challenges. As part of machine learning techniques, graph embedding approaches are originally conceived for graphs constructed from feature represented datasets, like image dataset, in which links between nodes are explicitly defined. These traditional approaches cannot cope with network data challenges. As a new learning paradigm, network representation learning has been proposed to map a real-world information network into a low-dimensional space while preserving inherent properties of the network. In this paper, we present a systematic comprehensive survey of network representation learning, known also as network embedding, from birth to the current development state. Through the undertaken survey, we provide a comprehensive view of reasons behind the emergence of network embedding and, types of settings and models used in the network embedding pipeline. Thus, we introduce a brief history of representation learning and word representation learning ancestor of network embedding. We provide also formal definitions of basic concepts required to understand network representation learning followed by a description of network embedding pipeline. Most commonly used downstream tasks to evaluate embeddings, their evaluation metrics and popular datasets are highlighted. Finally, we present the open-source libraries for network embedding.",0
"Information networks in the real world are now occurring in various fields, such as online social networks and citation networks. These networks are often sparse, nonlinear, and heterogeneous, which poses significant challenges for network analytics. Recently, artificial intelligence and machine learning have become valuable tools for understanding these networks, particularly through graph embedding approaches. However, traditional techniques are not suitable for dealing with network data challenges. As a new learning paradigm, network representation learning has been introduced to map real-world information networks into a low-dimensional space while maintaining their inherent properties. This paper presents a comprehensive survey of network representation learning, also known as network embedding, from its inception to its current state of development. It covers the reasons behind the emergence of network embedding, the types of settings and models used in the network embedding pipeline, and a brief history of representation learning. Formal definitions of basic concepts required to understand network representation learning are provided, followed by a description of the network embedding pipeline. The most commonly used downstream tasks to evaluate embeddings, their evaluation metrics, and popular datasets are highlighted, and open-source libraries for network embedding are presented.",1
"We extract and use player position time-series data, tagged along with the action types, to build a competent model for representing team tactics behavioral patterns and use this representation to predict the outcome of arbitrary movements. We provide a framework for the useful encoding of short tactics and space occupations in a more extended sequence of movements or tactical plans. We investigate game segments during a match in which the team in possession of the ball regularly attempts to reach a position where they can take a shot at goal for a single game. A carefully designed and efficient kernel is employed using a triangular fuzzy membership function to create multiple time series for players' potential of presence at different court regions. Unsupervised learning is then used for time series using triplet loss and deep neural networks with exponentially dilated causal convolutions for the derived multivariate time series. This works key contribution lies in its approach to model how short scenes contribute to other longer ones and how players occupies and creates new spaces in-game court. We discuss the effectiveness of the proposed approach for prediction and recognition tasks on the professional basketball SportVU dataset for the 2015-16 half-season. The proposed system demonstrates descent functionality even with relatively small data.",0
"Our model utilizes time-series data of player positions and their corresponding actions to develop a proficient representation of team tactics' behavioral patterns. This representation is then utilized to predict the outcome of any arbitrary movements. We have created a framework that allows for the useful encoding of short tactics and space occupations into a more extensive sequence of movements or tactical plans. We have analyzed game segments where the team in possession of the ball regularly attempts to reach a position where they can take a shot at goal during a single game. To achieve this, we have employed a well-designed and efficient kernel that uses a triangular fuzzy membership function to create multiple time-series for players' potential presence at different court regions. We have utilized unsupervised learning for time-series using triplet loss and deep neural networks with exponentially dilated causal convolutions for the derived multivariate time-series. Our approach is unique in how it models how short scenes contribute to other longer ones and how players occupy and create new spaces on the game court. We have discussed the effectiveness of our approach for prediction and recognition tasks, showcasing its descent functionality, even with relatively small data, using the professional basketball SportVU dataset for the 2015-16 half-season.",1
"Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\% to~10\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\% relatively.",0
"We propose w2v-BERT, a self-supervised speech representation learning framework that combines masked language modeling (MLM) and contrastive learning. Inspired by the success of MLM in pre-training natural language processing models, w2v-BERT discretizes continuous speech signals into discriminative speech tokens using contrastive learning and then trains the model to learn contextualized speech representations via MLM. Unlike other MLM-based speech pre-training frameworks, w2v-BERT can be optimized end-to-end by solving the contrastive task and MLM simultaneously, without the need for iterative re-clustering or concatenation of separately trained modules. Our experiments demonstrate that w2v-BERT achieves competitive results on the LibriSpeech benchmarks and outperforms conformer-based wav2vec 2.0 and HuBERT by 5-10% relative WER reduction. Moreover, w2v-BERT performs over 30% better than our internal conformer-based wav2vec 2.0 on Google's Voice Search traffic dataset.",1
"Zero-shot domain adaptation (ZDA) methods aim to transfer knowledge about a task learned in a source domain to a target domain, while data from target domain are not available. In this work, we address learning feature representations which are invariant to and shared among different domains considering task characteristics for ZDA. To this end, we propose a method for task-guided ZDA (TG-ZDA) which employs multi-branch deep neural networks to learn feature representations exploiting their domain invariance and shareability properties. The proposed TG-ZDA models can be trained end-to-end without requiring synthetic tasks and data generated from estimated representations of target domains. The proposed TG-ZDA has been examined using benchmark ZDA tasks on image classification datasets. Experimental results show that our proposed TG-ZDA outperforms state-of-the-art ZDA methods for different domains and tasks.",0
"The objective of Zero-shot domain adaptation (ZDA) methods is to transfer knowledge gained from a task in a source domain to a target domain, even when there is no data available from the target domain. This study focuses on developing feature representations that are task-specific and invariant to different domains for ZDA. The proposed method, called task-guided ZDA (TG-ZDA), employs multi-branch deep neural networks to learn feature representations that can be shared among domains and exploit their domain invariance properties. The TG-ZDA models can be trained end-to-end without synthetic tasks or data from estimated representations of target domains. We tested the proposed TG-ZDA on benchmark ZDA tasks using image classification datasets and found that it outperforms existing ZDA methods for different domains and tasks.",1
"Real world learning scenarios involve a nonstationary distribution of classes with sequential dependencies among the samples, in contrast to the standard machine learning formulation of drawing samples independently from a fixed, typically uniform distribution. Furthermore, real world interactions demand learning on-the-fly from few or no class labels. In this work, we propose an unsupervised model that simultaneously performs online visual representation learning and few-shot learning of new categories without relying on any class labels. Our model is a prototype-based memory network with a control component that determines when to form a new class prototype. We formulate it as an online Gaussian mixture model, where components are created online with only a single new example, and assignments do not have to be balanced, which permits an approximation to natural imbalanced distributions from uncurated raw data. Learning includes a contrastive loss that encourages different views of the same image to be assigned to the same prototype. The result is a mechanism that forms categorical representations of objects in nonstationary environments. Experiments show that our method can learn from an online stream of visual input data and is significantly better at category recognition compared to state-of-the-art self-supervised learning methods.",0
"Learning in real world situations is different from standard machine learning approaches, as it involves classes with dependencies on each other, and often requires learning with few or no labels. To address this, we propose an unsupervised model that can learn online visual representation and few-shot learning of new categories without relying on any class labels. Our model is a prototype-based memory network that creates new class prototypes as needed, and is formulated as an online Gaussian mixture model. This allows us to learn from imbalanced data and includes a contrastive loss to encourage different views of the same image to be assigned to the same prototype. Our method outperforms state-of-the-art self-supervised learning methods in recognizing categories from an online stream of visual input data.",1
"Adversarial representation learning aims to learn data representations for a target task while removing unwanted sensitive information at the same time. Existing methods learn model parameters iteratively through stochastic gradient descent-ascent, which is often unstable and unreliable in practice. To overcome this challenge, we adopt closed-form solvers for the adversary and target task. We model them as kernel ridge regressors and analytically determine an upper-bound on the optimal dimensionality of representation. Our solution, dubbed OptNet-ARL, reduces to a stable one one-shot optimization problem that can be solved reliably and efficiently. OptNet-ARL can be easily generalized to the case of multiple target tasks and sensitive attributes. Numerical experiments, on both small and large scale datasets, show that, from an optimization perspective, OptNet-ARL is stable and exhibits three to five times faster convergence. Performance wise, when the target and sensitive attributes are dependent, OptNet-ARL learns representations that offer a better trade-off front between (a) utility and bias for fair classification and (b) utility and privacy by mitigating leakage of private information than existing solutions.",0
"The goal of adversarial representation learning is to develop data representations for a specific task while eliminating unwanted sensitive information simultaneously. Common methods use stochastic gradient descent-ascent to learn model parameters, but this can be unreliable and unstable. To address this issue, we utilize closed-form solvers for both the adversary and target task. We model them as kernel ridge regressors and determine the optimal dimensionality of representation analytically. Our approach, called OptNet-ARL, simplifies the problem to a stable one-shot optimization that is both reliable and efficient. OptNet-ARL can also be extended to multiple target tasks and sensitive attributes. Our numerical experiments on small and large datasets demonstrate that OptNet-ARL is stable and converges three to five times faster. Additionally, OptNet-ARL performs better than existing solutions when the target and sensitive attributes are correlated, offering a better trade-off between utility and bias for fair classification and utility and privacy by reducing the leakage of private information.",1
"Existing research for image text retrieval mainly relies on sentence-level supervision to distinguish matched and mismatched sentences for a query image. However, semantic mismatch between an image and sentences usually happens in finer grain, i.e., phrase level. In this paper, we explore to introduce additional phrase-level supervision for the better identification of mismatched units in the text. In practice, multi-grained semantic labels are automatically constructed for a query image in both sentence-level and phrase-level. We construct text scene graphs for the matched sentences and extract entities and triples as the phrase-level labels. In order to integrate both supervision of sentence-level and phrase-level, we propose Semantic Structure Aware Multimodal Transformer (SSAMT) for multi-modal representation learning. Inside the SSAMT, we utilize different kinds of attention mechanisms to enforce interactions of multi-grain semantic units in both sides of vision and language. For the training, we propose multi-scale matching losses from both global and local perspectives, and penalize mismatched phrases. Experimental results on MS-COCO and Flickr30K show the effectiveness of our approach compared to some state-of-the-art models.",0
"The current research on retrieving text from images typically depends on sentence-level guidance to differentiate between matching and mismatching sentences for a given image. However, discrepancies between an image and sentences usually occur at a more granular level, namely the phrase level. This study aims to introduce additional phrase-level supervision to more effectively identify mismatched text units. To achieve this, we automatically generate multi-grained semantic labels for a given image at both the sentence and phrase levels. We construct text scene graphs for matching sentences and extract entities and triples as phrase-level labels. To integrate both sentence-level and phrase-level supervision, we propose the Semantic Structure Aware Multimodal Transformer (SSAMT) for multi-modal representation learning. The SSAMT employs various attention mechanisms to facilitate interactions between multi-grain semantic units in both the visual and language domains. For training, we suggest multi-scale matching losses from both global and local perspectives, while penalizing mismatched phrases. Our approach has been proven effective through experiments on MS-COCO and Flickr30K, surpassing some of the state-of-the-art models.",1
"Causal inference using observational text data is becoming increasingly popular in many research areas. This paper presents the Bayesian Topic Regression (BTR) model that uses both text and numerical information to model an outcome variable. It allows estimation of both discrete and continuous treatment effects. Furthermore, it allows for the inclusion of additional numerical confounding factors next to text data. To this end, we combine a supervised Bayesian topic model with a Bayesian regression framework and perform supervised representation learning for the text features jointly with the regression parameter training, respecting the Frisch-Waugh-Lovell theorem. Our paper makes two main contributions. First, we provide a regression framework that allows causal inference in settings when both text and numerical confounders are of relevance. We show with synthetic and semi-synthetic datasets that our joint approach recovers ground truth with lower bias than any benchmark model, when text and numerical features are correlated. Second, experiments on two real-world datasets demonstrate that a joint and supervised learning strategy also yields superior prediction results compared to strategies that estimate regression weights for text and non-text features separately, being even competitive with more complex deep neural networks.",0
"The use of observational text data for causal inference is becoming increasingly popular across various research fields. In this paper, we introduce the Bayesian Topic Regression (BTR) model, which combines both text and numerical information to create an outcome variable model that can estimate both continuous and discrete treatment effects. Additionally, it can incorporate numerical confounding factors alongside text data. Our approach combines a supervised Bayesian topic model and a Bayesian regression framework, allowing for supervised representation learning of text features while training regression parameters, adhering to the Frisch-Waugh-Lovell theorem. This paper offers two main contributions: firstly, a regression framework that facilitates causal inference in scenarios where both text and numerical confounders are significant. We demonstrate through synthetic and semi-synthetic datasets that our joint approach retrieves ground truth with less bias than other benchmark models when text and numerical features are correlated. Secondly, we show through experiments on two real-world datasets that a joint and supervised learning strategy produces better prediction results than strategies that estimate regression weights for text and non-text features separately, even performing competitively with more complex deep neural networks.",1
"One of the most promising approaches for unsupervised learning is combining deep representation learning and deep clustering. Some recent works propose to simultaneously learn representation using deep neural networks and perform clustering by defining a clustering loss on top of embedded features. However, these approaches are sensitive to imbalanced data and out-of-distribution samples. Hence, these methods optimize clustering by pushing data close to randomly initialized cluster centers. This is problematic when the number of instances varies largely in different classes or a cluster with few samples has less chance to be assigned a good centroid. To overcome these limitations, we introduce StatDEC, a new unsupervised framework for joint statistical representation learning and clustering. StatDEC simultaneously trains two deep learning models, a deep statistics network that captures the data distribution, and a deep clustering network that learns embedded features and performs clustering by explicitly defining a clustering loss. Specifically, the clustering network and representation network both take advantage of our proposed statistics pooling layer that represents mean, variance, and cardinality to handle the out-of-distribution samples as well as a class imbalance. Our experiments show that using these representations, one can considerably improve results on imbalanced image clustering across a variety of image datasets. Moreover, the learned representations generalize well when transferred to the out-of-distribution dataset.",0
"One promising method for unsupervised learning is the combination of deep representation learning and deep clustering. Some recent studies suggest learning representation through deep neural networks and clustering by defining a clustering loss on top of embedded features. However, these techniques are vulnerable to imbalanced data and out-of-distribution samples, leading to clustering optimization by pushing data near randomly initialized cluster centers. This poses a problem when the number of instances in various classes varies greatly or when a cluster with few samples has a lower chance of being assigned a good centroid. To overcome these limitations, we propose StatDEC, a new unsupervised framework for statistical representation learning and clustering. StatDEC trains two deep learning models simultaneously: a deep statistics network that captures the data distribution and a deep clustering network that learns embedded features and performs clustering by explicitly defining a clustering loss. Both the clustering network and the representation network utilize our proposed statistics pooling layer, which represents mean, variance, and cardinality to handle out-of-distribution samples and class imbalances. Our experiments demonstrate significant improvements in imbalanced image clustering across a variety of image datasets using these representations. Furthermore, the learned representations are generalizable when transferred to out-of-distribution datasets.",1
"In a regular open set detection problem, samples of known classes (also called closed set classes) are used to train a special classifier. In testing, the classifier can (1) classify the test samples of known classes to their respective classes and (2) also detect samples that do not belong to any of the known classes (we say they belong to some unknown or open set classes). This paper studies the problem of zero-shot open-set detection, which still performs the same two tasks in testing but has no training except using the given known class names. This paper proposes a novel and yet simple method (called ZO-CLIP) to solve the problem. ZO-CLIP builds on top of the recent advances in zero-shot classification through multi-modal representation learning. It first extends the pre-trained multi-modal model CLIP by training a text-based image description generator on top of CLIP. In testing, it uses the extended model to generate some candidate unknown class names for each test sample and computes a confidence score based on both the known class names and candidate unknown class names for zero-shot open set detection. Experimental results on 5 benchmark datasets for open set detection confirm that ZO-CLIP outperforms the baselines by a large margin.",0
"This paper focuses on the problem of zero-shot open-set detection, which involves detecting samples that do not belong to any of the known classes, without any prior training other than the given known class names. In regular open set detection, a classifier is trained using samples of known classes, which can identify test samples of known classes and detect samples that do not belong to any of the known classes. The proposed method, ZO-CLIP, extends the pre-trained multi-modal model CLIP by adding a text-based image description generator. During testing, it generates candidate unknown class names for each test sample and computes a confidence score based on both the known class names and candidate unknown class names. Experimental results on 5 benchmark datasets confirm that ZO-CLIP outperforms the baselines by a significant margin.",1
"Temporal grounding aims to temporally localize a video moment in the video whose semantics are related to a given natural language query. Existing methods typically apply a detection or regression pipeline on the fused representation with a focus on designing complicated heads and fusion strategies. Instead, from a perspective on temporal grounding as a metric-learning problem, we present a Dual Matching Network (DMN), to directly model the relations between language queries and video moments in a joint embedding space. This new metric-learning framework enables fully exploiting negative samples from two new aspects: constructing negative cross-modal pairs from a dual matching scheme and mining negative pairs across different videos. These new negative samples could enhance the joint representation learning of two modalities via cross-modal pair discrimination to maximize their mutual information. Experiments show that DMN achieves highly competitive performance compared with state-of-the-art methods on four video grounding benchmarks. Based on DMN, we present a winner solution for STVG challenge of the 3rd PIC workshop. This suggests that metric-learning is still a promising method for temporal grounding via capturing the essential cross-modal correlation in a joint embedding space.",0
"The objective of temporal grounding is to locate a specific moment in a video that is semantically relevant to a given natural language query. Current approaches involve using a detection or regression pipeline with the goal of designing complicated heads and fusion strategies. However, we propose a different perspective on temporal grounding by treating it as a metric-learning problem. Our Dual Matching Network (DMN) directly models the relationship between language queries and video moments in a joint embedding space. This metric-learning framework allows for the effective use of negative samples by constructing negative cross-modal pairs through dual matching and mining negative pairs across different videos. By enhancing joint representation learning through cross-modal pair discrimination, we can maximize mutual information. Our experiments demonstrate that DMN yields highly competitive results compared to state-of-the-art methods across four video grounding benchmarks. Additionally, our DMN-based solution was the winner of the STVG challenge at the 3rd PIC workshop. Our findings suggest that metric-learning remains a promising approach for temporal grounding, as it effectively captures the essential cross-modal correlation in a joint embedding space.",1
"Measuring concept generalization, i.e., the extent to which models trained on a set of (seen) visual concepts can be leveraged to recognize a new set of (unseen) concepts, is a popular way of evaluating visual representations, especially in a self-supervised learning framework. Nonetheless, the choice of unseen concepts for such an evaluation is usually made arbitrarily, and independently from the seen concepts used to train representations, thus ignoring any semantic relationships between the two. In this paper, we argue that the semantic relationships between seen and unseen concepts affect generalization performance and propose ImageNet-CoG, a novel benchmark on the ImageNet-21K (IN-21K) dataset that enables measuring concept generalization in a principled way. Our benchmark leverages expert knowledge that comes from WordNet in order to define a sequence of unseen IN-21K concept sets that are semantically more and more distant from the ImageNet-1K (IN-1K) subset, a ubiquitous training set. This allows us to benchmark visual representations learned on IN-1K out-of-the box. We conduct a large-scale study encompassing 31 convolution and transformer-based models and show how different architectures, levels of supervision, regularization techniques and use of web data impact the concept generalization performance.",0
"Measuring the ability of models trained on a set of visual concepts to recognize a new set of concepts, known as concept generalization, is a common method for evaluating visual representations, particularly in self-supervised learning. However, the selection of unseen concepts for this evaluation is usually arbitrary and ignores any semantic relationships between the seen and unseen concepts used for training and evaluation. In this study, we propose a new benchmark called ImageNet-CoG, which enables measuring concept generalization in a more principled way using the ImageNet-21K dataset. Our benchmark uses expert knowledge from WordNet to define a sequence of unseen concept sets that are increasingly semantically distant from the ubiquitous training set, ImageNet-1K. This allows us to evaluate visual representations learned on IN-1K out-of-the box. We conducted a large-scale study of 31 convolution and transformer-based models and analyzed how different architectures, levels of supervision, regularization techniques, and use of web data impact concept generalization performance.",1
"Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset harvested from wikiHow consisting of 772,277 images representing human actions, we show that our task is challenging for state-of-the-art multimodal models. Moreover, the multimodal representation learned from our data can be effectively transferred to other datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task will facilitate multimodal reasoning about procedural events.",0
"Artificial intelligence systems can improve their understanding of human activities by comprehending the sequence of steps required to achieve a goal. Prior research in NLP has explored the task of goal-step inference in text. We have introduced a new approach, called Visual Goal-Step Inference (VGSI), which involves presenting a model with a textual goal and asking it to identify which of four images represents a feasible step towards that goal. Our dataset, consisting of 772,277 images of human actions gathered from wikiHow, shows that the VGSI task is difficult even for advanced multimodal models. Furthermore, the multimodal representation learned from our dataset can be effectively transferred to other datasets such as HowTo100m, resulting in a 15-20% improvement in VGSI accuracy. Our task will facilitate multimodal reasoning about procedural events.",1
"A wide range of machine learning applications such as privacy-preserving learning, algorithmic fairness, and domain adaptation/generalization among others, involve learning \emph{invariant representations} of the data that aim to achieve two competing goals: (a) maximize information or accuracy with respect to a target response, and (b) maximize invariance or independence with respect to a set of protected features (e.g.\ for fairness, privacy, etc). Despite their wide applicability, theoretical understanding of the optimal tradeoffs -- with respect to accuracy, and invariance -- achievable by invariant representations is still severely lacking. In this paper, we provide precisely such an information-theoretic analysis of such tradeoffs under both classification and regression settings. We provide a geometric characterization of the accuracy and invariance achievable by any representation of the data; we term this feasible region the information plane. We provide a lower bound for this feasible region for the classification case, and an exact characterization for the regression case, which allows us to either bound or exactly characterize the Pareto optimal frontier between accuracy and invariance. Although our contributions are mainly theoretical, a key practical application of our results is in certifying the potential sub-optimality of any given representation learning algorithm for either classification or regression tasks. Our results shed new light on the fundamental interplay between accuracy and invariance, and may be useful in guiding the design of future representation learning algorithms.",0
"Learning \emph{invariant representations} is essential for a variety of machine learning applications, including privacy-preserving learning, algorithmic fairness, and domain adaptation/generalization. The goal of invariant representations is to simultaneously maximize accuracy with respect to a target response and independence with respect to a set of protected features (e.g. for fairness, privacy, etc.). Despite their widespread use, there is a lack of theoretical understanding of the optimal tradeoffs achievable by invariant representations. This paper presents an information-theoretic analysis of such tradeoffs, focusing on classification and regression settings. The feasible region of accuracy and invariance achievable by any representation of the data is characterized geometrically, which we call the information plane. We provide a lower bound for the feasible region in the classification case and an exact characterization in the regression case, enabling us to bound or exactly describe the Pareto optimal frontier between accuracy and invariance. Although our contributions are mainly theoretical, our results can be used to assess the potential sub-optimality of any given representation learning algorithm for classification or regression tasks. Our findings offer new insights into the relationship between accuracy and invariance and may guide the development of future representation learning algorithms.",1
"Preserving maximal information is one of principles of designing self-supervised learning methodologies. To reach this goal, contrastive learning adopts an implicit way which is contrasting image pairs. However, we believe it is not fully optimal to simply use the contrastive estimation for preservation. Moreover, it is necessary and complemental to introduce an explicit solution to preserve more information. From this perspective, we introduce Preservational Learning to reconstruct diverse image contexts in order to preserve more information in learned representations. Together with the contrastive loss, we present Preservational Contrastive Representation Learning (PCRL) for learning self-supervised medical representations. PCRL provides very competitive results under the pretraining-finetuning protocol, outperforming both self-supervised and supervised counterparts in 5 classification/segmentation tasks substantially.",0
"One of the principles of designing self-supervised learning methodologies is to preserve maximal information. Contrastive learning is an implicit approach that involves contrasting image pairs to achieve this objective. However, we believe that relying solely on contrastive estimation is not entirely optimal for preserving information, and it is necessary to introduce an explicit solution for better results. To address this, we propose Preservational Learning, which reconstructs diverse image contexts to preserve more information in learned representations. By combining Preservational Learning with contrastive loss, we introduce Preservational Contrastive Representation Learning (PCRL) for self-supervised medical representations. Our approach achieves highly competitive results compared to both self-supervised and supervised counterparts in five classification/segmentation tasks, demonstrating its effectiveness in the pretraining-finetuning protocol.",1
"Graph Neural Networks (GNNs) bring the power of deep representation learning to graph and relational data and achieve state-of-the-art performance in many applications. GNNs compute node representations by taking into account the topology of the node's ego-network and the features of the ego-network's nodes. When the nodes do not have high-quality features, GNNs learn an embedding layer to compute node embeddings and use them as input features. However, the size of the embedding layer is linear to the product of the number of nodes in the graph and the dimensionality of the embedding and does not scale to big data and graphs with hundreds of millions of nodes. To reduce the memory associated with this embedding layer, hashing-based approaches, commonly used in applications like NLP and recommender systems, can potentially be used. However, a direct application of these ideas fails to exploit the fact that in many real-world graphs, nodes that are topologically close will tend to be related to each other (homophily) and as such their representations will be similar.   In this work, we present approaches that take advantage of the nodes' position in the graph to dramatically reduce the memory required, with minimal if any degradation in the quality of the resulting GNN model. Our approaches decompose a node's embedding into two components: a position-specific component and a node-specific component. The position-specific component models homophily and the node-specific component models the node-to-node variation. Extensive experiments using different datasets and GNN models show that our methods are able to reduce the memory requirements by 88% to 97% while achieving, in nearly all cases, better classification accuracy than other competing approaches, including the full embeddings.",0
"The use of Graph Neural Networks (GNNs) has revolutionized the representation learning of graph and relational data by achieving remarkable performance in various applications. GNNs determine node representations by considering the topology of the node's ego-network and the nodes' features within that network. However, if the nodes lack high-quality features, GNNs must learn an embedding layer to compute node embeddings, but this embedding layer's size increases linearly with the number of nodes and dimensionality of the embedding. As a result, the use of hashing-based approaches is suggested to reduce the memory required. Still, these methods do not account for the fact that in real-world graphs, nodes that are topologically similar will have comparable representations. To address this issue, we present an approach that leverages the position of nodes in the graph to significantly decrease the necessary memory, with minimal impact on the GNN model's quality. Our approach splits a node's embedding into two parts: the position-specific component representing homophily and the node-specific component representing node-to-node variation. Through extensive experimentation with various datasets and GNN models, our methods reduced memory requirements by 88% to 97%, while achieving, in nearly all cases, higher classification accuracy than comparable methods, including full embeddings.",1
"With the recent success of representation learning methods, which includes deep learning as a special case, there has been considerable interest in developing representation learning techniques that can incorporate known physical constraints into the learned representation. As one example, in many applications that involve a signal propagating through physical media (e.g., optics, acoustics, fluid dynamics, etc), it is known that the dynamics of the signal must satisfy constraints imposed by the wave equation. Here we propose a matrix factorization technique that decomposes such signals into a sum of components, where each component is regularized to ensure that it satisfies wave equation constraints. Although our proposed formulation is non-convex, we prove that our model can be efficiently solved to global optimality in polynomial time. We demonstrate the benefits of our work by applications in structural health monitoring, where prior work has attempted to solve this problem using sparse dictionary learning approaches that do not come with any theoretical guarantees regarding convergence to global optimality and employ heuristics to capture desired physical constraints.",0
"Representation learning techniques, including deep learning, have gained significant momentum due to their recent success. Consequently, there is a growing interest in developing representation learning methods that incorporate known physical constraints into the learned representation. For instance, in applications involving signal propagation through physical media such as optics, acoustics, and fluid dynamics, it is necessary to satisfy constraints imposed by the wave equation. To address this, we propose a non-convex matrix factorization technique that decomposes signals into components, with each component being regularized to satisfy wave equation constraints. Despite the non-convexity of our proposed formulation, we prove that our model can be efficiently solved to global optimality in polynomial time. We demonstrate the effectiveness of our approach in structural health monitoring, where prior work utilized sparse dictionary learning approaches that lacked theoretical guarantees regarding convergence to global optimality and relied on heuristics to capture desired physical constraints.",1
"Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, efficient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets.",0
"Representation learning aims to create condensed representations of high-dimensional data by summarizing important features. However, establishing formal criteria for learned representations can be difficult, as intuitive desiderata such as non-spuriousness, efficiency, and disentanglement are challenging to measure and enhance using observed data. To address this issue, this paper adopts a causal perspective on representation learning and utilizes counterfactual quantities and observable consequences of causal assertions to formalize non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning). This approach yields computable metrics that can assess the degree to which representations satisfy the desired characteristics and enable the learning of non-spurious and disentangled representations from single observational datasets.",1
"Encouraged by the success of contrastive learning on image classification tasks, we propose a new self-supervised method for the structured regression task of 3D hand pose estimation. Contrastive learning makes use of unlabeled data for the purpose of representation learning via a loss formulation that encourages the learned feature representations to be invariant under any image transformation. For 3D hand pose estimation, it too is desirable to have invariance to appearance transformation such as color jitter. However, the task requires equivariance under affine transformations, such as rotation and translation. To address this issue, we propose an equivariant contrastive objective and demonstrate its effectiveness in the context of 3D hand pose estimation. We experimentally investigate the impact of invariant and equivariant contrastive objectives and show that learning equivariant features leads to better representations for the task of 3D hand pose estimation. Furthermore, we show that standard ResNets with sufficient depth, trained on additional unlabeled data, attain improvements of up to 14.5% in PA-EPE on FreiHAND and thus achieves state-of-the-art performance without any task specific, specialized architectures. Code and models are available at https://ait.ethz.ch/projects/2021/PeCLR/",0
"A new self-supervised method for 3D hand pose estimation is proposed, building on the success of contrastive learning on image classification tasks. Contrastive learning uses unlabeled data to encourage invariant feature representations through a loss formulation. However, while invariance to appearance transformations is desired in 3D hand pose estimation, equivariance under affine transformations is also required. To address this, an equivariant contrastive objective is proposed, which improves representation for the task. Experiments demonstrate the effectiveness of this approach, with ResNets achieving state-of-the-art results without task-specific architectures. Code and models are available at https://ait.ethz.ch/projects/2021/PeCLR/.",1
"Contrastive Learning (CL) is one of the most popular self-supervised learning frameworks for graph representation learning, which trains a Graph Neural Network (GNN) by discriminating positive and negative node pairs. However, there are two challenges for CL on graphs. On the one hand, traditional CL methods will unavoidably introduce semantic errors since they will treat some semantically similar nodes as negative pairs. On the other hand, most of the existing CL methods ignore the multiplexity nature of the real-world graphs, where nodes are connected by various relations and each relation represents a view of the graph. To address these challenges, we propose a novel Graph Multi-View Prototypical (Graph-MVP) framework to extract node embeddings on multiplex graphs. Firstly, we introduce a Graph Prototypical Contrastive Learning (Graph-PCL) framework to capture both node-level and semantic-level information for each view of multiplex graphs. Graph-PCL captures the node-level information by a simple yet effective data transformation technique. It captures the semantic-level information by an Expectation-Maximization (EM) algorithm, which alternatively performs clustering over node embeddings and parameter updating for GNN. Next, we introduce Graph-MVP based on Graph-PCL to jointly model different views of the multiplex graphs. Our key insight behind Graph-MVP is that different view-specific embeddings of the same node should have similar underlying semantic, based on which we propose two versions of Graph-MVP: Graph-MVP_hard and Graph-MVP_soft to align embeddings across views. Finally, we evaluate the proposed Graph-PCL and Graph-MVP on a variety of real-world datasets and downstream tasks. The experimental results demonstrate the effectiveness of the proposed Graph-PCL and Graph-MVP frameworks.",0
"Contrastive Learning (CL) is a self-supervised learning framework that is widely used for graph representation learning. It trains a Graph Neural Network (GNN) by distinguishing between positive and negative node pairs. However, there are two challenges associated with CL on graphs. Firstly, traditional CL methods may introduce semantic errors as they treat some semantically similar nodes as negative pairs. Secondly, most of the existing CL methods do not consider the multiplex nature of real-world graphs where nodes are linked by diverse relations, with each relation representing a different view of the graph. To overcome these challenges, we propose a new framework called Graph Multi-View Prototypical (Graph-MVP) to extract node embeddings on multiplex graphs. We introduce a Graph Prototypical Contrastive Learning (Graph-PCL) framework to capture both node-level and semantic-level information for each view of multiplex graphs. Graph-PCL captures the node-level information using a simple yet effective data transformation technique and the semantic-level information using an Expectation-Maximization (EM) algorithm. We then introduce Graph-MVP, which builds upon Graph-PCL to jointly model different views of multiplex graphs. Our key insight is that different view-specific embeddings of the same node should have similar underlying semantics. We propose two versions of Graph-MVP: Graph-MVP_hard and Graph-MVP_soft to align embeddings across views. Finally, we evaluate the proposed Graph-PCL and Graph-MVP frameworks on various real-world datasets and downstream tasks. The experimental results demonstrate the effectiveness of our proposed frameworks.",1
"Conventional RGB-D salient object detection methods aim to leverage depth as complementary information to find the salient regions in both modalities. However, the salient object detection results heavily rely on the quality of captured depth data which sometimes are unavailable. In this work, we make the first attempt to solve the RGB-D salient object detection problem with a novel depth-awareness framework. This framework only relies on RGB data in the testing phase, utilizing captured depth data as supervision for representation learning. To construct our framework as well as achieving accurate salient detection results, we propose a Ubiquitous Target Awareness (UTA) network to solve three important challenges in RGB-D SOD task: 1) a depth awareness module to excavate depth information and to mine ambiguous regions via adaptive depth-error weights, 2) a spatial-aware cross-modal interaction and a channel-aware cross-level interaction, exploiting the low-level boundary cues and amplifying high-level salient channels, and 3) a gated multi-scale predictor module to perceive the object saliency in different contextual scales. Besides its high performance, our proposed UTA network is depth-free for inference and runs in real-time with 43 FPS. Experimental evidence demonstrates that our proposed network not only surpasses the state-of-the-art methods on five public RGB-D SOD benchmarks by a large margin, but also verifies its extensibility on five public RGB SOD benchmarks.",0
"RGB-D salient object detection methods traditionally utilize depth data as supplementary information to identify salient regions in both modalities. However, the accuracy of salient object detection outcomes is heavily reliant on the quality of captured depth data, which can at times be unavailable. In this study, we introduce a depth-aware framework for solving the RGB-D salient object detection problem. During the testing phase, our framework solely relies on RGB data and uses captured depth data to supervise representation learning. To ensure accurate salient detection outcomes, we propose a Ubiquitous Target Awareness (UTA) network that addresses three crucial challenges in the RGB-D SOD task. These challenges include an awareness module for depth excavation, the exploitation of low-level boundary cues, amplification of high-level salient channels, and perception of object saliency in various contextual scales. Our proposed UTA network is depth-free during inference, runs in real-time with 43 FPS, and achieves high performance. Experimental results demonstrate that our network outperforms state-of-the-art methods on five public RGB-D SOD benchmarks, and also exhibits extensibility on five public RGB SOD benchmarks.",1
"Many applications of representation learning, such as privacy-preservation, algorithmic fairness and domain adaptation, desire explicit control over semantic information being discarded. This goal is often formulated as satisfying two potentially competing objectives: maximizing utility for predicting a target attribute while simultaneously being independent or invariant with respect to a known semantic attribute. In this paper, we \emph{identify and determine} two fundamental trade-offs between utility and semantic dependence induced by the statistical dependencies between the data and its corresponding target and semantic attributes. We derive closed-form solutions for the global optima of the underlying optimization problems under mild assumptions, which in turn yields closed formulae for the exact trade-offs. We also derive empirical estimates of the trade-offs and show their convergence to the corresponding population counterparts. Finally, we numerically quantify the trade-offs on representative problems and compare to the solutions achieved by baseline representation learning algorithms.",0
"Representation learning is used for various purposes, including privacy preservation, algorithmic fairness, and domain adaptation. These applications require explicit control over the discarded semantic information. The objective is to maximize the utility for predicting a target attribute while maintaining independence or invariance with respect to a known semantic attribute. This paper identifies two fundamental trade-offs between utility and semantic dependence, which are induced by the statistical dependencies between the data and its corresponding target and semantic attributes. Closed-form solutions are derived for the global optima of the optimization problems, yielding exact trade-offs. Empirical estimates of the trade-offs are also obtained and shown to converge to the population counterparts. Finally, the trade-offs are numerically quantified on representative problems and compared to the solutions obtained by baseline representation learning algorithms.",1
"Knowledge distillation (KD) is an effective framework that aims to transfer meaningful information from a large teacher to a smaller student. Generally, KD often involves how to define and transfer knowledge. Previous KD methods often focus on mining various forms of knowledge, for example, feature maps and refined information. However, the knowledge is derived from the primary supervised task and thus is highly task-specific. Motivated by the recent success of self-supervised representation learning, we propose an auxiliary self-supervision augmented task to guide networks to learn more meaningful features. Therefore, we can derive soft self-supervision augmented distributions as richer dark knowledge from this task for KD. Unlike previous knowledge, this distribution encodes joint knowledge from supervised and self-supervised feature learning. Beyond knowledge exploration, another crucial aspect is how to learn and distill our proposed knowledge effectively. To fully take advantage of hierarchical feature maps, we propose to append several auxiliary branches at various hidden layers. Each auxiliary branch is guided to learn self-supervision augmented task and distill this distribution from teacher to student. Thus we call our KD method as Hierarchical Self-Supervision Augmented Knowledge Distillation (HSSAKD). Experiments on standard image classification show that both offline and online HSSAKD achieves state-of-the-art performance in the field of KD. Further transfer experiments on object detection further verify that HSSAKD can guide the network to learn better features, which can be attributed to learn and distill an auxiliary self-supervision augmented task effectively.",0
"The effective framework of Knowledge Distillation (KD) aims to transfer valuable information from a larger teacher model to a smaller student model. Typically, KD involves defining and transferring knowledge, with previous methods focusing on extracting various types of knowledge, such as feature maps and refined information. However, this knowledge is task-specific and derived from the primary supervised task. To address this, we propose an auxiliary self-supervision augmented task to guide networks to learn more meaningful features, derived from self-supervised representation learning. This leads to the creation of richer dark knowledge, combining supervised and self-supervised feature learning, which can be effectively distilled from teacher to student through the use of auxiliary branches at various hidden layers. This approach, known as Hierarchical Self-Supervision Augmented Knowledge Distillation (HSSAKD), achieves state-of-the-art performance in both offline and online image classification experiments, and is also effective in transfer experiments on object detection.",1
"We present a novel learning-based approach to graph representations of road networks employing state-of-the-art graph convolutional neural networks. Our approach is applied to realistic road networks of 17 cities from Open Street Map. While edge features are crucial to generate descriptive graph representations of road networks, graph convolutional networks usually rely on node features only. We show that the highly representative edge features can still be integrated into such networks by applying a line graph transformation. We also propose a method for neighborhood sampling based on a topological neighborhood composed of both local and global neighbors. We compare the performance of learning representations using different types of neighborhood aggregation functions in transductive and inductive tasks and in supervised and unsupervised learning. Furthermore, we propose a novel aggregation approach, Graph Attention Isomorphism Network, GAIN. Our results show that GAIN outperforms state-of-the-art methods on the road type classification problem.",0
"Our study introduces a new method for creating graph representations of road networks that uses advanced graph convolutional neural networks. We apply this approach to actual road networks of 17 cities sourced from Open Street Map. Although node features are commonly used with graph convolutional networks, we demonstrate that including edge features can be achieved by using a line graph transformation. In addition, we suggest a technique for neighborhood sampling that involves a topological neighborhood comprising global and local neighbors. We evaluate the effectiveness of various neighborhood aggregation functions in supervised and unsupervised learning tasks. We also propose a novel aggregation method called Graph Attention Isomorphism Network (GAIN), which outperforms other techniques in the road type classification problem.",1
"Nonnegative matrix factorization is usually powerful for learning the ""shallow"" parts-based representation, but it clearly fails to discover deep hierarchical information within both the basis and representation spaces. In this paper, we technically propose a new enriched prior based Dual-constrained Deep Semi-Supervised Coupled Factorization Network, called DS2CF-Net, for learning the hierarchical coupled representations. To ex-tract hidden deep features, DS2CF-Net is modeled as a deep-structure and geometrical structure-constrained neural network. Specifically, DS2CF-Net designs a deep coupled factorization architecture using multi-layers of linear transformations, which coupled updates the bases and new representations in each layer. To improve the discriminating ability of learned deep representations and deep coefficients, our network clearly considers enriching the supervised prior by the joint deep coefficients-regularized label prediction, and incorporates enriched prior information as additional label and structure constraints. The label constraint can enable the samples of the same label to have the same coordinate in the new feature space, while the structure constraint forces the coefficient matrices in each layer to be block-diagonal so that the enhanced prior using the self-expressive label propagation are more accurate. Our network also integrates the adaptive dual-graph learning to retain the local manifold structures of both the data manifold and feature manifold by minimizing the reconstruction errors in each layer. Extensive experiments on several real databases demonstrate that our DS2CF-Net can obtain state-of-the-art performance for representation learning and clustering.",0
"Nonnegative matrix factorization is a useful tool for creating a parts-based representation, but it is not effective in discovering hierarchical information in the basis and representation spaces. To address this limitation, we present a new semi-supervised coupled factorization network called DS2CF-Net. This network is designed to extract hidden deep features using deep-structure and geometrical structure-constrained neural networks. DS2CF-Net employs a deep coupled factorization architecture with multi-layers of linear transformations that update the bases and new representations in each layer. To improve the discriminating ability of learned deep representations and coefficients, we integrate enriched prior information as additional label and structure constraints. The label constraint ensures that samples with the same label have the same coordinate in the new feature space, while the structure constraint forces coefficient matrices in each layer to be block-diagonal, enhancing the self-expressive label propagation. Our network also utilizes adaptive dual-graph learning to retain local manifold structures in both the data and feature manifolds. Extensive experiments demonstrate that DS2CF-Net achieves state-of-the-art performance for representation learning and clustering.",1
"Employing clustering strategy to assign unlabeled target images with pseudo labels has become a trend for person re-identification (re-ID) algorithms in domain adaptation. A potential limitation of these clustering-based methods is that they always tend to introduce noisy labels, which will undoubtedly hamper the performance of our re-ID system. To handle this limitation, an intuitive solution is to utilize collaborative training to purify the pseudo label quality. However, there exists a challenge that the complementarity of two networks, which inevitably share a high similarity, becomes weakened gradually as training process goes on; worse still, these approaches typically ignore to consider the self-discrepancy of intra-class relations. To address this issue, in this paper, we propose a multiple co-teaching framework for domain adaptive person re-ID, opening up a promising direction about self-discrepancy problem under unsupervised condition. On top of that, a mean-teaching mechanism is leveraged to enlarge the difference and discover more complementary features. Comprehensive experiments conducted on several large-scale datasets show that our method achieves competitive performance compared with the state-of-the-arts.",0
"The use of clustering methods to assign pseudo labels to unlabeled target images has become popular in person re-identification (re-ID) algorithms for domain adaptation. However, a potential drawback of these methods is the introduction of noisy labels that can harm the re-ID system's performance. To address this issue, collaborative training can be used to improve the quality of pseudo labels. Nevertheless, this approach faces a challenge when the complementarity of two networks is weakened during the training process. Furthermore, intra-class relations are often ignored in these methods. To overcome this challenge, this paper proposes a multiple co-teaching framework for domain adaptive person re-ID that addresses the self-discrepancy problem under unsupervised conditions. Additionally, a mean-teaching mechanism is used to increase the difference and discover more complementary features. Our method achieves competitive performance compared to state-of-the-art algorithms on several large-scale datasets.",1
"Graph Representation Learning (GRL) has become essential for modern graph data mining and learning tasks. GRL aims to capture the graph's structural information and exploit it in combination with node and edge attributes to compute low-dimensional representations. While Graph Neural Networks (GNNs) have been used in state-of-the-art GRL architectures, they have been shown to suffer from over smoothing when many GNN layers need to be stacked. In a different GRL approach, spectral methods based on graph filtering have emerged addressing over smoothing; however, up to now, they employ traditional neural networks that cannot efficiently exploit the structure of graph data. Motivated by this, we propose PointSpectrum, a spectral method that incorporates a set equivariant network to account for a graph's structure. PointSpectrum enhances the efficiency and expressiveness of spectral methods, while it outperforms or competes with state-of-the-art GRL methods. Overall, PointSpectrum addresses over smoothing by employing a graph filter and captures a graph's structure through set equivariance, lying on the intersection of GNNs and spectral methods. Our findings are promising for the benefits and applicability of this architectural shift for spectral methods and GRL.",0
"Modern graph data mining and learning tasks require Graph Representation Learning (GRL) to capture a graph's structural information and use it with node and edge attributes to compute low-dimensional representations. However, when many Graph Neural Network (GNN) layers are stacked, GNNs suffer from over smoothing. Spectral methods based on graph filtering have emerged to address over smoothing, but traditional neural networks cannot efficiently exploit the structure of graph data. To address this issue, we propose PointSpectrum, a spectral method that incorporates a set equivariant network to capture a graph's structure. PointSpectrum outperforms or competes with state-of-the-art GRL methods and enhances the efficiency and expressiveness of spectral methods. PointSpectrum lies at the intersection of GNNs and spectral methods, addressing over smoothing by employing a graph filter and capturing a graph's structure through set equivariance. Our findings suggest that this architectural shift is promising for the benefits and applicability of spectral methods and GRL.",1
"In this article, we propose a new variational approach to learn private and/or fair representations. This approach is based on the Lagrangians of a new formulation of the privacy and fairness optimization problems that we propose. In this formulation, we aim to generate representations of the data that keep a prescribed level of the relevant information that is not shared by the private or sensitive data, while minimizing the remaining information they keep. The proposed approach (i) exhibits the similarities of the privacy and fairness problems, (ii) allows us to control the trade-off between utility and privacy or fairness through the Lagrange multiplier parameter, and (iii) can be comfortably incorporated to common representation learning algorithms such as the VAE, the $\beta$-VAE, the VIB, or the nonlinear IB.",0
"A fresh variational method for acquiring private and/or fair representations is introduced in this article. Our method employs the Lagrangians of a novel formulation of the privacy and fairness optimization problems. Our aim is to generate data representations that maintain a predetermined amount of relevant information that is not shared with private or sensitive data, while minimizing the remaining information they hold. Our proposed approach reveals the similarities between privacy and fairness issues, enables us to regulate the balance between utility and privacy or fairness via the Lagrange multiplier parameter, and can be easily integrated into popular representation learning algorithms such as the VAE, the $\beta$-VAE, the VIB, or the nonlinear IB.",1
"Multi-view representation learning captures comprehensive information from multiple views of a shared context. Recent works intuitively apply contrastive learning (CL) to learn representations, regarded as a pairwise manner, which is still scalable: view-specific noise is not filtered in learning view-shared representations; the fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated; and evenly measuring the similarities between terms might interfere with optimization. Importantly, few works research the theoretical framework of generalized self-supervised multi-view learning, especially for more than two views. To this end, we rethink the existing multi-view learning paradigm from the information theoretical perspective and then propose a novel information theoretical framework for generalized multi-view learning. Guided by it, we build a multi-view coding method with a three-tier progressive architecture, namely Information theory-guided heuristic Progressive Multi-view Coding (IPMC). In the distribution-tier, IPMC aligns the distribution between views to reduce view-specific noise. In the set-tier, IPMC builds self-adjusted pools for contrasting, which utilizes a view filter to adaptively modify the pools. Lastly, in the instance-tier, we adopt a designed unified loss to learn discriminative representations and reduce the gradient interference. Theoretically and empirically, we demonstrate the superiority of IPMC over state-of-the-art methods.",0
"The concept of multi-view representation learning involves gathering vast information from multiple perspectives of a shared context. Recent studies have utilized contrastive learning (CL) to learn representations in a pairwise manner, which is scalable but has its limitations. For instance, the learning of view-shared representations fails to filter out view-specific noise. Additionally, negative pairs with terms from the same class as the positive are treated the same as real negative pairs, while evenly measuring similarities between terms may interfere with optimization. Few studies have explored the theoretical framework of generalized self-supervised multi-view learning, particularly for more than two views. In light of this, we have reevaluated the existing multi-view learning paradigm from an information theoretical viewpoint and proposed a new information theoretical framework for generalized multi-view learning. This has led to the creation of a three-tier progressive architecture multi-view coding method called Information theory-guided heuristic Progressive Multi-view Coding (IPMC). In the distribution-tier, IPMC minimizes view-specific noise by aligning views' distribution. In the set-tier, self-adjusted pools are created for contrasting using a view filter to modify the pools adaptively. Lastly, in the instance-tier, a unified loss is used to learn discriminative representations and minimize gradient interference. Our theoretical and empirical findings demonstrate that IPMC outperforms other state-of-the-art methods.",1
"The elementary operation of cropping underpins nearly every computer vision system, ranging from data augmentation and translation invariance to computational photography and representation learning. This paper investigates the subtle traces introduced by this operation. For example, despite refinements to camera optics, lenses will leave behind certain clues, notably chromatic aberration and vignetting. Photographers also leave behind other clues relating to image aesthetics and scene composition. We study how to detect these traces, and investigate the impact that cropping has on the image distribution. While our aim is to dissect the fundamental impact of spatial crops, there are also a number of practical implications to our work, such as revealing faulty photojournalism and equipping neural network researchers with a better understanding of shortcut learning. Code is available at https://github.com/basilevh/dissecting-image-crops.",0
"This paper delves into the basic function of cropping and its significance in various computer vision systems, including computational photography, data augmentation, and translation invariance. The study aims to explore the subtle indications that cropping can cause, such as chromatic aberration and vignetting, which can be left behind by camera lenses. Other clues, such as image aesthetics and scene composition, can also be left behind by photographers. The research aims to detect these traces and understand the impact of cropping on the image distribution. Although the primary goal is to analyze the fundamental impact of spatial crops, the study also has practical implications, such as detecting faulty photojournalism and enhancing the understanding of shortcut learning by neural network researchers. The code for this research is available at https://github.com/basilevh/dissecting-image-crops.",1
Self-supervised learning provides an opportunity to explore unlabeled chest X-rays and their associated free-text reports accumulated in clinical routine without manual supervision. This paper proposes a Joint Image Text Representation Learning Network (JoImTeRNet) for pre-training on chest X-ray images and their radiology reports. The model was pre-trained on both the global image-sentence level and the local image region-word level for visual-textual matching. Both are bidirectionally constrained on Cross-Entropy based and ranking-based Triplet Matching Losses. The region-word matching is calculated using the attention mechanism without direct supervision about their mapping. The pre-trained multi-modal representation learning paves the way for downstream tasks concerning image and/or text encoding. We demonstrate the representation learning quality by cross-modality retrievals and multi-label classifications on two datasets: OpenI-IU and MIMIC-CXR,0
"This paper introduces a Joint Image Text Representation Learning Network (JoImTeRNet) that utilizes self-supervised learning to pre-train on unlabeled chest X-rays and their corresponding free-text reports. The model is trained on both global image-sentence and local image region-word levels, using Cross-Entropy and ranking-based Triplet Matching Losses in a bidirectionally constrained manner. The region-word matching is calculated through an attention mechanism without direct supervision. The pre-trained multi-modal representation learning can be applied to downstream tasks involving image and/or text encoding. The effectiveness of the representation learning is demonstrated through cross-modality retrievals and multi-label classifications on two datasets: OpenI-IU and MIMIC-CXR.",1
"Pre-training visual and textual representations from large-scale image-text pairs is becoming a standard approach for many downstream vision-language tasks. The transformer-based models learn inter and intra-modal attention through a list of self-supervised learning tasks. This paper proposes LAViTeR, a novel architecture for visual and textual representation learning. The main module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks, GAN-based image synthesis and Image Captioning. We also propose a new evaluation metric measuring the similarity between the learnt visual and textual embedding. The experimental results on two public datasets, CUB and MS-COCO, demonstrate superior visual and textual representation alignment in the joint feature embedding space",0
"Many downstream vision-language tasks now use pre-training visual and textual representations from large-scale image-text pairs. Transformer-based models acquire inter and intra-modal attention through self-supervised learning tasks. In this paper, the authors introduce a new architecture called LAViTeR for visual and textual representation learning. The main module is the Visual Textual Alignment (VTA), which is supported by two auxiliary tasks: GAN-based image synthesis and Image Captioning. Additionally, a new evaluation metric is proposed to assess the similarity between the learned visual and textual embedding. The experimental results on two public datasets, CUB and MS-COCO, demonstrate that the joint feature embedding space has superior visual and textual representation alignment.",1
"For the Domain Generalization (DG) problem where the hypotheses are composed of a common representation function followed by a labeling function, we point out a shortcoming in existing approaches that fail to explicitly optimize for a term, appearing in a well-known and widely adopted upper bound to the risk on the unseen domain, that is dependent on the representation to be learned. To this end, we first derive a novel upper bound to the prediction risk. We show that imposing a mild assumption on the representation to be learned, namely manifold restricted invertibility, is sufficient to deal with this issue. Further, unlike existing approaches, our novel upper bound doesn't require the assumption of Lipschitzness of the loss function. In addition, the distributional discrepancy in the representation space is handled via the Wasserstein-2 barycenter cost. In this context, we creatively leverage old and recent transport inequalities, which link various optimal transport metrics, in particular the $L^1$ distance (also known as the total variation distance) and the Wasserstein-2 distances, with the Kullback-Liebler divergence. These analyses and insights motivate a new representation learning cost for DG that additively balances three competing objectives: 1) minimizing classification error across seen domains via cross-entropy, 2) enforcing domain-invariance in the representation space via the Wasserstein-2 barycenter cost, and 3) promoting non-degenerate, nearly-invertible representation via one of two mechanisms, viz., an autoencoder-based reconstruction loss or a mutual information loss. It is to be noted that the proposed algorithms completely bypass the use of any adversarial training mechanism that is typical of many current domain generalization approaches. Simulation results on several standard datasets demonstrate superior performance compared to several well-known DG algorithms.",0
"In the context of the Domain Generalization (DG) problem, which involves hypotheses consisting of a common representation function followed by a labeling function, we identify a deficiency in existing methods that fail to explicitly optimize for a term dependent on the representation to be learned, which appears in a widely accepted upper bound to the risk on the unseen domain. To address this issue, we introduce a new upper bound to the prediction risk and propose an assumption on the representation to be learned, called manifold restricted invertibility, which is sufficient to handle this problem. Unlike previous approaches, our new upper bound does not require the assumption of Lipschitzness of the loss function. Additionally, we handle the distributional discrepancy in the representation space using the Wasserstein-2 barycenter cost, and leverage transport inequalities to link various optimal transport metrics, including the $L^1$ distance and the Wasserstein-2 distances, with the Kullback-Liebler divergence. These insights lead to a new representation learning cost for DG that balances three competing objectives: minimizing classification error across seen domains, enforcing domain-invariance in the representation space, and promoting non-degenerate, nearly-invertible representation. Our proposed algorithms do not rely on adversarial training mechanisms, which are common in many current domain generalization approaches. Simulation results on standard datasets demonstrate superior performance compared to several well-known DG algorithms.",1
"Neural networks inspired by differential equations have proliferated for the past several years. Neural ordinary differential equations (NODEs) and neural controlled differential equations (NCDEs) are two representative examples of them. In theory, NCDEs provide better representation learning capability for time-series data than NODEs. In particular, it is known that NCDEs are suitable for processing irregular time-series data. Whereas NODEs have been successfully extended after adopting attention, however, it had not been studied yet how to integrate attention into NCDEs. To this end, we present the method of Attentive Neural Controlled Differential Equations (ANCDEs) for time-series classification and forecasting, where dual NCDEs are used: one for generating attention values, and the other for evolving hidden vectors for a downstream machine learning task. We conduct experiments with three real-world time-series datasets and 10 baselines. After dropping some values, we also conduct irregular time-series experiments. Our method consistently shows the best accuracy in all cases by non-trivial margins. Our visualizations also show that the presented attention mechanism works as intended by focusing on crucial information.",0
"For the past few years, there has been a proliferation of neural networks based on differential equations. Two prominent examples are neural ordinary differential equations (NODEs) and neural controlled differential equations (NCDEs). While NODEs have been successfully enhanced with attention, NCDEs are believed to offer superior representation learning capabilities for time-series data, particularly for irregular series. However, integrating attention into NCDEs has not yet been explored. In this study, we introduce Attentive Neural Controlled Differential Equations (ANCDEs) for time-series classification and forecasting, which uses dual NCDEs to generate attention values and evolve hidden vectors for downstream tasks. We perform experiments on three real-world datasets, including irregular series, and compare our results to 10 baselines. Our method consistently outperforms the other models by a significant margin, and our visualizations demonstrate that the attention mechanism effectively focuses on critical information.",1
"Catastrophic forgetting of previously learned knowledge while learning new tasks is a widely observed limitation of contemporary neural networks. Although many continual learning methods are proposed to mitigate this drawback, the main question remains unanswered: what is the root cause of catastrophic forgetting? In this work, we aim at answering this question by posing and validating a set of research hypotheses related to the specificity of representations built internally by neural models. More specifically, we design a set of empirical evaluations that compare the robustness of representations in discriminative and generative models against catastrophic forgetting. We observe that representations learned by discriminative models are more prone to catastrophic forgetting than their generative counterparts, which sheds new light on the advantages of developing generative models for continual learning. Finally, our work opens new research pathways and possibilities to adopt generative models in continual learning beyond mere replay mechanisms.",0
"Contemporary neural networks often suffer from catastrophic forgetting, which erases previously learned knowledge while learning new tasks. Although various methods have been proposed to address this problem, the root cause of catastrophic forgetting remains unclear. This study aims to answer this question by testing a set of research hypotheses that focus on the specificity of representations within neural models. We conduct empirical evaluations that compare the resilience of representations in discriminative and generative models against catastrophic forgetting. Our findings reveal that discriminative models are more susceptible to catastrophic forgetting than generative models, highlighting the benefits of using generative models for continual learning. Furthermore, this study opens up new avenues for research and suggests that generative models can be utilized beyond simple replay mechanisms in continual learning.",1
"Video advertisement content structuring aims to segment a given video advertisement and label each segment on various dimensions, such as presentation form, scene, and style. Different from real-life videos, video advertisements contain sufficient and useful multi-modal content like caption and speech, which provides crucial video semantics and would enhance the structuring process. In this paper, we propose a multi-modal encoder to learn multi-modal representation from video advertisements by interacting between video-audio and text. Based on multi-modal representation, we then apply Boundary-Matching Network to generate temporal proposals. To make the proposals more accurate, we refine generated proposals by scene-guided alignment and re-ranking. Finally, we incorporate proposal located embeddings into the introduced multi-modal encoder to capture temporal relationships between local features of each proposal and global features of the whole video for classification. Experimental results show that our method achieves significantly improvement compared with several baselines and Rank 1 on the task of Multi-modal Ads Video Understanding in ACM Multimedia 2021 Grand Challenge. Ablation study further shows that leveraging multi-modal content like caption and speech in video advertisements significantly improve the performance.",0
"The goal of structuring video advertisement content is to divide it into segments and identify each segment based on various dimensions such as presentation form, scene, and style. Unlike real-life videos, video advertisements have multi-modal content like captions and speech that provide important video semantics and can enhance the structuring process. This paper proposes a multi-modal encoder that learns multi-modal representation from video advertisements by interacting between video-audio and text. The Boundary-Matching Network is then applied to generate temporal proposals, which are refined by scene-guided alignment and re-ranking to improve accuracy. Proposal-located embeddings are incorporated into the multi-modal encoder to capture temporal relationships between local and global features for classification. Experimental results demonstrate that our approach achieves a significant improvement compared to several baselines and ranks first in the Multi-modal Ads Video Understanding task in the ACM Multimedia 2021 Grand Challenge. The ablation study further confirms that leveraging multi-modal content like captions and speech in video advertisements considerably enhances performance.",1
"Event cameras are novel sensors that perceive the per-pixel intensity changes and output asynchronous event streams with high dynamic range and less motion blur. It has been shown that events alone can be used for end-task learning, \eg, semantic segmentation, based on encoder-decoder-like networks. However, as events are sparse and mostly reflect edge information, it is difficult to recover original details merely relying on the decoder. Moreover, most methods resort to pixel-wise loss alone for supervision, which might be insufficient to fully exploit the visual details from sparse events, thus leading to less optimal performance. In this paper, we propose a simple yet flexible two-stream framework named Dual Transfer Learning (DTL) to effectively enhance the performance on the end-tasks without adding extra inference cost. The proposed approach consists of three parts: event to end-task learning (EEL) branch, event to image translation (EIT) branch, and transfer learning (TL) module that simultaneously explores the feature-level affinity information and pixel-level knowledge from the EIT branch to improve the EEL branch. This simple yet novel method leads to strong representation learning from events and is evidenced by the significant performance boost on the end-tasks such as semantic segmentation and depth estimation.",0
"Novel sensors called event cameras output asynchronous event streams with high dynamic range and less motion blur by perceiving per-pixel intensity changes. These events can be used for end-task learning, but as they mostly reflect edge information, recovering original details using the decoder alone is difficult. Additionally, pixel-wise loss alone for supervision may not fully exploit visual details from sparse events, leading to less optimal performance. To address these issues, we propose a flexible two-stream framework called Dual Transfer Learning (DTL) that enhances end-task performance without adding extra inference cost. The approach includes an event to end-task learning (EEL) branch, an event to image translation (EIT) branch, and a transfer learning (TL) module that explores feature-level affinity information and pixel-level knowledge from the EIT branch to improve the EEL branch. This simple yet novel method leads to strong representation learning from events and significantly boosts performance on end-tasks like semantic segmentation and depth estimation.",1
"Self-organization of complex morphological patterns from local interactions is a fascinating phenomenon in many natural and artificial systems. In the artificial world, typical examples of such morphogenetic systems are cellular automata. Yet, their mechanisms are often very hard to grasp and so far scientific discoveries of novel patterns have primarily been relying on manual tuning and ad hoc exploratory search. The problem of automated diversity-driven discovery in these systems was recently introduced [26, 62], highlighting that two key ingredients are autonomous exploration and unsupervised representation learning to describe ""relevant"" degrees of variations in the patterns. In this paper, we motivate the need for what we call Meta-diversity search, arguing that there is not a unique ground truth interesting diversity as it strongly depends on the final observer and its motives. Using a continuous game-of-life system for experiments, we provide empirical evidences that relying on monolithic architectures for the behavioral embedding design tends to bias the final discoveries (both for hand-defined and unsupervisedly-learned features) which are unlikely to be aligned with the interest of a final end-user. To address these issues, we introduce a novel dynamic and modular architecture that enables unsupervised learning of a hierarchy of diverse representations. Combined with intrinsically motivated goal exploration algorithms, we show that this system forms a discovery assistant that can efficiently adapt its diversity search towards preferences of a user using only a very small amount of user feedback.",0
"The phenomenon of self-organization of complex morphological patterns through local interactions is intriguing in both natural and artificial systems. In the artificial realm, cellular automata are a common example of such morphogenetic systems. However, understanding their mechanisms can be challenging, and discovering new patterns has primarily relied on manual tuning and exploratory search. Recently, the problem of automated diversity-driven discovery in these systems was introduced, emphasizing the importance of autonomous exploration and unsupervised representation learning. This paper proposes the need for Meta-diversity search, arguing that there is no unique ground truth for interesting diversity as it depends on the final observer and their motives. Empirical evidence from experiments using a continuous game-of-life system suggests that relying on monolithic architectures for behavioral embedding design can bias final discoveries, regardless of whether the features are hand-defined or unsupervisedly-learned. To address these issues, a novel dynamic and modular architecture that enables unsupervised learning of a hierarchy of diverse representations is introduced. Combined with intrinsically motivated goal exploration algorithms, this system serves as a discovery assistant that can adapt diversity search towards user preferences with minimal feedback.",1
"Extensive Unsupervised Domain Adaptation (UDA) studies have shown great success in practice by learning transferable representations across a labeled source domain and an unlabeled target domain with deep models. However, previous works focus on improving the generalization ability of UDA models on clean examples without considering the adversarial robustness, which is crucial in real-world applications. Conventional adversarial training methods are not suitable for the adversarial robustness on the unlabeled target domain of UDA since they train models with adversarial examples generated by the supervised loss function. In this work, we leverage intermediate representations learned by multiple robust ImageNet models to improve the robustness of UDA models. Our method works by aligning the features of the UDA model with the robust features learned by ImageNet pre-trained models along with domain adaptation training. It utilizes both labeled and unlabeled domains and instills robustness without any adversarial intervention or label requirement during domain adaptation training. Experimental results show that our method significantly improves adversarial robustness compared to the baseline while keeping clean accuracy on various UDA benchmarks.",0
"Studies on Extensive Unsupervised Domain Adaptation (UDA) have demonstrated practical success in transferring representations across a labeled source domain and an unlabeled target domain with deep models. However, previous research has concentrated on enhancing the UDA model's generalization ability on clean examples and has overlooked adversarial robustness, which is essential in real-world applications. Traditional adversarial training techniques are unsuitable for UDA's adversarial robustness on the unlabeled target domain since they train models with adversarial examples generated by the supervised loss function. This study employs intermediate representations learned by multiple robust ImageNet models to enhance the robustness of UDA models. Our approach aligns the features of the UDA model with the robust features learned by ImageNet pre-trained models during domain adaptation training to instill robustness without requiring any adversarial intervention or label. Experimental findings demonstrate that our method significantly improves adversarial robustness while maintaining clean accuracy on multiple UDA benchmarks compared to the baseline.",1
"Most deep neural networks (DNNs) based ultrasound (US) medical image analysis models use pretrained backbones (e.g., ImageNet) for better model generalization. However, the domain gap between natural and medical images causes an inevitable performance bottleneck. To alleviate this problem, an US dataset named US-4 is constructed for direct pretraining on the same domain. It contains over 23,000 images from four US video sub-datasets. To learn robust features from US-4, we propose an US semi-supervised contrastive learning method, named USCL, for pretraining. In order to avoid high similarities between negative pairs as well as mine abundant visual features from limited US videos, USCL adopts a sample pair generation method to enrich the feature involved in a single step of contrastive optimization. Extensive experiments on several downstream tasks show the superiority of USCL pretraining against ImageNet pretraining and other state-of-the-art (SOTA) pretraining approaches. In particular, USCL pretrained backbone achieves fine-tuning accuracy of over 94% on POCUS dataset, which is 10% higher than 84% of the ImageNet pretrained model. The source codes of this work are available at https://github.com/983632847/USCL.",0
"Many DNNs that analyze medical images from ultrasound (US) rely on pretrained backbones, such as ImageNet, to improve model generalization. However, the gap between natural and medical image domains can lead to performance issues. To address this, a dataset called US-4 was created, consisting of over 23,000 images from four US video sub-datasets, for direct pretraining within the same domain. To effectively learn features from US-4, a pretraining method called USCL is proposed, which utilizes US semi-supervised contrastive learning. To prevent high similarities between negative pairs and extract visual features from limited US videos, USCL employs a sample pair generation approach to enhance contrastive optimization in a single step. Results from multiple experiments on downstream tasks demonstrate USCL pretraining's superiority over ImageNet pretraining and other state-of-the-art approaches. Notably, the USCL pretrained backbone achieves a fine-tuning accuracy of over 94% on the POCUS dataset, which is 10% higher than that of the ImageNet pretrained model. The source codes for this work are available on https://github.com/983632847/USCL.",1
"To date, various 3D scene understanding tasks still lack practical and generalizable pre-trained models, primarily due to the intricate nature of 3D scene understanding tasks and their immense variations introduced by camera views, lighting, occlusions, etc. In this paper, we tackle this challenge by introducing a spatio-temporal representation learning (STRL) framework, capable of learning from unlabeled 3D point clouds in a self-supervised fashion. Inspired by how infants learn from visual data in the wild, we explore the rich spatio-temporal cues derived from the 3D data. Specifically, STRL takes two temporally-correlated frames from a 3D point cloud sequence as the input, transforms it with the spatial data augmentation, and learns the invariant representation self-supervisedly. To corroborate the efficacy of STRL, we conduct extensive experiments on three types (synthetic, indoor, and outdoor) of datasets. Experimental results demonstrate that, compared with supervised learning methods, the learned self-supervised representation facilitates various models to attain comparable or even better performances while capable of generalizing pre-trained models to downstream tasks, including 3D shape classification, 3D object detection, and 3D semantic segmentation. Moreover, the spatio-temporal contextual cues embedded in 3D point clouds significantly improve the learned representations.",0
"A lack of practical and generalizable pre-trained models has hindered progress in various 3D scene understanding tasks due to their complex nature and the many variations introduced by factors such as camera views, lighting, and occlusions. This paper addresses this challenge by introducing a spatio-temporal representation learning (STRL) framework that can learn from unlabeled 3D point clouds in a self-supervised manner. Inspired by how infants learn from visual data, the framework explores the rich spatio-temporal cues derived from the 3D data. The approach takes two temporally-correlated frames from a 3D point cloud sequence as input, applies spatial data augmentation, and learns the invariant representation self-supervisedly. Extensive experiments on synthetic, indoor, and outdoor datasets demonstrate that the learned self-supervised representation facilitates various models to achieve comparable or better performance and generalize pre-trained models to downstream tasks such as 3D shape classification, 3D object detection, and 3D semantic segmentation. The spatio-temporal contextual cues embedded in 3D point clouds are shown to significantly improve the learned representations.",1
"In the training of deep learning models, how the model parameters are initialized greatly affects the model performance, sample efficiency, and convergence speed. Representation learning for model initialization has recently been actively studied in the remote sensing field. In particular, the appearance characteristics of the imagery obtained using the a synthetic aperture radar (SAR) sensor are quite different from those of general electro-optical (EO) images, and thus representation learning is even more important in remote sensing domain. Motivated from contrastive multiview coding, we propose multi-modal representation learning for SAR semantic segmentation. Unlike previous studies, our method jointly uses EO imagery, SAR imagery, and a label mask. Several experiments show that our approach is superior to the existing methods in model performance, sample efficiency, and convergence speed.",0
"The initialization of model parameters has a significant impact on the performance, efficiency, and speed of deep learning models during training. Recently, the remote sensing field has been actively researching representation learning for model initialization. Specifically, the unique appearance characteristics of synthetic aperture radar (SAR) imagery requires a greater emphasis on representation learning in remote sensing. Our proposed approach for SAR semantic segmentation incorporates multi-modal representation learning, utilizing both SAR and electro-optical (EO) imagery, as well as a label mask. Our experiments demonstrate that our method outperforms existing approaches in terms of model performance, sample efficiency, and convergence speed.",1
"We present a novel technique for self-supervised video representation learning by: (a) decoupling the learning objective into two contrastive subtasks respectively emphasizing spatial and temporal features, and (b) performing it hierarchically to encourage multi-scale understanding. Motivated by their effectiveness in supervised learning, we first introduce spatial-temporal feature learning decoupling and hierarchical learning to the context of unsupervised video learning. We show by experiments that augmentations can be manipulated as regularization to guide the network to learn desired semantics in contrastive learning, and we propose a way for the model to separately capture spatial and temporal features at multiple scales. We also introduce an approach to overcome the problem of divergent levels of instance invariance at different hierarchies by modeling the invariance as loss weights for objective re-weighting. Experiments on downstream action recognition benchmarks on UCF101 and HMDB51 show that our proposed Hierarchically Decoupled Spatial-Temporal Contrast (HDC) makes substantial improvements over directly learning spatial-temporal features as a whole and achieves competitive performance when compared with other state-of-the-art unsupervised methods. Code will be made available.",0
"Our study presents a fresh technique for self-supervised video representation learning. This involves breaking down the learning objective into two contrastive subtasks, which focus on spatial and temporal features respectively. We apply a hierarchical approach to encourage multi-scale understanding. Drawing inspiration from the success of supervised learning, we introduce spatial-temporal feature learning decoupling and hierarchical learning to unsupervised video learning. Our experiments reveal that augmentations can be utilized as regularization to guide the network in learning desired semantics in contrastive learning. We propose a means of separately capturing spatial and temporal features at multiple scales. Additionally, we address the issue of divergent levels of instance invariance at different hierarchies by modeling the invariance as loss weights for objective re-weighting. Our proposed Hierarchically Decoupled Spatial-Temporal Contrast (HDC) produces substantial improvements over directly learning spatial-temporal features as a whole in downstream action recognition benchmarks on UCF101 and HMDB51. Furthermore, our HDC method achieves competitive performance when compared with other state-of-the-art unsupervised methods. We will make the code available.",1
"Autoencoders are commonly used in representation learning. They consist of an encoder and a decoder, which provide a straightforward way to map $n$-dimensional data in input space to a lower $m$-dimensional representation space and back. The decoder itself defines an $m$-dimensional manifold in input space. Inspired by manifold learning, we show that the decoder can be trained on its own by learning the representations of the training samples along with the decoder weights using gradient descent. A sum-of-squares loss then corresponds to optimizing the manifold to have the smallest Euclidean distance to the training samples, and similarly for other loss functions. We derive expressions for the number of samples needed to specify the encoder and decoder and show that the decoder generally requires much less training samples to be well-specified compared to the encoder. We discuss training of autoencoders in this perspective and relate to previous work in the field that use noisy training examples and other types of regularization. On the natural image data sets MNIST and CIFAR10, we demonstrate that the decoder is much better suited to learn a low-dimensional representation, especially when trained on small data sets. Using simulated gene regulatory data, we further show that the decoder alone leads to better generalization and meaningful representations. Our approach of training the decoder alone facilitates representation learning even on small data sets and can lead to improved training of autoencoders. We hope that the simple analyses presented will also contribute to an improved conceptual understanding of representation learning.",0
"Representation learning often involves the use of autoencoders, which consist of an encoder and a decoder that can map high-dimensional input data to a lower-dimensional representation space and back. The decoder defines an $m$-dimensional manifold in the input space, and can be trained separately by learning from the training samples and optimizing the loss function. The number of samples required to specify the encoder and decoder differs, with the decoder requiring fewer samples to be well-specified. We discuss the training of autoencoders in this context, including previous work that uses noisy training examples and regularization. We demonstrate the usefulness of training the decoder alone on small data sets with natural image data sets and simulated gene regulatory data, showing better generalization and meaningful representations. Our approach can improve the training of autoencoders and contribute to a better understanding of representation learning.",1
"Graph representation learning has achieved great success in many areas, including e-commerce, chemistry, biology, etc. However, the fundamental problem of choosing the appropriate dimension of node embedding for a given graph still remains unsolved. The commonly used strategies for Node Embedding Dimension Selection (NEDS) based on grid search or empirical knowledge suffer from heavy computation and poor model performance. In this paper, we revisit NEDS from the perspective of minimum entropy principle. Subsequently, we propose a novel Minimum Graph Entropy (MinGE) algorithm for NEDS with graph data. To be specific, MinGE considers both feature entropy and structure entropy on graphs, which are carefully designed according to the characteristics of the rich information in them. The feature entropy, which assumes the embeddings of adjacent nodes to be more similar, connects node features and link topology on graphs. The structure entropy takes the normalized degree as basic unit to further measure the higher-order structure of graphs. Based on them, we design MinGE to directly calculate the ideal node embedding dimension for any graph. Finally, comprehensive experiments with popular Graph Neural Networks (GNNs) on benchmark datasets demonstrate the effectiveness and generalizability of our proposed MinGE.",0
"The use of graph representation learning has been successful in various fields such as e-commerce, chemistry, and biology. However, determining the appropriate dimension of node embedding for a given graph remains an unresolved issue. The current methods of Node Embedding Dimension Selection (NEDS) based on grid search or empirical knowledge are not efficient and have poor model performance. This paper proposes a new approach to NEDS by applying the minimum entropy principle. The Minimum Graph Entropy (MinGE) algorithm factors in both feature and structure entropy of graphs to calculate the ideal node embedding dimension for any graph. MinGE is designed to consider the distinct characteristics of graph data, including the similarity of adjacent nodes and the higher-order structure of graphs. Our comprehensive experiments with popular Graph Neural Networks (GNNs) on benchmark datasets show that MinGE is effective and can be applied to various graphs.",1
"Graph neural networks for heterogeneous graph embedding is to project nodes into a low-dimensional space by exploring the heterogeneity and semantics of the heterogeneous graph. However, on the one hand, most of existing heterogeneous graph embedding methods either insufficiently model the local structure under specific semantic, or neglect the heterogeneity when aggregating information from it. On the other hand, representations from multiple semantics are not comprehensively integrated to obtain versatile node embeddings. To address the problem, we propose a Heterogeneous Graph Neural Network with Multi-View Representation Learning (named MV-HetGNN) for heterogeneous graph embedding by introducing the idea of multi-view representation learning. The proposed model consists of node feature transformation, view-specific ego graph encoding and auto multi-view fusion to thoroughly learn complex structural and semantic information for generating comprehensive node representations. Extensive experiments on three real-world heterogeneous graph datasets show that the proposed MV-HetGNN model consistently outperforms all the state-of-the-art GNN baselines in various downstream tasks, e.g., node classification, node clustering, and link prediction.",0
"The aim of using graph neural networks for heterogeneous graph embedding is to reduce the dimensionality of nodes by considering the heterogeneity and semantics of the graph. However, current methods have limitations in terms of modeling the local structure under specific semantics or aggregating information from heterogeneous sources. Additionally, there is a lack of integration of representations from multiple semantics to obtain versatile node embeddings. To overcome these issues, we propose a novel Heterogeneous Graph Neural Network with Multi-View Representation Learning (MV-HetGNN) that employs multi-view representation learning. Our approach involves node feature transformation, view-specific ego graph encoding, and auto multi-view fusion to learn complex structural and semantic information for generating comprehensive node representations. Our experiments on three real-world heterogeneous graph datasets demonstrate that the proposed MV-HetGNN model outperforms all the state-of-the-art GNN baselines in various downstream tasks such as node classification, node clustering, and link prediction.",1
"Contrastive self-supervised learning (SSL) has achieved great success in unsupervised visual representation learning by maximizing the similarity between two augmented views of the same image (positive pairs) and simultaneously contrasting other different images (negative pairs). However, this type of methods, such as SimCLR and MoCo, relies heavily on a large number of negative pairs and thus requires either large batches or memory banks. In contrast, some recent non-contrastive SSL methods, such as BYOL and SimSiam, attempt to discard negative pairs by introducing asymmetry and show remarkable performance. Unfortunately, to avoid collapsed solutions caused by not using negative pairs, these methods require sophisticated asymmetry designs. In this paper, we argue that negative pairs are still necessary but one is sufficient, i.e., triplet is all you need. A simple triplet-based loss can achieve surprisingly good performance without requiring large batches or asymmetry. Moreover, we observe that unsupervised visual representation learning can gain significantly from randomness. Based on this observation, we propose a simple plug-in RandOm MApping (ROMA) strategy by randomly mapping samples into other spaces and enforcing these randomly projected samples to satisfy the same correlation requirement. The proposed ROMA strategy not only achieves the state-of-the-art performance in conjunction with the triplet-based loss, but also can further effectively boost other SSL methods.",0
"Unsupervised visual representation learning has seen great success through contrastive self-supervised learning (SSL), which involves maximizing the similarity between two augmented views of the same image (positive pairs) and contrasting with other different images (negative pairs). However, methods like SimCLR and MoCo require a large number of negative pairs, necessitating either large batches or memory banks. In contrast, non-contrastive SSL methods like BYOL and SimSiam discard negative pairs by introducing asymmetry but require complex designs to avoid collapsed solutions. This paper argues that negative pairs are still necessary but that a triplet is sufficient, and a simple triplet-based loss can achieve excellent performance without large batches or asymmetry. Additionally, randomness can significantly benefit unsupervised visual representation learning. Thus, the proposed RandOm MApping (ROMA) strategy randomly maps samples into other spaces and enforces correlation requirements, effectively boosting both the proposed triplet-based loss and other SSL methods.",1
"Classic machine learning methods are built on the $i.i.d.$ assumption that training and testing data are independent and identically distributed. However, in real scenarios, the $i.i.d.$ assumption can hardly be satisfied, rendering the sharp drop of classic machine learning algorithms' performances under distributional shifts, which indicates the significance of investigating the Out-of-Distribution generalization problem. Out-of-Distribution (OOD) generalization problem addresses the challenging setting where the testing distribution is unknown and different from the training. This paper serves as the first effort to systematically and comprehensively discuss the OOD generalization problem, from the definition, methodology, evaluation to the implications and future directions. Firstly, we provide the formal definition of the OOD generalization problem. Secondly, existing methods are categorized into three parts based on their positions in the whole learning pipeline, namely unsupervised representation learning, supervised model learning and optimization, and typical methods for each category are discussed in detail. We then demonstrate the theoretical connections of different categories, and introduce the commonly used datasets and evaluation metrics. Finally, we summarize the whole literature and raise some future directions for OOD generalization problem. The summary of OOD generalization methods reviewed in this survey can be found at http://out-of-distribution-generalization.com.",0
"Classic machine learning techniques rely on the assumption that training and testing data are independent and identically distributed ($i.i.d.$). However, this assumption is often not realistic in real-life scenarios, which can result in a significant drop in the performance of classic machine learning algorithms when faced with distributional shifts. Therefore, it is crucial to investigate the Out-of-Distribution (OOD) generalization problem, which deals with the challenge of testing data being unknown and different from the training data. This paper presents a comprehensive discussion of the OOD generalization problem, including its definition, methodology, evaluation, and future directions. The existing methods are classified into three categories: unsupervised representation learning, supervised model learning and optimization. We also discuss the theoretical connections between these categories and provide an overview of commonly used datasets and evaluation metrics. Finally, we summarize the literature on OOD generalization and suggest future research directions. The summary of OOD generalization methods reviewed in this survey can be accessed at http://out-of-distribution-generalization.com.",1
"Self-supervised representation learning has achieved remarkable success in recent years. By subverting the need for supervised labels, such approaches are able to utilize the numerous unlabeled images that exist on the Internet and in photographic datasets. Yet to build truly intelligent agents, we must construct representation learning algorithms that can learn not only from datasets but also learn from environments. An agent in a natural environment will not typically be fed curated data. Instead, it must explore its environment to acquire the data it will learn from. We propose a framework, curious representation learning (CRL), which jointly learns a reinforcement learning policy and a visual representation model. The policy is trained to maximize the error of the representation learner, and in doing so is incentivized to explore its environment. At the same time, the learned representation becomes stronger and stronger as the policy feeds it ever harder data to learn from. Our learned representations enable promising transfer to downstream navigation tasks, performing better than or comparably to ImageNet pretraining without using any supervision at all. In addition, despite being trained in simulation, our learned representations can obtain interpretable results on real images. Code is available at https://yilundu.github.io/crl/.",0
"In recent years, self-supervised representation learning has demonstrated exceptional achievements. By eliminating the requirement for supervised labels, these approaches can make use of the vast amount of unlabeled images present on the Internet and in photographic datasets. However, to create truly intelligent agents, representation learning algorithms must be developed that can learn not only from datasets but also from environments. Agents in natural environments cannot rely on curated data and must explore their surroundings to obtain the data they require for learning. To address this issue, we propose a framework called curious representation learning (CRL), which combines reinforcement learning policy with a visual representation model. The policy is trained to maximize the representation learner's error, which encourages exploration of the environment. Concurrently, the representation learner becomes stronger as the policy provides it with more challenging data to learn from. Our learned representations demonstrate promising transfer to downstream navigation tasks, outperforming or performing comparably to ImageNet pre-training without any supervision. Additionally, our learned representations yield interpretable results on real images, despite being trained in simulation. Code for our framework is available at https://yilundu.github.io/crl/.",1
"Node classification and link prediction are widely studied in graph representation learning. While both transductive node classification and link prediction operate over a single input graph, they have so far been studied separately. Node classification models take an input graph with node features and incomplete node labels, and implicitly assume that the graph is relationally complete, i.e., no edges are missing. By contrast, link prediction models are solely motivated by relational incompleteness of the input graphs, and do not typically leverage node features or classes. We propose a unifying perspective and study the problems of (i) transductive node classification over incomplete graphs and (ii) link prediction over graphs with node features, introduce a new dataset for this setting, WikiAlumni, and conduct an extensive benchmarking study.",0
"Graph representation learning is a field that extensively examines node classification and link prediction. Although both these techniques work with a single input graph, they have been researched separately until now. Node classification models assume that the input graph is relationally complete, meaning there are no missing edges, and use node features and incomplete node labels. Link prediction models, on the other hand, focus on the relational incompleteness of the input graphs and do not typically use node features or classes. In this study, we propose a unified approach to examine (i) transductive node classification over incomplete graphs and (ii) link prediction over graphs with node features, along with a new dataset called WikiAlumni. We also conduct an extensive benchmarking study.",1
"Generating font glyphs of consistent style from one or a few reference glyphs, i.e., font completion, is an important task in topographical design. As the problem is more well-defined than general image style transfer tasks, thus it has received interest from both vision and machine learning communities. Existing approaches address this problem as a direct image-to-image translation task. In this work, we innovate to explore the generation of font glyphs as 2D graphic objects with the graph as an intermediate representation, so that more intrinsic graphic properties of font styles can be captured. Specifically, we formulate a cross-modality cycled image-to-image model structure with a graph constructor between an image encoder and an image renderer. The novel graph constructor maps a glyph's latent code to its graph representation that matches expert knowledge, which is trained to help the translation task. Our model generates improved results than both image-to-image baseline and previous state-of-the-art methods for glyph completion. Furthermore, the graph representation output by our model also provides an intuitive interface for users to do local editing and manipulation. Our proposed cross-modality cycled representation learning has the potential to be applied to other domains with prior knowledge from different data modalities. Our code is available at https://github.com/VITA-Group/Font_Completion_Graph.",0
"Font completion, which involves generating font glyphs with consistent style using a small number of reference glyphs, is a crucial task in topographical design. As the problem is more specific than general image style transfer tasks, it has garnered attention from both the vision and machine learning communities. Existing methods approach this problem as a direct image-to-image translation task. However, in this study, we introduce a novel approach that generates font glyphs as 2D graphic objects using a graph as an intermediate representation. This approach captures the intrinsic graphic properties of font styles more effectively. Our model utilizes a cross-modality cycled image-to-image structure with a graph constructor that maps a glyph's latent code to its graph representation. This graph representation matches expert knowledge and helps with the translation task. Our approach generates better results than both image-to-image baseline and previous state-of-the-art methods for glyph completion. Moreover, the graph representation output by our model offers a user-friendly interface for local editing and manipulation. Our proposed cross-modality cycled representation learning has the potential to be applied to other domains that leverage prior knowledge from different data modalities. The code for our work is available at https://github.com/VITA-Group/Font_Completion_Graph.",1
"Graph Neural Networks have recently become a prevailing paradigm for various high-impact graph learning tasks. Existing efforts can be mainly categorized as spectral-based and spatial-based methods. The major challenge for the former is to find an appropriate graph filter to distill discriminative information from input signals for learning. Recently, attempts such as Graph Convolutional Network (GCN) leverage Chebyshev polynomial truncation to seek an approximation of graph filters and bridge these two families of methods. It has been shown in recent studies that GCN and its variants are essentially employing fixed low-pass filters to perform information denoising. Thus their learning capability is rather limited and may over-smooth node representations at deeper layers. To tackle these problems, we develop a novel graph neural network framework AdaGNN with a well-designed adaptive frequency response filter. At its core, AdaGNN leverages a simple but elegant trainable filter that spans across multiple layers to capture the varying importance of different frequency components for node representation learning. The inherent differences among different feature channels are also well captured by the filter. As such, it empowers AdaGNN with stronger expressiveness and naturally alleviates the over-smoothing problem. We empirically validate the effectiveness of the proposed framework on various benchmark datasets. Theoretical analysis is also provided to show the superiority of the proposed AdaGNN. The implementation of AdaGNN is available at \url{https://github.com/yushundong/AdaGNN}.",0
"For a variety of important graph learning tasks, Graph Neural Networks have recently emerged as a dominant approach. These efforts can be broadly classified as either spectral-based or spatial-based methods. The spectral-based methods face the key challenge of selecting a suitable graph filter to extract informative features from input signals for training. Recently, the Graph Convolutional Network (GCN) has used Chebyshev polynomial truncation to approximate graph filters, bridging the gap between these two categories of methods. However, GCN and its variations have been found to use fixed low-pass filters that limit their learning ability and may result in excessively smoothed node representations at deeper levels. To overcome these drawbacks, we propose AdaGNN, a novel graph neural network framework that incorporates a well-designed adaptive frequency response filter. AdaGNN employs a simple but sophisticated trainable filter that spans multiple layers to capture the varying importance of different frequency components for node representation learning. Furthermore, it effectively captures differences among feature channels. As a result, AdaGNN offers greater expressiveness and naturally addresses the problem of over-smoothing. We demonstrate the effectiveness of our approach on various benchmark datasets and provide theoretical analysis to support its superiority. The implementation of AdaGNN is available at \url{https://github.com/yushundong/AdaGNN}.",1
"Point clouds have attracted increasing attention. Significant progress has been made in methods for point cloud analysis, which often requires costly human annotation as supervision. To address this issue, we propose a novel self-contrastive learning for self-supervised point cloud representation learning, aiming to capture both local geometric patterns and nonlocal semantic primitives based on the nonlocal self-similarity of point clouds. The contributions are two-fold: on the one hand, instead of contrasting among different point clouds as commonly employed in contrastive learning, we exploit self-similar point cloud patches within a single point cloud as positive samples and otherwise negative ones to facilitate the task of contrastive learning. On the other hand, we actively learn hard negative samples that are close to positive samples for discriminative feature learning. Experimental results show that the proposed method achieves state-of-the-art performance on widely used benchmark datasets for self-supervised point cloud segmentation and transfer learning for classification.",0
"The focus on point clouds has increased, leading to advancements in point cloud analysis methods. However, these methods often require expensive human annotation for supervision. To address this issue, we introduce a novel self-contrastive learning approach for self-supervised point cloud representation learning. Our approach aims to capture both local geometric patterns and nonlocal semantic primitives based on the nonlocal self-similarity of point clouds. Our contributions are twofold: firstly, we use self-similar point cloud patches within a single point cloud as positive samples, instead of contrasting among different point clouds as commonly done in contrastive learning. Secondly, we actively learn hard negative samples that are close to positive samples for discriminative feature learning. Our proposed method achieves state-of-the-art performance on widely used benchmark datasets for self-supervised point cloud segmentation and transfer learning for classification, as demonstrated by our experimental results.",1
"Real-world visual recognition problems often exhibit long-tailed distributions, where the amount of data for learning in different categories shows significant imbalance. Standard classification models learned on such data distribution often make biased predictions towards the head classes while generalizing poorly to the tail classes. In this paper, we present two effective modifications of CNNs to improve network learning from long-tailed distribution. First, we present a Class Activation Map Calibration (CAMC) module to improve the learning and prediction of network classifiers, by enforcing network prediction based on important image regions. The proposed CAMC module highlights the correlated image regions across data and reinforces the representations in these areas to obtain a better global representation for classification. Furthermore, we investigate the use of normalized classifiers for representation learning in long-tailed problems. Our empirical study demonstrates that by simply scaling the outputs of the classifier with an appropriate scalar, we can effectively improve the classification accuracy on tail classes without losing the accuracy of head classes. We conduct extensive experiments to validate the effectiveness of our design and we set new state-of-the-art performance on five benchmarks, including ImageNet-LT, Places-LT, iNaturalist 2018, CIFAR10-LT, and CIFAR100-LT.",0
"In real-world scenarios, problems with visual recognition often exhibit distributions that are long-tailed, meaning that there is significant imbalance in the amount of data available for learning different categories. This can result in biased predictions towards the head classes and poor generalization to the tail classes when using standard classification models. This paper presents two modifications of CNNs that can improve network learning from long-tailed distributions. The first modification is a Class Activation Map Calibration (CAMC) module that enforces network prediction based on important image regions, highlighting correlated image regions and reinforcing their representations for better global classification. The second modification investigates the use of normalized classifiers for representation learning in long-tailed problems. By scaling the outputs of the classifier with an appropriate scalar, the classification accuracy on tail classes can be effectively improved without losing accuracy on head classes. Extensive experiments validate the proposed design, resulting in new state-of-the-art performance on five benchmarks, including ImageNet-LT, Places-LT, iNaturalist 2018, CIFAR10-LT, and CIFAR100-LT.",1
"We propose to apply non-linear representation learning to voxelwise rs-fMRI data. Learning the non-linear representations is done using a variational autoencoder (VAE). The VAE is trained on voxelwise rs-fMRI data and performs non-linear dimensionality reduction that retains meaningful information. The retention of information in the model's representations is evaluated using downstream age regression and sex classification tasks. The results on these tasks are highly encouraging and a linear regressor trained with the representations of our unsupervised model performs almost as well as a supervised neural network, trained specifically for age regression on the same dataset. The model is also evaluated with a schizophrenia diagnosis prediction task, to assess its feasibility as a dimensionality reduction method for neuropsychiatric datasets. These results highlight the potential for pre-training on a larger set of individuals who do not have mental illness, to improve the downstream neuropsychiatric task results. The pre-trained model is fine-tuned for a variable number of epochs on a schizophrenia dataset and we find that fine-tuning for 1 epoch yields the best results. This work therefore not only opens up non-linear dimensionality reduction for voxelwise rs-fMRI data but also shows that pre-training a deep learning model on voxelwise rs-fMRI datasets greatly increases performance even on smaller datasets. It also opens up the ability to look at the distribution of rs-fMRI time series in the latent space of the VAE for heterogeneous neuropsychiatric disorders like schizophrenia in future work. This can be complemented with the generative aspect of the model that allows us to reconstruct points from the model's latent space back into brain space and obtain an improved understanding of the relation that the VAE learns between subjects, timepoints, and a subject's characteristics.",0
"Our proposal involves utilizing a variational autoencoder (VAE) for non-linear representation learning on voxelwise rs-fMRI data. With the VAE trained on this data, it can perform non-linear dimensionality reduction while retaining meaningful information. We evaluate the model's representation retention by conducting age regression and sex classification tasks. The results indicate that our unsupervised model's representations perform almost as well as a supervised neural network trained for age regression on the same dataset. We also test the model's feasibility as a dimensionality reduction method for neuropsychiatric datasets through a schizophrenia diagnosis prediction task. The results suggest that pre-training the model on a larger set of individuals without mental illness can improve downstream neuropsychiatric task outcomes. By fine-tuning the pre-trained model on a schizophrenia dataset for one epoch, we achieve the best results. Our work not only introduces non-linear dimensionality reduction for voxelwise rs-fMRI data but also highlights the potential of pre-training deep learning models on these datasets to enhance performance, even on smaller datasets. Moreover, future work can explore the distribution of rs-fMRI time series in the VAE's latent space for heterogeneous neuropsychiatric disorders like schizophrenia, in addition to the model's generative aspect that allows us to reconstruct points from the latent space back into brain space for a better understanding of the VAE's learned relation between subjects, timepoints, and a subject's characteristics.",1
"Anomaly detection has been a challenging task given high-dimensional multivariate time series data generated by networked sensors and actuators in Cyber-Physical Systems (CPS). Besides the highly nonlinear, complex, and dynamic natures of such time series, the lack of labeled data impedes data exploitation in a supervised manner and thus prevents an accurate detection of abnormal phenomenons. On the other hand, the collected data at the edge of the network is often privacy sensitive and large in quantity, which may hinder the centralized training at the main server. To tackle these issues, we propose an unsupervised time series anomaly detection framework in a federated fashion to continuously monitor the behaviors of interconnected devices within a network and alerts for abnormal incidents so that countermeasures can be taken before undesired consequences occur. To be specific, we leave the training data distributed at the edge to learn a shared Variational Autoencoder (VAE) based on Convolutional Gated Recurrent Unit (ConvGRU) model, which jointly captures feature and temporal dependencies in the multivariate time series data for representation learning and downstream anomaly detection tasks. Experiments on three real-world networked sensor datasets illustrate the advantage of our approach over other state-of-the-art models. We also conduct extensive experiments to demonstrate the effectiveness of our detection framework under non-federated and federated settings in terms of overall performance and detection latency.",0
"Detecting anomalies in Cyber-Physical Systems (CPS) is difficult due to the complex and dynamic nature of the high-dimensional multivariate time series data generated by networked sensors and actuators. The lack of labeled data also makes it challenging to detect abnormal events accurately. Additionally, the privacy-sensitive and large quantity of data collected at the edge of the network can hinder centralized training at the main server. Our proposed solution is an unsupervised time series anomaly detection framework that operates in a federated manner to monitor the behavior of interconnected devices continuously. This framework learns a shared Variational Autoencoder (VAE) based on a Convolutional Gated Recurrent Unit (ConvGRU) model, which captures feature and temporal dependencies for representation learning and downstream anomaly detection tasks. Our experiments on three real-world networked sensor datasets demonstrate the effectiveness of our approach over other state-of-the-art models. We also conduct extensive experiments to show the performance and detection latency of our detection framework under non-federated and federated settings.",1
"Decomposition of the evidence lower bound (ELBO) objective of VAE used for density estimation revealed the deficiency of VAE for representation learning and suggested ways to improve the model. In this paper, we investigate whether we can get similar insights by decomposing the ELBO for semi-supervised classification using VAE model. Specifically, we show that mutual information between input and class labels decreases during maximization of ELBO objective. We propose a method to address this issue. We also enforce cluster assumption to aid in classification. Experiments on a diverse datasets verify that our method can be used to improve the classification performance of existing VAE based semi-supervised models. Experiments also show that, this can be achieved without sacrificing the generative power of the model.",0
"By breaking down the evidence lower bound (ELBO) objective of the VAE model used for density estimation, it was discovered that VAE had limitations for representation learning and that there were ways to enhance the model. This study aims to explore whether a similar outcome can be achieved by decomposing the ELBO for semi-supervised classification using VAE. Our research indicates that maximizing the ELBO objective leads to a decrease in mutual information between input and class labels. To tackle this issue, we suggest a method and incorporate the cluster assumption to improve classification. Our experiments demonstrate that our proposed method can enhance the classification performance of current VAE-based semi-supervised models without compromising the generative capabilities of the model.",1
"Unsupervised person re-identification (re-ID) remains a challenging task. While extensive research has focused on the framework design or loss function, we show in this paper that sampling strategy plays an equally important role. We analyze the reasons for differences in performance between various sampling strategies under the same framework and loss function. We suggest that deteriorated over-fitting is an important factor causing poor performance, and enhancing statistical stability can rectify this issue. Inspired by that, a simple yet effective approach is proposed, known as group sampling, which gathers groups of samples from the same class into a mini-batch. The model is thereby trained using normalized group samples, which helps to alleviate the effects associated with a single sample. Group sampling updates the pipeline of pseudo label generation by guaranteeing that samples are more efficiently divided into the correct classes. Group sampling regulates the representation learning process, which enhances statistical stability for feature representation in a progressive fashion. Qualitative and quantitative experiments on Market-1501, DukeMTMC-reID, and MSMT17 show that group sampling improves upon state-of-the-art methods by between 3.3%~6.1%. Code has been available at https://github.com/ucas-vg/GroupSampling.",0
"This paper highlights the difficulty of unsupervised person re-identification, despite extensive research on framework design and loss function. The authors emphasize the importance of sampling strategy in achieving better results and identify over-fitting as a significant reason for poor performance. To address this issue, the authors propose a simple and effective approach called group sampling, which involves grouping samples from the same class into a mini-batch and training the model using normalized group samples. This ensures that samples are efficiently divided into the correct classes and enhances statistical stability for feature representation. The authors conduct qualitative and quantitative experiments on three datasets and show that group sampling outperforms state-of-the-art methods by 3.3%~6.1%. The code for this approach is available at https://github.com/ucas-vg/GroupSampling.",1
"Deep learning has achieved remarkable success in medicalimage segmentation, but it usually requires a large numberof images labeled with fine-grained segmentation masks, andthe annotation of these masks can be very expensive andtime-consuming. Therefore, recent methods try to use un-supervised domain adaptation (UDA) methods to borrow in-formation from labeled data from other datasets (source do-mains) to a new dataset (target domain). However, due tothe absence of labels in the target domain, the performance ofUDA methods is much worse than that of the fully supervisedmethod. In this paper, we propose a weakly supervised do-main adaptation setting, in which we can partially label newdatasets with bounding boxes, which are easier and cheaperto obtain than segmentation masks. Accordingly, we proposea new weakly-supervised domain adaptation method calledBox-Adapt, which fully explores the fine-grained segmenta-tion mask in the source domain and the weak bounding boxin the target domain. Our Box-Adapt is a two-stage methodthat first performs joint training on the source and target do-mains, and then conducts self-training with the pseudo-labelsof the target domain. We demonstrate the effectiveness of ourmethod in the liver segmentation task. Weakly supervised do-main adaptation",0
"Despite its success in medical image segmentation, deep learning typically requires a large number of images with detailed segmentation masks, which can be costly and time-consuming to annotate. To address this issue, recent methods utilize unsupervised domain adaptation (UDA) techniques to transfer labeled data from source domains to target domains. However, UDA methods perform poorly when there are no labels in the target domain. To overcome this challenge, we propose a weakly supervised domain adaptation approach that relies on partially labeling new datasets with bounding boxes, which are less expensive and easier to obtain than segmentation masks. Our method, called Box-Adapt, leverages both the fine-grained segmentation masks in the source domain and the weak bounding boxes in the target domain. Box-Adapt consists of two stages: joint training on both domains and self-training with the pseudo-labels of the target domain. We demonstrate the effectiveness of our approach in liver segmentation.",1
"The abundance of data collected by sensors in Internet of Things (IoT) devices, and the success of deep neural networks in uncovering hidden patterns in time series data have led to mounting privacy concerns. This is because private and sensitive information can be potentially learned from sensor data by applications that have access to this data. In this paper, we aim to examine the tradeoff between utility and privacy loss by learning low-dimensional representations that are useful for data obfuscation. We propose deterministic and probabilistic transformations in the latent space of a variational autoencoder to synthesize time series data such that intrusive inferences are prevented while desired inferences can still be made with sufficient accuracy. In the deterministic case, we use a linear transformation to move the representation of input data in the latent space such that the reconstructed data is likely to have the same public attribute but a different private attribute than the original input data. In the probabilistic case, we apply the linear transformation to the latent representation of input data with some probability. We compare our technique with autoencoder-based anonymization techniques and additionally show that it can anonymize data in real time on resource-constrained edge devices.",0
"The collection of abundant data from Internet of Things (IoT) devices and the success of deep neural networks in identifying hidden patterns in time series data have raised concerns over the privacy of sensitive information. Applications that have access to sensor data may potentially learn private information, making privacy loss a growing issue. This paper aims to explore the balance between utility and privacy loss by studying low-dimensional representations that can be used for data obfuscation. We propose deterministic and probabilistic transformations in the latent space of a variational autoencoder to generate time series data that can prevent intrusive inferences while allowing desired inferences to be made with sufficient accuracy. In the deterministic case, a linear transformation is utilized to move the representation of input data in the latent space so that the reconstructed data is likely to have the same public attribute but a different private attribute than the original input data. In the probabilistic case, the linear transformation is applied to the latent representation of input data with some probability. Our technique is compared to autoencoder-based anonymization techniques and is shown to be capable of anonymizing data in real time on resource-constrained edge devices.",1
"Videos on the Internet are paired with pieces of text, such as titles and descriptions. This text typically describes the most important content in the video, such as the objects in the scene and the actions being performed. Based on this observation, we propose to use text as a method for learning video representations. To accomplish this, we propose a data collection process and use it to collect 70M video clips shared publicly on the Internet, and we then train a model to pair each video with its associated text. We evaluate the model on several down-stream action recognition tasks, including Kinetics, HMDB-51, and UCF-101. We find that this approach is an effective method of pre-training video representations. Specifically, it outperforms all existing methods for self-supervised and cross-modal video representation learning.",0
"The internet often pairs videos with accompanying text, such as titles and descriptions, that provide important information about the video's content, including the objects and actions featured. Our research suggests that text can be used as a means of teaching video representations. To prove this, we created a data collection process and gathered 70 million public video clips, which we then trained a model to pair with their corresponding text. We tested the model on various action recognition tasks, including Kinetics, HMDB-51, and UCF-101, and found that our method was more effective than previous self-supervised and cross-modal video representation learning techniques.",1
"Recently, transfer subspace learning based approaches have shown to be a valid alternative to unsupervised subspace clustering and temporal data clustering for human motion segmentation (HMS). These approaches leverage prior knowledge from a source domain to improve clustering performance on a target domain, and currently they represent the state of the art in HMS. Bucking this trend, in this paper, we propose a novel unsupervised model that learns a representation of the data and digs clustering information from the data itself. Our model is reminiscent of temporal subspace clustering, but presents two critical differences. First, we learn an auxiliary data matrix that can deviate from the initial data, hence confer more degrees of freedom to the coding matrix. Second, we introduce a regularization term for this auxiliary data matrix that preserves the local geometrical structure present in the high-dimensional space. The proposed model is efficiently optimized by using an original Alternating Direction Method of Multipliers (ADMM) formulation allowing to learn jointly the auxiliary data representation, a nonnegative dictionary and a coding matrix. Experimental results on four benchmark datasets for HMS demonstrate that our approach achieves significantly better clustering performance then state-of-the-art methods, including both unsupervised and more recent semi-supervised transfer learning approaches.",0
"In recent times, transfer subspace learning-based methods have emerged as an effective substitute for unsupervised subspace clustering and temporal data clustering in human motion segmentation (HMS). These methods utilize prior knowledge from a source domain to enhance clustering performance on a target domain, and are currently the most advanced in HMS. However, in this paper, we propose a new unsupervised model that learns data representation and extracts clustering information from the data itself. Our model resembles temporal subspace clustering but with two crucial differences. Firstly, we learn an auxiliary data matrix that can differ from the initial data, providing more freedom to the coding matrix. Secondly, we introduce a regularization term for this auxiliary data matrix to maintain the local geometrical structure present in the high-dimensional space. Our proposed model is efficiently optimized using an original Alternating Direction Method of Multipliers (ADMM) formulation, enabling joint learning of the auxiliary data representation, a nonnegative dictionary, and a coding matrix. We substantiate the effectiveness of our approach on four benchmark datasets for HMS, demonstrating significantly superior clustering performance compared to state-of-the-art unsupervised and recent semi-supervised transfer learning methods.",1
"As online service systems continue to grow in terms of complexity and volume, how service incidents are managed will significantly impact company revenue and user trust. Due to the cascading effect, cloud failures often come with an overwhelming number of incidents from dependent services and devices. To pursue efficient incident management, related incidents should be quickly aggregated to narrow down the problem scope. To this end, in this paper, we propose GRLIA, an incident aggregation framework based on graph representation learning over the cascading graph of cloud failures. A representation vector is learned for each unique type of incident in an unsupervised and unified manner, which is able to simultaneously encode the topological and temporal correlations among incidents. Thus, it can be easily employed for online incident aggregation. In particular, to learn the correlations more accurately, we try to recover the complete scope of failures' cascading impact by leveraging fine-grained system monitoring data, i.e., Key Performance Indicators (KPIs). The proposed framework is evaluated with real-world incident data collected from a large-scale online service system of Huawei Cloud. The experimental results demonstrate that GRLIA is effective and outperforms existing methods. Furthermore, our framework has been successfully deployed in industrial practice.",0
"The management of service incidents will have a significant impact on company revenue and user trust as online service systems become more complex and voluminous. Cloud failures often result in an overwhelming number of incidents from dependent services and devices, creating a cascading effect. Effective incident management requires the quick aggregation of related incidents to narrow down the problem scope. In this paper, we propose GRLIA, an incident aggregation framework that utilizes graph representation learning over the cascading graph of cloud failures to learn a representation vector for each unique type of incident in an unsupervised and unified manner. This vector can encode topological and temporal correlations among incidents, making it ideal for online incident aggregation. To improve correlation accuracy, we use fine-grained system monitoring data, such as Key Performance Indicators (KPIs), to recover the complete scope of failures' cascading impact. We evaluate GRLIA using real-world incident data from Huawei Cloud's large-scale online service system and demonstrate that it outperforms existing methods. Our framework has been successfully deployed in industrial practice.",1
"Autonomous driving has attracted much attention over the years but turns out to be harder than expected, probably due to the difficulty of labeled data collection for model training. Self-supervised learning (SSL), which leverages unlabeled data only for representation learning, might be a promising way to improve model performance. Existing SSL methods, however, usually rely on the single-centric-object guarantee, which may not be applicable for multi-instance datasets such as street scenes. To alleviate this limitation, we raise two issues to solve: (1) how to define positive samples for cross-view consistency and (2) how to measure similarity in multi-instance circumstances. We first adopt an IoU threshold during random cropping to transfer global-inconsistency to local-consistency. Then, we propose two feature alignment methods to enable 2D feature maps for multi-instance similarity measurement. Additionally, we adopt intra-image clustering with self-attention for further mining intra-image similarity and translation-invariance. Experiments show that, when pre-trained on Waymo dataset, our method called Multi-instance Siamese Network (MultiSiam) remarkably improves generalization ability and achieves state-of-the-art transfer performance on autonomous driving benchmarks, including Cityscapes and BDD100K, while existing SSL counterparts like MoCo, MoCo-v2, and BYOL show significant performance drop. By pre-training on SODA10M, a large-scale autonomous driving dataset, MultiSiam exceeds the ImageNet pre-trained MoCo-v2, demonstrating the potential of domain-specific pre-training. Code will be available at https://github.com/KaiChen1998/MultiSiam.",0
"Over the years, autonomous driving has garnered a lot of attention but has proven to be more challenging than anticipated. This may be due to the difficulty in collecting labeled data for model training. Self-supervised learning (SSL) is a promising approach to improve model performance by using only unlabeled data for representation learning. However, existing SSL methods rely on the single-centric-object guarantee, which may not be suitable for multi-instance datasets such as street scenes. To address this limitation, we propose two solutions: defining positive samples for cross-view consistency and measuring similarity in multi-instance circumstances. We use an IoU threshold during random cropping to convert global-inconsistency to local-consistency and introduce two feature alignment methods to enable 2D feature maps for multi-instance similarity measurement. Furthermore, we use intra-image clustering with self-attention to mine intra-image similarity and translation-invariance. Our proposed method, Multi-instance Siamese Network (MultiSiam), significantly improves generalization ability and achieves state-of-the-art transfer performance on autonomous driving benchmarks, including Cityscapes and BDD100K, when pre-trained on Waymo dataset. In comparison, existing SSL counterparts like MoCo, MoCo-v2, and BYOL exhibit a significant performance drop. Pre-training on SODA10M, a large-scale autonomous driving dataset, MultiSiam surpasses the ImageNet pre-trained MoCo-v2, demonstrating the potential of domain-specific pre-training. The code for MultiSiam will be available at https://github.com/KaiChen1998/MultiSiam.",1
"Many real-world complex systems can be described as graphs. For a large-scale graph with low sparsity, a node's adjacency vector is a long and sparse representation, limiting the practical utilization of existing machine learning methods on nodal features. In practice, graph embedding (graph representation learning) attempts to learn a lower-dimensional representation vector for each node or the whole graph while maintaining the most basic information of graph. Since various machine learning methods can efficiently process lower-dimensional vectors, graph embedding has recently attracted a lot of attention. However, most node embedding or whole graph embedding methods suffer from the problem of having more sophisticated methodology, hyperparameter optimization, and low explainability. This paper proposes a hyperparameter-free, extensible, and explainable whole graph embedding method, combining the DHC (Degree, H-index and Coreness) theorem and Shannon Entropy (E), abbreviated as DHC-E. The new whole graph embedding scheme can obtain a trade-off between simplicity and quality under supervised classification learning tasks, using molecular, social, and brain networks. In addition, the proposed approach has a good performance in lower-dimensional graph visualization. Overall, the new methodology is simple, hyperparameter-free, extensible, and explainable for whole graph embedding with promising potential for exploring graph classification, prediction, and lower-dimensional graph visualization.",0
"Graphs are frequently used to describe complex systems in the real world. However, for large-scale graphs with low sparsity, the adjacency vector for a node becomes too long and sparse, making it challenging to utilize existing machine learning methods on nodal features. To address this issue, graph embedding has emerged as a technique to learn a lower-dimensional representation vector for each node or the entire graph while retaining the fundamental graph information. Although this approach has gained significant attention, most node or whole graph embedding methods are complicated, require hyperparameter optimization, and lack explainability. To overcome these challenges, this paper introduces a new whole graph embedding method called DHC-E, which combines the DHC theorem and Shannon Entropy. Unlike existing methods, DHC-E is hyperparameter-free, extensible, and explainable. Moreover, it strikes a balance between simplicity and quality and performs well in supervised classification learning tasks on molecular, social, and brain networks. Additionally, the proposed approach is effective in lower-dimensional graph visualization and has promising potential for graph classification, prediction, and visualization.",1
"Spatio-temporal representational learning has been widely adopted in various fields such as action recognition, video object segmentation, and action anticipation. Previous spatio-temporal representational learning approaches primarily employ ConvNets or sequential models,e.g., LSTM, to learn the intra-frame and inter-frame features. Recently, Transformer models have successfully dominated the study of natural language processing (NLP), image classification, etc. However, the pure-Transformer based spatio-temporal learning can be prohibitively costly on memory and computation to extract fine-grained features from a tiny patch. To tackle the training difficulty and enhance the spatio-temporal learning, we construct a shifted chunk Transformer with pure self-attention blocks. Leveraging the recent efficient Transformer design in NLP, this shifted chunk Transformer can learn hierarchical spatio-temporal features from a local tiny patch to a global video clip. Our shifted self-attention can also effectively model complicated inter-frame variances. Furthermore, we build a clip encoder based on Transformer to model long-term temporal dependencies. We conduct thorough ablation studies to validate each component and hyper-parameters in our shifted chunk Transformer, and it outperforms previous state-of-the-art approaches on Kinetics-400, Kinetics-600, UCF101, and HMDB51. Code and trained models will be released.",0
"The use of spatio-temporal representational learning is widespread in different fields like action recognition, video object segmentation, and action anticipation. Prior approaches to spatio-temporal representational learning relied on ConvNets or sequential models, such as LSTM, to acquire intra-frame and inter-frame features. Although Transformer models have become dominant in natural language processing (NLP) and image classification, pure-Transformer based spatio-temporal learning can be too costly in terms of memory and computation to extract detailed features from a small patch. To address this training challenge and enhance spatio-temporal learning, we have developed a shifted chunk Transformer with pure self-attention blocks. By using the efficient Transformer design in NLP, our shifted chunk Transformer can learn hierarchical spatio-temporal features ranging from a local tiny patch to a global video clip. Additionally, our shifted self-attention can effectively model complex inter-frame variances. Moreover, we have created a clip encoder based on Transformer to model long-term temporal dependencies. We have conducted comprehensive ablation studies to confirm the effectiveness of each component and hyper-parameter in our shifted chunk Transformer, and it has outperformed previous state-of-the-art methods on Kinetics-400, Kinetics-600, UCF101, and HMDB51. We will release the code and trained models.",1
"In this work, we consider the problem of sequence-to-sequence alignment for signals containing outliers. Assuming the absence of outliers, the standard Dynamic Time Warping (DTW) algorithm efficiently computes the optimal alignment between two (generally) variable-length sequences. While DTW is robust to temporal shifts and dilations of the signal, it fails to align sequences in a meaningful way in the presence of outliers that can be arbitrarily interspersed in the sequences. To address this problem, we introduce Drop-DTW, a novel algorithm that aligns the common signal between the sequences while automatically dropping the outlier elements from the matching. The entire procedure is implemented as a single dynamic program that is efficient and fully differentiable. In our experiments, we show that Drop-DTW is a robust similarity measure for sequence retrieval and demonstrate its effectiveness as a training loss on diverse applications. With Drop-DTW, we address temporal step localization on instructional videos, representation learning from noisy videos, and cross-modal representation learning for audio-visual retrieval and localization. In all applications, we take a weakly- or unsupervised approach and demonstrate state-of-the-art results under these settings.",0
"The focus of this study is on sequence-to-sequence alignment in signals that contain outliers. Dynamic Time Warping (DTW) is a commonly used algorithm for alignment of variable-length sequences in the absence of outliers. However, DTW is not effective in aligning sequences that contain arbitrary interspersed outliers. To overcome this limitation, a new algorithm called Drop-DTW is proposed. Drop-DTW aligns the common signal between the sequences and eliminates the outlier elements from the matching. The algorithm is implemented as a single dynamic program that is both efficient and fully differentiable. The proposed algorithm is shown to be a robust similarity measure for sequence retrieval, and it is also effective as a training loss for various applications, including temporal step localization on instructional videos, representation learning from noisy videos, and cross-modal representation learning for audio-visual retrieval and localization. The experiments demonstrate that Drop-DTW achieves state-of-the-art results in weakly- or unsupervised settings.",1
"In this paper, we present Co-scale conv-attentional image Transformers (CoaT), a Transformer-based image classifier equipped with co-scale and conv-attentional mechanisms. First, the co-scale mechanism maintains the integrity of Transformers' encoder branches at individual scales, while allowing representations learned at different scales to effectively communicate with each other; we design a series of serial and parallel blocks to realize the co-scale mechanism. Second, we devise a conv-attentional mechanism by realizing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities. On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolutional neural networks and image/vision Transformers. The effectiveness of CoaT's backbone is also illustrated on object detection and instance segmentation, demonstrating its applicability to downstream computer vision tasks.",0
"This paper introduces CoaT, a Transformer-based image classifier that incorporates co-scale and conv-attentional mechanisms. The co-scale mechanism maintains the integrity of Transformers' encoder branches at individual scales, allowing representations learned at different scales to communicate effectively. We designed a series of serial and parallel blocks to achieve this mechanism. Additionally, we devised a conv-attentional mechanism by implementing a relative position embedding formulation in the factorized attention module with an efficient convolution-like approach. CoaT enhances image Transformers' multi-scale and contextual modeling capabilities. Small CoaT models outperform similarly sized convolutional neural networks and image/vision Transformers in ImageNet classification. We also demonstrate the effectiveness of CoaT's backbone in object detection and instance segmentation, indicating its usefulness in downstream computer vision tasks.",1
"Time series data can be subject to changes in the underlying process that generates them and, because of these changes, models built on old samples can become obsolete or perform poorly. In this work, we present a way to incorporate information about the current data distribution and its evolution across time into machine learning algorithms. Our solution is based on efficiently maintaining statistics, particularly the mean and the variance, of data features at different time resolutions. These data summarisations can be performed over the input attributes, in which case they can then be fed into the model as additional input features, or over latent representations learned by models, such as those of Recurrent Neural Networks. In classification tasks, the proposed techniques can significantly outperform the prediction capabilities of equivalent architectures with no feature / latent summarisations. Furthermore, these modifications do not introduce notable computational and memory overhead when properly adjusted.",0
"Models based on old samples can become outdated or perform poorly due to potential changes in the underlying process that generates time series data. To address this issue, we have developed a method for integrating information about the current data distribution and its evolution over time into machine learning algorithms. Our approach involves the efficient maintenance of statistics, such as mean and variance, for data features at different time resolutions. These statistics can be applied to input attributes and used as additional input features in the model, or they can be applied to latent representations learned by models, such as Recurrent Neural Networks. Our proposed techniques have been shown to significantly improve classification accuracy compared to equivalent models without summarizations, without introducing significant computational or memory overhead.",1
"Active visual exploration aims to assist an agent with a limited field of view to understand its environment based on partial observations made by choosing the best viewing directions in the scene. Recent methods have tried to address this problem either by using reinforcement learning, which is difficult to train, or by uncertainty maps, which are task-specific and can only be implemented for dense prediction tasks. In this paper, we propose the Glimpse-Attend-and-Explore model which: (a) employs self-attention to guide the visual exploration instead of task-specific uncertainty maps; (b) can be used for both dense and sparse prediction tasks; and (c) uses a contrastive stream to further improve the representations learned. Unlike previous works, we show the application of our model on multiple tasks like reconstruction, segmentation and classification. Our model provides encouraging results while being less dependent on dataset bias in driving the exploration. We further perform an ablation study to investigate the features and attention learned by our model. Finally, we show that our self-attention module learns to attend different regions of the scene by minimizing the loss on the downstream task. Code: https://github.com/soroushseifi/glimpse-attend-explore.",0
"The goal of active visual exploration is to help an agent that has a limited field of view to gain an understanding of its environment through partial observations obtained by selecting the best viewing directions in the scene. Previous methods have attempted to tackle this issue using reinforcement learning, which is challenging to train, or uncertainty maps, which are task-specific and can only be used for dense prediction tasks. This study introduces the Glimpse-Attend-and-Explore model, which utilizes self-attention to direct the visual exploration instead of task-specific uncertainty maps. The model can be applied to both dense and sparse prediction tasks and incorporates a contrastive stream to enhance the learned representations. Unlike prior research, the proposed model is evaluated on multiple tasks, such as reconstruction, segmentation, and classification, demonstrating promising results while being less reliant on dataset bias to drive the exploration. The study also includes an ablation analysis to investigate the features and attention learned by the model. Finally, it is demonstrated that the self-attention module learns to attend to different areas of the scene by minimizing the loss on the downstream task. The code for the model is available at https://github.com/soroushseifi/glimpse-attend-explore.",1
"The application of deep learning to medical image segmentation has been hampered due to the lack of abundant pixel-level annotated data. Few-shot Semantic Segmentation (FSS) is a promising strategy for breaking the deadlock. However, a high-performing FSS model still requires sufficient pixel-level annotated classes for training to avoid overfitting, which leads to its performance bottleneck in medical image segmentation due to the unmet need for annotations. Thus, semi-supervised FSS for medical images is accordingly proposed to utilize unlabeled data for further performance improvement. Nevertheless, existing semi-supervised FSS methods has two obvious defects: (1) neglecting the relationship between the labeled and unlabeled data; (2) using unlabeled data directly for end-to-end training leads to degenerated representation learning. To address these problems, we propose a novel semi-supervised FSS framework for medical image segmentation. The proposed framework employs Poisson learning for modeling data relationship and propagating supervision signals, and Spatial Consistency Calibration for encouraging the model to learn more coherent representations. In this process, unlabeled samples do not involve in end-to-end training, but provide supervisory information for query image segmentation through graph-based learning. We conduct extensive experiments on three medical image segmentation datasets (i.e. ISIC skin lesion segmentation, abdominal organs segmentation for MRI and abdominal organs segmentation for CT) to demonstrate the state-of-the-art performance and broad applicability of the proposed framework.",0
"The lack of abundant pixel-level annotated data has hindered the application of deep learning to medical image segmentation. However, Few-shot Semantic Segmentation (FSS) shows promise as a solution. Nevertheless, high-performing FSS models require sufficient pixel-level annotated classes for training to avoid overfitting, which leads to a performance bottleneck in medical image segmentation due to the insufficient annotations. To address this issue, we propose a semi-supervised FSS framework for medical image segmentation that utilizes unlabeled data for further performance improvement. However, existing semi-supervised FSS methods have two major defects: neglecting the relationship between labeled and unlabeled data and using unlabeled data directly for end-to-end training, leading to degenerated representation learning. Our proposed framework overcomes these problems by employing Poisson learning for modeling data relationship and propagating supervision signals, and Spatial Consistency Calibration for encouraging the model to learn more coherent representations. In this process, unlabeled samples do not involve in end-to-end training but provide supervisory information for query image segmentation through graph-based learning. We demonstrate the state-of-the-art performance and broad applicability of the proposed framework by conducting extensive experiments on three medical image segmentation datasets (i.e. ISIC skin lesion segmentation, abdominal organs segmentation for MRI, and abdominal organs segmentation for CT).",1
"A variational autoencoder (VAE) derived from Tsallis statistics called q-VAE is proposed. In the proposed method, a standard VAE is employed to statistically extract latent space hidden in sampled data, and this latent space helps make robots controllable in feasible computational time and cost. To improve the usefulness of the latent space, this paper focuses on disentangled representation learning, e.g., $\beta$-VAE, which is the baseline for it. Starting from a Tsallis statistics perspective, a new lower bound for the proposed q-VAE is derived to maximize the likelihood of the sampled data, which can be considered an adaptive $\beta$-VAE with deformed Kullback-Leibler divergence. To verify the benefits of the proposed q-VAE, a benchmark task to extract the latent space from the MNIST dataset was performed. The results demonstrate that the proposed q-VAE improved disentangled representation while maintaining the reconstruction accuracy of the data. In addition, it relaxes the independency condition between data, which is demonstrated by learning the latent dynamics of nonlinear dynamical systems. By combining disentangled representation, the proposed q-VAE achieves stable and accurate long-term state prediction from the initial state and the action sequence.   The dataset for hexapod walking is available on IEEE Dataport, doi: https://dx.doi.org/10.21227/99af-jw71.",0
"The paper introduces a new approach to variational autoencoders (VAE) called q-VAE, which uses Tsallis statistics. The authors explain that standard VAEs are used to extract latent space from data, which in turn helps make robots more controllable. To improve the usefulness of latent space, the authors focus on disentangled representation learning, using $\beta$-VAE as a baseline. They derive a new lower bound for q-VAE from a Tsallis statistics perspective, which maximizes the likelihood of sampled data. This is similar to an adaptive $\beta$-VAE but with a deformed Kullback-Leibler divergence. The authors test the effectiveness of the proposed q-VAE on the MNIST dataset and find that it improves disentangled representation without sacrificing reconstruction accuracy. They also demonstrate that q-VAE relaxes the independency condition between data and can be used to learn the latent dynamics of nonlinear dynamical systems. Finally, by combining disentangled representation, q-VAE achieves stable and accurate long-term state prediction for hexapod walking, and the dataset used for this is available on IEEE Dataport, doi: https://dx.doi.org/10.21227/99af-jw71.",1
"Travel-time prediction constitutes a task of high importance in transportation networks, with web mapping services like Google Maps regularly serving vast quantities of travel time queries from users and enterprises alike. Further, such a task requires accounting for complex spatiotemporal interactions (modelling both the topological properties of the road network and anticipating events -- such as rush hours -- that may occur in the future). Hence, it is an ideal target for graph representation learning at scale. Here we present a graph neural network estimator for estimated time of arrival (ETA) which we have deployed in production at Google Maps. While our main architecture consists of standard GNN building blocks, we further detail the usage of training schedule methods such as MetaGradients in order to make our model robust and production-ready. We also provide prescriptive studies: ablating on various architectural decisions and training regimes, and qualitative analyses on real-world situations where our model provides a competitive edge. Our GNN proved powerful when deployed, significantly reducing negative ETA outcomes in several regions compared to the previous production baseline (40+% in cities like Sydney).",0
"In transportation networks, predicting travel time is a crucial task. Web mapping services, like Google Maps, receive a large number of travel time queries from both users and enterprises. This task involves considering complex spatiotemporal interactions, such as anticipating future events like rush hours and modeling the topological properties of the road network. Therefore, it is an ideal target for implementing graph representation learning at scale. This paper presents a graph neural network estimator for estimated time of arrival (ETA), which has been deployed by Google Maps. While the main architecture comprises standard GNN building blocks, the usage of training schedule methods, such as MetaGradients, has been detailed to make the model robust and production-ready. The paper also provides prescriptive studies, including analysis on various architectural decisions and training regimes, and qualitative analyses on real-world situations where the model provides a competitive edge. The GNN has proven to be powerful when deployed, significantly reducing negative ETA outcomes in several regions compared to the previous production baseline (40+% in cities like Sydney).",1
"Self-attention mechanism in graph neural networks (GNNs) led to state-of-the-art performance on many graph representation learning tasks. Currently, at every layer, attention is computed between connected pairs of nodes and depends solely on the representation of the two nodes. However, such attention mechanism does not account for nodes that are not directly connected but provide important network context. Here we propose Multi-hop Attention Graph Neural Network (MAGNA), a principled way to incorporate multi-hop context information into every layer of attention computation. MAGNA diffuses the attention scores across the network, which increases the receptive field for every layer of the GNN. Unlike previous approaches, MAGNA uses a diffusion prior on attention values, to efficiently account for all paths between the pair of disconnected nodes. We demonstrate in theory and experiments that MAGNA captures large-scale structural information in every layer, and has a low-pass effect that eliminates noisy high-frequency information from graph data. Experimental results on node classification as well as the knowledge graph completion benchmarks show that MAGNA achieves state-of-the-art results: MAGNA achieves up to 5.7 percent relative error reduction over the previous state-of-the-art on Cora, Citeseer, and Pubmed. MAGNA also obtains the best performance on a large-scale Open Graph Benchmark dataset. On knowledge graph completion MAGNA advances state-of-the-art on WN18RR and FB15k-237 across four different performance metrics.",0
"Graph neural networks (GNNs) have achieved impressive results in many graph representation learning tasks through the use of self-attention mechanisms. However, the current attention mechanism only considers connected pairs of nodes and ignores nodes that provide important network context but are not directly connected. To address this issue, we propose the Multi-hop Attention Graph Neural Network (MAGNA), which diffuses attention scores across the network to increase the receptive field for every layer of the GNN. Unlike previous methods, MAGNA uses a diffusion prior on attention values to efficiently account for all paths between disconnected nodes. Our theoretical and experimental results show that MAGNA captures large-scale structural information in every layer, and has a low-pass effect that eliminates noisy high-frequency information from graph data. We demonstrate the effectiveness of MAGNA on node classification and knowledge graph completion tasks, achieving state-of-the-art results on several benchmark datasets. Specifically, MAGNA achieves up to a 5.7 percent relative error reduction over the previous state-of-the-art on Cora, Citeseer, and Pubmed, as well as the best performance on a large-scale Open Graph Benchmark dataset. On knowledge graph completion, MAGNA outperforms the previous state-of-the-art on WN18RR and FB15k-237 across four different performance metrics.",1
"In self-supervised representation learning, a common idea behind most of the state-of-the-art approaches is to enforce the robustness of the representations to predefined augmentations. A potential issue of this idea is the existence of completely collapsed solutions (i.e., constant features), which are typically avoided implicitly by carefully chosen implementation details. In this work, we study a relatively concise framework containing the most common components from recent approaches. We verify the existence of complete collapse and discover another reachable collapse pattern that is usually overlooked, namely dimensional collapse. We connect dimensional collapse with strong correlations between axes and consider such connection as a strong motivation for feature decorrelation (i.e., standardizing the covariance matrix). The gains from feature decorrelation are verified empirically to highlight the importance and the potential of this insight.",0
"Most state-of-the-art self-supervised representation learning approaches aim to ensure the robustness of the representations to predefined augmentations. However, this approach may lead to completely collapsed solutions, namely constant features, which are often avoided through careful implementation. In this study, we examine a concise framework that includes common components from recent approaches and uncover a previously overlooked pattern of collapse, known as dimensional collapse. We establish a link between dimensional collapse and strong correlations between axes, which motivates feature decorrelation, or standardizing the covariance matrix. Empirical evidence confirms the benefits of feature decorrelation, emphasizing the importance and potential of this finding.",1
"Multiple instance learning (MIL) is the preferred approach for whole slide image classification. However, most MIL approaches do not exploit the interdependencies of tiles extracted from a whole slide image, which could provide valuable cues for classification. This paper presents a novel MIL approach that exploits the spatial relationship of tiles for classifying whole slide images. To do so, a sparse map is built from tiles embeddings, and is then classified by a sparse-input CNN. It obtained state-of-the-art performance over popular MIL approaches on the classification of cancer subtype involving 10000 whole slide images. Our results suggest that the proposed approach might (i) improve the representation learning of instances and (ii) exploit the context of instance embeddings to enhance the classification performance. The code of this work is open-source at {github censored for review}.",0
"Whole slide image classification is typically accomplished using multiple instance learning (MIL). However, most MIL methods overlook the interdependencies between tiles extracted from a slide, which could provide valuable classification clues. This paper proposes a new MIL method that employs the spatial relationship between tiles to classify whole slide images. The approach involves creating a sparse map from tile embeddings and then using a sparse-input CNN for classification. The results demonstrate that this method outperforms popular MIL approaches for cancer subtype classification using 10,000 whole slide images. The findings suggest that this approach could improve instance representation learning and leverage instance embeddings' context to enhance classification performance. The code for this work is open-source and available on {github censored for review}.",1
"Recent advances in 3D perception have shown impressive progress in understanding geometric structures of 3Dshapes and even scenes. Inspired by these advances in geometric understanding, we aim to imbue image-based perception with representations learned under geometric constraints. We introduce an approach to learn view-invariant,geometry-aware representations for network pre-training, based on multi-view RGB-D data, that can then be effectively transferred to downstream 2D tasks. We propose to employ contrastive learning under both multi-view im-age constraints and image-geometry constraints to encode3D priors into learned 2D representations. This results not only in improvement over 2D-only representation learning on the image-based tasks of semantic segmentation, instance segmentation, and object detection on real-world in-door datasets, but moreover, provides significant improvement in the low data regime. We show a significant improvement of 6.0% on semantic segmentation on full data as well as 11.9% on 20% data against baselines on ScanNet.",0
"The progress in 3D perception has been impressive lately, with a better understanding of the geometric structures of 3D shapes and scenes. Our goal is to incorporate geometric constraints into image-based perception by using representations learned through this understanding. To achieve this, we propose a method that learns view-invariant, geometry-aware representations for network pre-training using multi-view RGB-D data. We use contrastive learning under both multi-view image constraints and image-geometry constraints to encode 3D priors into learned 2D representations. This improves 2D-only representation learning on semantic segmentation, instance segmentation, and object detection on real-world indoor datasets and provides significant improvement in the low data regime. Our approach has shown a significant improvement of 6.0% on semantic segmentation on full data and 11.9% on 20% data compared to baselines on ScanNet.",1
"Self-supervised learning methods for computer vision have demonstrated the effectiveness of pre-training feature representations, resulting in well-generalizing Deep Neural Networks, even if the annotated data are limited. However, representation learning techniques require a significant amount of time for model training, with most of the time spent on precise hyper-parameter optimization and selection of augmentation techniques. We hypothesized that if the annotated dataset has enough morphological diversity to capture the diversity of the general population, as is common in medical imaging due to conserved similarities of tissue morphology, the variance error of the trained model is the dominant component of the Bias-Variance Trade-off. Therefore, we proposed the Variance Aware Training (VAT) method that exploits this data property by introducing the variance error into the model loss function, thereby, explicitly regularizing the model. Additionally, we provided a theoretical formulation and proof of the proposed method to aid interpreting the approach. Our method requires selecting only one hyper-parameter and matching or improving the performance of state-of-the-art self-supervised methods while achieving an order of magnitude reduction in the GPU training time. We validated VAT on three medical imaging datasets from diverse domains and for various learning objectives. These included a Magnetic Resonance Imaging (MRI) dataset for the heart semantic segmentation (MICCAI 2017 ACDC challenge), fundus photography dataset for ordinary regression of diabetic retinopathy progression (Kaggle 2019 APTOS Blindness Detection challenge), and classification of histopathologic scans of lymph node sections (PatchCamelyon dataset). Our code is available at https://github.com/DmitriiShubin/Variance-Aware-Training.",0
"The effectiveness of pre-training feature representations in self-supervised learning methods for computer vision has been demonstrated, resulting in Deep Neural Networks that generalize well, even with limited annotated data. However, these techniques require significant time for model training, mainly due to precise hyper-parameter optimization and augmentation technique selection. In medical imaging, where the annotated dataset captures the morphological diversity of the general population, the variance error of the trained model is the dominant component of the Bias-Variance Trade-off. To leverage this data property, we introduced the Variance Aware Training (VAT) method, which incorporates variance error into the model loss function to explicitly regularize the model. We provided a theoretical formulation and proof of the method and achieved an order of magnitude reduction in GPU training time while matching or surpassing state-of-the-art self-supervised methods' performance. VAT was validated on three medical imaging datasets from different domains and learning objectives, including heart semantic segmentation, diabetic retinopathy progression regression, and lymph node section histopathologic scan classification. Our code is available at https://github.com/DmitriiShubin/Variance-Aware-Training.",1
"This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.",0
"This book presents a theory approach for understanding deep neural networks that are practically relevant. It begins with a component-level picture of networks and describes how to determine an accurate description of the output of trained networks by solving iteration equations and nonlinear learning dynamics. The book demonstrates that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. It also explains how these effectively-deep networks learn nontrivial representations from training and analyzes the mechanism of representation learning for nonlinear models. The book proposes the notion of representation group flow (RG flow) to characterize the propagation of signals through the network and shows how tuning networks to criticality can solve the exploding and vanishing gradient problem. The book explains how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. It shows that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks and estimates the optimal aspect ratio at which we expect the network to be practically most useful using information-theoretic techniques. Finally, the book shows how residual connections can be used to push this scale to arbitrary depths and how these tools can help us learn about the inductive bias of architectures, hyperparameters, and optimizers.",1
"Interactive theorem proving is a challenging and tedious process, which requires non-trivial expertise and detailed low-level instructions (or tactics) from human experts. Tactic prediction is a natural way to automate this process. Existing methods show promising results on tactic prediction by learning a deep neural network (DNN) based model from proofs written by human experts. In this paper, we propose NeuroTactic, a novel extension with a special focus on improving the representation learning for theorem proving. NeuroTactic leverages graph neural networks (GNNs) to represent the theorems and premises, and applies graph contrastive learning for pre-training. We demonstrate that the representation learning of theorems is essential to predict tactics. Compared with other methods, NeuroTactic achieves state-of-the-art performance on the CoqGym dataset.",0
"The process of interactive theorem proving is a difficult and tedious task that requires in-depth knowledge and specific instructions from experts. To automate this process, tactic prediction is a viable solution. Previous studies have shown that a deep neural network (DNN) can learn tactic prediction from proofs drafted by human experts. This paper introduces NeuroTactic, which focuses on enhancing the representation learning for theorem proving. NeuroTactic uses graph neural networks (GNNs) to represent theorems and premises, and applies graph contrastive learning for pre-training. Our study reveals that theorems' representation learning is crucial for predicting tactics. NeuroTactic outperforms other methods and achieves state-of-the-art results on the CoqGym dataset.",1
"Many tasks in graph machine learning, such as link prediction and node classification, are typically solved by using representation learning, in which each node or edge in the network is encoded via an embedding. Though there exists a lot of network embeddings for static graphs, the task becomes much more complicated when the dynamic (i.e. temporal) network is analyzed. In this paper, we propose a novel approach for dynamic network representation learning based on Temporal Graph Network by using a highly custom message generating function by extracting Causal Anonymous Walks. For evaluation, we provide a benchmark pipeline for the evaluation of temporal network embeddings. This work provides the first comprehensive comparison framework for temporal network representation learning in every available setting for graph machine learning problems involving node classification and link prediction. The proposed model outperforms state-of-the-art baseline models. The work also justifies the difference between them based on evaluation in various transductive/inductive edge/node classification tasks. In addition, we show the applicability and superior performance of our model in the real-world downstream graph machine learning task provided by one of the top European banks, involving credit scoring based on transaction data.",0
"Representation learning is a popular solution for tasks in graph machine learning such as node classification and link prediction. This involves encoding each node or edge in the network using an embedding. However, when analyzing dynamic networks, the task becomes more complex. This paper proposes a new method for dynamic network representation learning using Temporal Graph Network and a custom message generating function extracted from Causal Anonymous Walks. The proposed model outperforms state-of-the-art baseline models in various transductive/inductive edge/node classification tasks. A benchmark pipeline is provided for evaluating temporal network embeddings, which is the first comprehensive comparison framework for temporal network representation learning. The model's applicability and superior performance are demonstrated in a real-world downstream graph machine learning task for credit scoring based on transaction data from a top European bank.",1
"The instance discrimination paradigm has become dominant in unsupervised learning. It always adopts a teacher-student framework, in which the teacher provides embedded knowledge as a supervision signal for the student. The student learns meaningful representations by enforcing instance spatial consistency with the views from the teacher. However, the outputs of the teacher can vary dramatically on the same instance during different training stages, introducing unexpected noise and leading to catastrophic forgetting caused by inconsistent objectives. In this paper, we first integrate instance temporal consistency into current instance discrimination paradigms, and propose a novel and strong algorithm named Temporal Knowledge Consistency (TKC). Specifically, our TKC dynamically ensembles the knowledge of temporal teachers and adaptively selects useful information according to its importance to learning instance temporal consistency. Experimental result shows that TKC can learn better visual representations on both ResNet and AlexNet on linear evaluation protocol while transfer well to downstream tasks. All experiments suggest the good effectiveness and generalization of our method.",0
"Unsupervised learning has primarily adopted the instance discrimination paradigm, which involves a teacher-student framework. The teacher imparts embedded knowledge to the student, who learns meaningful representations by maintaining instance spatial consistency in accordance with the teacher's viewpoints. However, during different training stages, the teacher's outputs for the same instance can differ significantly, leading to unexpected noise and inconsistent objectives that may cause catastrophic forgetting. In this study, we introduce instance temporal consistency into the existing instance discrimination paradigm and propose a robust algorithm, Temporal Knowledge Consistency (TKC), which dynamically combines the knowledge of temporal teachers and selects valuable information based on its significance in learning instance temporal consistency. Our experimental results demonstrate that TKC outperforms both ResNet and AlexNet on the linear evaluation protocol and transfers effectively to downstream tasks, indicating the effectiveness and generalizability of our approach.",1
"Big data methods are becoming an important tool for tax fraud detection around the world. Unsupervised learning approach is the dominant framework due to the lack of label and ground truth in corresponding data sets although these methods suffer from low interpretability. HUNOD, a novel hybrid unsupervised outlier detection method for tax evasion risk management, is presented in this paper. In contrast to previous methods proposed in the literature, the HUNOD method combines two outlier detection approaches based on two different machine learning designs (i.e, clustering and representational learning) to detect and internally validate outliers in a given tax dataset. The HUNOD method allows its users to incorporate relevant domain knowledge into both constituent outlier detection approaches in order to detect outliers relevant for a given economic context. The interpretability of obtained outliers is achieved by training explainable-by-design surrogate models over results of unsupervised outlier detection methods. The experimental evaluation of the HUNOD method is conducted on two datasets derived from the database on individual personal income tax declarations collected by the Tax Administration of Serbia. The obtained results show that the method indicates between 90% and 98% internally validated outliers depending on the clustering configuration and employed regularization mechanisms for representational learning.",0
"The use of big data methods in detecting tax fraud has become increasingly important worldwide. With the absence of labels and ground truth in corresponding data sets, unsupervised learning approaches are the dominant framework, despite their low interpretability. This paper introduces HUNOD, a novel hybrid unsupervised outlier detection method for tax evasion risk management. Unlike previous methods proposed in literature, HUNOD combines two outlier detection approaches based on different machine learning designs, clustering and representational learning, to detect and validate outliers in a given tax dataset. Users can incorporate relevant domain knowledge into both approaches to identify outliers specific to a given economic context. Interpretability of the obtained outliers is achieved by training explainable-by-design surrogate models over results of unsupervised outlier detection methods. The HUNOD method is evaluated on two datasets derived from the Tax Administration of Serbia's database on individual personal income tax declarations. The method indicates between 90% and 98% internally validated outliers, depending on the clustering configuration and regularization mechanisms employed in representational learning.",1
"With the development of computational power and techniques for data collection, deep learning demonstrates a superior performance over most existing algorithms on visual benchmark data sets. Many efforts have been devoted to studying the mechanism of deep learning. One important observation is that deep learning can learn the discriminative patterns from raw materials directly in a task-dependent manner. Therefore, the representations obtained by deep learning outperform hand-crafted features significantly. However, for some real-world applications, it is too expensive to collect the task-specific labels, such as visual search in online shopping. Compared to the limited availability of these task-specific labels, their coarse-class labels are much more affordable, but representations learned from them can be suboptimal for the target task. To mitigate this challenge, we propose an algorithm to learn the fine-grained patterns for the target task, when only its coarse-class labels are available. More importantly, we provide a theoretical guarantee for this. Extensive experiments on real-world data sets demonstrate that the proposed method can significantly improve the performance of learned representations on the target task, when only coarse-class information is available for training. Code is available at \url{https://github.com/idstcv/CoIns}.",0
"Deep learning has shown remarkable performance on visual benchmark datasets due to advances in computational power and data collection techniques. Researchers have studied deep learning and found that it can learn unique patterns directly from raw materials, resulting in superior representations compared to hand-crafted features. However, collecting task-specific labels for real-world applications can be prohibitively expensive. While coarse-class labels are more affordable, representations learned from them may not be optimal for the target task. To address this challenge, we propose an algorithm that can learn fine-grained patterns for the target task using only coarse-class labels. We also provide a theoretical guarantee for this approach. Our experiments on real-world data sets show that our method significantly improves the performance of learned representations when only coarse-class information is available for training. Our code is available at \url{https://github.com/idstcv/CoIns}.",1
"Self-supervised Learning (SSL) aims at learning representations of objects without relying on manual labeling. Recently, a number of SSL methods for graph representation learning have achieved performance comparable to SOTA semi-supervised GNNs. A Siamese network, which relies on data augmentation, is the popular architecture used in these methods. However, these methods rely on heuristically crafted data augmentation techniques. Furthermore, they use either contrastive terms or other tricks (e.g., asymmetry) to avoid trivial solutions that can occur in Siamese networks. In this study, we propose, GraphSurgeon, a novel SSL method for GNNs with the following features. First, instead of heuristics we propose a learnable data augmentation method that is jointly learned with the embeddings by leveraging the inherent signal encoded in the graph. In addition, we take advantage of the flexibility of the learnable data augmentation and introduce a new strategy that augments in the embedding space, called post augmentation. This strategy has a significantly lower memory overhead and run-time cost. Second, as it is difficult to sample truly contrastive terms, we avoid explicit negative sampling. Third, instead of relying on engineering tricks, we use a scalable constrained optimization objective motivated by Laplacian Eigenmaps to avoid trivial solutions. To validate the practical use of GraphSurgeon, we perform empirical evaluation using 14 public datasets across a number of domains and ranging from small to large scale graphs with hundreds of millions of edges. Our finding shows that GraphSurgeon is comparable to six SOTA semi-supervised and on par with five SOTA self-supervised baselines in node classification tasks. The source code is available at https://github.com/zekarias-tilahun/graph-surgeon.",0
"Self-supervised Learning (SSL) is a technique that involves learning object representations without relying on manual labeling. Recently, various SSL methods for graph representation learning have achieved performance that is comparable to state-of-the-art (SOTA) semi-supervised Graph Neural Networks (GNNs). These methods typically use a Siamese network architecture that relies on heuristically crafted data augmentation techniques and either contrastive terms or other tricks to avoid trivial solutions. In this study, we present GraphSurgeon, a novel SSL method for GNNs that offers several unique features. Firstly, we propose a learnable data augmentation method that is jointly learned with the embeddings by leveraging the inherent signal encoded in the graph, rather than relying on heuristics. Additionally, we introduce a new strategy called post-augmentation, which involves augmenting in the embedding space and has a significantly lower memory overhead and run-time cost. Secondly, we avoid explicit negative sampling, which is challenging to sample truly contrastive terms. Thirdly, we use a scalable constrained optimization objective motivated by Laplacian Eigenmaps to avoid trivial solutions, instead of relying on engineering tricks. To evaluate the effectiveness of GraphSurgeon, we perform empirical evaluation using 14 public datasets across a variety of domains and ranging from small to large scale graphs with hundreds of millions of edges. Our results demonstrate that GraphSurgeon is comparable to six SOTA semi-supervised and on par with five SOTA self-supervised baselines in node classification tasks. The source code for GraphSurgeon is available at https://github.com/zekarias-tilahun/graph-surgeon.",1
"Self-supervised learning has been successfully applied to pre-train video representations, which aims at efficient adaptation from pre-training domain to downstream tasks. Existing approaches merely leverage contrastive loss to learn instance-level discrimination. However, lack of category information will lead to hard-positive problem that constrains the generalization ability of this kind of methods. We find that the multi-task process of meta learning can provide a solution to this problem. In this paper, we propose a Meta-Contrastive Network (MCN), which combines the contrastive learning and meta learning, to enhance the learning ability of existing self-supervised approaches. Our method contains two training stages based on model-agnostic meta learning (MAML), each of which consists of a contrastive branch and a meta branch. Extensive evaluations demonstrate the effectiveness of our method. For two downstream tasks, i.e., video action recognition and video retrieval, MCN outperforms state-of-the-art approaches on UCF101 and HMDB51 datasets. To be more specific, with R(2+1)D backbone, MCN achieves Top-1 accuracies of 84.8% and 54.5% for video action recognition, as well as 52.5% and 23.7% for video retrieval.",0
"The successful application of self-supervised learning in pre-training video representations has led to a focus on efficient adaptation to downstream tasks. However, current methods that rely solely on contrastive loss for instance-level discrimination are limited by the lack of category information, resulting in a hard-positive problem that affects generalization. To address this issue, we propose a Meta-Contrastive Network (MCN) that combines contrastive learning and meta learning for enhanced learning ability. Our model consists of two stages of training using model-agnostic meta learning (MAML), each with a contrastive and meta branch. Extensive evaluations demonstrate MCN's effectiveness in outperforming state-of-the-art approaches for video action recognition and retrieval on UCF101 and HMDB51 datasets using R(2+1)D backbone, achieving Top-1 accuracies of 84.8% and 54.5% for video action recognition and 52.5% and 23.7% for video retrieval.",1
"Contrastive learning has been widely used to train transformer-based vision-language models for video-text alignment and multi-modal representation learning. This paper presents a new algorithm called Token-Aware Cascade contrastive learning (TACo) that improves contrastive learning using two novel techniques. The first is the token-aware contrastive loss which is computed by taking into account the syntactic classes of words. This is motivated by the observation that for a video-text pair, the content words in the text, such as nouns and verbs, are more likely to be aligned with the visual contents in the video than the function words. Second, a cascade sampling method is applied to generate a small set of hard negative examples for efficient loss estimation for multi-modal fusion layers. To validate the effectiveness of TACo, in our experiments we finetune pretrained models for a set of downstream tasks including text-video retrieval (YouCook2, MSR-VTT and ActivityNet), video action step localization (CrossTask), video action segmentation (COIN). The results show that our models attain consistent improvements across different experimental settings over previous methods, setting new state-of-the-art on three public text-video retrieval benchmarks of YouCook2, MSR-VTT and ActivityNet.",0
"The use of contrastive learning in training transformer-based vision-language models for multi-modal representation learning and video-text alignment is widespread. This study proposes a new approach named Token-Aware Cascade contrastive learning (TACo) that incorporates two innovative techniques to enhance contrastive learning. Firstly, the token-aware contrastive loss considers the syntactic classes of words, as content words like nouns and verbs are more likely to align with visual content in videos than function words. Secondly, a cascade sampling method generates a small set of hard negative examples, improving loss estimation for multi-modal fusion layers. The effectiveness of TACo is verified by fine-tuning pretrained models for several downstream tasks, such as text-video retrieval and video action segmentation. The results demonstrate consistent improvements over prior methods, achieving state-of-the-art performance on three public text-video retrieval benchmarks.",1
"Representation learning on static graph-structured data has shown a significant impact on many real-world applications. However, less attention has been paid to the evolving nature of temporal networks, in which the edges are often changing over time. The embeddings of such temporal networks should encode both graph-structured information and the temporally evolving pattern. Existing approaches in learning temporally evolving network representations fail to capture the temporal interdependence. In this paper, we propose Toffee, a novel approach for temporal network representation learning based on tensor decomposition. Our method exploits the tensor-tensor product operator to encode the cross-time information, so that the periodic changes in the evolving networks can be captured. Experimental results demonstrate that Toffee outperforms existing methods on multiple real-world temporal networks in generating effective embeddings for the link prediction tasks.",0
"Many real-world applications have benefited greatly from representation learning on static graph-structured data. However, the changing nature of temporal networks, where edges are frequently altered over time, has received less attention. Embeddings of temporal networks must incorporate both graph-structured information and evolving patterns. Current approaches to learning representations for evolving networks fail to capture temporal interdependence. This paper introduces Toffee, a novel method for temporal network representation learning that employs tensor decomposition. Our approach utilizes the tensor-tensor product operator to encode cross-time information, allowing periodic changes in evolving networks to be captured. Experimental results demonstrate that Toffee generates more effective embeddings for link prediction tasks on multiple real-world temporal networks compared to existing methods.",1
"Contrastive representation learning is an effective unsupervised method to alleviate the demand for expensive annotated data in medical image processing. Recent work mainly based on instance-wise discrimination to learn global features, while neglect local details, which limit their application in processing tiny anatomical structures, tissues and lesions. Therefore, we aim to propose a universal local discrmination framework to learn local discriminative features to effectively initialize medical models, meanwhile, we systematacially investigate its practical medical applications. Specifically, based on the common property of intra-modality structure similarity, i.e. similar structures are shared among the same modality images, a systematic local feature learning framework is proposed. Instead of making instance-wise comparisons based on global embedding, our method makes pixel-wise embedding and focuses on measuring similarity among patches and regions. The finer contrastive rule makes the learnt representation more generalized for segmentation tasks and outperform extensive state-of-the-art methods by wining 11 out of all 12 downstream tasks in color fundus and chest X-ray. Furthermore, based on the property of inter-modality shape similarity, i.e. structures may share similar shape although in different medical modalities, we joint across-modality shape prior into region discrimination to realize unsupervised segmentation. It shows the feaibility of segmenting target only based on shape description from other modalities and inner pattern similarity provided by region discrimination. Finally, we enhance the center-sensitive ability of patch discrimination by introducing center-sensitive averaging to realize one-shot landmark localization, this is an effective application for patch discrimination.",0
"To reduce the need for costly annotated data in medical image processing, contrastive representation learning has been found to be an effective unsupervised approach. However, recent studies have primarily focused on global features through instance-wise discrimination, thereby overlooking local details, which restricts their application in processing small anatomical structures, tissues, and lesions. Hence, we propose a universal local discrimination framework that learns local discriminative features to initialize medical models effectively. Moreover, we systematically investigate its practical medical applications. Our approach is based on the similarity of intra-modality structure, wherein patches and regions are compared pixel-wise instead of instance-wise. This finer contrastive rule enables us to learn more generalized representations for segmentation tasks. Our method outperforms several state-of-the-art techniques in 11 out of the 12 downstream tasks in color fundus and chest X-ray. We also incorporate inter-modality shape similarity by merging across-modality shape prior into region discrimination for unsupervised segmentation. We demonstrate the feasibility of segmenting targets based solely on shape descriptions from other modalities and inner pattern similarity provided by region discrimination. Finally, we introduce center-sensitive averaging to enhance the center-sensitive ability of patch discrimination, which enables one-shot landmark localization, a useful application of patch discrimination.",1
"Deep neural networks have shown the ability to extract universal feature representations from data such as images and text that have been useful for a variety of learning tasks. However, the fruits of representation learning have yet to be fully-realized in federated settings. Although data in federated settings is often non-i.i.d. across clients, the success of centralized deep learning suggests that data often shares a global feature representation, while the statistical heterogeneity across clients or tasks is concentrated in the labels. Based on this intuition, we propose a novel federated learning framework and algorithm for learning a shared data representation across clients and unique local heads for each client. Our algorithm harnesses the distributed computational power across clients to perform many local-updates with respect to the low-dimensional local parameters for every update of the representation. We prove that this method obtains linear convergence to the ground-truth representation with near-optimal sample complexity in a linear setting, demonstrating that it can efficiently reduce the problem dimension for each client. This result is of interest beyond federated learning to a broad class of problems in which we aim to learn a shared low-dimensional representation among data distributions, for example in meta-learning and multi-task learning. Further, extensive experimental results show the empirical improvement of our method over alternative personalized federated learning approaches in federated environments with heterogeneous data.",0
"Although deep neural networks have proven to be adept at extracting universal feature representations from various types of data, including images and text, their potential for use in federated settings remains largely untapped. While data in these settings may not be independently and identically distributed across clients, the success of centralized deep learning suggests that data often shares a global feature representation, with the statistical heterogeneity across clients or tasks concentrated in the labels. Drawing on this insight, we have developed a novel framework and algorithm for federated learning that enables the creation of a shared data representation across clients, while also allowing for unique local heads for each client. By leveraging the distributed computational power of multiple clients, our algorithm can perform many local updates with respect to low-dimensional local parameters for every update of the representation. Our approach achieves linear convergence to the ground-truth representation with near-optimal sample complexity in a linear setting, thereby efficiently reducing the problem dimension for each client. This result is applicable to a broad range of problems beyond federated learning, including meta-learning and multi-task learning. Our method has been extensively tested in federated environments with heterogeneous data, and the results demonstrate its superiority over alternative personalized federated learning approaches.",1
"Learning socially-aware motion representations is at the core of recent advances in multi-agent problems, such as human motion forecasting and robot navigation in crowds. Despite promising progress, existing representations learned with neural networks still struggle to generalize in closed-loop predictions (e.g., output colliding trajectories). This issue largely arises from the non-i.i.d. nature of sequential prediction in conjunction with ill-distributed training data. Intuitively, if the training data only comes from human behaviors in safe spaces, i.e., from ""positive"" examples, it is difficult for learning algorithms to capture the notion of ""negative"" examples like collisions. In this work, we aim to address this issue by explicitly modeling negative examples through self-supervision: (i) we introduce a social contrastive loss that regularizes the extracted motion representation by discerning the ground-truth positive events from synthetic negative ones; (ii) we construct informative negative samples based on our prior knowledge of rare but dangerous circumstances. Our method substantially reduces the collision rates of recent trajectory forecasting, behavioral cloning and reinforcement learning algorithms, outperforming state-of-the-art methods on several benchmarks. Our code is available at https://github.com/vita-epfl/social-nce.",0
"Recent advancements in multi-agent problems, such as predicting human motion and navigating robots in crowded areas, rely heavily on socially-aware motion representations. Despite some progress, neural networks used to learn these representations still struggle to predict closed-loop trajectories without collisions. This problem stems from the non-i.i.d. nature of sequential prediction and the lack of well-distributed training data. When training data consists only of ""positive"" examples, such as human behaviors in safe spaces, it is difficult for learning algorithms to capture the concept of ""negative"" examples, like collisions. To address this issue, we propose a solution that models negative examples through self-supervision. Our approach involves introducing a social contrastive loss to distinguish ground-truth positive events from synthetic negative ones and constructing informative negative samples based on prior knowledge of rare but dangerous circumstances. We have seen a significant reduction in collision rates for trajectory forecasting, behavioral cloning, and reinforcement learning algorithms compared to state-of-the-art methods on various benchmarks. Our code is publicly available at https://github.com/vita-epfl/social-nce.",1
"We propose a novel method for enforcing AI fairness with respect to protected or sensitive factors. This method uses a dual strategy performing training and representation alteration (TARA) for the mitigation of prominent causes of AI bias by including: a) the use of representation learning alteration via adversarial independence to suppress the bias-inducing dependence of the data representation from protected factors; and b) training set alteration via intelligent augmentation to address bias-causing data imbalance, by using generative models that allow the fine control of sensitive factors related to underrepresented populations via domain adaptation and latent space manipulation. When testing our methods on image analytics, experiments demonstrate that TARA significantly or fully debiases baseline models while outperforming competing debiasing methods that have the same amount of information, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs. the baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs. (69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in current metrics used for assessing debiasing performance, we propose novel conjunctive debiasing metrics. Our experiments also demonstrate the ability of these novel metrics in assessing the Pareto efficiency of the proposed methods.",0
"Our innovative approach to ensuring AI fairness involves the use of a dual strategy, which we refer to as training and representation alteration (TARA). TARA addresses the most common causes of AI bias by employing two methods: representation learning alteration via adversarial independence, which reduces the dependence of data representation on protected factors, and training set alteration via intelligent augmentation, which deals with data imbalance by using generative models to control sensitive factors related to underrepresented populations. Our experiments with image analytics show that TARA significantly or fully debiases baseline models and outperforms comparable debiasing methods. We also propose new metrics to assess debiasing performance and demonstrate their effectiveness in measuring the Pareto efficiency of our proposed methods.",1
"The point cloud representation of an object can have a large geometric variation in view of inconsistent data acquisition procedure, which thus leads to domain discrepancy due to diverse and uncontrollable shape representation cross datasets. To improve discrimination on unseen distribution of point-based geometries in a practical and feasible perspective, this paper proposes a new method of geometry-aware self-training (GAST) for unsupervised domain adaptation of object point cloud classification. Specifically, this paper aims to learn a domain-shared representation of semantic categories, via two novel self-supervised geometric learning tasks as feature regularization. On one hand, the representation learning is empowered by a linear mixup of point cloud samples with their self-generated rotation labels, to capture a global topological configuration of local geometries. On the other hand, a diverse point distribution across datasets can be normalized with a novel curvature-aware distortion localization. Experiments on the PointDA-10 dataset show that our GAST method can significantly outperform the state-of-the-art methods.",0
"Due to inconsistent data acquisition procedures, the point cloud representation of an object can have a large geometric variation, resulting in domain discrepancy and uncontrollable shape representation across datasets. To address this issue and improve discrimination of unseen distribution of point-based geometries, this paper proposes a novel method called geometry-aware self-training (GAST) for unsupervised domain adaptation of object point cloud classification. The goal is to learn a domain-shared representation of semantic categories through two self-supervised geometric learning tasks as feature regularization. The first task involves a linear mixup of point cloud samples with their self-generated rotation labels to capture the global topological configuration of local geometries. The second task involves a curvature-aware distortion localization to normalize a diverse point distribution across datasets. Experiments on the PointDA-10 dataset demonstrate that our GAST method outperforms state-of-the-art methods significantly.",1
"We focus on contrastive methods for self-supervised video representation learning. A common paradigm in contrastive learning is to construct positive pairs by sampling different data views for the same instance, with different data instances as negatives. These methods implicitly assume a set of representational invariances to the view selection mechanism (eg, sampling frames with temporal shifts), which may lead to poor performance on downstream tasks which violate these invariances (fine-grained video action recognition that would benefit from temporal information). To overcome this limitation, we propose an 'augmentation aware' contrastive learning framework, where we explicitly provide a sequence of augmentation parameterisations (such as the values of the time shifts used to create data views) as composable augmentation encodings (CATE) to our model when projecting the video representations for contrastive learning. We show that representations learned by our method encode valuable information about specified spatial or temporal augmentation, and in doing so also achieve state-of-the-art performance on a number of video benchmarks.",0
"Our focus is on contrastive methods for self-supervised video representation learning. A common approach is to create positive pairs by sampling different data views for the same instance, with different data instances serving as negatives. However, these methods assume a set of representational invariances to the view selection mechanism that may result in poor performance on downstream tasks that violate these invariances, such as fine-grained video action recognition that requires temporal information. To address this limitation, we introduce an 'augmentation aware' contrastive learning framework where we explicitly provide a sequence of augmentation parameterisations as composable augmentation encodings (CATE) to our model when projecting video representations for contrastive learning. Our method learns representations that encode valuable information about specific spatial or temporal augmentation while achieving state-of-the-art performance on several video benchmarks.",1
"Category-level 6D pose estimation, aiming to predict the location and orientation of unseen object instances, is fundamental to many scenarios such as robotic manipulation and augmented reality, yet still remains unsolved. Precisely recovering instance 3D model in the canonical space and accurately matching it with the observation is an essential point when estimating 6D pose for unseen objects. In this paper, we achieve accurate category-level 6D pose estimation via cascaded relation and recurrent reconstruction networks. Specifically, a novel cascaded relation network is dedicated for advanced representation learning to explore the complex and informative relations among instance RGB image, instance point cloud and category shape prior. Furthermore, we design a recurrent reconstruction network for iterative residual refinement to progressively improve the reconstruction and correspondence estimations from coarse to fine. Finally, the instance 6D pose is obtained leveraging the estimated dense correspondences between the instance point cloud and the reconstructed 3D model in the canonical space. We have conducted extensive experiments on two well-acknowledged benchmarks of category-level 6D pose estimation, with significant performance improvement over existing approaches. On the representatively strict evaluation metrics of $3D_{75}$ and $5^{\circ}2 cm$, our method exceeds the latest state-of-the-art SPD by $4.9\%$ and $17.7\%$ on the CAMERA25 dataset, and by $2.7\%$ and $8.5\%$ on the REAL275 dataset. Codes are available at https://wangjiaze.cn/projects/6DPoseEstimation.html.",0
"The estimation of the 6D pose at the category level is a crucial aspect of tasks like robotic manipulation and augmented reality, as it involves predicting the location and orientation of unknown object instances. However, this remains a challenging problem to solve. To accurately estimate the 6D pose of unseen objects, it is essential to precisely recover the instance's 3D model in the canonical space and match it with the observation. In this study, we propose a novel approach that utilizes cascaded relation and recurrent reconstruction networks to achieve accurate category-level 6D pose estimation. Our method employs a cascaded relation network for advanced representation learning, which explores the complex and informative relations among instance RGB image, instance point cloud, and category shape prior. Additionally, we design a recurrent reconstruction network for iterative residual refinement to progressively enhance the reconstruction and correspondence estimations from coarse to fine. Finally, we obtain the instance's 6D pose by leveraging the estimated dense correspondences between the instance point cloud and the reconstructed 3D model in the canonical space. The performance of our approach is evaluated on two well-known benchmarks of category-level 6D pose estimation, where it demonstrates significant improvement over existing methods, achieving better results on the evaluation metrics of $3D_{75}$ and $5^{\circ}2 cm$. The code for our method is available at https://wangjiaze.cn/projects/6DPoseEstimation.html.",1
"The state-of-the-art unsupervised contrastive visual representation learning methods that have emerged recently (SimCLR, MoCo, SwAV) all make use of data augmentations in order to construct a pretext task of instant discrimination consisting of similar and dissimilar pairs of images. Similar pairs are constructed by randomly extracting patches from the same image and applying several other transformations such as color jittering or blurring, while transformed patches from different image instances in a given batch are regarded as dissimilar pairs. We argue that this approach can result similar pairs that are \textit{semantically} dissimilar. In this work, we address this problem by introducing a \textit{batch curation} scheme that selects batches during the training process that are more inline with the underlying contrastive objective. We provide insights into what constitutes beneficial similar and dissimilar pairs as well as validate \textit{batch curation} on CIFAR10 by integrating it in the SimCLR model.",0
"Recently, there have been advancements in unsupervised contrastive visual representation learning methods such as SimCLR, MoCo, and SwAV, which utilize data augmentations to create a pretext task for instant discrimination. This task involves constructing similar and dissimilar pairs of images, where similar pairs are formed by randomly selecting patches from the same image and applying various transformations like color jittering or blurring. However, we believe that this approach can lead to semantically dissimilar similar pairs. To address this issue, we introduce a batch curation scheme that selects batches during training that align with the contrastive objective. We provide insights into what constitutes beneficial similar and dissimilar pairs and demonstrate the effectiveness of batch curation by integrating it into the SimCLR model and testing it on CIFAR10.",1
"In the past few years, we have witnessed remarkable breakthroughs in self-supervised representation learning. Despite the success and adoption of representations learned through this paradigm, much is yet to be understood about how different training methods and datasets influence performance on downstream tasks. In this paper, we analyze contrastive approaches as one of the most successful and popular variants of self-supervised representation learning. We perform this analysis from the perspective of the training algorithms, pre-training datasets and end tasks. We examine over 700 training experiments including 30 encoders, 4 pre-training datasets and 20 diverse downstream tasks. Our experiments address various questions regarding the performance of self-supervised models compared to their supervised counterparts, current benchmarks used for evaluation, and the effect of the pre-training data on end task performance. Our Visual Representation Benchmark (ViRB) is available at: https://github.com/allenai/virb.",0
"Over the last few years, there have been significant advancements in self-supervised representation learning. Despite its success, there is still much to be discovered about how different training techniques and datasets impact downstream task performance. The purpose of this paper is to examine contrastive approaches, one of the most popular and successful forms of self-supervised representation learning, from the perspectives of training algorithms, pre-training datasets, and end tasks. We conducted over 700 training experiments, analyzing 30 encoders, 4 pre-training datasets, and 20 diverse downstream tasks. Our research aims to answer questions about the performance of self-supervised models compared to supervised ones, current benchmarks used for evaluation, and how pre-training data affects end task performance. We have made our Visual Representation Benchmark (ViRB) available on https://github.com/allenai/virb.",1
"We present Region Similarity Representation Learning (ReSim), a new approach to self-supervised representation learning for localization-based tasks such as object detection and segmentation. While existing work has largely focused on solely learning global representations for an entire image, ReSim learns both regional representations for localization as well as semantic image-level representations. ReSim operates by sliding a fixed-sized window across the overlapping area between two views (e.g., image crops), aligning these areas with their corresponding convolutional feature map regions, and then maximizing the feature similarity across views. As a result, ReSim learns spatially and semantically consistent feature representation throughout the convolutional feature maps of a neural network. A shift or scale of an image region, e.g., a shift or scale of an object, has a corresponding change in the feature maps; this allows downstream tasks to leverage these representations for localization. Through object detection, instance segmentation, and dense pose estimation experiments, we illustrate how ReSim learns representations which significantly improve the localization and classification performance compared to a competitive MoCo-v2 baseline: $+2.7$ AP$^{\text{bb}}_{75}$ VOC, $+1.1$ AP$^{\text{bb}}_{75}$ COCO, and $+1.9$ AP$^{\text{mk}}$ Cityscapes. Code and pre-trained models are released at: \url{https://github.com/Tete-Xiao/ReSim}",0
"Introducing Region Similarity Representation Learning (ReSim), a novel approach to self-supervised representation learning for localization-based tasks such as object detection and segmentation. Previous research has primarily focused on learning global representations for an entire image, but ReSim distinguishes itself by learning regional representations for localization as well as semantic image-level representations. The technique involves sliding a fixed-sized window across the overlapping area between two views, aligning these areas with their corresponding convolutional feature map regions, and then maximizing the feature similarity across views. This process results in spatially and semantically consistent feature representation throughout the convolutional feature maps of a neural network. The feature maps change in response to a shift or scale of an image region, enabling downstream tasks to utilize these representations for localization. To demonstrate ReSim's efficacy, we conducted object detection, instance segmentation, and dense pose estimation experiments, which revealed significant improvements in localization and classification performance compared to a competitive MoCo-v2 baseline: $+2.7$ AP$^{\text{bb}}_{75}$ VOC, $+1.1$ AP$^{\text{bb}}_{75}$ COCO, and $+1.9$ AP$^{\text{mk}}$ Cityscapes. Our code and pre-trained models are available at: \url{https://github.com/Tete-Xiao/ReSim}.",1
"Unsupervised person re-identification (ReID) aims at learning discriminative identity features without annotations. Recently, self-supervised contrastive learning has gained increasing attention for its effectiveness in unsupervised representation learning. The main idea of instance contrastive learning is to match a same instance in different augmented views. However, the relationship between different instances has not been fully explored in previous contrastive methods, especially for instance-level contrastive loss. To address this issue, we propose Inter-instance Contrastive Encoding (ICE) that leverages inter-instance pairwise similarity scores to boost previous class-level contrastive ReID methods. We first use pairwise similarity ranking as one-hot hard pseudo labels for hard instance contrast, which aims at reducing intra-class variance. Then, we use similarity scores as soft pseudo labels to enhance the consistency between augmented and original views, which makes our model more robust to augmentation perturbations. Experiments on several large-scale person ReID datasets validate the effectiveness of our proposed unsupervised method ICE, which is competitive with even supervised methods. Code is made available at https://github.com/chenhao2345/ICE.",0
"The objective of unsupervised person re-identification (ReID) is to acquire distinctive identity features without annotations. Self-supervised contrastive learning has gained popularity for its efficacy in unsupervised representation learning. The central concept of instance contrastive learning is to match the same instance in diverse augmented views. However, the earlier contrastive methods did not entirely investigate the connection between different instances, particularly for instance-level contrastive loss. To tackle this issue, we propose Inter-instance Contrastive Encoding (ICE) that utilizes inter-instance pairwise similarity scores to improve previous class-level contrastive ReID methods. Initially, we employ pairwise similarity ranking as one-hot hard pseudo labels for hard instance contrast to reduce intra-class variance. Subsequently, we utilize similarity scores as soft pseudo labels to enhance the consistency between augmented and original views, which makes our model more resilient to augmentation perturbations. Experiments on several large-scale person ReID datasets authenticate the effectiveness of our proposed unsupervised method ICE, which is even competitive with supervised methods. The code is available on https://github.com/chenhao2345/ICE.",1
"\emph{Objective and Impact Statement}. With the renaissance of deep learning, automatic diagnostic systems for computed tomography (CT) have achieved many successful applications. However, they are mostly attributed to careful expert annotations, which are often scarce in practice. This drives our interest to the unsupervised representation learning. \emph{Introduction}. Recent studies have shown that self-supervised learning is an effective approach for learning representations, but most of them rely on the empirical design of transformations and pretext tasks. \emph{Methods}. To avoid the subjectivity associated with these methods, we propose the MVCNet, a novel unsupervised three dimensional (3D) representation learning method working in a transformation-free manner. We view each 3D lesion from different orientations to collect multiple two dimensional (2D) views. Then, an embedding function is learned by minimizing a contrastive loss so that the 2D views of the same 3D lesion are aggregated, and the 2D views of different lesions are separated. We evaluate the representations by training a simple classification head upon the embedding layer. \emph{Results}. Experimental results show that MVCNet achieves state-of-the-art accuracies on the LIDC-IDRI (89.55\%), LNDb (77.69\%) and TianChi (79.96\%) datasets for \emph{unsupervised representation learning}. When fine-tuned on 10\% of the labeled data, the accuracies are comparable to the supervised learning model (89.46\% vs. 85.03\%, 73.85\% vs. 73.44\%, 83.56\% vs. 83.34\% on the three datasets, respectively). \emph{Conclusion}. Results indicate the superiority of MVCNet in \emph{learning representations with limited annotations}.",0
"The renaissance of deep learning has led to successful applications of automatic diagnostic systems for computed tomography (CT). However, these achievements have been largely dependent on expert annotations, which are often scarce in practice. Therefore, the authors propose an unsupervised representation learning method, MVCNet, which avoids the subjectivity associated with empirical design of transformations and pretext tasks. This novel approach works in a transformation-free manner by collecting multiple 2D views of each 3D lesion from different orientations and learning an embedding function through contrastive loss minimization. The resulting representations achieve state-of-the-art accuracies on three datasets for unsupervised representation learning, and are comparable to supervised learning when fine-tuned on limited labeled data. These findings demonstrate the superiority of MVCNet in learning representations with limited annotations.",1
"Domain shift happens in cross-domain scenarios commonly because of the wide gaps between different domains: when applying a deep learning model well-trained in one domain to another target domain, the model usually performs poorly. To tackle this problem, unsupervised domain adaptation (UDA) techniques are proposed to bridge the gap between different domains, for the purpose of improving model performance without annotation in the target domain. Particularly, UDA has a great value for multimodal medical image analysis, where annotation difficulty is a practical concern. However, most existing UDA methods can only achieve satisfactory improvements in one adaptation direction (e.g., MRI to CT), but often perform poorly in the other (CT to MRI), limiting their practical usage. In this paper, we propose a bidirectional UDA (BiUDA) framework based on disentangled representation learning for equally competent two-way UDA performances. This framework employs a unified domain-aware pattern encoder which not only can adaptively encode images in different domains through a domain controller, but also improve model efficiency by eliminating redundant parameters. Furthermore, to avoid distortion of contents and patterns of input images during the adaptation process, a content-pattern consistency loss is introduced. Additionally, for better UDA segmentation performance, a label consistency strategy is proposed to provide extra supervision by recomposing target-domain-styled images and corresponding source-domain annotations. Comparison experiments and ablation studies conducted on two public datasets demonstrate the superiority of our BiUDA framework to current state-of-the-art UDA methods and the effectiveness of its novel designs. By successfully addressing two-way adaptations, our BiUDA framework offers a flexible solution of UDA techniques to the real-world scenario.",0
"In cross-domain scenarios, domain shift occurs frequently due to significant differences between domains. When a deep learning model, which is well-trained in one domain, is applied to another target domain, it often performs poorly. To overcome this issue, unsupervised domain adaptation (UDA) techniques have been proposed to bridge the gap between domains and improve model performance without annotation in the target domain. UDA is particularly valuable in multimodal medical image analysis, where annotation difficulty is a practical concern. However, existing UDA methods usually only work effectively in one adaptation direction, limiting their practical usage. In this paper, we introduce a bidirectional UDA (BiUDA) framework based on disentangled representation learning to achieve equally competent two-way UDA performances. The framework employs a unified domain-aware pattern encoder, which can adaptively encode images in different domains through a domain controller and improve model efficiency by eliminating redundant parameters. Additionally, a content-pattern consistency loss is introduced to avoid distortion of input image contents and patterns during the adaptation process. To enhance UDA segmentation performance, a label consistency strategy is proposed to provide extra supervision by recomposing target-domain-styled images and corresponding source-domain annotations. Comparison experiments and ablation studies conducted on two public datasets demonstrate that our BiUDA framework outperforms current state-of-the-art UDA methods and is effective in addressing two-way adaptations. Our BiUDA framework offers a flexible solution of UDA techniques for real-world scenarios.",1
"Advanced self-supervised visual representation learning methods rely on the instance discrimination (ID) pretext task. We point out that the ID task has an implicit semantic consistency (SC) assumption, which may not hold in unconstrained datasets. In this paper, we propose a novel contrastive mask prediction (CMP) task for visual representation learning and design a mask contrast (MaskCo) framework to implement the idea. MaskCo contrasts region-level features instead of view-level features, which makes it possible to identify the positive sample without any assumptions. To solve the domain gap between masked and unmasked features, we design a dedicated mask prediction head in MaskCo. This module is shown to be the key to the success of the CMP. We evaluated MaskCo on training datasets beyond ImageNet and compare its performance with MoCo V2. Results show that MaskCo achieves comparable performance with MoCo V2 using ImageNet training dataset, but demonstrates a stronger performance across a range of downstream tasks when COCO or Conceptual Captions are used for training. MaskCo provides a promising alternative to the ID-based methods for self-supervised learning in the wild.",0
"Current techniques for advanced self-supervised visual representation learning rely on the instance discrimination (ID) pretext task, which assumes a semantic consistency (SC) that may not hold in unconstrained datasets. To overcome this limitation, we propose a novel contrastive mask prediction (CMP) task and a MaskCo framework that contrasts region-level features instead of view-level features, enabling the identification of positive samples without any assumptions. To bridge the domain gap between masked and unmasked features, we introduce a dedicated mask prediction head in MaskCo, which is crucial for the CMP's success. Evaluations on datasets beyond ImageNet demonstrate that MaskCo outperforms MoCo V2 across a range of downstream tasks when COCO or Conceptual Captions are used for training. Our results suggest that MaskCo is a promising alternative to ID-based methods for self-supervised learning in unconstrained environments.",1
"We study self-supervised video representation learning, which is a challenging task due to 1) lack of labels for explicit supervision; 2) unstructured and noisy visual information. Existing methods mainly use contrastive loss with video clips as the instances and learn visual representation by discriminating instances from each other, but they need a careful treatment of negative pairs by either relying on large batch sizes, memory banks, extra modalities or customized mining strategies, which inevitably includes noisy data. In this paper, we observe that the consistency between positive samples is the key to learn robust video representation. Specifically, we propose two tasks to learn the appearance and speed consistency, respectively. The appearance consistency task aims to maximize the similarity between two clips of the same video with different playback speeds. The speed consistency task aims to maximize the similarity between two clips with the same playback speed but different appearance information. We show that optimizing the two tasks jointly consistently improves the performance on downstream tasks, e.g., action recognition and video retrieval. Remarkably, for action recognition on the UCF-101 dataset, we achieve 90.8\% accuracy without using any extra modalities or negative pairs for unsupervised pretraining, which outperforms the ImageNet supervised pretrained model. Codes and models will be available.",0
"Our focus is on self-supervised video representation learning, which presents difficulties such as the absence of explicit supervision labels and the presence of unstructured and noisy visual data. While current methods utilize contrastive loss with video clips as instances to distinguish between samples, they require careful handling of negative pairs, often relying on large batch sizes, memory banks, extra modalities, or customized mining strategies. Such approaches inevitably incorporate noisy data. Our paper proposes that consistency between positive samples is vital for learning robust video representation. We introduce two tasks to achieve this: appearance consistency, which maximizes similarity between two clips of the same video with different playback speeds, and speed consistency, which maximizes similarity between two clips with the same playback speed but different appearance information. Joint optimization of the two tasks improves performance on downstream tasks, including action recognition and video retrieval. We attain an impressive 90.8% accuracy for action recognition on the UCF-101 dataset without using any extra modalities or negative pairs for unsupervised pretraining, outperforming the ImageNet supervised pretrained model. Our codes and models will be available.",1
"The crux of self-supervised video representation learning is to build general features from unlabeled videos. However, most recent works have mainly focused on high-level semantics and neglected lower-level representations and their temporal relationship which are crucial for general video understanding. To address these challenges, this paper proposes a multi-level feature optimization framework to improve the generalization and temporal modeling ability of learned video representations. Concretely, high-level features obtained from naive and prototypical contrastive learning are utilized to build distribution graphs, guiding the process of low-level and mid-level feature learning. We also devise a simple temporal modeling module from multi-level features to enhance motion pattern learning. Experiments demonstrate that multi-level feature optimization with the graph constraint and temporal modeling can greatly improve the representation ability in video understanding. Code is available at https://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization.",0
"The main focus of self-supervised video representation learning is to develop general features from videos that are not labeled. However, recent works have prioritized high-level semantics and neglected lower-level representations and their temporal relationship, which are critical for overall video comprehension. This paper introduces a multi-level feature optimization framework to enhance the ability of learned video representations to generalize and model time. Specifically, naive and prototypical contrastive learning is used to generate high-level features, which are then utilized to create distribution graphs to guide the learning of low-level and mid-level features. Additionally, a straightforward temporal modeling module is created from multi-level features to improve motion pattern learning. Through experiments, it is demonstrated that multi-level feature optimization with the graph constraint and temporal modeling significantly improves video comprehension. Code for this research can be found at https://github.com/shvdiwnkozbw/Video-Representation-via-Multi-level-Optimization.",1
"Long-range and short-range temporal modeling are two complementary and crucial aspects of video recognition. Most of the state-of-the-arts focus on short-range spatio-temporal modeling and then average multiple snippet-level predictions to yield the final video-level prediction. Thus, their video-level prediction does not consider spatio-temporal features of how video evolves along the temporal dimension. In this paper, we introduce a novel Dynamic Segment Aggregation (DSA) module to capture relationship among snippets. To be more specific, we attempt to generate a dynamic kernel for a convolutional operation to aggregate long-range temporal information among adjacent snippets adaptively. The DSA module is an efficient plug-and-play module and can be combined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform powerful long-range modeling with minimal overhead. The final video architecture, coined as DSANet. We conduct extensive experiments on several video recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400, Something-Something V1 and ActivityNet) to show its superiority. Our proposed DSA module is shown to benefit various video recognition models significantly. For example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is improved from 74.9% to 78.2% on Kinetics-400. Codes are available at https://github.com/whwu95/DSANet.",0
"Video recognition requires both long-range and short-range temporal modeling. While many state-of-the-art models focus on short-range spatio-temporal modeling and average multiple snippet-level predictions to yield a final video-level prediction, this approach fails to consider how video evolves along the temporal dimension. To address this issue, we introduce a novel Dynamic Segment Aggregation (DSA) module that generates a dynamic kernel for a convolutional operation to adaptively aggregate long-range temporal information among adjacent snippets. The DSA module is efficient and can be combined with off-the-shelf clip-based models to perform powerful long-range modeling with minimal overhead. Our final video architecture, DSANet, is tested on several video recognition benchmarks and shown to outperform other models significantly. We provide codes for DSANet on https://github.com/whwu95/DSANet.",1
"We address the problem of visible-infrared person re-identification (VI-reID), that is, retrieving a set of person images, captured by visible or infrared cameras, in a cross-modal setting. Two main challenges in VI-reID are intra-class variations across person images, and cross-modal discrepancies between visible and infrared images. Assuming that the person images are roughly aligned, previous approaches attempt to learn coarse image- or rigid part-level person representations that are discriminative and generalizable across different modalities. However, the person images, typically cropped by off-the-shelf object detectors, are not necessarily well-aligned, which distract discriminative person representation learning. In this paper, we introduce a novel feature learning framework that addresses these problems in a unified way. To this end, we propose to exploit dense correspondences between cross-modal person images. This allows to address the cross-modal discrepancies in a pixel-level, suppressing modality-related features from person representations more effectively. This also encourages pixel-wise associations between cross-modal local features, further facilitating discriminative feature learning for VI-reID. Extensive experiments and analyses on standard VI-reID benchmarks demonstrate the effectiveness of our approach, which significantly outperforms the state of the art.",0
"Our focus is on solving the challenge of visible-infrared person re-identification (VI-reID), which involves retrieving a collection of person images from visible or infrared cameras in a cross-modal scenario. The challenges in VI-reID include differences in person images within a class and disparities between visible and infrared images. Existing approaches aim to learn rough image- or part-level person representations that are both discriminative and generalizable across modalities, assuming that the person images are approximately aligned. However, the person images, which are commonly cropped by off-the-shelf object detectors, may not be well-aligned and therefore impede discriminative person representation learning. In this study, we propose a new feature learning framework that addresses these issues. Our solution involves utilizing dense correspondences between cross-modal person images to address the cross-modal discrepancies at the pixel-level and to more effectively suppress modality-related features from person representations. This also encourages pixel-wise associations between cross-modal local features, further facilitating discriminative feature learning for VI-reID. We conducted extensive experiments and analyses on standard VI-reID benchmarks, demonstrating the effectiveness of our approach, which outperforms the state of the art.",1
"The natural association between visual observations and their corresponding sound provides powerful self-supervisory signals for learning video representations, which makes the ever-growing amount of online videos an attractive source of training data. However, large portions of online videos contain irrelevant audio-visual signals because of edited/overdubbed audio, and models trained on such uncurated videos have shown to learn suboptimal representations. Therefore, existing approaches rely almost exclusively on datasets with predetermined taxonomies of semantic concepts, where there is a high chance of audio-visual correspondence. Unfortunately, constructing such datasets require labor intensive manual annotation and/or verification, which severely limits the utility of online videos for large-scale learning. In this work, we present an automatic dataset curation approach based on subset optimization where the objective is to maximize the mutual information between audio and visual channels in videos. We demonstrate that our approach finds videos with high audio-visual correspondence and show that self-supervised models trained on our data achieve competitive performances compared to models trained on existing manually curated datasets. The most significant benefit of our approach is scalability: We release ACAV100M that contains 100 million videos with high audio-visual correspondence, ideal for self-supervised video representation learning.",0
"Learning video representations benefits from the natural connection between visual observations and their corresponding sound, which provides potent self-supervisory signals. This has made online videos an appealing source of training data due to their ever-growing amount. However, a large portion of online videos have irrelevant audio-visual signals due to edited or overdubbed audio, leading to suboptimal representations in models trained on uncurated videos. Existing methods rely on datasets with predetermined taxonomies of semantic concepts, but constructing these datasets requires manual annotation and verification, limiting their utility for large-scale learning. We present an automatic dataset curation approach based on subset optimization to maximize mutual information between audio and visual channels in videos. Our approach identifies videos with high audio-visual correspondence, allowing self-supervised models trained on our data to achieve competitive performances compared to models trained on existing manually curated datasets. The most significant advantage of our approach is scalability, as we release ACAV100M, which contains 100 million videos with high audio-visual correspondence, making it ideal for self-supervised video representation learning.",1
"Collecting large annotated datasets in Remote Sensing is often expensive and thus can become a major obstacle for training advanced machine learning models. Common techniques of addressing this issue, based on the underlying idea of pre-training the Deep Neural Networks (DNN) on freely available large datasets, cannot be used for Remote Sensing due to the unavailability of such large-scale labeled datasets and the heterogeneity of data sources caused by the varying spatial and spectral resolution of different sensors. Self-supervised learning is an alternative approach that learns feature representation from unlabeled images without using any human annotations. In this paper, we introduce a new method for land cover mapping by using a clustering based pretext task for self-supervised learning. We demonstrate the effectiveness of the method on two societally relevant applications from the aspect of segmentation performance, discriminative feature representation learning and the underlying cluster structure. We also show the effectiveness of the active sampling using the clusters obtained from our method in improving the mapping accuracy given a limited budget of annotating.",0
"Acquiring extensive annotated datasets for Remote Sensing can be a costly endeavor, posing a significant challenge for training advanced machine learning models. The conventional approach to address this problem involves pre-training Deep Neural Networks (DNN) on freely available vast datasets. However, this method cannot be used for Remote Sensing due to the unavailability of large-scale labeled datasets and the heterogeneity of data sources caused by different sensor resolutions. Alternatively, self-supervised learning can learn feature representation from unlabeled images without human annotations. In this paper, we propose a new self-supervised learning method for land cover mapping using a clustering-based pretext task. We demonstrate the effectiveness of our method on two socially relevant applications in terms of segmentation performance, discriminative feature representation learning, and cluster structure. Additionally, we showcase the effectiveness of active sampling using the clusters obtained from our method in improving mapping accuracy within a limited annotation budget.",1
"In this work, we address the problem of unsupervised domain adaptation for person re-ID where annotations are available for the source domain but not for target. Previous methods typically follow a two-stage optimization pipeline, where the network is first pre-trained on source and then fine-tuned on target with pseudo labels created by feature clustering. Such methods sustain two main limitations. (1) The label noise may hinder the learning of discriminative features for recognizing target classes. (2) The domain gap may hinder knowledge transferring from source to target. We propose three types of technical schemes to alleviate these issues. First, we propose a cluster-wise contrastive learning algorithm (CCL) by iterative optimization of feature learning and cluster refinery to learn noise-tolerant representations in the unsupervised manner. Second, we adopt a progressive domain adaptation (PDA) strategy to gradually mitigate the domain gap between source and target data. Third, we propose Fourier augmentation (FA) for further maximizing the class separability of re-ID models by imposing extra constraints in the Fourier space. We observe that these proposed schemes are capable of facilitating the learning of discriminative feature representations. Experiments demonstrate that our method consistently achieves notable improvements over the state-of-the-art unsupervised re-ID methods on multiple benchmarks, e.g., surpassing MMT largely by 8.1\%, 9.9\%, 11.4\% and 11.1\% mAP on the Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT tasks, respectively.",0
"The objective of this study is to address the issue of unsupervised domain adaptation for person re-ID, where annotations are available only for the source domain and not for the target. Prior methods follow a two-stage optimization pipeline where the network is pre-trained on the source and fine-tuned on the target, using pseudo labels created by feature clustering. However, this method has two primary limitations. Firstly, label noise may hinder the learning of discriminative features for recognizing target classes. Secondly, the domain gap may prevent knowledge transfer from source to target. To overcome these limitations, we propose three technical schemes. Firstly, we propose a cluster-wise contrastive learning algorithm (CCL) that learns noise-tolerant representations by iteratively optimizing feature learning and cluster refinery in an unsupervised manner. Secondly, we adopt a progressive domain adaptation (PDA) strategy to gradually mitigate the domain gap between source and target data. Lastly, we propose Fourier augmentation (FA) to maximize class separability of re-ID models by imposing extra constraints in the Fourier space. Our proposed schemes facilitate the learning of discriminative feature representations. Our experiments show that our method consistently outperforms state-of-the-art unsupervised re-ID methods on multiple benchmarks, including surpassing MMT by 8.1\%, 9.9\%, 11.4\% and 11.1\% mAP on the Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT tasks, respectively.",1
"Data analysis has become a necessity in the modern era of cricket. Everything from effective team management to match win predictions use some form of analytics. Meaningful data representations are necessary for efficient analysis of data. In this study we investigate the use of adaptive (learnable) embeddings to represent inter-related features (such as players, teams, etc). The data used for this study is collected from a classical T20 tournament IPL (Indian Premier League). To naturally facilitate the learning of meaningful representations of features for accurate data analysis, we formulate a deep representation learning framework which jointly learns a custom set of embeddings (which represents our features of interest) through the minimization of a contrastive loss. We base our objective on a set of classes obtained as a result of hierarchical clustering on the overall run rate of an innings. It's been assessed that the framework ensures greater generality in the obtained embeddings, on top of which a task based analysis of overall run rate prediction was done to show the reliability of the framework.",0
"In the modern era of cricket, data analysis is essential. Analytics are used for effective team management and match win predictions, among other things. Efficient data analysis requires meaningful representations of data. This study examines the use of adaptive embeddings to represent inter-related features, such as players and teams. The data used is from the IPL T20 tournament. To facilitate the learning of accurate representations of features, a deep representation learning framework is formulated. This framework jointly learns a custom set of embeddings by minimizing a contrastive loss. The objective is based on a set of classes obtained from hierarchical clustering on the overall run rate of an innings. The framework is shown to produce more generalized embeddings, and a task-based analysis of overall run rate prediction demonstrates its reliability.",1
"Automated segmentation in medical image analysis is a challenging task that requires a large amount of manually labeled data. However, most existing learning-based approaches usually suffer from limited manually annotated medical data, which poses a major practical problem for accurate and robust medical image segmentation. In addition, most existing semi-supervised approaches are usually not robust compared with the supervised counterparts, and also lack explicit modeling of geometric structure and semantic information, both of which limit the segmentation accuracy. In this work, we present SimCVD, a simple contrastive distillation framework that significantly advances state-of-the-art voxel-wise representation learning. We first describe an unsupervised training strategy, which takes two views of an input volume and predicts their signed distance maps of object boundaries in a contrastive objective, with only two independent dropout as mask. This simple approach works surprisingly well, performing on the same level as previous fully supervised methods with much less labeled data. We hypothesize that dropout can be viewed as a minimal form of data augmentation and makes the network robust to representation collapse. Then, we propose to perform structural distillation by distilling pair-wise similarities. We evaluate SimCVD on two popular datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT dataset. The results on the LA dataset demonstrate that, in two types of labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of 90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to previous best results. Our method can be trained in an end-to-end fashion, showing the promise of utilizing SimCVD as a general framework for downstream tasks, such as medical image synthesis and registration.",0
"Medical image analysis poses a challenge in automated segmentation, as it requires a significant amount of manually labeled data. Unfortunately, current learning-based approaches have limited access to annotated medical data, which creates practical issues for accurate and robust medical image segmentation. Semi-supervised approaches are not as robust as their supervised counterparts and lack explicit modeling of geometric structure and semantic information, which further limits segmentation accuracy. This study introduces SimCVD, a simple contrastive distillation framework that significantly improves voxel-wise representation learning. The unsupervised training strategy predicts two views of an input volume and their signed distance maps of object boundaries in a contrastive objective with only two independent dropout as mask. This simple approach works surprisingly well, performing similarly to previous fully supervised methods with much less labeled data. Structural distillation is performed by distilling pairwise similarities. SimCVD achieves an average Dice score of 90.85% and 89.03% on the Left Atrial Segmentation Challenge dataset with labeled ratios of 20% and 10%, respectively, resulting in a 0.91% and 2.22% improvement compared to previous best results. The framework shows promise for downstream tasks, such as medical image synthesis and registration.",1
"This paper presents solo-learn, a library of self-supervised methods for visual representation learning. Implemented in Python, using Pytorch and Pytorch lightning, the library fits both research and industry needs by featuring distributed training pipelines with mixed-precision, faster data loading via Nvidia DALI, online linear evaluation for better prototyping, and many additional training tricks. Our goal is to provide an easy-to-use library comprising a large amount of Self-supervised Learning (SSL) methods, that can be easily extended and fine-tuned by the community. solo-learn opens up avenues for exploiting large-budget SSL solutions on inexpensive smaller infrastructures and seeks to democratize SSL by making it accessible to all. The source code is available at https://github.com/vturrisi/solo-learn.",0
"The purpose of this article is to introduce solo-learn, a Python library that utilizes Pytorch and Pytorch lightning for self-supervised visual representation learning. The library caters to both industry and research needs by featuring distributed training pipelines with mixed-precision, fast data loading through Nvidia DALI, online linear evaluation for better prototyping, and other valuable training techniques. Our aim is to provide a user-friendly library with a wide range of Self-supervised Learning (SSL) methods that can be easily expanded and modified by the community. By making SSL accessible to all, solo-learn enables the use of large-budget SSL solutions on less expensive infrastructures. The source code is accessible at https://github.com/vturrisi/solo-learn.",1
"Machine learning on sets towards sequential output is an important and ubiquitous task, with applications ranging from language modeling and meta-learning to multi-agent strategy games and power grid optimization. Combining elements of representation learning and structured prediction, its two primary challenges include obtaining a meaningful, permutation invariant set representation and subsequently utilizing this representation to output a complex target permutation. This paper provides a comprehensive introduction to the field as well as an overview of important machine learning methods tackling both of these key challenges, with a detailed qualitative comparison of selected model architectures.",0
"The task of applying machine learning to sets for sequential output is significant and widely used in various areas such as language modeling, meta-learning, multi-agent strategy games, and power grid optimization. This task involves representation learning and structured prediction, and its primary difficulties include obtaining a significant set representation that is invariant to permutation and using this representation to produce a complicated target permutation. This article offers a complete introduction to the field and examines essential machine learning techniques that address both of these primary challenges. Furthermore, this paper presents a thorough qualitative comparison of selected model architectures.",1
"Tremendous progress has been made in visual representation learning, notably with the recent success of self-supervised contrastive learning methods. Supervised contrastive learning has also been shown to outperform its cross-entropy counterparts by leveraging labels for choosing where to contrast. However, there has been little work to explore the transfer capability of contrastive learning to a different domain. In this paper, we conduct a comprehensive study on the transferability of learned representations of different contrastive approaches for linear evaluation, full-network transfer, and few-shot recognition on 12 downstream datasets from different domains, and object detection tasks on MSCOCO and VOC0712. The results show that the contrastive approaches learn representations that are easily transferable to a different downstream task. We further observe that the joint objective of self-supervised contrastive loss with cross-entropy/supervised-contrastive loss leads to better transferability of these models over their supervised counterparts. Our analysis reveals that the representations learned from the contrastive approaches contain more low/mid-level semantics than cross-entropy models, which enables them to quickly adapt to a new task. Our codes and models will be publicly available to facilitate future research on transferability of visual representations.",0
"Recent advancements in visual representation learning have been remarkable, particularly with the success of self-supervised contrastive learning methods. Supervised contrastive learning has also proven to be more effective than its cross-entropy counterparts due to the utilization of labels for selecting where to contrast. However, little research has been conducted to explore the transferability of contrastive learning to a different field. This study comprehensively examines the transferability of learned representations from different contrastive methods for linear evaluation, full-network transfer, and few-shot recognition on 12 downstream datasets from various domains, as well as object detection tasks on MSCOCO and VOC0712. The outcomes demonstrate that contrastive approaches produce easily transferable representations for a different downstream task. Furthermore, our analysis shows that the joint objective of self-supervised contrastive loss with cross-entropy/supervised-contrastive loss yields better transferability of these models over their supervised counterparts. The low/mid-level semantics in the representations learned from contrastive approaches enable them to quickly adapt to a new task. Our codes and models will be made publicly accessible to aid future research on visual representation transferability.",1
"Disentangled visual representations have largely been studied with generative models such as Variational AutoEncoders (VAEs). While prior work has focused on generative methods for disentangled representation learning, these approaches do not scale to large datasets due to current limitations of generative models. Instead, we explore regularization methods with contrastive learning, which could result in disentangled representations that are powerful enough for large scale datasets and downstream applications. However, we find that unsupervised disentanglement is difficult to achieve due to optimization and initialization sensitivity, with trade-offs in task performance. We evaluate disentanglement with downstream tasks, analyze the benefits and disadvantages of each regularization used, and discuss future directions.",0
"Previous research has mainly examined disentangled visual representations using generative models like Variational AutoEncoders (VAEs). However, these models have limitations that hinder their scalability to handle large datasets. Therefore, we explore regularization methods using contrastive learning to achieve disentangled representations that are capable of handling downstream applications on a larger scale. However, we encounter challenges in achieving unsupervised disentanglement due to sensitivity in initialization and optimization, which can affect task performance. We evaluate disentanglement through downstream tasks and analyze the advantages and drawbacks of each regularization method used. Finally, we discuss future research directions.",1
"Visual engagement in social media platforms comprises interactions with photo posts including comments, shares, and likes. In this paper, we leverage such visual engagement clues as supervisory signals for representation learning. However, learning from engagement signals is non-trivial as it is not clear how to bridge the gap between low-level visual information and high-level social interactions. We present VisE, a weakly supervised learning approach, which maps social images to pseudo labels derived by clustered engagement signals. We then study how models trained in this way benefit subjective downstream computer vision tasks such as emotion recognition or political bias detection. Through extensive studies, we empirically demonstrate the effectiveness of VisE across a diverse set of classification tasks beyond the scope of conventional recognition.",0
"The engagement of social media users with photo posts, through comments, shares, and likes, is referred to as visual engagement. This paper explores the use of visual engagement as supervisory signals for representation learning. However, this process is challenging since it is unclear how to connect low-level visual information with high-level social interactions. To address this issue, we introduce VisE, a weakly supervised learning technique that maps social images to pseudo labels based on clustered engagement signals. We then investigate how models trained using VisE can enhance subjective downstream computer vision tasks, such as emotion recognition and political bias detection. We demonstrate the effectiveness of VisE across a broad range of classification tasks beyond standard recognition through extensive experimentation.",1
"Deep learning algorithms mine knowledge from the training data and thus would likely inherit the dataset's bias information. As a result, the obtained model would generalize poorly and even mislead the decision process in real-life applications. We propose to remove the bias information misused by the target task with a cross-sample adversarial debiasing (CSAD) method. CSAD explicitly extracts target and bias features disentangled from the latent representation generated by a feature extractor and then learns to discover and remove the correlation between the target and bias features. The correlation measurement plays a critical role in adversarial debiasing and is conducted by a cross-sample neural mutual information estimator. Moreover, we propose joint content and local structural representation learning to boost mutual information estimation for better performance. We conduct thorough experiments on publicly available datasets to validate the advantages of the proposed method over state-of-the-art approaches.",0
"The training data used by deep learning algorithms may contain biased information, which can negatively impact the model's ability to generalize and make accurate decisions in real-life scenarios. To address this issue, we suggest a cross-sample adversarial debiasing (CSAD) method that removes the biased information associated with the target task. The CSAD technique separates the target and bias features from the underlying latent representation generated by a feature extractor and then eliminates the correlation between these features. A neural mutual information estimator is used to measure this correlation, with joint content and local structural representation learning employed to enhance its accuracy. We demonstrate the effectiveness of our approach through extensive experiments on public datasets, outperforming existing state-of-the-art methods.",1
"Large-scale pretraining of visual representations has led to state-of-the-art performance on a range of benchmark computer vision tasks, yet the benefits of these techniques at extreme scale in complex production systems has been relatively unexplored. We consider the case of a popular visual discovery product, where these representations are trained with multi-task learning, from use-case specific visual understanding (e.g. skin tone classification) to general representation learning for all visual content (e.g. embeddings for retrieval). In this work, we describe how we (1) generate a dataset with over a billion images via large weakly-supervised pretraining to improve the performance of these visual representations, and (2) leverage Transformers to replace the traditional convolutional backbone, with insights into both system and performance improvements, especially at 1B+ image scale. To support this backbone model, we detail a systematic approach to deriving weakly-supervised image annotations from heterogenous text signals, demonstrating the benefits of clustering techniques to handle the long-tail distribution of image labels. Through a comprehensive study of offline and online evaluation, we show that large-scale Transformer-based pretraining provides significant benefits to industry computer vision applications. The model is deployed in a production visual shopping system, with 36% improvement in top-1 relevance and 23% improvement in click-through volume. We conduct extensive experiments to better understand the empirical relationships between Transformer-based architectures, dataset scale, and the performance of production vision systems.",0
"Although large-scale pretraining of visual representations has resulted in remarkable performance on benchmark computer vision tasks, its advantages in complex production systems at an extreme scale are still largely unexplored. We examine a widely used visual discovery product that utilizes multi-task learning to train representations for both specific visual understanding, such as skin tone classification, and general representation learning for all visual content, including embeddings for retrieval. Our work focuses on two main objectives: (1) generating a dataset of over a billion images through large weakly-supervised pretraining to enhance the performance of visual representations, and (2) utilizing Transformers to substitute the conventional convolutional backbone while providing insights into both system and performance improvements, particularly at an image scale of over one billion. We explain how we systematically derive weakly-supervised image annotations from various text signals to support this backbone model, demonstrating how clustering techniques can manage the long-tail distribution of image labels. Our comprehensive study of offline and online evaluation shows that large-scale Transformer-based pretraining offers significant benefits to computer vision applications in the industry. We deploy the model in a production visual shopping system, achieving a 36% increase in top-1 relevance and a 23% increase in click-through volume. We conduct extensive experiments to better comprehend the empirical relationships between Transformer-based architectures, dataset scale, and the performance of production vision systems.",1
"Deep networks for Monocular Depth Estimation (MDE) have achieved promising performance recently and it is of great importance to further understand the interpretability of these networks. Existing methods attempt to provide posthoc explanations by investigating visual cues, which may not explore the internal representations learned by deep networks. In this paper, we find that some hidden units of the network are selective to certain ranges of depth, and thus such behavior can be served as a way to interpret the internal representations. Based on our observations, we quantify the interpretability of a deep MDE network by the depth selectivity of its hidden units. Moreover, we then propose a method to train interpretable MDE deep networks without changing their original architectures, by assigning a depth range for each unit to select. Experimental results demonstrate that our method is able to enhance the interpretability of deep MDE networks by largely improving the depth selectivity of their units, while not harming or even improving the depth estimation accuracy. We further provide a comprehensive analysis to show the reliability of selective units, the applicability of our method on different layers, models, and datasets, and a demonstration on analysis of model error. Source code and models are available at https://github.com/youzunzhi/InterpretableMDE .",0
"Recently, deep networks for Monocular Depth Estimation (MDE) have shown promising performance. However, understanding the interpretability of these networks is crucial. Past methods for providing explanations have only investigated visual cues, which do not delve into the internal representations learned by deep networks. In this study, we have discovered that some hidden units of the network are selective to specific depth ranges. This behavior can be used as a way to interpret the internal representations. We have developed a method to measure the interpretability of a deep MDE network by determining the depth selectivity of its hidden units. Furthermore, we have proposed a technique to train interpretable MDE deep networks without altering their original architectures by assigning a depth range for each unit to select. Our experimental results demonstrate that our method can improve the interpretability of deep MDE networks by significantly enhancing the depth selectivity of their units, without compromising the depth estimation accuracy. We have also conducted a comprehensive analysis to show the dependability of selective units, the applicability of our method on different layers, models, and datasets, and a demonstration on analyzing model error. The source code and models are available at https://github.com/youzunzhi/InterpretableMDE.",1
"Image Retrieval is a fundamental task of obtaining images similar to the query one from a database. A common image retrieval practice is to firstly retrieve candidate images via similarity search using global image features and then re-rank the candidates by leveraging their local features. Previous learning-based studies mainly focus on either global or local image representation learning to tackle the retrieval task. In this paper, we abandon the two-stage paradigm and seek to design an effective single-stage solution by integrating local and global information inside images into compact image representations. Specifically, we propose a Deep Orthogonal Local and Global (DOLG) information fusion framework for end-to-end image retrieval. It attentively extracts representative local information with multi-atrous convolutions and self-attention at first. Components orthogonal to the global image representation are then extracted from the local information. At last, the orthogonal components are concatenated with the global representation as a complementary, and then aggregation is performed to generate the final representation. The whole framework is end-to-end differentiable and can be trained with image-level labels. Extensive experimental results validate the effectiveness of our solution and show that our model achieves state-of-the-art image retrieval performances on Revisited Oxford and Paris datasets.",0
"The task of Image Retrieval involves obtaining images from a database that are similar to a query image. The typical approach is to use global image features to retrieve candidate images, followed by re-ranking the candidates based on their local features. Past studies have focused on learning either global or local image representations. However, in this paper, we propose a single-stage solution that combines local and global information into compact image representations. We introduce the Deep Orthogonal Local and Global (DOLG) information fusion framework, which extracts local information using multi-atrous convolutions and self-attention, and then extracts orthogonal components from the local information that complement the global representation. Finally, the framework aggregates the orthogonal components and global representation to generate the final representation. This end-to-end differentiable framework can be trained with image-level labels and achieves state-of-the-art image retrieval performances on Revisited Oxford and Paris datasets.",1
"In the application of machine learning to remote sensing, labeled data is often scarce or expensive, which impedes the training of powerful models like deep convolutional neural networks. Although unlabeled data is abundant, recent self-supervised learning approaches are ill-suited to the remote sensing domain. In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. We propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. Using a dataset of 47 million unlabeled coterminous image triplets, we train an encoder to produce semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused. Our code is available at https://storage.cloud.google.com/public-published-datasets/csf_code.zip.",0
"Machine learning applied to remote sensing often faces challenges due to the scarcity or high cost of labeled data, which can hinder the training of powerful models like deep convolutional neural networks. Although there is an abundance of unlabeled data available, current self-supervised learning approaches are not well-suited to the remote sensing domain. Moreover, most remote sensing applications use only a fraction of the available multi-sensor, multi-channel information, highlighting the need for fused multi-sensor representations. To address these challenges, we propose a novel self-supervised training objective called Contrastive Sensor Fusion, which leverages data from multiple sources to learn useful representations of every possible combination of those sources. This method utilizes information common across multiple sensors and bands, training a single model to produce a representation that remains consistent when any subset of its input channels is used. We trained an encoder using a dataset of 47 million unlabeled coterminous image triplets, which produced semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperformed fully supervised ImageNet weights in a remote sensing classification task, with performance improving as more sensors are fused. Our code is available at https://storage.cloud.google.com/public-published-datasets/csf_code.zip.",1
"We propose a compact and effective framework to fuse multimodal features at multiple layers in a single network. The framework consists of two innovative fusion schemes. Firstly, unlike existing multimodal methods that necessitate individual encoders for different modalities, we verify that multimodal features can be learnt within a shared single network by merely maintaining modality-specific batch normalization layers in the encoder, which also enables implicit fusion via joint feature representation learning. Secondly, we propose a bidirectional multi-layer fusion scheme, where multimodal features can be exploited progressively. To take advantage of such scheme, we introduce two asymmetric fusion operations including channel shuffle and pixel shift, which learn different fused features with respect to different fusion directions. These two operations are parameter-free and strengthen the multimodal feature interactions across channels as well as enhance the spatial feature discrimination within channels. We conduct extensive experiments on semantic segmentation and image translation tasks, based on three publicly available datasets covering diverse modalities. Results indicate that our proposed framework is general, compact and is superior to state-of-the-art fusion frameworks.",0
"In this study, we present a concise and efficient framework for combining multimodal features across multiple layers within a single network. Our framework includes two innovative techniques for fusion. Firstly, contrary to current multimodal approaches that require separate encoders for each modality, we demonstrate that multimodal features can be acquired through a shared single network by maintaining modality-specific batch normalization layers in the encoder. This approach allows for implicit fusion through joint feature representation learning. Secondly, we introduce a bidirectional multi-layer fusion approach that allows for progressive utilization of multimodal features. We also propose two asymmetrical fusion operations  channel shuffle and pixel shift  that learn distinct fused features in different fusion directions. These parameter-free operations enhance multimodal feature interactions across channels and improve spatial feature discrimination within channels. We evaluate our framework on three publicly available datasets that cover diverse modalities and tasks such as semantic segmentation and image translation. Our experimental results show that our framework is more effective than state-of-the-art fusion frameworks and is both general and compact.",1
"Recently, deep neural networks have expanded the state-of-art in various scientific fields and provided solutions to long standing problems across multiple application domains. Nevertheless, they also suffer from weaknesses since their optimal performance depends on massive amounts of training data and the tuning of an extended number of parameters. As a countermeasure, some deep-forest methods have been recently proposed, as efficient and low-scale solutions. Despite that, these approaches simply employ label classification probabilities as induced features and primarily focus on traditional classification and regression tasks, leaving multi-output prediction under-explored. Moreover, recent work has demonstrated that tree-embeddings are highly representative, especially in structured output prediction. In this direction, we propose a novel deep tree-ensemble (DTE) model, where every layer enriches the original feature set with a representation learning component based on tree-embeddings. In this paper, we specifically focus on two structured output prediction tasks, namely multi-label classification and multi-target regression. We conducted experiments using multiple benchmark datasets and the obtained results confirm that our method provides superior results to state-of-the-art methods in both tasks.",0
"In various scientific fields, deep neural networks have made significant advancements and solved persistent issues across multiple application domains. However, they have limitations, relying on extensive training data and parameter tuning for optimal performance. Deep-forest methods have emerged as an efficient and low-scale solution to overcome these weaknesses. Yet, these approaches only utilize label classification probabilities as induced features, primarily focusing on traditional classification and regression tasks, while multi-output prediction remains under-explored. Recent work has shown that tree-embeddings are highly representative, particularly in structured output prediction. Therefore, we propose a novel deep tree-ensemble (DTE) model that enriches the original feature set with a representation learning component based on tree-embeddings in each layer. Our focus is on two structured output prediction tasks, multi-label classification, and multi-target regression, and our method outperforms state-of-the-art methods in both tasks, as confirmed by experiments using various benchmark datasets.",1
"We present a novel and flexible architecture for point cloud segmentation with dual-representation iterative learning. In point cloud processing, different representations have their own pros and cons. Thus, finding suitable ways to represent point cloud data structure while keeping its own internal physical property such as permutation and scale-invariant is a fundamental problem. Therefore, we propose our work, DRINet, which serves as the basic network structure for dual-representation learning with great flexibility at feature transferring and less computation cost, especially for large-scale point clouds. DRINet mainly consists of two modules called Sparse Point-Voxel Feature Extraction and Sparse Voxel-Point Feature Extraction. By utilizing these two modules iteratively, features can be propagated between two different representations. We further propose a novel multi-scale pooling layer for pointwise locality learning to improve context information propagation. Our network achieves state-of-the-art results for point cloud classification and segmentation tasks on several datasets while maintaining high runtime efficiency. For large-scale outdoor scenarios, our method outperforms state-of-the-art methods with a real-time inference speed of 62ms per frame.",0
"We introduce DRINet, a new and adaptable architecture for point cloud segmentation that employs dual-representation iterative learning. The challenge in point cloud processing is to identify suitable representations that retain the data structure's internal physical properties, such as permutation and scale-invariance, while accounting for their advantages and limitations. To address this, we propose DRINet as a flexible network structure that supports efficient feature transfer and less computation cost, particularly for large-scale point clouds. The network comprises two modules, Sparse Point-Voxel Feature Extraction and Sparse Voxel-Point Feature Extraction, which allow for iterative feature propagation between different representations. We also introduce a novel multi-scale pooling layer for pointwise locality learning to enhance context information transfer. Our method achieves state-of-the-art results for point cloud classification and segmentation on multiple datasets, with high runtime efficiency. For large-scale outdoor scenarios, our approach surpasses existing methods with a real-time inference speed of 62ms per frame.",1
"While self-supervised representation learning (SSL) has received widespread attention from the community, recent research argue that its performance will suffer a cliff fall when the model size decreases. The current method mainly relies on contrastive learning to train the network and in this work, we propose a simple yet effective Distilled Contrastive Learning (DisCo) to ease the issue by a large margin. Specifically, we find the final embedding obtained by the mainstream SSL methods contains the most fruitful information, and propose to distill the final embedding to maximally transmit a teacher's knowledge to a lightweight model by constraining the last embedding of the student to be consistent with that of the teacher. In addition, in the experiment, we find that there exists a phenomenon termed Distilling BottleNeck and present to enlarge the embedding dimension to alleviate this problem. Our method does not introduce any extra parameter to lightweight models during deployment. Experimental results demonstrate that our method achieves the state-of-the-art on all lightweight models. Particularly, when ResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear result of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50, but the number of parameters of EfficientNet-B0 is only 9.4\%/16.3\% of ResNet-101/ResNet-50. Code is available at https://github. com/Yuting-Gao/DisCo-pytorch.",0
"Although self-supervised representation learning (SSL) has gained significant attention, recent research indicates that its performance declines significantly when the model size is reduced. The current approach primarily relies on contrastive learning to train the network. To mitigate this issue, we propose a Distilled Contrastive Learning (DisCo) method that is simple yet effective. Our approach distills the final embedding of the mainstream SSL methods to maximize the transmission of a teacher's knowledge to a lightweight model while ensuring consistency between the last embedding of the student and that of the teacher. We also identify a phenomenon called Distilling BottleNeck and present a solution by increasing the embedding dimension. Our method does not introduce any extra parameters during deployment and achieves state-of-the-art results on all lightweight models. For instance, when ResNet-101/ResNet-50 is used as a teacher to teach EfficientNet-B0, our method achieves a linear result that is similar to ResNet-101/ResNet-50, despite EfficientNet-B0 having significantly fewer parameters. The code for our approach is available at https://github.com/Yuting-Gao/DisCo-pytorch.",1
"Skeleton-based human action recognition has attracted increasing attention in recent years. However, most of the existing works focus on supervised learning which requiring a large number of annotated action sequences that are often expensive to collect. We investigate unsupervised representation learning for skeleton action recognition, and design a novel skeleton cloud colorization technique that is capable of learning skeleton representations from unlabeled skeleton sequence data. Specifically, we represent a skeleton action sequence as a 3D skeleton cloud and colorize each point in the cloud according to its temporal and spatial orders in the original (unannotated) skeleton sequence. Leveraging the colorized skeleton point cloud, we design an auto-encoder framework that can learn spatial-temporal features from the artificial color labels of skeleton joints effectively. We evaluate our skeleton cloud colorization approach with action classifiers trained under different configurations, including unsupervised, semi-supervised and fully-supervised settings. Extensive experiments on NTU RGB+D and NW-UCLA datasets show that the proposed method outperforms existing unsupervised and semi-supervised 3D action recognition methods by large margins, and it achieves competitive performance in supervised 3D action recognition as well.",0
"Recently, there has been a growing interest in skeleton-based human action recognition. However, most of the current research focuses on supervised learning, which requires a significant amount of annotated action sequences. This process can be costly and time-consuming. In this study, we explore unsupervised representation learning for skeleton action recognition. Our approach involves a unique skeleton cloud colorization technique that learns skeleton representations from unlabelled skeleton sequence data. We represent a skeleton action sequence as a 3D skeleton cloud and colorize each point in the cloud based on its temporal and spatial orders in the original (unannotated) skeleton sequence. Using the colorized skeleton point cloud, we design an auto-encoder framework that can effectively learn spatial-temporal features from the artificial color labels of skeleton joints. We evaluate our approach with action classifiers trained under various configurations, including unsupervised, semi-supervised and fully-supervised settings. Our experiments on NTU RGB+D and NW-UCLA datasets demonstrate that our proposed method outperforms existing unsupervised and semi-supervised 3D action recognition methods significantly, while also achieving competitive performance in supervised 3D action recognition.",1
"We introduce the task of open-vocabulary visual instance search (OVIS). Given an arbitrary textual search query, Open-vocabulary Visual Instance Search (OVIS) aims to return a ranked list of visual instances, i.e., image patches, that satisfies the search intent from an image database. The term ""open vocabulary"" means that there are neither restrictions to the visual instance to be searched nor restrictions to the word that can be used to compose the textual search query. We propose to address such a search challenge via visual-semantic aligned representation learning (ViSA). ViSA leverages massive image-caption pairs as weak image-level (not instance-level) supervision to learn a rich cross-modal semantic space where the representations of visual instances (not images) and those of textual queries are aligned, thus allowing us to measure the similarities between any visual instance and an arbitrary textual query. To evaluate the performance of ViSA, we build two datasets named OVIS40 and OVIS1600 and also introduce a pipeline for error analysis. Through extensive experiments on the two datasets, we demonstrate ViSA's ability to search for visual instances in images not available during training given a wide range of textual queries including those composed of uncommon words. Experimental results show that ViSA achieves an mAP@50 of 21.9% on OVIS40 under the most challenging setting and achieves an mAP@6 of 14.9% on OVIS1600 dataset.",0
"The task of open-vocabulary visual instance search (OVIS) is presented, which involves returning a ranked list of image patches that satisfy an arbitrary textual search query from an image database. The ""open vocabulary"" aspect means that there are no restrictions on the visual instances or the words used in the queries. To tackle this challenge, a visual-semantic aligned representation learning (ViSA) approach is proposed, which leverages weak image-level supervision from massive image-caption pairs to learn a cross-modal semantic space where visual instance and textual query representations are aligned. Two datasets, OVIS40 and OVIS1600, are introduced to evaluate ViSA's performance, and an error analysis pipeline is developed. The experiments show that ViSA can successfully search for visual instances in unseen images using a wide range of textual queries, including uncommon words, achieving an mAP@50 of 21.9% on OVIS40 and an mAP@6 of 14.9% on OVIS1600.",1
"This paper strives for self-supervised learning of a feature space suitable for skeleton-based action recognition. Our proposal is built upon learning invariances to input skeleton representations and various skeleton augmentations via a noise contrastive estimation. In particular, we propose inter-skeleton contrastive learning, which learns from multiple different input skeleton representations in a cross-contrastive manner. In addition, we contribute several skeleton-specific spatial and temporal augmentations which further encourage the model to learn the spatio-temporal dynamics of skeleton data. By learning similarities between different skeleton representations as well as augmented views of the same sequence, the network is encouraged to learn higher-level semantics of the skeleton data than when only using the augmented views. Our approach achieves state-of-the-art performance for self-supervised learning from skeleton data on the challenging PKU and NTU datasets with multiple downstream tasks, including action recognition, action retrieval and semi-supervised learning. Code is available at https://github.com/fmthoker/skeleton-contrast.",0
"The aim of this paper is to achieve self-supervised learning for a feature space that is appropriate for action recognition based on skeletons. Our proposal is based on learning invariances to input skeleton representations and different skeleton augmentations through noise contrastive estimation. We suggest inter-skeleton contrastive learning, which involves learning from multiple input skeleton representations in a cross-contrastive manner. Additionally, we offer several skeleton-specific spatial and temporal augmentations that further encourage the model to learn the spatio-temporal dynamics of skeleton data. By learning similarities between different skeleton representations and augmented views of the same sequence, the network is able to learn higher-level semantics of the skeleton data than when only using the augmented views. Our approach achieves state-of-the-art performance for self-supervised learning from skeleton data on the challenging PKU and NTU datasets, including multiple downstream tasks such as action recognition, action retrieval, and semi-supervised learning. Code can be found at https://github.com/fmthoker/skeleton-contrast.",1
"Substantial increase in the use of Electronic Health Records (EHRs) has opened new frontiers for predictive healthcare. However, while EHR systems are nearly ubiquitous, they lack a unified code system for representing medical concepts. Heterogeneous formats of EHR present a substantial barrier for the training and deployment of state-of-the-art deep learning models at scale. To overcome this problem, we introduce Description-based Embedding, DescEmb, a code-agnostic description-based representation learning framework for predictive modeling on EHR. DescEmb takes advantage of the flexibility of neural language understanding models while maintaining a neutral approach that can be combined with prior frameworks for task-specific representation learning or predictive modeling. We tested our model's capacity on various experiments including prediction tasks, transfer learning and pooled learning. DescEmb shows higher performance in overall experiments compared to code-based approach, opening the door to a text-based approach in predictive healthcare research that is not constrained by EHR structure nor special domain knowledge.",0
"The use of Electronic Health Records (EHRs) has increased significantly, allowing for new possibilities in predictive healthcare. However, due to the lack of a standardized code system for medical concepts, the various formats of EHRs pose a significant obstacle in training and deploying deep learning models on a large scale. To address this issue, we have introduced a code-agnostic representation learning framework called DescEmb, which is based on descriptions. DescEmb utilizes neural language understanding models' flexibility while remaining neutral, allowing it to be combined with other frameworks for task-specific representation learning or predictive modeling. We conducted various experiments, including prediction tasks, transfer learning, and pooled learning, to assess the model's capabilities. Overall, DescEmb demonstrated superior performance compared to code-based approaches, which opens up a text-based approach to predictive healthcare research that is not limited by EHR structure or specialized domain knowledge.",1
"Background: Accurate diagnosis of skull base tumors is essential for providing personalized surgical treatment strategies. Intraoperative diagnosis can be challenging due to tumor diversity and lack of intraoperative pathology resources.   Objective: To develop an independent and parallel intraoperative pathology workflow that can provide rapid and accurate skull base tumor diagnoses using label-free optical imaging and artificial intelligence (AI).   Method: We used a fiber laser-based, label-free, non-consumptive, high-resolution microscopy method ($<$ 60 sec per 1 $\times$ 1 mm$^\text{2}$), called stimulated Raman histology (SRH), to image a consecutive, multicenter cohort of skull base tumor patients. SRH images were then used to train a convolutional neural network (CNN) model using three representation learning strategies: cross-entropy, self-supervised contrastive learning, and supervised contrastive learning. Our trained CNN models were tested on a held-out, multicenter SRH dataset.   Results: SRH was able to image the diagnostic features of both benign and malignant skull base tumors. Of the three representation learning strategies, supervised contrastive learning most effectively learned the distinctive and diagnostic SRH image features for each of the skull base tumor types. In our multicenter testing set, cross-entropy achieved an overall diagnostic accuracy of 91.5%, self-supervised contrastive learning 83.9%, and supervised contrastive learning 96.6%. Our trained model was able to identify tumor-normal margins and detect regions of microscopic tumor infiltration in whole-slide SRH images.   Conclusion: SRH with AI models trained using contrastive representation learning can provide rapid and accurate intraoperative diagnosis of skull base tumors.",0
"The accurate diagnosis of skull base tumors is crucial for creating personalized surgical treatment strategies. However, intraoperative diagnosis can be difficult due to the variety of tumors and lack of pathology resources. This study aimed to develop an independent and parallel intraoperative pathology workflow that can quickly and accurately diagnose skull base tumors using label-free optical imaging and artificial intelligence (AI). The researchers used stimulated Raman histology (SRH), a high-resolution microscopy method, to image a group of skull base tumor patients. They then trained a convolutional neural network (CNN) model using three different learning strategies and tested it on a separate dataset. The results showed that SRH with AI models trained using contrastive representation learning can provide rapid and accurate intraoperative diagnosis of skull base tumors. The findings also showed that supervised contrastive learning was the most effective strategy for learning the distinctive features of each tumor type.",1
"GNNs have been proven to perform highly effective in various node-level, edge-level, and graph-level prediction tasks in several domains. Existing approaches mainly focus on static graphs. However, many graphs change over time with their edge may disappear, or node or edge attribute may alter from one time to the other. It is essential to consider such evolution in representation learning of nodes in time varying graphs. In this paper, we propose a Temporal Multilayered Position-aware Graph Neural Network (TMP-GNN), a node embedding approach for dynamic graph that incorporates the interdependence of temporal relations into embedding computation. We evaluate the performance of TMP-GNN on two different representations of temporal multilayered graphs. The performance is assessed against the most popular GNNs on node-level prediction tasks. Then, we incorporate TMP-GNN into a deep learning framework to estimate missing data and compare the performance with their corresponding competent GNNs from our former experiment, and a baseline method. Experimental results on four real-world datasets yield up to 58% of lower ROC AUC for pairwise node classification task, and 96% of lower MAE in missing feature estimation, particularly for graphs with a relatively high number of nodes and lower mean degree of connectivity.",0
"GNNs have been proven to be highly effective in predicting nodes, edges, and graphs in various domains. However, most current approaches only consider static graphs, despite the fact that many graphs are dynamic and may change over time. To address this issue, we propose a novel approach called Temporal Multilayered Position-aware Graph Neural Network (TMP-GNN) that incorporates temporal relations into node embedding computation. We evaluate TMP-GNN on two types of temporal multilayered graphs and compare its performance to other popular GNNs for node-level prediction tasks. We also integrate TMP-GNN into a deep learning framework to estimate missing data and compare the results to competent GNNs from our previous experiment and a baseline method. Our experiments on four real-world datasets show that TMP-GNN performs significantly better than other models, particularly for graphs with a high number of nodes and low mean degree of connectivity.",1
"Animals exhibit an innate ability to learn regularities of the world through interaction. By performing experiments in their environment, they are able to discern the causal factors of variation and infer how they affect the world's dynamics. Inspired by this, we attempt to equip reinforcement learning agents with the ability to perform experiments that facilitate a categorization of the rolled-out trajectories, and to subsequently infer the causal factors of the environment in a hierarchical manner. We introduce {\em causal curiosity}, a novel intrinsic reward, and show that it allows our agents to learn optimal sequences of actions and discover causal factors in the dynamics of the environment. The learned behavior allows the agents to infer a binary quantized representation for the ground-truth causal factors in every environment. Additionally, we find that these experimental behaviors are semantically meaningful (e.g., our agents learn to lift blocks to categorize them by weight), and are learnt in a self-supervised manner with approximately 2.5 times less data than conventional supervised planners. We show that these behaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or other downstream tasks). Finally, we show that the knowledge of causal factor representations aids zero-shot learning for more complex tasks. Visit https://sites.google.com/usc.edu/causal-curiosity/home for website.",0
"Animals possess a natural ability to acquire knowledge about the world around them through interaction and experimentation. They can identify the causes of variations and understand their impact on the dynamics of the environment. In light of this, we aim to imbue reinforcement learning agents with the capacity to perform experiments that enable them to categorize trajectories and identify the causal factors of the environment in a hierarchical manner. Our method, called ""causal curiosity,"" introduces a novel intrinsic reward that facilitates optimal action sequences and the discovery of causal factors. The agents learn to represent ground-truth causal factors in a binary quantized manner and exhibit semantically meaningful experimental behaviors, such as lifting blocks to categorize them by weight. This self-supervised learning technique requires approximately 2.5 times less data than conventional supervised planners. Furthermore, the learned behaviors can be fine-tuned for other downstream tasks, and the knowledge of causal factor representations enhances zero-shot learning for complex tasks. For more information, visit https://sites.google.com/usc.edu/causal-curiosity/home.",1
"Contrastive Learning (CL) is a recent representation learning approach, which encourages inter-class separability and intra-class compactness in learned image representations. Since medical images often contain multiple semantic classes in an image, using CL to learn representations of local features (as opposed to global) is important. In this work, we present a novel semi-supervised 2D medical segmentation solution that applies CL on image patches, instead of full images. These patches are meaningfully constructed using the semantic information of different classes obtained via pseudo labeling. We also propose a novel consistency regularization (CR) scheme, which works in synergy with CL. It addresses the problem of confirmation bias, and encourages better clustering in the feature space. We evaluate our method on four public medical segmentation datasets and a novel histopathology dataset that we introduce. Our method obtains consistent improvements over state-of-the-art semi-supervised segmentation approaches for all datasets.",0
"Contrastive Learning (CL) is a modern method of representation learning that emphasizes the importance of creating distinguishable inter-class differences and compact intra-class similarities in the learned image representations. Given that medical images often contain multiple semantic classes, it is critical to use CL to learn local feature representations instead of global ones. In this study, we propose a new semi-supervised 2D medical segmentation solution that employs CL on image patches rather than full images. These patches are intelligently constructed using the semantic information of different classes obtained through pseudo labeling. Additionally, we introduce a novel consistency regularization (CR) technique that works in conjunction with CL to address the issue of confirmation bias and promote better feature space clustering. We assess our method on four public medical segmentation datasets and a new histopathology dataset. Our approach outperforms all existing semi-supervised segmentation methods for all datasets, demonstrating consistent improvements.",1
"In cases of serious crime, including sexual abuse, often the only available information with demonstrated potential for identification is images of the hands. Since this evidence is captured in uncontrolled situations, it is difficult to analyse. As global approaches to feature comparison are limited in this case, it is important to extend to consider local information. In this work, we propose hand-based person identification by learning both global and local deep feature representation. Our proposed method, Global and Part-Aware Network (GPA-Net), creates global and local branches on the conv-layer for learning robust discriminative global and part-level features. For learning the local (part-level) features, we perform uniform partitioning on the conv-layer in both horizontal and vertical directions. We retrieve the parts by conducting a soft partition without explicitly partitioning the images or requiring external cues such as pose estimation. We make extensive evaluations on two large multi-ethnic and publicly available hand datasets, demonstrating that our proposed method significantly outperforms competing approaches.",0
"When dealing with serious crimes like sexual abuse, identifying the perpetrator can often be difficult. Sometimes the only available evidence is images of the hands, which are captured in uncontrolled situations and can be hard to analyse. As a result, it's important to consider both global and local information when trying to identify a suspect. Our proposed method, known as the Global and Part-Aware Network (GPA-Net), uses a conv-layer with global and local branches to learn robust discriminative features. To learn the local features, we perform uniform partitioning on the conv-layer in both horizontal and vertical directions. We then retrieve the parts using a soft partition without explicitly partitioning the images or requiring external cues such as pose estimation. We evaluated our method on two large multi-ethnic hand datasets and found that it significantly outperforms other approaches.",1
"The goal of representation learning is different from the ultimate objective of machine learning such as decision making, it is therefore very difficult to establish clear and direct objectives for training representation learning models. It has been argued that a good representation should disentangle the underlying variation factors, yet how to translate this into training objectives remains unknown. This paper presents an attempt to establish direct training criterions and design principles for developing good representation learning models. We propose that a good representation learning model should be maximally expressive, i.e., capable of distinguishing the maximum number of input configurations. We formally define expressiveness and introduce the maximum expressiveness (MEXS) theorem of a general learning model. We propose to train a model by maximizing its expressiveness while at the same time incorporating general priors such as model smoothness. We present a conscience competitive learning algorithm which encourages the model to reach its MEXS whilst at the same time adheres to model smoothness prior. We also introduce a label consistent training (LCT) technique to boost model smoothness by encouraging it to assign consistent labels to similar samples. We present extensive experimental results to show that our method can indeed design representation learning models capable of developing representations that are as good as or better than state of the art. We also show that our technique is computationally efficient, robust against different parameter settings and can work effectively on a variety of datasets. Code available at https://github.com/qlilx/odgrlm.git",0
"Establishing clear and direct objectives for training representation learning models is challenging since the goal of representation learning differs from that of machine learning, which focuses on decision making. Despite the argument that a good representation should disentangle the underlying variation factors, the translation of this into training objectives remains unknown. This paper proposes direct training criterions and design principles for developing good representation learning models. The authors suggest that a good representation learning model should be maximally expressive, meaning it can distinguish the maximum number of input configurations. They formally define expressiveness and introduce the maximum expressiveness (MEXS) theorem of a general learning model. The authors propose training a model by maximizing its expressiveness while incorporating general priors such as model smoothness. They present a conscience competitive learning algorithm that encourages the model to reach its MEXS while adhering to model smoothness prior. Additionally, they introduce a label consistent training (LCT) technique to boost model smoothness by encouraging it to assign consistent labels to similar samples. The paper presents extensive experimental results demonstrating that their method can design representation learning models that are as good as or better than state of the art, and the technique is computationally efficient, robust against different parameter settings, and effective on a variety of datasets. The code is available at https://github.com/qlilx/odgrlm.git.",1
"Contrastive learning has revolutionized self-supervised image representation learning field, and recently been adapted to video domain. One of the greatest advantages of contrastive learning is that it allows us to flexibly define powerful loss objectives as long as we can find a reasonable way to formulate positive and negative samples to contrast. However, existing approaches rely heavily on the short-range spatiotemporal salience to form clip-level contrastive signals, thus limit themselves from using global context. In this paper, we propose a new video-level contrastive learning method based on segments to formulate positive pairs. Our formulation is able to capture global context in a video, thus robust to temporal content change. We also incorporate a temporal order regularization term to enforce the inherent sequential structure of videos. Extensive experiments show that our video-level contrastive learning framework (VCLR) is able to outperform previous state-of-the-arts on five video datasets for downstream action classification, action localization and video retrieval. Code is available at https://github.com/amazon-research/video-contrastive-learning.",0
"The field of self-supervised image representation learning has been transformed by contrastive learning, which has also been applied to the video domain. The major benefit of contrastive learning is that it enables the creation of powerful loss objectives, provided that reasonable positive and negative samples are available for comparison. However, current approaches rely heavily on short-range spatiotemporal salience to form contrastive signals at the clip level, which limits the use of global context. In this study, we introduce a new method for video-level contrastive learning based on segments, which allows for the definition of positive pairs and the capture of global context in a video. This approach is robust to changes in temporal content and includes a temporal order regularization term to maintain the sequential structure of videos. Extensive experiments demonstrate that our video-level contrastive learning framework (VCLR) outperforms previous state-of-the-art methods on five video datasets, including action classification, action localization, and video retrieval. Our code is available at https://github.com/amazon-research/video-contrastive-learning.",1
"With various face presentation attacks arising under unseen scenarios, face anti-spoofing (FAS) based on domain generalization (DG) has drawn growing attention due to its robustness. Most existing methods utilize DG frameworks to align the features to seek a compact and generalized feature space. However, little attention has been paid to the feature extraction process for the FAS task, especially the influence of normalization, which also has a great impact on the generalization of the learned representation. To address this issue, we propose a novel perspective of face anti-spoofing that focuses on the normalization selection in the feature extraction process. Concretely, an Adaptive Normalized Representation Learning (ANRL) framework is devised, which adaptively selects feature normalization methods according to the inputs, aiming to learn domain-agnostic and discriminative representation. Moreover, to facilitate the representation learning, Dual Calibration Constraints are designed, including Inter-Domain Compatible loss and Inter-Class Separable loss, which provide a better optimization direction for generalizable representation. Extensive experiments and visualizations are presented to demonstrate the effectiveness of our method against the SOTA competitors.",0
"The rise of various face presentation attacks in unseen scenarios has led to an increasing interest in face anti-spoofing (FAS) based on domain generalization (DG) due to its robustness. While existing methods use DG frameworks to align features and create a generalized feature space, little attention has been given to the feature extraction process for the FAS task, particularly the impact of normalization on the learned representation's generalization. To address this, we propose a new approach to FAS that focuses on normalization selection in the feature extraction process. Our Adaptive Normalized Representation Learning (ANRL) framework selects feature normalization methods based on the inputs to learn a domain-agnostic and discriminative representation. Additionally, we design Dual Calibration Constraints, including Inter-Domain Compatible loss and Inter-Class Separable loss, to facilitate representation learning and provide a better optimization direction for generalizable representation. We present extensive experiments and visualizations that demonstrate the effectiveness of our method compared to competitors.",1
"Hybrid FSO/RF system requires an efficient FSO and RF link switching mechanism to improve the system capacity by realizing the complementary benefits of both the links. The dynamics of network conditions, such as fog, dust, and sand storms compound the link switching problem and control complexity. To address this problem, we initiate the study of deep reinforcement learning (DRL) for link switching of hybrid FSO/RF systems. Specifically, in this work, we focus on actor-critic called Actor/Critic-FSO/RF and Deep-Q network (DQN) called DQN-FSO/RF for FSO/RF link switching under atmospheric turbulences. To formulate the problem, we define the state, action, and reward function of a hybrid FSO/RF system. DQN-FSO/RF frequently updates the deployed policy that interacts with the environment in a hybrid FSO/RF system, resulting in high switching costs. To overcome this, we lift this problem to ensemble consensus-based representation learning for deep reinforcement called DQNEnsemble-FSO/RF. The proposed novel DQNEnsemble-FSO/RF DRL approach uses consensus learned features representations based on an ensemble of asynchronous threads to update the deployed policy. Experimental results corroborate that the proposed DQNEnsemble-FSO/RF's consensus-learned features switching achieves better performance than Actor/Critic-FSO/RF, DQN-FSO/RF, and MyOpic for FSO/RF link switching while keeping the switching cost significantly low.",0
"An efficient switching mechanism is crucial for hybrid FSO/RF systems to optimize system capacity by leveraging the benefits of both links. However, network conditions such as fog, dust, and sand storms exacerbate the switching problem and increase control complexity. To tackle this issue, we explore the application of deep reinforcement learning (DRL) for link switching in hybrid FSO/RF systems. Specifically, our study focuses on two DRL methods: Actor/Critic-FSO/RF and DQN-FSO/RF, which address FSO/RF link switching under atmospheric turbulences. We define the state, action, and reward function of a hybrid FSO/RF system to formulate the problem. However, DQN-FSO/RF's frequent policy updates increase switching costs, which we mitigate using ensemble consensus-based representation learning for deep reinforcement (DQNEnsemble-FSO/RF). This novel approach employs consensus-learned feature representations based on an ensemble of asynchronous threads to update the policy. Our experimental results show that DQNEnsemble-FSO/RF's consensus-learned features switching outperforms Actor/Critic-FSO/RF, DQN-FSO/RF, and MyOpic for FSO/RF link switching while significantly reducing switching costs.",1
"Gait data captured by inertial sensors have demonstrated promising results on user authentication. However, most existing approaches stored the enrolled gait pattern insecurely for matching with the validating pattern, thus, posed critical security and privacy issues. In this study, we present a gait cryptosystem that generates from gait data the random key for user authentication, meanwhile, secures the gait pattern. First, we propose a revocable and random binary string extraction method using a deep neural network followed by feature-wise binarization. A novel loss function for network optimization is also designed, to tackle not only the intrauser stability but also the inter-user randomness. Second, we propose a new biometric key generation scheme, namely Irreversible Error Correct and Obfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme, to securely generate from the binary string the random and irreversible key. The model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. We showed that our model could generate the key of 139 bits from 5-second data sequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR) smaller than 5.441%. In addition, the security and user privacy analyses showed that our model was secure against existing attacks on biometric template protection, and fulfilled irreversibility and unlinkability.",0
"The use of inertial sensors to capture gait data has shown promise for user authentication, but current methods for storing enrolled gait patterns for matching with validating patterns are insecure and pose privacy concerns. This study presents a gait cryptosystem that generates a random key for authentication while also securing the gait pattern. A new method for extracting binary strings using a deep neural network and feature-wise binarization is proposed, along with a novel loss function for intrauser stability and inter-user randomness. The Irreversible Error Correct and Obfuscate (IECO) scheme is introduced for secure key generation from the binary string. The model is evaluated with two benchmark datasets and achieves a key length of 139 bits with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR) smaller than 5.441%. The model is also found to be secure against existing attacks on biometric template protection and meets irreversibility and unlinkability requirements.",1
"Over the last few decades, artificial intelligence research has made tremendous strides, but it still heavily relies on fixed datasets in stationary environments. Continual learning is a growing field of research that examines how AI systems can learn sequentially from a continuous stream of linked data in the same way that biological systems do. Simultaneously, fake media such as deepfakes and synthetic face images have emerged as significant to current multimedia technologies. Recently, numerous method has been proposed which can detect deepfakes with high accuracy. However, they suffer significantly due to their reliance on fixed datasets in limited evaluation settings. Therefore, in this work, we apply continuous learning to neural networks' learning dynamics, emphasizing its potential to increase data efficiency significantly. We propose Continual Representation using Distillation (CoReD) method that employs the concept of Continual Learning (CL), Representation Learning (RL), and Knowledge Distillation (KD). We design CoReD to perform sequential domain adaptation tasks on new deepfake and GAN-generated synthetic face datasets, while effectively minimizing the catastrophic forgetting in a teacher-student model setting. Our extensive experimental results demonstrate that our method is efficient at domain adaptation to detect low-quality deepfakes videos and GAN-generated images from several datasets, outperforming the-state-of-art baseline methods.",0
"Although artificial intelligence research has made significant strides, it still relies heavily on fixed datasets in stationary environments. To address this, continual learning has emerged as a field of research that investigates how AI systems can learn sequentially from a continuous stream of interconnected data, similar to biological systems. Meanwhile, fake media like deepfakes and synthetic face images have become increasingly important in multimedia technologies. While several methods have been proposed to detect deepfakes with high accuracy, they are limited by their reliance on fixed datasets in restricted evaluation settings. This study employs continual learning to enhance neural networks' learning dynamics, emphasizing its capacity to improve data efficiency. The authors introduce the Continual Representation using Distillation (CoReD) method, which leverages Continual Learning (CL), Representation Learning (RL), and Knowledge Distillation (KD). CoReD performs sequential domain adaptation tasks on new deepfake and GAN-generated synthetic face datasets while minimizing catastrophic forgetting in a teacher-student model setting. The experimental results demonstrate that CoReD is a highly efficient domain adaptation technique for detecting low-quality deepfake videos and GAN-generated images from various datasets, surpassing the state-of-the-art baseline methods.",1
"We explore the value of weak labels in learning transferable representations for medical images. Compared to hand-labeled datasets, weak or inexact labels can be acquired in large quantities at significantly lower cost and can provide useful training signals for data-hungry models such as deep neural networks. We consider weak labels in the form of pseudo-labels and propose a semi-weakly supervised contrastive learning (SWCL) framework for representation learning using semi-weakly annotated images. Specifically, we train a semi-supervised model to propagate labels from a small dataset consisting of diverse image-level annotations to a large unlabeled dataset. Using the propagated labels, we generate a patch-level dataset for pretraining and formulate a multi-label contrastive learning objective to capture position-specific features encoded in each patch. We empirically validate the transfer learning performance of SWCL on seven public retinal fundus datasets, covering three disease classification tasks and two anatomical structure segmentation tasks. Our experiment results suggest that, under very low data regime, large-scale ImageNet pretraining on improved architecture remains a very strong baseline, and recently proposed self-supervised methods falter in segmentation tasks, possibly due to the strong invariant constraint imposed. Our method surpasses all prior self-supervised methods and standard cross-entropy training, while closing the gaps with ImageNet pretraining.",0
"This article delves into the potential of weak labels in developing transferable representations for medical images. Obtaining inexact or weak labels is more cost-effective and can be done in much larger quantities than hand-labeled datasets. These labels provide useful training signals for data-hungry models, such as deep neural networks. The study introduces the concept of pseudo-labels as a form of weak labels and proposes a semi-weakly supervised contrastive learning framework (SWCL) for representation learning with semi-weakly annotated images. The SWCL trains a semi-supervised model to propagate labels from a small dataset to a larger unlabeled dataset. The propagated labels are then used to generate a patch-level dataset for pretraining, with a multi-label contrastive learning objective to capture position-specific features encoded in each patch. The study validates the transfer learning performance of SWCL on seven public retinal fundus datasets, covering three disease classification tasks and two anatomical structure segmentation tasks. The results indicate that SWCL surpasses all prior self-supervised methods and standard cross-entropy training, while closing the gaps with ImageNet pretraining, even under low data regime.",1
"Recently deep learning has achieved significant progress on point cloud analysis tasks. Learning good representations is of vital importance to these tasks. Most current methods rely on massive labelled data for training. We here propose a point discriminative learning method for unsupervised representation learning on 3D point clouds, which can learn local and global geometry features. We achieve this by imposing a novel point discrimination loss on the middle level and global level point features produced in the backbone network. This point discrimination loss enforces the features to be consistent with points belonging to the shape surface and inconsistent with randomly sampled noisy points. Our method is simple in design, which works by adding an extra adaptation module and a point consistency module for unsupervised training of the encoder in the backbone network. Once trained, these two modules can be discarded during supervised training of the classifier or decoder for down-stream tasks. We conduct extensive experiments on 3D object classification, 3D part segmentation and shape reconstruction in various unsupervised and transfer settings. Both quantitative and qualitative results show that our method learns powerful representations and achieves new state-of-the-art performance.",0
"Significant progress has been achieved in point cloud analysis tasks through recent advancements in deep learning. Learning representations of high quality is crucial for the success of these tasks. Majority of the existing methods rely on labeled data sets for training. Our proposed method involves unsupervised representation learning on 3D point clouds using a point discriminative learning approach. This approach enables the learning of global and local geometry features by utilizing a novel point discrimination loss on the middle and global level point features produced in the backbone network. This loss ensures that the features are consistent with shape surface points and not randomly selected noisy points. Our method is simple in design and involves the addition of an extra adaptation module and point consistency module to the encoder in the backbone network for unsupervised training. Once trained, these two modules can be discarded during supervised training of the classifier or decoder for downstream tasks. We conducted extensive experiments on 3D object classification, 3D part segmentation, and shape reconstruction using various unsupervised and transfer settings. Our method achieved new state-of-the-art performance, as shown by both quantitative and qualitative results, indicating the learning of powerful representations.",1
"Spectral graph convolutional networks (SGCNs) have been attracting increasing attention in graph representation learning partly due to their interpretability through the prism of the established graph signal processing framework. However, existing SGCNs are limited in implementing graph convolutions with rigid transforms that could not adapt to signals residing on graphs and tasks at hand. In this paper, we propose a novel class of spectral graph convolutional networks that implement graph convolutions with adaptive graph wavelets. Specifically, the adaptive graph wavelets are learned with neural network-parameterized lifting structures, where structure-aware attention-based lifting operations are developed to jointly consider graph structures and node features. We propose to lift based on diffusion wavelets to alleviate the structural information loss induced by partitioning non-bipartite graphs. By design, the locality and sparsity of the resulting wavelet transform as well as the scalability of the lifting structure for large and varying-size graphs are guaranteed. We further derive a soft-thresholding filtering operation by learning sparse graph representations in terms of the learned wavelets, which improves the scalability and interpretablity, and yield a localized, efficient and scalable spectral graph convolution. To ensure that the learned graph representations are invariant to node permutations, a layer is employed at the input of the networks to reorder the nodes according to their local topology information. We evaluate the proposed networks in both node-level and graph-level representation learning tasks on benchmark citation and bioinformatics graph datasets. Extensive experiments demonstrate the superiority of the proposed networks over existing SGCNs in terms of accuracy, efficiency and scalability.",0
"Spectral graph convolutional networks (SGCNs) are becoming increasingly popular in graph representation learning due to their interpretability within the graph signal processing framework. However, current SGCNs are limited in their ability to implement graph convolutions with fixed transforms that cannot adapt to signals on graphs and specific tasks. This paper introduces a new category of SGCNs that implement graph convolutions with adaptive graph wavelets. The wavelets are learned using neural network-parameterized lifting structures that include structure-aware attention-based lifting operations, which consider both graph structures and node features. The proposed method lifts based on diffusion wavelets to address the structural information loss caused by partitioning non-bipartite graphs. The resulting wavelet transform is both local and sparse, and the lifting structure is scalable for large and varying-size graphs. The paper also introduces a soft-thresholding filtering operation for learning sparse graph representations, which improves the scalability and interpretability of the spectral graph convolution. To ensure that the learned graph representations are invariant to node permutations, a layer is used to reorder the nodes based on their local topology information. The proposed networks are evaluated on benchmark citation and bioinformatics graph datasets for both node-level and graph-level representation learning tasks. The experiments demonstrate that the proposed networks outperform existing SGCNs in terms of accuracy, efficiency, and scalability.",1
Incentive salience attribution can be understood as a psychobiological process ascribing relevance to potentially rewarding objects and actions. Despite being an important component of the motivational process guiding our everyday behaviour its study in naturalistic contexts is not straightforward. Here we propose a methodology based on artificial neural networks (ANNs) for approximating latent states produced by this process in situations where large volumes of behavioural data are available but no strict experimental control is possible. Leveraging knowledge derived from theoretical and computational accounts of incentive salience attribution we designed an ANN for estimating duration and intensity of future interactions between individuals and a series of video games in a large-scale ($N> 3 \times 10^6$) longitudinal dataset. Through model comparison and inspection we show that our approach outperforms competing ones while also generating a representation that well approximate some of the functions of attributed incentive salience. We discuss our findings with reference to the adopted theoretical and computational frameworks and suggest how our methodology could be an initial step for estimating attributed incentive salience in large scale behavioural studies.,0
"The process of incentive salience attribution involves giving importance to objects and actions that may lead to rewards. Although it plays a crucial role in motivating our everyday behavior, studying it in naturalistic settings can be challenging. To address this issue, we propose a new approach based on artificial neural networks (ANNs) that can estimate the latent states produced by this process using large volumes of behavioral data. Our ANN was designed to predict the duration and intensity of future interactions between individuals and a series of video games, using a longitudinal dataset of over 3 million observations. We compared our model to others and found that it outperformed them while also providing a representation that approximates some of the functions of incentive salience attribution. Our findings are discussed in the context of existing theoretical and computational frameworks, and we suggest that our methodology could be a starting point for estimating incentive salience attribution in large-scale behavioral studies.",1
"Real estate appraisal refers to the process of developing an unbiased opinion for real property's market value, which plays a vital role in decision-making for various players in the marketplace (e.g., real estate agents, appraisers, lenders, and buyers). However, it is a nontrivial task for accurate real estate appraisal because of three major challenges: (1) The complicated influencing factors for property value; (2) The asynchronously spatiotemporal dependencies among real estate transactions; (3) The diversified correlations between residential communities. To this end, we propose a Multi-Task Hierarchical Graph Representation Learning (MugRep) framework for accurate real estate appraisal. Specifically, by acquiring and integrating multi-source urban data, we first construct a rich feature set to comprehensively profile the real estate from multiple perspectives (e.g., geographical distribution, human mobility distribution, and resident demographics distribution). Then, an evolving real estate transaction graph and a corresponding event graph convolution module are proposed to incorporate asynchronously spatiotemporal dependencies among real estate transactions. Moreover, to further incorporate valuable knowledge from the view of residential communities, we devise a hierarchical heterogeneous community graph convolution module to capture diversified correlations between residential communities. Finally, an urban district partitioned multi-task learning module is introduced to generate differently distributed value opinions for real estate. Extensive experiments on two real-world datasets demonstrate the effectiveness of MugRep and its components and features.",0
"Real estate appraisal involves developing an impartial estimation of the market value of real property, which is crucial for decision-making by various stakeholders such as real estate agents, appraisers, lenders, and buyers. However, accurate real estate appraisal is challenging due to three major factors: complex influencing factors for property value, asynchronous spatiotemporal dependencies among real estate transactions, and diverse correlations between residential communities. To address these challenges, we propose the MugRep framework, which employs a Multi-Task Hierarchical Graph Representation Learning approach. This involves acquiring and integrating multi-source urban data to create a feature-rich profile of real estate from various perspectives. The framework utilizes an evolving real estate transaction graph and an event graph convolution module to incorporate spatiotemporal dependencies among real estate transactions. Furthermore, a hierarchical heterogeneous community graph convolution module is employed to capture diverse correlations between residential communities. Finally, an urban district partitioned multi-task learning module generates different value opinions for real estate. Our experiments on real-world datasets demonstrate the effectiveness of MugRep and its components and features.",1
"Many current deep learning approaches make extensive use of backbone networks pre-trained on large datasets like ImageNet, which are then fine-tuned to perform a certain task. In remote sensing, the lack of comparable large annotated datasets and the wide diversity of sensing platforms impedes similar developments. In order to contribute towards the availability of pre-trained backbone networks in remote sensing, we devise a self-supervised approach for pre-training deep neural networks. By exploiting the correspondence between geo-tagged audio recordings and remote sensing imagery, this is done in a completely label-free manner, eliminating the need for laborious manual annotation. For this purpose, we introduce the SoundingEarth dataset, which consists of co-located aerial imagery and audio samples all around the world. Using this dataset, we then pre-train ResNet models to map samples from both modalities into a common embedding space, which encourages the models to understand key properties of a scene that influence both visual and auditory appearance. To validate the usefulness of the proposed approach, we evaluate the transfer learning performance of pre-trained weights obtained against weights obtained through other means. By fine-tuning the models on a number of commonly used remote sensing datasets, we show that our approach outperforms existing pre-training strategies for remote sensing imagery. The dataset, code and pre-trained model weights will be available at https://github.com/khdlr/SoundingEarth.",0
"Numerous deep learning approaches rely heavily on backbone networks that have been pre-trained on vast datasets such as ImageNet, and then fine-tuned to perform specific tasks. However, in remote sensing, the absence of large annotated datasets and the diverse range of sensing platforms pose challenges for similar advancements. To contribute to the availability of pre-trained backbone networks in remote sensing, we have developed a self-supervised approach for pre-training deep neural networks. This is achieved by utilizing the correlation between geo-tagged audio recordings and remote sensing imagery, resulting in a label-free approach that eliminates the need for manual annotation. To facilitate this process, we have introduced the SoundingEarth dataset, which encompasses aerial imagery and audio samples from around the world. We use this dataset to pre-train ResNet models, which map samples from both modalities into a shared embedding space, promoting the models to comprehend critical scene characteristics that affect both visual and auditory appearances. To validate the effectiveness of our approach, we assess the transfer learning performance of pre-trained weights obtained from our method versus other methods. By fine-tuning the models on various commonly used remote sensing datasets, we demonstrate that our approach outperforms existing pre-training strategies for remote sensing imagery. The dataset, code, and pre-trained model weights will be available at https://github.com/khdlr/SoundingEarth.",1
"Crowd localization is a new computer vision task, evolved from crowd counting. Different from the latter, it provides more precise location information for each instance, not just counting numbers for the whole crowd scene, which brings greater challenges, especially in extremely congested crowd scenes. In this paper, we focus on how to achieve precise instance localization in high-density crowd scenes, and to alleviate the problem that the feature extraction ability of the traditional model is reduced due to the target occlusion, the image blur, etc. To this end, we propose a Dilated Convolutional Swin Transformer (DCST) for congested crowd scenes. Specifically, a window-based vision transformer is introduced into the crowd localization task, which effectively improves the capacity of representation learning. Then, the well-designed dilated convolutional module is inserted into some different stages of the transformer to enhance the large-range contextual information. Extensive experiments evidence the effectiveness of the proposed methods and achieve state-of-the-art performance on five popular datasets. Especially, the proposed model achieves F1-measure of 77.5\% and MAE of 84.2 in terms of localization and counting performance, respectively.",0
"Crowd localization is a novel computer vision objective that has emerged from crowd counting. Unlike the latter, it provides more precise location details for each instance instead of just a count of the whole crowd scene. This poses significant challenges, particularly in extremely congested crowd scenes. In this study, our focus is on achieving accurate instance localization in high-density crowd scenes. We aim to address the reduction of feature extraction ability in traditional models due to target occlusion and image blur. To this end, we propose the Dilated Convolutional Swin Transformer (DCST) for congested crowd scenes. Our approach involves introducing a window-based vision transformer into the crowd localization task to improve representation learning capacity. We also insert a well-designed dilated convolutional module into different stages of the transformer to enhance large-range contextual information. Our experimental results demonstrate the effectiveness of our proposed methods, achieving state-of-the-art performance on five popular datasets. In particular, our proposed model achieves an F1-measure of 77.5% and MAE of 84.2 in terms of localization and counting performance, respectively.",1
"With the growing interest in the machine learning community to solve real-world problems, it has become crucial to uncover the hidden reasoning behind their decisions by focusing on the fairness and auditing the predictions made by these black-box models. In this paper, we propose a novel method to address two key issues: (a) Can we simultaneously learn fair disentangled representations while ensuring the utility of the learned representation for downstream tasks, and (b)Can we provide theoretical insights into when the proposed approach will be both fair and accurate. To address the former, we propose the method FRIED, Fair Representation learning using Interpolation Enabled Disentanglement. In our architecture, by imposing a critic-based adversarial framework, we enforce the interpolated points in the latent space to be more realistic. This helps in capturing the data manifold effectively and enhances the utility of the learned representation for downstream prediction tasks. We address the latter question by developing a theory on fairness-accuracy trade-offs using classifier-based conditional mutual information estimation. We demonstrate the effectiveness of FRIED on datasets of different modalities - tabular, text, and image datasets. We observe that the representations learned by FRIED are overall fairer in comparison to existing baselines and also accurate for downstream prediction tasks. Additionally, we evaluate FRIED on a real-world healthcare claims dataset where we conduct an expert aided model auditing study providing useful insights into opioid ad-diction patterns.",0
"Due to the increasing interest in using machine learning to tackle real-world problems, it is now imperative to examine the hidden reasoning behind the decisions made by black-box models, with a focus on fairness and auditing their predictions. This paper presents a new approach to address two critical issues: (a) Whether it is possible to learn fair disentangled representations while preserving the usefulness of the learned representation for downstream tasks, and (b) Whether it is feasible to provide theoretical insights into when the proposed approach will be both fair and accurate. To tackle the former, we introduce FRIED, which stands for Fair Representation learning using Interpolation Enabled Disentanglement. Our architecture enforces the interpolated points in the latent space to be more realistic by imposing a critic-based adversarial framework. This approach helps to effectively capture the data manifold and enhances the usefulness of the learned representation for downstream prediction tasks. To address the latter question, we develop a theory on fairness-accuracy trade-offs using classifier-based conditional mutual information estimation. We evaluate FRIED on datasets of various modalities, including tabular, text, and image datasets, and find that the representations learned by FRIED are fairer overall than existing baselines and are also accurate for downstream prediction tasks. Furthermore, we conduct an expert-aided model auditing study on a real-world healthcare claims dataset, providing valuable insights into opioid addiction patterns.",1
"The target representation learned by convolutional neural networks plays an important role in Thermal Infrared (TIR) tracking. Currently, most of the top-performing TIR trackers are still employing representations learned by the model trained on the RGB data. However, this representation does not take into account the information in the TIR modality itself, limiting the performance of TIR tracking. To solve this problem, we propose to distill representations of the TIR modality from the RGB modality with Cross-Modal Distillation (CMD) on a large amount of unlabeled paired RGB-TIR data. We take advantage of the two-branch architecture of the baseline tracker, i.e. DiMP, for cross-modal distillation working on two components of the tracker. Specifically, we use one branch as a teacher module to distill the representation learned by the model into the other branch. Benefiting from the powerful model in the RGB modality, the cross-modal distillation can learn the TIR-specific representation for promoting TIR tracking. The proposed approach can be incorporated into different baseline trackers conveniently as a generic and independent component. Furthermore, the semantic coherence of paired RGB and TIR images is utilized as a supervised signal in the distillation loss for cross-modal knowledge transfer. In practice, three different approaches are explored to generate paired RGB-TIR patches with the same semantics for training in an unsupervised way. It is easy to extend to an even larger scale of unlabeled training data. Extensive experiments on the LSOTB-TIR dataset and PTB-TIR dataset demonstrate that our proposed cross-modal distillation method effectively learns TIR-specific target representations transferred from the RGB modality. Our tracker outperforms the baseline tracker by achieving absolute gains of 2.3% Success, 2.7% Precision, and 2.5% Normalized Precision respectively.",0
"The significance of the target representation acquired by convolutional neural networks cannot be overstated in Thermal Infrared (TIR) tracking. Currently, most high-performing TIR trackers continue to use representations learned from RGB data models. However, this representation ignores the specifics of TIR modality, which limits the performance of TIR tracking. To remedy this, we suggest using Cross-Modal Distillation (CMD) to distill TIR modality representations from RGB modality representations using a large amount of unlabeled paired RGB-TIR data. We leverage the two-branch architecture of the DiMP tracker for cross-modal distillation on two tracker components. One branch serves as a teacher module to distill learned representations into the other branch. Using the powerful RGB modality model, cross-modal distillation can learn TIR-specific representations that enhance TIR tracking. The proposed approach can be easily incorporated into various baseline trackers as an independent and generic component. Additionally, the distillation loss uses the semantic coherence of paired RGB and TIR images as a supervised signal for cross-modal knowledge transfer. In practice, three approaches are used to unsupervisedly generate paired RGB-TIR patches with the same semantics for training. The method can be scaled up to an even larger scale of unlabeled training data. Extensive experiments on the LSOTB-TIR and PTB-TIR datasets demonstrate that our proposed cross-modal distillation approach effectively learns TIR-specific target representations transferred from the RGB modality. Our tracker outperforms the baseline tracker by achieving absolute gains of 2.3% Success, 2.7% Precision, and 2.5% Normalized Precision, respectively.",1
"Person re-identification (Re-ID) aims to match pedestrians under dis-joint cameras. Most Re-ID methods formulate it as visual representation learning and image search, and its accuracy is consequently affected greatly by the search space. Spatial-temporal information has been proven to be efficient to filter irrelevant negative samples and significantly improve Re-ID accuracy. However, existing spatial-temporal person Re-ID methods are still rough and do not exploit spatial-temporal information sufficiently. In this paper, we propose a novel Instance-level and Spatial-Temporal Disentangled Re-ID method (InSTD), to improve Re-ID accuracy. In our proposed framework, personalized information such as moving direction is explicitly considered to further narrow down the search space. Besides, the spatial-temporal transferring probability is disentangled from joint distribution to marginal distribution, so that outliers can also be well modeled. Abundant experimental analyses are presented, which demonstrates the superiority and provides more insights into our method. The proposed method achieves mAP of 90.8% on Market-1501 and 89.1% on DukeMTMC-reID, improving from the baseline 82.2% and 72.7%, respectively. Besides, in order to provide a better benchmark for person re-identification, we release a cleaned data list of DukeMTMC-reID with this paper: https://github.com/RenMin1991/cleaned-DukeMTMC-reID/",0
"The goal of Person re-identification (Re-ID) is to match pedestrians across different cameras. Currently, most Re-ID methods rely on visual representation learning and image search, which can be greatly impacted by the search space. To address this issue, spatial-temporal information has been found to be effective in filtering out irrelevant negative samples and improving accuracy. However, current spatial-temporal person Re-ID methods are not comprehensive enough. To improve accuracy, we propose a novel Instance-level and Spatial-Temporal Disentangled Re-ID method (InSTD). Our method considers personalized information such as moving direction to further narrow down the search space. Additionally, we disentangle the spatial-temporal transferring probability from joint distribution to marginal distribution, which allows for modeling of outliers. Our experimental analyses demonstrate the superiority of our method. We achieved mAP of 90.8% on Market-1501 and 89.1% on DukeMTMC-reID, improving from the baseline 82.2% and 72.7%, respectively. We also provide a cleaned data list of DukeMTMC-reID to improve benchmarking for person re-identification. This can be found at https://github.com/RenMin1991/cleaned-DukeMTMC-reID/.",1
"People live in a 3D world. However, existing works on person re-identification (re-id) mostly consider the semantic representation learning in a 2D space, intrinsically limiting the understanding of people. In this work, we address this limitation by exploring the prior knowledge of the 3D body structure. Specifically, we project 2D images to a 3D space and introduce a novel parameter-efficient Omni-scale Graph Network (OG-Net) to learn the pedestrian representation directly from 3D point clouds. OG-Net effectively exploits the local information provided by sparse 3D points and takes advantage of the structure and appearance information in a coherent manner. With the help of 3D geometry information, we can learn a new type of deep re-id feature free from noisy variants, such as scale and viewpoint. To our knowledge, we are among the first attempts to conduct person re-identification in the 3D space. We demonstrate through extensive experiments that the proposed method (1) eases the matching difficulty in the traditional 2D space, (2) exploits the complementary information of 2D appearance and 3D structure, (3) achieves competitive results with limited parameters on four large-scale person re-id datasets, and (4) has good scalability to unseen datasets. Our code, models and generated 3D human data are publicly available at https://github.com/layumi/person-reid-3d .",0
"While people live in a 3D world, most existing research on person re-identification (re-id) focuses on semantic representation learning in a 2D space, which limits our understanding of individuals. To address this, we utilized prior knowledge of the 3D body structure and projected 2D images to a 3D space. Our approach introduced a novel parameter-efficient Omni-scale Graph Network (OG-Net) that learns pedestrian representation directly from 3D point clouds. By exploiting local information and structure and appearance information effectively, OG-Net learns a new type of deep re-id feature that is free from noisy variants such as scale and viewpoint. Our method is among the first to conduct person re-identification in the 3D space, and our extensive experiments showed that it eases matching difficulties in the traditional 2D space, exploits complementary information, achieves competitive results with limited parameters on four large-scale person re-id datasets, and has good scalability to unseen datasets. We have made our code, models, and generated 3D human data publicly available at https://github.com/layumi/person-reid-3d.",1
"Contrastive self-supervised learning has shown impressive results in learning visual representations from unlabeled images by enforcing invariance against different data augmentations. However, the learned representations are often contextually biased to the spurious scene correlations of different objects or object and background, which may harm their generalization on the downstream tasks. To tackle the issue, we develop a novel object-aware contrastive learning framework that first (a) localizes objects in a self-supervised manner and then (b) debias scene correlations via appropriate data augmentations considering the inferred object locations. For (a), we propose the contrastive class activation map (ContraCAM), which finds the most discriminative regions (e.g., objects) in the image compared to the other images using the contrastively trained models. We further improve the ContraCAM to detect multiple objects and entire shapes via an iterative refinement procedure. For (b), we introduce two data augmentations based on ContraCAM, object-aware random crop and background mixup, which reduce contextual and background biases during contrastive self-supervised learning, respectively. Our experiments demonstrate the effectiveness of our representation learning framework, particularly when trained under multi-object images or evaluated under the background (and distribution) shifted images.",0
"The use of contrastive self-supervised learning has yielded impressive results in learning visual representations from unlabeled images. However, the learned representations may be contextually biased to spurious scene correlations, which can hinder their effectiveness in downstream tasks. To address this issue, we propose a new framework that localizes objects in a self-supervised manner and debiases scene correlations through appropriate data augmentations. To localize objects, we introduce the contrastive class activation map (ContraCAM), which identifies the most discriminative regions in the image compared to other images. We also improve ContraCAM to detect multiple objects and shapes through an iterative refinement process. To debias scene correlations, we introduce object-aware random crop and background mixup data augmentations. Our experiments demonstrate the effectiveness of our framework, particularly in multi-object images and background shifted images.",1
"The challenge of the Class Incremental Learning~(CIL) lies in difficulty for a learner to discern the old classes' data from the new as no previous classes' data is preserved. In this paper, we reveal three causes for catastrophic forgetting at the representational level, namely, representation forgetting, representation overlapping, and classifier deviation. Based on the observation above, we propose a new CIL framework, Contrastive Class Concentration for CIL (C4IL) to alleviate the phenomenon of representation overlapping that works in both memory-based and memory-free methods. Our framework leverages the class concentration effect of contrastive representation learning, therefore yielding a representation distribution with better intra-class compatibility and inter-class separability. Quantitative experiments showcase the effectiveness of our framework: it outperforms the baseline methods by 5% in terms of the average and top-1 accuracy in 10-phase and 20-phase CIL. Qualitative results also demonstrate that our method generates a more compact representation distribution that alleviates the overlapping problem.",0
"The main difficulty faced by learners in Class Incremental Learning (CIL) is distinguishing between old and new classes, as no previous class data is retained. This paper identifies three causes of catastrophic forgetting at the representational level: representation forgetting, representation overlapping, and classifier deviation. To address the problem of representation overlapping, we propose a new CIL framework called Contrastive Class Concentration for CIL (C4IL), which works in both memory-based and memory-free methods. Our framework utilizes the class concentration effect of contrastive representation learning to produce a representation distribution with better intra-class compatibility and inter-class separability. Our quantitative experiments demonstrate that our framework outperforms baseline methods by 5% in terms of average and top-1 accuracy in 10-phase and 20-phase CIL. Qualitative results also show that our method generates a more compact representation distribution, which reduces the overlapping problem.",1
"Learning to classify time series with limited data is a practical yet challenging problem. Current methods are primarily based on hand-designed feature extraction rules or domain-specific data augmentation. Motivated by the advances in deep speech processing models and the fact that voice data are univariate temporal signals, in this paper, we propose Voice2Series (V2S), a novel end-to-end approach that reprograms acoustic models for time series classification, through input transformation learning and output label mapping. Leveraging the representation learning power of a large-scale pre-trained speech processing model, on 30 different time series tasks we show that V2S either outperforms or is tied with state-of-the-art methods on 20 tasks, and improves their average accuracy by 1.84%. We further provide a theoretical justification of V2S by proving its population risk is upper bounded by the source risk and a Wasserstein distance accounting for feature alignment via reprogramming. Our results offer new and effective means to time series classification.",0
"Classifying time series with limited data is a challenging yet practical problem. Currently, methods rely on hand-designed feature extraction rules or domain-specific data augmentation techniques. Inspired by recent advancements in deep speech processing models and the univariate nature of voice data, this paper introduces Voice2Series (V2S), a novel end-to-end approach that reprograms acoustic models for time series classification. V2S achieves this through input transformation learning and output label mapping. By leveraging the representation learning capability of a pre-trained speech processing model on 30 different time series tasks, V2S either outperforms or equals state-of-the-art methods on 20 tasks, with an average accuracy improvement of 1.84%. Additionally, we provide a theoretical justification for V2S by proving its population risk is upper bounded by the source risk and a Wasserstein distance that accounts for feature alignment via reprogramming. Our results offer a new and effective approach to time series classification.",1
"Recent studies show that advanced priors play a major role in deep generative models. Exemplar VAE, as a variant of VAE with an exemplar-based prior, has achieved impressive results. However, due to the nature of model design, an exemplar-based model usually requires vast amounts of data to participate in training, which leads to huge computational complexity. To address this issue, we propose Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE), a new variant of VAE with a prior based on Bayesian pseudocoreset. The proposed prior is conditioned on a small-scale pseudocoreset rather than the whole dataset for reducing the computational cost and avoiding overfitting. Simultaneously, we obtain the optimal pseudocoreset via a stochastic optimization algorithm during VAE training aiming to minimize the Kullback-Leibler divergence between the prior based on the pseudocoreset and that based on the whole dataset. Experimental results show that ByPE-VAE can achieve competitive improvements over the state-of-the-art VAEs in the tasks of density estimation, representation learning, and generative data augmentation. Particularly, on a basic VAE architecture, ByPE-VAE is up to 3 times faster than Exemplar VAE while almost holding the performance. Code is available at our supplementary materials.",0
"Recent research indicates that deep generative models heavily rely on advanced priors. One impressive example is Exemplar VAE, which employs an exemplar-based prior and has demonstrated remarkable outcomes. Nevertheless, due to the nature of its design, an exemplar-based model necessitates a large amount of data to be involved in training, resulting in significant computational complexity. To overcome this issue, we introduce Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE), a novel VAE variation that employs a prior based on Bayesian pseudocoreset. This new prior is based on a small-scale pseudocoreset rather than the entire dataset to decrease computational costs and prevent overfitting. We also use a stochastic optimization algorithm to obtain the optimal pseudocoreset during VAE training, aiming to minimize the Kullback-Leibler divergence between the prior based on the pseudocoreset and that based on the entire dataset. Experimental results demonstrate that ByPE-VAE delivers competitive enhancements over state-of-the-art VAEs in density estimation, representation learning, and generative data augmentation tasks. Furthermore, ByPE-VAE is up to 3 times faster than Exemplar VAE on a basic VAE architecture while maintaining a similar level of performance. Our supplementary materials provide access to the code.",1
"Effective molecular representation learning is of great importance to facilitate molecular property prediction, which is a fundamental task for the drug and material industry. Recent advances in graph neural networks (GNNs) have shown great promise in applying GNNs for molecular representation learning. Moreover, a few recent studies have also demonstrated successful applications of self-supervised learning methods to pre-train the GNNs to overcome the problem of insufficient labeled molecules. However, existing GNNs and pre-training strategies usually treat molecules as topological graph data without fully utilizing the molecular geometry information. Whereas, the three-dimensional (3D) spatial structure of a molecule, a.k.a molecular geometry, is one of the most critical factors for determining molecular physical, chemical, and biological properties. To this end, we propose a novel Geometry Enhanced Molecular representation learning method (GEM) for Chemical Representation Learning (ChemRL). At first, we design a geometry-based GNN architecture that simultaneously models atoms, bonds, and bond angles in a molecule. To be specific, we devised double graphs for a molecule: The first one encodes the atom-bond relations; The second one encodes bond-angle relations. Moreover, on top of the devised GNN architecture, we propose several novel geometry-level self-supervised learning strategies to learn spatial knowledge by utilizing the local and global molecular 3D structures. We compare ChemRL-GEM with various state-of-the-art (SOTA) baselines on different molecular benchmarks and exhibit that ChemRL-GEM can significantly outperform all baselines in both regression and classification tasks. For example, the experimental results show an overall improvement of 8.8% on average compared to SOTA baselines on the regression tasks, demonstrating the superiority of the proposed method.",0
"The ability to effectively learn molecular representation is crucial in predicting molecular properties, a task fundamental to the drug and material industry. Graph neural networks (GNNs) have recently shown promise in this area, and some studies have successfully employed self-supervised learning methods to pre-train GNNs when labeled molecules are insufficient. However, current GNNs and pre-training strategies do not fully utilize the molecular geometry information, which is critical in determining molecular properties. To address this, we propose a novel method called Geometry Enhanced Molecular representation learning (GEM) for Chemical Representation Learning (ChemRL). Our approach involves a geometry-based GNN architecture that models the atoms, bonds, and bond angles in a molecule using double graphs. Additionally, we introduce several geometry-level self-supervised learning strategies to learn spatial knowledge by utilizing the local and global molecular 3D structures. Our experiments demonstrate that ChemRL-GEM outperforms state-of-the-art (SOTA) baselines in both regression and classification tasks, with an overall improvement of 8.8% on average in regression tasks. This highlights the superiority of our proposed method.",1
"Electricity load forecasting is crucial for the power systems' planning and maintenance. However, its un-stationary and non-linear characteristics impose significant difficulties in anticipating future demand. This paper proposes a novel ensemble deep Random Vector Functional Link (edRVFL) network for electricity load forecasting. The weights of hidden layers are randomly initialized and kept fixed during the training process. The hidden layers are stacked to enforce deep representation learning. Then, the model generates the forecasts by ensembling the outputs of each layer. Moreover, we also propose to augment the random enhancement features by empirical wavelet transformation (EWT). The raw load data is decomposed by EWT in a walk-forward fashion, not introducing future data leakage problems in the decomposition process. Finally, all the sub-series generated by the EWT, including raw data, are fed into the edRVFL for forecasting purposes. The proposed model is evaluated on twenty publicly available time series from the Australian Energy Market Operator of the year 2020. The simulation results demonstrate the proposed model's superior performance over eleven forecasting methods in three error metrics and statistical tests on electricity load forecasting tasks.",0
"Forecasting electricity loads is essential to plan and maintain power systems, but it is challenging due to its non-linear and un-stationary nature. This study introduces a novel approach called the ensemble deep Random Vector Functional Link (edRVFL) network for electricity load forecasting. The network's hidden layer weights are initialized randomly and kept fixed during training, and deep representation learning is enforced by stacking the layers. The model generates forecasts by ensembling the layer outputs. Additionally, the authors suggest augmenting the random enhancement features through empirical wavelet transformation (EWT). EWT decomposes the raw load data in a walk-forward manner, avoiding future data leakage issues. The edRVFL then uses all EWT-generated sub-series, including raw data, for forecasting. The proposed model is evaluated on twenty public time series from the Australian Energy Market Operator of 2020. Results demonstrate its superiority over eleven other forecasting methods based on three error metrics and statistical tests.",1
"As a crucial task of autonomous driving, 3D object detection has made great progress in recent years. However, monocular 3D object detection remains a challenging problem due to the unsatisfactory performance in depth estimation. Most existing monocular methods typically directly regress the scene depth while ignoring important relationships between the depth and various geometric elements (e.g. bounding box sizes, 3D object dimensions, and object poses). In this paper, we propose to learn geometry-guided depth estimation with projective modeling to advance monocular 3D object detection. Specifically, a principled geometry formula with projective modeling of 2D and 3D depth predictions in the monocular 3D object detection network is devised. We further implement and embed the proposed formula to enable geometry-aware deep representation learning, allowing effective 2D and 3D interactions for boosting the depth estimation. Moreover, we provide a strong baseline through addressing substantial misalignment between 2D annotation and projected boxes to ensure robust learning with the proposed geometric formula. Experiments on the KITTI dataset show that our method remarkably improves the detection performance of the state-of-the-art monocular-based method without extra data by 2.80% on the moderate test setting. The model and code will be released at https://github.com/YinminZhang/MonoGeo.",0
"Recent years have seen significant advancements in 3D object detection, a crucial aspect of autonomous driving. However, monocular 3D object detection remains a challenging problem due to unsatisfactory depth estimation performance. Most existing monocular methods tend to overlook vital relationships between depth and various geometric elements such as bounding box sizes, 3D object dimensions, and object poses, instead opting to directly regress the scene depth. This paper proposes a geometry-guided depth estimation approach using projective modeling to improve monocular 3D object detection. The approach devises a principled geometry formula with projective modeling to predict 2D and 3D depth, which is then implemented and embedded to facilitate geometry-aware deep representation learning. This allows for effective 2D and 3D interactions to enhance the depth estimation. The proposed geometric formula addresses substantial misalignment between 2D annotation and projected boxes, ensuring robust learning and providing a strong baseline. Experiments on the KITTI dataset demonstrate that the proposed approach significantly improves detection performance of state-of-the-art monocular-based methods by 2.80% on the moderate test setting. The model and code are available at https://github.com/YinminZhang/MonoGeo.",1
"Knowledge distillation often involves how to define and transfer knowledge from teacher to student effectively. Although recent self-supervised contrastive knowledge achieves the best performance, forcing the network to learn such knowledge may damage the representation learning of the original class recognition task. We therefore adopt an alternative self-supervised augmented task to guide the network to learn the joint distribution of the original recognition task and self-supervised auxiliary task. It is demonstrated as a richer knowledge to improve the representation power without losing the normal classification capability. Moreover, it is incomplete that previous methods only transfer the probabilistic knowledge between the final layers. We propose to append several auxiliary classifiers to hierarchical intermediate feature maps to generate diverse self-supervised knowledge and perform the one-to-one transfer to teach the student network thoroughly. Our method significantly surpasses the previous SOTA SSKD with an average improvement of 2.56\% on CIFAR-100 and an improvement of 0.77\% on ImageNet across widely used network pairs. Codes are available at https://github.com/winycg/HSAKD.",0
"The process of knowledge distillation involves effectively defining and transferring knowledge from a teacher to a student. While recent self-supervised contrastive knowledge has shown great performance, it may harm the representation learning of the original class recognition task. To address this issue, we propose an alternative self-supervised augmented task that guides the network to learn the joint distribution of the original recognition task and self-supervised auxiliary task. This approach provides richer knowledge that improves the representation power without sacrificing normal classification capability. In addition, our method goes beyond previous techniques that only transfer probabilistic knowledge between final layers. We append auxiliary classifiers to hierarchical intermediate feature maps, generating diverse self-supervised knowledge and performing one-to-one transfer to thoroughly teach the student network. Our method outperforms the previous state-of-the-art self-supervised knowledge distillation (SSKD) by an average improvement of 2.56% on CIFAR-100 and 0.77% on ImageNet across widely used network pairs. Our codes are available at https://github.com/winycg/HSAKD.",1
"EEG-based emotion recognition often requires sufficient labeled training samples to build an effective computational model. Labeling EEG data, on the other hand, is often expensive and time-consuming. To tackle this problem and reduce the need for output labels in the context of EEG-based emotion recognition, we propose a semi-supervised pipeline to jointly exploit both unlabeled and labeled data for learning EEG representations. Our semi-supervised framework consists of both unsupervised and supervised components. The unsupervised part maximizes the consistency between original and reconstructed input data using an autoencoder, while simultaneously the supervised part minimizes the cross-entropy between the input and output labels. We evaluate our framework using both a stacked autoencoder and an attention-based recurrent autoencoder. We test our framework on the large-scale SEED EEG dataset and compare our results with several other popular semi-supervised methods. Our semi-supervised framework with a deep attention-based recurrent autoencoder consistently outperforms the benchmark methods, even when small sub-sets (3\%, 5\% and 10\%) of the output labels are available during training, achieving a new state-of-the-art semi-supervised performance.",0
"To perform EEG-based emotion recognition effectively, a considerable number of labeled training samples are required, but labeling EEG data is time-consuming and expensive. To address this issue and reduce the demand for output labels in EEG-based emotion recognition, we suggest a semi-supervised approach that leverages both labeled and unlabeled data for learning EEG representations. Our semi-supervised framework includes unsupervised and supervised components. The unsupervised part employs an autoencoder to maximize consistency between the original and reconstructed input data, while the supervised part minimizes the cross-entropy between input and output labels. We assess the efficacy of our framework using a stacked autoencoder and an attention-based recurrent autoencoder. We test our framework on the SEED EEG dataset and compare it to other semi-supervised methods. Our deep attention-based recurrent autoencoder consistently outperforms benchmark methods, even with small subsets of output labels available during training, achieving a new state-of-the-art semi-supervised performance.",1
"Constructing appropriate representations of molecules lies at the core of numerous tasks such as material science, chemistry and drug designs. Recent researches abstract molecules as attributed graphs and employ graph neural networks (GNN) for molecular representation learning, which have made remarkable achievements in molecular graph modeling. Albeit powerful, current models either are based on local aggregation operations and thus miss higher-order graph properties or focus on only node information without fully using the edge information. For this sake, we propose a Communicative Message Passing Transformer (CoMPT) neural network to improve the molecular graph representation by reinforcing message interactions between nodes and edges based on the Transformer architecture. Unlike the previous transformer-style GNNs that treat molecules as fully connected graphs, we introduce a message diffusion mechanism to leverage the graph connectivity inductive bias and reduce the message enrichment explosion. Extensive experiments demonstrated that the proposed model obtained superior performances (around 4$\%$ on average) against state-of-the-art baselines on seven chemical property datasets (graph-level tasks) and two chemical shift datasets (node-level tasks). Further visualization studies also indicated a better representation capacity achieved by our model.",0
"Developing suitable molecular representations is crucial in various fields such as material science, drug design, and chemistry. In recent studies, molecules are represented as attribute graphs, and graph neural networks (GNN) are utilized for molecular representation learning, achieving impressive results in molecular graph modeling. However, current models either neglect higher-order graph properties or only focus on node information, overlooking edge information. This prompted us to propose a neural network called Communicative Message Passing Transformer (CoMPT), which enhances molecular graph representation by promoting message interactions between nodes and edges based on the Transformer architecture. Unlike previous transformer-style GNNs that treat molecules as fully connected graphs, our model introduces a message diffusion mechanism to leverage graph connectivity inductive bias and reduce message enrichment explosion. Extensive experiments showed that our model outperformed state-of-the-art baselines by approximately 4% on average on seven chemical property datasets (graph-level tasks) and two chemical shift datasets (node-level tasks). Further visualization studies indicated a better representation capacity achieved by our model.",1
"Training machine learning models with the only accuracy as a final goal may promote prejudices and discriminatory behaviors embedded in the data. One solution is to learn latent representations that fulfill specific fairness metrics. Different types of learning methods are employed to map data into the fair representational space. The main purpose is to learn a latent representation of data that scores well on a fairness metric while maintaining the usability for the downstream task. In this paper, we propose a new fair representation learning approach that leverages different levels of representation of data to tighten the fairness bounds of the learned representation. Our results show that stacking different auto-encoders and enforcing fairness at different latent spaces result in an improvement of fairness compared to other existing approaches.",0
"Focusing solely on accuracy when training machine learning models can perpetuate biases and discriminatory patterns already present in the data. To address this issue, it is recommended to train models to learn latent representations that meet specific fairness criteria. Various learning methods are utilized to map data into a fair representational space, with the goal of achieving a latent representation that satisfies fairness metrics while remaining useful for the intended task. This paper introduces a novel approach to fair representation learning, which involves using multiple levels of data representation to enhance fairness. Our findings indicate that incorporating different auto-encoders and applying fairness at various latent spaces can lead to better fairness outcomes compared to existing methods.",1
"Variants of Graph Neural Networks (GNNs) for representation learning have been proposed recently and achieved fruitful results in various fields. Among them, Graph Attention Network (GAT) first employs a self-attention strategy to learn attention weights for each edge in the spatial domain. However, learning the attentions over edges can only focus on the local information of graphs and greatly increases the computational costs. In this paper, we first introduce the attention mechanism in the spectral domain of graphs and present Spectral Graph Attention Network (SpGAT) that learns representations for different frequency components regarding weighted filters and graph wavelets bases. In this way, SpGAT can better capture global patterns of graphs in an efficient manner with much fewer learned parameters than that of GAT. Further, to reduce the computational cost of SpGAT brought by the eigen-decomposition, we propose a fast approximation variant SpGAT-Cheby. We thoroughly evaluate the performance of SpGAT and SpGAT-Cheby in semi-supervised node classification tasks and verify the effectiveness of the learned attentions in the spectral domain.",0
"Recently, various Graph Neural Network (GNN) variants have been proposed for representation learning and have shown promising results in different fields. One of these variants, the Graph Attention Network (GAT), utilizes a self-attention approach to learn attention weights for each edge in the spatial domain. However, this method only focuses on the local information of graphs and leads to high computational costs. To address this limitation, we introduce an attention mechanism in the spectral domain of graphs and propose the Spectral Graph Attention Network (SpGAT). SpGAT learns representations for different frequency components using weighted filters and graph wavelets bases, enabling it to capture global patterns of graphs more effectively while using fewer learned parameters than GAT. To further reduce the computational cost of SpGAT, we propose a faster approximation variant called SpGAT-Cheby. We evaluate the performance of both SpGAT and SpGAT-Cheby in semi-supervised node classification tasks and demonstrate the effectiveness of the learned attentions in the spectral domain.",1
"As a kind of generative self-supervised learning methods, generative adversarial nets have been widely studied in the field of anomaly detection. However, the representation learning ability of the generator is limited since it pays too much attention to pixel-level details, and generator is difficult to learn abstract semantic representations from label prediction pretext tasks as effective as discriminator. In order to improve the representation learning ability of generator, we propose a self-supervised learning framework combining generative methods and discriminative methods. The generator no longer learns representation by reconstruction error, but the guidance of discriminator, and could benefit from pretext tasks designed for discriminative methods. Our discriminative-generative representation learning method has performance close to discriminative methods and has a great advantage in speed. Our method used in one-class anomaly detection task significantly outperforms several state-of-the-arts on multiple benchmark data sets, increases the performance of the top-performing GAN-based baseline by 6% on CIFAR-10 and 2% on MVTAD.",0
"Generative adversarial nets are a popular type of self-supervised learning method used in anomaly detection. However, the generator's ability to learn abstract semantic representations is limited because it focuses too much on pixel-level details and struggles with label prediction tasks. To overcome this limitation, we propose a new self-supervised learning framework that combines generative and discriminative methods. Our approach allows the generator to learn from the discriminator's guidance and benefit from discriminative tasks, resulting in improved representation learning and faster performance. We tested our discriminative-generative method on several benchmark data sets for one-class anomaly detection, and it outperformed several state-of-the-art methods, increasing the performance of the top-performing GAN-based baseline by 6% on CIFAR-10 and 2% on MVTAD.",1
"The defining challenge for causal inference from observational data is the presence of `confounders', covariates that affect both treatment assignment and the outcome. To address this challenge, practitioners collect and adjust for the covariates, hoping that they adequately correct for confounding. However, including every observed covariate in the adjustment runs the risk of including `bad controls', variables that induce bias when they are conditioned on. The problem is that we do not always know which variables in the covariate set are safe to adjust for and which are not. To address this problem, we develop Nearly Invariant Causal Estimation (NICE). NICE uses invariant risk minimization (IRM) [Arj19] to learn a representation of the covariates that, under some assumptions, strips out bad controls but preserves sufficient information to adjust for confounding. Adjusting for the learned representation, rather than the covariates themselves, avoids the induced bias and provides valid causal inferences. We evaluate NICE on both synthetic and semi-synthetic data. When the covariates contain unknown collider variables and other bad controls, NICE performs better than adjusting for all the covariates.",0
"The primary obstacle to drawing causal conclusions from observational data is the existence of ""confounders,"" which refer to variables that impact both the treatment assignment and the outcome. To overcome this hurdle, researchers gather and account for the covariates, with the expectation that they will successfully address confounding. However, incorporating every observed covariate in the adjustment process can result in ""bad controls,"" or variables that introduce bias when adjusted for. The issue lies in the fact that it is challenging to determine which variables in the covariate set are safe to adjust for and which are not. Our solution to this issue is Nearly Invariant Causal Estimation (NICE), which employs invariant risk minimization (IRM) [Arj19] to develop a representation of the covariates that removes bad controls while retaining enough information to handle confounding. By adjusting for the learned representation instead of the covariates themselves, we avoid induced bias and generate valid causal inferences. We put NICE to the test using both synthetic and semi-synthetic data and discovered that it outperforms adjusting for all the covariates when the covariates include unknown collider variables and other bad controls.",1
"Deep learning has proven effective for various application tasks, but its applicability is limited by the reliance on annotated examples. Self-supervised learning has emerged as a promising direction to alleviate the supervision bottleneck, but existing work focuses on leveraging co-occurrences in unlabeled data for task-agnostic representation learning, as exemplified by masked language model pretraining. In this chapter, we explore task-specific self-supervision, which leverages domain knowledge to automatically annotate noisy training examples for end applications, either by introducing labeling functions for annotating individual instances, or by imposing constraints over interdependent label decisions. We first present deep probabilistic logic(DPL), which offers a unifying framework for task-specific self-supervision by composing probabilistic logic with deep learning. DPL represents unknown labels as latent variables and incorporates diverse self-supervision using probabilistic logic to train a deep neural network end-to-end using variational EM. Next, we present self-supervised self-supervision(S4), which adds to DPL the capability to learn new self-supervision automatically. Starting from an initial seed self-supervision, S4 iteratively uses the deep neural network to propose new self supervision. These are either added directly (a form of structured self-training) or verified by a human expert (as in feature-based active learning). Experiments on real-world applications such as biomedical machine reading and various text classification tasks show that task-specific self-supervision can effectively leverage domain expertise and often match the accuracy of supervised methods with a tiny fraction of human effort.",0
"Although deep learning has demonstrated effectiveness in several application tasks, its implementation is restricted by the dependence on annotated examples. To overcome this limitation, self-supervised learning has emerged as a promising approach. However, current research focuses on utilizing co-occurrences in unlabeled data for task-agnostic representation learning, such as masked language model pretraining. In this chapter, we investigate task-specific self-supervision, which employs domain knowledge to automatically annotate noisy training examples for end applications. This is achieved by introducing labeling functions for annotating individual instances or imposing constraints on interdependent label decisions. We introduce deep probabilistic logic (DPL), which provides a comprehensive framework for task-specific self-supervision by combining probabilistic logic with deep learning. DPL represents unknown labels as latent variables and includes diverse self-supervision using probabilistic logic to train a deep neural network end-to-end using variational EM. Additionally, we present self-supervised self-supervision (S4), which expands DPL's capabilities by automatically learning new self-supervision. S4 proposes new self-supervision using the deep neural network, which is either added directly or verified by a human expert. Our experiments on real-world applications, such as biomedical machine reading and various text classification tasks, demonstrate that task-specific self-supervision can leverage domain expertise effectively and frequently match supervised methods' accuracy with minimal human effort.",1
"We propose a new method to detect deepfake images using the cue of the source feature inconsistency within the forged images. It is based on the hypothesis that images' distinct source features can be preserved and extracted after going through state-of-the-art deepfake generation processes. We introduce a novel representation learning approach, called pair-wise self-consistency learning (PCL), for training ConvNets to extract these source features and detect deepfake images. It is accompanied by a new image synthesis approach, called inconsistency image generator (I2G), to provide richly annotated training data for PCL. Experimental results on seven popular datasets show that our models improve averaged AUC over the state of the art from 96.45% to 98.05% in the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset evaluation.",0
"A new technique has been proposed to identify deepfake images by utilizing the cue of source feature inconsistency present within the manipulated images. It is believed that despite undergoing advanced deepfake generation processes, images tend to retain their unique source features which can be extracted. To achieve this, a fresh representation learning approach called pair-wise self-consistency learning (PCL) has been introduced for ConvNets training. Additionally, an inconsistency image generator (I2G) has been introduced to provide well-annotated training data for PCL. The experimental results of this method on seven commonly used datasets have shown that it enhances the averaged AUC compared to the current state of the art, from 96.45% to 98.05% in the in-dataset evaluation, and from 86.03% to 92.18% in the cross-dataset evaluation.",1
"We propose a decentralised ""local2global"" approach to graph representation learning, that one can a-priori use to scale any embedding technique. Our local2global approach proceeds by first dividing the input graph into overlapping subgraphs (or ""patches"") and training local representations for each patch independently. In a second step, we combine the local representations into a globally consistent representation by estimating the set of rigid motions that best align the local representations using information from the patch overlaps, via group synchronization. A key distinguishing feature of local2global relative to existing work is that patches are trained independently without the need for the often costly parameter synchronisation during distributed training. This allows local2global to scale to large-scale industrial applications, where the input graph may not even fit into memory and may be stored in a distributed manner. Preliminary results on medium-scale data sets (up to $\sim$7K nodes and $\sim$200K edges) are promising, with a graph reconstruction performance for local2global that is comparable to that of globally trained embeddings. A thorough evaluation of local2global on large scale data and applications to downstream tasks, such as node classification and link prediction, constitutes ongoing work.",0
"Our proposal is a ""local2global"" approach for graph representation learning that enables scaling of any embedding technique. This approach involves dividing the input graph into overlapping subgraphs, or patches, and training individual local representations for each patch. The local representations are then combined into a globally consistent representation using group synchronization to estimate the set of rigid motions that best align the local representations. Unlike existing methods, our approach allows for independent training of patches without requiring costly parameter synchronization during distributed training. This enables local2global to scale to large-scale industrial applications where the input graph may be too large to fit into memory and stored in a distributed manner. Initial results on medium-scale data sets are promising, with comparable graph reconstruction performance to globally trained embeddings. Ongoing work includes thorough evaluation of local2global on large scale data and its application to downstream tasks such as node classification and link prediction.",1
"Highly complex deep learning models are increasingly integrated into modern cyber-physical systems (CPS), many of which have strict safety requirements. One problem arising from this is that deep learning lacks interpretability, operating as a black box. The reliability of deep learning is heavily impacted by how well the model training data represents runtime test data, especially when the input space dimension is high as natural images. In response, we propose a robust out-of-distribution (OOD) detection framework. Our approach detects unusual movements from driving video in real-time by combining classical optic flow operation with representation learning via variational autoencoder (VAE). We also design a method to locate OOD factors in images. Evaluation on a driving simulation data set shows that our approach is statistically more robust than related works.",0
"Modern cyber-physical systems (CPS) are increasingly utilizing highly complex deep learning models. However, these systems often have strict safety requirements, and the lack of interpretability of deep learning poses a problem as it operates as a black box. In particular, the reliability of deep learning is heavily affected by the adequacy of the model training data in representing runtime test data, especially when dealing with high-dimensional input spaces such as natural images. To address this issue, we present a robust out-of-distribution (OOD) detection framework that combines classical optic flow operation with representation learning via variational autoencoder (VAE) to detect unusual movements from driving video in real-time. Moreover, we also developed a method to locate OOD factors in images. Our evaluation on a driving simulation dataset demonstrates the statistical superiority of our approach over related works.",1
"We propose a novel framework for image clustering that incorporates joint representation learning and clustering. Our method consists of two heads that share the same backbone network - a ""representation learning"" head and a ""clustering"" head. The ""representation learning"" head captures fine-grained patterns of objects at the instance level which serve as clues for the ""clustering"" head to extract coarse-grain information that separates objects into clusters. The whole model is trained in an end-to-end manner by minimizing the weighted sum of two sample-oriented contrastive losses applied to the outputs of the two heads. To ensure that the contrastive loss corresponding to the ""clustering"" head is optimal, we introduce a novel critic function called ""log-of-dot-product"". Extensive experimental results demonstrate that our method significantly outperforms state-of-the-art single-stage clustering methods across a variety of image datasets, improving over the best baseline by about 5-7% in accuracy on CIFAR10/20, STL10, and ImageNet-Dogs. Further, the ""two-stage"" variant of our method also achieves better results than baselines on three challenging ImageNet subsets.",0
"Our proposed framework introduces a new approach to image clustering by combining joint representation learning and clustering. The framework comprises two heads that share a common backbone network, namely the ""representation learning"" head and the ""clustering"" head. The former captures detailed object patterns at an instance level, which serve as indications for the latter to extract coarse-grain information and separate objects into clusters. The whole model is trained end-to-end, minimizing the weighted sum of two sample-oriented contrastive losses applied to the outputs of the two heads. To enhance the performance of the ""clustering"" head's contrastive loss, we introduce a novel critic function called ""log-of-dot-product."" Our extensive experiments demonstrate that our method significantly outperforms state-of-the-art single-stage clustering methods across various image datasets, resulting in an accuracy improvement of about 5-7% over the best baseline on CIFAR10/20, STL10, and ImageNet-Dogs. Additionally, the ""two-stage"" variant of our method achieves better results than baselines on three challenging ImageNet subsets.",1
"We define disentanglement as how far class-different data points from each other are, relative to the distances among class-similar data points. When maximizing disentanglement during representation learning, we obtain a transformed feature representation where the class memberships of the data points are preserved. If the class memberships of the data points are preserved, we would have a feature representation space in which a nearest neighbour classifier or a clustering algorithm would perform well. We take advantage of this method to learn better natural language representation, and employ it on text classification and text clustering tasks. Through disentanglement, we obtain text representations with better-defined clusters and improve text classification performance. Our approach had a test classification accuracy of as high as 90.11% and test clustering accuracy of 88% on the AG News dataset, outperforming our baseline models -- without any other training tricks or regularization.",0
"Disentanglement is defined as the degree of separation between class-different data points in relation to those that are class-similar. By maximizing disentanglement during representation learning, we can achieve a transformed feature representation that maintains the class memberships of the data points. This results in a feature representation space that facilitates effective performance by nearest neighbour classifiers or clustering algorithms. Our approach leverages this method to enhance natural language representation, particularly for text classification and clustering. Through disentanglement, we obtain text representations with more distinct clusters and improved text classification accuracy. Our method achieved a test classification accuracy of 90.11% and test clustering accuracy of 88% on the AG News dataset, outperforming baseline models without the need for additional training techniques or regularization.",1
"Deep learning models have gained great popularity in statistical modeling because they lead to very competitive regression models, often outperforming classical statistical models such as generalized linear models. The disadvantage of deep learning models is that their solutions are difficult to interpret and explain, and variable selection is not easily possible because deep learning models solve feature engineering and variable selection internally in a nontransparent way. Inspired by the appealing structure of generalized linear models, we propose a new network architecture that shares similar features as generalized linear models, but provides superior predictive power benefiting from the art of representation learning. This new architecture allows for variable selection of tabular data and for interpretation of the calibrated deep learning model, in fact, our approach provides an additive decomposition in the spirit of Shapley values and integrated gradients.",0
"Statistical modeling has seen a surge in popularity with the rise of deep learning models. These models often surpass traditional methods like generalized linear models in regression accuracy, but their lack of interpretability and limited variable selection capabilities remain major drawbacks. Drawing inspiration from the structure of generalized linear models, we propose a novel network architecture that combines the benefits of representation learning with interpretability. Our approach allows for variable selection and includes an additive decomposition akin to Shapley values and integrated gradients, enabling the interpretation of calibrated deep learning models.",1
"An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world. In this paper, we test whether 17 unsupervised, weakly supervised, and fully supervised representation learning approaches correctly infer the generative factors of variation in simple datasets (dSprites, Shapes3D, MPI3D). In contrast to prior robustness work that introduces novel factors of variation during test time, such as blur or other (un)structured noise, we here recompose, interpolate, or extrapolate only existing factors of variation from the training data set (e.g., small and medium-sized objects during training and large objects during testing). Models that learn the correct mechanism should be able to generalize to this benchmark. In total, we train and test 2000+ models and observe that all of them struggle to learn the underlying mechanism regardless of supervision signal and architectural bias. Moreover, the generalization capabilities of all tested models drop significantly as we move from artificial datasets towards more realistic real-world datasets. Despite their inability to identify the correct mechanism, the models are quite modular as their ability to infer other in-distribution factors remains fairly stable, providing only a single factor is out-of-distribution. These results point to an important yet understudied problem of learning mechanistic models of observations that can facilitate generalization.",0
"To achieve generalization in machine learning, it is crucial to uncover the latent factors of variation and their mechanisms in the world. This study examines 17 unsupervised, weakly supervised, and fully supervised representation learning methods to determine their ability to identify generative factors of variation in simple datasets. Unlike prior work introducing novel factors of variation during testing, this study recomposes, interpolates, or extrapolates existing factors of variation from the training dataset. The study trains and tests over 2000 models and finds that all of them struggle to learn the underlying mechanism, regardless of supervision signal and architectural bias. Additionally, the models' generalization capabilities decrease significantly when moving from artificial datasets to more realistic real-world datasets. Despite their inability to identify the correct mechanism, the models remain modular and can infer other in-distribution factors. These findings highlight the need for further research on learning mechanistic models that facilitate generalization.",1
"Scene text editing (STE), which converts a text in a scene image into the desired text while preserving an original style, is a challenging task due to a complex intervention between text and style. To address this challenge, we propose a novel representational learning-based STE model, referred to as RewriteNet that employs textual information as well as visual information. We assume that the scene text image can be decomposed into content and style features where the former represents the text information and style represents scene text characteristics such as font, alignment, and background. Under this assumption, we propose a method to separately encode content and style features of the input image by introducing the scene text recognizer that is trained by text information. Then, a text-edited image is generated by combining the style feature from the original image and the content feature from the target text. Unlike previous works that are only able to use synthetic images in the training phase, we also exploit real-world images by proposing a self-supervised training scheme, which bridges the domain gap between synthetic and real data. Our experiments demonstrate that RewriteNet achieves better quantitative and qualitative performance than other comparisons. Moreover, we validate that the use of text information and the self-supervised training scheme improves text switching performance. The implementation and dataset will be publicly available.",0
"The task of converting text in an image to desired text while retaining the original style, known as Scene Text Editing (STE), is difficult due to the intricate interaction between text and style. To tackle this challenge, we introduce RewriteNet, a novel STE model that employs both visual and textual information for representational learning. Our approach assumes that scene text images can be broken down into content and style features, where content represents text information and style represents font, alignment, and background. We propose a method to separately encode these features using a scene text recognizer that is trained on text information. We then generate a text-edited image by combining the style feature from the original image and the content feature from the target text. Unlike previous methods that only use synthetic images for training, we use a self-supervised training scheme to bridge the gap between synthetic and real data. Results of our experiments show that RewriteNet outperforms other methods in terms of quantitative and qualitative performance, and that our use of text information and the self-supervised training scheme improves text switching performance. Our implementation and dataset will be publicly available.",1
"Supply chain network data is a valuable asset for businesses wishing to understand their ethical profile, security of supply, and efficiency. Possession of a dataset alone however is not a sufficient enabler of actionable decisions due to incomplete information. In this paper, we present a graph representation learning approach to uncover hidden dependency links that focal companies may not be aware of. To the best of our knowledge, our work is the first to represent a supply chain as a heterogeneous knowledge graph with learnable embeddings. We demonstrate that our representation facilitates state-of-the-art performance on link prediction of a global automotive supply chain network using a relational graph convolutional network. It is anticipated that our method will be directly applicable to businesses wishing to sever links with nefarious entities and mitigate risk of supply failure. More abstractly, it is anticipated that our method will be useful to inform representation learning of supply chain networks for downstream tasks beyond link prediction.",0
"Data on a company's supply chain network is a valuable resource that can assist in understanding their ethical standards, supply security, and efficiency. However, simply possessing this data is not enough to make informed decisions as it may be incomplete. This paper presents a novel approach to uncover hidden dependencies in a company's supply chain using a graph representation learning technique. This approach represents the supply chain as a heterogeneous knowledge graph with learnable embeddings, which is a first in the field. The authors demonstrate the effectiveness of their method by achieving state-of-the-art results in link prediction of a global automotive supply chain network using a relational graph convolutional network. The authors anticipate that this method will be useful for companies looking to sever ties with unethical entities and reduce the risk of supply failure. Additionally, this method has the potential to inform representation learning of supply chain networks for various other tasks beyond link prediction.",1
"Multimodal neuroimage can provide complementary information about the dementia, but small size of complete multimodal data limits the ability in representation learning. Moreover, the data distribution inconsistency from different modalities may lead to ineffective fusion, which fails to sufficiently explore the intra-modal and inter-modal interactions and compromises the disease diagnosis performance. To solve these problems, we proposed a novel multimodal representation learning and adversarial hypergraph fusion (MRL-AHF) framework for Alzheimer's disease diagnosis using complete trimodal images. First, adversarial strategy and pre-trained model are incorporated into the MRL to extract latent representations from multimodal data. Then two hypergraphs are constructed from the latent representations and the adversarial network based on graph convolution is employed to narrow the distribution difference of hyperedge features. Finally, the hyperedge-invariant features are fused for disease prediction by hyperedge convolution. Experiments on the public Alzheimer's Disease Neuroimaging Initiative(ADNI) database demonstrate that our model achieves superior performance on Alzheimer's disease detection compared with other related models and provides a possible way to understand the underlying mechanisms of disorder's progression by analyzing the abnormal brain connections.",0
"Although multimodal neuroimage can offer additional insights into dementia, the limited size of complete multimodal data restricts its ability to learn representations. Inconsistencies in data distribution across modalities can also hinder effective fusion, resulting in suboptimal exploration of intra-modal and inter-modal interactions and compromising disease diagnosis accuracy. To address these challenges, we propose a novel framework called Multimodal Representation Learning and Adversarial Hypergraph Fusion (MRL-AHF) for Alzheimer's disease diagnosis, which uses complete trimodal images. Our approach involves incorporating adversarial strategy and pre-trained models into MRL to extract latent representations from multimodal data. We then construct two hypergraphs from these representations and the adversarial network and employ graph convolution to reduce distribution differences in hyperedge features. Finally, we fuse the hyperedge-invariant features using hyperedge convolution for disease prediction. Our experiments on the public Alzheimer's Disease Neuroimaging Initiative database demonstrate that our model outperforms other related models in Alzheimer's disease detection and provides a possible avenue for understanding the underlying mechanisms of the disorder's progression by analyzing abnormal brain connections.",1
"The emergence of novel pathogens and zoonotic diseases like the SARS-CoV-2 have underlined the need for developing novel diagnosis and intervention pipelines that can learn rapidly from small amounts of labeled data. Combined with technological advances in next-generation sequencing, metagenome-based diagnostic tools hold much promise to revolutionize rapid point-of-care diagnosis. However, there are significant challenges in developing such an approach, the chief among which is to learn self-supervised representations that can help detect novel pathogen signatures with very low amounts of labeled data. This is particularly a difficult task given that closely related pathogens can share more than 90% of their genome structure. In this work, we address these challenges by proposing MG-Net, a self-supervised representation learning framework that leverages multi-modal context using pseudo-imaging data derived from clinical metagenome sequences. We show that the proposed framework can learn robust representations from unlabeled data that can be used for downstream tasks such as metagenome sequence classification with limited access to labeled data. Extensive experiments show that the learned features outperform current baseline metagenome representations, given only 1000 samples per class.",0
"The emergence of new diseases that can be transmitted from animals to humans, such as SARS-CoV-2, has highlighted the need for new methods to diagnose and treat them quickly, using only small amounts of labeled data. Advances in next-generation sequencing have led to the development of metagenome-based diagnostic tools that have the potential to revolutionize point-of-care diagnosis. However, there are significant challenges to developing such tools, particularly the need to learn self-supervised representations that can identify unique pathogen signatures with very little labeled data. This is particularly difficult as closely related pathogens can have similar genetic structures. In this study, we propose the use of MG-Net, a self-supervised representation learning framework which uses pseudo-imaging data derived from clinical metagenome sequences to overcome these challenges. Our results demonstrate that the proposed framework can learn robust representations from unlabeled data that outperform current baseline metagenome representations, even with only 1000 samples per class.",1
"Learning effective representations in image-based environments is crucial for sample efficient Reinforcement Learning (RL). Unfortunately, in RL, representation learning is confounded with the exploratory experience of the agent -- learning a useful representation requires diverse data, while effective exploration is only possible with coherent representations. Furthermore, we would like to learn representations that not only generalize across tasks but also accelerate downstream exploration for efficient task-specific training. To address these challenges we propose Proto-RL, a self-supervised framework that ties representation learning with exploration through prototypical representations. These prototypes simultaneously serve as a summarization of the exploratory experience of an agent as well as a basis for representing observations. We pre-train these task-agnostic representations and prototypes on environments without downstream task information. This enables state-of-the-art downstream policy learning on a set of difficult continuous control tasks.",0
"It is crucial for Reinforcement Learning (RL) to learn effective representations in image-based environments, in order to be sample efficient. However, representation learning is hindered by the exploratory experience of the agent in RL. Effective exploration requires coherent representations, while learning useful representations demands diverse data. Additionally, the ideal representations should not only generalize across tasks but also speed up downstream exploration for efficient task-specific training. To overcome these challenges, we introduce Proto-RL, a self-supervised framework that connects representation learning with exploration via prototypical representations. These prototypes serve both as a summary of the agent's exploratory experience and as a basis for representing observations. We pre-train these task-agnostic representations and prototypes on environments without downstream task information, leading to state-of-the-art downstream policy learning on difficult continuous control tasks.",1
"Effectively and efficiently deploying graph neural networks (GNNs) at scale remains one of the most challenging aspects of graph representation learning. Many powerful solutions have only ever been validated on comparatively small datasets, often with counter-intuitive outcomes -- a barrier which has been broken by the Open Graph Benchmark Large-Scale Challenge (OGB-LSC). We entered the OGB-LSC with two large-scale GNNs: a deep transductive node classifier powered by bootstrapping, and a very deep (up to 50-layer) inductive graph regressor regularised by denoising objectives. Our models achieved an award-level (top-3) performance on both the MAG240M and PCQM4M benchmarks. In doing so, we demonstrate evidence of scalable self-supervised graph representation learning, and utility of very deep GNNs -- both very important open issues. Our code is publicly available at: https://github.com/deepmind/deepmind-research/tree/master/ogb_lsc.",0
"One of the biggest challenges in graph representation learning is the effective and efficient deployment of graph neural networks (GNNs) at scale. While many powerful solutions have been developed, they have only been tested on small datasets, leading to unexpected outcomes. The Open Graph Benchmark Large-Scale Challenge (OGB-LSC) has broken this barrier. We participated in the OGB-LSC with two large-scale GNNs: a deep transductive node classifier and a very deep inductive graph regressor. Our models achieved top-3 performance on both the MAG240M and PCQM4M benchmarks, demonstrating the scalability of self-supervised graph representation learning and the importance of very deep GNNs. Our code is available at: https://github.com/deepmind/deepmind-research/tree/master/ogb_lsc.",1
"Despite recent success, most contrastive self-supervised learning methods are domain-specific, relying heavily on data augmentation techniques that require knowledge about a particular domain, such as image cropping and rotation. To overcome such limitation, we propose a novel domain-agnostic approach to contrastive learning, named DACL, that is applicable to domains where invariances, and thus, data augmentation techniques, are not readily available. Key to our approach is the use of Mixup noise to create similar and dissimilar examples by mixing data samples differently either at the input or hidden-state levels. To demonstrate the effectiveness of DACL, we conduct experiments across various domains such as tabular data, images, and graphs. Our results show that DACL not only outperforms other domain-agnostic noising methods, such as Gaussian-noise, but also combines well with domain-specific methods, such as SimCLR, to improve self-supervised visual representation learning. Finally, we theoretically analyze our method and show advantages over the Gaussian-noise based contrastive learning approach.",0
"Despite recent progress, most self-supervised learning methods that contrast different domains rely heavily on domain-specific data augmentation techniques such as image rotation or cropping. This limits their applicability to other domains where these techniques are not available. To overcome this limitation, we propose DACL, a novel domain-agnostic approach that utilizes Mixup noise to create similar and dissimilar examples by mixing data samples differently either at the input or hidden-state levels. We demonstrate the effectiveness of DACL across various domains, including images, tabular data, and graphs. Our results show that DACL outperforms other domain-agnostic noising methods like Gaussian-noise and also enhances domain-specific methods like SimCLR for self-supervised visual representation learning. Finally, we provide a theoretical analysis of our approach, highlighting its advantages over Gaussian-noise based contrastive learning techniques.",1
"Fusing satellite imagery acquired with different sensors has been a long-standing challenge of Earth observation, particularly across different modalities such as optical and Synthetic Aperture Radar (SAR) images. Here, we explore the joint analysis of imagery from different sensors in the light of representation learning: we propose to learn a joint, sensor-invariant embedding (feature representation) within a deep neural network. Our application problem is the monitoring of lake ice on Alpine lakes. To reach the temporal resolution requirement of the Swiss Global Climate Observing System (GCOS) office, we combine three image sources: Sentinel-1 SAR (S1-SAR), Terra MODIS and Suomi-NPP VIIRS. The large gaps between the optical and SAR domains and between the sensor resolutions make this a challenging instance of the sensor fusion problem. Our approach can be classified as a feature-level fusion that is learnt in a data-driven manner. The proposed network architecture has separate encoding branches for each image sensor, which feed into a single latent embedding. I.e., a common feature representation shared by all inputs, such that subsequent processing steps deliver comparable output irrespective of which sort of input image was used. By fusing satellite data, we map lake ice at a temporal resolution of <1.5 days. The network produces spatially explicit lake ice maps with pixel-wise accuracies >91.3% (respectively, mIoU scores >60.7%) and generalises well across different lakes and winters. Moreover, it sets a new state-of-the-art for determining the important ice-on and ice-off dates for the target lakes, in many cases meeting the GCOS requirement.",0
"The fusion of satellite imagery from different sensors has presented a long-standing challenge in Earth observation, particularly when dealing with distinct modalities such as optical and Synthetic Aperture Radar (SAR) images. In this study, we examine the joint analysis of sensor data in relation to representation learning, proposing a deep neural network that can learn a joint, sensor-invariant embedding or feature representation. Our objective is to monitor lake ice on Alpine lakes, which entails combining three image sources to meet the temporal resolution requirement of the Swiss Global Climate Observing System (GCOS) office. However, the gaps between the optical and SAR domains and the differences in sensor resolutions make this a particularly challenging instance of the sensor fusion problem. Our approach focuses on feature-level fusion, which is learned in a data-driven manner using a network architecture that has separate encoding branches for each image sensor, feeding into a single latent embedding. The resulting feature representation is shared by all inputs, allowing for comparable output regardless of the input image. By fusing satellite data, we are able to map lake ice at a temporal resolution of less than 1.5 days. Our network generates spatially explicit lake ice maps with pixel-wise accuracies exceeding 91.3% (or mIoU scores exceeding 60.7%), and it generalizes well across different lakes and winters. Furthermore, it establishes a new state-of-the-art for determining the important ice-on and ice-off dates for the target lakes, often meeting the GCOS requirement.",1
"Neural networks leverage robust internal representations in order to generalise. Learning them is difficult, and often requires a large training set that covers the data distribution densely. We study a common setting where our task is not purely opaque. Indeed, very often we may have access to information about the underlying system (e.g. that observations must obey certain laws of physics) that any ""tabula rasa"" neural network would need to re-learn from scratch, penalising data efficiency. We incorporate this information into a pre-trained reasoning module, and investigate its role in shaping the discovered representations in diverse self-supervised learning settings from pixels. Our approach paves the way for a new class of data-efficient representation learning.",0
"To generalize, neural networks use strong internal representations, but they are difficult to learn and require a large training set covering the data distribution. In certain scenarios, we may have knowledge about the underlying system, such as physical laws for observations, that a blank neural network would need to learn from the start, reducing data efficiency. Our research involves using a pre-trained reasoning module that incorporates this information, and examining its impact on the representations discovered in various self-supervised learning situations from pixels. This approach opens up possibilities for a more efficient type of representation learning.",1
"Treatment effect estimation, which refers to the estimation of causal effects and aims to measure the strength of the causal relationship, is of great importance in many fields but is a challenging problem in practice. As present, data-driven causal effect estimation faces two main challenges, i.e., selection bias and the missing of counterfactual. To address these two issues, most of the existing approaches tend to reduce the selection bias by learning a balanced representation, and then to estimate the counterfactual through the representation. However, they heavily rely on the finely hand-crafted metric functions when learning balanced representations, which generally doesn't work well for the situations where the original distribution is complicated. In this paper, we propose a CETransformer model for casual effect estimation via transformer based representation learning. To learn the representation of covariates(features) robustly, a self-supervised transformer is proposed, by which the correlation between covariates can be well exploited through self-attention mechanism. In addition, an adversarial network is adopted to balance the distribution of the treated and control groups in the representation space. Experimental results on three real-world datasets demonstrate the advantages of the proposed CETransformer, compared with the state-of-the-art treatment effect estimation methods.",0
"Estimating the effects of treatments, which involves determining causal relationships and measuring their strength, is a crucial task in various fields, but can be difficult in practice. Currently, data-driven approaches to estimating treatment effects face two primary challenges: selection bias and the lack of counterfactual data. To address these challenges, most existing methods attempt to reduce bias by learning a balanced representation and then estimate the counterfactual from this representation. However, these methods rely on metric functions that are often ineffective when the original distribution is complex. To overcome this issue, we propose the CETransformer model for treatment effect estimation using transformer-based representation learning. We introduce a self-supervised transformer to robustly learn covariate representations, leveraging the correlation between covariates through a self-attention mechanism. We also use an adversarial network to balance the treated and control groups in the representation space. Our experiments on three real-world datasets demonstrate the superiority of CETransformer over existing treatment effect estimation methods.",1
"By considering the spatial correspondence, dense self-supervised representation learning has achieved superior performance on various dense prediction tasks. However, the pixel-level correspondence tends to be noisy because of many similar misleading pixels, e.g., backgrounds. To address this issue, in this paper, we propose to explore \textbf{set} \textbf{sim}ilarity (SetSim) for dense self-supervised representation learning. We generalize pixel-wise similarity learning to set-wise one to improve the robustness because sets contain more semantic and structure information. Specifically, by resorting to attentional features of views, we establish corresponding sets, thus filtering out noisy backgrounds that may cause incorrect correspondences. Meanwhile, these attentional features can keep the coherence of the same image across different views to alleviate semantic inconsistency. We further search the cross-view nearest neighbours of sets and employ the structured neighbourhood information to enhance the robustness. Empirical evaluations demonstrate that SetSim is superior to state-of-the-art methods on object detection, keypoint detection, instance segmentation, and semantic segmentation.",0
"Superior performance on various dense prediction tasks has been achieved by dense self-supervised representation learning by considering the spatial correspondence. However, the pixel-level correspondence may be noisy due to misleading pixels such as backgrounds. To tackle this issue, we propose exploring SetSim (set similarity) for dense self-supervised representation learning in this paper. We extend pixel-wise similarity learning to set-wise similarity learning to enhance the robustness since sets contain more semantic and structural information. By utilizing attentional features of views, we establish corresponding sets, filter out noisy backgrounds that may cause incorrect correspondences, and maintain coherence of the same image across different views to alleviate semantic inconsistency. Additionally, we search the cross-view nearest neighbours of sets and utilize the structured neighbourhood information to further enhance robustness. Empirical evaluations show that SetSim outperforms state-of-the-art methods in object detection, keypoint detection, instance segmentation, and semantic segmentation.",1
"We consider graph representation learning in a self-supervised manner. Graph neural networks (GNNs) use neighborhood aggregation as a core component that results in feature smoothing among nodes in proximity. While successful in various prediction tasks, such a paradigm falls short of capturing nodes' similarities over a long distance, which proves to be important for high-quality learning. To tackle this problem, we strengthen the graph with two additional graph views, in which nodes are directly linked to those with the most similar features or local structures. Not restricted by connectivity in the original graph, the generated views allow the model to enhance its expressive power with new and complementary perspectives from which to look at the relationship between nodes. Following a contrastive learning approach, we propose a method that aims to maximize the agreement between representations across generated views and the original graph. We also propose a channel-level contrast approach that greatly reduces computation cost, compared to the commonly used node level contrast, which requires computation cost quadratic in the number of nodes. Extensive experiments on seven assortative graphs and four disassortative graphs demonstrate the effectiveness of our approach.",0
"Our focus is on self-supervised graph representation learning. Although Graph Neural Networks (GNNs) are successful in various prediction tasks, they fall short of capturing nodes' similarities over long distances, which is crucial for high-quality learning. To overcome this limitation, we introduce two additional graph views that link nodes based on their similar features and local structures, thereby enhancing the model's expressive power. Using a contrastive learning approach, we propose a method that maximizes agreement between representations across generated views and the original graph. We also suggest a channel-level contrast approach to reduce computation cost compared to node-level contrast. Our experiments on assortative and disassortative graphs demonstrate the effectiveness of our approach.",1
"Crowd movement guidance has been a fascinating problem in various fields, such as easing traffic congestion in unusual events and evacuating people from an emergency-affected area. To grab the reins of crowds, there has been considerable demand for a decision support system that can answer a typical question: ``what will be the outcomes of each of the possible options in the current situation. In this paper, we consider the problem of estimating the effects of crowd movement guidance from past data. To cope with limited amount of available data biased by past decision-makers, we leverage two recent techniques in deep representation learning for spatial data analysis and causal inference. We use a spatial convolutional operator to extract effective spatial features of crowds from a small amount of data and use balanced representation learning based on the integral probability metrics to mitigate the selection bias and missing counterfactual outcomes. To evaluate the performance on estimating the treatment effects of possible guidance, we use a multi-agent simulator to generate realistic data on evacuation scenarios in a crowded theater, since there are no available datasets recording outcomes of all possible crowd movement guidance. The results of three experiments demonstrate that our proposed method reduces the estimation error by at most 56% from state-of-the-art methods.",0
"Guiding crowd movement is a complex issue that affects various fields, including managing traffic during unusual events and evacuating people during emergencies. Decision-makers require a support system that can predict the outcomes of different options in a given situation. This study aims to estimate the effects of crowd movement guidance from past data using deep representation learning techniques for spatial data analysis and causal inference. To address limited data availability and selection bias, we employ a spatial convolutional operator and balanced representation learning based on integral probability metrics. We evaluate our method's performance using a multi-agent simulator to generate data on evacuation scenarios in a crowded theater. Our approach reduces estimation error by up to 56% compared to state-of-the-art methods.",1
"In order to train robust deep learning models, large amounts of labelled data is required. However, in the absence of such large repositories of labelled data, unlabeled data can be exploited for the same. Semi-Supervised learning aims to utilize such unlabeled data for training classification models. Recent progress of self-training based approaches have shown promise in this area, which leads to this study where we utilize an ensemble approach for the same. A by-product of any semi-supervised approach may be loss of calibration of the trained model especially in scenarios where unlabeled data may contain out-of-distribution samples, which leads to this investigation on how to adapt to such effects. Our proposed algorithm carefully avoids common pitfalls in utilizing unlabeled data and leads to a more accurate and calibrated supervised model compared to vanilla self-training based student-teacher algorithms. We perform several experiments on the popular STL-10 database followed by an extensive analysis of our approach and study its effects on model accuracy and calibration.",0
"To develop strong deep learning models, a vast amount of labeled data is required. However, when such a large pool of labeled data is not available, it is possible to utilize unlabeled data for the same purpose. Semi-supervised learning focuses on using this unlabeled data to train classification models. Recent advances in self-training-based approaches have shown potential in this area, prompting us to adopt an ensemble approach in our study. A potential side effect of a semi-supervised approach is the loss of calibration in the trained model, particularly when the unlabeled data includes out-of-distribution samples. Therefore, we investigate ways to adapt to such effects. Our proposed algorithm avoids typical issues associated with utilizing unlabeled data, resulting in a more accurate and calibrated supervised model compared to vanilla self-training-based student-teacher algorithms. We conduct multiple experiments on the widely used STL-10 database, followed by an extensive analysis of our approach, and examine its impact on model accuracy and calibration.",1
"We address representation learning for large-scale instance-level image retrieval. Apart from backbone, training pipelines and loss functions, popular approaches have focused on different spatial pooling and attention mechanisms, which are at the core of learning a powerful global image representation. There are different forms of attention according to the interaction of elements of the feature tensor (local and global) and the dimensions where it is applied (spatial and channel). Unfortunately, each study addresses only one or two forms of attention and applies it to different problems like classification, detection or retrieval.   We present global-local attention module (GLAM), which is attached at the end of a backbone network and incorporates all four forms of attention: local and global, spatial and channel. We obtain a new feature tensor and, by spatial pooling, we learn a powerful embedding for image retrieval. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.",0
"Our focus is on representation learning for instance-level image retrieval on a large scale. While popular approaches have emphasized the backbone, training pipelines, and loss functions, attention mechanisms and spatial pooling play a crucial role in developing a robust global image representation. Attention can take various forms depending on the feature tensor elements' interaction (local and global) and the dimensions where it applies (spatial and channel). However, existing studies have only examined one or two attention forms and used them for different tasks such as classification, detection, or retrieval. To address this limitation, we introduce the Global-Local Attention Module (GLAM). The module is added at the end of a backbone network and incorporates all four attention forms, including local and global, spatial and channel. We obtain a new feature tensor, and using spatial pooling, we generate a potent embedding for image retrieval. Our research focuses on global descriptors, and we provide empirical evidence of the interaction between all attention forms, leading to superior performance on standard benchmarks.",1
"Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant relationship with the target. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features and build an invariant predictor. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. We propose invariant Causal Representation Learning (iCaRL), an approach that enables out-of-distribution (OOD) generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: the prior over the data representation (i.e., a set of latent variables encoding the data) given the target and the environment belongs to general exponential family distributions. Based on this, we show that it is possible to identify the data representation up to simple transformations. We also prove that all direct causes of the target can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach outperforms a variety of baseline methods. Finally, in the discussion, we further explore the aforementioned assumption and propose a more general hypothesis, called the Agnostic Hypothesis: there exist a set of hidden causal factors affecting both inputs and outcomes. The Agnostic Hypothesis can provide a unifying view of machine learning. More importantly, it can inspire a new direction to explore a general theory for identifying hidden causal factors, which is key to enabling the OOD generalization guarantees.",0
"Machine learning systems often struggle to adapt to new environments due to spurious correlations. Past approaches have attempted to find a data representation that remains invariant to the target, using a diverse training set to minimize the effect of spurious features and build an invariant predictor. However, these methods only provide generalization guarantees for linear models. Our proposed approach, iCaRL, enables out-of-distribution (OOD) generalization in nonlinear settings by assuming that the prior over the data representation belongs to general exponential family distributions. This allows us to identify the data representation up to simple transformations and discover all direct causes of the target, providing generalization guarantees for nonlinear settings. Our experiments demonstrate that iCaRL outperforms baseline methods. Furthermore, we propose the Agnostic Hypothesis, which suggests that there exist hidden causal factors affecting both inputs and outcomes, providing a new direction for identifying these factors and enabling OOD generalization guarantees.",1
"In this paper we present a novel loss function, called class-agnostic segmentation (CAS) loss. With CAS loss the class descriptors are learned during training of the network. We don't require to define the label of a class a-priori, rather the CAS loss clusters regions with similar appearance together in a weakly-supervised manner. Furthermore, we show that the CAS loss function is sparse, bounded, and robust to class-imbalance. We first apply our CAS loss function with fully-convolutional ResNet101 and DeepLab-v3 architectures to the binary segmentation problem of salient object detection. We investigate the performance against the state-of-the-art methods in two settings of low and high-fidelity training data on seven salient object detection datasets. For low-fidelity training data (incorrect class label) class-agnostic segmentation loss outperforms the state-of-the-art methods on salient object detection datasets by staggering margins of around 50%. For high-fidelity training data (correct class labels) class-agnostic segmentation models perform as good as the state-of-the-art approaches while beating the state-of-the-art methods on most datasets. In order to show the utility of the loss function across different domains we then also test on general segmentation dataset, where class-agnostic segmentation loss outperforms competing losses by huge margins.",0
"This paper introduces a new loss function, known as the class-agnostic segmentation (CAS) loss, which enables the learning of class descriptors during network training without the need for defining class labels beforehand. The CAS loss function groups regions with similar appearances together in a weakly-supervised manner, and is demonstrated to be sparse, bounded, and robust to class-imbalance. The performance of the CAS loss function is evaluated on the binary segmentation problem of salient object detection using fully-convolutional ResNet101 and DeepLab-v3 architectures. The study compares the performance of the CAS loss function against state-of-the-art methods on seven salient object detection datasets under low and high-fidelity training data settings. Results show that the class-agnostic segmentation loss outperforms existing methods by around 50% on salient object detection datasets with low-fidelity training data, and performs as well as the state-of-the-art approaches on most datasets with high-fidelity training data. The utility of the loss function across different domains is also demonstrated by testing it on a general segmentation dataset, where it outperforms competing losses by significant margins.",1
"Reliable detection of anomalies is crucial when deploying machine learning models in practice, but remains challenging due to the lack of labeled data. To tackle this challenge, contrastive learning approaches are becoming increasingly popular, given the impressive results they have achieved in self-supervised representation learning settings. However, while most existing contrastive anomaly detection and segmentation approaches have been applied to images, none of them can use the contrastive losses directly for both anomaly detection and segmentation. In this paper, we close this gap by making use of the Contrastive Predictive Coding model (arXiv:1807.03748). We show that its patch-wise contrastive loss can directly be interpreted as an anomaly score, and how this allows for the creation of anomaly segmentation masks. The resulting model achieves promising results for both anomaly detection and segmentation on the challenging MVTec-AD dataset.",0
"Detecting anomalies reliably is of utmost importance in the implementation of machine learning models in practical scenarios. However, the scarcity of labeled data makes this task difficult. To address this issue, contrastive learning approaches are gaining popularity as they have delivered impressive outcomes in self-supervised representation learning environments. Despite the success of existing contrastive anomaly detection and segmentation methods in images, none of them can use contrastive losses directly for both detection and segmentation. In this study, we bridge this gap by utilizing the Contrastive Predictive Coding model (arXiv:1807.03748). We demonstrate how the model's patch-wise contrastive loss can be interpreted as an anomaly score, enabling the creation of anomaly segmentation masks. The resulting model shows promising results for both anomaly detection and segmentation on the challenging MVTec-AD dataset.",1
"The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.",0
"Disentanglement approaches have mainly focused on identifying independent factors of variation in data, but this does not always hold true for the causal variables in real-world observations. To address this gap, we conducted a large-scale empirical study that included 4260 models to analyze how prominent disentanglement approaches perform on correlated data. Our findings indicate that systematically induced correlations in the dataset are learned and reflected in the latent representations. This has significant implications for downstream applications of disentanglement, such as fairness. To address this issue, we propose two solutions: weak supervision during training or post-hoc correction of a pre-trained model with a small number of labels.",1
"We focus on the task of future frame prediction in video governed by underlying physical dynamics. We work with models which are object-centric, i.e., explicitly work with object representations, and propagate a loss in the latent space. Specifically, our research builds on recent work by Kipf et al. \cite{kipf&al20}, which predicts the next state via contrastive learning of object interactions in a latent space using a Graph Neural Network. We argue that injecting explicit inductive bias in the model, in form of general physical laws, can help not only make the model more interpretable, but also improve the overall prediction of model. As a natural by-product, our model can learn feature maps which closely resemble actual object positions in the image, without having any explicit supervision about the object positions at the training time. In comparison with earlier works \cite{jaques&al20}, which assume a complete knowledge of the dynamics governing the motion in the form of a physics engine, we rely only on the knowledge of general physical laws, such as, world consists of objects, which have position and velocity. We propose an additional decoder based loss in the pixel space, imposed in a curriculum manner, to further refine the latent space predictions. Experiments in multiple different settings demonstrate that while Kipf et al. model is effective at capturing object interactions, our model can be significantly more effective at localising objects, resulting in improved performance in 3 out of 4 domains that we experiment with. Additionally, our model can learn highly intrepretable feature maps, resembling actual object positions.",0
"Our research focuses on predicting future frames in videos based on underlying physical dynamics. We employ models that use object-centric representations and propagate loss in the latent space. Our work builds upon Kipf et al.'s recent study, which predicts the next state through contrastive learning of object interactions in a latent space using a Graph Neural Network. We propose that incorporating explicit inductive bias in the model, in the form of general physical laws, can enhance interpretability and improve the model's overall prediction. Furthermore, our model can learn feature maps that resemble object positions in the image, even without explicit supervision during training. Unlike earlier works that require knowledge of the dynamics governing motion in the form of a physics engine, we rely only on knowledge of general physical laws, such as object positions and velocities. We introduce an additional decoder-based loss in the pixel space, applied in a curriculum manner, to further refine the latent space predictions. Our experiments in multiple domains show that our model can more effectively localize objects, resulting in better performance in three out of four domains compared to Kipf et al.'s model. Furthermore, our model can learn highly interpretable feature maps that resemble actual object positions.",1
"The outstanding performance of deep learning in various fields has been a fundamental query, which can be potentially examined using information theory that interprets the learning process as the transmission and compression of information. Information plane analyses of the mutual information between the input-hidden-output layers demonstrated two distinct learning phases of fitting and compression. It is debatable if the compression phase is necessary to generalize the input-output relations extracted from training data. In this study, we investigated this through experiments with various species of autoencoders and evaluated their information processing phase with an accurate kernel-based estimator of mutual information. Given sufficient training data, vanilla autoencoders demonstrated the compression phase, which was amplified after imposing sparsity regularization for hidden activities. However, we found that the compression phase is not universally observed in different species of autoencoders, including variational autoencoders, that have special constraints on network weights or manifold of hidden space. These types of autoencoders exhibited perfect generalization ability for test data without requiring the compression phase. Thus, we conclude that the compression phase is not necessary for generalization in representation learning.",0
"The exceptional performance of deep learning in various fields has raised the question of whether the learning process can be analyzed through information theory, which views it as the transmission and compression of information. By analyzing the mutual information between the input-hidden-output layers, information plane analyses showed two distinct learning phases of fitting and compression. The necessity of the compression phase to generalize the input-output relations from the training data is a topic of debate. In this study, we conducted experiments with different types of autoencoders and assessed their information processing phase using a reliable kernel-based estimator of mutual information. Vanilla autoencoders exhibited the compression phase when given sufficient training data, which was enhanced by enforcing sparsity regularization for hidden activities. However, we found that the compression phase was not observed in all types of autoencoders, including variational autoencoders that have specific constraints on network weights or the manifold of hidden space. Despite this, these types of autoencoders possessed remarkable generalization ability for test data without necessitating the compression phase. As a result, we conclude that the compression phase is not a prerequisite for generalization in representation learning.",1
"Graph representation learning plays a vital role in processing graph-structured data. However, prior arts on graph representation learning heavily rely on labeling information. To overcome this problem, inspired by the recent success of graph contrastive learning and Siamese networks in visual representation learning, we propose a novel self-supervised approach in this paper to learn node representations by enhancing Siamese self-distillation with multi-scale contrastive learning. Specifically, we first generate two augmented views from the input graph based on local and global perspectives. Then, we employ two objectives called cross-view and cross-network contrastiveness to maximize the agreement between node representations across different views and networks. To demonstrate the effectiveness of our approach, we perform empirical experiments on five real-world datasets. Our method not only achieves new state-of-the-art results but also surpasses some semi-supervised counterparts by large margins. Code is made available at https://github.com/GRAND-Lab/MERIT",0
"The processing of graph-structured data heavily relies on graph representation learning. However, the previous methods for graph representation learning depend on labeling information, which poses a challenge. To address this issue, we introduce a novel self-supervised approach that enhances Siamese self-distillation with multi-scale contrastive learning, inspired by the success of graph contrastive learning and Siamese networks in visual representation learning. Our approach generates two augmented views from the input graph based on global and local perspectives, followed by the use of two objectives, cross-view and cross-network contrastiveness, to improve node representations' agreement across different views and networks. We demonstrate the effectiveness of our method by conducting empirical experiments on five real-world datasets, where it achieves new state-of-the-art results and outperforms some semi-supervised counterparts by significant margins. The code for our approach is available at https://github.com/GRAND-Lab/MERIT.",1
"We introduce Neural Contextual Anomaly Detection (NCAD), a framework for anomaly detection on time series that scales seamlessly from the unsupervised to supervised setting, and is applicable to both univariate and multivariate time series. This is achieved by effectively combining recent developments in representation learning for multivariate time series, with techniques for deep anomaly detection originally developed for computer vision that we tailor to the time series setting. Our window-based approach facilitates learning the boundary between normal and anomalous classes by injecting generic synthetic anomalies into the available data. Moreover, our method can effectively take advantage of all the available information, be it as domain knowledge, or as training labels in the semi-supervised setting. We demonstrate empirically on standard benchmark datasets that our approach obtains a state-of-the-art performance in these settings.",0
"The Neural Contextual Anomaly Detection (NCAD) framework is introduced for detecting anomalies in time series. It can be applied to both univariate and multivariate time series, and seamlessly scales from unsupervised to supervised settings. The framework combines recent advances in representation learning for multivariate time series with deep anomaly detection techniques originally developed for computer vision. Our approach uses a window-based method to learn the boundary between normal and anomalous classes by injecting synthetic anomalies into the data. It can also take advantage of all available information, including domain knowledge and training labels in the semi-supervised setting. Empirical results on standard benchmark datasets show that our method achieves state-of-the-art performance in these settings.",1
"Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves absolute improvements of 2.37% and 3.84% compared to the state-of-the-art, while enjoying faster inference speed. Code and pre-trained models are available at https://github.com/salesforce/ALBEF/.",0
"Various vision-language tasks have shown promising improvements through large-scale vision and language representation learning. The majority of current methods employ a transformer-based multimodal encoder to model visual tokens and word tokens jointly. However, as these tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. This paper introduces the ALBEF method, which uses a contrastive loss to align image and text representations before fusing them through cross-modal attention. This approach enables more grounded vision and language representation learning and does not require bounding box annotations or high-resolution images. The paper also proposes momentum distillation, a self-training method, to improve learning from noisy web data. Theoretical analysis of ALBEF from a mutual information maximization perspective shows that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks, outperforming methods that are pre-trained on larger datasets. On image-text retrieval, ALBEF achieves faster inference speed and outperforms the state-of-the-art. On VQA and NLVR$^2$, ALBEF achieves absolute improvements of 2.37% and 3.84%, respectively. The code and pre-trained models are available at https://github.com/salesforce/ALBEF/.",1
"Reward function specification, which requires considerable human effort and iteration, remains a major impediment for learning behaviors through deep reinforcement learning. In contrast, providing visual demonstrations of desired behaviors often presents an easier and more natural way to teach agents. We consider a setting where an agent is provided a fixed dataset of visual demonstrations illustrating how to perform a task, and must learn to solve the task using the provided demonstrations and unsupervised environment interactions. This setting presents a number of challenges including representation learning for visual observations, sample complexity due to high dimensional spaces, and learning instability due to the lack of a fixed reward or learning signal. Towards addressing these challenges, we develop a variational model-based adversarial imitation learning (V-MAIL) algorithm. The model-based approach provides a strong signal for representation learning, enables sample efficiency, and improves the stability of adversarial training by enabling on-policy learning. Through experiments involving several vision-based locomotion and manipulation tasks, we find that V-MAIL learns successful visuomotor policies in a sample-efficient manner, has better stability compared to prior work, and also achieves higher asymptotic performance. We further find that by transferring the learned models, V-MAIL can learn new tasks from visual demonstrations without any additional environment interactions. All results including videos can be found online at \url{https://sites.google.com/view/variational-mail}.",0
"Learning behaviors through deep reinforcement learning is hindered by the requirement of significant human effort and iteration for reward function specification. In contrast, teaching agents through visual demonstrations of desired behaviors presents a more natural and easier approach. This study focuses on a scenario where an agent is given a fixed dataset of visual demonstrations to learn how to perform a task using the provided demonstrations and unsupervised environment interactions. However, this setting presents challenges such as representation learning for visual observations, sample complexity due to high dimensional spaces, and learning instability due to the lack of a fixed reward or learning signal. To address these challenges, the researchers developed the variational model-based adversarial imitation learning (V-MAIL) algorithm, which provides a strong signal for representation learning, enhances sample efficiency, and improves the stability of adversarial training by enabling on-policy learning. The experiments conducted on various vision-based locomotion and manipulation tasks showed that V-MAIL learns visuomotor policies successfully and efficiently, has better stability, and achieves higher asymptotic performance compared to prior work. Additionally, V-MAIL can learn new tasks from visual demonstrations without any additional environment interactions by transferring the learned models. All results and videos are available online at \url{https://sites.google.com/view/variational-mail}.",1
"Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiBench, a systematic and unified large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9/15 datasets. Therefore, MultiBench presents a milestone in unifying disjoint efforts in multimodal research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MultiBench, our standardized code, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.",0
"The process of learning multimodal representations involves the combination of information from different sources of data, which can be challenging but is essential for various real-world applications such as multimedia, affective computing, and healthcare. Unfortunately, there has been a lack of resources dedicated to studying generalization across domains and modalities, complexity during training and inference, and robustness to noisy and missing modalities. To address these issues and promote progress in understudied modalities and tasks, we have developed MultiBench, a comprehensive benchmark that covers 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides a standardized machine learning pipeline that simplifies data loading, experimental setup, and model evaluation. It also offers a methodology to assess generalization, time and space complexity, and modality robustness. MultiBench presents challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. In addition, we provide a standardized implementation of 20 core approaches in multimodal learning, which can improve state-of-the-art performance on 9/15 datasets. MultiBench is a significant milestone in unifying efforts in multimodal research and promoting a better understanding of multimodal models' capabilities and limitations while ensuring ease of use, accessibility, and reproducibility. The MultiBench benchmark, standardized code, and leaderboards are publicly available and will be updated regularly, with community input welcomed.",1
"In this work, we present an investigation into the use of neural feature extraction in performing scribal hand analysis of the Linear B writing system. While prior work has demonstrated the usefulness of strategies such as phylogenetic systematics in tracing Linear B's history, these approaches have relied on manually extracted features which can be very time consuming to define by hand. Instead we propose learning features using a fully unsupervised neural network that does not require any human annotation. Specifically our model assigns each glyph written by the same scribal hand a shared vector embedding to represent that author's stylistic patterns, and each glyph representing the same syllabic sign a shared vector embedding to represent the identifying shape of that character. Thus the properties of each image in our dataset are represented as the combination of a scribe embedding and a sign embedding. We train this model using both a reconstructive loss governed by a decoder that seeks to reproduce glyphs from their corresponding embeddings, and a discriminative loss which measures the model's ability to predict whether or not an embedding corresponds to a given image. Among the key contributions of this work we (1) present a new dataset of Linear B glyphs, annotated by scribal hand and sign type, (2) propose a neural model for disentangling properties of scribal hands from glyph shape, and (3) quantitatively evaluate the learned embeddings on findplace prediction and similarity to manually extracted features, showing improvements over simpler baseline methods.",0
"This paper investigates the use of neural feature extraction for analyzing the Linear B writing system's scribal hands. Previous research has shown the usefulness of methods like phylogenetic systematics for tracing Linear B's history, but these methods require manual feature extraction, which is time-consuming. Instead, we propose a fully unsupervised neural network to learn features without any human annotation. Our model assigns a shared vector embedding to each glyph written by the same scribal hand and each glyph representing the same syllabic sign to represent its identifying shape. Thus, our dataset's image properties are represented as a combination of a scribe embedding and a sign embedding. We train our model using a reconstructive loss and a discriminative loss. We also introduce a new dataset of Linear B glyphs annotated by scribal hand and sign type. Our model disentangles scribal hand properties from glyph shape, and we evaluate the learned embeddings on findplace prediction and similarity to manually extracted features, showing improvements over simpler baseline methods.",1
"A common problem with most zero and few-shot learning approaches is they suffer from bias towards seen classes resulting in sub-optimal performance. Existing efforts aim to utilize unlabeled images from unseen classes (i.e transductive zero-shot) during training to enable generalization. However, this limits their use in practical scenarios where data from target unseen classes is unavailable or infeasible to collect. In this work, we present a practical setting of inductive zero and few-shot learning, where unlabeled images from other out-of-data classes, that do not belong to seen or unseen categories, can be used to improve generalization in any-shot learning. We leverage a formulation based on product-of-experts and introduce a new AUD module that enables us to use unlabeled samples from out-of-data classes which are usually easily available and practically entail no annotation cost. In addition, we also demonstrate the applicability of our model to address a more practical and challenging, Generalized Zero-shot under a limited supervision setting, where even base seen classes do not have sufficient annotated samples.",0
"Most approaches to zero and few-shot learning suffer from bias towards seen classes, resulting in suboptimal performance. To overcome this, existing efforts have utilized unlabeled images from unseen classes during training to enable generalization, but this approach is limited in practical scenarios where data from target unseen classes is unavailable or difficult to collect. This study presents a practical solution to inductive zero and few-shot learning, where unlabeled images from out-of-data classes can be used to improve generalization. The authors leverage a formulation based on product-of-experts and introduce a new AUD module that enables the use of unlabeled samples from out-of-data classes, which are easily available and entail no annotation cost. Additionally, the model's applicability is demonstrated in a Generalized Zero-shot under a limited supervision setting, where even base seen classes lack sufficient annotated samples.",1
"We study model-based offline Reinforcement Learning with general function approximation. We present an algorithm named Constrained Pessimistic Policy Optimization (CPPO) which leverages a general function class and uses a constraint to encode pessimism. Under the assumption that the ground truth model belongs to our function class, CPPO can learn with the offline data only providing partial coverage, i.e., it can learn a policy that completes against any policy that is covered by the offline data, in polynomial sample complexity with respect to the statistical complexity of the function class. We then demonstrate that this algorithmic framework can be applied to many specialized Markov Decision Processes where the additional structural assumptions can further refine the concept of partial coverage. One notable example is low-rank MDP with representation learning where the partial coverage is defined using the concept of relative condition number measured by the underlying unknown ground truth feature representation. Finally, we introduce and study the Bayesian setting in offline RL. The key benefit of Bayesian offline RL is that algorithmically, we do not need to explicitly construct pessimism or reward penalty which could be hard beyond models with linear structures. We present a posterior sampling-based incremental policy optimization algorithm (PS-PO) which proceeds by iteratively sampling a model from the posterior distribution and performing one-step incremental policy optimization inside the sampled model. Theoretically, in expectation with respect to the prior distribution, PS-PO can learn a near optimal policy under partial coverage with polynomial sample complexity.",0
"Our focus is on offline Reinforcement Learning that employs general function approximation models. Our algorithm, Constrained Pessimistic Policy Optimization (CPPO), utilizes a general function class and incorporates pessimism through a constraint. Assuming the ground truth model is part of our function class, CPPO can learn using partial coverage offline data, resulting in a policy that can compete against any policy covered by the data. The statistical complexity of the function class determines the polynomial sample complexity required for learning. We apply this algorithm to specialized Markov Decision Processes, such as low-rank MDP with representation learning, where the concept of partial coverage is refined using the relative condition number. Additionally, we explore the Bayesian setting in offline RL, introducing a posterior sampling-based incremental policy optimization algorithm (PS-PO). This algorithm enables us to learn a near-optimal policy using polynomial sample complexity, without the need for explicitly constructing pessimism or reward penalties.",1
"Domain generalization (DG) aims to help models trained on a set of source domains generalize better on unseen target domains. The performances of current DG methods largely rely on sufficient labeled data, which however are usually costly or unavailable. While unlabeled data are far more accessible, we seek to explore how unsupervised learning can help deep models generalizes across domains. Specifically, we study a novel generalization problem called unsupervised domain generalization, which aims to learn generalizable models with unlabeled data. Furthermore, we propose a Domain-Irrelevant Unsupervised Learning (DIUL) method to cope with the significant and misleading heterogeneity within unlabeled data and severe distribution shifts between source and target data. Surprisingly we observe that DIUL can not only counterbalance the scarcity of labeled data but also further strengthen the generalization ability of models when the labeled data are sufficient. As a pretraining approach, DIUL shows superior to ImageNet pretraining protocol even when the available data are unlabeled and of a greatly smaller amount compared to ImageNet. Extensive experiments clearly demonstrate the effectiveness of our method compared with state-of-the-art unsupervised learning counterparts.",0
"The objective of Domain Generalization (DG) is to enhance the performance of models trained on a specific set of domains to perform well on new and unseen target domains. Current DG methods rely heavily on labeled data, which can be costly or unavailable. In contrast, unlabeled data is more accessible, prompting us to investigate how unsupervised learning can help deep models generalize across domains. We have introduced a new generalization problem called Unsupervised Domain Generalization, which focuses on learning generalizable models using unlabeled data. To address the heterogeneity and severe distribution shifts in unlabeled data, we have proposed a Domain-Irrelevant Unsupervised Learning (DIUL) method. Surprisingly, we have found that DIUL can not only overcome the scarcity of labeled data but also improve the generalization ability of models, even when labeled data is sufficient. As a pretraining approach, DIUL outperforms the ImageNet pretraining protocol, even when unlabeled data is significantly smaller than ImageNet. Extensive experiments demonstrate the effectiveness of our method compared to state-of-the-art unsupervised learning methods.",1
"Radiomic representations can quantify properties of regions of interest in medical image data. Classically, they account for pre-defined statistics of shape, texture, and other low-level image features. Alternatively, deep learning-based representations are derived from supervised learning but require expensive annotations from experts and often suffer from overfitting and data imbalance issues. In this work, we address the challenge of learning representations of 3D medical images for an effective quantification under data imbalance. We propose a \emph{self-supervised} representation learning framework to learn high-level features of 3D volumes as a complement to existing radiomics features. Specifically, we demonstrate how to learn image representations in a self-supervised fashion using a 3D Siamese network. More importantly, we deal with data imbalance by exploiting two unsupervised strategies: a) sample re-weighting, and b) balancing the composition of training batches. When combining our learned self-supervised feature with traditional radiomics, we show significant improvement in brain tumor classification and lung cancer staging tasks covering MRI and CT imaging modalities.",0
"The quantification of properties in medical image data can be achieved through radiomic representations, which traditionally utilize pre-determined statistics pertaining to shape, texture, and other low-level image features. However, deep learning-based representations require expert annotations and often face challenges with overfitting and data imbalance. This study proposes a self-supervised representation learning framework to address the quantification challenge of 3D medical images under data imbalance. Utilizing a 3D Siamese network, high-level features are learned in a self-supervised manner, while data imbalance is tackled through unsupervised sample re-weighting and balancing of training batches. The combination of the learned self-supervised feature with traditional radiomics leads to significant improvements in brain tumor classification and lung cancer staging across MRI and CT imaging modalities.",1
"Segmenting 3D cell nuclei from microscopy image volumes is critical for biological and clinical analysis, enabling the study of cellular expression patterns and cell lineages. However, current datasets for neuronal nuclei usually contain volumes smaller than $10^{\text{-}3}\ mm^3$ with fewer than 500 instances per volume, unable to reveal the complexity in large brain regions and restrict the investigation of neuronal structures. In this paper, we have pushed the task forward to the sub-cubic millimeter scale and curated the NucMM dataset with two fully annotated volumes: one $0.1\ mm^3$ electron microscopy (EM) volume containing nearly the entire zebrafish brain with around 170,000 nuclei; and one $0.25\ mm^3$ micro-CT (uCT) volume containing part of a mouse visual cortex with about 7,000 nuclei. With two imaging modalities and significantly increased volume size and instance numbers, we discover a great diversity of neuronal nuclei in appearance and density, introducing new challenges to the field. We also perform a statistical analysis to illustrate those challenges quantitatively. To tackle the challenges, we propose a novel hybrid-representation learning model that combines the merits of foreground mask, contour map, and signed distance transform to produce high-quality 3D masks. The benchmark comparisons on the NucMM dataset show that our proposed method significantly outperforms state-of-the-art nuclei segmentation approaches. Code and data are available at https://connectomics-bazaar.github.io/proj/nucMM/index.html.",0
"The segmentation of 3D cell nuclei is crucial for biological and clinical analysis and allows for the examination of cellular expression patterns and cell lineages. However, neuronal nuclei datasets are limited, with volumes typically smaller than $10^{\text{-}3}\ mm^3$ and fewer than 500 instances per volume. As a result, the investigation of neuronal structures in large brain regions is restricted. This paper presents the NucMM dataset, which consists of two fully annotated volumes: one $0.1\ mm^3$ electron microscopy (EM) volume of the zebrafish brain with approximately 170,000 nuclei, and one $0.25\ mm^3$ micro-CT (uCT) volume of a mouse visual cortex with about 7,000 nuclei. Our dataset provides a greater diversity of neuronal nuclei in appearance and density, resulting in new challenges for the field. We propose a novel hybrid-representation learning model that combines foreground mask, contour map, and signed distance transform to produce high-quality 3D masks. Our proposed method outperforms state-of-the-art nuclei segmentation approaches. The code and data for the NucMM dataset are available at https://connectomics-bazaar.github.io/proj/nucMM/index.html.",1
"Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets and applications. Finally, we summarize existing literature and present some potential research topics for the future.",0
"The assumption of machine learning systems is that the distribution used for training and testing is the same. This means that models must be developed to generalize to new distributions. Recently, interest has grown in domain generalization (DG), which refers to the ability of a model to generalize to unseen test domains. This is challenging when dealing with one or more related but different domains. Though progress has been made in this area, this paper presents the first review of recent advances. It begins with a formal definition of domain generalization and an overview of related fields. The theories and analysis behind generalization are then explored, followed by a categorization of recent algorithms into data manipulation, representation learning, and learning strategy. Popular algorithms for each category are discussed in detail. Commonly used datasets and applications are introduced, and the existing literature is summarized. Finally, potential research topics for the future are presented.",1
"Learning data representations that are useful for various downstream tasks is a cornerstone of artificial intelligence. While existing methods are typically evaluated on downstream tasks such as classification or generative image quality, we propose to assess representations through their usefulness in downstream control tasks, such as reaching or pushing objects. By training over 10,000 reinforcement learning policies, we extensively evaluate to what extent different representation properties affect out-of-distribution (OOD) generalization. Finally, we demonstrate zero-shot transfer of these policies from simulation to the real world, without any domain randomization or fine-tuning. This paper aims to establish the first systematic characterization of the usefulness of learned representations for real-world OOD downstream tasks.",0
"The fundamental aspect of artificial intelligence is acquiring knowledge on data representations that can be applied in various downstream tasks. While current methods are commonly measured based on downstream tasks like image quality or classification, we suggest evaluating representations based on their applicability in downstream control tasks, such as object pushing or reaching. We undertake a thorough evaluation of over 10,000 reinforcement learning policies to determine how different representation features impact generalization outside the distribution. We conclude by demonstrating that these policies can be transferred from simulation to reality without any domain randomization or fine-tuning. This paper intends to establish the initial systematized characterization of the usefulness of learned representations for real-world OOD downstream tasks.",1
"Time-series generated by end-users, edge devices, and different wearables are mostly unlabelled. We propose a method to auto-generate labels of un-labelled time-series, exploiting very few representative labelled time-series. Our method is based on representation learning using Auto Encoded Compact Sequence (AECS) with a choice of best distance measure. It performs self-correction in iterations, by learning latent structure, as well as synthetically boosting representative time-series using Variational-Auto-Encoder (VAE) to improve the quality of labels. We have experimented with UCR and UCI archives, public real-world univariate, multivariate time-series taken from different application domains. Experimental results demonstrate that the proposed method is very close to the performance achieved by fully supervised classification. The proposed method not only produces close to benchmark results but outperforms the benchmark performance in some cases.",0
"Most time-series data from end-users, edge devices, and wearables are unlabeled. Our proposed approach uses a few labeled time-series to automatically generate labels for these unlabeled ones. Our method involves using Auto Encoded Compact Sequence (AECS) with the best distance measure to learn the latent structure and self-correct in iterations. We also use Variational-Auto-Encoder (VAE) to enhance the quality of labels by boosting representative time-series. We tested our method on UCR and UCI archives, which are public real-world datasets of univariate and multivariate time-series from various application domains. Our experimental results show that our approach is comparable to fully supervised classification and, in some cases, outperforms it. Therefore, our proposed method produces accurate results and can be used for labeling unlabeled time-series data.",1
"We present a new learning-based framework to recover vehicle pose in SO(3) from a single RGB image. In contrast to previous works that map from local appearance to observation angles, we explore a progressive approach by extracting meaningful Intermediate Geometrical Representations (IGRs) to estimate egocentric vehicle orientation. This approach features a deep model that transforms perceived intensities to IGRs, which are mapped to a 3D representation encoding object orientation in the camera coordinate system. Core problems are what IGRs to use and how to learn them more effectively. We answer the former question by designing IGRs based on an interpolated cuboid that derives from primitive 3D annotation readily. The latter question motivates us to incorporate geometry knowledge with a new loss function based on a projective invariant. This loss function allows unlabeled data to be used in the training stage to improve representation learning. Without additional labels, our system outperforms previous monocular RGB-based methods for joint vehicle detection and pose estimation on the KITTI benchmark, achieving performance even comparable to stereo methods. Code and pre-trained models are available at this https URL.",0
"A novel learning-based framework is presented for recovering vehicle pose in SO(3) using a single RGB image. Unlike prior research that relies on mapping from local appearance to observation angles, a progressive approach is adopted here that involves extracting meaningful Intermediate Geometrical Representations (IGRs) to estimate egocentric vehicle orientation. This method involves a deep model that transforms perceived intensities to IGRs, which are then mapped to a 3D representation encoding object orientation in the camera coordinate system. Key challenges are determining which IGRs to use and how to improve their learning efficiency. To address the former issue, IGRs based on an interpolated cuboid derived from primitive 3D annotation are designed. The latter issue is addressed by incorporating geometry knowledge into a new loss function based on a projective invariant. This loss function allows unlabeled data to be used in the training stage to improve representation learning. Without additional labels, this system outperforms previous monocular RGB-based methods for joint vehicle detection and pose estimation on the KITTI benchmark and achieves performance comparable to stereo methods. Code and pre-trained models are available at this https URL.",1
"The proliferation of remote sensing satellites has resulted in a massive amount of remote sensing images. However, due to human and material resource constraints, the vast majority of remote sensing images remain unlabeled. As a result, it cannot be applied to currently available deep learning methods. To fully utilize the remaining unlabeled images, we propose a Geographical Knowledge-driven Representation learning method for remote sensing images (GeoKR), improving network performance and reduce the demand for annotated data. The global land cover products and geographical location associated with each remote sensing image are regarded as geographical knowledge to provide supervision for representation learning and network pre-training. An efficient pre-training framework is proposed to eliminate the supervision noises caused by imaging times and resolutions difference between remote sensing images and geographical knowledge. A large scale pre-training dataset Levir-KR is proposed to support network pre-training. It contains 1,431,950 remote sensing images from Gaofen series satellites with various resolutions. Experimental results demonstrate that our proposed method outperforms ImageNet pre-training and self-supervised representation learning methods and significantly reduces the burden of data annotation on downstream tasks such as scene classification, semantic segmentation, object detection, and cloud / snow detection. It demonstrates that our proposed method can be used as a novel paradigm for pre-training neural networks. Codes will be available on https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.",0
"The increasing number of remote sensing satellites has led to a vast amount of remote sensing images, but most of them remain unlabeled due to limited resources. Therefore, these images cannot be utilized in deep learning methods. To overcome this challenge, we suggest a new approach called GeoKR, which employs geographical knowledge as supervision for representation learning and network pre-training. Global land cover products and geographical location are utilized as geographical knowledge to improve network performance and decrease the need for annotated data. We propose an efficient pre-training framework, Levir-KR, which eliminates the supervision noises caused by imaging times and resolutions difference between remote sensing images and geographical knowledge. Our experimental results demonstrate that GeoKR outperforms ImageNet pre-training and self-supervised representation learning methods and significantly reduces the burden of data annotation on downstream tasks. We believe that our approach can serve as a new model for pre-training neural networks, and the codes are accessible on https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.",1
"Dictionary learning is a key tool for representation learning, that explains the data as linear combination of few basic elements. Yet, this analysis is not amenable in the context of graph learning, as graphs usually belong to different metric spaces. We fill this gap by proposing a new online Graph Dictionary Learning approach, which uses the Gromov Wasserstein divergence for the data fitting term. In our work, graphs are encoded through their nodes' pairwise relations and modeled as convex combination of graph atoms, i.e. dictionary elements, estimated thanks to an online stochastic algorithm, which operates on a dataset of unregistered graphs with potentially different number of nodes. Our approach naturally extends to labeled graphs, and is completed by a novel upper bound that can be used as a fast approximation of Gromov Wasserstein in the embedding space. We provide numerical evidences showing the interest of our approach for unsupervised embedding of graph datasets and for online graph subspace estimation and tracking.",0
"Representation learning relies heavily on dictionary learning, which involves expressing data as a linear combination of a few basic elements. However, this method is not easily applicable to graph learning due to the fact that graphs typically belong to different metric spaces. To address this issue, we present a new approach to online Graph Dictionary Learning that utilizes the Gromov Wasserstein divergence for the data fitting term. Our method encodes graphs through pairwise relations between their nodes and models them as a convex combination of graph atoms, or dictionary elements, estimated via an online stochastic algorithm that operates on a dataset of unregistered graphs with potentially varying numbers of nodes. Our approach extends naturally to labeled graphs, and we also introduce a novel upper bound that provides a quick approximation of Gromov Wasserstein in the embedding space. Our numerical results demonstrate the effectiveness of our approach for unsupervised embedding of graph datasets and online graph subspace estimation and tracking.",1
"Many machine learning models have been built to tackle information overload issues on Massive Open Online Courses (MOOC) platforms. These models rely on learning powerful representations of MOOC entities. However, they suffer from the problem of scarce expert label data. To overcome this problem, we propose to learn pre-trained representations of MOOC entities using abundant unlabeled data from the structure of MOOCs which can directly be applied to the downstream tasks. While existing pre-training methods have been successful in NLP areas as they learn powerful textual representation, their models do not leverage the richer information about MOOC entities. This richer information includes the graph relationship between the lectures, concepts, and courses along with the domain knowledge about the complexity of a concept. We develop MOOCRep, a novel method based on Transformer language model trained with two pre-training objectives : 1) graph-based objective to capture the powerful signal of entities and relations that exist in the graph, and 2) domain-oriented objective to effectively incorporate the complexity level of concepts. Our experiments reveal that MOOCRep's embeddings outperform state-of-the-art representation learning methods on two tasks important for education community, concept pre-requisite prediction and lecture recommendation.",0
"Numerous machine learning models have been constructed to combat the issue of information overload on MOOC platforms. These models depend on acquiring potent representations of MOOC entities. Nonetheless, they face the challenge of not having enough expert label data. To surmount this predicament, we suggest acquiring pre-trained MOOC entity representations from abundant unlabeled data obtained from the MOOC structure. This approach can then be applied directly to downstream tasks. Although current pre-training techniques have been successful in the NLP field due to their ability to learn powerful textual representation, they do not utilize the richer information about MOOC entities. This information includes the graph relationship between lectures, concepts, and courses, as well as domain knowledge about concept complexity. Our novel MOOCRep method employs a Transformer language model trained with two pre-training objectives: 1) a graph-based objective to capture the powerful signal of entities and relations in the graph, and 2) a domain-oriented objective to effectively incorporate the complexity level of concepts. Our experiments indicate that MOOCRep's embeddings outperform state-of-the-art representation learning methods on two crucial education community tasks: concept pre-requisite prediction and lecture recommendation.",1
"A large labeled dataset is a key to the success of supervised deep learning, but for medical image segmentation, it is highly challenging to obtain sufficient annotated images for model training. In many scenarios, unannotated images are abundant and easy to acquire. Self-supervised learning (SSL) has shown great potentials in exploiting raw data information and representation learning. In this paper, we propose Hierarchical Self-Supervised Learning (HSSL), a new self-supervised framework that boosts medical image segmentation by making good use of unannotated data. Unlike the current literature on task-specific self-supervised pretraining followed by supervised fine-tuning, we utilize SSL to learn task-agnostic knowledge from heterogeneous data for various medical image segmentation tasks. Specifically, we first aggregate a dataset from several medical challenges, then pre-train the network in a self-supervised manner, and finally fine-tune on labeled data. We develop a new loss function by combining contrastive loss and classification loss and pretrain an encoder-decoder architecture for segmentation tasks. Our extensive experiments show that multi-domain joint pre-training benefits downstream segmentation tasks and outperforms single-domain pre-training significantly. Compared to learning from scratch, our new method yields better performance on various tasks (e.g., +0.69% to +18.60% in Dice scores with 5% of annotated data). With limited amounts of training data, our method can substantially bridge the performance gap w.r.t. denser annotations (e.g., 10% vs.~100% of annotated data).",0
"The success of supervised deep learning in medical image segmentation relies heavily on a large labeled dataset, but obtaining sufficient annotated images can be highly challenging. However, unannotated images are often abundant and easy to acquire. Self-supervised learning (SSL) has proven effective in exploiting raw data information and representation learning. This paper introduces Hierarchical Self-Supervised Learning (HSSL), a new self-supervised framework that enhances medical image segmentation by utilizing unannotated data. Unlike current literature on task-specific self-supervised pretraining followed by supervised fine-tuning, HSSL utilizes SSL to learn task-agnostic knowledge from heterogeneous data for various medical image segmentation tasks. The approach involves aggregating a dataset from multiple medical challenges, pre-training the network in a self-supervised manner, and finally fine-tuning on labeled data. HSSL introduces a new loss function that combines contrastive loss and classification loss and pretrains an encoder-decoder architecture for segmentation tasks. Extensive experiments demonstrate that multi-domain joint pre-training significantly outperforms single-domain pre-training and yields better performance on various tasks, bridging the performance gap even with limited amounts of training data.",1
"Learning disentangled and interpretable representations is an important step towards accomplishing comprehensive data representations on the manifold. In this paper, we propose a novel representation learning algorithm which combines the inference abilities of Variational Autoencoders (VAE) with the generalization capability of Generative Adversarial Networks (GAN). The proposed model, called InfoVAEGAN, consists of three networks~: Encoder, Generator and Discriminator. InfoVAEGAN aims to jointly learn discrete and continuous interpretable representations in an unsupervised manner by using two different data-free log-likelihood functions onto the variables sampled from the generator's distribution. We propose a two-stage algorithm for optimizing the inference network separately from the generator training. Moreover, we enforce the learning of interpretable representations through the maximization of the mutual information between the existing latent variables and those created through generative and inference processes.",0
"Developing comprehensible data representations on the manifold involves learning disentangled and interpretable representations. This paper introduces a new algorithm for representation learning, InfoVAEGAN, which merges the inference capabilities of Variational Autoencoders (VAE) with the generalization capabilities of Generative Adversarial Networks (GAN). The InfoVAEGAN model comprises three networks - Encoder, Generator, and Discriminator - and aims to learn interpretable representations that are both discrete and continuous in an unsupervised manner. We achieve this by utilizing two data-free log-likelihood functions on variables obtained from the generator's distribution. To optimize the inference network separately from the generator training, we propose a two-stage algorithm. Additionally, we prioritize interpretable representations by maximizing the mutual information between the current latent variables and those generated through inference and generative processes.",1
"While deep learning has revolutionized research and applications in NLP and computer vision, this has not yet been the case for behavioral modeling and behavioral health applications. This is because the domain's datasets are smaller, have heterogeneous datatypes, and typically exhibit a large degree of missingness. Therefore, off-the-shelf deep learning models require significant, often prohibitive, adaptation. Accordingly, many research applications still rely on manually coded features with boosted tree models, sometimes with task-specific features handcrafted by experts. Here, we address these challenges by providing a neural architecture framework for mobile sensing data that can learn generalizable feature representations from time series and demonstrates the feasibility of transfer learning on small data domains through finetuning. This architecture combines benefits from CNN and Trans-former architectures to (1) enable better prediction performance by learning directly from raw minute-level sensor data without the need for handcrafted features by up to 0.33 ROC AUC, and (2) use pretraining to outperform simpler neural models and boosted decision trees with data from as few a dozen participants.",0
"Although deep learning has transformed the fields of NLP and computer vision, it has not yet had the same impact on behavioral modeling and behavioral health applications due to smaller datasets with heterogeneous datatypes and a high degree of missingness. Consequently, off-the-shelf deep learning models require significant adaptation, which can be prohibitive. As a result, many research applications still rely on manual coding of features with boosted tree models that may include task-specific features created by experts. To tackle these challenges, we present a neural architecture framework for mobile sensing data that can learn generalizable feature representations from time series. Our approach demonstrates the feasibility of transfer learning on small data domains through finetuning. The proposed architecture combines the advantages of CNN and Transformer architectures to achieve better prediction performance by learning directly from raw minute-level sensor data without the need for handcrafted features by up to 0.33 ROC AUC. Additionally, we use pretraining to outperform simpler neural models and boosted decision trees with data from as few as a dozen participants.",1
"Recently, artificial neural networks have been gaining momentum in the field of gravitational wave astronomy, for example in surrogate modelling of computationally expensive waveform models for binary black hole inspiral and merger. Surrogate modelling yields fast and accurate approximations of gravitational waves and neural networks have been used in the final step of interpolating the coefficients of the surrogate model for arbitrary waveforms outside the training sample. We investigate the existence of underlying structures in the empirical interpolation coefficients using autoencoders. We demonstrate that when the coefficient space is compressed to only two dimensions, a spiral structure appears, wherein the spiral angle is linearly related to the mass ratio. Based on this finding, we design a spiral module with learnable parameters, that is used as the first layer in a neural network, which learns to map the input space to the coefficients. The spiral module is evaluated on multiple neural network architectures and consistently achieves better speed-accuracy trade-off than baseline models. A thorough experimental study is conducted and the final result is a surrogate model which can evaluate millions of input parameters in a single forward pass in under 1ms on a desktop GPU, while the mismatch between the corresponding generated waveforms and the ground-truth waveforms is better than the compared baseline methods. We anticipate the existence of analogous underlying structures and corresponding computational gains also in the case of spinning black hole binaries.",0
"In the realm of gravitational wave astronomy, artificial neural networks have gained popularity for surrogate modeling of complex waveform models, particularly for binary black hole inspiral and merger. This approach provides quick and precise approximations of gravitational waves, where neural networks are used for interpolating coefficients of the surrogate model for waveforms outside the training sample. Our study explores the presence of underlying structures in the empirical interpolation coefficients using autoencoders. We discovered that when compressed to two dimensions, the coefficients exhibit a spiral structure, with the spiral angle being linearly linked to the mass ratio. Based on this finding, we developed a spiral module with learnable parameters that serves as the first layer in a neural network, which can map the input space to coefficients effectively. Our spiral module consistently outperforms baseline models in terms of speed and accuracy trade-off across multiple neural network architectures. Through extensive experimentation, we created a surrogate model that can evaluate millions of input parameters in a single forward pass in under 1ms on a desktop GPU. The generated waveforms from our model show better accuracy than those from compared baseline methods. We expect to find similar underlying structures and computational benefits in the case of spinning black hole binaries.",1
"Video question answering (Video QA) presents a powerful testbed for human-like intelligent behaviors. The task demands new capabilities to integrate video processing, language understanding, binding abstract linguistic concepts to concrete visual artifacts, and deliberative reasoning over spacetime. Neural networks offer a promising approach to reach this potential through learning from examples rather than handcrafting features and rules. However, neural networks are predominantly feature-based - they map data to unstructured vectorial representation and thus can fall into the trap of exploiting shortcuts through surface statistics instead of true systematic reasoning seen in symbolic systems. To tackle this issue, we advocate for object-centric representation as a basis for constructing spatio-temporal structures from videos, essentially bridging the semantic gap between low-level pattern recognition and high-level symbolic algebra. To this end, we propose a new query-guided representation framework to turn a video into an evolving relational graph of objects, whose features and interactions are dynamically and conditionally inferred. The object lives are then summarized into resumes, lending naturally for deliberative relational reasoning that produces an answer to the query. The framework is evaluated on major Video QA datasets, demonstrating clear benefits of the object-centric approach to video reasoning.",0
"The Video QA task is a challenging evaluation of human-like intelligent behavior, as it requires the integration of video processing, language comprehension, and reasoning over space and time. Neural networks are a promising solution, as they learn from examples, rather than relying on handcrafted features and rules. However, they are often based on unstructured vectorial representations, which can lead to shortcuts based on surface statistics and a lack of systematic reasoning. To address this limitation, we propose a new object-centric representation framework that creates spatio-temporal structures from videos, bridging the gap between low-level pattern recognition and high-level symbolic algebra. We introduce a query-guided approach that turns a video into an evolving relational graph of objects, whose features and interactions are dynamically inferred. The object lives are then summarized into resumes, lending themselves naturally to deliberative relational reasoning that produces an answer to the query. We evaluate our framework on major Video QA datasets and demonstrate the clear benefits of the object-centric approach to video reasoning.",1
"Recent advances in deep clustering and unsupervised representation learning are based on the idea that different views of an input image (generated through data augmentation techniques) must either be closer in the representation space, or have a similar cluster assignment. Bootstrap Your Own Latent (BYOL) is one such representation learning algorithm that has achieved state-of-the-art results in self-supervised image classification on ImageNet under the linear evaluation protocol. However, the utility of the learnt features of BYOL to perform clustering is not explored. In this work, we study the clustering ability of BYOL and observe that features learnt using BYOL may not be optimal for clustering. We propose a novel consensus clustering based loss function, and train BYOL with the proposed loss in an end-to-end way that improves the clustering ability and outperforms similar clustering based methods on some popular computer vision datasets.",0
"Recent advancements in deep clustering and unsupervised representation learning are centered on the concept that various perspectives of an input image generated through data augmentation techniques must either be closer in the representation space or have comparable cluster assignments. One such algorithm that has achieved state-of-the-art results in self-supervised image classification on ImageNet under the linear evaluation protocol is Bootstrap Your Own Latent (BYOL). Nevertheless, the efficacy of the BYOL learnt features to conduct clustering remains unexplored. This study examines the clustering capability of BYOL and finds that the features learned through BYOL may not be ideal for clustering. To address this, we introduce a fresh consensus clustering-based loss function and train BYOL using this loss in an end-to-end way, resulting in improved clustering ability that outperforms similar clustering-based methods on some prevalent computer vision datasets.",1
"Most of the existing video self-supervised methods mainly leverage temporal signals of videos, ignoring that the semantics of moving objects and environmental information are all critical for video-related tasks. In this paper, we propose a novel self-supervised method for video representation learning, referred to as Video 3D Sampling (V3S). In order to sufficiently utilize the information (spatial and temporal) provided in videos, we pre-process a video from three dimensions (width, height, time). As a result, we can leverage the spatial information (the size of objects), temporal information (the direction and magnitude of motions) as our learning target. In our implementation, we combine the sampling of the three dimensions and propose the scale and projection transformations in space and time respectively. The experimental results show that, when applied to action recognition, video retrieval and action similarity labeling, our approach improves the state-of-the-arts with significant margins.",0
"The majority of current self-supervised video methods focus on using temporal signals and overlook the importance of moving object semantics and environmental information for video-related tasks. Our paper introduces a fresh self-supervised method for video representation learning, called Video 3D Sampling (V3S), which seeks to fully capitalize on the spatial and temporal information found in videos. To achieve this, we process videos in three dimensions (width, height, time) to capture both the size of objects and the direction and magnitude of movements. Our implementation combines the sampling of the three dimensions and introduces scale and projection transformations for space and time, respectively. Our experimental results demonstrate significant improvements in action recognition, video retrieval, and action similarity labeling compared to current state-of-the-art methods.",1
"Tensors, or multidimensional arrays, are data structures that can naturally represent visual data of multiple dimensions. Inherently able to efficiently capture structured, latent semantic spaces and high-order interactions, tensors have a long history of applications in a wide span of computer vision problems. With the advent of the deep learning paradigm shift in computer vision, tensors have become even more fundamental. Indeed, essential ingredients in modern deep learning architectures, such as convolutions and attention mechanisms, can readily be considered as tensor mappings. In effect, tensor methods are increasingly finding significant applications in deep learning, including the design of memory and compute efficient network architectures, improving robustness to random noise and adversarial attacks, and aiding the theoretical understanding of deep networks.   This article provides an in-depth and practical review of tensors and tensor methods in the context of representation learning and deep learning, with a particular focus on visual data analysis and computer vision applications. Concretely, besides fundamental work in tensor-based visual data analysis methods, we focus on recent developments that have brought on a gradual increase of tensor methods, especially in deep learning architectures, and their implications in computer vision applications. To further enable the newcomer to grasp such concepts quickly, we provide companion Python notebooks, covering key aspects of the paper and implementing them, step-by-step with TensorLy.",0
"Multidimensional arrays, also known as tensors, are a type of data structure that can effectively represent visual data with multiple dimensions. Tensors have a rich history of use in computer vision problems for capturing structured, latent semantic spaces and high-order interactions. In the era of deep learning, tensors have become even more essential, as key components like convolutions and attention mechanisms can be viewed as tensor mappings. As a result, tensor methods are increasingly being used in deep learning, including for designing efficient network architectures, improving robustness to noise and attacks, and enhancing theoretical understanding. This article provides a comprehensive review of tensors and their application in representation and deep learning, focusing particularly on visual data analysis and computer vision. We cover both fundamental tensor-based methods and recent developments that have led to their widespread use in deep learning architectures, with Python notebooks using TensorLy provided to aid understanding.",1
"Graph representation learning has attracted a surge of interest recently, whose target at learning discriminant embedding for each node in the graph. Most of these representation methods focus on supervised learning and heavily depend on label information. However, annotating graphs are expensive to obtain in the real world, especially in specialized domains (i.e. biology), as it needs the annotator to have the domain knowledge to label the graph. To approach this problem, self-supervised learning provides a feasible solution for graph representation learning. In this paper, we propose a Multi-Level Graph Contrastive Learning (MLGCL) framework for learning robust representation of graph data by contrasting space views of graphs. Specifically, we introduce a novel contrastive view - topological and feature space views. The original graph is first-order approximation structure and contains uncertainty or error, while the $k$NN graph generated by encoding features preserves high-order proximity. Thus $k$NN graph generated by encoding features not only provide a complementary view, but is more suitable to GNN encoder to extract discriminant representation. Furthermore, we develop a multi-level contrastive mode to preserve the local similarity and semantic similarity of graph-structured data simultaneously. Extensive experiments indicate MLGCL achieves promising results compared with the existing state-of-the-art graph representation learning methods on seven datasets.",0
"Recently, there has been a surge in interest in graph representation learning, which aims to learn discriminant embedding for each node in the graph. However, most of these methods rely heavily on label information and focus on supervised learning, which can be expensive to obtain in specialized domains like biology. To address this issue, self-supervised learning is proposed as a feasible solution for graph representation learning. Here, we introduce a Multi-Level Graph Contrastive Learning (MLGCL) framework that learns robust representations of graph data by contrasting space views of graphs. We propose a novel contrastive view that includes topological and feature space views, where the latter is more suitable for GNN encoder to extract discriminant representation. Moreover, we develop a multi-level contrastive mode that preserves both the local similarity and semantic similarity of graph-structured data simultaneously. Our extensive experiments demonstrate that MLGCL outperforms existing state-of-the-art graph representation learning methods on seven datasets.",1
"A real-world graph has a complex topological structure, which is often formed by the interaction of different latent factors. Disentanglement of these latent factors can effectively improve the robustness and expressiveness of node representation of graph. However, most existing methods lack consideration of the intrinsic differences in relations between nodes caused by factor entanglement. In this paper, we propose an Adversarial Disentangled Graph Convolutional Network (ADGCN) for disentangled graph representation learning. Specifically, a component-specific aggregation approach is proposed to achieve micro-disentanglement by inferring latent components that caused the links between nodes. On the basis of micro-disentanglement, we further propose a macro-disentanglement adversarial regularizer to improve the separability among component distributions, thus restricting the interdependence among components. Additionally, to reveal the topological graph structure, a diversity-preserving node sampling approach is proposed, by which the graph structure can be progressively refined in a way of local structure awareness. The experimental results on various real-world graph data verify that our ADGCN obtains more favorable performance over currently available alternatives.",0
"Real-world graphs are often formed by the interaction of different latent factors, resulting in a complex topological structure. To improve the robustness and expressiveness of node representation of these graphs, it is important to disentangle these latent factors. However, existing methods fail to consider the intrinsic differences in relations between nodes caused by factor entanglement. This paper proposes an Adversarial Disentangled Graph Convolutional Network (ADGCN) for disentangled graph representation learning. The ADGCN includes a component-specific aggregation approach for micro-disentanglement and a macro-disentanglement adversarial regularizer to improve the separability among component distributions. Additionally, a diversity-preserving node sampling approach is proposed to reveal the topological graph structure. Experimental results demonstrate that ADGCN outperforms currently available alternatives.",1
"This paper proposes a method for representation learning of multimodal data using contrastive losses. A traditional approach is to contrast different modalities to learn the information shared between them. However, that approach could fail to learn the complementary synergies between modalities that might be useful for downstream tasks. Another approach is to concatenate all the modalities into a tuple and then contrast positive and negative tuple correspondences. However, that approach could consider only the stronger modalities while ignoring the weaker ones. To address these issues, we propose a novel contrastive learning objective, TupleInfoNCE. It contrasts tuples based not only on positive and negative correspondences but also by composing new negative tuples using modalities describing different scenes. Training with these additional negatives encourages the learning model to examine the correspondences among modalities in the same tuple, ensuring that weak modalities are not ignored. We provide a theoretical justification based on mutual information for why this approach works, and we propose a sample optimization algorithm to generate positive and negative samples to maximize training efficacy. We find that TupleInfoNCE significantly outperforms the previous state of the arts on three different downstream tasks.",0
"The paper introduces a new method for learning representations of multimodal data using contrastive losses. While the traditional approach involves contrasting different modalities to discover shared information, this may not capture the synergies between modalities that could be beneficial for downstream tasks. Alternatively, concatenating all modalities into a tuple and contrasting positive and negative tuple correspondences may overlook weaker modalities. To address these issues, the paper proposes a novel contrastive learning objective called TupleInfoNCE, which contrasts tuples based on positive and negative correspondences and creates new negative tuples using modalities describing different scenes. This ensures that weak modalities are not ignored and improves training efficacy. The paper provides a theoretical justification based on mutual information and presents an optimization algorithm for generating positive and negative samples. The results show that TupleInfoNCE outperforms previous state-of-the-art methods on three downstream tasks.",1
"Any clustering algorithm must synchronously learn to model the clusters and allocate data to those clusters in the absence of labels. Mixture model-based methods model clusters with pre-defined statistical distributions and allocate data to those clusters based on the cluster likelihoods. They iteratively refine those distribution parameters and member assignments following the Expectation-Maximization (EM) algorithm. However, the cluster representability of such hand-designed distributions that employ a limited amount of parameters is not adequate for most real-world clustering tasks. In this paper, we realize mixture model-based clustering with a neural network where the final layer neurons, with the aid of an additional transformation, approximate cluster distribution outputs. The network parameters pose as the parameters of those distributions. The result is an elegant, much-generalized representation of clusters than a restricted mixture of hand-designed distributions. We train the network end-to-end via batch-wise EM iterations where the forward pass acts as the E-step and the backward pass acts as the M-step. In image clustering, the mixture-based EM objective can be used as the clustering objective along with existing representation learning methods. In particular, we show that when mixture-EM optimization is fused with consistency optimization, it improves the sole consistency optimization performance in clustering. Our trained networks outperform single-stage deep clustering methods that still depend on k-means, with unsupervised classification accuracy of 63.8% in STL10, 58% in CIFAR10, 25.9% in CIFAR100, and 98.9% in MNIST.",0
"To cluster data without labels, a clustering algorithm must learn to model clusters and allocate data accordingly. Mixture model-based methods use pre-defined statistical distributions to model clusters and allocate data based on cluster likelihoods. However, these hand-designed distributions with limited parameters may not be suitable for real-world clustering tasks. To address this, we propose a neural network approach where the final layer neurons approximate cluster distribution outputs with the network parameters serving as the distribution parameters. This results in a more generalized representation of clusters compared to restricted mixtures of hand-designed distributions. We train the network end-to-end using batch-wise EM iterations where the forward pass serves as the E-step and the backward pass serves as the M-step. We demonstrate the effectiveness of our approach in image clustering by combining mixture-EM optimization with consistency optimization, resulting in improved clustering performance compared to single-stage deep clustering methods that rely on k-means. Our trained networks achieve unsupervised classification accuracies of 63.8% in STL10, 58% in CIFAR10, 25.9% in CIFAR100, and 98.9% in MNIST.",1
"In general, graph representation learning methods assume that the train and test data come from the same distribution. In this work we consider an underexplored area of an otherwise rapidly developing field of graph representation learning: The task of out-of-distribution (OOD) graph classification, where train and test data have different distributions, with test data unavailable during training. Our work shows it is possible to use a causal model to learn approximately invariant representations that better extrapolate between train and test data. Finally, we conclude with synthetic and real-world dataset experiments showcasing the benefits of representations that are invariant to train/test distribution shifts.",0
"Graph representation learning methods typically assume that the train and test data originate from the same distribution. However, our study focuses on a relatively neglected aspect of graph representation learning: out-of-distribution (OOD) graph classification. This involves handling situations where the train and test data have distinct distributions and the test data is not available during training. Our research demonstrates that a causal model can be used to acquire nearly invariant representations that can more effectively extrapolate between train and test data. Finally, we provide experiments using synthetic and real-world datasets to demonstrate the advantages of invariant representations in dealing with shifts in train/test distributions.",1
"Online tracking of multiple objects in videos requires strong capacity of modeling and matching object appearances. Previous methods for learning appearance embedding mostly rely on instance-level matching without considering the temporal continuity provided by videos. We design a new instance-to-track matching objective to learn appearance embedding that compares a candidate detection to the embedding of the tracks persisted in the tracker. It enables us to learn not only from videos labeled with complete tracks, but also unlabeled or partially labeled videos. We implement this learning objective in a unified form following the spirit of constrastive loss. Experiments on multiple object tracking datasets demonstrate that our method can effectively learning discriminative appearance embeddings in a semi-supervised fashion and outperform state of the art methods on representative benchmarks.",0
"Having the ability to model and match object appearances is crucial for online tracking of multiple objects in videos. However, previous methods that learn appearance embedding primarily rely on instance-level matching and do not consider the temporal continuity provided by videos. To address this issue, we have devised a new instance-to-track matching objective that compares a candidate detection to the embedding of the tracks stored in the tracker. This approach enables us to learn from videos that are either labeled with complete tracks or unlabeled and partially labeled. We have implemented this learning objective in a unified form that follows the spirit of a constrastive loss. Our experiments on multiple object tracking datasets demonstrate that our method can effectively learn discriminative appearance embeddings in a semi-supervised manner, outperforming state-of-the-art methods on representative benchmarks.",1
"Person re-identification (re-ID) under various occlusions has been a long-standing challenge as person images with different types of occlusions often suffer from misalignment in image matching and ranking. Most existing methods tackle this challenge by aligning spatial features of body parts according to external semantic cues or feature similarities but this alignment approach is complicated and sensitive to noises. We design DRL-Net, a disentangled representation learning network that handles occluded re-ID without requiring strict person image alignment or any additional supervision. Leveraging transformer architectures, DRL-Net achieves alignment-free re-ID via global reasoning of local features of occluded person images. It measures image similarity by automatically disentangling the representation of undefined semantic components, e.g., human body parts or obstacles, under the guidance of semantic preference object queries in the transformer. In addition, we design a decorrelation constraint in the transformer decoder and impose it over object queries for better focus on different semantic components. To better eliminate interference from occlusions, we design a contrast feature learning technique (CFL) for better separation of occlusion features and discriminative ID features. Extensive experiments over occluded and holistic re-ID benchmarks (Occluded-DukeMTMC, Market1501 and DukeMTMC) show that the DRL-Net achieves superior re-ID performance consistently and outperforms the state-of-the-art by large margins for Occluded-DukeMTMC.",0
"Re-identification of individuals under various occlusions has been a persisting challenge due to misalignment in image matching and ranking. Most existing methods for addressing this challenge require aligning spatial features of body parts based on external semantic cues or feature similarities, but this approach is complicated and noise-sensitive. Our solution is DRL-Net, a disentangled representation learning network that can handle occluded re-ID without strict person image alignment or additional supervision. Using transformer architectures, DRL-Net achieves alignment-free re-ID by globally reasoning local features of occluded person images. It measures image similarity by disentangling the representation of undefined semantic components, such as human body parts or obstacles, through transformer guidance of semantic preference object queries. Additionally, we imposed a decorrelation constraint in the transformer decoder for better focus on different semantic components. To eliminate interference from occlusions, we developed a contrast feature learning technique (CFL) for separating occlusion features from discriminative ID features. Extensive experiments on occluded and holistic re-ID benchmarks (Occluded-DukeMTMC, Market1501, and DukeMTMC) demonstrate that DRL-Net consistently achieves superior re-ID performance and outperforms the state-of-the-art by large margins for Occluded-DukeMTMC.",1
"We consider the problem of training robust and accurate deep neural networks (DNNs) when subject to various proportions of noisy labels. Large-scale datasets tend to contain mislabeled samples that can be memorized by DNNs, impeding the performance. With appropriate handling, this degradation can be alleviated. There are two problems to consider: how to distinguish clean samples and how to deal with noisy samples. In this paper, we present Ensemble Noise-robust K-fold Cross-Validation Selection (E-NKCVS) to effectively select clean samples from noisy data, solving the first problem. For the second problem, we create a new pseudo label for any sample determined to have an uncertain or likely corrupt label. E-NKCVS obtains multiple predicted labels for each sample and the entropy of these labels is used to tune the weight given to the pseudo label and the given label. Theoretical analysis and extensive verification of the algorithms in the noisy label setting are provided. We evaluate our approach on various image and text classification tasks where the labels have been manually corrupted with different noise ratios. Additionally, two large real-world noisy datasets are also used, Clothing-1M and WebVision. E-NKCVS is empirically shown to be highly tolerant to considerable proportions of label noise and has a consistent improvement over state-of-the-art methods. Especially on more difficult datasets with higher noise ratios, we can achieve a significant improvement over the second-best model. Moreover, our proposed approach can easily be integrated into existing DNN methods to improve their robustness against label noise.",0
"This paper addresses the challenge of training accurate and robust deep neural networks (DNNs) in the presence of noisy labels. Mislabeled samples in large-scale datasets can hinder performance by being memorized by DNNs. To alleviate this degradation, we must address two problems: distinguishing clean samples from noisy ones and dealing with noisy samples. To solve the first problem, we introduce Ensemble Noise-robust K-fold Cross-Validation Selection (E-NKCVS), which effectively selects clean samples from noisy data. For the second problem, we create a new pseudo label for samples with uncertain or corrupt labels. E-NKCVS obtains multiple predicted labels for each sample, using the entropy of these labels to adjust the weight given to the pseudo label and the given label. We provide theoretical analysis and extensive verification of the algorithms in the noisy label setting, evaluating our approach on various image and text classification tasks. We also use two large real-world noisy datasets, Clothing-1M and WebVision. Our approach demonstrates high tolerance to considerable proportions of label noise and consistently outperforms state-of-the-art methods, especially on more challenging datasets with higher noise ratios. Furthermore, our proposed approach can be easily integrated into existing DNN methods to improve their robustness against label noise.",1
"Recent advances in deep representation learning on Riemannian manifolds extend classical deep learning operations to better capture the geometry of the manifold. One possible extension is the Fr\'echet mean, the generalization of the Euclidean mean; however, it has been difficult to apply because it lacks a closed form with an easily computable derivative. In this paper, we show how to differentiate through the Fr\'echet mean for arbitrary Riemannian manifolds. Then, focusing on hyperbolic space, we derive explicit gradient expressions and a fast, accurate, and hyperparameter-free Fr\'echet mean solver. This fully integrates the Fr\'echet mean into the hyperbolic neural network pipeline. To demonstrate this integration, we present two case studies. First, we apply our Fr\'echet mean to the existing Hyperbolic Graph Convolutional Network, replacing its projected aggregation to obtain state-of-the-art results on datasets with high hyperbolicity. Second, to demonstrate the Fr\'echet mean's capacity to generalize Euclidean neural network operations, we develop a hyperbolic batch normalization method that gives an improvement parallel to the one observed in the Euclidean setting.",0
"Classical deep learning operations have been extended to better capture the geometry of Riemannian manifolds through recent advances in deep representation learning. One such extension is the Fr\'echet mean, which is a generalization of the Euclidean mean. However, it has been difficult to use due to the lack of a closed form with an easily computable derivative. This paper addresses this issue by demonstrating how to differentiate through the Fr\'echet mean for arbitrary Riemannian manifolds. The focus is on hyperbolic space, where explicit gradient expressions and a fast, accurate, and hyperparameter-free Fr\'echet mean solver are derived. This allows for the full integration of the Fr\'echet mean into the hyperbolic neural network pipeline. Two case studies are presented to demonstrate this integration: first, the Fr\'echet mean is applied to the existing Hyperbolic Graph Convolutional Network, resulting in state-of-the-art results on datasets with high hyperbolicity. Second, a hyperbolic batch normalization method is developed to showcase the Fr\'echet mean's ability to generalize Euclidean neural network operations, resulting in a similar improvement to the one observed in the Euclidean setting.",1
"Much data with graph structures satisfy the principle of homophily, meaning that connected nodes tend to be similar with respect to a specific attribute. As such, ubiquitous datasets for graph machine learning tasks have generally been highly homophilous, rewarding methods that leverage homophily as an inductive bias. Recent work has pointed out this particular focus, as new non-homophilous datasets have been introduced and graph representation learning models better suited for low-homophily settings have been developed. However, these datasets are small and poorly suited to truly testing the effectiveness of new methods in non-homophilous settings. We present a series of improved graph datasets with node label relationships that do not satisfy the homophily principle. Along with this, we introduce a new measure of the presence or absence of homophily that is better suited than existing measures in different regimes. We benchmark a range of simple methods and graph neural networks across our proposed datasets, drawing new insights for further research. Data and codes can be found at https://github.com/CUAI/Non-Homophily-Benchmarks.",0
"The majority of data with graph structures adhere to the principle of homophily, which means that connected nodes typically share characteristics related to a specific attribute. Consequently, datasets used for graph machine learning tasks are often highly homophilous, favoring methods that utilize homophily as an inductive bias. However, recent research has highlighted the need for non-homophilous datasets and graph representation learning models that are better suited for low-homophily settings. Although some non-homophilous datasets have been introduced, they are limited in size and not ideal for thoroughly testing new methods in non-homophilous settings. To address this issue, we present a series of improved graph datasets with node label relationships that do not comply with the homophily principle. We also introduce a new measure for the presence or absence of homophily that is better suited for different regimes than existing measures. Through benchmarking a range of simple methods and graph neural networks on our proposed datasets, we can derive new insights for further research. The data and codes for our work can be accessed at https://github.com/CUAI/Non-Homophily-Benchmarks.",1
"While GANs have shown success in realistic image generation, the idea of using GANs for other tasks unrelated to synthesis is underexplored. Do GANs learn meaningful structural parts of objects during their attempt to reproduce those objects? In this work, we test this hypothesis and propose a simple and effective approach based on GANs for semantic part segmentation that requires as few as one label example along with an unlabeled dataset. Our key idea is to leverage a trained GAN to extract pixel-wise representation from the input image and use it as feature vectors for a segmentation network. Our experiments demonstrate that GANs representation is ""readily discriminative"" and produces surprisingly good results that are comparable to those from supervised baselines trained with significantly more labels. We believe this novel repurposing of GANs underlies a new class of unsupervised representation learning that is applicable to many other tasks. More results are available at https://repurposegans.github.io/.",0
"Although GANs have been successful in generating realistic images, their potential for tasks outside of synthesis remains largely unexplored. The question of whether GANs acquire meaningful structural information about objects during the reproduction process is yet to be answered. In this study, we investigate this theory and propose a straightforward and efficient approach to semantic part segmentation using GANs. This method only requires one labeled example and an unlabeled dataset. The approach involves using a trained GAN to extract pixel-wise representations from the input image, which are then used as feature vectors for a segmentation network. Our experiments demonstrate that GANs produce ""readily discriminative"" representations, and the results are remarkably comparable to those of supervised baselines trained with significantly more labeled data. We believe this innovative repurposing of GANs can be applied to many other tasks and represents a new class of unsupervised representation learning. Further results are available at https://repurposegans.github.io/.",1
"We consider the problem of building a state representation model for control, in a continual learning setting. As the environment changes, the aim is to efficiently compress the sensory state's information without losing past knowledge, and then use Reinforcement Learning on the resulting features for efficient policy learning. To this end, we propose S-TRIGGER, a general method for Continual State Representation Learning applicable to Variational Auto-Encoders and its many variants. The method is based on Generative Replay, i.e. the use of generated samples to maintain past knowledge. It comes along with a statistically sound method for environment change detection, which self-triggers the Generative Replay. Our experiments on VAEs show that S-TRIGGER learns state representations that allows fast and high-performing Reinforcement Learning, while avoiding catastrophic forgetting. The resulting system is capable of autonomously learning new information without using past data and with a bounded system size. Code for our experiments is attached in Appendix.",0
"In a continual learning setting, we aim to efficiently compress sensory state information without losing past knowledge to build a state representation model for control. The resulting features are then used for efficient policy learning through Reinforcement Learning. In this study, we propose S-TRIGGER, a general method for Continual State Representation Learning applicable to Variational Auto-Encoders and its many variants. The method utilizes Generative Replay, which involves generating samples to maintain past knowledge. Additionally, S-TRIGGER includes a statistically sound method for detecting changes in the environment, which triggers the Generative Replay. Our experiments on VAEs demonstrate that S-TRIGGER enables fast and high-performing Reinforcement Learning, while avoiding catastrophic forgetting. The resulting system can learn autonomously without relying on past data and with a bounded system size. The code for our experiments is provided in the Appendix.",1
"Generalizable person Re-Identification (ReID) has attracted growing attention in recent computer vision community. In this work, we construct a structural causal model among identity labels, identity-specific factors (clothes/shoes color etc), and domain-specific factors (background, viewpoints etc). According to the causal analysis, we propose a novel Domain Invariant Representation Learning for generalizable person Re-Identification (DIR-ReID) framework. Specifically, we first propose to disentangle the identity-specific and domain-specific feature spaces, based on which we propose an effective algorithmic implementation for backdoor adjustment, essentially serving as a causal intervention towards the SCM. Extensive experiments have been conducted, showing that DIR-ReID outperforms state-of-the-art methods on large-scale domain generalization ReID benchmarks.",0
"The concept of Generalizable person Re-Identification (ReID) has been gaining significant attention in the computer vision community lately. This study aims to establish a structural causal model that connects identity labels, identity-specific factors (such as clothes and shoes color), and domain-specific factors (such as background and viewpoints). Based on this analysis, a novel framework called Domain Invariant Representation Learning for generalizable person Re-Identification (DIR-ReID) has been proposed. The framework focuses on separating identity-specific and domain-specific feature spaces and implementing an effective algorithmic implementation for backdoor adjustment, which acts as a causal intervention towards the SCM. Extensive experiments have proven that DIR-ReID outperforms other state-of-the-art methods in large-scale domain generalization ReID benchmarks.",1
"Person re-identification (Re-ID) via gait features within 3D skeleton sequences is a newly-emerging topic with several advantages. Existing solutions either rely on hand-crafted descriptors or supervised gait representation learning. This paper proposes a self-supervised gait encoding approach that can leverage unlabeled skeleton data to learn gait representations for person Re-ID. Specifically, we first create self-supervision by learning to reconstruct unlabeled skeleton sequences reversely, which involves richer high-level semantics to obtain better gait representations. Other pretext tasks are also explored to further improve self-supervised learning. Second, inspired by the fact that motion's continuity endows adjacent skeletons in one skeleton sequence and temporally consecutive skeleton sequences with higher correlations (referred as locality in 3D skeleton data), we propose a locality-aware attention mechanism and a locality-aware contrastive learning scheme, which aim to preserve locality-awareness on intra-sequence level and inter-sequence level respectively during self-supervised learning. Last, with context vectors learned by our locality-aware attention mechanism and contrastive learning scheme, a novel feature named Constrastive Attention-based Gait Encodings (CAGEs) is designed to represent gait effectively. Empirical evaluations show that our approach significantly outperforms skeleton-based counterparts by 15-40% Rank-1 accuracy, and it even achieves superior performance to numerous multi-modal methods with extra RGB or depth information. Our codes are available at https://github.com/Kali-Hac/Locality-Awareness-SGE.",0
"Person re-identification (Re-ID) using gait features from 3D skeleton sequences is a newly developed field that offers several advantages. Current methods rely on either handcrafted descriptors or supervised gait representation learning. This paper presents a self-supervised gait encoding approach that can use unlabeled skeleton data to learn gait representations for person Re-ID. Initially, self-supervision is created by learning to reconstruct unlabeled skeleton sequences in reverse, which provides more sophisticated high-level semantics for better gait representations. Other pretext tasks are also explored to improve self-supervised learning further. Secondly, a locality-aware attention mechanism and a locality-aware contrastive learning scheme are proposed to preserve locality-awareness on the intra-sequence and inter-sequence levels, respectively. These mechanisms use motion's continuity, which endows adjacent skeletons in one skeleton sequence and temporally consecutive skeleton sequences with higher correlations. Lastly, with context vectors learned by the proposed mechanisms, a novel feature called Contrastive Attention-based Gait Encodings (CAGEs) is introduced to represent gait effectively. Empirical evaluations demonstrate that the proposed approach outperforms skeleton-based counterparts by 15-40% Rank-1 accuracy and even surpasses numerous multi-modal methods that use extra RGB or depth information. The codes for the proposed approach are available at https://github.com/Kali-Hac/Locality-Awareness-SGE.",1
"Contrastive learning applied to self-supervised representation learning has seen a resurgence in deep models. In this paper, we find that existing contrastive learning based solutions for self-supervised video recognition focus on inter-variance encoding but ignore the intra-variance existing in clips within the same video. We thus propose to learn dual representations for each clip which (\romannumeral 1) encode intra-variance through a shuffle-rank pretext task; (\romannumeral 2) encode inter-variance through a temporal coherent contrastive loss. Experiment results show that our method plays an essential role in balancing inter and intra variances and brings consistent performance gains on multiple backbones and contrastive learning frameworks. Integrated with SimCLR and pretrained on Kinetics-400, our method achieves $\textbf{82.0\%}$ and $\textbf{51.2\%}$ downstream classification accuracy on UCF101 and HMDB51 test sets respectively and $\textbf{46.1\%}$ video retrieval accuracy on UCF101, outperforming both pretext-task based and contrastive learning based counterparts.",0
"Deep models have recently revived the use of contrastive learning for self-supervised representation learning. However, current contrastive learning solutions for self-supervised video recognition tend to focus solely on inter-variance encoding, neglecting the intra-variance that exists in clips within the same video. To address this issue, we propose a method of learning dual representations for each clip. The first representation encodes intra-variance through a shuffle-rank pretext task, while the second representation encodes inter-variance through a temporal coherent contrastive loss. Our experiments demonstrate that our method effectively balances inter and intra variances and consistently improves performance across multiple backbones and contrastive learning frameworks. Specifically, when integrated with SimCLR and pretrained on Kinetics-400, our method achieves $\textbf{82.0\%}$ and $\textbf{51.2\%}$ downstream classification accuracy on UCF101 and HMDB51 test sets, respectively, as well as $\textbf{46.1\%}$ video retrieval accuracy on UCF101, surpassing both pretext-task and contrastive learning-based approaches.",1
"The slowness principle is a concept inspired by the visual cortex of the brain. It postulates that the underlying generative factors of a quickly varying sensory signal change on a slower time scale. Unsupervised learning of intermediate representations utilizing abundant unlabeled sensory data can be leveraged to perform data-efficient supervised downstream regression. In this paper, we propose a general formulation of slowness for unsupervised representation learning adding a slowness regularization term to the estimate lower bound of the beta-VAE to encourage temporal similarity in observation and latent space. Within this framework we compare existing slowness regularization terms such as the L1 and L2 loss used in existing end-to-end methods, the SlowVAE and propose a new term based on Brownian motion. We empirically evaluate these slowness regularization terms with respect to their downstream task performance and data efficiency. We find that slow representations lead to equal or better downstream task performance and data efficiency in different experiment domains when compared to representations without slowness regularization. Finally, we discuss how the Frechet Inception Distance (FID), traditionally used to determine the generative capabilities of GANs, can serve as a measure to predict the performance of pre-trained Autoencoder model in a supervised downstream task and accelerate hyperparameter search.",0
"The slowness principle, influenced by the visual cortex of the brain, suggests that the underlying causes of fast-changing sensory signals occur at a slower pace. By utilizing unsupervised learning to create intermediate representations from abundant, unlabeled sensory data, we can efficiently perform supervised downstream regression. This paper proposes a general slowness formulation for unsupervised representation learning that includes a slowness regularization term in the beta-VAE's estimate lower bound to encourage temporal consistency between observation and latent space. We compare various slowness regularization terms, including L1 and L2 loss, SlowVAE, and our new term based on Brownian motion, in terms of downstream task performance and data efficiency. The results show that slower representations lead to equal or better downstream task performance and data efficiency in various experimental domains compared to representations without slowness regularization. Finally, we discuss how the Frechet Inception Distance (FID) can serve as a measure to predict pre-trained Autoencoder model's supervised downstream task performance and accelerate hyperparameter search, traditionally used to determine the generative capabilities of GANs.",1
"Deep Reinforcement Learning has shown its ability in solving complicated problems directly from high-dimensional observations. However, in end-to-end settings, Reinforcement Learning algorithms are not sample-efficient and requires long training times and quantities of data. In this work, we proposed a framework for sample-efficient Reinforcement Learning that take advantage of state and action representations to transform a high-dimensional problem into a low-dimensional one. Moreover, we seek to find the optimal policy mapping latent states to latent actions. Because now the policy is learned on abstract representations, we enforce, using auxiliary loss functions, the lifting of such policy to the original problem domain. Results show that the novel framework can efficiently learn low-dimensional and interpretable state and action representations and the optimal latent policy.",0
"The capacity of Deep Reinforcement Learning to tackle intricate problems directly from high-dimensional observations is well-established. However, Reinforcement Learning algorithms are not particularly efficient in end-to-end settings, as they require prolonged training periods and vast amounts of data. To address this issue, we have developed a framework that leverages state and action representations to transform a high-dimensional problem into a low-dimensional one, thereby achieving sample-efficient Reinforcement Learning. Additionally, we aim to identify the optimal policy that maps latent states to latent actions. By learning the policy on abstract representations, we ensure that it is lifted to the original problem domain using auxiliary loss functions. Our findings demonstrate that our innovative framework can effectively learn low-dimensional and easily interpretable state and action representations, as well as the best latent policy.",1
"Strategies for improving the training and prediction quality of weakly supervised machine learning models vary in how much they are tailored to a specific task or integrated with a specific model architecture. In this work, we introduce Knodle, a software framework that treats weak data annotations, deep learning models, and methods for improving weakly supervised training as separate, modular components. This modularization gives the training process access to fine-grained information such as data set characteristics, matches of heuristic rules, or elements of the deep learning model ultimately used for prediction. Hence, our framework can encompass a wide range of training methods for improving weak supervision, ranging from methods that only look at correlations of rules and output classes (independently of the machine learning model trained with the resulting labels), to those that harness the interplay of neural networks and weakly labeled data. We illustrate the benchmarking potential of the framework with a performance comparison of several reference implementations on a selection of datasets that are already available in Knodle.   The framework is published as an open-source Python package knodle and available at https://github.com/knodle/knodle.",0
"There are various strategies to enhance the training and prediction quality of weakly supervised machine learning models, which can be task-specific or integrated with a specific model architecture. In this study, we present Knodle, a software framework that treats weak data annotations, deep learning models, and methods for improving weakly supervised training as distinct, modular components. This modularization allows for the training process to access detailed information such as dataset characteristics, heuristic rule matches, or deep learning model elements used for prediction. Therefore, our framework can accommodate a broad range of training approaches to improve weak supervision, from those that examine only rule correlations and output classes (independently of the machine learning model trained with the resulting labels) to those that utilize neural networks and weakly labeled data. We demonstrate the framework's benchmarking potential by comparing the performance of several reference implementations on a variety of datasets that are already available in Knodle. The framework is an open-source Python package called knodle and can be accessed at https://github.com/knodle/knodle.",1
"We study the problem of self-supervised structured representation learning using autoencoders for generative modeling. Unlike most methods which rely on matching an arbitrary, relatively unstructured, prior distribution for sampling, we propose a sampling technique that relies solely on the independence of latent variables, thereby avoiding the trade-off between reconstruction quality and generative performance inherent to VAEs. We design a novel autoencoder architecture capable of learning a structured representation without the need for aggressive regularization. Our structural decoders learn a hierarchy of latent variables, akin to structural causal models, thereby ordering the information without any additional regularization. We demonstrate how these models learn a representation that improves results in a variety of downstream tasks including generation, disentanglement, and extrapolation using several challenging and natural image datasets.",0
"Our research focuses on utilizing autoencoders for generative modeling in self-supervised structured representation learning. In contrast to many other techniques, which rely on an unstructured prior distribution for sampling, we propose a sampling method that solely depends on the independence of latent variables. By doing so, we avoid the trade-off between reconstruction quality and generative performance that VAEs inherently possess. Our unique autoencoder architecture is capable of learning a structured representation without requiring intensive regularization. Our structural decoders learn a hierarchy of latent variables, similar to structural causal models, which organizes information without the need for additional regularization. By using challenging and natural image datasets, we demonstrate how these models can enhance results in various downstream tasks, including generation, disentanglement, and extrapolation.",1
"We present a novel approach for unsupervised activity segmentation, which uses video frame clustering as a pretext task and simultaneously performs representation learning and online clustering. This is in contrast with prior works where representation learning and clustering are often performed sequentially. We leverage temporal information in videos by employing temporal optimal transport and temporal coherence loss. In particular, we incorporate a temporal regularization term into the standard optimal transport module, which preserves the temporal order of the activity, yielding the temporal optimal transport module for computing pseudo-label cluster assignments. Next, the temporal coherence loss encourages neighboring video frames to be mapped to nearby points while distant video frames are mapped to farther away points in the embedding space. The combination of these two components results in effective representations for unsupervised activity segmentation. Furthermore, previous methods require storing learned features for the entire dataset before clustering them in an offline manner, whereas our approach processes one mini-batch at a time in an online manner. Extensive evaluations on three public datasets, i.e. 50-Salads, YouTube Instructions, and Breakfast, and our dataset, i.e., Desktop Assembly, show that our approach performs on par or better than previous methods for unsupervised activity segmentation, despite having significantly less memory constraints.",0
"Our approach for unsupervised activity segmentation differs from previous works as we perform representation learning and online clustering simultaneously by using video frame clustering as a pretext task. To leverage temporal information in videos, we use temporal optimal transport and temporal coherence loss. Specifically, we incorporate a temporal regularization term into the optimal transport module, resulting in the temporal optimal transport module for computing pseudo-label cluster assignments. Additionally, the temporal coherence loss encourages neighboring frames to be mapped to nearby points while distant frames are mapped to farther away points in the embedding space. This combination leads to effective representations for unsupervised activity segmentation. Unlike previous methods that require storing learned features for the entire dataset before offline clustering, we process one mini-batch at a time in an online manner. Our evaluations on four datasets, including our own, demonstrate comparable or better performance than previous methods, with significantly less memory constraints.",1
"Learning with graphs has attracted significant attention recently. Existing representation learning methods on graphs have achieved state-of-the-art performance on various graph-related tasks such as node classification, link prediction, etc. However, we observe that these methods could leak serious private information. For instance, one can accurately infer the links (or node identity) in a graph from a node classifier (or link predictor) trained on the learnt node representations by existing methods. To address the issue, we propose a privacy-preserving representation learning framework on graphs from the \emph{mutual information} perspective. Specifically, our framework includes a primary learning task and a privacy protection task, and we consider node classification and link prediction as the two tasks of interest. Our goal is to learn node representations such that they can be used to achieve high performance for the primary learning task, while obtaining performance for the privacy protection task close to random guessing. We formally formulate our goal via mutual information objectives. However, it is intractable to compute mutual information in practice. Then, we derive tractable variational bounds for the mutual information terms, where each bound can be parameterized via a neural network. Next, we train these parameterized neural networks to approximate the true mutual information and learn privacy-preserving node representations. We finally evaluate our framework on various graph datasets.",0
"Recently, there has been a significant focus on using graphs for learning. Various representation learning methods have achieved state-of-the-art results on graph-related tasks such as node classification and link prediction. However, it has been observed that these methods can reveal private information, such as links or node identity. To address this issue, we suggest a privacy-preserving representation learning approach for graphs based on mutual information. Our framework involves a primary learning task and a privacy protection task, with node classification and link prediction as the two areas of interest. Our aim is to develop node representations that achieve high performance on the primary learning task, while ensuring that the performance on the privacy protection task is close to random guessing. We use mutual information objectives to formally formulate our goal, but it is not practical to compute mutual information. As a result, we derive tractable variational bounds for the mutual information terms, which can be parameterized using a neural network. We then train these parameterized neural networks to approximate the true mutual information and acquire privacy-preserving node representations. Finally, we evaluate our framework on various graph datasets.",1
"Graph-based Anomaly Detection (GAD) is becoming prevalent due to the powerful representation abilities of graphs as well as recent advances in graph mining techniques. These GAD tools, however, expose a new attacking surface, ironically due to their unique advantage of being able to exploit the relations among data. That is, attackers now can manipulate those relations (i.e., the structure of the graph) to allow some target nodes to evade detection. In this paper, we exploit this vulnerability by designing a new type of targeted structural poisoning attacks to a representative regression-based GAD system termed OddBall. Specially, we formulate the attack against OddBall as a bi-level optimization problem, where the key technical challenge is to efficiently solve the problem in a discrete domain. We propose a novel attack method termed BinarizedAttack based on gradient descent. Comparing to prior arts, BinarizedAttack can better use the gradient information, making it particularly suitable for solving combinatorial optimization problems. Furthermore, we investigate the attack transferability of BinarizedAttack by employing it to attack other representation-learning-based GAD systems. Our comprehensive experiments demonstrate that BinarizedAttack is very effective in enabling target nodes to evade graph-based anomaly detection tools with limited attackers' budget, and in the black-box transfer attack setting, BinarizedAttack is also tested effective and in particular, can significantly change the node embeddings learned by the GAD systems. Our research thus opens the door to studying a new type of attack against security analytic tools that rely on graph data.",0
"Due to the strong representation capabilities of graphs and recent advancements in graph mining techniques, Graph-based Anomaly Detection (GAD) is becoming increasingly popular. However, these tools also present a new attack surface as they can manipulate data relations, allowing certain nodes to escape detection. In this study, we use this vulnerability to design targeted structural poisoning attacks on the regression-based GAD system, OddBall. Our attack is formulated as a bi-level optimization problem, which we solve efficiently in a discrete domain using a novel method called BinarizedAttack. Compared to previous methods, BinarizedAttack can better use gradient information, making it suitable for solving combinatorial optimization problems. We also investigate the attack transferability of BinarizedAttack by testing it on other representation-learning-based GAD systems. Our experiments demonstrate that BinarizedAttack is highly effective in allowing target nodes to evade graph-based anomaly detection tools within a limited budget. Furthermore, in the black-box transfer attack setting, BinarizedAttack can significantly alter the node embeddings learned by GAD systems. Our research highlights a new type of attack on security analytic tools that rely on graph data.",1
"Single-pixel imaging is a novel imaging scheme that has gained popularity due to its huge computational gain and potential for a low-cost alternative to imaging beyond the visible spectrum. The traditional reconstruction methods struggle to produce a clear recovery when one limits the number of illumination patterns from a spatial light modulator. As a remedy, several deep-learning-based solutions have been proposed which lack good generalization ability due to the architectural setup and loss functions. In this paper, we propose a generative adversarial network-based reconstruction framework for single-pixel imaging, referred to as SPI-GAN. Our method can reconstruct images with 17.92 dB PSNR and 0.487 SSIM, even if the sampling ratio drops to 5%. This facilitates much faster reconstruction making our method suitable for single-pixel video. Furthermore, our ResNet-like architecture for the generator leads to useful representation learning that allows us to reconstruct completely unseen objects. The experimental results demonstrate that SPI-GAN achieves significant performance gain, e.g. near 3dB PSNR gain, over the current state-of-the-art method.",0
"Single-pixel imaging is becoming increasingly popular due to its significant computational advantage and potential as a low-cost alternative for imaging beyond the visible spectrum. However, traditional reconstruction methods struggle to produce clear results when the number of illumination patterns from a spatial light modulator is limited. To address this issue, various deep-learning-based solutions have been proposed, but they lack good generalization ability because of their architectural setup and loss functions. In this study, we introduce a generative adversarial network-based reconstruction framework for single-pixel imaging, SPI-GAN. Our method can reconstruct images with 17.92 dB PSNR and 0.487 SSIM, even when the sampling ratio drops to 5%, enabling much faster reconstruction and making our method suitable for single-pixel video. Moreover, our ResNet-like generator architecture leads to useful representation learning, allowing us to reconstruct completely unseen objects. Our experimental results demonstrate that SPI-GAN achieves significant performance gains, such as a near 3dB PSNR gain, over the current state-of-the-art method.",1
"This work presents improvements in monocular hand shape estimation by building on top of recent advances in unsupervised learning. We extend momentum contrastive learning and contribute a structured collection of hand images, well suited for visual representation learning, which we call HanCo. We find that the representation learned by established contrastive learning methods can be improved significantly by exploiting advanced background removal techniques and multi-view information. These allow us to generate more diverse instance pairs than those obtained by augmentations commonly used in exemplar based approaches. Our method leads to a more suitable representation for the hand shape estimation task and shows a 4.7% reduction in mesh error and a 3.6% improvement in F-score compared to an ImageNet pretrained baseline. We make our benchmark dataset publicly available, to encourage further research into this direction.",0
"By leveraging recent advancements in unsupervised learning, this research enhances the accuracy of monocular hand shape estimation. The study builds upon momentum contrastive learning and introduces a structured collection of hand images named HanCo, which is ideal for visual representation learning. The research indicates that employing advanced background removal techniques and multi-view information can significantly improve the representation learned by established contrastive learning methods. This approach generates more diverse instance pairs than those obtained by commonly used augmentations in exemplar based approaches. The results show that our method performs better than an ImageNet pretrained baseline, exhibiting a 4.7% reduction in mesh error and a 3.6% improvement in F-score. In addition, the researchers have made their benchmark dataset openly available to encourage further exploration in this area.",1
"Inducing causal relationships from observations is a classic problem in machine learning. Most work in causality starts from the premise that the causal variables themselves are observed. However, for AI agents such as robots trying to make sense of their environment, the only observables are low-level variables like pixels in images. To generalize well, an agent must induce high-level variables, particularly those which are causal or are affected by causal variables. A central goal for AI and causality is thus the joint discovery of abstract representations and causal structure. However, we note that existing environments for studying causal induction are poorly suited for this objective because they have complicated task-specific causal graphs which are impossible to manipulate parametrically (e.g., number of nodes, sparsity, causal chain length, etc.). In this work, our goal is to facilitate research in learning representations of high-level variables as well as causal structures among them. In order to systematically probe the ability of methods to identify these variables and structures, we design a suite of benchmarking RL environments. We evaluate various representation learning algorithms from the literature and find that explicitly incorporating structure and modularity in models can help causal induction in model-based reinforcement learning.",0
"The issue of determining causal relationships from observations is a common problem in machine learning. Most causality research assumes that the causal variables are observable, but this is not the case for AI agents like robots who only have access to low-level variables, such as image pixels. To be successful, these agents must induce high-level variables that are affected by or causal themselves. The main objective for AI and causality is to jointly discover abstract representations and causal structure. However, current environments for studying causal induction are not ideal because they have complex task-specific causal graphs that cannot be parametrically manipulated. This study aims to aid research in learning representations of high-level variables and causal structures. To test methods' ability to identify these variables and structures, we create benchmarking RL environments. Our evaluation of representation learning algorithms suggests that incorporating structure and modularity improves causal induction in model-based reinforcement learning.",1
"A crucial ability of human intelligence is to build up models of individual 3D objects from partial scene observations. Recent works achieve object-centric generation but without the ability to infer the representation, or achieve 3D scene representation learning but without object-centric compositionality. Therefore, learning to represent and render 3D scenes with object-centric compositionality remains elusive. In this paper, we propose a probabilistic generative model for learning to build modular and compositional 3D object models from partial observations of a multi-object scene. The proposed model can (i) infer the 3D object representations by learning to search and group object areas and also (ii) render from an arbitrary viewpoint not only individual objects but also the full scene by compositing the objects. The entire learning process is unsupervised and end-to-end. In experiments, in addition to generation quality, we also demonstrate that the learned representation permits object-wise manipulation and novel scene generation, and generalizes to various settings. Results can be found on our project website: https://sites.google.com/view/roots3d",0
"The ability to construct models of 3D objects from incomplete observations is a vital aspect of human intelligence. While recent research has made progress in generating object-specific models, they lack the capacity to deduce representation, or in achieving the compositionality of 3D scene representation learning. Consequently, learning to represent and render 3D scenes with object-centric compositionality remains challenging. This paper presents a probabilistic generative model that can learn to build modular and compositional 3D object models from partial observations of a multi-object scene. The proposed model can (i) deduce 3D object representations by learning to search and group object areas and (ii) render individual objects and the entire scene from any perspective by combining the objects. The learning process is end-to-end and unsupervised. In addition to generating high-quality results, our experiments demonstrate that the learned representation enables object-wise manipulation and novel scene generation, and generalizes to various settings. Further details and results can be found on our project website: https://sites.google.com/view/roots3d",1
"Image super-resolution (SR) research has witnessed impressive progress thanks to the advance of convolutional neural networks (CNNs) in recent years. However, most existing SR methods are non-blind and assume that degradation has a single fixed and known distribution (e.g., bicubic) which struggle while handling degradation in real-world data that usually follows a multi-modal, spatially variant, and unknown distribution. The recent blind SR studies address this issue via degradation estimation, but they do not generalize well to multi-source degradation and cannot handle spatially variant degradation. We design CRL-SR, a contrastive representation learning network that focuses on blind SR of images with multi-modal and spatially variant distributions. CRL-SR addresses the blind SR challenges from two perspectives. The first is contrastive decoupling encoding which introduces contrastive learning to extract resolution-invariant embedding and discard resolution-variant embedding under the guidance of a bidirectional contrastive loss. The second is contrastive feature refinement which generates lost or corrupted high-frequency details under the guidance of a conditional contrastive loss. Extensive experiments on synthetic datasets and real images show that the proposed CRL-SR can handle multi-modal and spatially variant degradation effectively under blind settings and it also outperforms state-of-the-art SR methods qualitatively and quantitatively.",0
"Convolutional neural networks have greatly advanced the field of image super-resolution research. However, most existing methods assume that image degradation follows a single known distribution, making it difficult to handle real-world data that has unknown, multi-modal, and spatially variant degradation. Blind SR methods attempt to address this issue by estimating degradation, but they struggle with multi-source degradation and spatially variant degradation. To tackle these challenges, we propose CRL-SR, a contrastive representation learning network that uses contrastive decoupling encoding and contrastive feature refinement to extract resolution-invariant embedding and generate lost or corrupted high-frequency details. Our experiments show that CRL-SR can effectively handle multi-modal and spatially variant degradation in blind settings and outperforms existing SR methods.",1
"The idea behind object-centric representation learning is that natural scenes can better be modeled as compositions of objects and their relations as opposed to distributed representations. This inductive bias can be injected into neural networks to potentially improve systematic generalization and learning efficiency of downstream tasks in scenes with multiple objects. In this paper, we train state-of-the-art unsupervised models on five common multi-object datasets and evaluate segmentation accuracy and downstream object property prediction. In addition, we study systematic generalization and robustness by investigating the settings where either single objects are out-of-distribution -- e.g., having unseen colors, textures, and shapes -- or global properties of the scene are altered -- e.g., by occlusions, cropping, or increasing the number of objects. From our experimental study, we find object-centric representations to be generally useful for downstream tasks and robust to shifts in the data distribution, especially if shifts affect single objects.",0
"Object-centric representation learning suggests that natural scenes can be better represented as compositions of objects and their relationships, rather than through distributed representations. By incorporating this approach into neural networks, we can potentially improve systematic generalization and learning efficiency for scenes with multiple objects. In this study, we trained advanced unsupervised models on five commonly used multi-object datasets to evaluate segmentation accuracy and predict downstream object properties. We also examined systematic generalization and robustness by testing scenarios where single objects were out-of-distribution due to unseen colors, textures, and shapes, or where global scene properties were altered through occlusions, cropping, or increasing the number of objects. Our experimental findings show that object-centric representations are generally beneficial for downstream tasks and are resilient to shifts in the data distribution, particularly when shifts impact single objects.",1
"In this paper, we explore the mask representation in instance segmentation with Point-of-Interest (PoI) features. Differentiating multiple potential instances within a single PoI feature is challenging because learning a high-dimensional mask feature for each instance using vanilla convolution demands a heavy computing burden. To address this challenge, we propose an instance-aware convolution. It decomposes this mask representation learning task into two tractable modules as instance-aware weights and instance-agnostic features. The former is to parametrize convolution for producing mask features corresponding to different instances, improving mask learning efficiency by avoiding employing several independent convolutions. Meanwhile, the latter serves as mask templates in a single point. Together, instance-aware mask features are computed by convolving the template with dynamic weights, used for the mask prediction. Along with instance-aware convolution, we propose PointINS, a simple and practical instance segmentation approach, building upon dense one-stage detectors. Through extensive experiments, we evaluated the effectiveness of our framework built upon RetinaNet and FCOS. PointINS in ResNet101 backbone achieves a 38.3 mask mean average precision (mAP) on COCO dataset, outperforming existing point-based methods by a large margin. It gives a comparable performance to the region-based Mask R-CNN with faster inference.",0
"The aim of this paper is to investigate the use of Point-of-Interest (PoI) features in the representation of masks for instance segmentation. However, it is difficult to differentiate between multiple potential instances within a single PoI feature because the use of vanilla convolution to learn a high-dimensional mask feature for each instance demands a lot of computing power. To overcome this challenge, we propose an instance-aware convolution that separates the mask representation learning task into two modules: instance-aware weights and instance-agnostic features. The former parametrizes convolution to produce mask features for different instances, while the latter serves as mask templates in a single point. Together, these features are used for mask prediction. We also introduce PointINS, a simple and practical instance segmentation approach that builds on dense one-stage detectors. Our experiments show that PointINS achieves a mask mean average precision (mAP) of 38.3 on the COCO dataset, outperforming existing point-based methods by a large margin and giving a comparable performance to the region-based Mask R-CNN but with faster inference.",1
"Visual localization is one of the most important components for robotics and autonomous driving. Recently, inspiring results have been shown with CNN-based methods which provide a direct formulation to end-to-end regress 6-DoF absolute pose. Additional information like geometric or semantic constraints is generally introduced to improve performance. Especially, the latter can aggregate high-level semantic information into localization task, but it usually requires enormous manual annotations. To this end, we propose a novel auxiliary learning strategy for camera localization by introducing scene-specific high-level semantics from self-supervised representation learning task. Viewed as a powerful proxy task, image colorization task is chosen as complementary task that outputs pixel-wise color version of grayscale photograph without extra annotations. In our work, feature representations from colorization network are embedded into localization network by design to produce discriminative features for pose regression. Meanwhile an attention mechanism is introduced for the benefit of localization performance. Extensive experiments show that our model significantly improve localization accuracy over state-of-the-arts on both indoor and outdoor datasets.",0
"Robotics and autonomous driving rely heavily on visual localization, which has recently seen promising results using CNN-based methods that directly regress 6-DoF absolute pose. However, to improve performance, additional information such as geometric or semantic constraints is often introduced, with the latter requiring significant manual annotations. To address this, we propose an innovative auxiliary learning strategy for camera localization that introduces high-level semantic information from a self-supervised representation learning task. We use image colorization as a complementary task, which outputs a pixel-wise color version of a grayscale photograph without extra annotations. Our approach embeds feature representations from the colorization network into the localization network, producing discriminative features for pose regression. An attention mechanism is also introduced for further localization performance gains. Our experiments demonstrate significant improvements in localization accuracy over state-of-the-art methods on both indoor and outdoor datasets.",1
"Benefiting from the powerful expressive capability of graphs, graph-based approaches have achieved impressive performance in various biomedical applications. Most existing methods tend to define the adjacency matrix among samples manually based on meta-features, and then obtain the node embeddings for downstream tasks by Graph Representation Learning (GRL). However, it is not easy for these approaches to generalize to unseen samples. Meanwhile, the complex correlation between modalities is also ignored. As a result, these factors inevitably yield the inadequacy of providing valid information about the patient's condition for a reliable diagnosis. In this paper, we propose an end-to-end Multimodal Graph Learning framework (MMGL) for disease prediction. To effectively exploit the rich information across multi-modality associated with diseases, amodal-attentional multi-modal fusion is proposed to integrate the features of each modality by leveraging the correlation and complementarity between the modalities. Furthermore, instead of defining the adjacency matrix manually as existing methods, the latent graph structure can be captured through a novel way of adaptive graph learning. It could be jointly optimized with the prediction model, thus revealing the intrinsic connections among samples. Unlike the previous transductive methods, our model is also applicable to the scenario of inductive learning for those unseen data. An extensive group of experiments on two disease prediction problems is then carefully designed and presented, demonstrating that MMGL obtains more favorable performances. In addition, we also visualize and analyze the learned graph structure to provide more reliable decision support for doctors in real medical applications and inspiration for disease research.",0
"Graph-based approaches have proven to be highly effective in various biomedical applications due to their powerful expressive capability. However, current methods often manually define the adjacency matrix among samples based on meta-features and utilize Graph Representation Learning (GRL) to obtain node embeddings for downstream tasks, resulting in poor generalization to unseen samples and disregarding the complex correlation between modalities. Consequently, these approaches fail to provide valid information for a reliable diagnosis. To address this issue, we propose a novel end-to-end Multimodal Graph Learning framework (MMGL) for disease prediction that leverages amodal-attentional multi-modal fusion to integrate the features of each modality and capture latent graph structures through adaptive graph learning. Our model is applicable to both inductive and transductive learning scenarios and outperforms existing methods in disease prediction. Additionally, we provide decision support for doctors in real medical applications through visualization and analysis of the learned graph structure.",1
"Self-supervised learning and pre-training strategies have developed over the last few years especially for Convolutional Neural Networks (CNNs). Recently application of such methods can also be noticed for Graph Neural Networks (GNNs) . In this paper, we have used a graph based self-supervised learning strategy with different loss functions (Barlow Twins[Zbontar et al., 2021], HSIC[Tsai et al., 2021], VICReg[Bardes et al., 2021]) which have shown promising results when applied with CNNs previously. We have also proposed a hybrid loss function combining the advantages of VICReg and HSIC and called it as VICRegHSIC. The performance of these aforementioned methods have been compared when applied to different datasets such as MUTAG, PROTEINS and IMDB-Binary. Moreover, the impact of different batch sizes, projector dimensions and data augmentation strategies have also been explored",0
"Self-supervised learning and pre-training techniques have advanced significantly in recent years, particularly for Convolutional Neural Networks (CNNs). However, these methods are now being applied to Graph Neural Networks (GNNs). This study employs a graph-based self-supervised learning approach utilizing various loss functions (Barlow Twins[Zbontar et al., 2021], HSIC[Tsai et al., 2021], VICReg[Bardes et al., 2021]). These loss functions have demonstrated promising outcomes with CNNs. Additionally, a hybrid loss function, dubbed VICRegHSIC, was proposed, which combines the strengths of VICReg and HSIC. The performance of these methods was evaluated on different datasets, including MUTAG, PROTEINS, and IMDB-Binary, while also examining the impact of batch sizes, projector dimensions, and data augmentation strategies.",1
"In this paper, we utilize deep visual Representation Learning to address an important problem in fashion e-commerce: color variants identification, i.e., identifying fashion products that match exactly in their design (or style), but only to differ in their color. At first we attempt to tackle the problem by obtaining manual annotations (depicting whether two products are color variants), and train a supervised triplet loss based neural network model to learn representations of fashion products. However, for large scale real-world industrial datasets such as addressed in our paper, it is infeasible to obtain annotations for the entire dataset, while capturing all the difficult corner cases. Interestingly, we observed that color variants are essentially manifestations of color jitter based augmentations. Thus, we instead explore Self-Supervised Learning (SSL) to solve this problem. We observed that existing state-of-the-art SSL methods perform poor, for our problem. To address this, we propose a novel SSL based color variants model that simultaneously focuses on different parts of an apparel. Quantitative and qualitative evaluation shows that our method outperforms existing SSL methods, and at times, the supervised model.",0
"This paper employs deep visual Representation Learning to tackle an important issue in fashion e-commerce: identifying color variants. This involves identifying fashion products that are identical in style but differ in color. Initially, we attempt to solve the problem by obtaining manual annotations, training a supervised triplet loss based neural network model to learn representations of fashion products. However, for large-scale industrial datasets, this approach is impractical due to the difficulty of capturing all corner cases. We discovered that color variants are essentially color jitter-based augmentations. Therefore, we explore Self-Supervised Learning (SSL) as an alternative solution. Unfortunately, existing state-of-the-art SSL methods perform poorly for our problem. To overcome this challenge, we propose a novel SSL-based color variants model that focuses on different parts of an apparel simultaneously. Our method outperforms existing SSL methods and, in some cases, even the supervised model, as demonstrated by quantitative and qualitative evaluations.",1
"Unsupervised anomaly detection (UAD) learns one-class classifiers exclusively with normal (i.e., healthy) images to detect any abnormal (i.e., unhealthy) samples that do not conform to the expected normal patterns. UAD has two main advantages over its fully supervised counterpart. Firstly, it is able to directly leverage large datasets available from health screening programs that contain mostly normal image samples, avoiding the costly manual labelling of abnormal samples and the subsequent issues involved in training with extremely class-imbalanced data. Further, UAD approaches can potentially detect and localise any type of lesions that deviate from the normal patterns. One significant challenge faced by UAD methods is how to learn effective low-dimensional image representations to detect and localise subtle abnormalities, generally consisting of small lesions. To address this challenge, we propose a novel self-supervised representation learning method, called Constrained Contrastive Distribution learning for anomaly detection (CCD), which learns fine-grained feature representations by simultaneously predicting the distribution of augmented data and image contexts using contrastive learning with pretext constraints. The learned representations can be leveraged to train more anomaly-sensitive detection models. Extensive experiment results show that our method outperforms current state-of-the-art UAD approaches on three different colonoscopy and fundus screening datasets. Our code is available at https://github.com/tianyu0207/CCD.",0
"The focus of Unsupervised Anomaly Detection (UAD) is to build one-class classifiers using healthy images to identify any abnormal images that do not follow the expected patterns. UAD presents two key advantages over its fully supervised counterpart. Firstly, it can leverage large datasets from health screening programs that mainly contain normal images, thus avoiding the expensive manual labeling of abnormal images and training issues that arise due to extreme class imbalances. Secondly, UAD techniques can identify and locate any kind of lesions that deviate from the normal pattern. However, UAD methods face a significant challenge in developing effective low-dimensional image representations to detect and locate subtle abnormalities, often consisting of small lesions. To address this challenge, we introduce Constrained Contrastive Distribution learning for anomaly detection (CCD), a novel self-supervised representation learning method that uses contrastive learning with pretext constraints to simultaneously predict the distribution of augmented data and image contexts and learn fine-grained feature representations. The learned representations can be used to train models that are more sensitive to anomalies. Our experiments demonstrate that CCD outperforms existing state-of-the-art UAD methods on three different colonoscopy and fundus screening datasets. Our code is available at https://github.com/tianyu0207/CCD.",1
"Graph neural networks have recently achieved remarkable success in representing graph-structured data, with rapid progress in both the node embedding and graph pooling methods. Yet, they mostly focus on capturing information from the nodes considering their connectivity, and not much work has been done in representing the edges, which are essential components of a graph. However, for tasks such as graph reconstruction and generation, as well as graph classification tasks for which the edges are important for discrimination, accurately representing edges of a given graph is crucial to the success of the graph representation learning. To this end, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. We validate our edge representation learning method with hypergraphs on diverse graph datasets for graph representation and generation performance, on which our method largely outperforms existing graph representation learning methods. Moreover, our edge representation learning and pooling method also largely outperforms state-of-the-art graph pooling methods on graph classification, not only because of its accurate edge representation learning, but also due to its lossless compression of the nodes and removal of irrelevant edges for effective message passing.",0
"Recently, graph neural networks have achieved remarkable success in representing graph-structured data by making rapid progress in both node embedding and graph pooling methods. However, their focus has mainly been on capturing information from the nodes, neglecting the representation of edges, which are essential components of a graph. Accurately representing edges is crucial to the success of graph representation learning, especially for tasks such as graph reconstruction and generation, and graph classification tasks where edges play a significant role in discrimination. To address this, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which converts the edges of a graph into the nodes of a hypergraph. This method enables the application of message passing techniques to edges. After obtaining edge representations from the hypergraphs, we cluster or drop edges to obtain holistic graph-level edge representations. We validate our method with hypergraphs on diverse graph datasets for graph representation and generation performance, and it largely outperforms existing graph representation learning methods. Our edge representation learning and pooling method also outperforms state-of-the-art graph pooling methods on graph classification, not only because of its accurate edge representation learning but also due to its lossless compression of nodes and removal of irrelevant edges for effective message passing.",1
"An effective approach in meta-learning is to utilize multiple ""train tasks"" to learn a good initialization for model parameters that can help solve unseen ""test tasks"" with very few samples by fine-tuning from this initialization. Although successful in practice, theoretical understanding of such methods is limited. This work studies an important aspect of these methods: splitting the data from each task into train (support) and validation (query) sets during meta-training. Inspired by recent work (Raghu et al., 2020), we view such meta-learning methods through the lens of representation learning and argue that the train-validation split encourages the learned representation to be low-rank without compromising on expressivity, as opposed to the non-splitting variant that encourages high-rank representations. Since sample efficiency benefits from low-rankness, the splitting strategy will require very few samples to solve unseen test tasks. We present theoretical results that formalize this idea for linear representation learning on a subspace meta-learning instance, and experimentally verify this practical benefit of splitting in simulations and on standard meta-learning benchmarks.",0
"Meta-learning can be approached effectively by utilizing various ""train tasks"" to learn a good initialization for model parameters, which can help solve ""test tasks"" with very few samples by fine-tuning from this initialization. Despite being practical, the theoretical understanding of such methods is limited. This study examines an essential aspect of these methods, which involves dividing the data from each task into train (support) and validation (query) sets during meta-training. Drawing inspiration from recent work (Raghu et al., 2020), we view these meta-learning methods from the perspective of representation learning and argue that the train-validation split encourages the learned representation to be low-rank while maintaining expressivity, unlike the non-splitting variant that encourages high-rank representations. Given that low-rankness enhances sample efficiency, the splitting strategy requires very few samples to solve unseen test tasks. We present theoretical results that formalize this idea for linear representation learning on a subspace meta-learning instance and experimentally validate the practical benefit of splitting in simulations and standard meta-learning benchmarks.",1
"Networks have been widely used to represent the relations between objects such as academic networks and social networks, and learning embedding for networks has thus garnered plenty of research attention. Self-supervised network representation learning aims at extracting node embedding without external supervision. Recently, maximizing the mutual information between the local node embedding and the global summary (e.g. Deep Graph Infomax, or DGI for short) has shown promising results on many downstream tasks such as node classification. However, there are two major limitations of DGI. Firstly, DGI merely considers the extrinsic supervision signal (i.e., the mutual information between node embedding and global summary) while ignores the intrinsic signal (i.e., the mutual dependence between node embedding and node attributes). Secondly, nodes in a real-world network are usually connected by multiple edges with different relations, while DGI does not fully explore the various relations among nodes. To address the above-mentioned problems, we propose a novel framework, called High-order Deep Multiplex Infomax (HDMI), for learning node embedding on multiplex networks in a self-supervised way. To be more specific, we first design a joint supervision signal containing both extrinsic and intrinsic mutual information by high-order mutual information, and we propose a High-order Deep Infomax (HDI) to optimize the proposed supervision signal. Then we propose an attention based fusion module to combine node embedding from different layers of the multiplex network. Finally, we evaluate the proposed HDMI on various downstream tasks such as unsupervised clustering and supervised classification. The experimental results show that HDMI achieves state-of-the-art performance on these tasks.",0
"Learning embedding for networks has received significant research attention due to their wide use in representing the relations between objects, such as academic and social networks. Self-supervised network representation learning aims to extract node embedding without external supervision. However, the popular technique, Deep Graph Infomax (DGI), has two major limitations: it only considers extrinsic supervision signals and does not explore the various relations among nodes in real-world networks. To address these issues, we propose a novel framework, High-order Deep Multiplex Infomax (HDMI), for learning node embedding on multiplex networks in a self-supervised way. Our approach utilizes a joint supervision signal, which contains both extrinsic and intrinsic mutual information by high-order mutual information, and we propose a High-order Deep Infomax (HDI) to optimize the proposed supervision signal. Moreover, we introduce an attention-based fusion module to combine node embedding from different layers of the multiplex network. The evaluation results show that the proposed HDMI achieves state-of-the-art performance on various downstream tasks, such as unsupervised clustering and supervised classification.",1
"Automatically evaluating the quality of image captions can be very challenging since human language is quite flexible that there can be various expressions for the same meaning. Most of the current captioning metrics rely on token level matching between candidate caption and the ground truth label sentences. It usually neglects the sentence-level information. Motivated by the auto-encoder mechanism and contrastive representation learning advances, we propose a learning-based metric for image captioning, which we call Intrinsic Image Captioning Evaluation($I^2CE$). We develop three progressive model structures to learn the sentence level representations--single branch model, dual branches model, and triple branches model. Our empirical tests show that $I^2CE$ trained with dual branches structure achieves better consistency with human judgments to contemporary image captioning evaluation metrics. Furthermore, We select several state-of-the-art image captioning models and test their performances on the MS COCO dataset concerning both contemporary metrics and the proposed $I^2CE$. Experiment results show that our proposed method can align well with the scores generated from other contemporary metrics. On this concern, the proposed metric could serve as a novel indicator of the intrinsic information between captions, which may be complementary to the existing ones.",0
"Evaluating the quality of image captions automatically can be quite difficult due to the flexibility of human language, which allows for different expressions to convey the same meaning. Current captioning metrics mainly rely on token level matching between candidate captions and the ground truth labels, overlooking the sentence-level information. Our proposed Intrinsic Image Captioning Evaluation ($I^2CE$) is a learning-based metric for image captioning inspired by auto-encoder mechanisms and contrastive representation learning. We developed three model structures to learn sentence-level representations: single branch, dual branches, and triple branches. Our experiments demonstrate that $I^2CE$ with the dual branches structure achieves higher consistency with human judgments than contemporary image captioning evaluation metrics. We also tested several state-of-the-art image captioning models on the MS COCO dataset using both contemporary metrics and $I^2CE$. The results show that our proposed method aligns well with scores generated from other metrics, making it a novel indicator of intrinsic information between captions that could complement existing metrics.",1
"Visual recognition tasks are often limited to dealing with a small subset of classes simply because the labels for the remaining classes are unavailable. We are interested in identifying novel concepts in a dataset through representation learning based on the examples in both labeled and unlabeled classes, and extending the horizon of recognition to both known and novel classes. To address this challenging task, we propose a combinatorial learning approach, which naturally clusters the examples in unseen classes using the compositional knowledge given by multiple supervised meta-classifiers on heterogeneous label spaces. We also introduce a metric learning strategy to estimate pairwise pseudo-labels for improving representations of unlabeled examples, which preserves semantic relations across known and novel classes effectively. The proposed algorithm discovers novel concepts via a joint optimization of enhancing the discrimitiveness of unseen classes as well as learning the representations of known classes generalizable to novel ones. Our extensive experiments demonstrate remarkable performance gains by the proposed approach in multiple image retrieval and novel class discovery benchmarks.",0
"The recognition of visual tasks is typically limited to a small portion of classes due to the lack of labels for other classes. We aim to identify new concepts in a dataset through representation learning by using examples from both labeled and unlabeled classes, expanding the scope of recognition to include both known and unknown classes. To tackle this challenge, we suggest a combinatorial learning method that clusters examples in unknown classes using the compositional knowledge provided by multiple supervised meta-classifiers on diverse label spaces. We also introduce a metric learning approach that estimates pairwise pseudo-labels to enhance the representations of unlabeled examples, preserving semantic relationships between known and unknown classes effectively. The proposed algorithm optimizes both the discriminative abilities of unknown classes and the generalizability of representations of known classes to novel ones. Our extensive experiments have shown that the proposed method performs exceptionally well in multiple image retrieval and novel class discovery benchmarks.",1
"Recent work on graph generative models has made remarkable progress towards generating increasingly realistic graphs, as measured by global graph features such as degree distribution, density, and clustering coefficients. Deep generative models have also made significant advances through better modelling of the local correlations in the graph topology, which have been very useful for predicting unobserved graph components, such as the existence of a link or the class of a node, from nearby observed graph components. A complete scientific understanding of graph data should address both global and local structure. In this paper, we propose a joint model for both as complementary objectives in a graph VAE framework. Global structure is captured by incorporating graph kernels in a probabilistic model whose loss function is closely related to the maximum mean discrepancy(MMD) between the global structures of the reconstructed and the input graphs. The ELBO objective derived from the model regularizes a standard local link reconstruction term with an MMD term. Our experiments demonstrate a significant improvement in the realism of the generated graph structures, typically by 1-2 orders of magnitude of graph structure metrics, compared to leading graph VAEand GAN models. Local link reconstruction improves as well in many cases.",0
"There has been impressive progress in generating realistic graphs using graph generative models, which have focused on global graph features such as degree distribution, density, and clustering coefficients. Deep generative models have also improved by modeling local correlations in the graph topology to predict unobserved graph components. However, a complete understanding of graph data requires considering both global and local structure. This paper proposes a joint model in a graph VAE framework that incorporates graph kernels to capture global structure and uses the maximum mean discrepancy (MMD) to measure the similarity between the reconstructed and input graphs. The ELBO objective regularizes local link reconstruction with the MMD term, resulting in significantly more realistic generated graph structures compared to leading graph VAE and GAN models. Local link reconstruction also improves in many cases.",1
"Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domain-specific and little has been done to leverage this technique on real-world tabular datasets. We propose SCARF, a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, SCARF not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. We show that SCARF complements existing strategies and outperforms alternatives like autoencoders. We conduct comprehensive ablations, detailing the importance of a range of factors.",0
"The success of self-supervised contrastive representation learning in vision and natural language domains is well-established, with remarkable achievements in state-of-the-art performance using significantly less labeled data. However, the application of this technique to real-world tabular datasets remains limited, and domain-specific. This paper introduces SCARF, a simple and widely applicable approach to contrastive learning where views are created by corrupting a random subset of features. The effectiveness of SCARF is demonstrated through pre-training deep neural networks on 69 tabular classification datasets from OpenML-CC18. In addition to improving classification accuracy in fully-supervised settings, SCARF also proves effective in the presence of label noise and in the semi-supervised setting where only a portion of training data is labeled. SCARF is shown to complement existing strategies and outperform alternatives like autoencoders, with a comprehensive ablation analysis conducted to highlight the importance of various factors.",1
"Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models were evaluated on different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of objects. To close this gap, we design a benchmark with four data sets of varying complexity and seven additional test sets featuring challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four object-centric approaches: ViMON, a video-extension of MONet, based on recurrent spatial attention, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use explicit factorization via spatial transformers. Our results suggest that the architectures with unconstrained latent representations learn more powerful representations in terms of object detection, segmentation and tracking than the spatial transformer based architectures. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.",0
"The ability to perceive objects and follow them over time is essential for reasoning and understanding scenes. Various unsupervised learning methods have been proposed for object-centric representations, but it remains unclear how they compare in basic perceptual skills such as object detection, figure-ground segmentation, and object tracking. To address this gap, we created a benchmark consisting of four datasets of varying complexity and seven additional test sets with challenging tracking scenarios relevant to natural videos. We evaluated four object-centric approaches: ViMON, based on recurrent spatial attention, OP3, which uses clustering via spatial mixture models, TBA, and SCALOR, which utilize explicit factorization via spatial transformers. Our findings indicate that unconstrained latent representation architectures perform better in terms of object detection, segmentation, and tracking than spatial transformer-based ones. However, none of the methods can handle the most challenging tracking scenarios, even though they are synthetic, suggesting that our benchmark can help in developing more robust object-centric video representations.",1
"Contrastive learning (CL) is effective in learning data representations without label supervision, where the encoder needs to contrast each positive sample over multiple negative samples via a one-vs-many softmax cross-entropy loss. However, conventional CL is sensitive to how many negative samples are included and how they are selected. Proposed in this paper is a doubly CL strategy that contrasts positive samples and negative ones within themselves separately. We realize this strategy with contrastive attraction and contrastive repulsion (CACR) makes the query not only exert a greater force to attract more distant positive samples but also do so to repel closer negative samples. Theoretical analysis reveals the connection between CACR and CL from the perspectives of both positive attraction and negative repulsion and shows the benefits in both efficiency and robustness brought by separately contrasting within the sampled positive and negative pairs. Extensive large-scale experiments on standard vision tasks show that CACR not only consistently outperforms existing CL methods on benchmark datasets in representation learning, but also provides interpretable contrastive weights, demonstrating the efficacy of the proposed doubly contrastive strategy.",0
"The utilization of Contrastive Learning (CL) allows for the learning of data representations without the need for label supervision. The encoder accomplishes this by contrasting each positive sample with multiple negative samples via a one-vs-many softmax cross-entropy loss. However, traditional CL is sensitive to the inclusion and selection of negative samples. This paper introduces a new approach, the Doubly Contrastive Learning (DCL) strategy, which separately contrasts positive and negative samples. The Contrastive Attraction and Contrastive Repulsion (CACR) technique is utilized in DCL to attract more distant positive samples while repelling closer negative ones. Theoretical analysis shows the connection between CACR and CL in terms of positive attraction and negative repulsion. The proposed strategy demonstrates improved efficiency and robustness by separately contrasting within sampled positive and negative pairs. Large-scale experiments on vision tasks prove that CACR consistently outperforms existing CL methods and provides interpretable contrastive weights, highlighting the effectiveness of the DCL approach.",1
"Confounding bias is a crucial problem when applying machine learning to practice, especially in clinical practice. We consider the problem of learning representations independent to multiple biases. In literature, this is mostly solved by purging the bias information from learned representations. We however expect this strategy to harm the diversity of information in the representation, and thus limiting its prospective usage (e.g., interpretation). Therefore, we propose to mitigate the bias while keeping almost all information in the latent representations, which enables us to observe and interpret them as well. To achieve this, we project latent features onto a learned vector direction, and enforce the independence between biases and projected features rather than all learned features. To interpret the mapping between projected features and input data, we propose projection-wise disentangling: a sampling and reconstruction along the learned vector direction. The proposed method was evaluated on the analysis of 3D facial shape and patient characteristics (N=5011). Experiments showed that this conceptually simple method achieved state-of-the-art fair prediction performance and interpretability, showing its great potential for clinical applications.",0
"When applying machine learning in clinical practice, confounding bias is a significant issue. To address the problem of learning independent representations in the face of multiple biases, previous solutions have focused on removing bias information from learned representations. However, we believe that this strategy reduces the diversity of information in the representation, limiting its potential usage and interpretation. Instead, we propose mitigating bias while retaining almost all information in the latent representations, allowing for observation and interpretation. Our approach involves projecting latent features onto a learned vector direction, enforcing independence between biases and projected features rather than all learned features. To interpret the mapping between projected features and input data, we introduce projection-wise disentangling, which involves sampling and reconstruction along the learned vector direction. We evaluated our method on the analysis of 3D facial shape and patient characteristics (N=5011), and found that it achieved state-of-the-art fair prediction performance and interpretability, demonstrating its potential for clinical applications.",1
"Low-prior targets are common among many important clinical events, which introduces the challenge of having enough data to support learning of their predictive models. Many prior works have addressed this problem by first building a general patient-state representation model, and then adapting it to a new low-prior prediction target. In this schema, there is potential for the predictive performance to be hindered by the misalignment between the general patient-state model and the target task. To overcome this challenge, we propose a new method that simultaneously optimizes a shared model through multi-task learning of both the low-prior supervised target and general purpose patient-state representation (GPSR). More specifically, our method improves prediction performance of a low-prior task by jointly optimizing a shared model that combines the loss of the target event and a broad range of generic clinical events. We study the approach in the context of Recurrent Neural Networks (RNNs). Through extensive experiments on multiple clinical event targets using MIMIC-III data, we show that the inclusion of general patient-state representation tasks during model training improves the prediction of individual low-prior targets.",0
"It is a common challenge to have insufficient data to support learning of predictive models for low-priority clinical events. Previous studies have attempted to address this by creating a general patient-state representation model and adapting it to the new low-priority target. However, this approach may hinder predictive performance due to misalignment between the general model and the target task. To overcome this, we propose a new method that simultaneously optimizes a shared model through multi-task learning of both the low-priority target and general purpose patient-state representation. Our approach improves prediction performance by combining the loss of the target event with a range of generic clinical events. We test our approach using Recurrent Neural Networks and MIMIC-III data, and demonstrate that including general patient-state representation tasks during model training enhances the prediction of individual low-priority targets.",1
"Representations of the world environment play a crucial role in machine intelligence. It is often inefficient to conduct reasoning and inference directly in the space of raw sensory representations, such as pixel values of images. Representation learning allows us to automatically discover suitable representations from raw sensory data. For example, given raw sensory data, a multilayer perceptron learns nonlinear representations at its hidden layers, which are subsequently used for classification (or regression) at its output layer. This happens implicitly during training through minimizing a supervised or unsupervised loss. In this paper, we study the dynamics of such implicit nonlinear representation learning. We identify a pair of a new assumption and a novel condition, called the common model structure assumption and the data-architecture alignment condition. Under the common model structure assumption, the data-architecture alignment condition is shown to be sufficient for the global convergence and necessary for the global optimality. Our results provide practical guidance for designing a model structure: e.g., the common model structure assumption can be used as a justification for using a particular model structure instead of others. As an application, we then derive a new training framework, which satisfies the data-architecture alignment condition without assuming it by automatically modifying any given training algorithm dependently on each data and architecture. Given a standard training algorithm, the framework running its modified version is empirically shown to maintain competitive (practical) test performances while providing global convergence guarantees for ResNet-18 with convolutions, skip connections, and batch normalization with standard benchmark datasets, including MNIST, CIFAR-10, CIFAR-100, Semeion, KMNIST and SVHN.",0
"Machine intelligence heavily relies on representations of the world environment. Directly reasoning and inferring using raw sensory representations, such as pixel values of images, is often impractical. To address this, representation learning is used to automatically discover suitable representations from raw sensory data. For instance, a multilayer perceptron can learn nonlinear representations at its hidden layers using raw sensory data, which are subsequently utilized for classification or regression at its output layer. In this paper, we investigate the implicit nonlinear representation learning dynamics. We introduce a new assumption and condition, called the common model structure assumption and the data-architecture alignment condition, respectively. Under the common model structure assumption, the data-architecture alignment condition is sufficient for global convergence and necessary for global optimality. Our findings offer practical insights for designing a model structure and present a new training framework that modifies any given training algorithm to satisfy the data-architecture alignment condition. Empirical tests show that our framework maintains competitive test performances while ensuring global convergence guarantees for ResNet-18 with convolutions, skip connections, and batch normalization using standard benchmark datasets, including MNIST, CIFAR-10, CIFAR-100, Semeion, KMNIST, and SVHN.",1
"For an image query, unsupervised contrastive learning labels crops of the same image as positives, and other image crops as negatives. Although intuitive, such a native label assignment strategy cannot reveal the underlying semantic similarity between a query and its positives and negatives, and impairs performance, since some negatives are semantically similar to the query or even share the same semantic class as the query. In this work, we first prove that for contrastive learning, inaccurate label assignment heavily impairs its generalization for semantic instance discrimination, while accurate labels benefit its generalization. Inspired by this theory, we propose a novel self-labeling refinement approach for contrastive learning. It improves the label quality via two complementary modules: (i) self-labeling refinery (SLR) to generate accurate labels and (ii) momentum mixup (MM) to enhance similarity between query and its positive. SLR uses a positive of a query to estimate semantic similarity between a query and its positive and negatives, and combines estimated similarity with vanilla label assignment in contrastive learning to iteratively generate more accurate and informative soft labels. We theoretically show that our SLR can exactly recover the true semantic labels of label-corrupted data, and supervises networks to achieve zero prediction error on classification tasks. MM randomly combines queries and positives to increase semantic similarity between the generated virtual queries and their positives so as to improves label accuracy. Experimental results on CIFAR10, ImageNet, VOC and COCO show the effectiveness of our method. PyTorch code and model will be released online.",0
"The typical approach to labeling image queries in unsupervised contrastive learning designates crops of the same image as positives and other image crops as negatives. However, this strategy lacks the ability to reveal the underlying semantic similarity between a query and its respective positives and negatives, which can negatively impact performance. In some cases, negatives may share the same semantic class as a query, resulting in inaccurate labeling. Our research has demonstrated that accurate labeling is crucial for successful semantic instance discrimination in contrastive learning. Therefore, we present a new self-labeling refinement approach that improves label quality through two modules: the self-labeling refinery (SLR) and momentum mixup (MM). The SLR utilizes a query's positive to estimate semantic similarity and generate more accurate and informative soft labels. Our SLR is capable of recovering true semantic labels of corrupted data and supervising networks to achieve zero prediction error for classification tasks. The MM randomly combines queries and positives to increase semantic similarity and improve label accuracy. We tested our approach on CIFAR10, ImageNet, VOC, and COCO, and the results demonstrate its effectiveness. Our PyTorch code and model will be publicly available online.",1
"Hyperbolic space has become a popular choice of manifold for representation learning of arbitrary data, from tree-like structures and text to graphs. Building on the success of deep learning with prototypes in Euclidean and hyperspherical spaces, a few recent works have proposed hyperbolic prototypes for classification. Such approaches enable effective learning in low-dimensional output spaces and can exploit hierarchical relations amongst classes, but require privileged information about class labels to position the hyperbolic prototypes. In this work, we propose Hyperbolic Busemann Learning. The main idea behind our approach is to position prototypes on the ideal boundary of the Poincare ball, which does not require prior label knowledge. To be able to compute proximities to ideal prototypes, we introduce the penalised Busemann loss. We provide theory supporting the use of ideal prototypes and the proposed loss by proving its equivalence to logistic regression in the one-dimensional case. Empirically, we show that our approach provides a natural interpretation of classification confidence, while outperforming recent hyperspherical and hyperbolic prototype approaches.",0
"Hyperbolic space has gained popularity as a manifold for representation learning of various data types such as tree-like structures, text, and graphs. Recently, some studies have suggested hyperbolic prototypes for classification, following the success of deep learning with prototypes in Euclidean and hyperspherical spaces. Although these approaches enable effective learning in low-dimensional output spaces and can utilize hierarchical relations among classes, they require prior knowledge of class labels to position the hyperbolic prototypes. In this study, we introduce Hyperbolic Busemann Learning, which involves positioning prototypes on the ideal boundary of the Poincare ball, eliminating the need for prior label knowledge. We also introduce the penalized Busemann loss to compute proximities to ideal prototypes. We support the use of ideal prototypes and the proposed loss by proving its equivalence to logistic regression in the one-dimensional case. Our empirical results indicate that our approach provides a natural interpretation of classification confidence and outperforms recent hyperspherical and hyperbolic prototype approaches.",1
"In representation learning on the graph-structured data, under heterophily (or low homophily), many popular GNNs may fail to capture long-range dependencies, which leads to their performance degradation. To solve the above-mentioned issue, we propose a graph convolutional networks with structure learning (GCN-SL), and furthermore, the proposed approach can be applied to node classification. The proposed GCN-SL contains two improvements: corresponding to node features and edges, respectively. In the aspect of node features, we propose an efficient-spectral-clustering (ESC) and an ESC with anchors (ESC-ANCH) algorithms to efficiently aggregate feature representations from all similar nodes. In the aspect of edges, we build a re-connected adjacency matrix by using a special data preprocessing technique and similarity learning, and the re-connected adjacency matrix can be optimized directly along with GCN-SL parameters. Considering that the original adjacency matrix may provide misleading information for aggregation in GCN, especially the graphs being with a low level of homophily. The proposed GCN-SL can aggregate feature representations from nearby nodes via re-connected adjacency matrix and is applied to graphs with various levels of homophily. Experimental results on a wide range of benchmark datasets illustrate that the proposed GCN-SL outperforms the stateof-the-art GNN counterparts.",0
"When dealing with graph-structured data, heterophily (or low homophily) can cause popular GNNs to struggle with capturing long-range dependencies, leading to performance degradation. Our solution is GCN-SL, a graph convolutional network with structure learning that can be used for node classification. GCN-SL has two improvements: one for node features and one for edges. To efficiently aggregate feature representations from similar nodes, we propose ESC and ESC-ANCH algorithms. To optimize the re-connected adjacency matrix, we use a special data preprocessing technique and similarity learning. The original adjacency matrix can be misleading, especially for graphs with low homophily. GCN-SL uses the re-connected adjacency matrix to aggregate feature representations from nearby nodes and can be used with various levels of homophily. Experimental results on benchmark datasets show that GCN-SL outperforms state-of-the-art GNNs.",1
"Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable, transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged with promising representation learning performance. Unfortunately, unlike its counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data augmentations, which have to be manually picked per dataset, by either rules of thumb or trial-and-errors, owing to the diverse nature of graph data. That significantly limits the more general applicability of GraphCL. Aiming to fill in this crucial gap, this paper proposes a unified bi-level optimization framework to automatically, adaptively and dynamically select data augmentations when performing GraphCL on specific graph data. The general framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as min-max optimization. The selections of augmentations made by JOAO are shown to be in general aligned with previous ""best practices"" observed from handcrafted tuning: yet now being automated, more flexible and versatile. Moreover, we propose a new augmentation-aware projection head mechanism, which will route output features through different projection heads corresponding to different augmentations chosen at each training step. Extensive experiments demonstrate that JOAO performs on par with or sometimes better than the state-of-the-art competitors including GraphCL, on multiple graph datasets of various scales and types, yet without resorting to any laborious dataset-specific tuning on augmentation selection. We release the code at https://github.com/Shen-Lab/GraphCL_Automated.",0
"Recently, there has been a growing interest in self-supervised learning on graph-structured data for creating generalizable, transferable, and robust representations from unlabeled graphs. One promising approach is graph contrastive learning (GraphCL). However, unlike its counterpart on image data, GraphCL's effectiveness depends on manually selecting data augmentations tailored to each dataset, which limits its general applicability. To address this issue, this paper proposes a unified bi-level optimization framework called JOint Augmentation Optimization (JOAO) that automatically, adaptively, and dynamically selects data augmentations for GraphCL on specific graph data. JOAO's augmentations align with the best practices observed from handcrafted tuning but are now automated, more flexible, and versatile. Additionally, the paper proposes a new augmentation-aware projection head mechanism that routes output features through different projection heads corresponding to different augmentations chosen at each training step. Extensive experiments show that JOAO performs on par with or better than state-of-the-art competitors, including GraphCL, on various graph datasets without requiring dataset-specific tuning on augmentation selection. The code is available at https://github.com/Shen-Lab/GraphCL_Automated.",1
"We consider the problem of interpretable network representation learning for samples of network-valued data. We propose the Principal Component Analysis for Networks (PCAN) algorithm to identify statistically meaningful low-dimensional representations of a network sample via subgraph count statistics. The PCAN procedure provides an interpretable framework for which one can readily visualize, explore, and formulate predictive models for network samples. We furthermore introduce a fast sampling-based algorithm, sPCAN, which is significantly more computationally efficient than its counterpart, but still enjoys advantages of interpretability. We investigate the relationship between these two methods and analyze their large-sample properties under the common regime where the sample of networks is a collection of kernel-based random graphs. We show that under this regime, the embeddings of the sPCAN method enjoy a central limit theorem and moreover that the population level embeddings of PCAN and sPCAN are equivalent. We assess PCAN's ability to visualize, cluster, and classify observations in network samples arising in nature, including functional connectivity network samples and dynamic networks describing the political co-voting habits of the U.S. Senate. Our analyses reveal that our proposed algorithm provides informative and discriminatory features describing the networks in each sample. The PCAN and sPCAN methods build on the current literature of network representation learning and set the stage for a new line of research in interpretable learning on network-valued data. Publicly available software for the PCAN and sPCAN methods are available at https://www.github.com/jihuilee/.",0
"In this study, we address the issue of achieving interpretable network representation learning for network-valued data samples. We introduce the Principal Component Analysis for Networks (PCAN) algorithm, which utilizes subgraph count statistics to identify low-dimensional representations that are statistically significant. This approach offers an interpretable framework that allows for easy visualization, exploration, and predictive modeling of network samples. To improve computational efficiency, we also present sPCAN, a fast sampling-based algorithm that retains the benefits of interpretability. We examine the relationship between these two methods and analyze their properties under the assumption that the samples of networks are kernel-based random graphs. Our findings show that sPCAN's embeddings follow a central limit theorem, and that the population level embeddings of PCAN and sPCAN are equivalent. We demonstrate the effectiveness of PCAN in visualizing, clustering, and classifying observations in various network samples, including functional connectivity networks and political co-voting dynamic networks of the U.S. Senate. Our results indicate that the proposed algorithms offer informative and discriminative features for describing the networks in each sample. These methods contribute to the field of network representation learning, providing a new avenue for interpretable learning in network-valued data. The PCAN and sPCAN methods are publicly available on https://www.github.com/jihuilee/.",1
"Facial Expression Recognition (FER) is a classification task that points to face variants. Hence, there are certain affinity features between facial expressions, receiving little attention in the FER literature. Convolution padding, despite helping capture the edge information, causes erosion of the feature map simultaneously. After multi-layer filling convolution, the output feature map named albino feature definitely weakens the representation of the expression. To tackle these challenges, we propose a novel architecture named Amending Representation Module (ARM). ARM is a substitute for the pooling layer. Theoretically, it can be embedded in the back end of any network to deal with the Padding Erosion. ARM efficiently enhances facial expression representation from two different directions: 1) reducing the weight of eroded features to offset the side effect of padding, and 2) sharing affinity features over mini-batch to strengthen the representation learning. Experiments on public benchmarks prove that our ARM boosts the performance of FER remarkably. The validation accuracies are respectively 92.05% on RAF-DB, 65.2% on Affect-Net, and 58.71% on SFEW, exceeding current state-of-the-art methods. Our implementation and trained models are available at https://github.com/JiaweiShiCV/Amend-Representation-Module.",0
"The task of Facial Expression Recognition (FER) involves identifying different facial variations, but the existing literature on FER has overlooked certain important facial expression features. Although convolution padding helps in capturing edge information, it also leads to erosion of the feature map. Additionally, the multi-layer filling convolution results in a weakened representation of the expression, known as albino feature. To overcome these challenges, we propose a novel architecture called Amending Representation Module (ARM), which can be integrated into the back end of any network to address Padding Erosion. ARM enhances facial expression representation by reducing the weight of eroded features and sharing affinity features over mini-batch to improve representation learning. Our experiments on public benchmarks demonstrate that ARM significantly improves FER performance, achieving validation accuracies of 92.05% on RAF-DB, 65.2% on Affect-Net, and 58.71% on SFEW, surpassing current state-of-the-art methods. Our implementation and trained models can be accessed at https://github.com/JiaweiShiCV/Amend-Representation-Module.",1
"Learning decent representations from unlabeled time-series data with temporal dynamics is a very challenging task. In this paper, we propose an unsupervised Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC), to learn time-series representation from unlabeled data. First, the raw time-series data are transformed into two different yet correlated views by using weak and strong augmentations. Second, we propose a novel temporal contrasting module to learn robust temporal representations by designing a tough cross-view prediction task. Last, to further learn discriminative representations, we propose a contextual contrasting module built upon the contexts from the temporal contrasting module. It attempts to maximize the similarity among different contexts of the same sample while minimizing similarity among contexts of different samples. Experiments have been carried out on three real-world time-series datasets. The results manifest that training a linear classifier on top of the features learned by our proposed TS-TCC performs comparably with the supervised training. Additionally, our proposed TS-TCC shows high efficiency in few-labeled data and transfer learning scenarios. The code is publicly available at https://github.com/emadeldeen24/TS-TCC.",0
"Developing adequate representations from unlabeled time-series data that incorporate temporal dynamics is an arduous undertaking. This study introduces an unsupervised framework for Time-Series representation learning via Temporal and Contextual Contrasting (TS-TCC) to acquire time-series representations from unlabelled data. Initially, raw time-series data are transformed into two distinct yet interrelated perspectives by employing weak and strong augmentations. Next, the authors propose a new temporal contrasting component to learn robust temporal representations through a challenging cross-view prediction task. Lastly, to further enhance discriminative representations, they propose a contextual contrasting component based on the temporal contrasting module's contexts. This component attempts to increase the similarity among different contexts of the same sample while minimizing the similarity among contexts of different samples. The authors conducted experiments on three real-world time-series datasets and found that training a linear classifier using the features learned by the TS-TCC framework performed comparably to supervised training. In addition, the proposed TS-TCC framework exhibited high efficiency in few-labeled data and transfer learning scenarios. The code is publicly accessible at https://github.com/emadeldeen24/TS-TCC.",1
"In vision-based reinforcement learning (RL) tasks, it is prevalent to assign the auxiliary task with a surrogate self-supervised loss so as to obtain more semantic representations and improve sample efficiency. However, abundant information in self-supervised auxiliary tasks has been disregarded, since the representation learning part and the decision-making part are separated. To sufficiently utilize information in the auxiliary task, we present a simple yet effective idea to employ self-supervised loss as an intrinsic reward, called Intrinsically Motivated Self-Supervised learning in Reinforcement learning (IM-SSR). We formally show that the self-supervised loss can be decomposed as exploration for novel states and robustness improvement from nuisance elimination. IM-SSR can be effortlessly plugged into any reinforcement learning with self-supervised auxiliary objectives with nearly no additional cost. Combined with IM-SSR, the previous underlying algorithms achieve salient improvements on both sample efficiency and generalization in various vision-based robotics tasks from the DeepMind Control Suite, especially when the reward signal is sparse.",0
"A common practice in vision-based reinforcement learning tasks is to use a surrogate self-supervised loss in the auxiliary task to improve semantic representations and sample efficiency. However, this approach ignores valuable information in the self-supervised auxiliary task since the representation learning and decision-making components are separate. To address this issue, we propose the use of self-supervised loss as an intrinsic reward, which we call Intrinsically Motivated Self-Supervised learning in Reinforcement learning (IM-SSR). We demonstrate that the self-supervised loss can be decomposed into exploration for novel states and robustness improvement from nuisance elimination. IM-SSR can be easily incorporated into any reinforcement learning framework with self-supervised auxiliary objectives at a minimal additional cost. When combined with IM-SSR, existing algorithms show significant improvements in sample efficiency and generalization in multiple vision-based robotics tasks from the DeepMind Control Suite, especially when the reward signal is sparse.",1
"Machine learning analysis of longitudinal neuroimaging data is typically based on supervised learning, which requires a large number of ground-truth labels to be informative. As ground-truth labels are often missing or expensive to obtain in neuroscience, we avoid them in our analysis by combing factor disentanglement with self-supervised learning to identify changes and consistencies across the multiple MRIs acquired of each individual over time. Specifically, we propose a new definition of disentanglement by formulating a multivariate mapping between factors (e.g., brain age) associated with an MRI and a latent image representation. Then, factors that evolve across acquisitions of longitudinal sequences are disentangled from that mapping by self-supervised learning in such a way that changes in a single factor induce change along one direction in the representation space. We implement this model, named Longitudinal Self-Supervised Learning (LSSL), via a standard autoencoding structure with a cosine loss to disentangle brain age from the image representation. We apply LSSL to two longitudinal neuroimaging studies to highlight its strength in extracting the brain-age information from MRI and revealing informative characteristics associated with neurodegenerative and neuropsychological disorders. Moreover, the representations learned by LSSL facilitate supervised classification by recording faster convergence and higher (or similar) prediction accuracy compared to several other representation learning techniques.",0
"In typical machine learning analysis of longitudinal neuroimaging data, supervised learning is utilized. However, this approach requires a large number of ground-truth labels to be informative, which can often be missing or expensive to obtain in neuroscience. To address this issue, we propose a new method that combines factor disentanglement with self-supervised learning to identify changes and consistencies across multiple MRIs acquired over time for each individual. Our approach involves formulating a multivariate mapping between factors associated with an MRI and a latent image representation, and disentangling factors that evolve across acquisitions of longitudinal sequences from that mapping by self-supervised learning. This enables changes in a single factor to induce change along one direction in the representation space. We implement this model, called Longitudinal Self-Supervised Learning (LSSL), using a standard autoencoding structure with a cosine loss to disentangle brain age from the image representation. By applying LSSL to two longitudinal neuroimaging studies, we demonstrate its ability to extract brain-age information from MRI and reveal informative characteristics associated with neurodegenerative and neuropsychological disorders. Additionally, the representations learned by LSSL facilitate supervised classification with faster convergence and higher or similar prediction accuracy compared to several other representation learning techniques.",1
"This work improves the quality of automated machine learning (AutoML) systems by using dataset and function descriptions while significantly decreasing computation time from minutes to milliseconds by using a zero-shot approach. Given a new dataset and a well-defined machine learning task, humans begin by reading a description of the dataset and documentation for the algorithms to be used. This work is the first to use these textual descriptions, which we call privileged information, for AutoML. We use a pre-trained Transformer model to process the privileged text and demonstrate that using this information improves AutoML performance. Thus, our approach leverages the progress of unsupervised representation learning in natural language processing to provide a significant boost to AutoML. We demonstrate that using only textual descriptions of the data and functions achieves reasonable classification performance, and adding textual descriptions to data meta-features improves classification across tabular datasets. To achieve zero-shot AutoML we train a graph neural network with these description embeddings and the data meta-features. Each node represents a training dataset, which we use to predict the best machine learning pipeline for a new test dataset in a zero-shot fashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a supervised learning task and dataset. In contrast, most AutoML systems require tens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces running and prediction times from minutes to milliseconds, consistently across datasets. By speeding up AutoML by orders of magnitude this work demonstrates real-time AutoML.",0
"This study enhances the effectiveness of automated machine learning (AutoML) systems by utilizing dataset and function descriptions, while also significantly reducing computation time from minutes to milliseconds through a zero-shot method. Typically, when presented with a new dataset and machine learning objective, humans review the dataset description and algorithm documentation to begin the process. However, this study introduces a novel approach by incorporating these textual descriptions as ""privileged information"" for AutoML. By leveraging pre-trained Transformer models to process this information, AutoML performance is improved. The study proves that reasonable classification performance can be achieved simply by using textual descriptions of the data and functions, and classification can be further improved by adding these descriptions to data meta-features for tabular datasets. To achieve zero-shot AutoML, the authors train a graph neural network using description embeddings and data meta-features. This network predicts the best machine learning pipeline for a new test dataset, resulting in a high-quality pipeline for a supervised learning task and dataset within milliseconds. This zero-shot approach drastically reduces the time required for AutoML, making real-time AutoML a reality.",1
"We introduce the Graph Mixture Density Networks, a new family of machine learning models that can fit multimodal output distributions conditioned on graphs of arbitrary topology. By combining ideas from mixture models and graph representation learning, we address a broader class of challenging conditional density estimation problems that rely on structured data. In this respect, we evaluate our method on a new benchmark application that leverages random graphs for stochastic epidemic simulations. We show a significant improvement in the likelihood of epidemic outcomes when taking into account both multimodality and structure. The empirical analysis is complemented by two real-world regression tasks showing the effectiveness of our approach in modeling the output prediction uncertainty. Graph Mixture Density Networks open appealing research opportunities in the study of structure-dependent phenomena that exhibit non-trivial conditional output distributions.",0
"We have developed the Graph Mixture Density Networks, which are a novel set of machine learning models capable of fitting multimodal output distributions based on graphs of any topology. By integrating concepts from mixture models and graph representation learning, we can solve a wider range of difficult conditional density estimation problems that rely on structured data. To demonstrate the effectiveness of our approach, we have tested it on a new benchmark application that uses random graphs for stochastic epidemic simulations. Our results show a significant improvement in the likelihood of epidemic outcomes by considering both multimodality and structure. Additionally, our method has proven successful in two real-world regression tasks, illustrating its ability to model output prediction uncertainty. The Graph Mixture Density Networks have the potential to open up exciting research opportunities in the study of structure-dependent phenomena with complex conditional output distributions.",1
"Recent contrastive representation learning methods rely on estimating mutual information (MI) between multiple views of an underlying context. E.g., we can derive multiple views of a given image by applying data augmentation, or we can split a sequence into views comprising the past and future of some step in the sequence. Contrastive lower bounds on MI are easy to optimize, but have a strong underestimation bias when estimating large amounts of MI. We propose decomposing the full MI estimation problem into a sum of smaller estimation problems by splitting one of the views into progressively more informed subviews and by applying the chain rule on MI between the decomposed views. This expression contains a sum of unconditional and conditional MI terms, each measuring modest chunks of the total MI, which facilitates approximation via contrastive bounds. To maximize the sum, we formulate a contrastive lower bound on the conditional MI which can be approximated efficiently. We refer to our general approach as Decomposed Estimation of Mutual Information (DEMI). We show that DEMI can capture a larger amount of MI than standard non-decomposed contrastive bounds in a synthetic setting, and learns better representations in a vision domain and for dialogue generation.",0
"Current methods for contrastive representation learning rely on estimating mutual information (MI) between multiple views of a context. For example, different views of an image can be obtained through data augmentation or by splitting a sequence into past and future views at a specific step. However, estimating large amounts of MI using contrastive lower bounds can result in significant underestimation bias. To address this issue, we propose Decomposed Estimation of Mutual Information (DEMI), which breaks down the full MI estimation problem into smaller estimation problems by splitting one view into progressively more informed subviews and applying the chain rule on MI between the decomposed views. This approach enables approximation via contrastive bounds and maximization of the sum using a contrastive lower bound on the conditional MI, which can be efficiently approximated. DEMI outperforms standard non-decomposed contrastive bounds in capturing MI and learning representations in both a synthetic setting and real-world applications such as vision and dialogue.",1
"Causality knowledge is vital to building robust AI systems. Deep learning models often perform poorly on tasks that require causal reasoning, which is often derived using some form of commonsense knowledge not immediately available in the input but implicitly inferred by humans. Prior work has unraveled spurious observational biases that models fall prey to in the absence of causality. While language representation models preserve contextual knowledge within learned embeddings, they do not factor in causal relationships during training. By blending causal relationships with the input features to an existing model that performs visual cognition tasks (such as scene understanding, video captioning, video question-answering, etc.), better performance can be achieved owing to the insight causal relationships bring about. Recently, several models have been proposed that have tackled the task of mining causal data from either the visual or textual modality. However, there does not exist widespread research that mines causal relationships by juxtaposing the visual and language modalities. While images offer a rich and easy-to-process resource for us to mine causality knowledge from, videos are denser and consist of naturally time-ordered events. Also, textual information offers details that could be implicit in videos. We propose iReason, a framework that infers visual-semantic commonsense knowledge using both videos and natural language captions. Furthermore, iReason's architecture integrates a causal rationalization module to aid the process of interpretability, error analysis and bias detection. We demonstrate the effectiveness of iReason using a two-pronged comparative analysis with language representation learning models (BERT, GPT-2) as well as current state-of-the-art multimodal causality models.",0
"Having knowledge of causality is crucial for developing resilient AI systems. Deep learning models often struggle with tasks that necessitate causal reasoning, which is usually derived from commonsense knowledge not immediately available in the input but inferred by humans. Previous research has identified observational biases that models fall prey to in the absence of causality. Though language representation models retain contextual knowledge in learned embeddings, they fail to factor in causal relationships during training. Incorporating causal relationships with input features to an existing model that performs visual cognition tasks can improve performance by providing insight into causal relationships. Several models have been proposed that mine causal data from either the visual or textual modality, but there is limited research that mines causal relationships by combining the visual and language modalities. Images offer a rich and simple resource to mine causality knowledge from, while videos are denser and consist of naturally time-ordered events. Textual information provides details that may be implicit in videos. We propose iReason, a framework that infers visual-semantic commonsense knowledge using both videos and natural language captions. Moreover, iReason's architecture includes a causal rationalization module to aid interpretability, error analysis, and bias detection. We demonstrate the effectiveness of iReason using a two-pronged comparative analysis with language representation learning models (BERT, GPT-2) and current state-of-the-art multimodal causality models.",1
"Machine learning models that incorporate concept learning as an intermediate step in their decision making process can match the performance of black-box predictive models while retaining the ability to explain outcomes in human understandable terms. However, we demonstrate that the concept representations learned by these models encode information beyond the pre-defined concepts, and that natural mitigation strategies do not fully work, rendering the interpretation of the downstream prediction misleading. We describe the mechanism underlying the information leakage and suggest recourse for mitigating its effects.",0
"Incorporating concept learning into machine learning models as an intermediate step in the decision-making process can yield performance on par with black-box predictive models, while still allowing for human-understandable explanations of outcomes. Nevertheless, our findings indicate that the concept representations acquired by these models contain information that goes beyond the predefined concepts. Consequently, typical approaches to minimize this issue are inadequate, resulting in inaccurate interpretations of downstream predictions. We explain the mechanism responsible for this information leakage and provide recommendations to mitigate its effects.",1
"Understanding the 3D world from 2D projected natural images is a fundamental challenge in computer vision and graphics. Recently, an unsupervised learning approach has garnered considerable attention owing to its advantages in data collection. However, to mitigate training limitations, typical methods need to impose assumptions for viewpoint distribution (e.g., a dataset containing various viewpoint images) or object shape (e.g., symmetric objects). These assumptions often restrict applications; for instance, the application to non-rigid objects or images captured from similar viewpoints (e.g., flower or bird images) remains a challenge. To complement these approaches, we propose aperture rendering generative adversarial networks (AR-GANs), which equip aperture rendering on top of GANs, and adopt focus cues to learn the depth and depth-of-field (DoF) effect of unlabeled natural images. To address the ambiguities triggered by unsupervised setting (i.e., ambiguities between smooth texture and out-of-focus blurs, and between foreground and background blurs), we develop DoF mixture learning, which enables the generator to learn real image distribution while generating diverse DoF images. In addition, we devise a center focus prior to guiding the learning direction. In the experiments, we demonstrate the effectiveness of AR-GANs in various datasets, such as flower, bird, and face images, demonstrate their portability by incorporating them into other 3D representation learning GANs, and validate their applicability in shallow DoF rendering.",0
"The task of comprehending the 3D environment based on 2D projected natural images poses a significant challenge in the fields of computer vision and graphics. An unsupervised learning approach has recently gained attention for its advantages in data collection. However, typical methods have to impose assumptions for viewpoint distribution or object shape to overcome training limitations. This often restricts their application to non-rigid objects or images captured from similar viewpoints. To complement these approaches, we propose aperture rendering generative adversarial networks (AR-GANs). These networks employ aperture rendering on top of GANs and adopt focus cues to learn the depth and depth-of-field (DoF) effect of unlabeled natural images. Our DoF mixture learning addresses the ambiguities caused by the unsupervised setting and enables the generator to learn real image distribution while generating diverse DoF images. We also develop a center focus prior to guide the learning direction. Our experiments demonstrate the effectiveness of AR-GANs in various datasets and their portability by incorporating them into other 3D representation learning GANs. We also validate their applicability in shallow DoF rendering.",1
"TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions. In this challenge, we use generative model T5 for TextVQA task. Based on pre-trained checkpoint T5-3B from HuggingFace repository, two other pre-training tasks including masked language modeling(MLM) and relative position prediction(RPP) are designed to better align object feature and scene text. In the stage of pre-training, encoder is dedicate to handle the fusion among multiple modalities: question text, object text labels, scene text labels, object visual features, scene visual features. After that decoder generates the text sequence step-by-step, cross entropy loss is required by default. We use a large-scale scene text dataset in pre-training and then fine-tune the T5-3B with the TextVQA dataset only.",0
"To answer questions about images, TextVQA models must read and analyze the text within them. This involves incorporating the text modality present in the images and reasoning over it. To accomplish this, the TextVQA challenge employs the T5 generative model. This model utilizes the pre-trained checkpoint T5-3B from the HuggingFace repository, which includes two pre-training tasks (masked language modeling and relative position prediction) to better align object features and scene text. During pre-training, the encoder is responsible for handling the fusion of multiple modalities, including question text, object text labels, scene text labels, object visual features, and scene visual features. The decoder then generates the text sequence, with cross entropy loss being required by default. To perform pre-training, a large-scale scene text dataset is used, followed by fine-tuning of the T5-3B with the TextVQA dataset exclusively.",1
"The claims data, containing medical codes, services information, and incurred expenditure, can be a good resource for estimating an individual's health condition and medical risk level. In this study, we developed Transformer-based Multimodal AutoEncoder (TMAE), an unsupervised learning framework that can learn efficient patient representation by encoding meaningful information from the claims data. TMAE is motivated by the practical needs in healthcare to stratify patients into different risk levels for improving care delivery and management. Compared to previous approaches, TMAE is able to 1) model inpatient, outpatient, and medication claims collectively, 2) handle irregular time intervals between medical events, 3) alleviate the sparsity issue of the rare medical codes, and 4) incorporate medical expenditure information. We trained TMAE using a real-world pediatric claims dataset containing more than 600,000 patients and compared its performance with various approaches in two clustering tasks. Experimental results demonstrate that TMAE has superior performance compared to all baselines. Multiple downstream applications are also conducted to illustrate the effectiveness of our framework. The promising results confirm that the TMAE framework is scalable to large claims data and is able to generate efficient patient embeddings for risk stratification and analysis.",0
"One can use claims data, which includes medical codes, service details, and incurred expenses, to estimate a person's health condition and medical risk level. To address the practical healthcare need of categorizing patients into different risk levels for better care management, we developed an unsupervised learning framework called Transformer-based Multimodal AutoEncoder (TMAE). This framework encodes meaningful information from claims data to learn efficient patient representation. TMAE can model inpatient, outpatient, and medication claims together, handle irregular time intervals between medical events, mitigate the sparsity issue of rare medical codes, and integrate medical expenditure data. We trained TMAE on a pediatric claims dataset with over 600,000 patients and compared its performance with various approaches in two clustering tasks. Our experimental results demonstrate that TMAE outperforms all baseline models and is effective for risk stratification and analysis. We also conducted multiple downstream applications to validate our framework's effectiveness. Our findings confirm that TMAE is scalable to large claims data and generates efficient patient embeddings for risk analysis.",1
"We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets.",0
"We present a straightforward yet efficient model that can learn self-supervised representations with graph data. Our approach is in line with previous methods that create two versions of an input graph using data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we use a novel feature-level objective that draws inspiration from the traditional Canonical Correlation Analysis. Our method does not require a parameterized mutual information estimator, additional projector, asymmetric structures, or negative samples, which can be expensive. Our objective primarily aims to discard augmentation-dependent information and learn invariant representations. It also prevents degenerate solutions by decorrelating features in different dimensions. We show that our approach is an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets.",1
"In this work, we propose a novel straightforward method for medical volume and sequence segmentation with limited annotations. To avert laborious annotating, the recent success of self-supervised learning(SSL) motivates the pre-training on unlabeled data. Despite its success, it is still challenging to adapt typical SSL methods to volume/sequence segmentation, due to their lack of mining on local semantic discrimination and rare exploitation on volume and sequence structures. Based on the continuity between slices/frames and the common spatial layout of organs across volumes/sequences, we introduced a novel bootstrap self-supervised representation learning method by leveraging the predictable possibility of neighboring slices. At the core of our method is a simple and straightforward dense self-supervision on the predictions of local representations and a strategy of predicting locals based on global context, which enables stable and reliable supervision for both global and local representation mining among volumes. Specifically, we first proposed an asymmetric network with an attention-guided predictor to enforce distance-specific prediction and supervision on slices within and across volumes/sequences. Secondly, we introduced a novel prototype-based foreground-background calibration module to enhance representation consistency. The two parts are trained jointly on labeled and unlabeled data. When evaluated on three benchmark datasets of medical volumes and sequences, our model outperforms existing methods with a large margin of 4.5\% DSC on ACDC, 1.7\% on Prostate, and 2.3\% on CAMUS. Intensive evaluations reveals the effectiveness and superiority of our method.",0
"Our work proposes an innovative approach for segmenting medical volumes and sequences with limited annotations. In order to avoid the tedious task of annotation, we leverage the success of self-supervised learning (SSL) and pre-train on unlabeled data. However, adapting typical SSL methods for volume/sequence segmentation is challenging due to their lack of emphasis on local semantic discrimination and exploitation of volume and sequence structures. To address this issue, we introduce a bootstrap self-supervised representation learning method that leverages the predictable possibility of neighboring slices/frames and the common spatial layout of organs across volumes/sequences. Our method utilizes a dense self-supervision on local representation predictions and a strategy of predicting locals based on global context, which allows for stable and reliable supervision for both global and local representation mining among volumes. We propose an asymmetric network with an attention-guided predictor to enforce distance-specific prediction and supervision on slices within and across volumes/sequences, and a novel prototype-based foreground-background calibration module to enhance representation consistency. Our model outperforms existing methods on three benchmark datasets of medical volumes and sequences, with a large margin of 4.5\% DSC on ACDC, 1.7\% on Prostate, and 2.3\% on CAMUS. Through intensive evaluations, we demonstrate the effectiveness and superiority of our approach.",1
"The goal of this work is to characterize the representational impact that foveation operations have for machine vision systems, inspired by the foveated human visual system, which has higher acuity at the center of gaze and texture-like encoding in the periphery. To do so, we introduce models consisting of a first-stage \textit{fixed} image transform followed by a second-stage \textit{learnable} convolutional neural network, and we varied the first stage component. The primary model has a foveated-textural input stage, which we compare to a model with foveated-blurred input and a model with spatially-uniform blurred input (both matched for perceptual compression), and a final reference model with minimal input-based compression. We find that: 1) the foveated-texture model shows similar scene classification accuracy as the reference model despite its compressed input, with greater i.i.d. generalization than the other models; 2) the foveated-texture model has greater sensitivity to high-spatial frequency information and greater robustness to occlusion, w.r.t the comparison models; 3) both the foveated systems, show a stronger center image-bias relative to the spatially-uniform systems even with a weight sharing constraint. Critically, these results are preserved over different classical CNN architectures throughout their learning dynamics. Altogether, this suggests that foveation with peripheral texture-based computations yields an efficient, distinct, and robust representational format of scene information, and provides symbiotic computational insight into the representational consequences that texture-based peripheral encoding may have for processing in the human visual system, while also potentially inspiring the next generation of computer vision models via spatially-adaptive computation. Code + Data available here: https://github.com/ArturoDeza/EmergentProperties",0
"The aim of this study is to examine how foveation operations impact the representation of machine vision systems, drawing inspiration from the foveated human visual system, which prioritizes acuity in the center of gaze and uses texture-based encoding in the periphery. The study introduces various models comprising a fixed image transform and a learnable convolutional neural network, with variations in the first-stage component. The primary model features a foveated-textural input stage, which is compared to models with foveated-blurred input and spatially-uniform blurred input (both with equivalent perceptual compression), as well as a reference model with minimal input-based compression. Results show that the foveated-texture model has similar scene classification accuracy to the reference model, greater i.i.d. generalization, greater sensitivity to high-spatial frequency information, and greater robustness to occlusion compared to the other models. Both foveated systems exhibit a stronger center image-bias than spatially-uniform systems, even with weight sharing constraints. These findings suggest that foveation with peripheral texture-based computations can yield an efficient, distinct, and robust representational format for scene information, and provide insight into the impact of texture-based peripheral encoding on human visual processing, potentially inspiring future spatially-adaptive computer vision models. Code + Data available here: https://github.com/ArturoDeza/EmergentProperties.",1
"Sketch drawings capture the salient information of visual concepts. Previous work has shown that neural networks are capable of producing sketches of natural objects drawn from a small number of classes. While earlier approaches focus on generation quality or retrieval, we explore properties of image representations learned by training a model to produce sketches of images. We show that this generative, class-agnostic model produces informative embeddings of images from novel examples, classes, and even novel datasets in a few-shot setting. Additionally, we find that these learned representations exhibit interesting structure and compositionality.",0
"The essential details of visual concepts are captured by sketch drawings. Studies have demonstrated that neural networks can create sketches of natural objects belonging to a limited number of categories. Rather than concentrating on the quality of generation or retrieval, we investigate the qualities of image representations acquired by teaching a model to generate image sketches. Our findings demonstrate that this generative model, which is not limited to any specific class, generates informative embeddings of images from new examples, categories, and even novel datasets using a few-shot approach. Moreover, we observe that these acquired representations possess intriguing compositionality and structure.",1
"While representation learning has yielded a great success on many graph learning tasks, there is little understanding behind the structures that are being captured by these embeddings. For example, we wonder if the topological features, such as the Triangle Count, the Degree of the node and other centrality measures are concretely encoded in the embeddings. Furthermore, we ask if the presence of these structures in the embeddings is necessary for a better performance on the downstream tasks, such as clustering and classification. To address these questions, we conduct an extensive empirical study over three classes of unsupervised graph embedding models and seven different variants of Graph Autoencoders. Our results show that five topological features: the Degree, the Local Clustering Score, the Betweenness Centrality, the Eigenvector Centrality, and Triangle Count are concretely preserved in the first layer of the graph autoencoder that employs the SUM aggregation rule, under the condition that the model preserves the second-order proximity. We supplement further evidence for the presence of these features by revealing a hierarchy in the distribution of the topological features in the embeddings of the aforementioned model. We also show that a model with such properties can outperform other models on certain downstream tasks, especially when the preserved features are relevant to the task at hand. Finally, we evaluate the suitability of our findings through a test case study related to social influence prediction.",0
"Although representation learning has been successful in many graph learning tasks, there is little comprehension regarding the structures captured by these embeddings. It is uncertain if topological features, such as Triangle Count, node Degree, and other centrality measures, are encoded in the embeddings. Additionally, it is unclear if these structures are necessary for better performance on downstream tasks like clustering and classification. To address these questions, we conducted an extensive empirical study on three classes of unsupervised graph embedding models and seven variants of Graph Autoencoders. Our results indicate that the first layer of the graph autoencoder that applies the SUM rule preserves five topological features when the model retains second-order proximity. We also discovered a hierarchy in the distribution of these features in the embeddings. Furthermore, models with these properties can outperform others on certain downstream tasks, especially when the preserved features are relevant to the task. Finally, we tested our findings through a case study related to social influence prediction.",1
"Contrastive self-supervised learning has largely narrowed the gap to supervised pre-training on ImageNet. However, its success highly relies on the object-centric priors of ImageNet, i.e., different augmented views of the same image correspond to the same object. Such a heavily curated constraint becomes immediately infeasible when pre-trained on more complex scene images with many objects. To overcome this limitation, we introduce Object-level Representation Learning (ORL), a new self-supervised learning framework towards scene images. Our key insight is to leverage image-level self-supervised pre-training as the prior to discover object-level semantic correspondence, thus realizing object-level representation learning from scene images. Extensive experiments on COCO show that ORL significantly improves the performance of self-supervised learning on scene images, even surpassing supervised ImageNet pre-training on several downstream tasks. Furthermore, ORL improves the downstream performance when more unlabeled scene images are available, demonstrating its great potential of harnessing unlabeled data in the wild. We hope our approach can motivate future research on more general-purpose unsupervised representation learning from scene data. Project page: https://www.mmlab-ntu.com/project/orl/.",0
"The success of contrastive self-supervised learning has nearly closed the gap with supervised pre-training on ImageNet. However, this success is heavily dependent on the object-centric priors of ImageNet, which dictate that various augmented views of an image correspond to the same object. This constraint becomes impractical when pre-training on more complex images with many objects. To address this limitation, we propose a new self-supervised learning framework called Object-level Representation Learning (ORL), which focuses on scene images. We leverage image-level self-supervised pre-training as a prior to uncover object-level semantic correspondence, enabling object-level representation learning from scene images. Our experiments on COCO demonstrate that ORL significantly improves self-supervised learning on scene images and even outperforms supervised ImageNet pre-training on several downstream tasks. Furthermore, ORL enhances downstream performance when more unlabeled scene images are available, indicating its potential in utilizing unlabeled data in real-world settings. We hope our approach inspires further research on unsupervised representation learning from scene data. For more information, please visit our project page at https://www.mmlab-ntu.com/project/orl/.",1
"The success of deep reinforcement learning (DRL) is due to the power of learning a representation that is suitable for the underlying exploration and exploitation task. However, existing provable reinforcement learning algorithms with linear function approximation often assume the feature representation is known and fixed. In order to understand how representation learning can improve the efficiency of RL, we study representation learning for a class of low-rank Markov Decision Processes (MDPs) where the transition kernel can be represented in a bilinear form. We propose a provably efficient algorithm called ReLEX that can simultaneously learn the representation and perform exploration. We show that ReLEX always performs no worse than a state-of-the-art algorithm without representation learning, and will be strictly better in terms of sample efficiency if the function class of representations enjoys a certain mild ""coverage'' property over the whole state-action space.",0
"The success of deep reinforcement learning (DRL) is attributed to its ability to learn a representation that is suitable for the exploration and exploitation task at hand. However, current provable reinforcement learning algorithms, which use linear function approximation, assume that the feature representation is predetermined and unchangeable. To evaluate the potential benefits of representation learning in RL, we examine representation learning for a specific group of low-rank Markov Decision Processes (MDPs) with a transition kernel that can be represented bilinearly. We introduce an efficient algorithm, ReLEX, that can learn the representation and explore simultaneously. Our results indicate that ReLEX consistently performs as well as state-of-the-art algorithms without representation learning, and is more efficient in terms of sample use when the representation function class has a mild ""coverage"" property across the state-action space.",1
"As financial services (FS) companies have experienced drastic technology driven changes, the availability of new data streams provides the opportunity for more comprehensive customer understanding. We propose Dynamic Customer Embeddings (DCE), a framework that leverages customers' digital activity and a wide range of financial context to learn dense representations of customers in the FS industry. Our method examines customer actions and pageviews within a mobile or web digital session, the sequencing of the sessions themselves, and snapshots of common financial features across our organization at the time of login. We test our customer embeddings using real world data in three prediction problems: 1) the intent of a customer in their next digital session, 2) the probability of a customer calling the call centers after a session, and 3) the probability of a digital session to be fraudulent. DCE showed performance lift in all three downstream problems.",0
"Due to significant technological advancements in financial services (FS) companies, they now have access to various new data sources that can be utilized to gain a more comprehensive understanding of their customers. Our proposed framework, Dynamic Customer Embeddings (DCE), leverages customers' digital activity and a broad range of financial contexts to generate dense representations of customers within the FS industry. DCE examines customer behavior, pageviews, session sequencing, and financial features within our organization during login. We tested the effectiveness of our customer embeddings using actual data to solve three prediction problems: 1) predicting a customer's next digital session, 2) determining the probability of a customer calling the call center after a session, and 3) calculating the likelihood of a digital session being fraudulent. Our framework produced superior results in all three areas.",1
"Graph representation learning is a fundamental problem for modeling relational data and benefits a number of downstream applications. Traditional Bayesian-based graph models and recent deep learning based GNN either suffer from impracticability or lack interpretability, thus combined models for undirected graphs have been proposed to overcome the weaknesses. As a large portion of real-world graphs are directed graphs (of which undirected graphs are special cases), in this paper, we propose a Deep Latent Space Model (DLSM) for directed graphs to incorporate the traditional latent variable based generative model into deep learning frameworks. Our proposed model consists of a graph convolutional network (GCN) encoder and a stochastic decoder, which are layer-wise connected by a hierarchical variational auto-encoder architecture. By specifically modeling the degree heterogeneity using node random factors, our model possesses better interpretability in both community structure and degree heterogeneity. For fast inference, the stochastic gradient variational Bayes (SGVB) is adopted using a non-iterative recognition model, which is much more scalable than traditional MCMC-based methods. The experiments on real-world datasets show that the proposed model achieves the state-of-the-art performances on both link prediction and community detection tasks while learning interpretable node embeddings. The source code is available at https://github.com/upperr/DLSM.",0
"The problem of modeling relational data through graph representation learning is crucial for various downstream applications. However, traditional Bayesian-based graph models and recent deep learning-based GNN have limitations in terms of interpretability and practicality. To address this issue, combined models for undirected graphs have been proposed. Nevertheless, as most real-world graphs are directed, we introduce a Deep Latent Space Model (DLSM) for directed graphs that incorporates a traditional latent variable-based generative model into deep learning frameworks. Our model includes a graph convolutional network (GCN) encoder and a stochastic decoder, connected layer-wise through a hierarchical variational auto-encoder architecture. Additionally, our model provides better interpretability in both community structure and degree heterogeneity by modeling degree heterogeneity using node random factors. We utilize the stochastic gradient variational Bayes (SGVB) for faster inference, which is more scalable than traditional MCMC-based methods. Our experiments on real-world datasets demonstrate that our proposed model performs better than state-of-the-art models in both link prediction and community detection tasks while learning interpretable node embeddings. The source code for our model is available at https://github.com/upperr/DLSM.",1
"Self-supervised learning (especially contrastive learning) has attracted great interest due to its tremendous potentials in learning discriminative representations in an unsupervised manner. Despite the acknowledged successes, existing contrastive learning methods suffer from very low learning efficiency, e.g., taking about ten times more training epochs than supervised learning for comparable recognition accuracy. In this paper, we discover two contradictory phenomena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilarity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differentiate all the actual object categories. Over-clustering implies that the model cannot efficiently learn the feature representation from excessive negative sample pairs, which enforces the model to over-cluster samples of the same actual categories into different clusters. To simultaneously overcome these two problems, we propose a novel self-supervised learning framework using a median triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the positive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting the negative sample of a median similarity score from all negative samples to avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model. We extensively evaluate our proposed framework in several large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results demonstrate the superior performance (e.g., the learning efficiency) of our model over the latest state-of-the-art methods by a clear margin. Codes available at: https://github.com/wanggrun/triplet.",0
"The potential of self-supervised learning, particularly contrastive learning, to learn discriminative representations in an unsupervised manner has garnered significant interest. However, existing contrastive learning approaches suffer from low learning efficiency, requiring approximately ten times more training epochs than supervised learning to achieve comparable recognition accuracy. Our research identifies two major challenges in contrastive learning: under-clustering and over-clustering. Under-clustering occurs when the model is unable to differentiate inter-class samples due to insufficient negative sample pairs for contrastive learning. Over-clustering arises when the model struggles to learn feature representations from excessive negative sample pairs, resulting in the clustering of samples from the same categories into different clusters. To address both issues, we propose a novel self-supervised learning framework that utilizes a median triplet loss. Our framework employs a triplet loss to address under-clustering and selects the negative sample of a median similarity score from all negative samples to avoid over-clustering, guaranteeing the Bernoulli Distribution model. We evaluate our proposed framework on several large-scale benchmarks and demonstrate superior performance, including learning efficiency, over the latest state-of-the-art methods. Our codes are available at https://github.com/wanggrun/triplet.",1
"Benefited from considerable pixel-level annotations collected from a specific situation (source), the trained semantic segmentation model performs quite well, but fails in a new situation (target) due to the large domain shift. To mitigate the domain gap, previous cross-domain semantic segmentation methods always assume the co-existence of source data and target data during distribution alignment. However, the access to source data in the real scenario may raise privacy concerns and violate intellectual property. To tackle this problem, we focus on an interesting and challenging cross-domain semantic segmentation task where only the trained source model is provided to the target domain, and further propose a unified framework called Domain Adaptive Semantic Segmentation without Source data (DAS$^3$ for short). Specifically, DAS$^3$ consists of three schemes, i.e., feature alignment, self-training, and information propagation. First, we mainly develop a focal entropic loss on the network outputs to implicitly align the target features with unseen source features via the provided source model. Second, besides positive pseudo labels in vanilla self-training, we first introduce negative pseudo labels to the field and develop a bi-directional self-training strategy to enhance the representation learning in the target domain. Finally, the information propagation scheme further reduces the intra-domain discrepancy within the target domain via pseudo semi-supervised learning. Extensive results on synthesis-to-real and cross-city driving datasets validate DAS$^3$ yields state-of-the-art performance, even on par with methods that need access to source data.",0
"The trained semantic segmentation model performs well with pixel-level annotations collected from a specific situation, but fails when applied to a new situation due to a significant domain shift. Past cross-domain semantic segmentation methods have relied on having access to both source and target data to address this issue, but obtaining source data can raise privacy concerns and intellectual property violations. To overcome this, we focus on a challenging cross-domain semantic segmentation task where only the trained source model is provided to the target domain and propose a unified framework called DAS$^3$. DAS$^3$ has three schemes: feature alignment, self-training, and information propagation, which work together to implicitly align target features with unseen source features, enhance representation learning in the target domain, and reduce intra-domain discrepancy. Our results on various datasets demonstrate that DAS$^3$ achieves state-of-the-art performance, even competing with methods that have access to source data.",1
"Multi-label image classification (MLIC) is a fundamental and practical task, which aims to assign multiple possible labels to an image. In recent years, many deep convolutional neural network (CNN) based approaches have been proposed which model label correlations to discover semantics of labels and learn semantic representations of images. This paper advances this research direction by improving both the modeling of label correlations and the learning of semantic representations. On the one hand, besides the local semantics of each label, we propose to further explore global semantics shared by multiple labels. On the other hand, existing approaches mainly learn the semantic representations at the last convolutional layer of a CNN. But it has been noted that the image representations of different layers of CNN capture different levels or scales of features and have different discriminative abilities. We thus propose to learn semantic representations at multiple convolutional layers. To this end, this paper designs a Multi-layered Semantic Representation Network (MSRN) which discovers both local and global semantics of labels through modeling label correlations and utilizes the label semantics to guide the semantic representations learning at multiple layers through an attention mechanism. Extensive experiments on four benchmark datasets including VOC 2007, COCO, NUS-WIDE, and Apparel show a competitive performance of the proposed MSRN against state-of-the-art models.",0
"The task of Multi-label image classification (MLIC) involves assigning multiple labels to an image, and is both fundamental and practical. Over the years, various deep convolutional neural network (CNN) based approaches have been developed to model label correlations and learn semantic representations of images. This paper contributes to this research area by improving the modeling of label correlations and semantic representation learning. Firstly, the paper proposes exploring global semantics shared by multiple labels, in addition to the local semantics of each label. Secondly, while existing approaches primarily learn semantic representations at the last convolutional layer of a CNN, this paper suggests semantic representation learning at multiple convolutional layers. A Multi-layered Semantic Representation Network (MSRN) is designed to accomplish this by modeling label correlations and using an attention mechanism to guide the semantic representations learning at multiple layers. The proposed MSRN is evaluated on four benchmark datasets, including VOC 2007, COCO, NUS-WIDE, and Apparel, and shows competitive performance against state-of-the-art models.",1
"Learning node representations on temporal graphs is a fundamental step to learn real-word dynamic graphs efficiently. Real-world graphs have the nature of continuously evolving over time, such as changing edges weights, removing and adding nodes and appearing and disappearing of edges, while previous graph representation learning methods focused generally on static graphs. We present ConvDySAT as an enhancement of DySAT, one of the state-of-the-art dynamic methods, by augmenting convolution neural networks with the self-attention mechanism, the employed method in DySAT to express the structural and temporal evolution. We conducted single-step link prediction on a communication network and rating network, Experimental results show significant performance gains for ConvDySAT over various state-of-the-art methods.",0
"In order to efficiently learn dynamic graphs in the real world, it is essential to first learn node representations on temporal graphs. Real-world graphs are constantly evolving over time, with changes in edge weights, addition and removal of nodes, and the appearance and disappearance of edges. However, previous methods for graph representation learning have mainly focused on static graphs. To address this issue, we introduce ConvDySAT as an improvement on DySAT, one of the top dynamic methods. We achieve this by combining convolutional neural networks with the self-attention mechanism employed in DySAT to express structural and temporal evolution. Our experiments involve single-step link prediction on communication and rating networks, and the results demonstrate significant performance improvements for ConvDySAT compared to other state-of-the-art methods.",1
"In this paper, we introduce a novel visual representation learning which relies on a handful of adaptively learned tokens, and which is applicable to both image and video understanding tasks. Instead of relying on hand-designed splitting strategies to obtain visual tokens and processing a large number of densely sampled patches for attention, our approach learns to mine important tokens in visual data. This results in efficiently and effectively finding a few important visual tokens and enables modeling of pairwise attention between such tokens, over a longer temporal horizon for videos, or the spatial content in images. Our experiments demonstrate strong performance on several challenging benchmarks for both image and video recognition tasks. Importantly, due to our tokens being adaptive, we accomplish competitive results at significantly reduced compute amount.",0
"This paper presents a new method of visual representation learning that uses a small number of tokens which are adaptively learned and can be applied to both image and video tasks. Rather than relying on predetermined splitting strategies and processing numerous patches for attention, our approach mines significant tokens in visual data, resulting in the identification of a few key tokens. This allows for the modeling of attention between these tokens, enabling temporal modeling for videos and spatial modeling for images. Our experiments demonstrate superior performance on various challenging benchmarks for image and video recognition tasks. Our adaptive tokens also lead to competitive results with significantly reduced compute resources.",1
"The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via ""shortcuts"", i.e., by inadvertently suppressing important predictive features. We find that feature extraction is influenced by the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks. The code is available at: \url{https://github.com/joshr17/IFM}.",0
"The effectiveness of generalized representations learned through contrastive learning is heavily reliant on the extracted data features. However, it has been noticed that the contrastive loss does not always provide sufficient guidance in determining which features to extract. This can lead to negative impacts on downstream tasks, as important predictive features may be inadvertently suppressed through ""shortcuts"". The difficulty of the instance discrimination task influences feature extraction, where harder pairs can enhance the representation of certain features at the cost of suppressing previously well-represented ones. To address this issue, we suggest implicit feature modification (IFM), a technique that modifies positive and negative samples to guide contrastive models towards capturing a broader range of predictive features. Our empirical observations indicate that IFM reduces feature suppression and improves performance in vision and medical imaging tasks. The code for IFM is accessible at: \url{https://github.com/joshr17/IFM}.",1
"Recently introduced self-supervised methods for image representation learning provide on par or superior results to their fully supervised competitors, yet the corresponding efforts to explain the self-supervised approaches lag behind. Motivated by this observation, we introduce a novel visual probing framework for explaining the self-supervised models by leveraging probing tasks employed previously in natural language processing. The probing tasks require knowledge about semantic relationships between image parts. Hence, we propose a systematic approach to obtain analogs of natural language in vision, such as visual words, context, and taxonomy. Our proposal is grounded in Marr's computational theory of vision and concerns features like textures, shapes, and lines. We show the effectiveness and applicability of those analogs in the context of explaining self-supervised representations. Our key findings emphasize that relations between language and vision can serve as an effective yet intuitive tool for discovering how machine learning models work, independently of data modality. Our work opens a plethora of research pathways towards more explainable and transparent AI.",0
"Self-supervised methods for image representation learning have recently been shown to provide comparable or even better results than fully supervised methods. However, explanations for these self-supervised approaches are lacking. To address this gap, we present a new visual probing framework that utilizes probing tasks used in natural language processing to explain self-supervised models. These probing tasks require knowledge of semantic relationships between image parts, and we propose a systematic approach to obtain analogs of natural language in vision, such as visual words, context, and taxonomy. Our approach is based on Marr's computational theory of vision and focuses on features such as textures, shapes, and lines. We demonstrate the usefulness of these analogs for explaining self-supervised representations and show that the relationship between language and vision can be an effective tool for understanding machine learning models across different data modalities. Our work suggests new research directions towards more transparent and interpretable AI.",1
"Instance-level contrastive learning techniques, which rely on data augmentation and a contrastive loss function, have found great success in the domain of visual representation learning. They are not suitable for exploiting the rich dynamical structure of video however, as operations are done on many augmented instances. In this paper we propose ""Video Cross-Stream Prototypical Contrasting"", a novel method which predicts consistent prototype assignments from both RGB and optical flow views, operating on sets of samples. Specifically, we alternate the optimization process; while optimizing one of the streams, all views are mapped to one set of stream prototype vectors. Each of the assignments is predicted with all views except the one matching the prediction, pushing representations closer to their assigned prototypes. As a result, more efficient video embeddings with ingrained motion information are learned, without the explicit need for optical flow computation during inference. We obtain state-of-the-art results on nearest neighbour video retrieval and action recognition, outperforming previous best by +3.2% on UCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and +15.1% on HMDB51 using the R(2+1)D backbone.",0
"The use of instance-level contrastive learning techniques has been successful in visual representation learning, but it is not suitable for exploiting the dynamic structure of video due to the operations being done on multiple augmented instances. To address this, we propose a new method called ""Video Cross-Stream Prototypical Contrasting"" that predicts consistent prototype assignments from both RGB and optical flow views, operating on sets of samples. During optimization, all views are mapped to one set of stream prototype vectors, and assignments are predicted with all views except for the one matching the prediction, resulting in more efficient video embeddings with motion information learned without the need for optical flow computation during inference. Our method achieves state-of-the-art results on nearest neighbor video retrieval and action recognition, surpassing previous best results by +3.2% on UCF101 using the S3D backbone (90.5% Top-1 accuracy), and by +7.2% on UCF101 and +15.1% on HMDB51 using the R(2+1)D backbone.",1
"Existing research for image captioning usually represents an image using a scene graph with low-level facts (objects and relations) and fails to capture the high-level semantics. In this paper, we propose a Theme Concepts extended Image Captioning (TCIC) framework that incorporates theme concepts to represent high-level cross-modality semantics. In practice, we model theme concepts as memory vectors and propose Transformer with Theme Nodes (TTN) to incorporate those vectors for image captioning. Considering that theme concepts can be learned from both images and captions, we propose two settings for their representations learning based on TTN. On the vision side, TTN is configured to take both scene graph based features and theme concepts as input for visual representation learning. On the language side, TTN is configured to take both captions and theme concepts as input for text representation re-construction. Both settings aim to generate target captions with the same transformer-based decoder. During the training, we further align representations of theme concepts learned from images and corresponding captions to enforce the cross-modality learning. Experimental results on MS COCO show the effectiveness of our approach compared to some state-of-the-art models.",0
"Previous studies on image captioning typically utilize a scene graph consisting of low-level facts such as objects and relations, which fail to capture high-level semantics. Our paper introduces a Theme Concepts extended Image Captioning (TCIC) framework that integrates theme concepts to represent cross-modality semantics. We represent theme concepts as memory vectors and propose a Transformer with Theme Nodes (TTN) to incorporate these vectors for image captioning. To learn theme concepts from both images and captions, we propose two settings for their representation learning using TTN. For visual representation learning, TTN takes both scene graph-based features and theme concepts as input. For text representation reconstruction, TTN takes captions and theme concepts as input. Both settings generate target captions using the same transformer-based decoder. During training, we align representations of theme concepts learned from images and captions to enhance cross-modality learning. Our experimental results on MS COCO demonstrate the effectiveness of our approach compared to state-of-the-art models.",1
"While deep-learning based methods for visual tracking have achieved substantial progress, these schemes entail large-scale and high-quality annotated data for sufficient training. To eliminate expensive and exhaustive annotation, we study self-supervised learning for visual tracking. In this work, we develop the Crop-Transform-Paste operation, which is able to synthesize sufficient training data by simulating various kinds of scene variations during tracking, including appearance variations of objects and background changes. Since the object state is known in all synthesized data, existing deep trackers can be trained in routine ways without human annotation. Different from typical self-supervised learning methods performing visual representation learning as an individual step, the proposed self-supervised learning mechanism can be seamlessly integrated into any existing tracking framework to perform training. Extensive experiments show that our method 1) achieves favorable performance than supervised learning in few-shot tracking scenarios; 2) can deal with various tracking challenges such as object deformation, occlusion, or background clutter due to its design; 3) can be combined with supervised learning to further boost the performance, particularly effective in few-shot tracking scenarios.",0
"Although deep-learning based techniques for visual tracking have made significant advancements, these approaches require large-scale and high-quality annotated data for effective training. To address the issue of costly and exhaustive annotation, we investigate self-supervised learning for visual tracking. Our approach introduces the Crop-Transform-Paste operation, which can generate ample training data by simulating different scene variations during tracking, including object appearance variations and background changes. As the object state is known in all synthesized data, existing deep trackers can be trained without human annotation. Unlike typical self-supervised learning methods that perform visual representation learning as a separate step, our proposed self-supervised learning mechanism can be integrated into any existing tracking framework for seamless training. We conducted extensive experiments to demonstrate that our method: 1) outperforms supervised learning in few-shot tracking scenarios; 2) can handle various tracking challenges, such as object deformation, occlusion, or background clutter; and 3) can be combined with supervised learning to further enhance performance, particularly effective in few-shot tracking scenarios.",1
"Neural shape representations have recently shown to be effective in shape analysis and reconstruction tasks. Existing neural network methods require point coordinates and corresponding normal vectors to learn the implicit level sets of the shape. Normal vectors are often not provided as raw data, therefore, approximation and reorientation are required as pre-processing stages, both of which can introduce noise. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal shape representation networks that further improves convergence to the desired solution. We evaluate the effectiveness of our approach on the task of surface reconstruction and show state-of-the-art performance compared to other unoriented methods and on-par performance compared to oriented methods.",0
"Recent studies have demonstrated the efficacy of neural shape representations in shape analysis and reconstruction tasks. However, existing neural network methods require point coordinates and corresponding normal vectors as input to learn the implicit level sets of the shape. As normal vectors are not always provided in raw data, approximation and reorientation become necessary pre-processing stages, which can introduce noise. To address this issue, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. By incorporating a soft constraint on the divergence of the distance function, we promote smooth solutions that reliably align gradients with the unknown normal at each point, often outperforming approaches that use ground truth normal vectors directly. Furthermore, we introduce a novel geometric initialization method for sinusoidal shape representation networks that enhances convergence to the desired solution. Our approach is evaluated on the task of surface reconstruction and shown to achieve state-of-the-art performance compared to other unoriented methods and on-par performance compared to oriented methods.",1
"Action-value estimation is a critical component of many reinforcement learning (RL) methods whereby sample complexity relies heavily on how fast a good estimator for action value can be learned. By viewing this problem through the lens of representation learning, good representations of both state and action can facilitate action-value estimation. While advances in deep learning have seamlessly driven progress in learning state representations, given the specificity of the notion of agency to RL, little attention has been paid to learning action representations. We conjecture that leveraging the combinatorial structure of multi-dimensional action spaces is a key ingredient for learning good representations of action. To test this, we set forth the action hypergraph networks framework -- a class of functions for learning action representations in multi-dimensional discrete action spaces with a structural inductive bias. Using this framework we realise an agent class based on a combination with deep Q-networks, which we dub hypergraph Q-networks. We show the effectiveness of our approach on a myriad of domains: illustrative prediction problems under minimal confounding effects, Atari 2600 games, and discretised physical control benchmarks.",0
"Many reinforcement learning (RL) methods rely on action-value estimation, which can be challenging due to the time required to obtain a good estimator. Representation learning can help by providing good representations of both state and action. However, while deep learning has improved state representation, little attention has been given to action representation. To address this, we propose the action hypergraph networks framework, which leverages the combinatorial structure of multi-dimensional action spaces to learn good representations of action. We combine this framework with deep Q-networks to create hypergraph Q-networks, which we test on various domains, including prediction problems, Atari 2600 games, and physical control benchmarks. Our results demonstrate the effectiveness of our approach.",1
"We present a method that learns a spatiotemporal neural irradiance field for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradiance field from a single video poses significant challenges because the video contains only one observation of the scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appearance and vice versa. We address this ambiguity by constraining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from individual frames into a single global representation. We provide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results.",0
"Our approach involves training an algorithm to learn a spatiotemporal neural irradiance field from a single video, which can then be used to render the video from any viewpoint. This is made possible by recent developments in implicit representations. However, learning a spatiotemporal irradiance field from a single video is challenging due to the fact that only one observation of the scene is available at any given time. To overcome this issue, we use scene depth estimates from video depth estimation methods to constrain our dynamic scene representation, which aggregates information from individual frames into a global representation. Our method achieves impressive results, as demonstrated by a thorough quantitative evaluation.",1
"Radars and cameras are mature, cost-effective, and robust sensors and have been widely used in the perception stack of mass-produced autonomous driving systems. Due to their complementary properties, outputs from radar detection (radar pins) and camera perception (2D bounding boxes) are usually fused to generate the best perception results. The key to successful radar-camera fusion is the accurate data association. The challenges in the radar-camera association can be attributed to the complexity of driving scenes, the noisy and sparse nature of radar measurements, and the depth ambiguity from 2D bounding boxes. Traditional rule-based association methods are susceptible to performance degradation in challenging scenarios and failure in corner cases. In this study, we propose to address radar-camera association via deep representation learning, to explore feature-level interaction and global reasoning. Additionally, we design a loss sampling mechanism and an innovative ordinal loss to overcome the difficulty of imperfect labeling and to enforce critical human-like reasoning. Despite being trained with noisy labels generated by a rule-based algorithm, our proposed method achieves a performance of 92.2% F1 score, which is 11.6% higher than the rule-based teacher. Moreover, this data-driven method also lends itself to continuous improvement via corner case mining.",0
"Autonomous driving systems commonly use mature, cost-effective, and robust sensors such as radars and cameras in their perception stack. These sensors provide complementary outputs, which are often fused to generate the best perception results. However, accurate data association is crucial for successful radar-camera fusion, and traditional rule-based methods are susceptible to performance degradation and failure in challenging scenarios. In this study, we propose a deep representation learning approach that explores feature-level interaction and global reasoning to address radar-camera association. We also use a loss sampling mechanism and innovative ordinal loss to overcome imperfect labeling and enforce critical human-like reasoning. Despite being trained with noisy labels, our proposed method achieves a performance of 92.2% F1 score, which is 11.6% higher than the rule-based teacher. Additionally, this data-driven approach allows for continuous improvement through corner case mining.",1
"We propose the Graph Context Encoder (GCE), a simple but efficient approach for graph representation learning based on graph feature masking and reconstruction.   GCE models are trained to efficiently reconstruct input graphs similarly to a graph autoencoder where node and edge labels are masked. In particular, our model is also allowed to change graph structures by masking and reconstructing graphs augmented by random pseudo-edges.   We show that GCE can be used for novel graph generation, with applications for molecule generation. Used as a pretraining method, we also show that GCE improves baseline performances in supervised classification tasks tested on multiple standard benchmark graph datasets.",0
"Our proposal is the Graph Context Encoder (GCE), which is a simple yet effective method for learning graph representation through graph feature masking and reconstruction. The GCE models are capable of reconstructing input graphs similar to a graph autoencoder, where node and edge labels are masked, and can even modify graph structures by masking and reconstructing graphs with random pseudo-edges. GCE can be utilized for generating novel graphs, particularly for molecule generation. Additionally, when used as a pretraining technique, GCE enhances the baseline performance in supervised classification tasks on various standard benchmark graph datasets.",1
"Representing games through their pixels offers a promising approach for building general-purpose and versatile game models. While games are not merely images, neural network models trained on game pixels often capture differences of the visual style of the image rather than the content of the game. As a result, such models cannot generalize well even within similar games of the same genre. In this paper we build on recent advances in contrastive learning and showcase its benefits for representation learning in games. Learning to contrast images of games not only classifies games in a more efficient manner; it also yields models that separate games in a more meaningful fashion by ignoring the visual style and focusing, instead, on their content. Our results in a large dataset of sports video games containing 100k images across 175 games and 10 game genres suggest that contrastive learning is better suited for learning generalized game representations compared to conventional supervised learning. The findings of this study bring us closer to universal visual encoders for games that can be reused across previously unseen games without requiring retraining or fine-tuning.",0
"Using pixels to represent games is a promising method for creating adaptable and versatile game models. However, neural network models trained on game pixels tend to focus on the visual style of the image instead of the game's content, making it difficult for them to generalize even among similar games of the same genre. This paper explores the benefits of contrastive learning in representation learning for games, which enables more efficient classification of games and creates models that differentiate games based on their content rather than their visual style. Our study on a comprehensive dataset of sports video games with 100k images across 175 games and 10 game genres suggests that contrastive learning is more effective than conventional supervised learning for developing generalized game representations. These results bring us closer to developing universal visual encoders for games that can be used across new games without requiring retraining or fine-tuning.",1
"Training a Convolutional Neural Network (CNN) to be robust against rotation has mostly been done with data augmentation. In this paper, another progressive vision of research direction is highlighted to encourage less dependence on data augmentation by achieving structural rotational invariance of a network. The deep equivariance-bridged SO(2) invariant network is proposed to echo such vision. First, Self-Weighted Nearest Neighbors Graph Convolutional Network (SWN-GCN) is proposed to implement Graph Convolutional Network (GCN) on the graph representation of an image to acquire rotationally equivariant representation, as GCN is more suitable for constructing deeper network than spectral graph convolution-based approaches. Then, invariant representation is eventually obtained with Global Average Pooling (GAP), a permutation-invariant operation suitable for aggregating high-dimensional representations, over the equivariant set of vertices retrieved from SWN-GCN. Our method achieves the state-of-the-art image classification performance on rotated MNIST and CIFAR-10 images, where the models are trained with a non-augmented dataset only. Quantitative validations over invariance of the representations also demonstrate strong invariance of deep representations of SWN-GCN over rotations.",0
"Traditionally, increasing the robustness of a Convolutional Neural Network (CNN) against rotation has relied heavily on data augmentation. However, this paper proposes an alternative approach to reduce the dependence on data augmentation by establishing structural rotational invariance within the network. To achieve this, a deep equivariance-bridged SO(2) invariant network is suggested. The network employs a Self-Weighted Nearest Neighbors Graph Convolutional Network (SWN-GCN) to implement Graph Convolutional Network (GCN) on the graph representation of an image, resulting in rotationally equivariant representation. The equivariant set of vertices retrieved from SWN-GCN is then subjected to Global Average Pooling (GAP), a permutation-invariant operation, to acquire an invariant representation. Remarkably, the proposed method achieves superior image classification performance on rotated MNIST and CIFAR-10 images, using a non-augmented dataset only. In addition, quantitative validations prove the robustness of deep representations of SWN-GCN against rotations.",1
"Noise contrastive learning is a popular technique for unsupervised representation learning. In this approach, a representation is obtained via reduction to supervised learning, where given a notion of semantic similarity, the learner tries to distinguish a similar (positive) example from a collection of random (negative) examples. The success of modern contrastive learning pipelines relies on many parameters such as the choice of data augmentation, the number of negative examples, and the batch size; however, there is limited understanding as to how these parameters interact and affect downstream performance. We focus on disambiguating the role of one of these parameters: the number of negative examples. Theoretically, we show the existence of a collision-coverage trade-off suggesting that the optimal number of negative examples should scale with the number of underlying concepts in the data. Empirically, we scrutinize the role of the number of negatives in both NLP and vision tasks. In the NLP task, we find that the results broadly agree with our theory, while our vision experiments are murkier with performance sometimes even being insensitive to the number of negatives. We discuss plausible explanations for this behavior and suggest future directions to better align theory and practice.",0
"One popular method for unsupervised representation learning is noise contrastive learning. This strategy involves obtaining a representation through supervised learning by distinguishing a positive example from a group of negative examples, with similarity determined by a semantic notion. Contrastive learning pipelines rely on several factors, including the quantity of negative examples, batch size, and data augmentation. However, little is known about how these factors interact and influence downstream performance. Our study focuses on clarifying the significance of one of these factors, specifically the number of negative examples. Our theoretical analysis reveals a trade-off between collision and coverage, indicating that the optimal number of negatives should increase with the number of underlying concepts in the data. We evaluate the role of negative examples in NLP and vision tasks. Our NLP results largely align with our theory, but our vision experiments are less conclusive, with performance sometimes being unaffected by the number of negatives. We explore potential reasons for this disparity and suggest avenues for future research to bridge the gap between theory and practice.",1
"Graph convolution networks, like message passing graph convolution networks (MPGCNs), have been a powerful tool in representation learning of networked data. However, when data is heterogeneous, most architectures are limited as they employ a single strategy to handle multi-channel graph signals and they typically focus on low-frequency information. In this paper, we present a novel graph convolution operator, termed BankGCN, which keeps benefits of message passing models, but extends their capabilities beyond `low-pass' features. It decomposes multi-channel signals on graphs into subspaces and handles particular information in each subspace with an adapted filter. The filters of all subspaces have different frequency responses and together form a filter bank. Furthermore, each filter in the spectral domain corresponds to a message passing scheme, and diverse schemes are implemented via the filter bank. Importantly, the filter bank and the signal decomposition are jointly learned to adapt to the spectral characteristics of data and to target applications. Furthermore, this is implemented almost without extra parameters in comparison with most existing MPGCNs. Experimental results show that the proposed convolution operator permits to achieve excellent performance in graph classification on a collection of benchmark graph datasets.",0
"Representation learning of networked data has been revolutionized by graph convolution networks, particularly message passing graph convolution networks (MPGCNs). However, these architectures have limitations when working with heterogeneous data as they employ a single strategy for handling multi-channel graph signals and only focus on low-frequency information. This study introduces BankGCN, a novel graph convolution operator that maintains the advantages of message passing models while extending their capabilities beyond low-pass features. BankGCN decomposes multi-channel signals on graphs into subspaces and uses an adapted filter to handle particular information in each subspace. The filters of all subspaces have different frequency responses and form a filter bank that corresponds to a message passing scheme in the spectral domain. Various message passing schemes are implemented using the filter bank, which is jointly learned with signal decomposition to adapt to data spectral characteristics and target applications. Surprisingly, BankGCN is implemented with almost no extra parameters compared to most MPGCNs. Experimental results demonstrate that BankGCN achieves excellent performance in graph classification on a variety of benchmark graph datasets.",1
"This paper investigates two techniques for developing efficient self-supervised vision transformers (EsViT) for visual representation learning. First, we show through a comprehensive empirical study that multi-stage architectures with sparse self-attentions can significantly reduce modeling complexity but with a cost of losing the ability to capture fine-grained correspondences between image regions. Second, we propose a new pre-training task of region matching which allows the model to capture fine-grained region dependencies and as a result significantly improves the quality of the learned vision representations. Our results show that combining the two techniques, EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation, outperforming prior arts with around an order magnitude of higher throughput. When transferring to downstream linear classification tasks, EsViT outperforms its supervised counterpart on 17 out of 18 datasets. The code and models will be publicly available.",0
"This research investigates two methods for creating efficient self-supervised vision transformers (EsViT) to learn visual representations. Firstly, we conducted an extensive empirical study and found that multi-stage architectures with sparse self-attentions can significantly reduce complexity. However, this comes at the cost of losing the ability to capture fine-grained correspondences between image regions. Secondly, we propose a novel pre-training task called region matching, which enables the model to capture fine-grained region dependencies and enhances the quality of learned vision representations. Our findings demonstrate that by combining these two techniques, EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation, surpassing prior methods with a higher throughput. Additionally, EsViT outperforms its supervised counterpart on 17 out of 18 datasets when transferring to downstream linear classification tasks. The code and models will be publicly accessible.",1
"Longitudinal MRIs are often used to capture the gradual deterioration of brain structure and function caused by aging or neurological diseases. Analyzing this data via machine learning generally requires a large number of ground-truth labels, which are often missing or expensive to obtain. Reducing the need for labels, we propose a self-supervised strategy for representation learning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts in contrastive learning, LNE explicitly models the similarity between trajectory vectors across different subjects. We do so by building a graph in each training iteration defining neighborhoods in the latent space so that the progression direction of a subject follows the direction of its neighbors. This results in a smooth trajectory field that captures the global morphological change of the brain while maintaining the local continuity. We apply LNE to longitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274 healthy subjects, and Alzheimer's Disease Neuroimaging Initiative (ADNI, N=632). The visualization of the smooth trajectory vector field and superior performance on downstream tasks demonstrate the strength of the proposed method over existing self-supervised methods in extracting information associated with normal aging and in revealing the impact of neurodegenerative disorders. The code is available at \url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding.git}.",0
"Longitudinal MRIs are frequently utilized to observe the gradual decline of brain function and structure caused by neurological diseases or aging. However, machine learning analysis of this data typically necessitates numerous ground-truth labels, which can be difficult and expensive to obtain. To address this issue, we propose a self-supervised representation learning strategy called Longitudinal Neighborhood Embedding (LNE), which reduces the need for labels. LNE is inspired by contrastive learning concepts and explicitly models the similarity between trajectory vectors across different subjects. We achieve this by constructing a graph in each training iteration that defines neighborhoods in the latent space, allowing a subject's progression direction to follow that of its neighbors. This results in a smooth trajectory field that captures the global morphological change of the brain while maintaining local continuity. We evaluate LNE on longitudinal T1w MRIs from two neuroimaging studies: a dataset comprising 274 healthy subjects and Alzheimer's Disease Neuroimaging Initiative (ADNI, N=632). The visualization of the smooth trajectory vector field and superior performance on downstream tasks demonstrate the effectiveness of the proposed method in extracting information associated with normal aging and revealing the impact of neurodegenerative disorders. The code is available at \url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding.git}.",1
"We present MoDist as a novel method to explicitly distill motion information into self-supervised video representations. Compared to previous video representation learning methods that mostly focus on learning motion cues implicitly from RGB inputs, we show that the representation learned with our MoDist method focus more on foreground motion regions and thus generalizes better to downstream tasks. To achieve this, MoDist enriches standard contrastive learning objectives for RGB video clips with a cross-modal learning objective between a Motion pathway and a Visual pathway. We evaluate MoDist on several datasets for both action recognition (UCF101/HMDB51/SSv2) as well as action detection (AVA), and demonstrate state-of-the-art self-supervised performance on all datasets. Furthermore, we show that MoDist representation can be as effective as (in some cases even better than) representations learned with full supervision. Given its simplicity, we hope MoDist could serve as a strong baseline for future research in self-supervised video representation learning.",0
"Introducing MoDist, a novel approach to distilling motion information into self-supervised video representations. Unlike previous methods that implicitly learn motion cues from RGB inputs, our MoDist method explicitly focuses on foreground motion regions, resulting in better generalization to downstream tasks. To achieve this, we enhance standard contrastive learning objectives for RGB video clips with a cross-modal learning objective between a Motion pathway and a Visual pathway. Our evaluation on multiple datasets for action recognition and detection (UCF101/HMDB51/SSv2 and AVA) demonstrates state-of-the-art self-supervised performance. Additionally, we show that MoDist representations are as effective as (and in some cases better than) those learned with full supervision. Given its straightforwardness, we believe that MoDist can serve as a robust baseline for future research in self-supervised video representation learning.",1
"Generalization has been a long-standing challenge for reinforcement learning (RL). Visual RL, in particular, can be easily distracted by irrelevant factors in high-dimensional observation space. In this work, we consider robust policy learning which targets zero-shot generalization to unseen visual environments with large distributional shift. We propose SECANT, a novel self-expert cloning technique that leverages image augmentation in two stages to decouple robust representation learning from policy optimization. Specifically, an expert policy is first trained by RL from scratch with weak augmentations. A student network then learns to mimic the expert policy by supervised learning with strong augmentations, making its representation more robust against visual variations compared to the expert. Extensive experiments demonstrate that SECANT significantly advances the state of the art in zero-shot generalization across 4 challenging domains. Our average reward improvements over prior SOTAs are: DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based autonomous driving (+47.7%), and indoor object navigation (+15.8%). Code release and video are available at https://linxifan.github.io/secant-site/.",0
"For a long time, reinforcement learning (RL) has struggled with generalization, especially in visual RL. The latter is prone to getting distracted by irrelevant factors in observation space that has a high dimensionality. This study focuses on robust policy learning, which aims to achieve zero-shot generalization in visual environments that have large distributional shifts. To achieve this goal, the authors of the study propose a novel self-expert cloning technique called SECANT, which uses two stages of image augmentation to separate robust representation learning from policy optimization. The first stage involves training an expert policy using RL with weak augmentations, while the second stage involves a student network that learns to mimic the expert policy using strong augmentations. This approach makes the student network's representation more robust than the expert's against visual variations. The researchers conducted extensive experiments that show SECANT significantly improves the state of the art in zero-shot generalization across four challenging domains: DeepMind Control, robotic manipulation, vision-based autonomous driving, and indoor object navigation. The average reward improvements over prior SOTAs were +26.5%, +337.8%, +47.7%, and +15.8%, respectively. The code release and a video are available at https://linxifan.github.io/secant-site/.",1
"Multiple views of data, both naturally acquired (e.g., image and audio) and artificially produced (e.g., via adding different noise to data samples), have proven useful in enhancing representation learning. Natural views are often handled by multiview analysis tools, e.g., (deep) canonical correlation analysis [(D)CCA], while the artificial ones are frequently used in self-supervised learning (SSL) paradigms, e.g., SimCLR and Barlow Twins. Both types of approaches often involve learning neural feature extractors such that the embeddings of data exhibit high cross-view correlations. Although intuitive, the effectiveness of correlation-based neural embedding is only empirically validated. This work puts forth a theory-backed framework for unsupervised multiview learning. Our development starts with proposing a multiview model, where each view is a nonlinear mixture of shared and private components. Consequently, the learning problem boils down to shared/private component identification and disentanglement. Under this model, latent correlation maximization is shown to guarantee the extraction of the shared components across views (up to certain ambiguities). In addition, the private information in each view can be provably disentangled from the shared using proper regularization design. The method is tested on a series of tasks, e.g., downstream clustering, which all show promising performance. Our development also provides a unifying perspective for understanding various DCCA and SSL schemes.",0
"Representation learning has been enhanced by utilizing multiple views of data, such as naturally acquired ones like image and audio, as well as artificially produced ones through the addition of different noise to data samples. Multiview analysis tools like (deep) canonical correlation analysis have been used for natural views, while self-supervised learning paradigms like SimCLR and Barlow Twins have been utilized for artificial views. Both approaches involve learning neural feature extractors that produce embeddings of data with high cross-view correlations. While this approach is intuitive, its effectiveness has only been empirically validated. This work proposes a theory-backed framework for unsupervised multiview learning, starting with a multiview model where each view is a nonlinear mixture of shared and private components. The learning problem is identifying and disentangling shared and private components. Latent correlation maximization is shown to extract shared components across views with proper regularization design disentangling private information from shared. The method is tested on various tasks, showing promising performance and providing a unifying perspective for understanding DCCA and SSL schemes.",1
"Graph-level representations are critical in various real-world applications, such as predicting the properties of molecules. But in practice, precise graph annotations are generally very expensive and time-consuming. To address this issue, graph contrastive learning constructs instance discrimination task which pulls together positive pairs (augmentation pairs of the same graph) and pushes away negative pairs (augmentation pairs of different graphs) for unsupervised representation learning. However, since for a query, its negatives are uniformly sampled from all graphs, existing methods suffer from the critical sampling bias issue, i.e., the negatives likely having the same semantic structure with the query, leading to performance degradation. To mitigate this sampling bias issue, in this paper, we propose a Prototypical Graph Contrastive Learning (PGCL) approach. Specifically, PGCL models the underlying semantic structure of the graph data via clustering semantically similar graphs into the same group, and simultaneously encourages the clustering consistency for different augmentations of the same graph. Then given a query, it performs negative sampling via drawing the graphs from those clusters that differ from the cluster of query, which ensures the semantic difference between query and its negative samples. Moreover, for a query, PGCL further reweights its negative samples based on the distance between their prototypes (cluster centroids) and the query prototype such that those negatives having moderate prototype distance enjoy relatively large weights. This reweighting strategy is proved to be more effective than uniform sampling. Experimental results on various graph benchmarks testify the advantages of our PGCL over state-of-the-art methods.",0
"In real-world applications such as predicting molecule properties, graph-level representations are crucial. However, creating precise graph annotations is typically expensive and time-consuming. To solve this problem, graph contrastive learning creates an instance discrimination task that brings together positive pairs (augmentation pairs of the same graph) and separates negative pairs (augmentation pairs of different graphs) for unsupervised representation learning. Nevertheless, current methods suffer from a critical sampling bias issue where the negatives are uniformly sampled from all graphs, leading to performance degradation because the negatives likely have the same semantic structure as the query. To address this bias issue, this paper introduces a Prototypical Graph Contrastive Learning (PGCL) approach. PGCL models the semantic structure of graph data by clustering similar graphs together and encouraging clustering consistency for different augmentations of the same graph. For a query, it selects negative samples by drawing graphs from clusters that differ from the query's cluster to ensure semantic differences. Additionally, PGCL reweights negative samples based on their distance from the query's prototype to improve performance. Experimental results on various graph benchmarks demonstrate the superiority of PGCL over existing methods.",1
"Learning maps between data samples is fundamental. Applications range from representation learning, image translation and generative modeling, to the estimation of spatial deformations. Such maps relate feature vectors, or map between feature spaces. Well-behaved maps should be regular, which can be imposed explicitly or may emanate from the data itself. We explore what induces regularity for spatial transformations, e.g., when computing image registrations. Classical optimization-based models compute maps between pairs of samples and rely on an appropriate regularizer for well-posedness. Recent deep learning approaches have attempted to avoid using such regularizers altogether by relying on the sample population instead. We explore if it is possible to obtain spatial regularity using an inverse consistency loss only and elucidate what explains map regularity in such a context. We find that deep networks combined with an inverse consistency loss and randomized off-grid interpolation yield well behaved, approximately diffeomorphic, spatial transformations. Despite the simplicity of this approach, our experiments present compelling evidence, on both synthetic and real data, that regular maps can be obtained without carefully tuned explicit regularizers, while achieving competitive registration performance.",0
"The ability to learn maps between data samples is crucial in a variety of applications, such as representation learning, image translation, generative modeling, and spatial deformation estimation. These maps establish a relationship between feature vectors, or between feature spaces. To ensure well-behaved maps, regularity is necessary, either explicitly imposed or arising from the data itself. We investigate the factors that induce regularity in spatial transformations, particularly in image registration. Traditional optimization-based models rely on a regularizer to achieve well-posedness, but recent deep learning approaches avoid such regularizers and rely on the sample population instead. We examine whether inverse consistency loss alone can achieve spatial regularity and explain the factors that contribute to map regularity in such a scenario. Our experiments demonstrate that combining deep networks with inverse consistency loss and randomized off-grid interpolation can produce well-behaved, approximately diffeomorphic spatial transformations, without the need for explicitly tuned regularizers, while still achieving competitive registration performance on both synthetic and real data.",1
"The goal of supervised representation learning is to construct effective data representations for prediction. Among all the characteristics of an ideal nonparametric representation of high-dimensional complex data, sufficiency, low dimensionality and disentanglement are some of the most essential ones. We propose a deep dimension reduction approach to learning representations with these characteristics. The proposed approach is a nonparametric generalization of the sufficient dimension reduction method. We formulate the ideal representation learning task as that of finding a nonparametric representation that minimizes an objective function characterizing conditional independence and promoting disentanglement at the population level. We then estimate the target representation at the sample level nonparametrically using deep neural networks. We show that the estimated deep nonparametric representation is consistent in the sense that its excess risk converges to zero. Our extensive numerical experiments using simulated and real benchmark data demonstrate that the proposed methods have better performance than several existing dimension reduction methods and the standard deep learning models in the context of classification and regression.",0
"Supervised representation learning aims to create effective data representations to make predictions. Sufficiency, low dimensionality, and disentanglement are crucial features of an ideal nonparametric representation of complex high-dimensional data. Our proposed approach uses deep dimension reduction to learn representations with these features, as a nonparametric extension of the sufficient dimension reduction method. Our objective is to find a nonparametric representation that minimizes an objective function promoting disentanglement and characterizing conditional independence at the population level. We then estimate this target representation using deep neural networks at the sample level. Our numerical experiments using both simulated and real benchmark data show that our method outperforms several existing dimension reduction methods and standard deep learning models in classification and regression tasks, with consistent estimation and zero excess risk convergence.",1
"The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.",0
"Although the Transformer architecture is popular in various domains, such as natural language processing and computer vision, it has not been able to achieve competitive performance on widely used leaderboards for graph-level prediction, unlike mainstream GNN variants. This has left researchers puzzled about how Transformers can excel in graph representation learning. In this study, we introduce Graphormer, which is based on the standard Transformer architecture and can deliver impressive outcomes for a wide range of graph representation learning tasks, particularly the recent OGB Large-Scale Challenge. Our primary insight into using Transformers for graphs is the need to encode the structural information of a graph effectively in the model. To this end, we propose simple and effective methods for structural encoding to help Graphormer better represent graph-structured data. Additionally, we mathematically describe the expressive power of Graphormer and demonstrate that our structural encoding methods allow Graphormer to cover many popular GNN variants as special cases.",1
"Path representations are critical in a variety of transportation applications, such as estimating path ranking in path recommendation systems and estimating path travel time in navigation systems. Existing studies often learn task-specific path representations in a supervised manner, which require a large amount of labeled training data and generalize poorly to other tasks. We propose an unsupervised learning framework Path InfoMax (PIM) to learn generic path representations that work for different downstream tasks. We first propose a curriculum negative sampling method, for each input path, to generate a small amount of negative paths, by following the principles of curriculum learning. Next, \emph{PIM} employs mutual information maximization to learn path representations from both a global and a local view. In the global view, PIM distinguishes the representations of the input paths from those of the negative paths. In the local view, \emph{PIM} distinguishes the input path representations from the representations of the nodes that appear only in the negative paths. This enables the learned path representations to encode both global and local information at different scales. Extensive experiments on two downstream tasks, ranking score estimation and travel time estimation, using two road network datasets suggest that PIM significantly outperforms other unsupervised methods and is also able to be used as a pre-training method to enhance supervised path representation learning.",0
"In transportation applications, such as path recommendation and navigation systems, path representations are crucial for estimating path ranking and travel time. However, conventional supervised learning methods for creating task-specific path representations require a large amount of labeled training data and have limited generalizability. To address this, we propose an unsupervised learning framework called Path InfoMax (PIM), which generates generic path representations that work for various downstream tasks. PIM utilizes a curriculum negative sampling method to produce a small number of negative paths for each input path, following the principles of curriculum learning. Then, it uses mutual information maximization to learn path representations from both global and local perspectives. PIM is able to distinguish input path representations from those of negative paths in the global view and from representations of nodes that only appear in negative paths in the local view. This approach allows the learned path representations to encode global and local information at different scales. Our experiments on two road network datasets and two downstream tasks, ranking score estimation and travel time estimation, demonstrate that PIM outperforms other unsupervised methods and can also be used as a pre-training method to improve supervised path representation learning.",1
"Attention mechanism enables the Graph Neural Networks(GNNs) to learn the attention weights between the target node and its one-hop neighbors, the performance is further improved. However, the most existing GNNs are oriented to homogeneous graphs and each layer can only aggregate the information of one-hop neighbors. Stacking multi-layer networks will introduce a lot of noise and easily lead to over smoothing. We propose a Multi-hop Heterogeneous Neighborhood information Fusion graph representation learning method (MHNF). Specifically, we first propose a hybrid metapath autonomous extraction model to efficiently extract multi-hop hybrid neighbors. Then, we propose a hop-level heterogeneous Information aggregation model, which selectively aggregates different-hop neighborhood information within the same hybrid metapath. Finally, a hierarchical semantic attention fusion model (HSAF) is proposed, which can efficiently integrate different-hop and different-path neighborhood information respectively. This paper can solve the problem of aggregating the multi-hop neighborhood information and can learn hybrid metapaths for target task, reducing the limitation of manually specifying metapaths. In addition, HSAF can extract the internal node information of the metapaths and better integrate the semantic information of different levels. Experimental results on real datasets show that MHNF is superior to state-of-the-art methods in node classification and clustering tasks (10.94% - 69.09% and 11.58% - 394.93% relative improvement on average, respectively).",0
"By utilizing the attention mechanism, Graph Neural Networks (GNNs) can effectively learn the attention weights between the target node and its one-hop neighbors, resulting in enhanced performance. However, current GNNs are primarily designed for homogeneous graphs, and their layers can only aggregate information from one-hop neighbors. This limitation often leads to over-smoothing and increased noise when stacking multiple layers. To address this issue, we propose the Multi-hop Heterogeneous Neighborhood information Fusion (MHNF) method, which can efficiently extract multi-hop hybrid neighbors using a hybrid metapath autonomous extraction model. The MHNF method also utilizes a hop-level heterogeneous Information aggregation model to selectively aggregate different-hop neighborhood information within the same hybrid metapath. To integrate different-hop and different-path neighborhood information respectively, we introduce a hierarchical semantic attention fusion model (HSAF). Our approach can overcome the challenge of aggregating multi-hop neighborhood information and learn hybrid metapaths for the target task, thereby reducing the need for manually specifying metapaths. Furthermore, HSAF can extract the internal node information of the metapaths and better integrate the semantic information of different levels. Our experimental results on real datasets demonstrate that MHNF outperforms state-of-the-art methods in node classification and clustering tasks (with relative improvements of 10.94% - 69.09% and 11.58% - 394.93% on average, respectively).",1
"As the representations output by Graph Neural Networks (GNNs) are increasingly employed in real-world applications, it becomes important to ensure that these representations are fair and stable. In this work, we establish a key connection between counterfactual fairness and stability and leverage it to propose a novel framework, NIFTY (uNIfying Fairness and stabiliTY), which can be used with any GNN to learn fair and stable representations. We introduce a novel objective function that simultaneously accounts for fairness and stability and develop a layer-wise weight normalization using the Lipschitz constant to enhance neural message passing in GNNs. In doing so, we enforce fairness and stability both in the objective function as well as in the GNN architecture. Further, we show theoretically that our layer-wise weight normalization promotes counterfactual fairness and stability in the resulting representations. We introduce three new graph datasets comprising of high-stakes decisions in criminal justice and financial lending domains. Extensive experimentation with the above datasets demonstrates the efficacy of our framework.",0
"The utilization of Graph Neural Networks (GNNs) in practical applications is increasing, making it crucial to ensure that the produced representations are fair and stable. This study establishes a correlation between counterfactual fairness and stability and proposes a new framework called NIFTY (uNIfying Fairness and stabiliTY), which can be used with any GNN to learn equitable and stable representations. A new objective function, which considers fairness and stability simultaneously, is introduced, and a layer-wise weight normalization that employs the Lipschitz constant to enhance neural message passing in GNNs is developed. By doing so, fairness and stability are enforced in both the objective function and GNN architecture. Additionally, it is theoretically demonstrated that the layer-wise weight normalization enhances counterfactual fairness and stability in the resulting representations. Three new graph datasets that contain high-stakes decisions in criminal justice and financial lending domains are introduced. Extensive experimentation with these datasets demonstrates the effectiveness of the proposed framework.",1
"Sensory input from multiple sources is crucial for robust and coherent human perception. Different sources contribute complementary explanatory factors. Similarly, research studies often collect multimodal imaging data, each of which can provide shared and unique information. This observation motivated the design of powerful multimodal self-supervised representation-learning algorithms. In this paper, we unify recent work on multimodal self-supervised learning under a single framework. Observing that most self-supervised methods optimize similarity metrics between a set of model components, we propose a taxonomy of all reasonable ways to organize this process. We first evaluate models on toy multimodal MNIST datasets and then apply them to a multimodal neuroimaging dataset with Alzheimer's disease patients. We find that (1) multimodal contrastive learning has significant benefits over its unimodal counterpart, (2) the specific composition of multiple contrastive objectives is critical to performance on a downstream task, (3) maximization of the similarity between representations has a regularizing effect on a neural network, which can sometimes lead to reduced downstream performance but still reveal multimodal relations. Results show that the proposed approach outperforms previous self-supervised encoder-decoder methods based on canonical correlation analysis (CCA) or the mixture-of-experts multimodal variational autoEncoder (MMVAE) on various datasets with a linear evaluation protocol. Importantly, we find a promising solution to uncover connections between modalities through a jointly shared subspace that can help advance work in our search for neuroimaging biomarkers.",0
"Human perception requires input from multiple sources, which offer complementary explanatory factors. Similarly, research studies often gather multimodal imaging data, each providing both shared and unique information. To address this, powerful multimodal self-supervised representation-learning algorithms have been designed. This paper unifies recent work on multimodal self-supervised learning under a single framework. By proposing a taxonomy of all reasonable ways to organize the process of optimizing similarity metrics between a set of model components, we evaluate the models on toy multimodal MNIST datasets and then apply them to a multimodal neuroimaging dataset with Alzheimer's disease patients. Our results show that multimodal contrastive learning has significant benefits over its unimodal counterpart, and the specific composition of multiple contrastive objectives is critical to performance on a downstream task. We also find that maximization of the similarity between representations has a regularizing effect on a neural network, which can sometimes lead to reduced downstream performance but still reveal multimodal relations. Moreover, this approach outperforms previous self-supervised encoder-decoder methods based on canonical correlation analysis (CCA) or the mixture-of-experts multimodal variational autoEncoder (MMVAE) on various datasets with a linear evaluation protocol. Lastly, we find a promising solution to uncover connections between modalities through a jointly shared subspace that can help advance work in our search for neuroimaging biomarkers.",1
"We present an online multi-task learning approach for adaptive nonlinear control, which we call Online Meta-Adaptive Control (OMAC). The goal is to control a nonlinear system subject to adversarial disturbance and unknown $\textit{environment-dependent}$ nonlinear dynamics, under the assumption that the environment-dependent dynamics can be well captured with some shared representation. Our approach is motivated by robot control, where a robotic system encounters a sequence of new environmental conditions that it must quickly adapt to. A key emphasis is to integrate online representation learning with established methods from control theory, in order to arrive at a unified framework that yields both control-theoretic and learning-theoretic guarantees. We provide instantiations of our approach under varying conditions, leading to the first non-asymptotic end-to-end convergence guarantee for multi-task adaptive nonlinear control. OMAC can also be integrated with deep representation learning. Experiments show that OMAC significantly outperforms conventional adaptive control approaches which do not learn the shared representation.",0
"We introduce Online Meta-Adaptive Control (OMAC), an online multi-task learning method for adaptive nonlinear control. Our focus is on controlling a nonlinear system in the presence of adversarial disturbance and unknown nonlinear dynamics that depend on the environment. We assume that these dynamics can be captured using a shared representation. Our inspiration comes from robot control, where a robotic system must adapt quickly to new environmental conditions. We aim to combine online representation learning with control theory techniques to create a unified framework that provides both control-theoretic and learning-theoretic guarantees. We demonstrate the effectiveness of OMAC under various conditions and show that it yields the first non-asymptotic end-to-end convergence guarantee for multi-task adaptive nonlinear control. Additionally, OMAC can be combined with deep representation learning. Our experiments show that OMAC outperforms conventional adaptive control approaches that do not utilize the shared representation.",1
"Convolutional neural networks for visual recognition require large amounts of training samples and usually benefit from data augmentation. This paper proposes PatchMix, a data augmentation method that creates new samples by composing patches from pairs of images in a grid-like pattern. These new samples' ground truth labels are set as proportional to the number of patches from each image. We then add a set of additional losses at the patch-level to regularize and to encourage good representations at both the patch and image levels. A ResNet-50 model trained on ImageNet using PatchMix exhibits superior transfer learning capabilities across a wide array of benchmarks. Although PatchMix can rely on random pairings and random grid-like patterns for mixing, we explore evolutionary search as a guiding strategy to discover optimal grid-like patterns and image pairing jointly. For this purpose, we conceive a fitness function that bypasses the need to re-train a model to evaluate each choice. In this way, PatchMix outperforms a base model on CIFAR-10 (+1.91), CIFAR-100 (+5.31), Tiny Imagenet (+3.52), and ImageNet (+1.16) by significant margins, also outperforming previous state-of-the-art pairwise augmentation strategies.",0
"To effectively recognize visuals, convolutional neural networks require a considerable number of training samples and often benefit from data augmentation. An innovative data augmentation technique called PatchMix is suggested in this study, which produces new samples by merging image patches in a grid-like formation from pairs of images. The labels of these new samples are proportioned according to the number of patches from each image. Additionally, a set of patch-level losses are introduced to maintain stable representations at both the patch and image levels. A ResNet-50 model trained on ImageNet with PatchMix shows superior transfer learning abilities across various benchmarks. While PatchMix can use random pairings and grid-like patterns for mixing, the authors also explore evolutionary search as a guiding strategy to identify optimal grid-like patterns and image pairing. To do this, they develop a fitness function that avoids the need to retrain the model to evaluate each option. PatchMix outperforms a base model on CIFAR-10 (+1.91), CIFAR-100 (+5.31), Tiny Imagenet (+3.52), and ImageNet (+1.16) by considerable margins, surpassing prior state-of-the-art pairwise augmentation strategies.",1
"Real-world applications of machine learning tools in high-stakes domains are often regulated to be fair, in the sense that the predicted target should satisfy some quantitative notion of parity with respect to a protected attribute. However, the exact tradeoff between fairness and accuracy with a real-valued target is not clear. In this paper, we characterize the inherent tradeoff between statistical parity and accuracy in the regression setting by providing a lower bound on the error of any fair regressor. Our lower bound is sharp, algorithm-independent, and admits a simple interpretation: when the moments of the target differ between groups, any fair algorithm has to make a large error on at least one of the groups. We further extend this result to give a lower bound on the joint error of any (approximately) fair algorithm, using the Wasserstein distance to measure the quality of the approximation. On the upside, we establish the first connection between individual fairness, accuracy parity, and the Wasserstein distance by showing that if a regressor is individually fair, it also approximately verifies the accuracy parity, where the gap is given by the Wasserstein distance between the two groups. Inspired by our theoretical results, we develop a practical algorithm for fair regression through the lens of representation learning, and conduct experiments on a real-world dataset to corroborate our findings.",0
"Machine learning tools used in high-stakes domains must adhere to regulations that ensure fairness by maintaining quantitative parity with respect to a protected attribute. However, the balance between accuracy and fairness is unclear when dealing with a real-valued target. In this study, we establish the tradeoff between statistical parity and accuracy in regression using a lower bound on the error of any fair regressor. Our independent, sharp lower bound highlights that any fair algorithm has to make a large error on at least one group when the moments of the target differ between groups. We extend this result to provide a lower bound on the joint error of any (approximately) fair algorithm using the Wasserstein distance to measure the approximation quality. Our study also establishes the relationship between individual fairness, accuracy parity, and the Wasserstein distance. We present a practical algorithm for fair regression using representation learning and validate our findings through experiments on a real-world dataset.",1
"Link prediction is a very fundamental task on graphs. Inspired by traditional path-based methods, in this paper we propose a general and flexible representation learning framework based on paths for link prediction. Specifically, we define the representation of a pair of nodes as the generalized sum of all path representations, with each path representation as the generalized product of the edge representations in the path. Motivated by the Bellman-Ford algorithm for solving the shortest path problem, we show that the proposed path formulation can be efficiently solved by the generalized Bellman-Ford algorithm. To further improve the capacity of the path formulation, we propose the Neural Bellman-Ford Network (NBFNet), a general graph neural network framework that solves the path formulation with learned operators in the generalized Bellman-Ford algorithm. The NBFNet parameterizes the generalized Bellman-Ford algorithm with 3 neural components, namely INDICATOR, MESSAGE and AGGREGATE functions, which corresponds to the boundary condition, multiplication operator, and summation operator respectively. The NBFNet is very general, covers many traditional path-based methods, and can be applied to both homogeneous graphs and multi-relational graphs (e.g., knowledge graphs) in both transductive and inductive settings. Experiments on both homogeneous graphs and knowledge graphs show that the proposed NBFNet outperforms existing methods by a large margin in both transductive and inductive settings, achieving new state-of-the-art results.",0
"In this article, we introduce a versatile and adaptable framework for link prediction based on paths, which is a crucial task on graphs. Our approach draws inspiration from conventional path-based methods and defines the representation of a node pair as the overall sum of all path representations. Each path representation is calculated as the product of the edge representations in the path. We demonstrate that our proposed path formulation can be efficiently solved by using the generalized Bellman-Ford algorithm, which is based on the Bellman-Ford algorithm for finding the shortest path. To enhance the path formulation's capability, we propose the Neural Bellman-Ford Network (NBFNet), a general graph neural network framework that utilizes learned operators to solve the path formulation through the generalized Bellman-Ford algorithm. The NBFNet comprises three neural components: INDICATOR, MESSAGE, and AGGREGATE functions, which correspond to the multiplication operator, summation operator, and boundary condition, respectively. Our NBFNet is very general and can be applied in both transductive and inductive settings on homogeneous graphs and multi-relational graphs like knowledge graphs. We conducted experiments on both homogeneous graphs and knowledge graphs, which demonstrated that our proposed NBFNet significantly outperforms existing methods, achieving new state-of-the-art results.",1
"The inductive biases of graph representation learning algorithms are often encoded in the background geometry of their embedding space. In this paper, we show that general directed graphs can be effectively represented by an embedding model that combines three components: a pseudo-Riemannian metric structure, a non-trivial global topology, and a unique likelihood function that explicitly incorporates a preferred direction in embedding space. We demonstrate the representational capabilities of this method by applying it to the task of link prediction on a series of synthetic and real directed graphs from natural language applications and biology. In particular, we show that low-dimensional cylindrical Minkowski and anti-de Sitter spacetimes can produce equal or better graph representations than curved Riemannian manifolds of higher dimensions.",0
"The background geometry of the embedding space often encodes the inductive biases of graph representation learning algorithms. This paper proposes an embedding model that effectively represents general directed graphs through a combination of three components: a non-trivial global topology, a pseudo-Riemannian metric structure, and a unique likelihood function that incorporates a preferred direction in embedding space. The method's representational capabilities are demonstrated through link prediction tasks on synthetic and real directed graphs from natural language applications and biology. The study reveals that low-dimensional cylindrical Minkowski and anti-de Sitter spacetimes can produce graph representations that are equal or even better than those of higher-dimensional curved Riemannian manifolds.",1
"Recently, transformation-based self-supervised learning has been applied to generative adversarial networks (GANs) to mitigate the catastrophic forgetting problem of discriminator by learning stable representations. However, the separate self-supervised tasks in existing self-supervised GANs cause an inconsistent goal with generative modeling due to the learning of the generator from their generator distribution-agnostic classifiers. To address this issue, we propose a novel self-supervised GANs framework with label augmentation, i.e., augmenting the GAN labels (real or fake) with the self-supervised pseudo-labels. In particular, the discriminator and the self-supervised classifier are unified to learn a single task that predicts the augmented label such that the discriminator/classifier is aware of the generator distribution, while the generator tries to confuse the discriminator/classifier by optimizing the discrepancy between the transformed real and generated distributions. Theoretically, we prove that the generator, at the equilibrium point, converges to replicate the data distribution. Empirically, we demonstrate that the proposed method significantly outperforms competitive baselines on both generative modeling and representation learning across benchmark datasets.",0
"The use of transformation-based self-supervised learning in generative adversarial networks (GANs) has been successful in overcoming the catastrophic forgetting problem of the discriminator by creating stable representations. However, current self-supervised GANs have separate self-supervised tasks that do not align with generative modeling, as the generator is learned from agnostic classifiers. To solve this problem, we present a new self-supervised GANs framework that uses label augmentation, where GAN labels (real or fake) are combined with self-supervised pseudo-labels. This framework unifies the discriminator and self-supervised classifier to predict the augmented label, allowing the discriminator/classifier to understand the generator distribution while the generator attempts to confuse the discriminator/classifier by optimizing the discrepancy between transformed real and generated distributions. We show that the generator, at equilibrium, replicates the data distribution, and our method outperforms competitive baselines on generative modeling and representation learning across benchmark datasets.",1
"The abundance and ease of utilizing sound, along with the fact that auditory clues reveal so much about what happens in the scene, make the audio-visual space a perfectly intuitive choice for self-supervised representation learning. However, the current literature suggests that training on \textit{uncurated} data yields considerably poorer representations compared to the \textit{curated} alternatives collected in supervised manner, and the gap only narrows when the volume of data significantly increases. Furthermore, the quality of learned representations is known to be heavily influenced by the size and taxonomy of the curated datasets used for self-supervised training. This begs the question of whether we are celebrating too early on catching up with supervised learning when our self-supervised efforts still rely almost exclusively on curated data. In this paper, we study the efficacy of learning from Movies and TV Shows as forms of uncurated data for audio-visual self-supervised learning. We demonstrate that a simple model based on contrastive learning, trained on a collection of movies and TV shows, not only dramatically outperforms more complex methods which are trained on orders of magnitude larger uncurated datasets, but also performs very competitively with the state-of-the-art that learns from large-scale curated data. We identify that audiovisual patterns like the appearance of the main character or prominent scenes and mise-en-sc\`ene which frequently occur through the whole duration of a movie, lead to an overabundance of easy negative instances in the contrastive learning formulation. Capitalizing on such observation, we propose a hierarchical sampling policy, which despite its simplicity, effectively improves the performance, particularly when learning from TV shows which naturally face less semantic diversity.",0
"The audio-visual space is an ideal choice for self-supervised representation learning due to the abundance and ease of using sound, as well as the valuable information provided by auditory clues. However, current literature suggests that utilizing uncurated data results in poorer representations compared to supervised alternatives, even with significant increases in data volume. The quality of learned representations is heavily influenced by the size and taxonomy of curated datasets used for self-supervised training, raising questions about the celebration of catching up with supervised learning while relying almost exclusively on curated data. This paper investigates the effectiveness of using Movies and TV Shows as uncurated data for audio-visual self-supervised learning. A simple model based on contrastive learning, trained on a collection of movies and TV shows, outperforms more complex methods trained on larger uncurated datasets and performs competitively with state-of-the-art models using curated data. The study identifies audiovisual patterns, such as the appearance of main characters or prominent scenes, that lead to an overabundance of easy negative instances in the contrastive learning formulation. A hierarchical sampling policy is proposed to improve performance, particularly when learning from TV shows with less semantic diversity.",1
"The recent emergence of contrastive learning approaches facilitates the research on graph representation learning (GRL), introducing graph contrastive learning (GCL) into the literature. These methods contrast semantically similar and dissimilar sample pairs to encode the semantics into node or graph embeddings. However, most existing works only performed model-level evaluation, and did not explore the combination space of modules for more comprehensive and systematic studies. For effective module-level evaluation, we propose a framework that decomposes GCL models into four modules: (1) a sampler to generate anchor, positive and negative data samples (nodes or graphs); (2) an encoder and a readout function to get sample embeddings; (3) a discriminator to score each sample pair (anchor-positive and anchor-negative); and (4) an estimator to define the loss function. Based on this framework, we conduct controlled experiments over a wide range of architectural designs and hyperparameter settings on node and graph classification tasks. Specifically, we manage to quantify the impact of a single module, investigate the interaction between modules, and compare the overall performance with current model architectures. Our key findings include a set of module-level guidelines for GCL, e.g., simple samplers from LINE and DeepWalk are strong and robust; an MLP encoder associated with Sum readout could achieve competitive performance on graph classification. Finally, we release our implementations and results as OpenGCL, a modularized toolkit that allows convenient reproduction, standard model and module evaluation, and easy extension.",0
"The development of contrastive learning techniques has provided an opportunity to advance research on graph representation learning (GRL) through the introduction of graph contrastive learning (GCL) in the field. These methods compare pairs of samples with similar and dissimilar meanings to encode semantics into node or graph embeddings. However, prior research has only evaluated models at a high level, neglecting to explore the combination space of modules in a more comprehensive and systematic manner. To address this, we propose a framework that decomposes GCL models into four modules: a sampler, an encoder and readout function, a discriminator, and an estimator. By using this framework, we conduct controlled experiments on node and graph classification tasks, allowing us to quantify the impact of individual modules, explore module interactions, and compare overall performance with existing model architectures. Our findings provide a set of module-level guidelines for GCL, including the effectiveness of simple samplers and MLP encoders with Sum readout for graph classification. Finally, we release our results and implementations as OpenGCL, a toolkit that allows for easy reproduction, standard model and module evaluation, and simple extension.",1
"Learning meaningful representations of data that can address challenges such as batch effect correction, data integration and counterfactual inference is a central problem in many domains including computational biology. Adopting a Conditional VAE framework, we identify the mathematical principle that unites these challenges: learning a representation that is marginally independent of a condition variable. We therefore propose the Contrastive Mixture of Posteriors (CoMP) method that uses a novel misalignment penalty to enforce this independence. This penalty is defined in terms of mixtures of the variational posteriors themselves, unlike prior work which uses external discrepancy measures such as MMD to ensure independence in latent space. We show that CoMP has attractive theoretical properties compared to previous approaches, especially when there is complex global structure in latent space. We further demonstrate state of the art performance on a number of real-world problems, including the challenging tasks of aligning human tumour samples with cancer cell-lines and performing counterfactual inference on single-cell RNA sequencing data. Incidentally, we find parallels with the fair representation learning literature, and demonstrate CoMP has competitive performance in learning fair yet expressive latent representations.",0
"The central issue in various domains, such as computational biology, is to learn meaningful representations of data that can overcome difficulties like batch effect correction, data integration, and counterfactual inference. By adopting a Conditional VAE framework, we have discovered that the mathematical principle that unites these challenges is learning a representation that is marginally independent of a condition variable. To enforce this independence, we propose the Contrastive Mixture of Posteriors (CoMP) method, which uses a novel misalignment penalty defined in terms of mixtures of the variational posteriors themselves, unlike previous methods that use external discrepancy measures like MMD. CoMP has superior theoretical properties, particularly when global structure in latent space is complex, and we demonstrate state-of-the-art performance on real-world problems, including aligning human tumor samples with cancer cell-lines and performing counterfactual inference on single-cell RNA sequencing data. Interestingly, we observe similarities with the fair representation learning literature and show that CoMP achieves competitive performance in learning fair yet expressive latent representations.",1
"While multitask representation learning has become a popular approach in reinforcement learning (RL), theoretical understanding of why and when it works remains limited. This paper presents analyses for the statistical benefit of multitask representation learning in linear Markov Decision Process (MDP) under a generative model. In this paper, we consider an agent to learn a representation function $\phi$ out of a function class $\Phi$ from $T$ source tasks with $N$ data per task, and then use the learned $\hat{\phi}$ to reduce the required number of sample for a new task. We first discover a \emph{Least-Activated-Feature-Abundance} (LAFA) criterion, denoted as $\kappa$, with which we prove that a straightforward least-square algorithm learns a policy which is $\tilde{O}(H^2\sqrt{\frac{\mathcal{C}(\Phi)^2 \kappa d}{NT}+\frac{\kappa d}{n}})$ sub-optimal. Here $H$ is the planning horizon, $\mathcal{C}(\Phi)$ is $\Phi$'s complexity measure, $d$ is the dimension of the representation (usually $d\ll \mathcal{C}(\Phi)$) and $n$ is the number of samples for the new task. Thus the required $n$ is $O(\kappa d H^4)$ for the sub-optimality to be close to zero, which is much smaller than $O(\mathcal{C}(\Phi)^2\kappa d H^4)$ in the setting without multitask representation learning, whose sub-optimality gap is $\tilde{O}(H^2\sqrt{\frac{\kappa \mathcal{C}(\Phi)^2d}{n}})$. This theoretically explains the power of multitask representation learning in reducing sample complexity. Further, we note that to ensure high sample efficiency, the LAFA criterion $\kappa$ should be small. In fact, $\kappa$ varies widely in magnitude depending on the different sampling distribution for new task. This indicates adaptive sampling technique is important to make $\kappa$ solely depend on $d$. Finally, we provide empirical results of a noisy grid-world environment to corroborate our theoretical findings.",0
"Although multitask representation learning is popular in reinforcement learning (RL), there is limited theoretical understanding of why and when it is effective. This paper examines the statistical benefits of multitask representation learning in a linear Markov Decision Process (MDP) under a generative model. Specifically, the agent learns a representation function $\phi$ from $T$ source tasks with $N$ data per task, and then uses the learned function to reduce the number of samples required for a new task. The paper introduces the \emph{Least-Activated-Feature-Abundance} (LAFA) criterion, denoted as $\kappa$, which is used to prove that a straightforward least-square algorithm learns a policy that is $\tilde{O}(H^2\sqrt{\frac{\mathcal{C}(\Phi)^2 \kappa d}{NT}+\frac{\kappa d}{n}})$ sub-optimal. Here, $H$ is the planning horizon, $\mathcal{C}(\Phi)$ is the complexity measure of $\Phi$, $d$ is the dimension of the representation, and $n$ is the number of samples for the new task. The required $n$ is $O(\kappa d H^4)$ for the sub-optimality to be close to zero, which is much smaller than $O(\mathcal{C}(\Phi)^2\kappa d H^4)$ in the setting without multitask representation learning. This explains the power of multitask representation learning in reducing sample complexity. However, the LAFA criterion $\kappa$ must be small to ensure high sample efficiency, and it varies depending on the sampling distribution for the new task. Therefore, adaptive sampling techniques are important to make $\kappa$ depend solely on $d$. Finally, the paper provides empirical results from a noisy grid-world environment to support the theoretical findings.",1
"Learning meaningful representations of data is an important aspect of machine learning and has recently been successfully applied to many domains like language understanding or computer vision. Instead of training a model for one specific task, representation learning is about training a model to capture all useful information in the underlying data and make it accessible for a predictor. For predictive process analytics, it is essential to have all explanatory characteristics of a process instance available when making predictions about the future, as well as for clustering and anomaly detection. Due to the large variety of perspectives and types within business process data, generating a good representation is a challenging task. In this paper, we propose a novel approach for representation learning of business process instances which can process and combine most perspectives in an event log. In conjunction with a self-supervised pre-training method, we show the capabilities of the approach through a visualization of the representation space and case retrieval. Furthermore, the pre-trained model is fine-tuned to multiple process prediction tasks and demonstrates its effectiveness in comparison with existing approaches.",0
"Gaining valuable insights from data is a crucial aspect of machine learning and has been successfully implemented in various domains such as language comprehension and computer vision. Rather than training a model for a specific task, representation learning focuses on training a model to gather all useful information from the data and make it readily available for predictions. For predictive process analytics, having all explanatory features of a process instance is essential for future predictions, as well as clustering and anomaly detection. However, generating a suitable representation is challenging due to the vast range of perspectives and types in business process data. In this study, we introduce a new approach for representation learning of business process instances that incorporates most perspectives in an event log. We also demonstrate the approach's capabilities through a visualization of the representation space and case retrieval, in conjunction with a self-supervised pre-training method. Furthermore, we fine-tune the pre-trained model for multiple process prediction tasks and compare its effectiveness with existing approaches.",1
"In many control problems that include vision, optimal controls can be inferred from the location of the objects in the scene. This information can be represented using keypoints, which is a list of spatial locations in the input image. Previous works show that keypoint representations learned during unsupervised pre-training using encoder-decoder architectures can provide good features for control tasks. In this paper, we show that it is possible to learn efficient keypoint representations end-to-end, without the need for unsupervised pre-training, decoders, or additional losses. Our proposed architecture consists of a differentiable keypoint extractor that feeds the coordinates of the estimated keypoints directly to a soft actor-critic agent. The proposed algorithm yields performance competitive to the state-of-the art on DeepMind Control Suite tasks.",0
"The optimal controls for vision-based control problems can often be determined by the location of objects in the scene. These locations can be represented as keypoints, which are spatial locations in the input image. Previous studies have demonstrated that unsupervised pre-training using encoder-decoder architectures can produce effective keypoint representations for control tasks. However, our study presents a novel approach that enables the learning of efficient keypoint representations in an end-to-end manner, without the need for unsupervised pre-training, decoders, or additional losses. Our proposed architecture includes a differentiable keypoint extractor that directly feeds the estimated keypoint coordinates to a soft actor-critic agent. This algorithm achieves competitive performance on DeepMind Control Suite tasks compared to state-of-the-art methods.",1
"Mutual information maximization provides an appealing formalism for learning representations of data. In the context of reinforcement learning (RL), such representations can accelerate learning by discarding irrelevant and redundant information, while retaining the information necessary for control. Much of the prior work on these methods has addressed the practical difficulties of estimating mutual information from samples of high-dimensional observations, while comparatively less is understood about which mutual information objectives yield representations that are sufficient for RL from a theoretical perspective. In this paper, we formalize the sufficiency of a state representation for learning and representing the optimal policy, and study several popular mutual-information based objectives through this lens. Surprisingly, we find that two of these objectives can yield insufficient representations given mild and common assumptions on the structure of the MDP. We corroborate our theoretical results with empirical experiments on a simulated game environment with visual observations.",0
"Learning representations of data through maximizing mutual information is an attractive approach. In the field of reinforcement learning, these representations can enhance the learning process by disregarding unnecessary and repetitive information, while keeping the critical data required for control. Previous research has focused on overcoming the challenges of estimating mutual information from samples of high-dimensional observations, but little is known about which mutual information objectives generate adequate representations for RL from a theoretical perspective. This paper establishes a formal definition of the sufficiency of a state representation for learning and representing the optimal policy and examines several popular mutual-information based objectives within this framework. Surprisingly, we discover that two of these objectives can produce inadequate representations, even with mild and common assumptions about the MDP's structure. To support our theoretical findings, we conduct empirical experiments on a simulated game environment with visual observations.",1
"High-dimensional observations are a major challenge in the application of model-based reinforcement learning (MBRL) to real-world environments. To handle high-dimensional sensory inputs, existing approaches use representation learning to map high-dimensional observations into a lower-dimensional latent space that is more amenable to dynamics estimation and planning. In this work, we present an information-theoretic approach that employs temporal predictive coding to encode elements in the environment that can be predicted across time. Since this approach focuses on encoding temporally-predictable information, we implicitly prioritize the encoding of task-relevant components over nuisance information within the environment that are provably task-irrelevant. By learning this representation in conjunction with a recurrent state space model, we can then perform planning in latent space. We evaluate our model on a challenging modification of standard DMControl tasks where the background is replaced with natural videos that contain complex but irrelevant information to the planning task. Our experiments show that our model is superior to existing methods in the challenging complex-background setting while remaining competitive with current state-of-the-art models in the standard setting.",0
"Applying model-based reinforcement learning (MBRL) to real-world scenarios is difficult when dealing with high-dimensional observations. To address this issue, current approaches utilize representation learning to transform these observations into a lower-dimensional latent space that is easier for dynamics estimation and planning. Our study proposes an information-theoretic technique that involves temporal predictive coding to encode elements in the environment that can be predicted across time. By prioritizing the encoding of task-relevant components, we implicitly filter out nuisance information that is task-irrelevant. With a recurrent state space model, we can perform planning in latent space. Our model outperforms existing methods in a challenging background setting that includes complex but irrelevant information, while remaining competitive with state-of-the-art models in standard settings. We demonstrate this through our evaluation on a modified DMControl task.",1
"A key problem in the theory of meta-learning is to understand how the task distributions influence transfer risk, the expected error of a meta-learner on a new task drawn from the unknown task distribution. In this paper, focusing on fixed design linear regression with Gaussian noise and a Gaussian task (or parameter) distribution, we give distribution-dependent lower bounds on the transfer risk of any algorithm, while we also show that a novel, weighted version of the so-called biased regularized regression method is able to match these lower bounds up to a fixed constant factor. Notably, the weighting is derived from the covariance of the Gaussian task distribution. Altogether, our results provide a precise characterization of the difficulty of meta-learning in this Gaussian setting. While this problem setting may appear simple, we show that it is rich enough to unify the ""parameter sharing"" and ""representation learning"" streams of meta-learning; in particular, representation learning is obtained as the special case when the covariance matrix of the task distribution is unknown. For this case we propose to adopt the EM method, which is shown to enjoy efficient updates in our case. The paper is completed by an empirical study of EM. In particular, our experimental results show that the EM algorithm can attain the lower bound as the number of tasks grows, while the algorithm is also successful in competing with its alternatives when used in a representation learning context.",0
"Understanding how task distributions affect transfer risk, the expected error of a meta-learner on a new task, is a key challenge in meta-learning theory. This paper focuses on fixed design linear regression with Gaussian noise and a Gaussian task distribution, providing distribution-dependent lower bounds on transfer risk for any algorithm. A weighted version of the biased regularized regression method is introduced, which matches these lower bounds up to a fixed constant factor. The weighting is based on the covariance of the Gaussian task distribution, providing a precise characterization of meta-learning difficulty in this setting. The ""parameter sharing"" and ""representation learning"" streams of meta-learning are unified in this Gaussian setting, with representation learning obtained as a special case when the covariance matrix of the task distribution is unknown. The EM method is proposed for this case, with efficient updates shown in the paper. Empirical results demonstrate that the EM algorithm can reach the lower bound as the number of tasks grows, and is competitive with alternatives in a representation learning context.",1
"We explore the use of a topological manifold, represented as a collection of charts, as the target space of neural network based representation learning tasks. This is achieved by a simple adjustment to the output of an encoder's network architecture plus the addition of a maximal mean discrepancy (MMD) based loss function for regularization. Most algorithms in representation and metric learning are easily adaptable to our framework and we demonstrate its effectiveness by adjusting SimCLR (for representation learning) and standard triplet loss training (for metric learning) to have manifold encoding spaces. Our experiments show that we obtain a substantial performance boost over the baseline for low dimensional encodings. In the case of triplet training, we also find, independent of the manifold setup, that the MMD loss alone (i.e. keeping a flat, euclidean target space but using an MMD loss to regularize it) increases performance over the baseline in the typical, high-dimensional Euclidean target spaces. Code for reproducing experiments is provided at https://github.com/ekorman/neurve .",0
"In this study, we examine the feasibility of utilizing a topological manifold, which can be represented as a series of charts, as the destination for neural network-based representation learning tasks. To accomplish this, we make a simple modification to the encoder's network architecture, coupled with a maximal mean discrepancy (MMD) based loss function for regularization. Most algorithms used in representation and metric learning can be easily adapted to our framework, and we demonstrate its effectiveness by adjusting SimCLR and standard triplet loss training to incorporate manifold encoding spaces. Our experiments reveal that we achieve a significant improvement over the baseline for low dimensional encodings. Additionally, we discover that, in the case of triplet training, the MMD loss alone can enhance performance over the baseline in typical, high-dimensional Euclidean target spaces regardless of the manifold setup. Code for reproducing our experiments can be found at https://github.com/ekorman/neurve.",1
"Driver vigilance estimation is an important task for transportation safety. Wearable and portable brain-computer interface devices provide a powerful means for real-time monitoring of the vigilance level of drivers to help with avoiding distracted or impaired driving. In this paper, we propose a novel multimodal architecture for in-vehicle vigilance estimation from Electroencephalogram and Electrooculogram. To enable the system to focus on the most salient parts of the learned multimodal representations, we propose an architecture composed of a capsule attention mechanism following a deep Long Short-Term Memory (LSTM) network. Our model learns hierarchical dependencies in the data through the LSTM and capsule feature representation layers. To better explore the discriminative ability of the learned representations, we study the effect of the proposed capsule attention mechanism including the number of dynamic routing iterations as well as other parameters. Experiments show the robustness of our method by outperforming other solutions and baseline techniques, setting a new state-of-the-art. We then provide an analysis on different frequency bands and brain regions to evaluate their suitability for driver vigilance estimation. Lastly, an analysis on the role of capsule attention, multimodality, and robustness to noise is performed, highlighting the advantages of our approach.",0
"Ensuring transportation safety requires accurately gauging driver vigilance levels. Portable and wearable brain-computer interface devices offer a strong tool for real-time monitoring of driver vigilance, which can help prevent distracted or impaired driving. In this paper, we introduce a new multimodal architecture for estimating in-vehicle vigilance based on Electroencephalogram and Electrooculogram data. Our proposed architecture leverages a capsule attention mechanism and a deep Long Short-Term Memory (LSTM) network to focus on the most important aspects of the learned multimodal representations. By utilizing the LSTM and capsule feature representation layers, our model learns hierarchical dependencies within the data. We also analyze the effect of the proposed capsule attention mechanism and other parameters on the discriminative ability of the learned representations. Our experiments demonstrate the robustness of our method, surpassing other solutions and baseline techniques to achieve state-of-the-art performance. We further evaluate the suitability of different frequency bands and brain regions for driver vigilance estimation and examine the role of capsule attention, multimodality, and robustness to noise, highlighting the advantages of our approach.",1
"Cross-modal correlation provides an inherent supervision for video unsupervised representation learning. Existing methods focus on distinguishing different video clips by visual and audio representations. We human visual perception could attend to regions where sounds are made, and our auditory perception could also ground their frequencies of sounding objects, which we call bidirectional local correspondence. Such supervision is intuitive but not well explored in the contrastive learning framework. This paper introduces a pretext task, Cross-Modal Attention Consistency (CMAC), for exploring the bidirectional local correspondence property. The CMAC approach aims to align the regional attention generated purely from the visual signal with the target attention generated under the guidance of acoustic signal, and do a similar alignment for frequency grounding on the acoustic attention. Accompanied by a remoulded cross-modal contrastive loss where we consider additional within-modal interactions, the CMAC approach works effectively for enforcing the bidirectional alignment. Extensive experiments on six downstream benchmarks demonstrate that CMAC can improve the state-of-the-art performance on both visual and audio modalities.",0
"The relationship between different senses can be used to supervise unsupervised representation learning in videos. Previous methods have focused on distinguishing video clips based on visual and audio features. Humans have the ability to focus on regions where sounds originate from and can also identify the frequency of sounding objects, which is known as bidirectional local correspondence. However, this type of supervision has not been well explored in the contrastive learning framework. This paper introduces a new approach called Cross-Modal Attention Consistency (CMAC) which aims to align visual and acoustic attention. This is achieved by aligning regional attention generated by the visual signal with the target attention generated by the acoustic signal, and vice versa. Additionally, a revamped cross-modal contrastive loss function is used to enforce within-modal interactions. Results from six downstream benchmarks show that CMAC improves the state-of-the-art performance for both visual and audio modalities.",1
"While the advent of Graph Neural Networks (GNNs) has greatly improved node and graph representation learning in many applications, the neighborhood aggregation scheme exposes additional vulnerabilities to adversaries seeking to extract node-level information about sensitive attributes. In this paper, we study the problem of protecting sensitive attributes by information obfuscation when learning with graph structured data. We propose a framework to locally filter out pre-determined sensitive attributes via adversarial training with the total variation and the Wasserstein distance. Our method creates a strong defense against inference attacks, while only suffering small loss in task performance. Theoretically, we analyze the effectiveness of our framework against a worst-case adversary, and characterize an inherent trade-off between maximizing predictive accuracy and minimizing information leakage. Experiments across multiple datasets from recommender systems, knowledge graphs and quantum chemistry demonstrate that the proposed approach provides a robust defense across various graph structures and tasks, while producing competitive GNN encoders for downstream tasks.",0
"Although Graph Neural Networks (GNNs) have significantly enhanced the learning of node and graph representations in various applications, the neighborhood aggregation method exposes potential vulnerabilities to attackers who try to retrieve node-level information on sensitive attributes. This research investigates the protection of sensitive attributes through information obfuscation when dealing with graph structured data. Our approach involves the use of adversarial training with the total variation and the Wasserstein distance to locally filter out predetermined sensitive attributes. This technique offers strong defense against inference attacks while only experiencing minimal loss in task performance. We analyze the effectiveness of our framework against a worst-case adversary and identify an inherent trade-off between maximizing predictive accuracy and minimizing information leakage. Experiments across several datasets, including recommender systems, knowledge graphs, and quantum chemistry, demonstrate that our proposed method provides reliable defense across diverse graph structures and tasks, while also producing competitive GNN encoders for downstream tasks.",1
"In representation learning, there has been recent interest in developing algorithms to disentangle the ground-truth generative factors behind a dataset, and metrics to quantify how fully this occurs. However, these algorithms and metrics often assume that both representations and ground-truth factors are flat, continuous, and factorized, whereas many real-world generative processes involve rich hierarchical structure, mixtures of discrete and continuous variables with dependence between them, and even varying intrinsic dimensionality. In this work, we develop benchmarks, algorithms, and metrics for learning such hierarchical representations.",0
"Recently, there has been a growing interest in creating algorithms and metrics for representation learning that can separate the underlying generative factors of a dataset, and measure the extent of their separation. However, these methods typically make assumptions that the representations and generative factors are flat, continuous, and separated, which does not always reflect the complexity of real-world generative processes. These processes often involve intricate hierarchical structures, combinations of discrete and continuous variables with interdependencies, and varying intrinsic dimensionality. Our research aims to develop benchmarks, algorithms, and metrics that can learn such hierarchical representations.",1
"Learning node representations that incorporate information from graph structure benefits wide range of tasks on graph. The majority of existing graph neural networks (GNNs) have limited power in capturing position information for a given node. The idea of positioning nodes with selected anchors has been exploited, yet mainly relying on explicit labeling of distance information. Here we propose Graph Inference Representation (GIR), an anchor based GNN model encoding path information related to pre-selected anchors for each node. Abilities to get position-aware embeddings are theoretically and experimentally investigated on GIR and its core variants. Further, the complementarity between GIRs and typical GNNs is demonstrated. We show that GIRs get outperformed results in position-aware scenarios, and performances on typical GNNs could be improved by fusing GIR embeddings.",0
"Incorporating graph structure information into node representations is advantageous for a variety of graph-related tasks. However, most existing graph neural networks (GNNs) have limited ability to capture positional information for a given node. Although some methods using selected anchors to position nodes have been proposed, they mainly rely on explicit labeling of distance information. To address this issue, we introduce Graph Inference Representation (GIR), an anchor-based GNN model that encodes path information related to pre-selected anchors for each node. We investigate GIR's ability to obtain position-aware embeddings both theoretically and experimentally, as well as its core variants. Additionally, we demonstrate the complementarity between GIRs and typical GNNs, showing that GIRs yield superior results in position-aware scenarios and that the performance of typical GNNs can be enhanced by fusing GIR embeddings.",1
"This paper studies zero-shot domain adaptation where each domain is indexed on a multi-dimensional array, and we only have data from a small subset of domains. Our goal is to produce predictors that perform well on \emph{unseen} domains. We propose a model which consists of a domain-invariant latent representation layer and a domain-specific linear prediction layer with a low-rank tensor structure. Theoretically, we present explicit sample complexity bounds to characterize the prediction error on unseen domains in terms of the number of domains with training data and the number of data per domain. To our knowledge, this is the first finite-sample guarantee for zero-shot domain adaptation. In addition, we provide experiments on two-way MNIST and four-way fiber sensing datasets to demonstrate the effectiveness of our proposed model.",0
"The focus of this research is zero-shot domain adaptation, where domains are identified by a multi-dimensional array, and we have limited data available from only a few domains. Our objective is to develop predictors that can perform well on domains that have not been seen before. To achieve this, we propose a model that comprises a domain-invariant latent representation layer and a domain-specific linear prediction layer with a low-rank tensor structure. We also provide explicit sample complexity bounds that determine the prediction error for unseen domains based on the number of domains with training data and the amount of data per domain. This is the first time a finite-sample guarantee has been established for zero-shot domain adaptation. We validate our model using experiments on two-way MNIST and four-way fiber sensing datasets, which demonstrate its effectiveness.",1
"A fundamental challenge in artificial intelligence is learning useful representations of data that yield good performance on a downstream task, without overfitting to spurious input features. Extracting such task-relevant predictive information is particularly difficult for real-world datasets. In this work, we propose Contrastive Input Morphing (CIM), a representation learning framework that learns input-space transformations of the data to mitigate the effect of irrelevant input features on downstream performance. Our method leverages a perceptual similarity metric via a triplet loss to ensure that the transformation preserves task-relevant information.Empirically, we demonstrate the efficacy of our approach on tasks which typically suffer from the presence of spurious correlations: classification with nuisance information, out-of-distribution generalization, and preservation of subgroup accuracies. We additionally show that CIM is complementary to other mutual information-based representation learning techniques, and demonstrate that it improves the performance of variational information bottleneck (VIB) when used together.",0
"One of the main challenges in artificial intelligence involves developing useful data representations that can deliver optimal performance in a downstream task without being influenced by irrelevant input features. This task is even more difficult when working with real-world datasets. To address this issue, we propose Contrastive Input Morphing (CIM), a framework that learns how to transform input-space data to minimize the impact of irrelevant features on downstream performance. Our approach relies on a triplet loss that employs a perceptual similarity metric, ensuring that the transformation preserves task-relevant information. Our empirical results demonstrate that CIM is effective in tasks prone to spurious correlations, such as classification with nuisance information, out-of-distribution generalization, and subgroup accuracy preservation. We also show that CIM complements other mutual information-based representation learning methods and improves the performance of variational information bottleneck (VIB) when used in conjunction.",1
"Recent works have suggested that finite Bayesian neural networks may outperform their infinite cousins because finite networks can flexibly adapt their internal representations. However, our theoretical understanding of how the learned hidden layer representations of finite networks differ from the fixed representations of infinite networks remains incomplete. Perturbative finite-width corrections to the network prior and posterior have been studied, but the asymptotics of learned features have not been fully characterized. Here, we argue that the leading finite-width corrections to the average feature kernels for any Bayesian network with linear readout and quadratic cost have a largely universal form. We illustrate this explicitly for two classes of fully connected networks: deep linear networks and networks with a single nonlinear hidden layer. Our results begin to elucidate which features of data wide Bayesian neural networks learn to represent.",0
"Recent research suggests that finite Bayesian neural networks may perform better than infinite networks due to their ability to adapt their internal representations. However, our understanding of how the learned hidden layer representations of finite networks differ from the fixed representations of infinite networks is still incomplete. Although perturbative finite-width corrections to the network prior and posterior have been analyzed, the asymptotics of learned features have not been fully characterized. In this study, we demonstrate that the leading finite-width corrections to the average feature kernels for any Bayesian network with linear readout and quadratic cost have a mostly universal form. We provide explicit examples for two types of fully connected networks: deep linear networks and networks with a single nonlinear hidden layer. Our findings shed light on the features of data that wide Bayesian neural networks learn to represent.",1
"Metric learning algorithms aim to learn a distance function that brings the semantically similar data items together and keeps dissimilar ones at a distance. The traditional Mahalanobis distance learning is equivalent to find a linear projection. In contrast, Deep Metric Learning (DML) methods are proposed that automatically extract features from data and learn a non-linear transformation from input space to a semantically embedding space. Recently, many DML methods are proposed focused to enhance the discrimination power of the learned metric by providing novel sampling strategies or loss functions. This approach is very helpful when both the training and test examples are coming from the same set of categories. However, it is less effective in many applications of DML such as image retrieval and person-reidentification. Here, the DML should learn general semantic concepts from observed classes and employ them to rank or identify objects from unseen categories. Neglecting the generalization ability of the learned representation and just emphasizing to learn a more discriminative embedding on the observed classes may lead to the overfitting problem. To address this limitation, we propose a framework to enhance the generalization power of existing DML methods in a Zero-Shot Learning (ZSL) setting by general yet discriminative representation learning and employing a class adversarial neural network. To learn a more general representation, we propose to employ feature maps of intermediate layers in a deep neural network and enhance their discrimination power through an attention mechanism. Besides, a class adversarial network is utilized to enforce the deep model to seek class invariant features for the DML task. We evaluate our work on widely used machine vision datasets in a ZSL setting.",0
"The objective of metric learning algorithms is to develop a distance function that brings together data items that are semantically similar and keeps dissimilar ones at a distance. The traditional Mahalanobis distance learning method involves finding a linear projection. On the other hand, Deep Metric Learning (DML) techniques automatically extract features from data and learn a non-linear transformation from input space to a semantically embedding space. Many DML methods have recently been proposed to enhance the discrimination power of the learned metric through novel sampling strategies or loss functions, which is particularly useful when both the training and test examples come from the same set of categories. However, this approach may not be as effective in DML applications such as person-reidentification and image retrieval, where the DML should learn general semantic concepts from observed classes and employ them to rank or identify objects from unseen categories. Focusing solely on learning a more discriminative embedding on the observed classes can lead to the overfitting problem and neglect the generalization ability of the learned representation. To address this limitation, we present a framework that enhances the generalization power of existing DML methods in a Zero-Shot Learning (ZSL) setting by employing a class adversarial neural network for general yet discriminative representation learning. Our approach leverages feature maps of intermediate layers in a deep neural network and enhances their discrimination power through an attention mechanism. Additionally, we utilize a class adversarial network to enforce the deep model to seek class invariant features for the DML task. We evaluate our framework on widely used machine vision datasets in a ZSL setting.",1
"Most of the achievements in artificial intelligence so far were accomplished by supervised learning which requires numerous annotated training data and thus costs innumerable manpower for labeling. Unsupervised learning is one of the effective solutions to overcome such difficulties. In our work, we propose AugNet, a new deep learning training paradigm to learn image features from a collection of unlabeled pictures. We develop a method to construct the similarities between pictures as distance metrics in the embedding space by leveraging the inter-correlation between augmented versions of samples. Our experiments demonstrate that the method is able to represent the image in low dimensional space and performs competitively in downstream tasks such as image classification and image similarity comparison. Specifically, we achieved over 60% and 27% accuracy on the STL10 and CIFAR100 datasets with unsupervised clustering, respectively. Moreover, unlike many deep-learning-based image retrieval algorithms, our approach does not require access to external annotated datasets to train the feature extractor, but still shows comparable or even better feature representation ability and easy-to-use characteristics. In our evaluations, the method outperforms all the state-of-the-art image retrieval algorithms on some out-of-domain image datasets. The code for the model implementation is available at https://github.com/chenmingxiang110/AugNet.",0
"Achieving progress in artificial intelligence has mostly relied on supervised learning, which demands extensive annotated training data and considerable manpower for labeling. To tackle these challenges, unsupervised learning has emerged as an effective solution. Our work introduces AugNet, a novel deep learning training paradigm that learns image features from a collection of unlabeled images. We devise a method for establishing similarities between images by using distance metrics in the embedding space, leveraging correlations between augmented versions of samples. Our experiments demonstrate that this approach can represent images in a low-dimensional space and perform well on downstream tasks such as image classification and similarity comparison, achieving over 60% and 27% accuracy on the STL10 and CIFAR100 datasets, respectively. Unlike many deep-learning-based image retrieval algorithms, our approach does not rely on external annotated datasets for training the feature extractor, yet still yields comparable or superior feature representation and ease of use. Our evaluations show that the method outperforms state-of-the-art image retrieval algorithms on some out-of-domain image datasets. The code for implementing the model is available at https://github.com/chenmingxiang110/AugNet.",1
"We develop a novel method for carrying out model selection for Bayesian autoencoders (BAEs) by means of prior hyper-parameter optimization. Inspired by the common practice of type-II maximum likelihood optimization and its equivalence to Kullback-Leibler divergence minimization, we propose to optimize the distributional sliced-Wasserstein distance (DSWD) between the output of the autoencoder and the empirical data distribution. The advantages of this formulation are that we can estimate the DSWD based on samples and handle high-dimensional problems. We carry out posterior estimation of the BAE parameters via stochastic gradient Hamiltonian Monte Carlo and turn our BAE into a generative model by fitting a flexible Dirichlet mixture model in the latent space. Consequently, we obtain a powerful alternative to variational autoencoders, which are the preferred choice in modern applications of autoencoders for representation learning with uncertainty. We evaluate our approach qualitatively and quantitatively using a vast experimental campaign on a number of unsupervised learning tasks and show that, in small-data regimes where priors matter, our approach provides state-of-the-art results, outperforming multiple competitive baselines.",0
"Our study presents a fresh method for model selection of Bayesian autoencoders (BAEs) by using prior hyper-parameter optimization. We were motivated by the widely adopted practice of type-II maximum likelihood optimization and its relation to minimizing Kullback-Leibler divergence. Therefore, we suggest optimizing the distributional sliced-Wasserstein distance (DSWD) between the empirical data distribution and the autoencoder output. This approach offers the benefit of estimating the DSWD through samples and dealing with high-dimensional problems. To estimate the BAE parameters posteriorly, we employ stochastic gradient Hamiltonian Monte Carlo and use a flexible Dirichlet mixture model to fit our BAE in the latent space, thus transforming it into a generative model. This provides a compelling alternative to variational autoencoders, which are commonly used for representation learning with uncertainty in current applications of autoencoders. We assess the effectiveness of our method both qualitatively and quantitatively through various experiments on unsupervised learning tasks. Our results demonstrate that our approach is superior to multiple competitive baselines, particularly in small-data scenarios where priors play a critical role.",1
"Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.",0
"The importance of pre-trained representations has grown significantly for various NLP and perception tasks. Although representation learning for NLP has progressed to training on raw text without human annotations, visual and vision-language representations still heavily depend on costly curated training datasets that require expert knowledge. For instance, explicit class label datasets such as ImageNet or OpenImages are mostly used for vision applications, while Conceptual Captions, MSCOCO, or CLIP are popular for vision-language, all requiring a complicated data collection and cleaning process. The expensive curation process restricts the dataset size, hindering the scaling of trained models. To combat this, we use a noisy dataset of over one billion image alt-text pairs obtained from the Conceptual Captions dataset without expensive filtering or post-processing steps. Using a simple dual-encoder architecture and a contrastive loss, we align the visual and language representations of the image and text pairs. Despite the noise in the dataset, our corpus's scale compensates, leading to state-of-the-art representations, even with a simple learning scheme. Our visual representation performs well in classification tasks like ImageNet and VTAB, while the aligned visual and language representations enable zero-shot image classification, setting new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks. Even compared to more sophisticated cross-attention models, our representations allow for cross-modality search with complex text and text + image queries.",1
"Unsupervised representation learning has recently received lots of interest due to its powerful generalizability through effectively leveraging large-scale unlabeled data. There are two prevalent approaches for this, contrastive learning and generative pre-training, where the former learns representations from instance-wise discrimination tasks and the latter learns them from estimating the likelihood. These seemingly orthogonal approaches have their own strengths and weaknesses. Contrastive learning tends to extract semantic information and discards details irrelevant for classifying objects, making the representations effective for discriminative tasks while degrading robustness to out-of-distribution data. On the other hand, the generative pre-training directly estimates the data distribution, so the representations tend to be robust but not optimal for discriminative tasks. In this paper, we show that we could achieve the best of both worlds by a hybrid training scheme. Specifically, we demonstrated that a transformer-based encoder-decoder architecture trained with both contrastive and generative losses can learn highly discriminative and robust representations without hurting the generative performance. We extensively validate our approach on various tasks.",0
"Recently, there has been a surge of interest in unsupervised representation learning because of its ability to effectively leverage large-scale unlabeled data, resulting in powerful generalizability. Two common approaches for this are contrastive learning, which learns representations through instance-wise discrimination tasks, and generative pre-training, which learns representations by estimating the likelihood. These approaches have their own strengths and weaknesses. Contrastive learning extracts semantic information and discards irrelevant details, making representations effective for discriminative tasks but less robust to out-of-distribution data. Generative pre-training estimates the data distribution, resulting in robust representations but not optimal for discriminative tasks. In this paper, we propose a hybrid training scheme that combines contrastive and generative losses to achieve the best of both worlds. Our transformer-based encoder-decoder architecture can learn highly discriminative and robust representations without compromising generative performance. We validate our approach on various tasks.",1
"Clustering is one of the fundamental tasks in computer vision and pattern recognition. Recently, deep clustering methods (algorithms based on deep learning) have attracted wide attention with their impressive performance. Most of these algorithms combine deep unsupervised representation learning and standard clustering together. However, the separation of representation learning and clustering will lead to suboptimal solutions because the two-stage strategy prevents representation learning from adapting to subsequent tasks (e.g., clustering according to specific cues). To overcome this issue, efforts have been made in the dynamic adaption of representation and cluster assignment, whereas current state-of-the-art methods suffer from heuristically constructed objectives with representation and cluster assignment alternatively optimized. To further standardize the clustering problem, we audaciously formulate the objective of clustering as finding a precise feature as the cue for cluster assignment. Based on this, we propose a general-purpose deep clustering framework which radically integrates representation learning and clustering into a single pipeline for the first time. The proposed framework exploits the powerful ability of recently developed generative models for learning intrinsic features, and imposes an entropy minimization on the distribution of the cluster assignment by a dedicated variational algorithm. Experimental results show that the performance of the proposed method is superior, or at least comparable to, the state-of-the-art methods on the handwritten digit recognition, fashion recognition, face recognition and object recognition benchmark datasets.",0
"Computer vision and pattern recognition rely heavily on clustering, which is considered a vital task. Deep clustering methods, which leverage the power of deep learning, have gained significant attention due to their impressive performance. However, most of these methods combine deep unsupervised representation learning with standard clustering, which can result in suboptimal solutions. The two-stage strategy hinders representation learning from adapting to subsequent tasks, such as clustering according to specific cues. To address this issue, researchers have attempted to dynamically adapt representation and cluster assignment. However, current state-of-the-art methods suffer from heuristically constructed objectives. To standardize the clustering problem, we propose a novel approach that formulates the objective of clustering as finding a precise feature for cluster assignment. This approach integrates representation learning and clustering into a single pipeline for the first time, leveraging the powerful ability of generative models for learning intrinsic features. Additionally, we minimize entropy in the distribution of cluster assignment using a dedicated variational algorithm. Experimental results demonstrate that our proposed method outperforms or achieves comparable performance to state-of-the-art methods on various benchmark datasets related to handwritten digit recognition, fashion recognition, face recognition, and object recognition.",1
"We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement Learning (RL), which extends conventional value function approximator (VFA) to take as input not only the state (and action) but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies at the same time and brings an appealing characteristic, i.e., \emph{value generalization among policies}. We formally analyze the value generalization under Generalized Policy Iteration (GPI). From theoretical and empirical lens, we show that generalized value estimates offered by PeVFA may have lower initial approximation error to true values of successive policies, which is expected to improve consecutive value approximation during GPI. Based on above clues, we introduce a new form of GPI with PeVFA which leverages the value generalization along policy improvement path. Moreover, we propose a representation learning framework for RL policy, providing several approaches to learn effective policy embeddings from policy network parameters or state-action pairs. In our experiments, we evaluate the efficacy of value generalization offered by PeVFA and policy representation learning in several OpenAI Gym continuous control tasks. For a representative instance of algorithm implementation, Proximal Policy Optimization (PPO) re-implemented under the paradigm of GPI with PeVFA achieves about 40\% performance improvement on its vanilla counterpart in most environments.",0
"In Reinforcement Learning (RL), we examine the Policy-extended Value Function Approximator (PeVFA), an extension of the conventional value function approximator (VFA) that accepts not only the state (and action), but also an explicit policy representation as input. This extension permits PeVFA to maintain the values of multiple policies and provides an attractive feature called ""value generalization among policies."" We conduct a formal analysis of value generalization under Generalized Policy Iteration (GPI) and demonstrate that the generalized value estimates given by PeVFA may have a lower initial approximation error than the true values of successive policies, resulting in improved consecutive value approximation during GPI. We introduce a new form of GPI with PeVFA that takes advantage of the value generalization along the policy improvement path, as well as a representation learning framework for RL policy that offers various methods for learning effective policy embeddings from policy network parameters or state-action pairs. In multiple OpenAI Gym continuous control tasks, we evaluate the effectiveness of value generalization provided by PeVFA and policy representation learning. Using Proximal Policy Optimization (PPO), a representative instance of algorithm implementation re-implemented under the GPI with PeVFA paradigm, we achieve approximately 40% better performance than its vanilla counterpart in most environments.",1
"Neural networks are composed of multiple layers arranged in a hierarchical structure jointly trained with a gradient-based optimization, where the errors are back-propagated from the last layer back to the first one. At each optimization step, neurons at a given layer receive feedback from neurons belonging to higher layers of the hierarchy. In this paper, we propose to complement this traditional 'between-layer' feedback with additional 'within-layer' feedback to encourage diversity of the activations within the same layer. To this end, we measure the pairwise similarity between the outputs of the neurons and use it to model the layer's overall diversity. By penalizing similarities and promoting diversity, we encourage each neuron to learn a distinctive representation and, thus, to enrich the data representation learned within the layer and to increase the total capacity of the model. We theoretically study how the within-layer activation diversity affects the generalization performance of a neural network and prove that increasing the diversity of hidden activations reduces the estimation error. In addition to the theoretical guarantees, we present an empirical study on three datasets confirming that the proposed approach enhances the performance of state-of-the-art neural network models and decreases the generalization gap.",0
"Neural networks consist of hierarchical layers that are trained together using gradient-based optimization, with errors propagated backwards from the last layer to the first. Neurons in each layer receive feedback from those in higher layers. In this study, we suggest adding 'within-layer' feedback to encourage diversity of activations within a layer. We measure pairwise similarity between neuron outputs and use it to model overall layer diversity. By penalizing similarities and promoting diversity, we encourage each neuron to learn a unique representation, enriching data representation and increasing overall model capacity. Our theoretical analysis proves that increasing within-layer activation diversity reduces estimation error. Empirical studies on three datasets confirm that this approach enhances state-of-the-art neural network models and reduces generalization gap.",1
"Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach offering more rigorous fairness guarantees for learned representations. Specifically, we consider a practical setting where we can estimate the probability density for sensitive groups. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. We experimentally demonstrate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning, on a variety of challenging real-world datasets.",0
"Fair representation learning is an alluring method that assures equity in downstream predictors by encoding sensitive information. However, recent research has discovered that powerful antagonistic predictors can still display prejudice by recovering sensitive attributes from these encoded representations. In this study, we introduce Fair Normalizing Flows (FNF), a novel technique that provides more comprehensive fairness assurances for learned representations. Specifically, we examine a practical situation where we can evaluate the likelihood of sensitive groups. Our primary aim is to model the encoder as a normalizing flow that is trained to minimize the statistical variation between latent representations of different groups. The primary advantage of FNF is that it allows us to compute the exact likelihood and provide guarantees on the maximum biasness of any potential adversarial downstream predictor. Through our experiments on challenging real-world datasets, we demonstrate that FNF is effective in enforcing various group fairness concepts, as well as other appealing features such as interpretability and transfer learning.",1
"Representational learning forms the backbone of most deep learning applications, and the value of a learned representation is intimately tied to its information content regarding different factors of variation. Finding good representations depends on the nature of supervision and the learning algorithm. We propose a novel algorithm that relies on a weak form of supervision where the data is partitioned into sets according to certain inactive factors of variation. Our key insight is that by seeking approximate correspondence between elements of different sets, we learn strong representations that exclude the inactive factors of variation and isolate the active factors which vary within all sets. We demonstrate that the method can work in a semi-supervised scenario, and that a portion of the unsupervised data can belong to a different domain entirely. Further control over the content of the learned representations is possible by folding in data augmentation to suppress nuisance factors. We outperform competing baselines on the challenging problem of synthetic-to-real object pose transfer.",0
"Most deep learning applications rely on representational learning, which is closely linked to the information content of a learned representation with regards to various factors of variation. The effectiveness of finding good representations depends on the learning algorithm and the nature of supervision. Our proposed algorithm utilizes a weak form of supervision wherein data is divided into sets based on certain inactive factors of variation. By seeking correspondence between elements of different sets, we can isolate the active factors and exclude the inactive ones, resulting in strong representations. This approach can work in a semi-supervised scenario, even when some of the unsupervised data belongs to a different domain. Data augmentation can be used to further control the content of the learned representations. We have demonstrated the effectiveness of this approach in outperforming competing baselines in the challenging task of synthetic-to-real object pose transfer.",1
"We show that viewing graphs as sets of node features and incorporating structural and positional information into a transformer architecture is able to outperform representations learned with classical graph neural networks (GNNs). Our model, GraphiT, encodes such information by (i) leveraging relative positional encoding strategies in self-attention scores based on positive definite kernels on graphs, and (ii) enumerating and encoding local sub-structures such as paths of short length. We thoroughly evaluate these two ideas on many classification and regression tasks, demonstrating the effectiveness of each of them independently, as well as their combination. In addition to performing well on standard benchmarks, our model also admits natural visualization mechanisms for interpreting graph motifs explaining the predictions, making it a potentially strong candidate for scientific applications where interpretation is important. Code available at https://github.com/inria-thoth/GraphiT.",0
"Our study demonstrates that incorporating structural and positional information into a transformer architecture, while viewing graphs as sets of node features, outperforms representations learned using classical graph neural networks (GNNs). Our proposed model, GraphiT, integrates this information by utilizing relative positional encoding strategies in self-attention scores, based on positive definite kernels on graphs, and by encoding local sub-structures, such as short paths. We comprehensively evaluate these strategies on various classification and regression tasks, showcasing their individual effectiveness as well as their combination. Apart from excelling on standard benchmarks, our model also facilitates natural visualization mechanisms for interpreting graph motifs, thereby rendering it suitable for scientific applications where interpretability is paramount. The code for our model is available on https://github.com/inria-thoth/GraphiT.",1
"The COVID-19 pandemic has drastically changed accepted norms globally. Within the past year, masks have been used as a public health response to limit the spread of the virus. This sudden change has rendered many face recognition based access control, authentication and surveillance systems ineffective. Official documents such as passports, driving license and national identity cards are enrolled with fully uncovered face images. However, in the current global situation, face matching systems should be able to match these reference images with masked face images. As an example, in an airport or security checkpoint it is safer to match the unmasked image of the identifying document to the masked person rather than asking them to remove the mask. We find that current facial recognition techniques are not robust to this form of occlusion.   To address this unique requirement presented due to the current circumstance, we propose a set of re-purposed datasets and a benchmark for researchers to use. We also propose a contrastive visual representation learning based pre-training workflow which is specialized to masked vs unmasked face matching. We ensure that our method learns robust features to differentiate people across varying data collection scenarios. We achieve this by training over many different datasets and validating our result by testing on various holdout datasets. The specialized weights trained by our method outperform standard face recognition features for masked to unmasked face matching. We believe the provided synthetic mask generating code, our novel training approach and the trained weights from the masked face models will help in adopting existing face recognition systems to operate in the current global environment. We open-source all contributions for broader use by the research community.",0
"The COVID-19 pandemic has had a significant impact on global norms, leading to the widespread use of masks as a means of limiting the virus's spread. However, this has caused issues with face recognition-based systems that rely on uncovered faces for access control, authentication, and surveillance. Official documents such as passports and driving licenses include fully exposed face images, but face matching systems must now be able to match these images with masked faces. To address this problem, we propose a new benchmark and re-purposed datasets for researchers to use. We also suggest a specialized pre-training workflow based on contrastive visual representation learning, which focuses on masked vs. unmasked face matching. Our method trains on multiple datasets and validates results by testing on various holdout datasets, providing robust features to differentiate people in different data collection scenarios. Our approach outperforms standard face recognition features for masked to unmasked face matching. We believe that our synthetic mask generating code, novel training approach, and trained weights from masked face models will help existing face recognition systems to operate in the current global environment. All contributions are open-sourced for broader use by the research community.",1
"Recent advances in representation learning have demonstrated an ability to represent information from different modalities such as video, text, and audio in a single high-level embedding vector. In this work we present a self-supervised learning framework that is able to learn a representation that captures finer levels of granularity across different modalities such as concepts or events represented by visual objects or spoken words. Our framework relies on a discretized embedding space created via vector quantization that is shared across different modalities. Beyond the shared embedding space, we propose a Cross-Modal Code Matching objective that forces the representations from different views (modalities) to have a similar distribution over the discrete embedding space such that cross-modal objects/actions localization can be performed without direct supervision. In our experiments we show that the proposed discretized multi-modal fine-grained representation (e.g., pixel/word/frame) can complement high-level summary representations (e.g., video/sentence/waveform) for improved performance on cross-modal retrieval tasks. We also observe that the discretized representation uses individual clusters to represent the same semantic concept across modalities.",0
"Advancements in representation learning have exhibited the ability to create a high-level embedding vector that encompasses information from various modalities, including text, audio, and video. This study introduces a self-supervised learning structure that can capture finer levels of granularity across different modalities, such as events and concepts represented by spoken words or visual objects. The framework relies on a discretized embedding space, formed through vector quantization, that is shared across modalities. In addition to the shared embedding space, a Cross-Modal Code Matching objective has been proposed, which ensures that the representations from different modalities possess a comparable distribution over the discrete embedding space, allowing for cross-modal objects/actions localization without direct supervision. The study demonstrates that the proposed discretized multi-modal fine-grained representation can enhance high-level summary representations for better performance on cross-modal retrieval tasks. Furthermore, the discretized representation employs individual clusters to represent the same semantic concept across modalities.",1
"Node representation learning has demonstrated its effectiveness for various applications on graphs. Particularly, recent developments in contrastive learning have led to promising results in unsupervised node representation learning for a number of tasks. Despite the success of graph contrastive learning and consequent growing interest, fairness is largely under-explored in the field. To this end, this study addresses fairness issues in graph contrastive learning with fairness-aware graph augmentation designs, through adaptive feature masking and edge deletion. In the study, different fairness notions on graphs are introduced, which serve as guidelines for the proposed graph augmentations. Furthermore, theoretical analysis is provided to quantitatively prove that the proposed feature masking approach can reduce intrinsic bias. Experimental results on real social networks are presented to demonstrate that the proposed augmentations can enhance fairness in terms of statistical parity and equal opportunity, while providing comparable classification accuracy to state-of-the-art contrastive methods for node classification.",0
"Various applications on graphs have proven the effectiveness of node representation learning. Recent developments in contrastive learning have led to promising results in unsupervised node representation learning for several tasks. Despite the growing interest in graph contrastive learning, fairness remains largely unexplored in the field. This study addresses fairness issues in graph contrastive learning through adaptive feature masking and edge deletion, using fairness-aware graph augmentation designs. The study introduces different fairness notions on graphs to guide the proposed graph augmentations and provides theoretical analysis to quantitatively prove that the proposed feature masking approach can reduce intrinsic bias. Experimental results on real social networks demonstrate that the proposed augmentations can enhance fairness in terms of statistical parity and equal opportunity, while maintaining comparable classification accuracy to state-of-the-art contrastive methods for node classification.",1
"Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple ""views"" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more ""model zoos"" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is released on our project page: https://ali-design.github.io/GenRep/",0
"The ability of generative models to produce images that closely resemble the training data raises a question about the necessity of datasets. To explore this, we examine the potential of learning general visual representations from a black-box generative model instead of directly from data. Our approach involves training representations from the generator's output without access to its training data. We compare various representation learning methods that use the generator's latent space to generate multiple ""views"" of the same semantic content. We demonstrate that contrastive methods can use this multiview data to distinguish between positive and negative pairs based on their proximity in latent space. Our results reveal that the representations obtained from this approach are comparable to those learned directly from real data, but require careful sampling and training methods for optimal performance. We anticipate that generative models will continue to grow in popularity, acting as organized and compressed copies of datasets, while datasets may become more challenging to handle. This paper presents techniques to address visual representation learning in the face of such a future. The code for this project is available on our project page: https://ali-design.github.io/GenRep/",1
"Hierarchy and compositionality are common latent properties in many natural and scientific datasets. Determining when a deep network's hidden activations represent hierarchy and compositionality is important both for understanding deep representation learning and for applying deep networks in domains where interpretability is crucial. However, current benchmark machine learning datasets either have little hierarchical or compositional structure, or the structure is not known. This gap impedes precise analysis of a network's representations and thus hinders development of new methods that can learn such properties. To address this gap, we developed a new benchmark dataset with known hierarchical and compositional structure. The Hangul Fonts Dataset (HFD) is comprised of 35 fonts from the Korean writing system (Hangul), each with 11,172 blocks (syllables) composed from the product of initial consonant, medial vowel, and final consonant glyphs. All blocks can be grouped into a few geometric types which induces a hierarchy across blocks. In addition, each block is composed of individual glyphs with rotations, translations, scalings, and naturalistic style variation across fonts. We find that both shallow and deep unsupervised methods only show modest evidence of hierarchy and compositionality in their representations of the HFD compared to supervised deep networks. Supervised deep network representations contain structure related to the geometrical hierarchy of the characters, but the compositional structure of the data is not evident. Thus, HFD enables the identification of shortcomings in existing methods, a critical first step toward developing new machine learning algorithms to extract hierarchical and compositional structure in the context of naturalistic variability.",0
"Properties of hierarchy and compositionality are present in various natural and scientific datasets, but identifying when a deep network's hidden activations represent these properties is crucial for both understanding deep representation learning and using deep networks in domains where interpretability is necessary. Unfortunately, current benchmark machine learning datasets lack hierarchical or compositional structure or have unknown structure, which makes it challenging to analyze a network's representations accurately and develop new methods that can learn such properties. To address this issue, we created the Hangul Fonts Dataset (HFD), which contains 35 fonts from the Korean writing system with known hierarchical and compositional structure. The dataset's blocks can be grouped into geometric types, creating a hierarchy, and each block is composed of individual glyphs with various transformations and naturalistic style variation across fonts. Our analysis reveals that supervised deep network representations contain structure related to the geometrical hierarchy of the characters, but the compositional structure of the data is not apparent. Shallow and deep unsupervised methods show limited evidence of hierarchy and compositionality in their representations of the HFD compared to supervised deep networks. HFD provides a critical first step in identifying shortcomings in existing methods and developing new machine learning algorithms for extracting hierarchical and compositional structure within naturalistic variability.",1
"Independent component analysis provides a principled framework for unsupervised representation learning, with solid theory on the identifiability of the latent code that generated the data, given only observations of mixtures thereof. Unfortunately, when the mixing is nonlinear, the model is provably nonidentifiable, since statistical independence alone does not sufficiently constrain the problem. Identifiability can be recovered in settings where additional, typically observed variables are included in the generative process. We investigate an alternative path and consider instead including assumptions reflecting the principle of independent causal mechanisms exploited in the field of causality. Specifically, our approach is motivated by thinking of each source as independently influencing the mixing process. This gives rise to a framework which we term independent mechanism analysis. We provide theoretical and empirical evidence that our approach circumvents a number of nonidentifiability issues arising in nonlinear blind source separation.",0
"The use of independent component analysis is a reliable method for unsupervised representation learning. It is based on a sound theory that can identify the hidden code responsible for generating the data, even with only observations of mixtures. However, the model becomes nonidentifiable in cases where the mixing is nonlinear. This is because statistical independence is not sufficient to solve the problem. To address this issue, additional variables that are typically observed should be incorporated into the generative process. Alternatively, our approach is to include assumptions that reflect the principle of independent causal mechanisms used in causality. Our method considers each source as independently influencing the mixing process, resulting in a framework called independent mechanism analysis. We have found that our approach is effective in overcoming nonidentifiability issues that arise in nonlinear blind source separation, and we have provided theoretical and empirical evidence to support this claim.",1
"Data-efficiency and generalization are key challenges in deep learning and deep reinforcement learning as many models are trained on large-scale, domain-specific, and expensive-to-label datasets. Self-supervised models trained on large-scale uncurated datasets have shown successful transfer to diverse settings. We investigate using pretrained image representations and spatio-temporal attention for state representation learning in Atari. We also explore fine-tuning pretrained representations with self-supervised techniques, i.e., contrastive predictive coding, spatio-temporal contrastive learning, and augmentations. Our results show that pretrained representations are at par with state-of-the-art self-supervised methods trained on domain-specific data. Pretrained representations, thus, yield data and compute-efficient state representations. https://github.com/PAL-ML/PEARL_v1",0
"Deep learning and deep reinforcement learning pose challenges related to data-efficiency and generalization due to the use of large-scale, domain-specific, and costly-to-label datasets. However, self-supervised models that are trained on uncurated datasets have proven to be effective in transferring to various settings. To address this, we examine the use of pretrained image representations and spatio-temporal attention to achieve state representation learning in Atari. We also explore the application of self-supervised techniques such as contrastive predictive coding, spatio-temporal contrastive learning, and augmentations to fine-tune pretrained representations. Our findings reveal that pretrained representations are comparable to state-of-the-art self-supervised methods trained on domain-specific data, resulting in data and compute-efficient state representations.",1
"In this paper, we focus on the fairness issues regarding unsupervised outlier detection. Traditional algorithms, without a specific design for algorithmic fairness, could implicitly encode and propagate statistical bias in data and raise societal concerns. To correct such unfairness and deliver a fair set of potential outlier candidates, we propose Deep Clustering based Fair Outlier Detection (DCFOD) that learns a good representation for utility maximization while enforcing the learnable representation to be subgroup-invariant on the sensitive attribute. Considering the coupled and reciprocal nature between clustering and outlier detection, we leverage deep clustering to discover the intrinsic cluster structure and out-of-structure instances. Meanwhile, an adversarial training erases the sensitive pattern for instances for fairness adaptation. Technically, we propose an instance-level weighted representation learning strategy to enhance the joint deep clustering and outlier detection, where the dynamic weight module re-emphasizes contributions of likely-inliers while mitigating the negative impact from outliers. Demonstrated by experiments on eight datasets comparing to 17 outlier detection algorithms, our DCFOD method consistently achieves superior performance on both the outlier detection validity and two types of fairness notions in outlier detection.",0
"The aim of this article is to address the issue of fairness in unsupervised outlier detection. Conventional algorithms that do not have an algorithmic fairness design can unintentionally encode and disseminate statistical biases in data, causing societal concerns. To tackle this problem and generate a fair group of potential outlier candidates, we present the Deep Clustering based Fair Outlier Detection (DCFOD) method. It learns a representation that maximizes utility and ensures that the learned representation is subgroup-invariant when it comes to sensitive attributes. As clustering and outlier detection are interdependent, we utilize deep clustering to identify the inherent cluster structure and out-of-structure instances. Additionally, by employing adversarial training, we remove sensitive patterns from instances to achieve fairness adaptation. We also suggest an instance-level weighted representation learning technique to augment the joint deep clustering and outlier detection process. The dynamic weight module emphasizes the contributions of likely-inliers while mitigating the negative impact of outliers. Our DCFOD method outperforms 17 outlier detection algorithms on eight datasets, as demonstrated by experiments, in terms of outlier detection validity and two types of fairness notions in outlier detection.",1
"We describe a new approach to estimating relative risks in time-to-event prediction problems with censored data in a fully parametric manner. Our approach does not require making strong assumptions of constant proportional hazard of the underlying survival distribution, as required by the Cox-proportional hazard model. By jointly learning deep nonlinear representations of the input covariates, we demonstrate the benefits of our approach when used to estimate survival risks through extensive experimentation on multiple real world datasets with different levels of censoring. We further demonstrate advantages of our model in the competing risks scenario. To the best of our knowledge, this is the first work involving fully parametric estimation of survival times with competing risks in the presence of censoring.",0
"Our new method of estimating relative risks in time-to-event prediction problems with censored data is fully parametric and does not rely on the constant proportional hazard assumption of the Cox-proportional hazard model. Through joint learning of deep nonlinear representations of input covariates, we have conducted extensive experiments on various real-world datasets with varying levels of censoring to demonstrate the benefits of our approach in estimating survival risks. Our model also performs well in competing risks scenarios. This is the first work to use fully parametric estimation of survival times with competing risks while accounting for censoring.",1
"A central challenge in training classification models in the real-world federated system is learning with non-IID data. To cope with this, most of the existing works involve enforcing regularization in local optimization or improving the model aggregation scheme at the server. Other works also share public datasets or synthesized samples to supplement the training of under-represented classes or introduce a certain level of personalization. Though effective, they lack a deep understanding of how the data heterogeneity affects each layer of a deep classification model. In this paper, we bridge this gap by performing an experimental analysis of the representations learned by different layers. Our observations are surprising: (1) there exists a greater bias in the classifier than other layers, and (2) the classification performance can be significantly improved by post-calibrating the classifier after federated training. Motivated by the above findings, we propose a novel and simple algorithm called Classifier Calibration with Virtual Representations (CCVR), which adjusts the classifier using virtual representations sampled from an approximated gaussian mixture model. Experimental results demonstrate that CCVR achieves state-of-the-art performance on popular federated learning benchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple yet effective method can shed some light on the future research of federated learning with non-IID data.",0
"The primary difficulty in training classification models in real-world federated systems is dealing with non-IID data. Existing methods attempt to address this challenge by incorporating regularization in local optimization or improving the model aggregation scheme at the server. Some approaches involve sharing public datasets or synthesized samples to supplement the training of under-represented classes or introduce a certain level of personalization. However, these methods lack a comprehensive understanding of how data heterogeneity affects each layer of a deep classification model. To bridge this gap, we conducted an experimental analysis of the representations learned by various layers and made several surprising observations. Firstly, the classifier is more biased than other layers, and secondly, the classification performance can be greatly enhanced by post-calibrating the classifier after federated training. Based on our findings, we propose a straightforward yet effective algorithm called Classifier Calibration with Virtual Representations (CCVR), which adjusts the classifier using virtual representations sampled from an approximated gaussian mixture model. Our experimental results demonstrate that CCVR outperforms existing methods on popular federated learning benchmarks such as CIFAR-10, CIFAR-100, and CINIC-10. We hope that our approach will provide valuable insights for future research on federated learning with non-IID data.",1
"Graph convolutional neural networks (GCN) have been the model of choice for graph representation learning, which is mainly due to the effective design of graph convolution that computes the representation of a node by aggregating those of its neighbors. However, existing GCN variants commonly use 1-D graph convolution that solely operates on the object link graph without exploring informative relational information among object attributes. This significantly limits their modeling capability and may lead to inferior performance on noisy and sparse real-world networks. In this paper, we explore 2-D graph convolution to jointly model object links and attribute relations for graph representation learning. Specifically, we propose a computationally efficient dimensionwise separable 2-D graph convolution (DSGC) for filtering node features. Theoretically, we show that DSGC can reduce intra-class variance of node features on both the object dimension and the attribute dimension to learn more effective representations. Empirically, we demonstrate that by modeling attribute relations, DSGC achieves significant performance gain over state-of-the-art methods for node classification and clustering on a variety of real-world networks. The source code for reproducing the experimental results is available at https://github.com/liqimai/DSGC.",0
"The design of graph convolution in graph convolutional neural networks (GCN) has made them the preferred choice for graph representation learning. However, the current GCN variants only utilize 1-D graph convolution, which limits their modeling capability and leads to inferior performance on noisy and sparse real-world networks. In this study, we propose a novel approach, using 2-D graph convolution to model object links and attribute relations for graph representation learning. To achieve this, we introduce a computationally efficient dimensionwise separable 2-D graph convolution (DSGC) for filtering node features. The DSGC can reduce intra-class variance of node features on both the object dimension and the attribute dimension, which results in more effective representations. Our empirical results demonstrate that by modeling attribute relations, DSGC outperforms state-of-the-art methods in node classification and clustering on different real-world networks. The source code for reproducing our experimental results is available at https://github.com/liqimai/DSGC.",1
"Learning faithful graph representations as sets of vertex embeddings has become a fundamental intermediary step in a wide range of machine learning applications. We propose the systematic use of symmetric spaces in representation learning, a class encompassing many of the previously used embedding targets. This enables us to introduce a new method, the use of Finsler metrics integrated in a Riemannian optimization scheme, that better adapts to dissimilar structures in the graph. We develop a tool to analyze the embeddings and infer structural properties of the data sets. For implementation, we choose Siegel spaces, a versatile family of symmetric spaces. Our approach outperforms competitive baselines for graph reconstruction tasks on various synthetic and real-world datasets. We further demonstrate its applicability on two downstream tasks, recommender systems and node classification.",0
"The acquisition of accurate graph representations as collections of vertex embeddings is a crucial intermediate process for a diverse range of machine learning applications. To improve representation learning, we suggest utilizing symmetric spaces, a category that encompasses many previously utilized embedding targets. This allows us to introduce a novel technique, the integration of Finsler metrics with a Riemannian optimization system, which better adapts to non-identical structures within the graph. We have established a tool that examines the embeddings and infers the data set's structural attributes. For implementation purposes, we have chosen Siegel spaces, an adaptable group of symmetric spaces. Our approach surpasses comparable baselines for graph reconstruction tasks, both synthetic and real-world datasets. We also demonstrate its effectiveness in two downstream tasks: recommender systems and node classification.",1
"Recent progress in self-supervised learning has resulted in models that are capable of extracting rich representations from image collections without requiring any explicit label supervision. However, to date the vast majority of these approaches have restricted themselves to training on standard benchmark datasets such as ImageNet. We argue that fine-grained visual categorization problems, such as plant and animal species classification, provide an informative testbed for self-supervised learning. In order to facilitate progress in this area we present two new natural world visual classification datasets, iNat2021 and NeWT. The former consists of 2.7M images from 10k different species uploaded by users of the citizen science application iNaturalist. We designed the latter, NeWT, in collaboration with domain experts with the aim of benchmarking the performance of representation learning algorithms on a suite of challenging natural world binary classification tasks that go beyond standard species classification. These two new datasets allow us to explore questions related to large-scale representation and transfer learning in the context of fine-grained categories. We provide a comprehensive analysis of feature extractors trained with and without supervision on ImageNet and iNat2021, shedding light on the strengths and weaknesses of different learned features across a diverse set of tasks. We find that features produced by standard supervised methods still outperform those produced by self-supervised approaches such as SimCLR. However, improved self-supervised learning methods are constantly being released and the iNat2021 and NeWT datasets are a valuable resource for tracking their progress.",0
"Recent advancements in self-supervised learning have led to the development of models that can extract rich representations from image collections without needing explicit label supervision. However, most of these models have only been trained on standard benchmark datasets like ImageNet. We posit that fine-grained visual categorization problems, including plant and animal species classification, are a useful testbed for self-supervised learning. To further research in this area, we have introduced two new natural world visual classification datasets, namely iNat2021 and NeWT. iNat2021 comprises 2.7M images from 10k species uploaded by users of the iNaturalist citizen science app, while NeWT was created in consultation with domain experts to benchmark representation learning algorithms on a set of challenging natural world binary classification tasks that go beyond standard species classification. These datasets enable us to investigate questions related to large-scale representation and transfer learning in the context of fine-grained categories. We conducted a comprehensive analysis of feature extractors trained with and without supervision on ImageNet and iNat2021, which revealed the strengths and weaknesses of different learned features across a diverse range of tasks. Although features produced by standard supervised methods outperformed those produced by self-supervised methods like SimCLR, new and improved self-supervised learning methods are constantly being introduced, and iNat2021 and NeWT are valuable resources for monitoring their progress.",1
"Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.",0
"The success of self-supervised representation learning has been notable across various domains. Typically, this involves the use of hand-crafted transformations that preserve the data's meaning. Our goal is to examine this method from a theoretical viewpoint. We accomplish this by formulating the augmentation process as a latent variable model, with a partition of the latent representation into content and style components. Our approach differs from previous studies on disentanglement and independent component analysis by allowing for nontrivial statistical and causal dependencies in the latent space. By studying pairs of observations, we establish sufficient conditions for identifying the invariant content partition, up to an invertible mapping in both generative and discriminative settings. Our numerical simulations with dependent latent variables support our theory. Finally, we introduce Causal3DIdent, a dataset of high-dimensional and visually complex images with rich causal dependencies, which we use to evaluate the impact of data augmentations in practice.",1
"We study neural-linear bandits for solving problems where {\em both} exploration and representation learning play an important role. Neural-linear bandits harnesses the representation power of Deep Neural Networks (DNNs) and combines it with efficient exploration mechanisms by leveraging uncertainty estimation of the model, designed for linear contextual bandits on top of the last hidden layer. In order to mitigate the problem of representation change during the process, new uncertainty estimations are computed using stored data from an unlimited buffer. Nevertheless, when the amount of stored data is limited, a phenomenon called catastrophic forgetting emerges. To alleviate this, we propose a likelihood matching algorithm that is resilient to catastrophic forgetting and is completely online. We applied our algorithm, Limited Memory Neural-Linear with Likelihood Matching (NeuralLinear-LiM2) on a variety of datasets and observed that our algorithm achieves comparable performance to the unlimited memory approach while exhibits resilience to catastrophic forgetting.",0
"Our focus is on studying neural-linear bandits, which are effective for problems that require both exploration and representation learning. By incorporating the power of Deep Neural Networks (DNNs), neural-linear bandits provide accurate representation alongside efficient exploration mechanisms. These mechanisms depend on the uncertainty estimation of the model, which is designed for linear contextual bandits on top of the last hidden layer. However, to deal with representation change during the process, new uncertainty estimations are generated using stored data from a buffer. Despite this, the problem of catastrophic forgetting may arise when the amount of stored data is limited. To address this issue, we propose a likelihood matching algorithm that is both online and resilient to catastrophic forgetting. We tested our Limited Memory Neural-Linear with Likelihood Matching (NeuralLinear-LiM2) algorithm on various datasets and found that it performs comparably to the unlimited memory approach while maintaining resilience to catastrophic forgetting.",1
"Recommender systems are often designed based on a collaborative filtering approach, where user preferences are predicted by modelling interactions between users and items. Many common approaches to solve the collaborative filtering task are based on learning representations of users and items, including simple matrix factorization, Gaussian process latent variable models, and neural-network based embeddings. While matrix factorization approaches fail to model nonlinear relations, neural networks can potentially capture such complex relations with unprecedented predictive power and are highly scalable. However, neither of them is able to model predictive uncertainties. In contrast, Gaussian Process based models can generate a predictive distribution, but cannot scale to large amounts of data. In this manuscript, we propose a novel approach combining the representation learning paradigm of collaborative filtering with multi-output Gaussian processes in a joint framework to generate uncertainty-aware recommendations. We introduce an efficient strategy for model training and inference, resulting in a model that scales to very large and sparse datasets and achieves competitive performance in terms of classical metrics quantifying the reconstruction error. In addition to accurately predicting user preferences, our model also provides meaningful uncertainty estimates about that prediction.",0
"Collaborative filtering is a popular method for designing recommender systems. It involves predicting user preferences by analyzing their interactions with items. Many approaches to this task involve learning representations of users and items, such as matrix factorization, Gaussian process latent variable models, and neural network embeddings. However, these methods have limitations, such as being unable to model nonlinear relations or provide predictive uncertainties. In this paper, we propose a novel approach that combines collaborative filtering with multi-output Gaussian processes in a joint framework to generate uncertainty-aware recommendations. Our model is efficient, scalable, and achieves competitive performance. It not only accurately predicts user preferences but also provides meaningful uncertainty estimates.",1
"We study the role of information complexity in privacy leakage about an attribute of an adversary's interest, which is not known a priori to the system designer. Considering the supervised representation learning setup and using neural networks to parameterize the variational bounds of information quantities, we study the impact of the following factors on the amount of information leakage: information complexity regularizer weight, latent space dimension, the cardinalities of the known utility and unknown sensitive attribute sets, the correlation between utility and sensitive attributes, and a potential bias in a sensitive attribute of adversary's interest. We conduct extensive experiments on Colored-MNIST and CelebA datasets to evaluate the effect of information complexity on the amount of intrinsic leakage.",0
"Our research delves into the relationship between information complexity and the disclosure of private information regarding an adversary's unknown interest. Specifically, we explore this concept in a supervised representation learning environment by utilizing neural networks to parameterize variational bounds of information quantities. Our investigation examines a variety of factors that impact the level of information leakage, including the weight of the information complexity regularizer, the dimension of the latent space, the sizes of both the known utility and unknown sensitive attribute sets, the correlation between these attributes, and any biases in the adversary's sensitive attribute. To evaluate the impact of information complexity, we conducted extensive experiments on the Colored-MNIST and CelebA datasets.",1
"We tackle the problem of unsupervised synthetic-to-realistic domain adaptation for single image depth estimation. An essential building block of single image depth estimation is an encoder-decoder task network that takes RGB images as input and produces depth maps as output. In this paper, we propose a novel training strategy to force the task network to learn domain invariant representations in a self-supervised manner. Specifically, we extend self-supervised learning from traditional representation learning, which works on images from a single domain, to domain invariant representation learning, which works on images from two different domains by utilizing an image-to-image translation network. Firstly, we use our bidirectional image-to-image translation network to transfer domain-specific styles between synthetic and real domains. This style transfer operation allows us to obtain similar images from the different domains. Secondly, we jointly train our task network and Siamese network with the same images from the different domains to obtain domain invariance for the task network. Finally, we fine-tune the task network using labeled synthetic and unlabeled real-world data. Our training strategy yields improved generalization capability in the real-world domain. We carry out an extensive evaluation on two popular datasets for depth estimation, KITTI and Make3D. The results demonstrate that our proposed method outperforms the state-of-the-art both qualitatively and quantitatively. The source code and model weights will be made available.",0
"This paper addresses the challenge of adapting synthetic images to realistic ones in single image depth estimation without supervision. To achieve this, we focus on the encoder-decoder task network, which generates depth maps from RGB images. Our proposed approach involves a new training method that encourages the task network to learn domain-invariant representations in a self-supervised manner. We achieve this by extending self-supervised learning beyond traditional representation learning, which deals with images from a single domain, to domain invariant representation learning. This is done by utilizing an image-to-image translation network that transfers styles between synthetic and real domains. By jointly training our task network and Siamese network with images from different domains, we obtain domain invariance for the task network. We then fine-tune the network using labeled synthetic and unlabeled real-world data. Our approach is evaluated on two popular datasets, KITTI and Make3D, and outperforms existing methods in both qualitative and quantitative measures. The source code and model weights will be shared.",1
"Learning good feature representations is important for deep reinforcement learning (RL). However, with limited experience, RL often suffers from data inefficiency for training. For un-experienced or less-experienced trajectories (i.e., state-action sequences), the lack of data limits the use of them for better feature learning. In this work, we propose a novel method, dubbed PlayVirtual, which augments cycle-consistent virtual trajectories to enhance the data efficiency for RL feature representation learning. Specifically, PlayVirtual predicts future states based on the current state and action by a dynamics model and then predicts the previous states by a backward dynamics model, which forms a trajectory cycle. Based on this, we augment the actions to generate a large amount of virtual state-action trajectories. Being free of groudtruth state supervision, we enforce a trajectory to meet the cycle consistency constraint, which can significantly enhance the data efficiency. We validate the effectiveness of our designs on the Atari and DeepMind Control Suite benchmarks. Our method outperforms the current state-of-the-art methods by a large margin on both benchmarks.",0
"Developing effective feature representations is crucial in deep reinforcement learning (RL). However, due to limited experience, RL often faces challenges with data inefficiency during training. Inexperienced or less experienced trajectories (i.e., state-action sequences) have limited use for improving feature learning due to insufficient data. To address this issue, we propose a new approach called PlayVirtual, which boosts data efficiency for RL feature representation learning by augmenting cycle-consistent virtual trajectories. PlayVirtual utilizes a forward dynamics model to predict future states based on the current state and action, followed by a backward dynamics model to predict previous states, thus forming a trajectory cycle. This generates a large volume of virtual state-action trajectories by augmenting actions. Since we don't have ground truth state supervision, we enforce cycle consistency constraints to enhance data efficiency. We evaluated our design on Atari and DeepMind Control Suite benchmarks, where it significantly outperformed current state-of-the-art methods.",1
"This paper studies unsupervised/self-supervised whole-graph representation learning, which is critical in many tasks such as molecule properties prediction in drug and material discovery. Existing methods mainly focus on preserving the local similarity structure between different graph instances but fail to discover the global semantic structure of the entire data set. In this paper, we propose a unified framework called Local-instance and Global-semantic Learning (GraphLoG) for self-supervised whole-graph representation learning. Specifically, besides preserving the local similarities, GraphLoG introduces the hierarchical prototypes to capture the global semantic clusters. An efficient online expectation-maximization (EM) algorithm is further developed for learning the model. We evaluate GraphLoG by pre-training it on massive unlabeled graphs followed by fine-tuning on downstream tasks. Extensive experiments on both chemical and biological benchmark data sets demonstrate the effectiveness of the proposed approach.",0
"The main focus of this research is to investigate unsupervised/self-supervised whole-graph representation learning, which is crucial in various tasks such as predicting molecule properties for drug and material discovery. Although current methods concentrate on maintaining the local similarity structure among different graph instances, they fail to uncover the overall semantic structure of the entire dataset. To address this issue, the authors propose GraphLoG, a unified framework for self-supervised whole-graph representation learning that introduces hierarchical prototypes to capture global semantic clusters while preserving local similarities. Furthermore, an efficient online expectation-maximization algorithm is developed to learn the model. Pre-training GraphLoG on vast unlabeled graphs followed by fine-tuning on downstream tasks shows its effectiveness in various chemical and biological benchmark datasets.",1
"We present Language-mediated, Object-centric Representation Learning (LORL), a paradigm for learning disentangled, object-centric scene representations from vision and language. LORL builds upon recent advances in unsupervised object discovery and segmentation, notably MONet and Slot Attention. While these algorithms learn an object-centric representation just by reconstructing the input image, LORL enables them to further learn to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. These object-centric concepts derived from language facilitate the learning of object-centric representations. LORL can be integrated with various unsupervised object discovery algorithms that are language-agnostic. Experiments show that the integration of LORL consistently improves the performance of unsupervised object discovery methods on two datasets via the help of language. We also show that concepts learned by LORL, in conjunction with object discovery methods, aid downstream tasks such as referring expression comprehension.",0
"Our article introduces Language-mediated, Object-centric Representation Learning (LORL), a method for acquiring disentangled, object-centric scene representations from vision and language. LORL expands on recent advancements in unsupervised object discovery and segmentation, specifically MONet and Slot Attention. While these algorithms attain an object-centric representation by reconstructing the input image, LORL allows them to additionally learn to associate the learned representations to concepts derived from language input, such as words for object categories, properties, and spatial relationships. These object-centric concepts acquired from language aid in the learning of object-centric representations. LORL can be combined with various language-agnostic unsupervised object discovery algorithms. Our experiments demonstrate that integrating LORL consistently boosts the performance of unsupervised object discovery methods on two datasets with the help of language. Furthermore, we demonstrate that the concepts learned by LORL, in combination with object discovery methods, improve downstream tasks like referring expression comprehension.",1
"Instance contrast for unsupervised representation learning has achieved great success in recent years. In this work, we explore the idea of instance contrastive learning in unsupervised domain adaptation (UDA) and propose a novel Category Contrast technique (CaCo) that introduces semantic priors on top of instance discrimination for visual UDA tasks. By considering instance contrastive learning as a dictionary look-up operation, we construct a semantics-aware dictionary with samples from both source and target domains where each target sample is assigned a (pseudo) category label based on the category priors of source samples. This allows category contrastive learning (between target queries and the category-level dictionary) for category-discriminative yet domain-invariant feature representations: samples of the same category (from either source or target domain) are pulled closer while those of different categories are pushed apart simultaneously. Extensive UDA experiments in multiple visual tasks ($e.g.$, segmentation, classification and detection) show that the simple implementation of CaCo achieves superior performance as compared with the highly-optimized state-of-the-art methods. Analytically and empirically, the experiments also demonstrate that CaCo is complementary to existing UDA methods and generalizable to other learning setups such as semi-supervised learning, unsupervised model adaptation, etc.",0
"Recently, unsupervised representation learning has seen great success through instance contrast. This study explores the use of instance contrastive learning in unsupervised domain adaptation (UDA) and introduces a new technique called Category Contrast (CaCo) that utilizes semantic priors to enhance instance discrimination for visual UDA tasks. By constructing a semantics-aware dictionary with samples from both source and target domains, and assigning (pseudo) category labels to target samples based on category priors of source samples, CaCo enables category contrastive learning between target queries and the category-level dictionary. This facilitates the creation of category-discriminative yet domain-invariant feature representations, which pull samples of the same category (from either source or target domain) closer while pushing those of different categories apart. Extensive UDA experiments across multiple visual tasks (e.g. segmentation, classification, and detection) demonstrate that the simple implementation of CaCo outperforms highly-optimized state-of-the-art methods. Additionally, analytical and empirical findings suggest that CaCo is complementary to existing UDA methods and can be applied to other learning setups, including semi-supervised learning and unsupervised model adaptation.",1
"Medical image analysis typically includes several tasks such as enhancement, segmentation, and classification. Traditionally, these tasks are implemented using separate deep learning models for separate tasks, which is not efficient because it involves unnecessary training repetitions, demands greater computational resources, and requires a relatively large amount of labeled data. In this paper, we propose a multi-task training approach for medical image analysis, where individual tasks are fine-tuned simultaneously through relevant knowledge transfer using a unified modality-specific feature representation (UMS-Rep). We explore different fine-tuning strategies to demonstrate the impact of the strategy on the performance of target medical image tasks. We experiment with different visual tasks (e.g., image denoising, segmentation, and classification) to highlight the advantages offered with our approach for two imaging modalities, chest X-ray and Doppler echocardiography. Our results demonstrate that the proposed approach reduces the overall demand for computational resources and improves target task generalization and performance. Further, our results prove that the performance of target tasks in medical images is highly influenced by the utilized fine-tuning strategy.",0
"The process of medical image analysis includes various tasks such as enhancement, segmentation, and classification. Traditionally, these tasks are tackled using separate deep learning models, which can be inefficient due to repeated training, high computational requirements, and the need for a large amount of labeled data. In this study, we propose a multi-task training method for medical image analysis that simultaneously fine-tunes individual tasks using a unified modality-specific feature representation (UMS-Rep) to enable knowledge transfer. We explore different fine-tuning approaches to evaluate their impact on the performance of target medical image tasks, including image denoising, segmentation, and classification for two imaging modalities, chest X-ray and Doppler echocardiography. Our findings demonstrate that this approach reduces the computational resources required and improves target task performance and generalization. Additionally, we discovered that the fine-tuning strategy has a significant impact on the performance of target tasks in medical images.",1
"Detecting out-of-distribution (OOD) samples plays a key role in open-world and safety-critical applications such as autonomous systems and healthcare. Self-supervised representation learning techniques (e.g., contrastive learning and pretext learning) are well suited for learning representation that can identify OOD samples. In this paper, we propose a simple framework that leverages multi-task transformation learning for training effective representation for OOD detection which outperforms state-of-the-art OOD detection performance and robustness on several image datasets. We empirically observe that the OOD performance depends on the choice of data transformations which itself depends on the in-domain training set. To address this problem, we propose a simple mechanism for selecting the transformations automatically and modulate their effect on representation learning without requiring any OOD training samples. We characterize the criteria for a desirable OOD detector for real-world applications and demonstrate the efficacy of our proposed technique against a diverse range of the state-of-the-art OOD detection techniques.",0
"The identification of out-of-distribution (OOD) samples is crucial in fields such as autonomous systems and healthcare where safety is a top priority. Self-supervised representation learning methods, such as contrastive learning and pretext learning, are well-suited for developing representations that can accurately detect OOD samples. This study introduces a basic framework that utilizes multi-task transformation learning to train effective representations for OOD detection, surpassing previously established benchmarks in the area of OOD detection performance and robustness for numerous image datasets. The authors note that the performance of OOD detection is impacted by the choice of data transformations, which is determined by the in-domain training data. To address this issue, a simple approach is proposed for automatically selecting the transformations and regulating their impact on representation learning without requiring any OOD training samples. The authors define the key characteristics of an ideal OOD detector for real-world applications and demonstrate the effectiveness of their proposed technique against a diverse range of the most advanced OOD detection methods.",1
"Our work focuses on unsupervised and generative methods that address the following goals: (a) learning unsupervised generative representations that discover latent factors controlling image semantic attributes, (b) studying how this ability to control attributes formally relates to the issue of latent factor disentanglement, clarifying related but dissimilar concepts that had been confounded in the past, and (c) developing anomaly detection methods that leverage representations learned in (a). For (a), we propose a network architecture that exploits the combination of multiscale generative models with mutual information (MI) maximization. For (b), we derive an analytical result (Lemma 1) that brings clarity to two related but distinct concepts: the ability of generative networks to control semantic attributes of images they generate, resulting from MI maximization, and the ability to disentangle latent space representations, obtained via total correlation minimization. More specifically, we demonstrate that maximizing semantic attribute control encourages disentanglement of latent factors. Using Lemma 1 and adopting MI in our loss function, we then show empirically that, for image generation tasks, the proposed approach exhibits superior performance as measured in the quality and disentanglement trade space, when compared to other state of the art methods, with quality assessed via the Frechet Inception Distance (FID), and disentanglement via mutual information gap. For (c), we design several systems for anomaly detection exploiting representations learned in (a), and demonstrate their performance benefits when compared to state-of-the-art generative and discriminative algorithms. The above contributions in representation learning have potential applications in addressing other important problems in computer vision, such as bias and privacy in AI.",0
"Our research focuses on unsupervised and generative techniques to achieve three goals. Firstly, we aim to develop unsupervised generative models that can identify latent factors controlling the semantic attributes of an image. Secondly, we aim to explore how this ability to control attributes is related to the disentanglement of latent space representations. We clarify two related but distinct concepts, namely the ability to control semantic attributes and the ability to disentangle latent factors. We show that maximizing semantic attribute control leads to disentanglement of latent factors. Thirdly, we seek to develop anomaly detection methods that use the representations learned in (a). To achieve these goals, we propose a network architecture that combines multiscale generative models with mutual information maximization. We also derive an analytical result that clarifies the relationship between semantic attribute control and disentanglement. We demonstrate the effectiveness of our approach by comparing it with state-of-the-art methods in terms of quality and disentanglement trade space, using Frechet Inception Distance and mutual information gap measures. Finally, we show that our approach can be used for anomaly detection and has potential applications in addressing problems such as bias and privacy in AI.",1
"Unsupervised multi-object representation learning depends on inductive biases to guide the discovery of object-centric representations that generalize. However, we observe that methods for learning these representations are either impractical due to long training times and large memory consumption or forego key inductive biases. In this work, we introduce EfficientMORL, an efficient framework for the unsupervised learning of object-centric representations. We show that optimization challenges caused by requiring both symmetry and disentanglement can in fact be addressed by high-cost iterative amortized inference by designing the framework to minimize its dependence on it. We take a two-stage approach to inference: first, a hierarchical variational autoencoder extracts symmetric and disentangled representations through bottom-up inference, and second, a lightweight network refines the representations with top-down feedback. The number of refinement steps taken during training is reduced following a curriculum, so that at test time with zero steps the model achieves 99.1% of the refined decomposition performance. We demonstrate strong object decomposition and disentanglement on the standard multi-object benchmark while achieving nearly an order of magnitude faster training and test time inference over the previous state-of-the-art model.",0
"The learning of multi-object representation without supervision relies on inductive biases to facilitate the discovery of object-centric representations that can generalize. However, existing methods for this process have either been impractical due to long training times and high memory usage or have not incorporated essential inductive biases. In this study, we present EfficientMORL, a proficient framework for unsupervised object-centric representation learning. We address the optimization challenges that arise from requiring both symmetry and disentanglement by minimizing the framework's reliance on high-cost iterative amortized inference. Our approach involves a two-stage inference process, where a hierarchical variational autoencoder extracts symmetric and disentangled representations through bottom-up inference, and a lightweight network refines the representations with top-down feedback. The number of refinement steps taken during training is reduced gradually, and the model achieves 99.1% of the refined decomposition performance with zero steps during testing. We demonstrate robust object decomposition and disentanglement on the standard multi-object benchmark, and the model achieves nearly ten times faster training and test time inference compared to the previous state-of-the-art model.",1
"We propose SelfDoc, a task-agnostic pre-training framework for document image understanding. Because documents are multimodal and are intended for sequential reading, our framework exploits the positional, textual, and visual information of every semantically meaningful component in a document, and it models the contextualization between each block of content. Unlike existing document pre-training models, our model is coarse-grained instead of treating individual words as input, therefore avoiding an overly fine-grained with excessive contextualization. Beyond that, we introduce cross-modal learning in the model pre-training phase to fully leverage multimodal information from unlabeled documents. For downstream usage, we propose a novel modality-adaptive attention mechanism for multimodal feature fusion by adaptively emphasizing language and vision signals. Our framework benefits from self-supervised pre-training on documents without requiring annotations by a feature masking training strategy. It achieves superior performance on multiple downstream tasks with significantly fewer document images used in the pre-training stage compared to previous works.",0
"SelfDoc is a pre-training framework designed for document image understanding that is not limited to specific tasks. Our approach takes into account the fact that documents are multimodal and meant for sequential reading. Thus, our framework leverages the positional, textual, and visual information of every semantically meaningful component in a document, as well as the contextualization between each block of content. Unlike existing models, our approach is coarse-grained, avoiding overly fine-grained contextualization that may lead to excessive processing. Additionally, we have introduced cross-modal learning in the model pre-training phase to fully utilize multimodal information from unlabeled documents. To enhance downstream usage, we have proposed a modality-adaptive attention mechanism for multimodal feature fusion, which emphasizes language and vision signals as needed. Our framework benefits from self-supervised pre-training on documents without requiring annotations by a feature masking training strategy. The proposed approach achieves superior performance on multiple downstream tasks with significantly fewer document images used in the pre-training stage compared to previous methods.",1
"Instance discriminative self-supervised representation learning has been attracted attention thanks to its unsupervised nature and informative feature representation for downstream tasks. In practice, it commonly uses a larger number of negative samples than the number of supervised classes. However, there is an inconsistency in the existing analysis; theoretically, a large number of negative samples degrade classification performance on a downstream supervised task, while empirically, they improve the performance. We provide a novel framework to analyze this empirical result regarding negative samples using the coupon collector's problem. Our bound can implicitly incorporate the supervised loss of the downstream task in the self-supervised loss by increasing the number of negative samples. We confirm that our proposed analysis holds on real-world benchmark datasets.",0
"The self-supervised representation learning that focuses on discriminating instances has gained attention for its ability to provide informative feature representation for downstream tasks without supervision. Typically, this approach uses more negative samples than supervised classes, but there is a discrepancy in the analysis. While theory suggests that a large number of negative samples could harm classification performance in downstream supervised tasks, empirical evidence shows the opposite. To address this inconsistency, we present a new framework that leverages the coupon collector's problem to analyze the impact of negative samples on performance. Our approach can implicitly integrate the supervised loss of the downstream task into the self-supervised loss by increasing the number of negative samples. We validate our analysis using real-world benchmark datasets.",1
"Powered by the ImageNet dataset, unsupervised learning on large-scale data has made significant advances for classification tasks. There are two major challenges to allow such an attractive learning modality for segmentation tasks: i) a large-scale benchmark for assessing algorithms is missing; ii) unsupervised shape representation learning is difficult. We propose a new problem of large-scale unsupervised semantic segmentation (LUSS) with a newly created benchmark dataset to track the research progress. Based on the ImageNet dataset, we propose the ImageNet-S dataset with 1.2 million training images and 40k high-quality semantic segmentation annotations for evaluation. Our benchmark has a high data diversity and a clear task objective. We also present a simple yet effective baseline method that works surprisingly well for LUSS. In addition, we benchmark related un/weakly supervised methods accordingly, identifying the challenges and possible directions of LUSS.",0
"Unsupervised learning on large-scale data, powered by the ImageNet dataset, has made significant strides in classification tasks. However, two major challenges prevent this learning modality from being applied to segmentation tasks: the absence of a large-scale benchmark for assessing algorithms and the difficulty of unsupervised shape representation learning. To address these challenges, we introduce a new problem, large-scale unsupervised semantic segmentation (LUSS), along with a newly created benchmark dataset to track research progress. Our benchmark, the ImageNet-S dataset, is based on the ImageNet dataset and contains 1.2 million training images and 40k high-quality semantic segmentation annotations for evaluation. It has high data diversity and a clear task objective. Additionally, we present a simple yet effective baseline method that performs surprisingly well for LUSS. We also benchmark related un/weakly supervised methods, identifying the challenges and possible directions of LUSS.",1
"The optimal way for a deep reinforcement learning (DRL) agent to explore is to learn a set of skills that achieves a uniform distribution of states. Following this,we introduce DisTop, a new model that simultaneously learns diverse skills and focuses on improving rewarding skills. DisTop progressively builds a discrete topology of the environment using an unsupervised contrastive loss, a growing network and a goal-conditioned policy. Using this topology, a state-independent hierarchical policy can select where the agent has to keep discovering skills in the state space. In turn, the newly visited states allows an improved learnt representation and the learning loop continues. Our experiments emphasize that DisTop is agnostic to the ground state representation and that the agent can discover the topology of its environment whether the states are high-dimensional binary data, images, or proprioceptive inputs. We demonstrate that this paradigm is competitiveon MuJoCo benchmarks with state-of-the-art algorithms on both single-task dense rewards and diverse skill discovery. By combining these two aspects, we showthat DisTop achieves state-of-the-art performance in comparison with hierarchical reinforcement learning (HRL) when rewards are sparse. We believe DisTop opens new perspectives by showing that bottom-up skill discovery combined with representation learning can unlock the exploration challenge in DRL.",0
"To achieve a uniform distribution of states, deep reinforcement learning (DRL) agents should learn a set of skills for optimal exploration. In this regard, we present DisTop, a novel model that concurrently enhances rewarding skills and acquires diverse skills. DisTop constructs a discrete topology of the environment through an unsupervised contrastive loss, a growing network, and a goal-conditioned policy, which allows a state-independent hierarchical policy to select where the agent should explore new skills in the state space. As a result, the agent's learned representation improves, and the learning loop continues. Our experiments demonstrate that DisTop is independent of the ground state representation and can discover the environment's topology, regardless of the type of input. Furthermore, DisTop achieves state-of-the-art performance in comparison with hierarchical reinforcement learning (HRL) when rewards are sparse. By combining bottom-up skill discovery with representation learning, DisTop offers new perspectives for overcoming the exploration challenge in DRL.",1
"Cross-view geo-localization is to spot images of the same geographic target from different platforms, e.g., drone-view cameras and satellites. It is challenging in the large visual appearance changes caused by extreme viewpoint variations. Existing methods usually concentrate on mining the fine-grained feature of the geographic target in the image center, but underestimate the contextual information in neighbor areas. In this work, we argue that neighbor areas can be leveraged as auxiliary information, enriching discriminative clues for geolocalization. Specifically, we introduce a simple and effective deep neural network, called Local Pattern Network (LPN), to take advantage of contextual information in an end-to-end manner. Without using extra part estimators, LPN adopts a square-ring feature partition strategy, which provides the attention according to the distance to the image center. It eases the part matching and enables the part-wise representation learning. Owing to the square-ring partition design, the proposed LPN has good scalability to rotation variations and achieves competitive results on three prevailing benchmarks, i.e., University-1652, CVUSA and CVACT. Besides, we also show the proposed LPN can be easily embedded into other frameworks to further boost performance.",0
"The process of identifying images of the same location from different platforms, such as drones and satellites, is known as cross-view geo-localization. This is a difficult task due to the significant changes in visual appearance caused by varying viewpoints. Current methods focus on extracting detailed features from the center of the image, while neglecting important information from surrounding areas. In this study, we propose that contextual information from neighboring regions can enhance the accuracy of geolocalization. To achieve this, we developed a deep neural network called Local Pattern Network (LPN), which utilizes contextual information in an end-to-end manner. LPN uses a square-ring feature partition strategy to provide attention based on distance from the image center, facilitating part matching and representation learning. The LPN design is scalable to rotation variations and achieves competitive results on three popular benchmarks, including University-1652, CVUSA and CVACT. Furthermore, our proposed LPN can be easily integrated into other frameworks to further enhance performance.",1
"The recent breakthrough achieved by contrastive learning accelerates the pace for deploying unsupervised training on real-world data applications. However, unlabeled data in reality is commonly imbalanced and shows a long-tail distribution, and it is unclear how robustly the latest contrastive learning methods could perform in the practical scenario. This paper proposes to explicitly tackle this challenge, via a principled framework called Self-Damaging Contrastive Learning (SDCLR), to automatically balance the representation learning without knowing the classes. Our main inspiration is drawn from the recent finding that deep models have difficult-to-memorize samples, and those may be exposed through network pruning. It is further natural to hypothesize that long-tail samples are also tougher for the model to learn well due to insufficient examples. Hence, the key innovation in SDCLR is to create a dynamic self-competitor model to contrast with the target model, which is a pruned version of the latter. During training, contrasting the two models will lead to adaptive online mining of the most easily forgotten samples for the current target model, and implicitly emphasize them more in the contrastive loss. Extensive experiments across multiple datasets and imbalance settings show that SDCLR significantly improves not only overall accuracies but also balancedness, in terms of linear evaluation on the full-shot and few-shot settings. Our code is available at: https://github.com/VITA-Group/SDCLR.",0
"Contrastive learning has made progress in deploying unsupervised training on real-world data, but the challenge of imbalanced and long-tail distribution of unlabeled data remains. This paper presents Self-Damaging Contrastive Learning (SDCLR), which addresses this issue by automatically balancing representation learning without knowledge of classes. The approach is inspired by recent findings that deep models struggle with difficult-to-memorize samples and long-tail samples due to insufficient examples. SDCLR creates a dynamic self-competitor model to contrast with the target model, leading to adaptive online mining of easily forgotten samples and emphasizing them more in the contrastive loss. Experiments on multiple datasets and imbalance settings demonstrate SDCLR's ability to improve overall accuracy and balancedness in full-shot and few-shot settings. Code is available at https://github.com/VITA-Group/SDCLR.",1
"There have been many recent advances in representation learning; however, unsupervised representation learning can still struggle with model identification issues. Variational Auto-Encoders (VAEs) and their extensions such as $\beta$-VAEs have been shown to locally align latent variables with PCA directions, which can help to improve model disentanglement under some conditions. Borrowing inspiration from Independent Component Analysis (ICA) and sparse coding, we propose applying an $L_1$ loss to the VAE's generative Jacobian during training to encourage local latent variable alignment with independent factors of variation in the data. We demonstrate our results on a variety of datasets, giving qualitative and quantitative results using information theoretic and modularity measures that show our added $L_1$ cost encourages local axis alignment of the latent representation with individual factors of variation.",0
"Representation learning has made significant progress in recent times, but unsupervised representation learning still faces challenges related to model identification. One solution that has emerged is the use of Variational Auto-Encoders (VAEs) and their extensions, such as $\beta$-VAEs, which have been found to align latent variables with PCA directions to enhance model disentanglement under specific circumstances. Our proposal draws inspiration from Independent Component Analysis (ICA) and sparse coding, and involves applying an $L_1$ loss to the VAE's generative Jacobian during training to promote local alignment of latent variables with independent factors of variation in the data. Through our experiments on diverse datasets, we provide qualitative and quantitative evidence, based on information theoretic and modularity measures, that our approach of adding an $L_1$ cost ensures that the latent representation aligns with individual factors of variation by encouraging local axis alignment.",1
"Learning the representation of data with hierarchical structures in the hyperbolic space attracts increasing attention in recent years. Due to the constant negative curvature, the hyperbolic space resembles tree metrics and captures the tree-like properties naturally, which enables the hyperbolic embeddings to improve over traditional Euclidean models. However, many real-world hierarchically structured data such as taxonomies and multitree networks have varying local structures and they are not trees, thus they do not ubiquitously match the constant curvature property of the hyperbolic space. To address this limitation of hyperbolic embeddings, we explore the complex hyperbolic space, which has the variable negative curvature, for representation learning. Specifically, we propose to learn the embeddings of hierarchically structured data in the unit ball model of the complex hyperbolic space. The unit ball model based embeddings have a more powerful representation capacity to capture a variety of hierarchical structures. Through experiments on synthetic and real-world data, we show that our approach improves over the hyperbolic embedding models significantly.",0
"In recent years, there has been a growing interest in learning how to represent data with hierarchical structures in the hyperbolic space. This is due to the fact that the hyperbolic space, with its constant negative curvature, is similar to tree metrics and is able to naturally capture tree-like properties. As a result, hyperbolic embeddings are able to outperform traditional Euclidean models. However, many real-world hierarchically structured data, such as taxonomies and multitree networks, have varying local structures that do not always match the constant curvature property of the hyperbolic space. To address this limitation, we propose exploring the complex hyperbolic space, which has variable negative curvature, for representation learning. Our approach involves learning the embeddings of hierarchically structured data in the unit ball model of the complex hyperbolic space. These embeddings have a more powerful representation capacity, allowing them to capture a wider range of hierarchical structures. Our experiments on both synthetic and real-world data demonstrate the significant improvement of our approach over existing hyperbolic embedding models.",1
"Self-supervised learning is a form of unsupervised learning that leverages rich information in data to learn representations. However, data sometimes contains certain information that may be undesirable for downstream tasks. For instance, gender information may lead to biased decisions on many gender-irrelevant tasks. In this paper, we develop conditional contrastive learning to remove undesirable information in self-supervised representations. To remove the effect of the undesirable variable, our proposed approach conditions on the undesirable variable (i.e., by fixing the variations of it) during the contrastive learning process. In particular, inspired by the contrastive objective InfoNCE, we introduce Conditional InfoNCE (C-InfoNCE), and its computationally efficient variant, Weak-Conditional InfoNCE (WeaC-InfoNCE), for conditional contrastive learning. We demonstrate empirically that our methods can successfully learn self-supervised representations for downstream tasks while removing a great level of information related to the undesirable variables. We study three scenarios, each with a different type of undesirable variables: task-irrelevant meta-information for self-supervised speech representation learning, sensitive attributes for fair representation learning, and domain specification for multi-domain visual representation learning.",0
"Self-supervised learning is a variant of unsupervised learning that utilizes the vast amount of information in data to acquire representations. However, the data may contain certain information that is not desirable for subsequent tasks. One example is gender information, which can lead to biased decisions in gender-neutral tasks. To address this issue, we present a new approach called conditional contrastive learning that eliminates undesirable information from self-supervised representations. Our method conditions on the unwanted variable, keeping its variations fixed during the contrastive learning process. We introduce two variants of the contrastive objective InfoNCE, namely Conditional InfoNCE (C-InfoNCE) and Weak-Conditional InfoNCE (WeaC-InfoNCE), for efficient conditional contrastive learning. Our experiments demonstrate that our methods can effectively learn self-supervised representations while eliminating a significant amount of unwanted information in various scenarios, including task-irrelevant meta-information for self-supervised speech representation learning, sensitive attributes for fair representation learning, and domain specification for multi-domain visual representation learning.",1
"The dramatically growing availability of observational data is being witnessed in various domains of science and technology, which facilitates the study of causal inference. However, estimating treatment effects from observational data is faced with two major challenges, missing counterfactual outcomes and treatment selection bias. Matching methods are among the most widely used and fundamental approaches to estimating treatment effects, but existing matching methods have poor performance when facing data with high dimensional and complicated variables. We propose a feature selection representation matching (FSRM) method based on deep representation learning and matching, which maps the original covariate space into a selective, nonlinear, and balanced representation space, and then conducts matching in the learned representation space. FSRM adopts deep feature selection to minimize the influence of irrelevant variables for estimating treatment effects and incorporates a regularizer based on the Wasserstein distance to learn balanced representations. We evaluate the performance of our FSRM method on three datasets, and the results demonstrate superiority over the state-of-the-art methods.",0
"Observational data is increasingly available in numerous fields, which is helpful for studying causal inference. However, two main obstacles to estimating treatment effects from such data are the lack of counterfactual outcomes and treatment selection bias. While matching methods are commonly used for this purpose, they are not effective when dealing with complex and high-dimensional variables. To address this issue, we propose a new method called feature selection representation matching (FSRM), which employs deep representation learning and matching to map the original covariate space into a selective, nonlinear, and balanced representation space. Our FSRM method uses deep feature selection to minimize the effect of irrelevant variables in estimating treatment effects and includes a regularizer based on the Wasserstein distance to learn balanced representations. We evaluated our FSRM method on three datasets and found it to outperform state-of-the-art methods.",1
"Image-level contrastive representation learning has proven to be highly effective as a generic model for transfer learning. Such generality for transfer learning, however, sacrifices specificity if we are interested in a certain downstream task. We argue that this could be sub-optimal and thus advocate a design principle which encourages alignment between the self-supervised pretext task and the downstream task. In this paper, we follow this principle with a pretraining method specifically designed for the task of object detection. We attain alignment in the following three aspects: 1) object-level representations are introduced via selective search bounding boxes as object proposals; 2) the pretraining network architecture incorporates the same dedicated modules used in the detection pipeline (e.g. FPN); 3) the pretraining is equipped with object detection properties such as object-level translation invariance and scale invariance. Our method, called Selective Object COntrastive learning (SoCo), achieves state-of-the-art results for transfer performance on COCO detection using a Mask R-CNN framework. Code and models will be made available.",0
"Generic models for transfer learning have shown that image-level contrastive representation learning is highly effective. However, when considering a specific downstream task, this generality may come at the cost of specificity. Therefore, we propose a design principle that emphasizes alignment between the self-supervised pretext task and the downstream task, which we believe is more optimal. In this study, we apply this principle to pretraining for object detection. We achieve alignment in three ways: 1) object-level representations are introduced using selective search bounding boxes as object proposals; 2) the pretraining network architecture incorporates the same dedicated modules used in the detection pipeline (e.g. FPN); 3) the pretraining is equipped with object detection properties such as object-level translation invariance and scale invariance. Our method, called Selective Object COntrastive learning (SoCo), achieves state-of-the-art results for transfer performance on COCO detection using a Mask R-CNN framework. We will provide access to the code and models used in this study.",1
"Change points are abrupt alterations in the distribution of sequential data. A change-point detection (CPD) model aims at quick detection of such changes. Classic approaches perform poorly for semi-structured sequential data because of the absence of adequate data representation learning. To deal with it, we introduce a principled differentiable loss function that considers the specificity of the CPD task. The theoretical results suggest that this function approximates well classic rigorous solutions. For such loss function, we propose an end-to-end method for the training of deep representation learning CPD models. Our experiments provide evidence that the proposed approach improves baseline results of change point detection for various data types, including real-world videos and image sequences, and improve representations for them.",0
"Abrupt changes in sequential data are referred to as change points, and detecting them quickly is the goal of a change-point detection (CPD) model. However, classic methods tend to struggle with semi-structured sequential data because of inadequate data representation learning. To address this, we have introduced a principled and differentiable loss function that takes into account the specificity of the CPD task. Theoretical findings suggest that this function approximates classical solutions well. To train deep representation learning CPD models using this loss function, we propose an end-to-end method. Our experiments demonstrate that this approach enhances baseline results for detecting change points across various data types, including real-world videos and image sequences, and also improves their representations.",1
"We introduce the forward-backward (FB) representation of the dynamics of a reward-free Markov decision process. It provides explicit near-optimal policies for any reward specified a posteriori. During an unsupervised phase, we use reward-free interactions with the environment to learn two representations via off-the-shelf deep learning methods and temporal difference (TD) learning. In the test phase, a reward representation is estimated either from observations or an explicit reward description (e.g., a target state). The optimal policy for that reward is directly obtained from these representations, with no planning. We assume access to an exploration scheme or replay buffer for the first phase.   The unsupervised FB loss is well-principled: if training is perfect, the policies obtained are provably optimal for any reward function. With imperfect training, the sub-optimality is proportional to the unsupervised approximation error. The FB representation learns long-range relationships between states and actions, via a predictive occupancy map, without having to synthesize states as in model-based approaches.   This is a step towards learning controllable agents in arbitrary black-box stochastic environments. This approach compares well to goal-oriented RL algorithms on discrete and continuous mazes, pixel-based MsPacman, and the FetchReach virtual robot arm. We also illustrate how the agent can immediately adapt to new tasks beyond goal-oriented RL.",0
"Our paper introduces the forward-backward (FB) framework, which represents the dynamics of a reward-free Markov decision process and provides explicit near-optimal policies for any reward specified a posteriori. In an unsupervised phase, we utilize reward-free interactions with the environment to learn two representations using off-the-shelf deep learning methods and temporal difference (TD) learning. During the test phase, we estimate a reward representation either from observations or an explicit reward description, and the optimal policy for that reward is directly obtained from these representations without planning. We assume access to an exploration scheme or replay buffer during the first phase. The unsupervised FB loss is well-principled, and with perfect training, the policies obtained are proven optimal for any reward function. The FB representation learns long-range relationships between states and actions without synthesizing states as in model-based approaches, making it a promising approach for learning controllable agents in arbitrary black-box stochastic environments. Our approach outperforms goal-oriented RL algorithms on discrete and continuous mazes, pixel-based MsPacman, and the FetchReach virtual robot arm. Furthermore, we demonstrate that the agent can immediately adapt to new tasks beyond goal-oriented RL.",1
"Graph neural networks (GNNs) are effective models for representation learning on relational data. However, standard GNNs are limited in their expressive power, as they cannot distinguish graphs beyond the capability of the Weisfeiler-Leman graph isomorphism heuristic. In order to break this expressiveness barrier, GNNs have been enhanced with random node initialization (RNI), where the idea is to train and run the models with randomized initial node features. In this work, we analyze the expressive power of GNNs with RNI, and prove that these models are universal, a first such result for GNNs not relying on computationally demanding higher-order properties. This universality result holds even with partially randomized initial node features, and preserves the invariance properties of GNNs in expectation. We then empirically analyze the effect of RNI on GNNs, based on carefully constructed datasets. Our empirical findings support the superior performance of GNNs with RNI over standard GNNs.",0
"Representation learning on relational data can be effectively achieved using Graph neural networks (GNNs). However, these models have limited expressive power since they cannot distinguish graphs beyond the Weisfeiler-Leman graph isomorphism heuristic. To overcome this limitation, GNNs have been enhanced with random node initialization (RNI) by training and running the models with randomized initial node features. We have examined the expressive power of GNNs with RNI and demonstrated that these models are universal, which is a significant result for GNNs not reliant on computationally demanding higher-order properties. This universality result holds even with partially randomized initial node features and maintains the invariance properties of GNNs in expectation. We have also conducted empirical analysis using carefully constructed datasets and found that GNNs with RNI outperform standard GNNs.",1
"The self-supervised learning (SSL) paradigm is an essential exploration area, which tries to eliminate the need for expensive data labeling. Despite the great success of SSL methods in computer vision and natural language processing, most of them employ contrastive learning objectives that require negative samples, which are hard to define. This becomes even more challenging in the case of graphs and is a bottleneck for achieving robust representations. To overcome such limitations, we propose a framework for self-supervised graph representation learning -- Graph Barlow Twins, which utilizes a cross-correlation-based loss function instead of negative samples. Moreover, it does not rely on non-symmetric neural network architectures -- in contrast to state-of-the-art self-supervised graph representation learning method BGRL. We show that our method achieves as competitive results as BGRL, best self-supervised methods, and fully supervised ones while requiring substantially fewer hyperparameters and converging in an order of magnitude training steps earlier.",0
"The realm of self-supervised learning (SSL) is an important area of study that aims to eliminate the need for costly data labeling. Despite the success of SSL approaches in fields like computer vision and natural language processing, most of these methods use contrastive learning objectives that necessitate negative samples, which are difficult to define. This challenge is magnified in the context of graphs, where generating robust representations is hindered by this bottleneck. To address these limitations, we introduce a self-supervised graph representation learning framework called Graph Barlow Twins, which substitutes a cross-correlation-based loss function for negative samples. Additionally, our approach does not rely on asymmetrical neural network architectures, as opposed to the current state-of-the-art self-supervised graph representation learning approach BGRL. We demonstrate that our method achieves similarly impressive results to BGRL, as well as other top self-supervised and fully supervised methods, while requiring fewer hyperparameters and converging much faster in training.",1
"RGBT tracking has attracted increasing attention since RGB and thermal infrared data have strong complementary advantages, which could make trackers all-day and all-weather work. However, how to effectively represent RGBT data for visual tracking remains unstudied well. Existing works usually focus on extracting modality-shared or modality-specific information, but the potentials of these two cues are not well explored and exploited in RGBT tracking. In this paper, we propose a novel multi-adapter network to jointly perform modality-shared, modality-specific and instance-aware target representation learning for RGBT tracking. To this end, we design three kinds of adapters within an end-to-end deep learning framework. In specific, we use the modified VGG-M as the generality adapter to extract the modality-shared target representations.To extract the modality-specific features while reducing the computational complexity, we design a modality adapter, which adds a small block to the generality adapter in each layer and each modality in a parallel manner. Such a design could learn multilevel modality-specific representations with a modest number of parameters as the vast majority of parameters are shared with the generality adapter. We also design instance adapter to capture the appearance properties and temporal variations of a certain target. Moreover, to enhance the shared and specific features, we employ the loss of multiple kernel maximum mean discrepancy to measure the distribution divergence of different modal features and integrate it into each layer for more robust representation learning. Extensive experiments on two RGBT tracking benchmark datasets demonstrate the outstanding performance of the proposed tracker against the state-of-the-art methods.",0
"The use of RGBT tracking is gaining popularity due to the complementary strengths of RGB and thermal infrared data, allowing for tracking in various weather conditions. However, effective representation of RGBT data for visual tracking has not been thoroughly studied. Existing research has mainly focused on extracting modality-specific or shared information, with limited exploration of their potential in RGBT tracking. This paper proposes a new multi-adapter network that incorporates modality-specific, modality-shared, and instance-aware target representation learning for RGBT tracking. The network includes three types of adapters: a generality adapter to extract modality-shared target representations, a modality adapter to extract modality-specific features, and an instance adapter to capture the appearance and temporal properties of a target. Additionally, the proposed method employs the loss of multiple kernel maximum mean discrepancy to enhance shared and specific features. Experiments on two RGBT tracking benchmark datasets demonstrate the superior performance of the proposed tracker compared to state-of-the-art methods.",1
"State representation learning (SRL) in partially observable Markov decision processes has been studied to learn abstract features of data useful for robot control tasks. For SRL, acquiring domain-agnostic states is essential for achieving efficient imitation learning. Without these states, imitation learning is hampered by domain-dependent information useless for control. However, existing methods fail to remove such disturbances from the states when the data from experts and agents show large domain shifts. To overcome this issue, we propose a domain-adversarial and conditional state space model (DAC-SSM) that enables control systems to obtain domain-agnostic and task- and dynamics-aware states. DAC-SSM jointly optimizes the state inference, observation reconstruction, forward dynamics, and reward models. To remove domain-dependent information from the states, the model is trained with domain discriminators in an adversarial manner, and the reconstruction is conditioned on domain labels. We experimentally evaluated the model predictive control performance via imitation learning for continuous control of sparse reward tasks in simulators and compared it with the performance of the existing SRL method. The agents from DAC-SSM achieved performance comparable to experts and more than twice the baselines. We conclude domain-agnostic states are essential for imitation learning that has large domain shifts and can be obtained using DAC-SSM.",0
"The focus of research in partially observable Markov decision processes has been on State Representation Learning (SRL), which aims to identify abstract features of data that are useful for controlling robots. In order to achieve efficient imitation learning, it is crucial to obtain domain-agnostic states for SRL. If domain-dependent information is present, it can obstruct imitation learning, rendering it ineffective for control. However, current methods have failed to eliminate such disturbances from the states in cases where data from experts and agents exhibit large shifts in domain. To address this issue, we propose a Domain-Adversarial and Conditional State Space Model (DAC-SSM), which enables control systems to acquire domain-agnostic and task- and dynamics-aware states. DAC-SSM optimizes state inference, observation reconstruction, forward dynamics, and reward models together. To eliminate domain-dependent information from the states, the model is trained using domain discriminators in an adversarial way, with reconstruction conditioned on domain labels. We evaluated the performance of model predictive control through imitation learning for continuous control of sparse reward tasks using simulators, and compared it to that of the existing SRL method. The agents from DAC-SSM achieved performance similar to that of experts and more than double the baselines. We conclude that domain-agnostic states are critical for effective imitation learning in cases of large domain shifts, and that DAC-SSM can be used to obtain them.",1
"A highly desirable property of a reinforcement learning (RL) agent -- and a major difficulty for deep RL approaches -- is the ability to generalize policies learned on a few tasks over a high-dimensional observation space to similar tasks not seen during training. Many promising approaches to this challenge consider RL as a process of training two functions simultaneously: a complex nonlinear encoder that maps high-dimensional observations to a latent representation space, and a simple linear policy over this space. We posit that a superior encoder for zero-shot generalization in RL can be trained by using solely an auxiliary SSL objective if the training process encourages the encoder to map behaviorally similar observations to similar representations, as reward-based signal can cause overfitting in the encoder (Raileanu et al., 2021). We propose Cross-Trajectory Representation Learning (CTRL), a method that runs within an RL agent and conditions its encoder to recognize behavioral similarity in observations by applying a novel SSL objective to pairs of trajectories from the agent's policies. CTRL can be viewed as having the same effect as inducing a pseudo-bisimulation metric but, crucially, avoids the use of rewards and associated overfitting risks. Our experiments ablate various components of CTRL and demonstrate that in combination with PPO it achieves better generalization performance on the challenging Procgen benchmark suite (Cobbe et al., 2020).",0
"Deep reinforcement learning faces a major challenge in the ability to generalize policies learned on a few tasks over a high-dimensional observation space to similar tasks not encountered during training. To address this issue, many RL approaches propose training two functions simultaneously: a complex nonlinear encoder and a simple linear policy. However, the reward-based signal can lead to overfitting in the encoder. To overcome this, we suggest training the encoder using only an auxiliary SSL objective, which encourages the encoder to map behaviorally similar observations to similar representations. We introduce a new method called Cross-Trajectory Representation Learning (CTRL) that applies this SSL objective to pairs of trajectories from the agent's policies. CTRL induces a pseudo-bisimulation metric without using rewards, thus mitigating the overfitting risks. Our experiments demonstrate that CTRL, combined with PPO, achieves superior generalization performance on the challenging Procgen benchmark suite.",1
"Current deep reinforcement learning (RL) approaches incorporate minimal prior knowledge about the environment, limiting computational and sample efficiency. \textit{Objects} provide a succinct and causal description of the world, and many recent works have proposed unsupervised object representation learning using priors and losses over static object properties like visual consistency. However, object dynamics and interactions are also critical cues for objectness. In this paper we propose a framework for reasoning about object dynamics and behavior to rapidly determine minimal and task-specific object representations. To demonstrate the need to reason over object behavior and dynamics, we introduce a suite of RGBD MuJoCo object collection and avoidance tasks that, while intuitive and visually simple, confound state-of-the-art unsupervised object representation learning algorithms. We also highlight the potential of this framework on several Atari games, using our object representation and standard RL and planning algorithms to learn dramatically faster than existing deep RL algorithms.",0
"At present, deep reinforcement learning methods have limited computational and sample efficiency due to their minimal prior knowledge about the environment. To address this limitation, recent studies have focused on unsupervised object representation learning using priors and losses over static object properties like visual consistency. However, this approach fails to consider critical cues such as object dynamics and interactions that are essential for understanding objectness. To overcome this issue, we present a framework that utilizes object dynamics and behavior to rapidly determine task-specific and minimal object representations. We demonstrate the importance of object behavior and dynamics by introducing a set of RGBD MuJoCo object collection and avoidance tasks that challenge state-of-the-art unsupervised object representation learning algorithms. Additionally, we showcase the potential of our framework on several Atari games, where our object representation combined with standard RL and planning algorithms significantly outperforms existing deep RL methods.",1
"In recent years self-supervised learning has emerged as a promising candidate for unsupervised representation learning. In the visual domain its applications are mostly studied in the context of images of natural scenes. However, its applicability is especially interesting in specific areas, like remote sensing and medicine, where it is hard to obtain huge amounts of labeled data. In this work, we conduct an extensive analysis of the applicability of self-supervised learning in remote sensing image classification. We analyze the influence of the number and domain of images used for self-supervised pre-training on the performance on downstream tasks. We show that, for the downstream task of remote sensing image classification, using self-supervised pre-training on remote sensing images can give better results than using supervised pre-training on images of natural scenes. Besides, we also show that self-supervised pre-training can be easily extended to multispectral images producing even better results on our downstream tasks.",0
"Self-supervised learning has emerged as a promising approach to unsupervised representation learning in recent years. While its applications in the visual domain have mostly been studied in the context of natural scene images, it is particularly interesting in fields such as remote sensing and medicine where obtaining labeled data is challenging. This study extensively examines the effectiveness of self-supervised learning in remote sensing image classification. The impact of the number and type of images used for self-supervised pre-training on downstream task performance is analyzed. Results indicate that utilizing self-supervised pre-training on remote sensing images yields better results than supervised pre-training on natural scene images for remote sensing image classification. Furthermore, extending self-supervised pre-training to multispectral images leads to even better performance on downstream tasks.",1
"This paper addresses the problem of 3D hand pose estimation from a monocular RGB image. While previous methods have shown great success, the structure of hands has not been fully exploited, which is critical in pose estimation. To this end, we propose a regularized graph representation learning under a conditional adversarial learning framework for 3D hand pose estimation, aiming to capture structural inter-dependencies of hand joints. In particular, we estimate an initial hand pose from a parametric hand model as a prior of hand structure, which regularizes the inference of the structural deformation in the prior pose for accurate graph representation learning via residual graph convolution. To optimize the hand structure further, we propose two bone-constrained loss functions, which characterize the morphable structure of hand poses explicitly. Also, we introduce an adversarial learning framework conditioned on the input image with a multi-source discriminator, which imposes the structural constraints onto the distribution of generated 3D hand poses for anthropomorphically valid hand poses. Extensive experiments demonstrate that our model sets the new state-of-the-art in 3D hand pose estimation from a monocular image on five standard benchmarks.",0
"The objective of this study is to improve the accuracy of 3D hand pose estimation from a single RGB image through exploiting the structure of hands, which has been overlooked in previous methods. The proposed approach involves a regularized graph representation learning technique within a conditional adversarial learning framework. This method aims to capture the inter-dependencies of hand joints by estimating the initial hand pose from a parametric hand model. This prior pose is then used to regulate the inference of structural deformation for accurate graph representation learning with residual graph convolution. Additionally, two bone-constrained loss functions are proposed to further optimize the hand structure and explicitly characterize the morphable structure of hand poses. An adversarial learning framework is also introduced, which is conditioned on the input image and incorporates a multi-source discriminator to ensure the generated 3D hand poses comply with structural constraints for anthropomorphically valid hand poses. The proposed method was evaluated on five standard benchmarks, and experimental results demonstrate state-of-the-art performance in 3D hand pose estimation from a monocular image.",1
"Recent studies on unsupervised object detection based on spatial attention have achieved promising results. Models, such as AIR and SPAIR, output ""what"" and ""where"" latent variables that represent the attributes and locations of objects in a scene, respectively. Most of the previous studies concentrate on the ""where"" localization performance; however, we claim that acquiring ""what"" object attributes is also essential for representation learning. This paper presents a framework, GMAIR, for unsupervised object detection. It incorporates spatial attention and a Gaussian mixture in a unified deep generative model. GMAIR can locate objects in a scene and simultaneously cluster them without supervision. Furthermore, we analyze the ""what"" latent variables and clustering process. Finally, we evaluate our model on MultiMNIST and Fruit2D datasets and show that GMAIR achieves competitive results on localization and clustering compared to state-of-the-art methods.",0
"Recent research has shown promising results in unsupervised object detection using spatial attention. Models like AIR and SPAIR generate latent variables for ""what"" and ""where"" attributes of objects in a scene. While previous studies have focused on the ""where"" attribute, we argue that obtaining ""what"" object attributes is also crucial for representation learning. Our study introduces GMAIR, a framework for unsupervised object detection that combines spatial attention and a Gaussian mixture into a deep generative model. GMAIR can locate and cluster objects in a scene without supervision. We also analyze the ""what"" variables and clustering process. Finally, we evaluate our approach on the MultiMNIST and Fruit2D datasets and demonstrate that GMAIR achieves competitive results compared to other state-of-the-art methods in both localization and clustering.",1
"Representation learning on graphs that evolve has recently received significant attention due to its wide application scenarios, such as bioinformatics, knowledge graphs, and social networks. The propagation of information in graphs is important in learning dynamic graph representations, and most of the existing methods achieve this by aggregation. However, relying only on aggregation to propagate information in dynamic graphs can result in delays in information propagation and thus affect the performance of the method. To alleviate this problem, we propose an aggregation-diffusion (AD) mechanism that actively propagates information to its neighbor by diffusion after the node updates its embedding through the aggregation mechanism. In experiments on two real-world datasets in the dynamic link prediction task, the AD mechanism outperforms the baseline models that only use aggregation to propagate information. We further conduct extensive experiments to discuss the influence of different factors in the AD mechanism.",0
"Recently, there has been a lot of interest in representation learning for evolving graphs due to its broad range of applications, such as bioinformatics, knowledge graphs, and social networks. In order to learn dynamic graph representations, it is crucial to understand the propagation of information in graphs, which is often achieved through aggregation in existing methods. However, relying solely on aggregation for information propagation in dynamic graphs may lead to delays and negatively impact performance. To address this issue, we propose an aggregation-diffusion (AD) mechanism that actively propagates information through diffusion to its neighbors after the node updates its embedding through aggregation. Our experiments on two real-world datasets for dynamic link prediction demonstrate that the AD mechanism outperforms baseline models that only rely on aggregation. We also conduct extensive experiments to investigate the impact of different factors in the AD mechanism.",1
"In this paper we present a novel method for learning hierarchical representations of Markov decision processes. Our method works by partitioning the state space into subsets, and defines subtasks for performing transitions between the partitions. We formulate the problem of partitioning the state space as an optimization problem that can be solved using gradient descent given a set of sampled trajectories, making our method suitable for high-dimensional problems with large state spaces. We empirically validate the method, by showing that it can successfully learn a useful hierarchical representation in a navigation domain. Once learned, the hierarchical representation can be used to solve different tasks in the given domain, thus generalizing knowledge across tasks.",0
"A new approach to acquiring hierarchical representations of Markov decision processes is introduced in this paper. The approach involves dividing the state space into subsets and defining subtasks that enable transitions between the partitions. An optimization problem that can be solved through gradient descent with a set of sampled trajectories is used to partition the state space, making it applicable to high-dimensional issues with extensive state spaces. In the navigation domain, we prove the method's effectiveness by demonstrating that it can acquire a valuable hierarchical representation. This hierarchical representation can be used to solve a variety of tasks in the field, allowing for knowledge generalization across tasks.",1
"Self-supervised or weakly supervised models trained on large-scale datasets have shown sample-efficient transfer to diverse datasets in few-shot settings. We consider how upstream pretrained models can be leveraged for downstream few-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIP PERsonalized) uses image representations from CLIP, a large-scale image representation learning model trained using weak natural language supervision. We developed a technique, called Multi-label Weight Imprinting (MWI), for multi-label, continual, and few-shot learning, and CLIPPER uses MWI with image representations from CLIP. We evaluated CLIPPER on 10 single-label and 5 multi-label datasets. Our model shows robust and competitive performance, and we set new benchmarks for few-shot, multi-label, and continual learning. Our lightweight technique is also compute-efficient and enables privacy-preserving applications as the data is not sent to the upstream model for fine-tuning.",0
"Large-scale datasets have been utilized to train weakly supervised or self-supervised models that demonstrate efficient transfer to diverse datasets in few-shot settings. Our focus is on leveraging upstream pretrained models for downstream tasks including few-shot, multilabel, and continual learning. Using image representations from the weak natural language supervised CLIP model, we introduce our CLIPPER (CLIP Personalized) model. We have developed an innovative technique, Multi-label Weight Imprinting (MWI), for continual, multi-label, and few-shot learning, which CLIPPER utilizes with image representations from CLIP. Our study evaluated CLIPPER on 10 single-label and 5 multi-label datasets, demonstrating its outstanding and competitive performance, setting new benchmarks for few-shot, multi-label, and continual learning. Our approach is efficient in terms of both computation and privacy, as data does not need to be sent to the upstream model for fine-tuning.",1
"Learning to reach goal states and learning diverse skills through mutual information (MI) maximization have been proposed as principled frameworks for self-supervised reinforcement learning, allowing agents to acquire broadly applicable multitask policies with minimal reward engineering. Starting from a simple observation that the standard goal-conditioned RL (GCRL) is encapsulated by the optimization objective of variational empowerment, we discuss how GCRL and MI-based RL can be generalized into a single family of methods, which we name variational GCRL (VGCRL), interpreting variational MI maximization, or variational empowerment, as representation learning methods that acquire functionally-aware state representations for goal reaching. This novel perspective allows us to: (1) derive simple but unexplored variants of GCRL to study how adding small representation capacity can already expand its capabilities; (2) investigate how discriminator function capacity and smoothness determine the quality of discovered skills, or latent goals, through modifying latent dimensionality and applying spectral normalization; (3) adapt techniques such as hindsight experience replay (HER) from GCRL to MI-based RL; and lastly, (4) propose a novel evaluation metric, named latent goal reaching (LGR), for comparing empowerment algorithms with different choices of latent dimensionality and discriminator parameterization. Through principled mathematical derivations and careful experimental studies, our work lays a novel foundation from which to evaluate, analyze, and develop representation learning techniques in goal-based RL.",0
"Self-supervised reinforcement learning can be achieved through maximizing mutual information (MI) to learn diverse skills and reach goal states. This approach allows agents to develop versatile policies without extensive reward engineering. By recognizing that standard goal-conditioned RL (GCRL) is a type of variational empowerment optimization, we introduce the concept of variational GCRL (VGCRL), which combines GCRL and MI-based RL into a single family of methods. VGCRL employs variational MI maximization or variational empowerment as a means of representation learning to acquire functionally-aware state representations for goal reaching. Our perspective enables us to: (1) explore simple variants of GCRL to examine how adding representation capacity can improve its capabilities; (2) study how discriminator function capacity and smoothness affect the quality of skills discovered through latent dimensionality modification and spectral normalization; (3) apply techniques such as hindsight experience replay (HER) to MI-based RL; and (4) introduce latent goal reaching (LGR) as a novel evaluation metric for comparing empowerment algorithms with different latent dimensionality and discriminator parameterization choices. Our work provides a principled foundation for assessing, analyzing, and developing representation learning techniques in goal-based RL.",1
"In this paper, we propose Multiresolution Graph Networks (MGN) and Multiresolution Graph Variational Autoencoders (MGVAE) to learn and generate graphs in a multiresolution and equivariant manner. At each resolution level, MGN employs higher order message passing to encode the graph while learning to partition it into mutually exclusive clusters and coarsening into a lower resolution. MGVAE constructs a hierarchical generative model based on MGN to variationally autoencode the hierarchy of coarsened graphs. Our proposed framework is end-to-end permutation equivariant with respect to node ordering. Our methods have been successful with several generative tasks including link prediction on citation graphs, unsupervised molecular representation learning to predict molecular properties, molecular generation, general graph generation and graph-based image generation.",0
"The paper introduces the Multiresolution Graph Networks (MGN) and Multiresolution Graph Variational Autoencoders (MGVAE) as novel methods for learning and generating graphs in a multiresolution and equivariant manner. MGN uses higher order message passing to encode the graph and partitions it into mutually exclusive clusters while coarsening it into a lower resolution. MGVAE constructs a hierarchical generative model based on MGN to variationally autoencode the hierarchy of coarsened graphs. The proposed framework is end-to-end permutation equivariant with respect to node ordering and has been successful in various generative tasks such as link prediction, molecular representation learning, molecular and general graph generation, and graph-based image generation.",1
"Time series are often complex and rich in information but sparsely labeled and therefore challenging to model. In this paper, we propose a self-supervised framework for learning generalizable representations for non-stationary time series. Our approach, called Temporal Neighborhood Coding (TNC), takes advantage of the local smoothness of a signal's generative process to define neighborhoods in time with stationary properties. Using a debiased contrastive objective, our framework learns time series representations by ensuring that in the encoding space, the distribution of signals from within a neighborhood is distinguishable from the distribution of non-neighboring signals. Our motivation stems from the medical field, where the ability to model the dynamic nature of time series data is especially valuable for identifying, tracking, and predicting the underlying patients' latent states in settings where labeling data is practically impossible. We compare our method to recently developed unsupervised representation learning approaches and demonstrate superior performance on clustering and classification tasks for multiple datasets.",0
"The complexity and richness of time series data make them difficult to model due to a lack of labeling. Our paper introduces a self-supervised framework, Temporal Neighborhood Coding (TNC), for learning representations that can be applied to non-stationary time series. TNC utilizes the local smoothness of a signal's generative process to define stationary neighborhoods in time. By employing a debiased contrastive objective, our framework learns time series representations that distinguish signals from within a neighborhood from those outside of it. We were motivated to develop this approach by the medical field's need to model time series data to identify, track, and predict underlying patients' latent states. We compare TNC to other unsupervised representation learning approaches and demonstrate its superior performance on clustering and classification tasks across multiple datasets.",1
"Geometric feature extraction is a crucial component of point cloud registration pipelines. Recent work has demonstrated how supervised learning can be leveraged to learn better and more compact 3D features. However, those approaches' reliance on ground-truth annotation limits their scalability. We propose BYOC: a self-supervised approach that learns visual and geometric features from RGB-D video without relying on ground-truth pose or correspondence. Our key observation is that randomly-initialized CNNs readily provide us with good correspondences; allowing us to bootstrap the learning of both visual and geometric features. Our approach combines classic ideas from point cloud registration with more recent representation learning approaches. We evaluate our approach on indoor scene datasets and find that our method outperforms traditional and learned descriptors, while being competitive with current state-of-the-art supervised approaches.",0
"The extraction of geometric features is essential in the process of registering point clouds. Previous studies have shown that supervised learning can be useful in creating better and more concise 3D features. However, these methods are limited in their scalability due to their dependence on ground-truth annotation. We propose a new approach called BYOC, which is self-supervised and can learn visual and geometric features from RGB-D video without relying on pose or correspondence ground-truths. Our approach utilizes randomly-initialized CNNs to establish good correspondences, enabling us to initiate the learning process for both visual and geometric features. We combine traditional ideas from point cloud registration with recent representation learning approaches. Our approach surpasses traditional and learned descriptors, while remaining competitive with current state-of-the-art supervised approaches, as we demonstrate by evaluating it on indoor scene datasets.",1
"We consider the problem of learning a neural network classifier. Under the information bottleneck (IB) principle, we associate with this classification problem a representation learning problem, which we call ""IB learning"". We show that IB learning is, in fact, equivalent to a special class of the quantization problem. The classical results in rate-distortion theory then suggest that IB learning can benefit from a ""vector quantization"" approach, namely, simultaneously learning the representations of multiple input objects. Such an approach assisted with some variational techniques, result in a novel learning framework, ""Aggregated Learning"", for classification with neural network models. In this framework, several objects are jointly classified by a single neural network. The effectiveness of this framework is verified through extensive experiments on standard image recognition and text classification tasks.",0
"Our focus is on the task of training a neural network classifier, which we view through the lens of the information bottleneck (IB) principle. This leads us to consider a related problem, which we refer to as ""IB learning"", where the aim is to learn a representation that can be used for classification. We demonstrate that IB learning can be formulated as a specific type of quantization problem, and we leverage the insights from rate-distortion theory to propose a novel approach called ""Aggregated Learning"". This approach involves jointly classifying multiple objects using a single neural network, aided by variational techniques. We evaluate the effectiveness of our framework on standard image recognition and text classification tasks through extensive experimentation.",1
"This paper presents a novel graph-theoretic deep representation learning method in the framework of multi-label remote sensing (RS) image retrieval problems. The proposed method aims to extract and exploit multi-label co-occurrence relationships associated to each RS image in the archive. To this end, each training image is initially represented with a graph structure that provides region-based image representation combining both local information and the related spatial organization. Unlike the other graph-based methods, the proposed method contains a novel learning strategy to train a deep neural network for automatically predicting a graph structure of each RS image in the archive. This strategy employs a region representation learning loss function to characterize the image content based on its multi-label co-occurrence relationship. Experimental results show the effectiveness of the proposed method for retrieval problems in RS compared to state-of-the-art deep representation learning methods. The code of the proposed method is publicly available at https://git.tu-berlin.de/rsim/GT-DRL-CBIR .",0
"In this paper, a new approach to graph-theoretic deep representation learning is presented for multi-label remote sensing image retrieval. The objective of the proposed method is to identify and utilize multi-label co-occurrence relationships that are associated with each RS image in the archive. The initial step involves representing each training image with a graph structure that combines local information with spatial organization. The proposed method distinguishes itself from other graph-based methods by introducing a unique learning strategy that employs a region representation learning loss function to characterize the image content based on its multi-label co-occurrence relationship. The strategy trains a deep neural network to automatically predict the graph structure of each RS image in the archive. The experimental results demonstrate the effectiveness of the proposed method for retrieval problems in RS compared to state-of-the-art deep representation learning methods. The code for the proposed method is publicly accessible at https://git.tu-berlin.de/rsim/GT-DRL-CBIR.",1
"Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. This work tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems. We propose a simple yet generic representation learning framework, named \emph{Source HypOthesis Transfer} (SHOT). SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.",0
"The main objective of unsupervised domain adaptation (UDA) is to apply the knowledge acquired from a labeled source dataset to resolve similar tasks in a new unlabeled domain. However, previous UDA methods necessitated access to the source data during model adaptation, making them inefficient and risky for decentralized private data. This study addresses a practical scenario where only a trained source model is accessible, and investigates how to effectively exploit such a model without source data to tackle UDA issues. We suggest a straightforward yet universal representation learning framework called ""Source HypOthesis Transfer"" (SHOT). SHOT fixes the hypothesis module (classifier) of the source model and trains the target-specific feature extraction module by utilizing both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. To demonstrate its adaptability, we evaluate SHOT using various adaptation cases such as closed-set, partial-set, and open-set domain adaptation. The experiments show that SHOT delivers state-of-the-art outcomes among several domain adaptation benchmarks.",1
"Clustering is one of the most fundamental tasks in machine learning. Recently, deep clustering has become a major trend in clustering techniques. Representation learning often plays an important role in the effectiveness of deep clustering, and thus can be a principal cause of performance degradation. In this paper, we propose a clustering-friendly representation learning method using instance discrimination and feature decorrelation. Our deep-learning-based representation learning method is motivated by the properties of classical spectral clustering. Instance discrimination learns similarities among data and feature decorrelation removes redundant correlation among features. We utilize an instance discrimination method in which learning individual instance classes leads to learning similarity among instances. Through detailed experiments and examination, we show that the approach can be adapted to learning a latent space for clustering. We design novel softmax-formulated decorrelation constraints for learning. In evaluations of image clustering using CIFAR-10 and ImageNet-10, our method achieves accuracy of 81.5% and 95.4%, respectively. We also show that the softmax-formulated constraints are compatible with various neural networks.",0
"Machine learning relies heavily on clustering, a fundamental task. Deep clustering has gained popularity in recent times. However, the effectiveness of deep clustering is heavily dependent on representation learning, which can also lead to performance degradation. To tackle this issue, we propose a method for clustering-friendly representation learning that employs instance discrimination and feature decorrelation. Our approach draws inspiration from classical spectral clustering. The instance discrimination method we use learns similarities between data points, whereas feature decorrelation eliminates redundant correlation among features. Our method can be adapted to learning a latent space for clustering, as demonstrated through experiments on CIFAR-10 and ImageNet-10 datasets, achieving accuracy rates of 81.5% and 95.4%, respectively. We also show that the softmax-formulated constraints we designed for learning feature decorrelation are compatible with various neural networks.",1
"Recent papers on the theory of representation learning has shown the importance of a quantity called diversity when generalizing from a set of source tasks to a target task. Most of these papers assume that the function mapping shared representations to predictions is linear, for both source and target tasks. In practice, researchers in deep learning use different numbers of extra layers following the pretrained model based on the difficulty of the new task. This motivates us to ask whether diversity can be achieved when source tasks and the target task use different prediction function spaces beyond linear functions. We show that diversity holds even if the target task uses a neural network with multiple layers, as long as source tasks use linear functions. If source tasks use nonlinear prediction functions, we provide a negative result by showing that depth-1 neural networks with ReLu activation function need exponentially many source tasks to achieve diversity. For a general function class, we find that eluder dimension gives a lower bound on the number of tasks required for diversity. Our theoretical results imply that simpler tasks generalize better. Though our theoretical results are shown for the global minimizer of empirical risks, their qualitative predictions still hold true for gradient-based optimization algorithms as verified by our simulations on deep neural networks.",0
"Recent research on representation learning theory has highlighted the significance of diversity in generalizing from a group of source tasks to a target task. These studies typically assume that the function linking shared representations to predictions is linear for both the source and target tasks. However, in practical deep learning applications, researchers use varying numbers of extra layers following the pre-trained model, depending on the difficulty of the new task. This has prompted us to investigate whether diversity can be achieved when the prediction function spaces differ beyond linear functions. Our findings reveal that diversity remains achievable even if the target task uses a neural network with multiple layers, as long as the source tasks use linear functions. Nevertheless, we provide negative results if the source tasks use nonlinear prediction functions, demonstrating that depth-1 neural networks with ReLu activation function require an exponentially large number of source tasks to achieve diversity. Furthermore, for a general function class, we establish that the eluder dimension provides a lower bound on the number of tasks necessary for diversity. These theoretical results suggest that simpler tasks generalize better, and our simulations on deep neural networks confirm their qualitative predictions, even for gradient-based optimization algorithms.",1
"Graph pooling that summaries the information in a large graph into a compact form is essential in hierarchical graph representation learning. Existing graph pooling methods either suffer from high computational complexity or cannot capture the global dependencies between graphs before and after pooling. To address the problems of existing graph pooling methods, we propose Coarsened Graph Infomax Pooling (CGIPool) that maximizes the mutual information between the input and the coarsened graph of each pooling layer to preserve graph-level dependencies. To achieve mutual information neural maximization, we apply contrastive learning and propose a self-attention-based algorithm for learning positive and negative samples. Extensive experimental results on seven datasets illustrate the superiority of CGIPool comparing to the state-of-the-art methods.",0
"Hierarchical graph representation learning requires the use of graph pooling to condense large amounts of information into a more concise format. However, current graph pooling methods either have high computational demands or fail to capture global dependencies between graphs before and after pooling. To improve upon these methods, we present Coarsened Graph Infomax Pooling (CGIPool), which maximizes mutual information between input and coarsened graphs for each pooling layer to maintain graph-level dependencies. We accomplish this by utilizing contrastive learning and proposing a self-attention-based algorithm for learning positive and negative samples. Through extensive experimentation on seven datasets, we demonstrate that CGIPool outperforms state-of-the-art methods.",1
"Goal-conditioned hierarchical reinforcement learning (HRL) serves as a successful approach to solving complex and temporally extended tasks. Recently, its success has been extended to more general settings by concurrently learning hierarchical policies and subgoal representations. However, online subgoal representation learning exacerbates the non-stationary issue of HRL and introduces challenges for exploration in high-level policy learning. In this paper, we propose a state-specific regularization that stabilizes subgoal embeddings in well-explored areas while allowing representation updates in less explored state regions. Benefiting from this stable representation, we design measures of novelty and potential for subgoals, and develop an efficient hierarchical exploration strategy that actively seeks out new promising subgoals and states. Experimental results show that our method significantly outperforms state-of-the-art baselines in continuous control tasks with sparse rewards and further demonstrate the stability and efficiency of the subgoal representation learning of this work, which promotes superior policy learning.",0
"Goal-conditioned hierarchical reinforcement learning (HRL) has proven to be effective in solving complex and extended tasks. Recent advancements have expanded its success to more general settings by simultaneously learning hierarchical policies and subgoal representations. However, online subgoal representation learning presents challenges for exploration in high-level policy learning and exacerbates the non-stationary issue of HRL. To address this, we propose a state-specific regularization that stabilizes subgoal embeddings in well-explored areas while allowing representation updates in less explored state regions. With this stable representation, we develop measures of novelty and potential for subgoals and an efficient hierarchical exploration strategy that actively seeks out new promising subgoals and states. Our experimental results demonstrate that our method outperforms state-of-the-art baselines in continuous control tasks with sparse rewards. Additionally, our work highlights the stability and efficiency of subgoal representation learning, which promotes superior policy learning.",1
"Few-shot object detection has made substantial progressby representing novel class objects using the feature representation learned upon a set of base class objects. However,an implicit contradiction between novel class classification and representation is unfortunately ignored. On the one hand, to achieve accurate novel class classification, the distributions of either two base classes must be far away fromeach other (max-margin). On the other hand, to precisely represent novel classes, the distributions of base classes should be close to each other to reduce the intra-class distance of novel classes (min-margin). In this paper, we propose a class margin equilibrium (CME) approach, with the aim to optimize both feature space partition and novel class reconstruction in a systematic way. CME first converts the few-shot detection problem to the few-shot classification problem by using a fully connected layer to decouple localization features. CME then reserves adequate margin space for novel classes by introducing simple-yet-effective class margin loss during feature learning. Finally, CME pursues margin equilibrium by disturbing the features of novel class instances in an adversarial min-max fashion. Experiments on Pascal VOC and MS-COCO datasets show that CME significantly improves upon two baseline detectors (up to $3\sim 5\%$ in average), achieving state-of-the-art performance. Code is available at https://github.com/Bohao-Lee/CME .",0
"Substantial advancements have been made in few-shot object detection by utilizing the feature representation learned from a set of base class objects to represent novel class objects. However, there is an implicit contradiction between novel class classification and representation that has been overlooked. To achieve accurate novel class classification, the distributions of two base classes must be far apart, while to precisely represent novel classes, the distributions of base classes should be close to each other to reduce the intra-class distance of novel classes. In this study, we propose a systematic approach called class margin equilibrium (CME) to optimize both feature space partition and novel class reconstruction. CME first converts the few-shot detection problem to the few-shot classification problem by utilizing a fully connected layer to decouple localization features. CME then introduces a simple yet effective class margin loss during feature learning to reserve adequate margin space for novel classes. Finally, CME pursues margin equilibrium by perturbing the features of novel class instances in an adversarial min-max fashion. Our experiments on the Pascal VOC and MS-COCO datasets show that CME significantly improves upon two baseline detectors (up to $3\sim 5\%$ in average) and achieves state-of-the-art performance. The code for CME is available at https://github.com/Bohao-Lee/CME.",1
"Recent works show that mean-teaching is an effective framework for unsupervised domain adaptive person re-identification. However, existing methods perform contrastive learning on selected samples between teacher and student networks, which is sensitive to noises in pseudo labels and neglects the relationship among most samples. Moreover, these methods are not effective in cooperation of different teacher networks. To handle these issues, this paper proposes a Graph Consistency based Mean-Teaching (GCMT) method with constructing the Graph Consistency Constraint (GCC) between teacher and student networks. Specifically, given unlabeled training images, we apply teacher networks to extract corresponding features and further construct a teacher graph for each teacher network to describe the similarity relationships among training images. To boost the representation learning, different teacher graphs are fused to provide the supervise signal for optimizing student networks. GCMT fuses similarity relationships predicted by different teacher networks as supervision and effectively optimizes student networks with more sample relationships involved. Experiments on three datasets, i.e., Market-1501, DukeMTMCreID, and MSMT17, show that proposed GCMT outperforms state-of-the-art methods by clear margin. Specially, GCMT even outperforms the previous method that uses a deeper backbone. Experimental results also show that GCMT can effectively boost the performance with multiple teacher and student networks. Our code is available at https://github.com/liu-xb/GCMT .",0
"Recent research has demonstrated the effectiveness of mean-teaching for unsupervised domain adaptive person re-identification. However, current methods that use contrastive learning on selected samples between teacher and student networks are sensitive to noise in pseudo labels and neglect the relationship among most samples. Additionally, these methods do not cooperate effectively with different teacher networks. This paper proposes a new method, the Graph Consistency based Mean-Teaching (GCMT) approach, which constructs the Graph Consistency Constraint (GCC) between teacher and student networks to address these issues. With this method, teacher networks extract features from unlabeled training images and construct a teacher graph to describe the similarity relationships among the training images. The GCMT method fuses similarity relationships predicted by different teacher networks to optimize student networks. Experiments on three datasets show that GCMT outperforms state-of-the-art methods and can effectively boost performance with multiple teacher and student networks. The code for GCMT is available at https://github.com/liu-xb/GCMT.",1
"Eye movements are intricate and dynamic biosignals that contain a wealth of cognitive information about the subject. However, these are ambiguous signals and therefore require meticulous feature engineering to be used by machine learning algorithms. We instead propose to learn feature vectors of eye movements in a self-supervised manner. We adopt a contrastive learning approach and propose a set of data transformations that encourage a deep neural network to discern salient and granular gaze patterns. This paper presents a novel experiment utilizing six eye-tracking data sets despite different data specifications and experimental conditions. We assess the learned features on biometric tasks with only a linear classifier, achieving 84.6% accuracy on a mixed dataset, and up to 97.3% accuracy on a single dataset. Our work advances the state of machine learning for eye movements and provides insights into a general representation learning method not only for eye movements but also for similar biosignals.",0
"Eye movements can provide valuable cognitive information about a subject; however, these signals can be difficult to interpret and therefore require thorough feature engineering for use in machine learning algorithms. Instead, we propose a self-supervised approach to learn feature vectors of eye movements. Our method involves using a contrastive learning approach and a set of data transformations that help a deep neural network identify significant and detailed gaze patterns. In this paper, we present an experiment that utilizes six eye-tracking datasets with varying specifications and experimental conditions. We evaluate the learned features on biometric tasks with a linear classifier and achieve up to 97.3% accuracy on a single dataset and 84.6% accuracy on a mixed dataset. Our research advances the field of machine learning for eye movements and provides insight into general representation learning methods for similar biosignals.",1
"A crucial aspect in reliable machine learning is to design a deployable system in generalizing new related but unobserved environments. Domain generalization aims to alleviate such a prediction gap between the observed and unseen environments. Previous approaches commonly incorporated learning invariant representation for achieving good empirical performance. In this paper, we reveal that merely learning invariant representation is vulnerable to the unseen environment. To this end, we derive novel theoretical analysis to control the unseen test environment error in the representation learning, which highlights the importance of controlling the smoothness of representation. In practice, our analysis further inspires an efficient regularization method to improve the robustness in domain generalization. Our regularization is orthogonal to and can be straightforwardly adopted in existing domain generalization algorithms for invariant representation learning. Empirical results show that our algorithm outperforms the base versions in various dataset and invariance criteria.",0
"Creating a deployable system that can generalize to new but related environments is a crucial aspect of reliable machine learning. Domain generalization is aimed at reducing the gap in prediction accuracy between observed and unseen environments. Previous approaches have used learning invariant representation to achieve good performance, but we have found that this method is vulnerable to the unseen environment. Therefore, we have developed a novel theoretical analysis that controls the representation learning error in the unseen test environment by emphasizing the importance of maintaining smooth representation. In practice, our analysis has inspired an efficient regularization method that enhances the robustness of domain generalization. This regularization can be easily integrated into existing invariant representation learning algorithms. The empirical results demonstrate that our algorithm outperforms the base versions across various datasets and invariance criteria.",1
"Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GATs can only compute a restricted kind of attention where the ranking of attended nodes is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_gats .",0
"The state-of-the-art architecture for representation learning with graphs is considered to be Graph Attention Networks (GATs), which are a popular type of GNN architecture. GATs allow each node to attend to its neighbors based on its own representation as the query. However, in this study, we demonstrate that GATs are only capable of computing a limited form of attention, known as static attention, where the rank of attended nodes is not conditioned on the query node. We distinguish static attention from dynamic attention, which is more expressive. Due to the use of a static attention mechanism, GATs are unable to express simple graph problems. In a controlled problem, we demonstrate that static attention hinders GATs from fitting the training data. To address this issue, we propose GATv2, a dynamic graph attention variant that is more expressive than GAT. By modifying the order of operations, we introduce a simple fix that removes this limitation. We conduct an extensive evaluation and demonstrate that GATv2 outperforms GAT across 11 OGB and other benchmarks while maintaining the same parametric costs. Our code is publicly available at https://github.com/tech-srl/how_attentive_are_gats.",1
"In Graph Neural Networks (GNNs), the embedding of each node is obtained by aggregating information with its direct and indirect neighbors. As the messages passed among nodes contain both information and noise, the critical issue in GNN representation learning is how to retrieve information effectively while suppressing noise. Generally speaking, interactions with distant nodes usually introduce more noise for a particular node than those with close nodes. However, in most existing works, the messages being passed among nodes are mingled together, which is inefficient from a communication perspective. Mixing the information from clean sources (low-order neighbors) and noisy sources (high-order neighbors) makes discriminative feature extraction challenging. Motivated by the above, we propose a simple yet effective ladder-style GNN architecture, namely LADDER-GNN. Specifically, we separate messages from different hops and assign different dimensions for them before concatenating them to obtain the node representation. Such disentangled representations facilitate extracting information from messages passed from different hops, and their corresponding dimensions are determined with a reinforcement learning-based neural architecture search strategy. The resulted hop-aware representations generally contain more dimensions for low-order neighbors and fewer dimensions for high-order neighbors, leading to a ladder-style aggregation scheme. We verify the proposed LADDER-GNN on several semi-supervised node classification datasets. Experimental results show that the proposed simple hop-aware representation learning solution can achieve state-of-the-art performance on most datasets.",0
"The process of obtaining the embedding of each node in Graph Neural Networks (GNNs) involves gathering information from its direct and indirect neighbors. However, this information contains both useful data and noise, making it challenging to retrieve valuable information while suppressing noise. Typically, distant nodes introduce more noise than closer ones, yet most current methods mix the messages being passed among nodes, which is an ineffective communication strategy. This blending of information from both clean (low-order neighbors) and noisy (high-order neighbors) sources makes it difficult to extract discriminative features. To address this issue, we introduce LADDER-GNN, a simple yet effective GNN architecture that disentangles messages from different hops, assigning them different dimensions before concatenating them to obtain the node representation. This approach facilitates the extraction of information from messages passed from different hops, with reinforcement learning-based neural architecture search strategy determining the corresponding dimensions. The resulting hop-aware representations contain more dimensions for low-order neighbors and fewer dimensions for high-order neighbors, resulting in a ladder-style aggregation scheme. We evaluate LADDER-GNN on various semi-supervised node classification datasets, where it achieves state-of-the-art performance, demonstrating its effectiveness as a hop-aware representation learning solution.",1
"Transformers have recently gained increasing attention in computer vision. However, existing studies mostly use Transformers for feature representation learning, e.g. for image classification and dense predictions. In this work, we further investigate the possibility of applying Transformers for image matching and metric learning given pairs of images. We find that the Vision Transformer (ViT) and the vanilla Transformer with decoders are not adequate for image matching due to their lack of image-to-image attention. Thus, we further design two naive solutions, i.e. query-gallery concatenation in ViT, and query-gallery cross-attention in the vanilla Transformer. The latter improves the performance, but it is still limited. This implies that the attention mechanism in Transformers is primarily designed for global feature aggregation, which is not naturally suitable for image matching. Accordingly, we propose a new simplified decoder, which drops the full attention implementation with the softmax weighting, keeping only the query-key similarity computation. Additionally, global max pooling and a multilayer perceptron (MLP) head are applied to decode the matching result. This way, the simplified decoder is computationally more efficient, while at the same time more effective for image matching. The proposed method, called TransMatcher, achieves state-of-the-art performance in generalizable person re-identification, with up to 6.1% and 5.7% performance gains in Rank-1 and mAP, respectively, on several popular datasets. The source code of this study will be made publicly available.",0
"The use of Transformers in computer vision has recently become a popular topic of study. However, current research primarily focuses on using Transformers to learn feature representations for tasks like image classification and dense predictions. In this study, we explore the potential of Transformers for image matching and metric learning when given pairs of images. We discovered that the Vision Transformer and vanilla Transformer with decoders were not suitable for image matching due to their lack of image-to-image attention. We attempted to solve this problem by creating two naive solutions, namely query-gallery concatenation in ViT and query-gallery cross-attention in the vanilla Transformer. Although the latter improved performance, it was still limited. This indicates that the attention mechanism in Transformers is mainly designed for global feature aggregation, which is not ideal for image matching. Therefore, we propose a new simplified decoder that drops the full attention implementation with softmax weighting in favor of only the query-key similarity computation. We also incorporate global max pooling and a multilayer perceptron head to decode the matching result. This simplified decoder is more computationally efficient and more effective for image matching. Our proposed method, TransMatcher, achieves state-of-the-art performance in generalizable person re-identification, with up to 6.1% and 5.7% performance gains in Rank-1 and mAP, respectively, on several popular datasets. We will make the source code for this study publicly available.",1
"Infant motion analysis is a topic with critical importance in early childhood development studies. However, while the applications of human pose estimation have become more and more broad, models trained on large-scale adult pose datasets are barely successful in estimating infant poses due to the significant differences in their body ratio and the versatility of their poses. Moreover, the privacy and security considerations hinder the availability of adequate infant pose data required for training of a robust model from scratch. To address this problem, this paper presents (1) building and publicly releasing a hybrid synthetic and real infant pose (SyRIP) dataset with small yet diverse real infant images as well as generated synthetic infant poses and (2) a multi-stage invariant representation learning strategy that could transfer the knowledge from the adjacent domains of adult poses and synthetic infant images into our fine-tuned domain-adapted infant pose (FiDIP) estimation model. In our ablation study, with identical network structure, models trained on SyRIP dataset show noticeable improvement over the ones trained on the only other public infant pose datasets. Integrated with pose estimation backbone networks with varying complexity, FiDIP performs consistently better than the fine-tuned versions of those models. One of our best infant pose estimation performers on the state-of-the-art DarkPose model shows mean average precision (mAP) of 93.6.",0
"The analysis of infant motion is a critical topic in early childhood development research. However, models trained on large adult pose datasets struggle to accurately estimate infant poses due to differences in body ratio and pose variability. Additionally, privacy concerns make it difficult to obtain adequate infant pose data for robust model training. To address this issue, this paper introduces a hybrid synthetic and real infant pose dataset (SyRIP) and a multi-stage invariant representation learning strategy to transfer knowledge from adjacent domains into a domain-adapted infant pose estimation model (FiDIP). Models trained on SyRIP outperform those trained on other public infant pose datasets, and FiDIP, integrated with varying complexity pose estimation backbone networks, consistently performs better than fine-tuned versions of those models. The best FiDIP performer on the DarkPose model achieves a mean average precision (mAP) of 93.6.",1
"Deep clustering has the potential to learn a strong representation and hence better clustering performance compared to traditional clustering methods such as $k$-means and spectral clustering. However, this strong representation learning ability may make the clustering unfair by discovering surrogates for protected information which we empirically show in our experiments. In this work, we study a general notion of group-level fairness for both binary and multi-state protected status variables (PSVs). We begin by formulating the group-level fairness problem as an integer linear programming formulation whose totally unimodular constraint matrix means it can be efficiently solved via linear programming. We then show how to inject this solver into a discriminative deep clustering backbone and hence propose a refinement learning algorithm to combine the clustering goal with the fairness objective to learn fair clusters adaptively. Experimental results on real-world datasets demonstrate that our model consistently outperforms state-of-the-art fair clustering algorithms. Our framework shows promising results for novel clustering tasks including flexible fairness constraints, multi-state PSVs and predictive clustering.",0
"Compared to traditional clustering methods like $k$-means and spectral clustering, deep clustering has the potential to learn a more robust representation, leading to improved clustering performance. However, this strength in representation learning may result in unfair clustering by discovering surrogates for protected information, as we demonstrate in our experiments. Our work addresses the concept of group-level fairness for binary and multi-state protected status variables (PSVs). We propose an integer linear programming formulation for the group-level fairness problem, which can be efficiently solved using linear programming. We integrate this solver into a discriminative deep clustering backbone, creating a refinement learning algorithm that combines the clustering goal with the fairness objective to learn fair clusters adaptively. Our model consistently outperforms state-of-the-art fair clustering algorithms on real-world datasets, and our framework can be applied to novel clustering tasks that involve flexible fairness constraints, multi-state PSVs, and predictive clustering.",1
"Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.",0
"The learning capabilities of Multilayer-perceptrons (MLP) are limited when it comes to functions with high-frequency and wide frequency bands. However, a solution has been developed in the form of a spatially adaptive progressive encoding (SAPE) scheme for MLP network input signals. This approach allows the MLP network to better accommodate a wide range of frequencies without compromising training stability or necessitating domain-specific preprocessing. The SAPE technique gradually reveals signal components with increasing frequencies over time and space. A feedback loop monitors the progressive frequency exposure during the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. The advantages of SAPE are demonstrated in various domains and applications, including low-dimensional signal and image regression, occupancy network representation learning, and a geometric task involving mesh transfer between 3D shapes.",1
"Unsupervised/self-supervised pre-training methods for graph representation learning have recently attracted increasing research interests, and they are shown to be able to generalize to various downstream applications. Yet, the adversarial robustness of such pre-trained graph learning models remains largely unexplored. More importantly, most existing defense techniques designed for end-to-end graph representation learning methods require pre-specified label definitions, and thus cannot be directly applied to the pre-training methods. In this paper, we propose an unsupervised defense technique to robustify pre-trained deep graph models, so that the perturbations on the input graph can be successfully identified and blocked before the model is applied to different downstream tasks. Specifically, we introduce a mutual information-based measure, \textit{graph representation vulnerability (GRV)}, to quantify the robustness of graph encoders on the representation space. We then formulate an optimization problem to learn the graph representation by carefully balancing the trade-off between the expressive power and the robustness (\emph{i.e.}, GRV) of the graph encoder. The discrete nature of graph topology and the joint space of graph data make the optimization problem intractable to solve. To handle the above difficulty and to reduce computational expense, we further relax the problem and thus provide an approximate solution. Additionally, we explore a provable connection between the robustness of the unsupervised graph encoder and that of models on downstream tasks. Extensive experiments demonstrate that even without access to labels and tasks, our model is still able to enhance robustness against adversarial attacks on three downstream tasks (node classification, link prediction, and community detection) by an average of +16.5% compared with existing methods.",0
"Recently, there has been a growing interest in unsupervised/self-supervised pre-training methods for graph representation learning, which have demonstrated the ability to generalize to various downstream applications. However, the adversarial robustness of these pre-trained graph learning models has not been thoroughly explored. Furthermore, existing defense techniques for end-to-end graph representation learning methods require pre-specified label definitions, which cannot be directly applied to pre-training methods. This paper proposes an unsupervised defense technique to improve the robustness of pre-trained deep graph models. A measure called ""graph representation vulnerability (GRV)"" is introduced to quantify the robustness of graph encoders on the representation space. An optimization problem is formulated to balance the expressive power and the robustness (GRV) of the graph encoder. Due to the intractability of the optimization problem, a relaxed solution is provided to reduce computational expense. Additionally, a provable connection between the robustness of the unsupervised graph encoder and that of models on downstream tasks is explored. Extensive experiments show that the proposed model enhances the robustness against adversarial attacks on three downstream tasks (node classification, link prediction, and community detection) by an average of +16.5% compared to existing methods, even without access to labels and tasks.",1
"This study introduces using measure theoretic basis the notion of membership-mapping for representing data points through attribute values (motivated by fuzzy theory). A property of the membership-mapping, that can be exploited for data representation learning, is of providing an interpolation on the given data points in the data space. The study outlines an analytical approach to the variational learning of a membership-mappings based data representation model. An alternative idea of deep autoencoder, referred to as Bregman Divergence Based Conditionally Deep Autoencoder (that consists of layers such that each layer learns data representation at certain abstraction level through a membership-mappings based autoencoder), is presented. Experiments are provided to demonstrate the competitive performance of the proposed framework in classifying high-dimensional feature vectors and in rendering robustness to the classification.",0
"In this study, the concept of membership-mapping is introduced for representing data points using attribute values, based on measure theoretic principles and inspired by fuzzy theory. The membership-mapping has the advantage of providing interpolation on the data points in the data space, which can be utilized for data representation learning. The study presents an analytical approach to variational learning of a membership-mappings based data representation model. Additionally, a new approach called Bregman Divergence Based Conditionally Deep Autoencoder is proposed, which consists of layers that learn data representation at different abstraction levels using membership-mappings based autoencoder. The proposed framework is evaluated through experiments and shown to perform competitively in classifying high-dimensional feature vectors and improving robustness of the classification.",1
"As GAN-based video and image manipulation technologies become more sophisticated and easily accessible, there is an urgent need for effective deepfake detection technologies. Moreover, various deepfake generation techniques have emerged over the past few years. While many deepfake detection methods have been proposed, their performance suffers from new types of deepfake methods on which they are not sufficiently trained. To detect new types of deepfakes, the model should learn from additional data without losing its prior knowledge about deepfakes (catastrophic forgetting), especially when new deepfakes are significantly different. In this work, we employ the Representation Learning (ReL) and Knowledge Distillation (KD) paradigms to introduce a transfer learning-based Feature Representation Transfer Adaptation Learning (FReTAL) method. We use FReTAL to perform domain adaptation tasks on new deepfake datasets while minimizing catastrophic forgetting. Our student model can quickly adapt to new types of deepfake by distilling knowledge from a pre-trained teacher model and applying transfer learning without using source domain data during domain adaptation. Through experiments on FaceForensics++ datasets, we demonstrate that FReTAL outperforms all baselines on the domain adaptation task with up to 86.97% accuracy on low-quality deepfakes.",0
"Due to the increasing sophistication and accessibility of GAN-based video and image manipulation technology, there is a pressing need for effective deepfake detection methods. However, with the emergence of various deepfake generation techniques, many proposed detection methods suffer from insufficient training on new forms of deepfake. To detect new types of deepfakes, it is necessary for the model to learn from additional data without losing its prior knowledge (catastrophic forgetting), particularly when new deepfakes are significantly dissimilar. To address this issue, we propose a Feature Representation Transfer Adaptation Learning (FReTAL) method that employs Representation Learning (ReL) and Knowledge Distillation (KD) paradigms for transfer learning. Our FReTAL method can adapt to new deepfake datasets quickly and without using source domain data during adaptation, while minimizing catastrophic forgetting. Through experiments on the FaceForensics++ datasets, we demonstrate that FReTAL outperforms all baselines on the domain adaptation task, achieving up to 86.97% accuracy on low-quality deepfakes.",1
"The objective of Open set recognition (OSR) is to learn a classifier that can reject the unknown samples while classifying the known classes accurately. In this paper, we propose a self-supervision method, Detransformation Autoencoder (DTAE), for the OSR problem. This proposed method engages in learning representations that are invariant to the transformations of the input data. Experiments on several standard image datasets indicate that the pre-training process significantly improves the model performance in the OSR tasks. Meanwhile, our proposed self-supervision method achieves significant gains in detecting the unknown class and classifying the known classes. Moreover, our analysis indicates that DTAE can yield representations that contain more target class information and less transformation information than RotNet.",0
"The aim of Open set recognition (OSR) is to create a classifier that can accurately classify known classes while rejecting unknown samples. Our paper introduces a self-supervision technique called Detransformation Autoencoder (DTAE) for OSR. DTAE learns representations that are invariant to input data transformations. Standard image datasets experiments demonstrate that pre-training improves model performance in OSR tasks. Furthermore, our self-supervision method detects unknown classes and classifies known classes with significant improvements. Our analysis suggests that DTAE yields representations with more target class information and less transformation information than RotNet.",1
"Agents trained by reinforcement learning (RL) often fail to generalize beyond the environment they were trained in, even when presented with new scenarios that seem similar to the training environment. We study the query complexity required to train RL agents that generalize to multiple environments. Intuitively, tractable generalization is only possible when the environments are similar or close in some sense. To capture this, we introduce Weak Proximity, a natural structural condition that requires the environments to have highly similar transition and reward functions and share a policy providing optimal value. Despite such shared structure, we prove that tractable generalization is impossible in the worst case. This holds even when each individual environment can be efficiently solved to obtain an optimal linear policy, and when the agent possesses a generative model. Our lower bound applies to the more complex task of representation learning for the purpose of efficient generalization to multiple environments. On the positive side, we introduce Strong Proximity, a strengthened condition which we prove is sufficient for efficient generalization.",0
"Reinforcement learning (RL) agents often struggle to apply their training to new scenarios, even if they appear similar to the training environment. To address this issue, we investigate the amount of query complexity necessary to train RL agents that can generalize across multiple environments. Our research suggests that generalization is feasible only if the environments are similar or proximate to one another. To this end, we propose Weak Proximity as a condition that mandates the environments have similar transition and reward functions and share a policy that offers optimal value. However, we establish that even with such shared structure, generalization is not always possible in the worst-case scenario. Our lower bound applies to the more intricate task of representation learning that aims to enable efficient generalization across multiple environments. On a positive note, we introduce Strong Proximity, a more stringent condition that guarantees efficient generalization.",1
"With the increasing popularity of graph-based methods for dimensionality reduction and representation learning, node embedding functions have become important objects of study in the literature. In this paper, we take an axiomatic approach to understanding node embedding methods, first stating three properties for embedding dissimilarity networks, then proving that all three cannot be satisfied simultaneously by any node embedding method. Similar to existing results on the impossibility of clustering under certain axiomatic assumptions, this points to fundamental difficulties inherent to node embedding tasks. Once these difficulties are identified, we then relax these axioms to allow for certain node embedding methods to be admissible in our framework.",0
"Node embedding functions have gained significant attention in the literature due to the rising popularity of graph-based techniques for dimensionality reduction and representation learning. This paper adopts an axiomatic approach to comprehending node embedding methods by outlining three properties for embedding dissimilarity networks and subsequently demonstrating that no node embedding method can simultaneously fulfill all three properties. The findings are comparable to prior research on the impracticality of clustering with specific axiomatic assumptions, highlighting the inherent challenges of node embedding tasks. After identifying these difficulties, the axioms are relaxed to enable some node embedding methods to be accepted in our framework.",1
"Self-attention has been successfully applied to video representation learning due to the effectiveness of modeling long range dependencies. Existing approaches build the dependencies merely by computing the pairwise correlations along spatial and temporal dimensions simultaneously. However, spatial correlations and temporal correlations represent different contextual information of scenes and temporal reasoning. Intuitively, learning spatial contextual information first will benefit temporal modeling. In this paper, we propose a separable self-attention (SSA) module, which models spatial and temporal correlations sequentially, so that spatial contexts can be efficiently used in temporal modeling. By adding SSA module into 2D CNN, we build a SSA network (SSAN) for video representation learning. On the task of video action recognition, our approach outperforms state-of-the-art methods on Something-Something and Kinetics-400 datasets. Our models often outperform counterparts with shallower network and fewer modalities. We further verify the semantic learning ability of our method in visual-language task of video retrieval, which showcases the homogeneity of video representations and text embeddings. On MSR-VTT and Youcook2 datasets, video representations learnt by SSA significantly improve the state-of-the-art performance.",0
"Due to its efficacy in modeling long range dependencies, self-attention has been successfully employed in video representation learning. However, current methods simply compute pairwise correlations along both spatial and temporal dimensions, which fails to capture the distinct contextual information of spatial and temporal correlations. To address this, we propose a new approach called separable self-attention (SSA) that models spatial and temporal correlations sequentially. By doing so, spatial context can be efficiently used in temporal modeling. We integrate the SSA module into a 2D CNN to create a SSA network (SSAN) for video representation learning. Our approach achieves superior performance on the Something-Something and Kinetics-400 datasets, even outperforming shallower networks with fewer modalities. We also demonstrate the semantic learning ability of our method in a visual-language task of video retrieval, which shows the consistency of video representations and text embeddings. Additionally, our method significantly improves the state-of-the-art performance on the MSR-VTT and Youcook2 datasets.",1
"In this work, we propose CARLS, a novel framework for augmenting the capacity of existing deep learning frameworks by enabling multiple components -- model trainers, knowledge makers and knowledge banks -- to concertedly work together in an asynchronous fashion across hardware platforms. The proposed CARLS is particularly suitable for learning paradigms where model training benefits from additional knowledge inferred or discovered during training, such as node embeddings for graph neural networks or reliable pseudo labels from model predictions. We also describe three learning paradigms -- semi-supervised learning, curriculum learning and multimodal learning -- as examples that can be scaled up efficiently by CARLS. One version of CARLS has been open-sourced and available for download at: https://github.com/tensorflow/neural-structured-learning/tree/master/research/carls",0
"Our work introduces a new framework called CARLS, which enhances the capabilities of existing deep learning frameworks by enabling multiple components to work together asynchronously across different hardware platforms. This framework is particularly useful for learning paradigms that require additional knowledge during training, such as node embeddings for graph neural networks or reliable pseudo labels from model predictions. We provide examples of three learning paradigms, including semi-supervised learning, curriculum learning, and multimodal learning, that can be efficiently scaled up using CARLS. We have also made one version of CARLS available for download at the following link: https://github.com/tensorflow/neural-structured-learning/tree/master/research/carls.",1
"Evaluating the quality of learned representations without relying on a downstream task remains one of the challenges in representation learning. In this work, we present Geometric Component Analysis (GeomCA) algorithm that evaluates representation spaces based on their geometric and topological properties. GeomCA can be applied to representations of any dimension, independently of the model that generated them. We demonstrate its applicability by analyzing representations obtained from a variety of scenarios, such as contrastive learning models, generative models and supervised learning models.",0
"One of the challenges in representation learning is to assess the quality of learned representations without depending on a downstream task. This study introduces the Geometric Component Analysis (GeomCA) algorithm, which evaluates representation spaces by considering their geometric and topological attributes. GeomCA can be applied to representations of any dimension, regardless of the model used to generate them. The effectiveness of GeomCA is demonstrated by analyzing representations obtained from diverse scenarios, including contrastive learning models, generative models, and supervised learning models.",1
"In imitation learning, it is common to learn a behavior policy to match an unknown target policy via max-likelihood training on a collected set of target demonstrations. In this work, we consider using offline experience datasets - potentially far from the target distribution - to learn low-dimensional state representations that provably accelerate the sample-efficiency of downstream imitation learning. A central challenge in this setting is that the unknown target policy itself may not exhibit low-dimensional behavior, and so there is a potential for the representation learning objective to alias states in which the target policy acts differently. Circumventing this challenge, we derive a representation learning objective which provides an upper bound on the performance difference between the target policy and a lowdimensional policy trained with max-likelihood, and this bound is tight regardless of whether the target policy itself exhibits low-dimensional structure. Moving to the practicality of our method, we show that our objective can be implemented as contrastive learning, in which the transition dynamics are approximated by either an implicit energy-based model or, in some special cases, an implicit linear model with representations given by random Fourier features. Experiments on both tabular environments and high-dimensional Atari games provide quantitative evidence for the practical benefits of our proposed objective.",0
"When using imitation learning, it's typical to use max-likelihood training on a set of target demonstrations to learn a behavior policy that matches an unknown target policy. This study explores the use of offline experience datasets, which may not match the target distribution, to learn low-dimensional state representations that can speed up downstream imitation learning. However, the challenge is that the unknown target policy may not have low-dimensional behavior, which can lead to the representation learning objective aliasing states where the target policy acts differently. To address this challenge, the study derives a representation learning objective that provides an upper bound on the performance difference between the target policy and a low-dimensional policy trained with max-likelihood. This bound is tight regardless of whether the target policy itself exhibits low-dimensional structure. The study shows that the objective can be implemented as contrastive learning, using either an implicit energy-based model or, in some cases, an implicit linear model with representations given by random Fourier features. Experiments on both tabular environments and high-dimensional Atari games provide quantitative evidence for the practical benefits of the proposed objective.",1
"Assembly modeling is a core task of computer aided design (CAD), comprising around one third of the work in a CAD workflow. Optimizing this process therefore represents a huge opportunity in the design of a CAD system, but current research of assembly based modeling is not directly applicable to modern CAD systems because it eschews the dominant data structure of modern CAD: parametric boundary representations (BREPs). CAD assembly modeling defines assemblies as a system of pairwise constraints, called mates, between parts, which are defined relative to BREP topology rather than in world coordinates common to existing work. We propose SB-GCN, a representation learning scheme on BREPs that retains the topological structure of parts, and use these learned representations to predict CAD type mates. To train our system, we compiled the first large scale dataset of BREP CAD assemblies, which we are releasing along with benchmark mate prediction tasks. Finally, we demonstrate the compatibility of our model with an existing commercial CAD system by building a tool that assists users in mate creation by suggesting mate completions, with 72.2% accuracy.",0
"Around one third of the work in a CAD workflow involves assembly modeling, which is a fundamental task in computer aided design (CAD). As such, optimizing this process presents a significant opportunity for CAD system design. However, current assembly modeling research is not directly applicable to modern CAD systems because it does not use the dominant data structure of modern CAD, parametric boundary representations (BREPs). In CAD assembly modeling, assemblies are defined as a system of pairwise constraints, called mates, between parts, which are defined relative to BREP topology rather than in world coordinates. We propose SB-GCN, a representation learning scheme on BREPs that retains the topological structure of parts, and use these learned representations to predict CAD type mates. To train our system, we created the first large scale dataset of BREP CAD assemblies, which we are releasing with benchmark mate prediction tasks. Finally, we demonstrate our model's compatibility with an existing commercial CAD system by developing a tool that assists users in mate creation with 72.2% accuracy.",1
"We investigate the discounting mismatch in actor-critic algorithm implementations from a representation learning perspective. Theoretically, actor-critic algorithms usually have discounting for both actor and critic, i.e., there is a $\gamma^t$ term in the actor update for the transition observed at time $t$ in a trajectory and the critic is a discounted value function. Practitioners, however, usually ignore the discounting ($\gamma^t$) for the actor while using a discounted critic. We investigate this mismatch in two scenarios. In the first scenario, we consider optimizing an undiscounted objective $(\gamma = 1)$ where $\gamma^t$ disappears naturally $(1^t = 1)$. We then propose to interpret the discounting in critic in terms of a bias-variance-representation trade-off and provide supporting empirical results. In the second scenario, we consider optimizing a discounted objective ($\gamma < 1$) and propose to interpret the omission of the discounting in the actor update from an auxiliary task perspective and provide supporting empirical results.",0
"Our focus is on exploring the discrepancy in actor-critic algorithm implementations' discounting practices from the angle of representation learning. In theory, both the actor and critic in actor-critic algorithms are discounted, with a $\gamma^t$ factor included in the actor update for the observed transition at time $t$ in a trajectory, while the critic is a discounted value function. However, in practice, practitioners often neglect the discounting ($\gamma^t$) in the actor but use a discounted critic. We examine this mismatch in two scenarios. Firstly, we consider optimizing an undiscounted objective $(\gamma = 1)$ where $\gamma^t$ disappears naturally $(1^t = 1)$ and suggest an interpretation of the critic's discounting in terms of bias-variance-representation trade-off, with empirical support. Secondly, we examine optimizing a discounted objective ($\gamma < 1$) and propose an auxiliary task perspective to understand the omission of the discounting in the actor update, with empirical support.",1
"Few-shot learning is a challenging task since only few instances are given for recognizing an unseen class. One way to alleviate this problem is to acquire a strong inductive bias via meta-learning on similar tasks. In this paper, we show that such inductive bias can be learned from a flat collection of unlabeled images, and instantiated as transferable representations among seen and unseen classes. Specifically, we propose a novel part-based self-supervised representation learning scheme to learn transferable representations by maximizing the similarity of an image to its discriminative part. To mitigate the overfitting in few-shot classification caused by data scarcity, we further propose a part augmentation strategy by retrieving extra images from a base dataset. We conduct systematic studies on miniImageNet and tieredImageNet benchmarks. Remarkably, our method yields impressive results, outperforming the previous best unsupervised methods by 7.74% and 9.24% under 5-way 1-shot and 5-way 5-shot settings, which are comparable with state-of-the-art supervised methods.",0
"Due to limited instances available for recognizing an unseen class, few-shot learning is a challenging task. To address this issue, one possible solution is to acquire a strong inductive bias through meta-learning on similar tasks. This study demonstrates that such bias can be obtained from a flat collection of unlabeled images and used as transferable representations among both seen and unseen classes. The proposed approach involves a novel part-based self-supervised representation learning scheme that aims to learn transferable representations by maximizing the similarity between an image and its discriminative part. To counter overfitting in few-shot classification due to data scarcity, a part augmentation strategy is also introduced, which retrieves additional images from a base dataset. The proposed method is evaluated on miniImageNet and tieredImageNet benchmarks, yielding impressive results that outperform previous unsupervised methods by 7.74% and 9.24% under 5-way 1-shot and 5-way 5-shot settings, respectively, and are comparable to state-of-the-art supervised methods.",1
"Person re-identification (re-ID) tackles the problem of matching person images with the same identity from different cameras. In practical applications, due to the differences in camera performance and distance between cameras and persons of interest, captured person images usually have various resolutions. We name this problem as Cross-Resolution Person Re-identification which brings a great challenge for matching correctly. In this paper, we propose a Deep High-Resolution Pseudo-Siamese Framework (PS-HRNet) to solve the above problem. Specifically, in order to restore the resolution of low-resolution images and make reasonable use of different channel information of feature maps, we introduce and innovate VDSR module with channel attention (CA) mechanism, named as VDSR-CA. Then we reform the HRNet by designing a novel representation head to extract discriminating features, named as HRNet-ReID. In addition, a pseudo-siamese framework is constructed to reduce the difference of feature distributions between low-resolution images and high-resolution images. The experimental results on five cross-resolution person datasets verify the effectiveness of our proposed approach. Compared with the state-of-the-art methods, our proposed PS-HRNet improves 3.4\%, 6.2\%, 2.5\%,1.1\% and 4.2\% at Rank-1 on MLR-Market-1501, MLR-CUHK03, MLR-VIPeR, MLR-DukeMTMC-reID, and CAVIAR datasets, respectively. Our code is available at \url{https://github.com/zhguoqing}.",0
"The task of person re-identification (re-ID) pertains to identifying persons in images having the same identity from different cameras. Due to camera performance differences and the distance between cameras and persons of interest, captured person images have varying resolutions in practical applications. This challenge is referred to as Cross-Resolution Person Re-identification and poses difficulty in matching accurately. In this paper, we introduce a Deep High-Resolution Pseudo-Siamese Framework (PS-HRNet) to address this issue. To restore the resolution of low-resolution images and make efficient use of the channel information of feature maps, we present the VDSR-CA module with a channel attention (CA) mechanism. We also design a novel representation head for the HRNet to extract distinguishing features, called HRNet-ReID. Additionally, we construct a pseudo-siamese framework to reduce the disparity in feature distributions between low-resolution and high-resolution images. The results of our experiments on five cross-resolution person datasets confirm the effectiveness of our proposed approach. Compared to the state-of-the-art methods, our PS-HRNet enhances the Rank-1 performance by 3.4%, 6.2%, 2.5%, 1.1%, and 4.2% on the MLR-Market-1501, MLR-CUHK03, MLR-VIPeR, MLR-DukeMTMC-reID, and CAVIAR datasets, respectively. Our code is available at \url{https://github.com/zhguoqing}.",1
"We present the Topology Transformation Equivariant Representation learning, a general paradigm of self-supervised learning for node representations of graph data to enable the wide applicability of Graph Convolutional Neural Networks (GCNNs). We formalize the proposed model from an information-theoretic perspective, by maximizing the mutual information between topology transformations and node representations before and after the transformations. We derive that maximizing such mutual information can be relaxed to minimizing the cross entropy between the applied topology transformation and its estimation from node representations. In particular, we seek to sample a subset of node pairs from the original graph and flip the edge connectivity between each pair to transform the graph topology. Then, we self-train a representation encoder to learn node representations by reconstructing the topology transformations from the feature representations of the original and transformed graphs. In experiments, we apply the proposed model to the downstream node and graph classification tasks, and results show that the proposed method outperforms the state-of-the-art unsupervised approaches.",0
"Our article introduces a new approach to self-supervised learning for node representations in graph data, called Topology Transformation Equivariant Representation learning. Our goal is to enhance the applicability of Graph Convolutional Neural Networks (GCNNs). We base our proposed model on information theory, specifically by maximizing the mutual information between topology transformations and node representations before and after the transformations. To achieve this, we aim to minimize the cross entropy between the applied topology transformation and its estimated representation from node representations. Our method involves selecting a subset of node pairs from the original graph and flipping the edge connectivity between each pair to transform the graph topology. We then train a representation encoder to learn node representations by reconstructing the topology transformations using the feature representations of the original and transformed graphs. Our experiments show that our approach outperforms existing unsupervised methods in downstream node and graph classification tasks.",1
"Social reviews are indispensable resources for modern consumers' decision making. For financial gain, companies pay fraudsters preferably in groups to demote or promote products and services since consumers are more likely to be misled by a large number of similar reviews from groups. Recent approaches on fraudster group detection employed handcrafted features of group behaviors without considering the semantic relation between reviews from the reviewers in a group. In this paper, we propose the first neural approach, HIN-RNN, a Heterogeneous Information Network (HIN) Compatible RNN for fraudster group detection that requires no handcrafted features. HIN-RNN provides a unifying architecture for representation learning of each reviewer, with the initial vector as the sum of word embeddings of all review text written by the same reviewer, concatenated by the ratio of negative reviews. Given a co-review network representing reviewers who have reviewed the same items with the same ratings and the reviewers' vector representation, a collaboration matrix is acquired through HIN-RNN training. The proposed approach is confirmed to be effective with marked improvement over state-of-the-art approaches on both the Yelp (22% and 12% in terms of recall and F1-value, respectively) and Amazon (4% and 2% in terms of recall and F1-value, respectively) datasets.",0
"Modern consumers rely heavily on social reviews when making purchasing decisions. Unfortunately, some companies pay groups of fraudsters to manipulate these reviews in order to promote or demote their products and services. To combat this issue, previous fraudster group detection methods have focused on analyzing group behavior without considering the semantic relationships between reviews from individual group members. In this paper, we propose a new approach called HIN-RNN, which is the first neural method for fraudster group detection that does not require handcrafted features. HIN-RNN utilizes a Heterogeneous Information Network (HIN) to represent each reviewer, and trains a collaboration matrix based on their co-review network and vector representations. Our approach outperforms existing methods on both Yelp and Amazon datasets, with improvements of up to 22% and 12% in recall and F1-value on Yelp, and 4% and 2% on Amazon.",1
"Recent work on explainable clustering allows describing clusters when the features are interpretable. However, much modern machine learning focuses on complex data such as images, text, and graphs where deep learning is used but the raw features of data are not interpretable. This paper explores a novel setting for performing clustering on complex data while simultaneously generating explanations using interpretable tags. We propose deep descriptive clustering that performs sub-symbolic representation learning on complex data while generating explanations based on symbolic data. We form good clusters by maximizing the mutual information between empirical distribution on the inputs and the induced clustering labels for clustering objectives. We generate explanations by solving an integer linear programming that generates concise and orthogonal descriptions for each cluster. Finally, we allow the explanation to inform better clustering by proposing a novel pairwise loss with self-generated constraints to maximize the clustering and explanation module's consistency. Experimental results on public data demonstrate that our model outperforms competitive baselines in clustering performance while offering high-quality cluster-level explanations.",0
"Recent research has focused on explainable clustering, which allows for the description of clusters when features can be interpreted. However, modern machine learning usually involves complex data, like images, text, and graphs, where deep learning is used, but the raw features of the data are not interpretable. This study proposes a new approach to performing clustering on complex data while simultaneously generating explanations using interpretable tags. This approach involves deep descriptive clustering, which performs sub-symbolic representation learning on complex data while generating explanations based on symbolic data. The study aims to form good clusters by maximizing the mutual information between empirical distribution on the inputs and the induced clustering labels for clustering objectives. Explanations are generated by solving an integer linear programming that generates concise and orthogonal descriptions for each cluster. The study also allows the explanation to inform better clustering by proposing a novel pairwise loss with self-generated constraints to maximize the clustering and explanation module's consistency. Experimental results from public data show that this approach outperforms competitive baselines in clustering performance while offering high-quality cluster-level explanations.",1
"Cluster discrimination is an effective pretext task for unsupervised representation learning, which often consists of two phases: clustering and discrimination. Clustering is to assign each instance a pseudo label that will be used to learn representations in discrimination. The main challenge resides in clustering since many prevalent clustering methods (e.g., k-means) have to run in a batch mode that goes multiple iterations over the whole data. Recently, a balanced online clustering method, i.e., SwAV, is proposed for representation learning. However, the assignment is optimized within only a small subset of data, which can be suboptimal. To address these challenges, we first investigate the objective of clustering-based representation learning from the perspective of distance metric learning. Based on this, we propose a novel clustering-based pretext task with online \textbf{Co}nstrained \textbf{K}-m\textbf{e}ans (\textbf{CoKe}) to learn representations and relations between instances simultaneously. Compared with the balanced clustering that each cluster has exactly the same size, we only constrain the minimum size of clusters to flexibly capture the inherent data structure. More importantly, our online assignment method has a theoretical guarantee to approach the global optimum. Finally, two variance reduction strategies are proposed to make the clustering robust for different augmentations. Without keeping representations of instances, the data is accessed in an online mode in CoKe while a single view of instances at each iteration is sufficient to demonstrate a better performance than contrastive learning methods relying on two views. Extensive experiments on ImageNet verify the efficacy of our proposal. Code will be released.",0
"Unsupervised representation learning often involves two phases: clustering and discrimination, and cluster discrimination is an effective task for this purpose. In the clustering phase, each instance is assigned a pseudo label that is used to learn representations in discrimination. However, clustering poses a challenge as prevalent methods have to run multiple iterations over the whole data, which can be suboptimal. To tackle this, a new clustering method called SwAV has been proposed, but it optimizes assignments within a small subset of data. To overcome these challenges, this study proposes a novel clustering-based task called CoKe for representation learning and relations between instances. CoKe only constrains the minimum size of clusters to capture the data structure flexibly and uses an online assignment method that approaches the global optimum. Additionally, two variance reduction strategies are proposed to make clustering robust for different augmentations. CoKe accesses data in an online mode and demonstrates better performance than contrastive learning methods relying on two views. Extensive experiments on ImageNet validate the effectiveness of CoKe.",1
"Learning concepts that are consistent with human perception is important for Deep Neural Networks to win end-user trust. Post-hoc interpretation methods lack transparency in the feature representations learned by the models. This work proposes a guided learning approach with an additional concept layer in a CNN- based architecture to learn the associations between visual features and word phrases. We design an objective function that optimizes both prediction accuracy and semantics of the learned feature representations. Experiment results demonstrate that the proposed model can learn concepts that are consistent with human perception and their corresponding contributions to the model decision without compromising accuracy. Further, these learned concepts are transferable to new classes of objects that have similar concepts.",0
"It is crucial for Deep Neural Networks to adopt concepts that align with human perception in order to gain the trust of end-users. However, post-hoc interpretation methods are not transparent in displaying the feature representations that the models learn. To address this issue, this study proposes a guided learning approach that includes an additional concept layer within a CNN-based architecture. This helps to establish connections between visual features and word phrases. An objective function is also created to optimize the accuracy of predictions and the semantics of the learned feature representations. The results of experiments show that the proposed model successfully learns concepts that are in line with human perception, without sacrificing accuracy. Furthermore, these concepts can be applied to new classes of objects that share similar concepts.",1
"Recently a number of studies demonstrated impressive performance on diverse vision-language multi-modal tasks such as image captioning and visual question answering by extending the BERT architecture with multi-modal pre-training objectives. In this work we explore a broad set of multi-modal representation learning tasks in the medical domain, specifically using radiology images and the unstructured report. We propose Medical Vision Language Learner (MedViLL) which adopts a Transformer-based architecture combined with a novel multimodal attention masking scheme to maximize generalization performance for both vision-language understanding tasks (image-report retrieval, disease classification, medical visual question answering) and vision-language generation task (report generation). By rigorously evaluating the proposed model on four downstream tasks with two chest X-ray image datasets (MIMIC-CXR and Open-I), we empirically demonstrate the superior downstream task performance of MedViLL against various baselines including task-specific architectures.",0
"Several recent studies have shown impressive results in vision-language multi-modal tasks such as image captioning and visual question answering through the use of BERT architecture with multi-modal pre-training objectives. In this study, we investigate multi-modal representation learning tasks in the medical field using radiology images and unstructured reports. Our proposed solution, Medical Vision Language Learner (MedViLL), utilizes a Transformer-based architecture and a unique multimodal attention masking technique to optimize generalization performance for vision-language understanding tasks (image-report retrieval, disease classification, medical visual question answering) and vision-language generation tasks (report generation). Through rigorous evaluation of MedViLL on four downstream tasks using two chest X-ray image datasets (MIMIC-CXR and Open-I), we empirically demonstrate its superior performance compared to various baselines, including task-specific architectures.",1
"Representation learning on heterogeneous graphs aims to obtain meaningful node representations to facilitate various downstream tasks, such as node classification and link prediction. Existing heterogeneous graph learning methods are primarily developed by following the propagation mechanism of node representations. There are few efforts on studying the role of relations for improving the learning of more fine-grained node representations. Indeed, it is important to collaboratively learn the semantic representations of relations and discern node representations with respect to different relation types. To this end, in this paper, we propose a novel Relation-aware Heterogeneous Graph Neural Network, namely R-HGNN, to learn node representations on heterogeneous graphs at a fine-grained level by considering relation-aware characteristics. Specifically, a dedicated graph convolution component is first designed to learn unique node representations from each relation-specific graph separately. Then, a cross-relation message passing module is developed to improve the interactions of node representations across different relations. Also, the relation representations are learned in a layer-wise manner to capture relation semantics, which are used to guide the node representation learning process. Moreover, a semantic fusing module is presented to aggregate relation-aware node representations into a compact representation with the learned relation representations. Finally, we conduct extensive experiments on a variety of graph learning tasks, and experimental results demonstrate that our approach consistently outperforms existing methods among all the tasks.",0
"The objective of representation learning on heterogeneous graphs is to obtain node representations that are meaningful and useful for various downstream tasks, such as node classification and link prediction. However, existing methods for heterogeneous graph learning primarily rely on the propagation mechanism of node representations, with little attention given to the role of relations in improving the learning of more detailed node representations. To address this issue, we propose a new approach called Relation-aware Heterogeneous Graph Neural Network (R-HGNN). Our approach considers relation-aware characteristics to learn node representations on heterogeneous graphs at a fine-grained level. We first design a dedicated graph convolution component to learn unique node representations from each relation-specific graph separately. We then develop a cross-relation message passing module to enhance the interactions of node representations across different relations. Additionally, we learn relation representations in a layer-wise manner to capture relation semantics, which guide the node representation learning process. Finally, we present a semantic fusing module to aggregate relation-aware node representations into a compact representation with the learned relation representations. Our experimental results show that R-HGNN outperforms existing methods across a variety of graph learning tasks.",1
"Learning discriminative representation using large-scale face datasets in the wild is crucial for real-world applications, yet it remains challenging. The difficulties lie in many aspects and this work focus on computing resource constraint and long-tailed class distribution. Recently, classification-based representation learning with deep neural networks and well-designed losses have demonstrated good recognition performance. However, the computing and memory cost linearly scales up to the number of identities (classes) in the training set, and the learning process suffers from unbalanced classes. In this work, we propose a dynamic class queue (DCQ) to tackle these two problems. Specifically, for each iteration during training, a subset of classes for recognition are dynamically selected and their class weights are dynamically generated on-the-fly which are stored in a queue. Since only a subset of classes is selected for each iteration, the computing requirement is reduced. By using a single server without model parallel, we empirically verify in large-scale datasets that 10% of classes are sufficient to achieve similar performance as using all classes. Moreover, the class weights are dynamically generated in a few-shot manner and therefore suitable for tail classes with only a few instances. We show clear improvement over a strong baseline in the largest public dataset Megaface Challenge2 (MF2) which has 672K identities and over 88% of them have less than 10 instances. Code is available at https://github.com/bilylee/DCQ",0
"Obtaining distinctive representations from large-scale face datasets in natural conditions is crucial for practical purposes, but this remains a challenging task. The challenges are diverse, but this research concentrates on two issues: the limitation of computing resources and the uneven distribution of classes. Recently, deep neural networks and well-designed losses have demonstrated efficient recognition performance by learning classification-based representations. However, the cost of computing and memory increases linearly with the number of identities in the training set, and the learning process is affected by unbalanced classes. To address these problems, this research proposes a dynamic class queue (DCQ) that selects a subset of classes for recognition and generates their class weights dynamically on-the-fly, which are stored in a queue. By selecting only a subset of classes for each iteration, the computing requirement is reduced. The proposed method enables achieving similar performance as using all classes by using only 10% of classes. Furthermore, the class weights are generated dynamically and suitable for tail classes with only a few instances. The proposed method outperforms a strong baseline in the largest public dataset Megaface Challenge2 (MF2), which contains 672K identities with over 88% of them having less than 10 instances. The code is available at https://github.com/bilylee/DCQ.",1
"Graph representation learning has attracted increasing research attention. However, most existing studies fuse all structural features and node attributes to provide an overarching view of graphs, neglecting finer substructures' semantics, and suffering from interpretation enigmas. This paper presents a novel hierarchical subgraph-level selection and embedding based graph neural network for graph classification, namely SUGAR, to learn more discriminative subgraph representations and respond in an explanatory way. SUGAR reconstructs a sketched graph by extracting striking subgraphs as the representative part of the original graph to reveal subgraph-level patterns. To adaptively select striking subgraphs without prior knowledge, we develop a reinforcement pooling mechanism, which improves the generalization ability of the model. To differentiate subgraph representations among graphs, we present a self-supervised mutual information mechanism to encourage subgraph embedding to be mindful of the global graph structural properties by maximizing their mutual information. Extensive experiments on six typical bioinformatics datasets demonstrate a significant and consistent improvement in model quality with competitive performance and interpretability.",0
"The area of graph representation learning has received more attention from researchers, but many studies combine all structural features and node attributes, resulting in a lack of understanding of finer substructures and causing interpretation issues. To address this, our paper introduces SUGAR, a novel graph neural network that utilizes a hierarchical subgraph-level selection and embedding approach for graph classification. SUGAR extracts noteworthy subgraphs from the original graph to reveal subgraph-level patterns and improve model generalization through a reinforcement pooling mechanism. To differentiate subgraph representations among graphs, we propose a self-supervised mutual information mechanism. Our experiments on six bioinformatics datasets demonstrate that SUGAR significantly improves model quality while maintaining competitive performance and interpretability.",1
"Despite the prevalence of hypergraphs in a variety of high-impact applications, there are relatively few works on hypergraph representation learning, most of which primarily focus on hyperlink prediction, often restricted to the transductive learning setting. Among others, a major hurdle for effective hypergraph representation learning lies in the label scarcity of nodes and/or hyperedges. To address this issue, this paper presents an end-to-end, bi-level pre-training strategy with Graph Neural Networks for hypergraphs. The proposed framework named HyperGene bears three distinctive advantages. First, it is capable of ingesting the labeling information when available, but more importantly, it is mainly designed in the self-supervised fashion which significantly broadens its applicability. Second, at the heart of the proposed HyperGene are two carefully designed pretexts, one on the node level and the other on the hyperedge level, which enable us to encode both the local and the global context in a mutually complementary way. Third, the proposed framework can work in both transductive and inductive settings. When applying the two proposed pretexts in tandem, it can accelerate the adaptation of the knowledge from the pre-trained model to downstream applications in the transductive setting, thanks to the bi-level nature of the proposed method. The extensive experimental results demonstrate that: (1) HyperGene achieves up to 5.69% improvements in hyperedge classification, and (2) improves pre-training efficiency by up to 42.80% on average.",0
"Despite the widespread use of hypergraphs in high-impact applications, there is a lack of research on hypergraph representation learning. Most existing works focus on hyperlink prediction and are limited to transductive learning. One of the main challenges in effective hypergraph representation learning is the scarcity of labeled nodes and/or hyperedges. To address this issue, this paper proposes an end-to-end, bi-level pre-training strategy called HyperGene, which utilizes Graph Neural Networks. HyperGene has three advantages: (1) it can incorporate labeling information when available, but can also operate in a self-supervised manner, expanding its applicability, (2) it uses two pretexts on the node and hyperedge levels to encode both local and global context in a complementary way, and (3) it can work in both transductive and inductive settings, allowing for knowledge transfer to downstream applications. Experimental results show that HyperGene improves hyperedge classification by up to 5.69% and increases pre-training efficiency by up to 42.80% on average.",1
"Generative adversarial networks (GANs) are one of the greatest advances in AI in recent years. With their ability to directly learn the probability distribution of data, and then sample synthetic realistic data. Many applications have emerged, using GANs to solve classical problems in machine learning, such as data augmentation, class unbalance problems, and fair representation learning. In this paper, we analyze and highlight fairness concerns of GANs model. In this regard, we show empirically that GANs models may inherently prefer certain groups during the training process and therefore they're not able to homogeneously generate data from different groups during the testing phase. Furthermore, we propose solutions to solve this issue by conditioning the GAN model towards samples' group or using ensemble method (boosting) to allow the GAN model to leverage distributed structure of data during the training phase and generate groups at equal rate during the testing phase.",0
"GANs, an exceptional advancement in AI, have the power to learn the probability distribution of data and generate synthetic realistic data. They have been utilized in various machine learning applications such as data augmentation, class imbalance problems, and fair representation learning. However, fairness concerns regarding GANs models have been raised. Our study empirically demonstrates that GANs models may exhibit preference towards specific groups during the training process, resulting in the inability to generate data equally from various groups during the testing phase. To address this issue, we propose conditioning the GAN model towards samples' group or using ensemble methods (boosting) to enable the GAN model to utilize the distributed structure of data and generate groups at a balanced rate during the testing phase.",1
"In this paper, we propose a novel framework for Deep Clustering and multi-manifold Representation Learning (DCRL) that preserves the geometric structure of data. In the proposed framework, manifold clustering is done in the latent space guided by a clustering loss. To overcome the problem that clustering-oriented losses may deteriorate the geometric structure of embeddings in the latent space, an isometric loss is proposed for preserving intra-manifold structure locally and a ranking loss for inter-manifold structure globally. Experimental results on various datasets show that DCRL leads to performances comparable to current state-of-the-art deep clustering algorithms, yet exhibits superior performance for manifold representation. Our results also demonstrate the importance and effectiveness of the proposed losses in preserving geometric structure in terms of visualization and performance metrics.",0
"This paper introduces a fresh framework called DCRL (Deep Clustering and multi-manifold Representation Learning) that maintains the geometric structure of data. The framework utilizes a clustering loss to guide the manifold clustering process in the latent space. To prevent the clustering-oriented losses from damaging the embeddings' geometric structure in the latent space, an isometric loss is proposed for local intra-manifold structure preservation and a ranking loss for global inter-manifold structure preservation. Experimental results on various datasets demonstrate that DCRL performs similarly to current state-of-the-art deep clustering algorithms, but its manifold representation outperforms them. The proposed losses are crucial and effective in preserving geometric structure, as evidenced by the visualization and performance metrics.",1
"Time-series representation learning is a fundamental task for time-series analysis. While significant progress has been made to achieve accurate representations for downstream applications, the learned representations often lack interpretability and do not expose semantic meanings. Different from previous efforts on the entangled feature space, we aim to extract the semantic-rich temporal correlations in the latent interpretable factorized representation of the data. Motivated by the success of disentangled representation learning in computer vision, we study the possibility of learning semantic-rich time-series representations, which remains unexplored due to three main challenges: 1) sequential data structure introduces complex temporal correlations and makes the latent representations hard to interpret, 2) sequential models suffer from KL vanishing problem, and 3) interpretable semantic concepts for time-series often rely on multiple factors instead of individuals. To bridge the gap, we propose Disentangle Time Series (DTS), a novel disentanglement enhancement framework for sequential data. Specifically, to generate hierarchical semantic concepts as the interpretable and disentangled representation of time-series, DTS introduces multi-level disentanglement strategies by covering both individual latent factors and group semantic segments. We further theoretically show how to alleviate the KL vanishing problem: DTS introduces a mutual information maximization term, while preserving a heavier penalty on the total correlation and the dimension-wise KL to keep the disentanglement property. Experimental results on various real-world benchmark datasets demonstrate that the representations learned by DTS achieve superior performance in downstream applications, with high interpretability of semantic concepts.",0
"Learning representations for time-series analysis is a crucial task, but current methods often lack interpretability and do not convey semantic meanings. Our approach aims to extract semantic-rich temporal correlations in a latent interpretable factorized representation, unlike previous efforts that focus on entangled feature spaces. However, there are three main challenges in learning such representations: complex temporal correlations, the KL vanishing problem, and the reliance on multiple factors for interpretable semantic concepts. To address these challenges, we propose Disentangle Time Series (DTS), a disentanglement enhancement framework for sequential data. DTS introduces multi-level disentanglement strategies to generate hierarchical semantic concepts, while also addressing the KL vanishing problem through mutual information maximization and heavy penalties on total correlation and dimension-wise KL. Our experimental results on various real-world datasets demonstrate that DTS achieves superior performance in downstream applications, with highly interpretable semantic concepts.",1
"We propose a self-supervised visual learning method by predicting the variable playback speeds of a video. Without semantic labels, we learn the spatio-temporal visual representation of the video by leveraging the variations in the visual appearance according to different playback speeds under the assumption of temporal coherence. To learn the spatio-temporal visual variations in the entire video, we have not only predicted a single playback speed but also generated clips of various playback speeds and directions with randomized starting points. Hence the visual representation can be successfully learned from the meta information (playback speeds and directions) of the video. We also propose a new layer dependable temporal group normalization method that can be applied to 3D convolutional networks to improve the representation learning performance where we divide the temporal features into several groups and normalize each one using the different corresponding parameters. We validate the effectiveness of our method by fine-tuning it to the action recognition and video retrieval tasks on UCF-101 and HMDB-51.",0
"Our proposed method for self-supervised visual learning involves predicting variable playback speeds of a video. We do not use semantic labels, but instead, we leverage the variations in visual appearance at different playback speeds assuming temporal coherence to learn the spatio-temporal visual representation of the video. To capture the spatio-temporal visual variations, we generate clips with various playback speeds and directions with randomized starting points. By training on this meta information (playback speeds and directions), we can successfully learn the visual representation. Additionally, we introduce a new layer-dependent temporal group normalization method that can enhance the representation learning performance of 3D convolutional networks. This method involves dividing the temporal features into groups and normalizing each group using corresponding parameters. We evaluate our approach on action recognition and video retrieval tasks on UCF-101 and HMDB-51 datasets and demonstrate its effectiveness.",1
"First-person object-interaction tasks in high-fidelity, 3D, simulated environments such as the AI2Thor virtual home-environment pose significant sample-efficiency challenges for reinforcement learning (RL) agents learning from sparse task rewards. To alleviate these challenges, prior work has provided extensive supervision via a combination of reward-shaping, ground-truth object-information, and expert demonstrations. In this work, we show that one can learn object-interaction tasks from scratch without supervision by learning an attentive object-model as an auxiliary task during task learning with an object-centric relational RL agent. Our key insight is that learning an object-model that incorporates object-attention into forward prediction provides a dense learning signal for unsupervised representation learning of both objects and their relationships. This, in turn, enables faster policy learning for an object-centric relational RL agent. We demonstrate our agent by introducing a set of challenging object-interaction tasks in the AI2Thor environment where learning with our attentive object-model is key to strong performance. Specifically, we compare our agent and relational RL agents with alternative auxiliary tasks to a relational RL agent equipped with ground-truth object-information, and show that learning with our object-model best closes the performance gap in terms of both learning speed and maximum success rate. Additionally, we find that incorporating object-attention into an object-model's forward predictions is key to learning representations which capture object-category and object-state.",0
"Reinforcement learning agents face significant challenges when attempting first-person object-interaction tasks in high-fidelity, 3D, simulated environments like the AI2Thor virtual home-environment. Sparse task rewards make it difficult to learn without extensive supervision, including reward-shaping, ground-truth object-information, and expert demonstrations. This study demonstrates that it is possible to learn object-interaction tasks without such guidance by training an attentive object-model as an auxiliary task during task learning with an object-centric relational RL agent. The object-model incorporates object-attention into forward prediction, providing a dense learning signal for unsupervised representation learning of both objects and their relationships. Our agent performs well on challenging object-interaction tasks in the AI2Thor environment, closing the performance gap in terms of both learning speed and maximum success rate when compared to other approaches. Incorporating object-attention into an object-model's forward predictions is key to capturing object-category and object-state representations.",1
"In recommendation systems, the existence of the missing-not-at-random (MNAR) problem results in the selection bias issue, degrading the recommendation performance ultimately. A common practice to address MNAR is to treat missing entries from the so-called ""exposure"" perspective, i.e., modeling how an item is exposed (provided) to a user. Most of the existing approaches use heuristic models or re-weighting strategy on observed ratings to mimic the missing-at-random setting. However, little research has been done to reveal how the ratings are missing from a causal perspective. To bridge the gap, we propose an unbiased and robust method called DENC (De-bias Network Confounding in Recommendation) inspired by confounder analysis in causal inference. In general, DENC provides a causal analysis on MNAR from both the inherent factors (e.g., latent user or item factors) and auxiliary network's perspective. Particularly, the proposed exposure model in DENC can control the social network confounder meanwhile preserves the observed exposure information. We also develop a deconfounding model through the balanced representation learning to retain the primary user and item features, which enables DENC generalize well on the rating prediction. Extensive experiments on three datasets validate that our proposed model outperforms the state-of-the-art baselines.",0
"The MNAR problem in recommendation systems causes selection bias, ultimately leading to degraded recommendation performance. To address this issue, missing entries are typically treated from the ""exposure"" perspective, where the focus is on modeling how an item is exposed to a user. However, existing approaches rely on heuristic models or re-weighting strategies, which fail to address the problem from a causal perspective. To bridge this gap, we propose a method called DENC (De-bias Network Confounding in Recommendation), which is based on confounder analysis in causal inference. DENC provides a causal analysis on MNAR from both the inherent factors and auxiliary network's perspective, controlling the social network confounder while preserving the observed exposure information. We also develop a deconfounding model to retain the primary user and item features, enabling DENC to generalize well on rating prediction. Our extensive experiments on three datasets demonstrate that DENC outperforms existing baselines.",1
"Deep convolutional networks have attracted great attention in image restoration and enhancement. Generally, restoration quality has been improved by building more and more convolutional block. However, these methods mostly learn a specific model to handle all images and ignore difficulty diversity. In other words, an area in the image with high frequency tend to lose more information during compressing while an area with low frequency tends to lose less. In this article, we adrress the efficiency issue in image SR by incorporating a patch-wise rolling network(PRN) to content-adaptively recover images according to difficulty levels. In contrast to existing studies that ignore difficulty diversity, we adopt different stage of a neural network to perform image restoration. In addition, we propose a rolling strategy that utilizes the parameters of each stage more flexible. Extensive experiments demonstrate that our model not only shows a significant acceleration but also maintain state-of-the-art performance.",0
"Image restoration and enhancement have become areas of great interest due to the increased attention given to deep convolutional networks. Current methods focus on improving restoration quality by incorporating more convolutional blocks. However, these methods tend to learn a specific model that can handle all images, which ignores the diversity of difficulties. In simpler terms, high-frequency areas of an image tend to lose more information during compression, while low-frequency areas tend to lose less. This article addresses the efficiency issue in image SR by introducing a patch-wise rolling network (PRN) that can content-adaptively recover images based on their difficulty level. Unlike previous studies that overlook difficulty diversity, we adopt different stages of a neural network to perform image restoration while proposing a rolling strategy that utilizes each stage's parameters flexibly. Our extensive experiments demonstrate that our model not only accelerates the process significantly, but it also maintains its state-of-the-art performance.",1
"Entanglement is a physical phenomenon, which has fueled recent successes of quantum algorithms. Although quantum neural networks (QNNs) have shown promising results in solving simple machine learning tasks recently, for the time being, the effect of entanglement in QNNs and the behavior of QNNs in binary pattern classification are still underexplored. In this work, we provide some theoretical insight into the properties of QNNs by presenting and analyzing a new form of invariance embedded in QNNs for both quantum binary classification and quantum representation learning, which we term negational symmetry. Given a quantum binary signal and its negational counterpart where a bitwise NOT operation is applied to each quantum bit of the binary signal, a QNN outputs the same logits. That is to say, QNNs cannot differentiate a quantum binary signal and its negational counterpart in a binary classification task. We further empirically evaluate the negational symmetry of QNNs in binary pattern classification tasks using Google's quantum computing framework. The theoretical and experimental results suggest that negational symmetry is a fundamental property of QNNs, which is not shared by classical models. Our findings also imply that negational symmetry is a double-edged sword in practical quantum applications.",0
"Recent successes of quantum algorithms have been fueled by entanglement, a physical phenomenon. While quantum neural networks (QNNs) have shown promise in simple machine learning tasks, their behavior in binary pattern classification and the effect of entanglement in QNNs are still underexplored. In this study, we introduce and analyze a new form of invariance in QNNs, called negational symmetry, for quantum binary classification and representation learning. When a bitwise NOT operation is applied to each quantum bit of a binary signal and its negational counterpart, a QNN outputs the same logits, meaning QNNs cannot differentiate between them in a binary classification task. We evaluate the negational symmetry of QNNs in binary pattern classification tasks using Google's quantum computing framework. Our theoretical and experimental results suggest that negational symmetry is a fundamental property of QNNs that is not present in classical models. However, we also note that negational symmetry can be a double-edged sword in practical quantum applications.",1
"Unsupervised outlier detection, which predicts if a test sample is an outlier or not using only the information from unlabelled inlier data, is an important but challenging task. Recently, methods based on the two-stage framework achieve state-of-the-art performance on this task. The framework leverages self-supervised representation learning algorithms to train a feature extractor on inlier data, and applies a simple outlier detector in the feature space. In this paper, we explore the possibility of avoiding the high cost of training a distinct representation for each outlier detection task, and instead using a single pre-trained network as the universal feature extractor regardless of the source of in-domain data. In particular, we replace the task-specific feature extractor by one network pre-trained on ImageNet with a self-supervised loss. In experiments, we demonstrate competitive or better performance on a variety of outlier detection benchmarks compared with previous two-stage methods, suggesting that learning representations from in-domain data may be unnecessary for outlier detection.",0
"Detecting outliers without supervision is a challenging task that involves predicting if a test sample is an outlier or not using only unlabelled inlier data. Recently, a two-stage framework has been used to achieve state-of-the-art performance in this task. This framework uses self-supervised representation learning algorithms to train a feature extractor on inlier data and applies a simple outlier detector in the feature space. In this study, we investigate the possibility of using a single pre-trained network as a universal feature extractor instead of training a distinct representation for each outlier detection task, thereby reducing the high cost of training. We replace the task-specific feature extractor with a network pre-trained on ImageNet with a self-supervised loss. Our experiments demonstrate competitive or better performance on several outlier detection benchmarks compared to previous two-stage methods, indicating that learning representations from in-domain data may not be necessary for outlier detection.",1
"View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on three public datasets shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at https://github.com/Gword/Recursive-NeRF.",0
"Methods for view synthesis, which use implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have become increasingly popular due to their ability to produce high quality imagery and scale to high resolution. However, the volumetric approach used by NeRF requires heavy computation, making it impractical as it takes minutes to render a single image of a few megapixels. To address this issue, we propose rendering images of a scene in a level-of-detail manner, where a complicated region of the scene is represented by a large neural network, while a simple region is encoded by a small neural network. This approach allows for a balance between efficiency and quality. Our proposed method, Recursive-NeRF, embodies this idea and provides an efficient and adaptive rendering and training approach for NeRF. Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on three public datasets demonstrates that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code for Recursive-NeRF will be available at https://github.com/Gword/Recursive-NeRF.",1
"Federated Learning (FL) is a framework which enables distributed model training using a large corpus of decentralized training data. Existing methods aggregate models disregarding their internal representations, which are crucial for training models in vision tasks. System and statistical heterogeneity (e.g., highly imbalanced and non-i.i.d. data) further harm model training. To this end, we introduce a method, called FedProto, which computes client deviations using margins of prototypical representations learned on distributed data, and applies them to drive federated optimization via an attention mechanism. In addition, we propose three methods to analyse statistical properties of feature representations learned in FL, in order to elucidate the relationship between accuracy, margins and feature discrepancy of FL models. In experimental analyses, FedProto demonstrates state-of-the-art accuracy and convergence rate across image classification and semantic segmentation benchmarks by enabling maximum margin training of FL models. Moreover, FedProto reduces uncertainty of predictions of FL models compared to the baseline. To our knowledge, this is the first work evaluating FL models in dense prediction tasks, such as semantic segmentation.",0
"FL is a framework that facilitates distributed model training by using a vast amount of decentralized training data. The current methods of aggregating models overlook the importance of their internal representations, which are essential for training models in visual tasks. Statistical heterogeneity, such as highly imbalanced and non-i.i.d. data, further deteriorates model training. Hence, we propose FedProto, a technique that calculates client deviations by using margins of prototypical representations learned on distributed data. It applies these deviations to guide federated optimization via an attention mechanism. Additionally, we suggest three methods to analyze statistical properties of feature representations learned in FL, which helps to understand the relationship between accuracy, margins, and feature discrepancy of FL models. In experimental analyses, FedProto proves to be superior by achieving state-of-the-art accuracy and convergence rate across image classification and semantic segmentation benchmarks through maximum margin training of FL models. Furthermore, it reduces the uncertainty of predictions of FL models compared to the baseline. This study is the first to evaluate FL models in dense prediction tasks like semantic segmentation.",1
"Recent research in self-supervised learning (SSL) has shown its capability in learning useful semantic representations from images for classification tasks. Through our work, we study the usefulness of SSL for Fine-Grained Visual Categorization (FGVC). FGVC aims to distinguish objects of visually similar sub categories within a general category. The small inter-class, but large intra-class variations within the dataset makes it a challenging task. The limited availability of annotated labels for such a fine-grained data encourages the need for SSL, where additional supervision can boost learning without the cost of extra annotations. Our baseline achieves $86.36\%$ top-1 classification accuracy on CUB-200-2011 dataset by utilizing random crop augmentation during training and center crop augmentation during testing. In this work, we explore the usefulness of various pretext tasks, specifically, rotation, pretext invariant representation learning (PIRL), and deconstruction and construction learning (DCL) for FGVC. Rotation as an auxiliary task promotes the model to learn global features, and diverts it from focusing on the subtle details. PIRL that uses jigsaw patches attempts to focus on discriminative local regions, but struggles to accurately localize them. DCL helps in learning local discriminating features and outperforms the baseline by achieving $87.41\%$ top-1 accuracy. The deconstruction learning forces the model to focus on local object parts, while reconstruction learning helps in learning the correlation between the parts. We perform extensive experiments to reason our findings. Our code is available at https://github.com/mmaaz60/ssl_for_fgvc.",0
"The capacity of self-supervised learning (SSL) to acquire valuable semantic representations from images for classification tasks has been demonstrated by recent research. Our study focuses on investigating the usefulness of SSL for Fine-Grained Visual Categorization (FGVC), which aims to differentiate visually similar sub categories within a general category. This is a challenging task due to the small inter-class, but large intra-class variations within the dataset. The scarcity of annotated labels for such fine-grained data highlights the need for SSL, which can enhance learning without requiring additional annotations. Our baseline approach, which utilizes random crop augmentation during training and center crop augmentation during testing, achieves a top-1 classification accuracy of 86.36% on the CUB-200-2011 dataset. In this work, we explore the effectiveness of various pretext tasks, namely rotation, pretext invariant representation learning (PIRL), and deconstruction and construction learning (DCL), for FGVC. Rotation as an auxiliary task encourages the model to learn global features and divert its attention from subtle details. PIRL, which employs jigsaw patches, attempts to focus on discriminative local regions but struggles to localize them accurately. DCL facilitates the learning of local discriminating features and outperforms the baseline by achieving a top-1 accuracy of 87.41%. Deconstruction learning compels the model to concentrate on local object parts, while reconstruction learning aids in understanding the correlation between the parts. We conduct comprehensive experiments to validate our findings, and our code is accessible at https://github.com/mmaaz60/ssl_for_fgvc.",1
"Recent advances in deep pose estimation models have proven to be effective in a wide range of applications such as health monitoring, sports, animations, and robotics. However, pose estimation models fail to generalize when facing images acquired from in-bed pressure sensing systems. In this paper, we address this challenge by presenting a novel end-to-end framework capable of accurately locating body parts from vague pressure data. Our method exploits the idea of equipping an off-the-shelf pose estimator with a deep trainable neural network, which pre-processes and prepares the pressure data for subsequent pose estimation. Our model transforms the ambiguous pressure maps to images containing shapes and structures similar to the common input domain of the pre-existing pose estimation methods. As a result, we show that our model is able to reconstruct unclear body parts, which in turn enables pose estimators to accurately and robustly estimate the pose. We train and test our method on a manually annotated public pressure map dataset using a combination of loss functions. Results confirm the effectiveness of our method by the high visual quality in the generated images and the high pose estimation rates achieved.",0
"Deep pose estimation models have demonstrated their effectiveness in diverse applications such as sports, health monitoring, robotics, and animations. However, these models encounter difficulties in generalizing when confronted with images obtained from in-bed pressure sensing systems. To address this limitation, we propose an innovative end-to-end framework that can accurately locate body parts from vague pressure data. Our approach involves utilizing a trainable neural network to preprocess and prepare the pressure data for subsequent pose estimation by an off-the-shelf pose estimator. Our model transforms the uncertain pressure maps into images that contain shapes and structures similar to the input domain of pre-existing pose estimation methods. This strategy enables our model to reconstruct unclear body parts, thereby aiding the pose estimator in accurately and robustly estimating the pose. We evaluate our approach on a publicly available pressure map dataset, training and testing it using a combination of loss functions. Our results indicate the efficacy of our method, as evidenced by the high pose estimation rates achieved and the high visual quality of the generated images.",1
"Encoding the scale information explicitly into the representation learned by a convolutional neural network (CNN) is beneficial for many computer vision tasks especially when dealing with multiscale inputs. We study, in this paper, a scaling-translation-equivariant (ST-equivariant) CNN with joint convolutions across the space and the scaling group, which is shown to be both sufficient and necessary to achieve equivariance for the regular representation of the scaling-translation group ST . To reduce the model complexity and computational burden, we decompose the convolutional filters under two pre-fixed separable bases and truncate the expansion to low-frequency components. A further benefit of the truncated filter expansion is the improved deformation robustness of the equivariant representation, a property which is theoretically analyzed and empirically verified. Numerical experiments demonstrate that the proposed scaling-translation-equivariant network with decomposed convolutional filters (ScDCFNet) achieves significantly improved performance in multiscale image classification and better interpretability than regular CNNs at a reduced model size.",0
"Explicitly encoding scale information into the learned representation of a convolutional neural network (CNN) is advantageous for various computer vision tasks, particularly those involving multiscale inputs. Our paper examines a scaling-translation-equivariant (ST-equivariant) CNN with joint convolutions across the spatial and scaling groups, which is both necessary and sufficient for achieving equivariance for the regular representation of the scaling-translation group ST. To reduce model complexity and computational load, we decompose the convolutional filters under two pre-determined separable bases and truncate the expansion to lower frequency components. This truncated filter expansion also enhances the deformation robustness of the equivariant representation, a feature that is theoretically analyzed and empirically verified. Numerical experiments demonstrate that our proposed ScDCFNet, a scaling-translation-equivariant network with decomposed convolutional filters, achieves significantly better performance in multiscale image classification and greater interpretability than regular CNNs at a reduced model size.",1
"Detecting tiny objects in a high-resolution video is challenging because the visual information is little and unreliable. Specifically, the challenge includes very low resolution of the objects, MPEG artifacts due to compression and a large searching area with many hard negatives. Tracking is equally difficult because of the unreliable appearance, and the unreliable motion estimation. Luckily, we found that by combining this two challenging tasks together, there will be mutual benefits. Following the idea, in this paper, we present a neural network model called the Recurrent Correlational Network, where detection and tracking are jointly performed over a multi-frame representation learned through a single, trainable, and end-to-end network. The framework exploits a convolutional long short-term memory network for learning informative appearance changes for detection, while the learned representation is shared in tracking for enhancing its performance. In experiments with datasets containing images of scenes with small flying objects, such as birds and unmanned aerial vehicles, the proposed method yielded consistent improvements in detection performance over deep single-frame detectors and existing motion-based detectors. Furthermore, our network performs as well as state-of-the-art generic object trackers when it was evaluated as a tracker on a bird image dataset.",0
"The difficulty of detecting small objects in high-resolution videos arises from the limited and unreliable visual information available. This challenge encompasses several factors, including the low resolution of the objects, MPEG compression artifacts, and a vast search area with numerous difficult negatives. Tracking is also problematic due to the unstable appearance and motion estimation. However, the authors of this paper discovered that combining these challenging tasks could be mutually beneficial. To that end, they introduced a neural network model, the Recurrent Correlational Network, that jointly performs detection and tracking on a multi-frame representation learned through a single, trainable, and end-to-end network. The model uses a convolutional long short-term memory network to learn informative appearance changes for detection and share the learned representation to enhance tracking performance. Experimental results on datasets containing small flying objects, such as birds and unmanned aerial vehicles, demonstrated that the proposed method outperformed deep single-frame detectors and existing motion-based detectors in detection performance. Moreover, when evaluated as a tracker on a bird image dataset, the network performed as well as state-of-the-art generic object trackers.",1
"Current supervised sketch-based image retrieval (SBIR) methods achieve excellent performance. However, the cost of data collection and labeling imposes an intractable barrier to practical deployment of real applications. In this paper, we present the first attempt at unsupervised SBIR to remove the labeling cost (category annotations and sketch-photo pairings) that is conventionally needed for training. Existing single-domain unsupervised representation learning methods perform poorly in this application, due to the unique cross-domain (sketch and photo) nature of the problem. We therefore introduce a novel framework that simultaneously performs unsupervised representation learning and sketch-photo domain alignment. Technically this is underpinned by exploiting joint distribution optimal transport (JDOT) to align data from different domains during representation learning, which we extend with trainable cluster prototypes and feature memory banks to further improve scalability and efficacy. Extensive experiments show that our framework achieves excellent performance in the new unsupervised setting, and performs comparably or better than state-of-the-art in the zero-shot setting.",0
"Although current supervised sketch-based image retrieval (SBIR) methods have shown remarkable results, the high cost of data collection and labeling hinders their practical application. To address this challenge, this paper presents an innovative approach for unsupervised SBIR, which eliminates the need for category annotations and sketch-photo pairings that are typically required for training. Previous unsupervised representation learning methods have failed to work optimally in this application due to the unique cross-domain nature of the problem. To address this issue, the paper proposes a new framework that combines unsupervised representation learning and sketch-photo domain alignment. The framework leverages joint distribution optimal transport (JDOT) to align data from different domains during representation learning and extends it with trainable cluster prototypes and feature memory banks to enhance scalability and efficacy. The study's findings show that the proposed framework achieves outstanding performance in the new unsupervised setting and performs comparably or better than the state-of-the-art in the zero-shot setting.",1
"Dynamic graph modeling has recently attracted much attention due to its extensive applications in many real-world scenarios, such as recommendation systems, financial transactions, and social networks. Although many works have been proposed for dynamic graph modeling in recent years, effective and scalable models are yet to be developed. In this paper, we propose a novel graph neural network approach, called TCL, which deals with the dynamically-evolving graph in a continuous-time fashion and enables effective dynamic node representation learning that captures both the temporal and topology information. Technically, our model contains three novel aspects. First, we generalize the vanilla Transformer to temporal graph learning scenarios and design a graph-topology-aware transformer. Secondly, on top of the proposed graph transformer, we introduce a two-stream encoder that separately extracts representations from temporal neighborhoods associated with the two interaction nodes and then utilizes a co-attentional transformer to model inter-dependencies at a semantic level. Lastly, we are inspired by the recently developed contrastive learning and propose to optimize our model by maximizing mutual information (MI) between the predictive representations of two future interaction nodes. Benefiting from this, our dynamic representations can preserve high-level (or global) semantics about interactions and thus is robust to noisy interactions. To the best of our knowledge, this is the first attempt to apply contrastive learning to representation learning on dynamic graphs. We evaluate our model on four benchmark datasets for interaction prediction and experiment results demonstrate the superiority of our model.",0
"The popularity of dynamic graph modeling has grown recently due to its widespread use in practical situations such as financial transactions, recommendation systems, and social networks. Despite the numerous proposed approaches for dynamic graph modeling, effective and scalable models have yet to be developed. Our paper presents a novel graph neural network method, called TCL, which addresses the dynamically-evolving graph in a continuous-time manner, enabling efficient dynamic node representation learning that captures both temporal and topological information. Our model has three novel aspects. Firstly, we introduce a graph-topology-aware transformer that generalizes the vanilla Transformer for temporal graph learning scenarios. Secondly, we add a two-stream encoder on top of the proposed graph transformer, which separately extracts representations from temporal neighborhoods associated with the two interaction nodes. A co-attentional transformer is then used to model inter-dependencies at a semantic level. Lastly, inspired by the recently developed contrastive learning, we propose to optimize our model by maximizing mutual information (MI) between the predictive representations of two future interaction nodes. This allows our dynamic representations to preserve high-level semantics about interactions, making them robust to noisy interactions. Our approach is the first attempt to apply contrastive learning to representation learning on dynamic graphs. We evaluate our model on four benchmark datasets for interaction prediction and show that our model outperforms other methods.",1
"A common practice in unsupervised representation learning is to use labeled data to evaluate the quality of the learned representations. This supervised evaluation is then used to guide critical aspects of the training process such as selecting the data augmentation policy. However, guiding an unsupervised training process through supervised evaluations is not possible for real-world data that does not actually contain labels (which may be the case, for example, in privacy sensitive fields such as medical imaging). Therefore, in this work we show that evaluating the learned representations with a self-supervised image rotation task is highly correlated with a standard set of supervised evaluations (rank correlation $> 0.94$). We establish this correlation across hundreds of augmentation policies, training settings, and network architectures and provide an algorithm (SelfAugment) to automatically and efficiently select augmentation policies without using supervised evaluations. Despite not using any labeled data, the learned augmentation policies perform comparably with augmentation policies that were determined using exhaustive supervised evaluations.",0
"A common technique in unsupervised representation learning is to evaluate the quality of learned representations using labeled data. This supervised evaluation is then used to guide crucial aspects of the training process, such as selecting the data augmentation policy. However, it is not possible to guide an unsupervised training process through supervised evaluations for real-world data that lacks labels, particularly in privacy-sensitive fields like medical imaging. Thus, this study demonstrates that evaluating the learned representations with a self-supervised image rotation task is highly correlated with a standard set of supervised evaluations. This correlation is demonstrated across numerous augmentation policies, training settings, and network architectures, and an algorithm (SelfAugment) is provided to automatically and efficiently select augmentation policies without using supervised evaluations. Despite the absence of labeled data, the learned augmentation policies perform comparably with exhaustive supervised evaluations.",1
"Recent works in medical image segmentation have actively explored various deep learning architectures or objective functions to encode high-level features from volumetric data owing to limited image annotations. However, most existing approaches tend to ignore cross-volume global context and define context relations in the decision space. In this work, we propose a novel voxel-level Siamese representation learning method for abdominal multi-organ segmentation to improve representation space. The proposed method enforces voxel-wise feature relations in the representation space for leveraging limited datasets more comprehensively to achieve better performance. Inspired by recent progress in contrastive learning, we suppressed voxel-wise relations from the same class to be projected to the same point without using negative samples. Moreover, we introduce a multi-resolution context aggregation method that aggregates features from multiple hidden layers, which encodes both the global and local contexts for segmentation. Our experiments on the multi-organ dataset outperformed the existing approaches by 2% in Dice score coefficient. The qualitative visualizations of the representation spaces demonstrate that the improvements were gained primarily by a disentangled feature space.",0
"Medical image segmentation has been exploring various deep learning architectures and objective functions to encode high-level features from volumetric data due to limited image annotations. However, most current approaches neglect cross-volume global context and define context relations in the decision space. This study proposes a novel voxel-level Siamese representation learning approach for abdominal multi-organ segmentation to enhance the representation space. The proposed method enforces voxel-wise feature relations in the representation space to better use limited datasets for improved performance. Inspired by recent advances in contrastive learning, we suppressed voxel-wise relations from the same class to be projected to the same point without negative samples. Additionally, we introduced a multi-resolution context aggregation method that encodes both global and local contexts for segmentation. Our experiments on the multi-organ dataset surpassed existing approaches by 2% in Dice score coefficient. The qualitative visualizations of the representation spaces show that the enhancements were mainly due to a disentangled feature space.",1
"We systematically evaluate the performance of deep learning models in the presence of diseases not labeled for or present during training. First, we evaluate whether deep learning models trained on a subset of diseases (seen diseases) can detect the presence of any one of a larger set of diseases. We find that models tend to falsely classify diseases outside of the subset (unseen diseases) as ""no disease"". Second, we evaluate whether models trained on seen diseases can detect seen diseases when co-occurring with diseases outside the subset (unseen diseases). We find that models are still able to detect seen diseases even when co-occurring with unseen diseases. Third, we evaluate whether feature representations learned by models may be used to detect the presence of unseen diseases given a small labeled set of unseen diseases. We find that the penultimate layer of the deep neural network provides useful features for unseen disease detection. Our results can inform the safe clinical deployment of deep learning models trained on a non-exhaustive set of disease classes.",0
"Our study involves a systematic assessment of deep learning models' performance when dealing with diseases not included in their training data. Our first evaluation involves testing whether models trained on a subset of diseases (known as ""seen diseases"") can identify any of the additional diseases (known as ""unseen diseases"") in a larger set. We discovered that models tend to misclassify unseen diseases as ""no disease."" In our second evaluation, we assessed whether models trained on seen diseases can still detect them when they appear alongside unseen diseases. We found that the models can still detect seen diseases even when co-existing with unseen diseases. In our third evaluation, we analyzed whether the models' features could identify unseen diseases with a small labeled set of them. We found that the penultimate layer of the deep neural network provides useful features for detecting unseen diseases. Our results can guide the safe clinical implementation of deep learning models trained on an incomplete collection of disease classes.",1
"Multiview data contain information from multiple modalities and have potentials to provide more comprehensive features for diverse machine learning tasks. A fundamental question in multiview analysis is what is the additional information brought by additional views and can quantitatively identify this additional information. In this work, we try to tackle this challenge by decomposing the entangled multiview features into shared latent representations that are common across all views and private representations that are specific to each single view. We formulate this feature disentanglement in the framework of information bottleneck and propose disentangled variational information bottleneck (DVIB). DVIB explicitly defines the properties of shared and private representations using constrains from mutual information. By deriving variational upper and lower bounds of mutual information terms, representations are efficiently optimized. We demonstrate the shared and private representations learned by DVIB well preserve the common labels shared between two views and unique labels corresponding to each single view, respectively. DVIB also shows comparable performance in classification task on images with corruptions. DVIB implementation is available at https://github.com/feng-bao-ucsf/DVIB.",0
"Multiview data incorporates information from multiple sources and has the potential to offer more comprehensive features for various machine learning tasks. A fundamental query in multiview analysis is identifying the additional information provided by each view and quantifying it. To address this challenge, this study decomposes multiview features into shared latent representations that are common across all views and private representations that are specific to each view. This feature disentanglement is formulated in the information bottleneck framework, and a disentangled variational information bottleneck (DVIB) is proposed. By constraining mutual information, DVIB explicitly defines the properties of shared and private representations. Variational upper and lower bounds of mutual information terms are derived to optimize the representations effectively. Shared and private representations learned by DVIB preserve the common labels shared between two views and unique labels corresponding to each view, respectively. Furthermore, DVIB shows comparable performance in classification tasks on images with corruptions. The implementation of DVIB is available at https://github.com/feng-bao-ucsf/DVIB.",1
"Due to its powerful capability of representation learning and high-efficiency computation, deep hashing has made significant progress in large-scale image retrieval. However, deep hashing networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge, this is the first generation-based method to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e., a PrototypeNet, a generator, and a discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for a flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator is against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments verify that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing. The related codes could be available at https://github.com/xunguangwang/ProS-GAN .",0
"Significant advances have been made in large-scale image retrieval with deep hashing due to its powerful capability of representation learning and high-efficiency computation. However, the vulnerability of deep hashing networks to adversarial examples is a practical security concern that has been largely unexplored in the hashing-based retrieval field. This paper presents a novel prototype-supervised adversarial network (ProS-GAN) that offers a flexible generative architecture for efficient and effective targeted hashing attack. To our knowledge, this is the first generation-based method for attacking deep hashing networks. The proposed framework comprises a PrototypeNet, a generator, and a discriminator. The PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. The generator then constructs the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. The generator is also against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments demonstrate that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability than state-of-the-art targeted attack methods of deep hashing. The related codes are available at https://github.com/xunguangwang/ProS-GAN.",1
"In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments. Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others. We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks. Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation. Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at https://github.com/astooke/rlpyt/tree/master/rlpyt/ul.",0
"To overcome the limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we suggest separating representation learning from policy learning. To achieve this, we propose a new unsupervised learning (UL) task called Augmented Temporal Contrast (ATC). This task trains a convolutional encoder to connect pairs of observations separated by a short time difference, using image augmentations and a contrastive loss. Our online RL experiments reveal that training the encoder exclusively with ATC performs equally or better than end-to-end RL in most environments. Additionally, we compare several leading UL algorithms by pre-training encoders on expert demonstrations and find that agents using ATC-trained encoders outperform others. We also exhibit the generalization of multi-task encoders to different downstream RL tasks. Finally, we analyze the different components of ATC and introduce a new data augmentation to facilitate replay of compressed latent images from pre-trained encoders when RL requires augmentation. Our experiments cover visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and the complete code is open-source and accessible at https://github.com/astooke/rlpyt/tree/master/rlpyt/ul.",1
"Automated segmentation in medical image analysis is a challenging task that requires a large amount of manually labeled data. However, manually annotating medical data is often laborious, and most existing learning-based approaches fail to accurately delineate object boundaries without effective geometric constraints. Contrastive learning, a sub-area of self-supervised learning, has recently been noted as a promising direction in multiple application fields. In this work, we present a novel Contrastive Voxel-wise Representation Learning (CVRL) method with geometric constraints to learn global-local visual representations for volumetric medical image segmentation with limited annotations. Our framework can effectively learn global and local features by capturing 3D spatial context and rich anatomical information. Specifically, we introduce a voxel-to-volume contrastive algorithm to learn global information from 3D images, and propose to perform local voxel-to-voxel contrast to explicitly make use of local cues in the embedding space. Moreover, we integrate an elastic interaction-based active contour model as a geometric regularization term to enable fast and reliable object delineations in an end-to-end learning manner. Results on the Atrial Segmentation Challenge dataset demonstrate superiority of our proposed scheme, especially in a setting with a very limited number of annotated data.",0
"The task of automated segmentation in medical image analysis is difficult, requiring a significant amount of manually labeled data. However, manual annotation of medical data can be tedious, and current learning-based approaches often struggle to accurately delineate object boundaries without effective geometric constraints. Contrastive learning, a subfield of self-supervised learning, is being recognized as a promising approach in multiple application areas. In this study, we introduce a novel method called Contrastive Voxel-wise Representation Learning (CVRL) with geometric constraints to learn global-local visual representations for volumetric medical image segmentation with limited annotations. Our framework captures 3D spatial context and rich anatomical information to effectively learn global and local features. We propose a voxel-to-volume contrastive algorithm to learn global information from 3D images and perform local voxel-to-voxel contrast to take advantage of local cues in the embedding space. Additionally, we integrate an elastic interaction-based active contour model as a geometric regularization term to facilitate fast and reliable object delineations in an end-to-end learning manner. Our proposed scheme outperforms existing approaches, especially in a setting with a very limited number of annotated data, as demonstrated on the Atrial Segmentation Challenge dataset.",1
"Most of the current self-supervised representation learning (SSL) methods are based on the contrastive loss and the instance-discrimination task, where augmented versions of the same image instance (""positives"") are contrasted with instances extracted from other images (""negatives""). For the learning to be effective, many negatives should be compared with a positive pair, which is computationally demanding. In this paper, we propose a different direction and a new loss function for SSL, which is based on the whitening of the latent-space features. The whitening operation has a ""scattering"" effect on the batch samples, avoiding degenerate solutions where all the sample representations collapse to a single point. Our solution does not require asymmetric networks and it is conceptually simple. Moreover, since negatives are not needed, we can extract multiple positive pairs from the same image instance. The source code of the method and of all the experiments is available at: https://github.com/htdt/self-supervised.",0
"Currently, most methods for self-supervised representation learning (SSL) rely on the instance-discrimination task and contrastive loss. This involves contrasting augmented versions of the same image instance (""positives"") with instances from other images (""negatives""). To ensure effective learning, a large number of negatives must be compared to a positive pair, which can be computationally intensive. However, this paper proposes a novel approach to SSL that utilizes a different loss function based on the whitening of latent-space features. This method avoids degenerate solutions that cause all sample representations to collapse into a single point by scattering batch samples. Additionally, this method does not require asymmetric networks and enables the extraction of multiple positive pairs from a single image instance since negatives are not required. The source code for this method and all experiments can be found at: https://github.com/htdt/self-supervised.",1
"Recently, maximizing mutual information has emerged as a powerful method for unsupervised graph representation learning. The existing methods are typically effective to capture information from the topology view but ignore the feature view. To circumvent this issue, we propose a novel approach by exploiting mutual information maximization across feature and topology views. Specifically, we first utilize a multi-view representation learning module to better capture both local and global information content across feature and topology views on graphs. To model the information shared by the feature and topology spaces, we then develop a common representation learning module using mutual information maximization and reconstruction loss minimization. To explicitly encourage diversity between graph representations from the same view, we also introduce a disagreement regularization to enlarge the distance between representations from the same view. Experiments on synthetic and real-world datasets demonstrate the effectiveness of integrating feature and topology views. In particular, compared with the previous supervised methods, our proposed method can achieve comparable or even better performance under the unsupervised representation and linear evaluation protocol.",0
"Unsupervised graph representation learning has found a powerful method in maximizing mutual information. However, existing methods tend to overlook the feature view while focusing on the topology view. To address this, we present a new approach that exploits mutual information maximization across both feature and topology views. Our method involves a multi-view representation learning module that captures local and global information content across both views. We also develop a common representation learning module that models the information shared by the feature and topology spaces using mutual information maximization and reconstruction loss minimization. To promote diversity between graph representations from the same view, we introduce a disagreement regularization that increases the distance between such representations. Our experiments on synthetic and real-world datasets demonstrate the effectiveness of integrating both feature and topology views. Compared to previous supervised methods, our proposed method achieves comparable or even better performance under the unsupervised representation and linear evaluation protocol.",1
"Recent works have advanced the performance of self-supervised representation learning by a large margin. The core among these methods is intra-image invariance learning. Two different transformations of one image instance are considered as a positive sample pair, where various tasks are designed to learn invariant representations by comparing the pair. Analogically, for video data, representations of frames from the same video are trained to be closer than frames from other videos, i.e. intra-video invariance. However, cross-video relation has barely been explored for visual representation learning. Unlike intra-video invariance, ground-truth labels of cross-video relation is usually unavailable without human labors. In this paper, we propose a novel contrastive learning method which explores the cross-video relation by using cycle-consistency for general image representation learning. This allows to collect positive sample pairs across different video instances, which we hypothesize will lead to higher-level semantics. We validate our method by transferring our image representation to multiple downstream tasks including visual object tracking, image classification, and action recognition. We show significant improvement over state-of-the-art contrastive learning methods. Project page is available at https://happywu.github.io/cycle_contrast_video.",0
"Significant progress has been made in self-supervised representation learning, particularly in intra-image invariance learning. This involves considering two different transformations of an image as a positive sample pair and comparing them to learn invariant representations. Similarly, for video data, intra-video invariance is achieved by training representations of frames from the same video to be closer than frames from other videos. However, cross-video relation has not been explored much for visual representation learning, as ground-truth labels are usually unavailable. In this study, we propose a novel contrastive learning method that uses cycle-consistency to explore the cross-video relation for general image representation learning. This enables us to collect positive sample pairs across different video instances, which we believe will lead to higher-level semantics. Our method outperforms state-of-the-art contrastive learning methods in multiple downstream tasks such as visual object tracking, image classification, and action recognition. Visit our project page at https://happywu.github.io/cycle_contrast_video.",1
"We first pose the Unsupervised Progressive Learning (UPL) problem: an online representation learning problem in which the learner observes a non-stationary and unlabeled data stream, learning a growing number of features that persist over time even though the data is not stored or replayed. To solve the UPL problem we propose the Self-Taught Associative Memory (STAM) architecture. Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical features rather than specific examples. We evaluate STAM representations using clustering and classification tasks. While there are no existing learning scenarios that are directly comparable to UPL, we compare the STAM architecture with two recent continual learning models, Memory Aware Synapses (MAS) and Gradient Episodic Memories (GEM), after adapting them in the UPL setting.",0
"The Unsupervised Progressive Learning (UPL) problem is introduced as a challenge of online representation learning. The task involves observing an unlabeled and non-stationary data stream and acquiring a growing number of features that remain relevant over time, without the need to store or replay the data. To address this problem, we propose the Self-Taught Associative Memory (STAM) architecture, which consists of layered hierarchies of modules that learn through online clustering, novelty detection, outlier removal, and prototypical feature storage. We assess the effectiveness of STAM representations by testing them in clustering and classification tasks. Although UPL has no direct comparison scenarios, we compare the performance of STAM with two recent continual learning models, Memory Aware Synapses (MAS) and Gradient Episodic Memories (GEM), by adapting them to the UPL setting.",1
"Knowledge Graphs (KGs) are ubiquitous structures for information storagein several real-world applications such as web search, e-commerce, social networks, and biology. Querying KGs remains a foundational and challenging problem due to their size and complexity. Promising approaches to tackle this problem include embedding the KG units (e.g., entities and relations) in a Euclidean space such that the query embedding contains the information relevant to its results. These approaches, however, fail to capture the hierarchical nature and semantic information of the entities present in the graph. Additionally, most of these approaches only utilize multi-hop queries (that can be modeled by simple translation operations) to learn embeddings and ignore more complex operations such as intersection and union of simpler queries. To tackle such complex operations, in this paper, we formulate KG representation learning as a self-supervised logical query reasoning problem that utilizes translation, intersection and union queries over KGs. We propose Hyperboloid Embeddings (HypE), a novel self-supervised dynamic reasoning framework, that utilizes positive first-order existential queries on a KG to learn representations of its entities and relations as hyperboloids in a Poincar\'e ball. HypE models the positive first-order queries as geometrical translation, intersection, and union. For the problem of KG reasoning in real-world datasets, the proposed HypE model significantly outperforms the state-of-the art results. We also apply HypE to an anomaly detection task on a popular e-commerce website product taxonomy as well as hierarchically organized web articles and demonstrate significant performance improvements compared to existing baseline methods. Finally, we also visualize the learned HypE embeddings in a Poincar\'e ball to clearly interpret and comprehend the representation space.",0
"Knowledge Graphs (KGs) are used extensively for storing information in various real-world applications including web search, e-commerce, social networks, and biology. However, querying KGs is a challenging problem due to their complexity and size. Current approaches embed KG units such as entities and relations in a Euclidean space, but they fail to capture the hierarchical nature and semantic information of entities in the graph. Additionally, these approaches only consider multi-hop queries and ignore more complex operations like intersection and union of simpler queries. To address these challenges, this paper formulates KG representation learning as a self-supervised logical query reasoning problem using translation, intersection, and union queries over KGs. The proposed Hyperboloid Embeddings (HypE) model utilizes positive first-order existential queries on a KG and learns representations of its entities and relations as hyperboloids in a Poincar\'e ball. HypE significantly outperforms the state-of-the-art results in KG reasoning in real-world datasets and is also applied to an anomaly detection task on a popular e-commerce website product taxonomy and hierarchically organized web articles, demonstrating significant performance improvements over existing baseline methods. Finally, the paper visualizes the learned HypE embeddings in a Poincar\'e ball to enable clear interpretation and comprehension of the representation space.",1
"Recent self-supervised representation learning techniques have largely closed the gap between supervised and unsupervised learning on ImageNet classification. While the particulars of pretraining on ImageNet are now relatively well understood, the field still lacks widely accepted best practices for replicating this success on other datasets. As a first step in this direction, we study contrastive self-supervised learning on four diverse large-scale datasets. By looking through the lenses of data quantity, data domain, data quality, and task granularity, we provide new insights into the necessary conditions for successful self-supervised learning. Our key findings include observations such as: (i) the benefit of additional pretraining data beyond 500k images is modest, (ii) adding pretraining images from another domain does not lead to more general representations, (iii) corrupted pretraining images have a disparate impact on supervised and self-supervised pretraining, and (iv) contrastive learning lags far behind supervised learning on fine-grained visual classification tasks.",0
"Self-supervised representation learning methods have made significant progress in bridging the gap between supervised and unsupervised learning for ImageNet classification. However, while the process of pretraining on ImageNet is now well-understood, there is still no universally accepted approach for replicating this success on other datasets. To address this, we conducted a study on four diverse large-scale datasets focusing on data quantity, domain, quality, and task granularity. Our findings suggest that additional pretraining data beyond 500k images has only a modest impact, pretraining with images from different domains does not lead to more general representations, and corrupted pretraining images have a differential effect on supervised and self-supervised pretraining. Furthermore, contrastive learning is not as effective as supervised learning for fine-grained visual classification tasks.",1
"Digital pathology tasks have benefited greatly from modern deep learning algorithms. However, their need for large quantities of annotated data has been identified as a key challenge. This need for data can be countered by using unsupervised learning in situations where data are abundant but access to annotations is limited. Feature representations learned from unannotated data using contrastive predictive coding (CPC) have been shown to enable classifiers to obtain state of the art performance from relatively small amounts of annotated computer vision data. We present a modification to the CPC framework for use with digital pathology patches. This is achieved by introducing an alternative mask for building the latent context and using a multi-directional PixelCNN autoregressor. To demonstrate our proposed method we learn feature representations from the Patch Camelyon histology dataset. We show that our proposed modification can yield improved deep classification of histology patches.",0
"Modern deep learning algorithms have greatly benefited digital pathology tasks, but the challenge of needing a large amount of annotated data has been identified. To overcome this challenge, unsupervised learning can be used in situations where there is an abundance of data but limited access to annotations. Contrastive predictive coding (CPC) can learn feature representations from unannotated data, enabling classifiers to achieve state-of-the-art performance with relatively small amounts of annotated computer vision data. We have modified the CPC framework to be used with digital pathology patches by introducing an alternative mask for building the latent context and using a multi-directional PixelCNN autoregressor. Our proposed method was demonstrated by learning feature representations from the Patch Camelyon histology dataset, which showed improved deep classification of histology patches.",1
"Existing research on action recognition treats activities as monolithic events occurring in videos. Recently, the benefits of formulating actions as a combination of atomic-actions have shown promise in improving action understanding with the emergence of datasets containing such annotations, allowing us to learn representations capturing this information. However, there remains a lack of studies that extend action composition and leverage multiple viewpoints and multiple modalities of data for representation learning. To promote research in this direction, we introduce Home Action Genome (HOMAGE): a multi-view action dataset with multiple modalities and view-points supplemented with hierarchical activity and atomic action labels together with dense scene composition labels. Leveraging rich multi-modal and multi-view settings, we propose Cooperative Compositional Action Understanding (CCAU), a cooperative learning framework for hierarchical action recognition that is aware of compositional action elements. CCAU shows consistent performance improvements across all modalities. Furthermore, we demonstrate the utility of co-learning compositions in few-shot action recognition by achieving 28.6% mAP with just a single sample.",0
"Previous studies on action recognition consider activities as singular events that occur in videos. However, recent research has shown that breaking down actions into atomic-actions can enhance action comprehension, especially with the availability of datasets containing such annotations. Despite this progress, there is still a lack of research that explores action composition across multiple viewpoints and modalities. To encourage further investigation in this area, we present the Home Action Genome (HOMAGE) dataset, which includes multiple modalities and viewpoints, as well as hierarchical activity and atomic action labels and dense scene composition labels. By utilizing this rich dataset, we propose the Cooperative Compositional Action Understanding (CCAU) framework for hierarchical action recognition that takes into account compositional action elements. CCAU performs consistently well across all modalities and demonstrates its effectiveness in few-shot action recognition with a 28.6% mAP achieved using just one sample.",1
"We introduce a weakly supervised method for representation learning based on aligning temporal sequences (e.g., videos) of the same process (e.g., human action). The main idea is to use the global temporal ordering of latent correspondences across sequence pairs as a supervisory signal. In particular, we propose a loss based on scoring the optimal sequence alignment to train an embedding network. Our loss is based on a novel probabilistic path finding view of dynamic time warping (DTW) that contains the following three key features: (i) the local path routing decisions are contrastive and differentiable, (ii) pairwise distances are cast as probabilities that are contrastive as well, and (iii) our formulation naturally admits a global cycle consistency loss that verifies correspondences. For evaluation, we consider the tasks of fine-grained action classification, few shot learning, and video synchronization. We report significant performance increases over previous methods. In addition, we report two applications of our temporal alignment framework, namely 3D pose reconstruction and fine-grained audio/visual retrieval.",0
"A technique for representation learning using weak supervision is presented in this study. The method involves aligning temporal sequences of the same process, such as human actions in videos, to generate a supervisory signal based on global temporal ordering of latent correspondences across sequence pairs. To train an embedding network, a novel loss function is proposed that scores the optimal sequence alignment. Dynamic time warping (DTW) is used to formulate the loss function, which includes three key features: (i) differentiable and contrastive local path routing decisions, (ii) contrastive probabilities for pairwise distances, and (iii) a global cycle consistency loss that verifies correspondences. The proposed method is evaluated using fine-grained action classification, few shot learning, and video synchronization tasks, and significant performance improvements compared to previous methods are reported. Additionally, the method is applied to 3D pose reconstruction and fine-grained audio/visual retrieval using a temporal alignment framework.",1
"Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.",0
"Self-supervised methods for learning image representations have recently relied on maximizing the agreement between embedding vectors from different views of the same image. However, a potential issue arises when the encoder produces constant vectors, leading to a trivial solution. Although implicit biases in the learning architecture can prevent this collapse problem, they often lack clear justification or interpretation. To address this, our paper proposes VICReg (Variance-Invariance-Covariance Regularization), which explicitly avoids the collapse problem by introducing a simple regularization term on the variance of the embeddings along each dimension. VICReg also includes a decorrelation mechanism based on redundancy reduction and covariance regularization. Our method achieves results on par with the state of the art on several downstream tasks and incorporating our variance term into other methods stabilizes training and improves performance.",1
"Representation learning of static and more recently dynamically evolving graphs has gained noticeable attention. Existing approaches for modelling graph dynamics focus extensively on the evolution of individual nodes independently of the evolution of mesoscale community structures. As a result, current methods do not provide useful tools to study and cannot explicitly capture temporal community dynamics. To address this challenge, we propose GRADE - a probabilistic model that learns to generate evolving node and community representations by imposing a random walk prior over their trajectories. Our model also learns node community membership which is updated between time steps via a transition matrix. At each time step link generation is performed by first assigning node membership from a distribution over the communities, and then sampling a neighbor from a distribution over the nodes for the assigned community. We parametrize the node and community distributions with neural networks and learn their parameters via variational inference. Experiments demonstrate GRADE outperforms baselines in dynamic link prediction, shows favourable performance on dynamic community detection, and identifies coherent and interpretable evolving communities.",0
"There has been a significant increase in interest in representation learning for static and, more recently, dynamically evolving graphs. However, current approaches to modelling graph dynamics focus primarily on the evolution of individual nodes, neglecting the evolution of mesoscale community structures. This results in a lack of useful tools for studying and explicitly capturing temporal community dynamics. To address this challenge, we propose GRADE, a probabilistic model that generates evolving node and community representations by imposing a random walk prior over their trajectories. Our model also learns node community membership, which is updated between time steps using a transition matrix. At each time step, link generation is performed by assigning node membership from a community distribution and sampling a neighbor from a node distribution for the assigned community. We use neural networks to parametrize the node and community distributions and learn their parameters through variational inference. Our experiments show that GRADE outperforms baseline methods in dynamic link prediction, performs well in dynamic community detection, and identifies coherent and interpretable evolving communities.",1
"We present PSEUDo, an adaptive feature learning technique for exploring visual patterns in multi-track sequential data. Our approach is designed with the primary focus to overcome the uneconomic retraining requirements and inflexible representation learning in current deep learning-based systems. Multi-track time series data are generated on an unprecedented scale due to increased sensors and data storage. These datasets hold valuable patterns, like in neuromarketing, where researchers try to link patterns in multivariate sequential data from physiological sensors to the purchase behavior of products and services. But a lack of ground truth and high variance make automatic pattern detection unreliable. Our advancements are based on a novel query-aware locality-sensitive hashing technique to create a feature-based representation of multivariate time series windows. Most importantly, our algorithm features sub-linear training and inference time. We can even accomplish both the modeling and comparison of 10,000 different 64-track time series, each with 100 time steps (a typical EEG dataset) under 0.8 seconds. This performance gain allows for a rapid relevance feedback-driven adaption of the underlying pattern similarity model and enables the user to modify the speed-vs-accuracy trade-off gradually. We demonstrate superiority of PSEUDo in terms of efficiency, accuracy, and steerability through a quantitative performance comparison and a qualitative visual quality comparison to the state-of-the-art algorithms in the field. Moreover, we showcase the usability of PSEUDo through a case study demonstrating our visual pattern retrieval concepts in a large meteorological dataset. We find that our adaptive models can accurately capture the user's notion of similarity and allow for an understandable exploratory visual pattern retrieval in large multivariate time series datasets.",0
"PSEUDo is a technique for adaptive feature learning that explores visual patterns in multi-track sequential data. Our approach aims to overcome the limitations of current deep learning systems, which require uneconomical retraining and inflexible representation learning. With the increasing availability of sensors and data storage, multi-track time series data are generated on an unprecedented scale. These datasets contain valuable patterns, such as in neuromarketing, where researchers try to link physiological sensor patterns to purchase behavior. However, automatic pattern detection is unreliable due to a lack of ground truth and high variance. Our algorithm uses a novel query-aware locality-sensitive hashing technique to create a feature-based representation of multivariate time series windows, with sub-linear training and inference time. Our performance gain allows for rapid relevance feedback-driven adaption of the pattern similarity model, enabling the user to modify the speed-vs-accuracy trade-off gradually. We demonstrate the superiority of PSEUDo in terms of efficiency, accuracy, and steerability through a quantitative performance comparison and a qualitative visual quality comparison to state-of-the-art algorithms. We also showcase the usability of PSEUDo through a case study demonstrating our visual pattern retrieval concepts in a large meteorological dataset. Our adaptive models accurately capture the user's notion of similarity and allow for an understandable exploratory visual pattern retrieval in large multivariate time series datasets.",1
"Scene text recognition is a challenging task due to diverse variations of text instances in natural scene images. Conventional methods based on CNN-RNN-CTC or encoder-decoder with attention mechanism may not fully investigate stable and efficient feature representations for multi-oriented scene texts. In this paper, we propose a primitive representation learning method that aims to exploit intrinsic representations of scene text images. We model elements in feature maps as the nodes of an undirected graph. A pooling aggregator and a weighted aggregator are proposed to learn primitive representations, which are transformed into high-level visual text representations by graph convolutional networks. A Primitive REpresentation learning Network (PREN) is constructed to use the visual text representations for parallel decoding. Furthermore, by integrating visual text representations into an encoder-decoder model with the 2D attention mechanism, we propose a framework called PREN2D to alleviate the misalignment problem in attention-based methods. Experimental results on both English and Chinese scene text recognition tasks demonstrate that PREN keeps a balance between accuracy and efficiency, while PREN2D achieves state-of-the-art performance.",0
"Recognizing text in natural scene images is a difficult task due to the various forms of text instances. Traditional CNN-RNN-CTC or encoder-decoder with attention methods may not provide stable and effective feature representations for multi-oriented scene texts. This study proposes a method for learning primitive representations that seeks to leverage the intrinsic properties of scene text images. The feature maps' elements are modeled as undirected graph nodes, and a pooling aggregator and weighted aggregator are introduced to learn primitive representations. The graph convolutional networks are used to transform the primitive representations into high-level visual text representations. A Primitive REpresentation learning Network (PREN) is constructed to decode the visual text representations in parallel. Additionally, the visual text representations are integrated into an encoder-decoder model with the 2D attention mechanism, resulting in a framework named PREN2D that addresses the misalignment problem. The experimental results on English and Chinese scene text recognition tasks demonstrate that PREN achieves an excellent balance between accuracy and efficiency, while PREN2D achieves state-of-the-art performance.",1
"Effective feature representation is key to the predictive performance of any algorithm. This paper introduces a meta-procedure, called Non-Euclidean Upgrading (NEU), which learns feature maps that are expressive enough to embed the universal approximation property (UAP) into most model classes while only outputting feature maps that preserve any model class's UAP. We show that NEU can learn any feature map with these two properties if that feature map is asymptotically deformable into the identity. We also find that the feature-representations learned by NEU are always submanifolds of the feature space. NEU's properties are derived from a new deep neural model that is universal amongst all orientation-preserving homeomorphisms on the input space. We derive qualitative and quantitative approximation guarantees for this architecture. We quantify the number of parameters required for this new architecture to memorize any set of input-output pairs while simultaneously fixing every point of the input space lying outside some compact set, and we quantify the size of this set as a function of our model's depth. Moreover, we show that no deep feed-forward network with commonly used activation function has all these properties. NEU's performance is evaluated against competing machine learning methods on various regression and dimension reduction tasks both with financial and simulated data.",0
"The success of an algorithm in making predictions relies heavily on its ability to effectively represent features. This study presents a meta-procedure called Non-Euclidean Upgrading (NEU), which teaches feature maps that are expressive enough to incorporate the universal approximation property (UAP) into most model classes while retaining the UAP of any model class's feature maps. It has been demonstrated that NEU can learn any feature map possessing these two properties provided that the feature map can be asymptotically transformed into the identity. The feature representations attained through NEU are consistently submanifolds of the feature space. NEU's attributes are derived from a novel deep neural model that is universal among all orientation-preserving homeomorphisms on the input space. We have calculated the qualitative and quantitative approximation guarantees for this architecture. We have also determined the number of parameters required for this new architecture to memorize any set of input-output pairs while simultaneously fixing every point of the input space lying outside some compact set, and we have determined the size of this set as a function of our model's depth. In addition, we have demonstrated that no deep feed-forward network with commonly used activation function possesses all of these properties. NEU's effectiveness has been tested against other machine learning techniques in various regression and dimension reduction tasks, both with financial and simulated data.",1
"This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.",0
"The objective of this paper is to present E(n)-Equivariant Graph Neural Networks (EGNNs), a fresh approach to learning graph neural networks that are equivariant to rotations, translations, reflections, and permutations. Unlike other methods, our technique does not necessitate computationally expensive higher-order representations in intermediate layers, yet it yields comparable or superior results. Furthermore, while existing methods are confined to equivariance on 3D spaces, our model can be effortlessly extended to higher-dimensional spaces. We showcase the performance of our approach on various tasks such as dynamical systems modeling, graph autoencoder representation learning, and molecular property prediction.",1
"Within Convolutional Neural Network (CNN), the convolution operations are good at extracting local features but experience difficulty to capture global representations. Within visual transformer, the cascaded self-attention modules can capture long-distance feature dependencies but unfortunately deteriorate local feature details. In this paper, we propose a hybrid network structure, termed Conformer, to take advantage of convolutional operations and self-attention mechanisms for enhanced representation learning. Conformer roots in the Feature Coupling Unit (FCU), which fuses local features and global representations under different resolutions in an interactive fashion. Conformer adopts a concurrent structure so that local features and global representations are retained to the maximum extent. Experiments show that Conformer, under the comparable parameter complexity, outperforms the visual transformer (DeiT-B) by 2.3% on ImageNet. On MSCOCO, it outperforms ResNet-101 by 3.7% and 3.6% mAPs for object detection and instance segmentation, respectively, demonstrating the great potential to be a general backbone network. Code is available at https://github.com/pengzhiliang/Conformer.",0
"The convolution operations in Convolutional Neural Network (CNN) are effective in extracting local features, but struggle to capture global representations. On the other hand, the cascaded self-attention modules in visual transformer can capture long-distance feature dependencies but may compromise local feature details. To address these limitations, we introduce Conformer, a hybrid network structure that leverages both convolutional operations and self-attention mechanisms for improved representation learning. Conformer is based on the Feature Coupling Unit (FCU), which interactively combines local features and global representations of different resolutions. The concurrent structure of Conformer ensures that local features and global representations are both maximally retained. Our experiments demonstrate that, with comparable parameter complexity, Conformer outperforms visual transformer (DeiT-B) by 2.3% on ImageNet, and ResNet-101 by 3.7% and 3.6% mAPs for object detection and instance segmentation, respectively, indicating its potential as a general backbone network. The code for Conformer is available at https://github.com/pengzhiliang/Conformer.",1
"Graph Convolutional Networks (GCNs) are powerful models for node representation learning tasks. However, the node representation in existing GCN models is usually generated by performing recursive neighborhood aggregation across multiple graph convolutional layers with certain sampling methods, which may lead to redundant feature mixing, needless information loss, and extensive computations. Therefore, in this paper, we propose a novel architecture named Non-Recursive Graph Convolutional Network (NRGCN) to improve both the training efficiency and the learning performance of GCNs in the context of node classification. Specifically, NRGCN proposes to represent different hops of neighbors for each node based on inner-layer aggregation and layer-independent sampling. In this way, each node can be directly represented by concatenating the information extracted independently from each hop of its neighbors thereby avoiding the recursive neighborhood expansion across layers. Moreover, the layer-independent sampling and aggregation can be precomputed before the model training, thus the training process can be accelerated considerably. Extensive experiments on benchmark datasets verify that our NRGCN outperforms the state-of-the-art GCN models, in terms of the node classification performance and reliability.",0
"Node representation learning tasks can benefit greatly from Graph Convolutional Networks (GCNs). However, existing GCN models generate node representation through recursive neighborhood aggregation across multiple graph convolutional layers, along with certain sampling methods. This approach can lead to redundant feature mixing, information loss, and extensive computations. Therefore, we introduce a new architecture called Non-Recursive Graph Convolutional Network (NRGCN), which enhances the training efficiency and learning performance of GCNs in node classification. NRGCN employs inner-layer aggregation and layer-independent sampling to represent different hops of neighbors for each node. This enables each node to be represented directly by concatenating independent information extracted from each hop of its neighbors, avoiding recursive neighborhood expansion across layers. Additionally, precomputed layer-independent sampling and aggregation accelerate the training process considerably. Our experiments on benchmark datasets demonstrate that NRGCN outperforms state-of-the-art GCN models in terms of node classification performance and reliability.",1
"One of the fundamental challenges in reinforcement learning (RL) is the one of data efficiency: modern algorithms require a very large number of training samples, especially compared to humans, for solving environments with high-dimensional observations. The severity of this problem is increased when the reward signal is sparse. In this work, we propose learning a state representation in a self-supervised manner for reward prediction. The reward predictor learns to estimate either a raw or a smoothed version of the true reward signal in environment with a single, terminating, goal state. We augment the training of out-of-the-box RL agents by shaping the reward using our reward predictor during policy learning. Using our representation for preprocessing high-dimensional observations, as well as using the predictor for reward shaping, is shown to significantly enhance Actor Critic using Kronecker-factored Trust Region and Proximal Policy Optimization in single-goal environments with visual inputs.",0
"Reinforcement learning (RL) faces a major obstacle in terms of data efficiency, requiring a large number of training samples, which surpasses human capabilities, to solve environments with high-dimensional observations, especially if the reward signal is sparse. Our proposed solution in this study involves self-supervised state representation learning for reward prediction. The reward predictor estimates the raw or smoothed version of the true reward signal in an environment with a single, terminating, goal state. We enhance the training of RL agents by incorporating our reward predictor in policy learning to shape the reward. Our representation approach for processing high-dimensional observations and the predictor for reward shaping significantly improve Actor Critic using Kronecker-factored Trust Region and Proximal Policy Optimization in single-goal environments with visual inputs.",1
"We present a representation learning framework for financial time series forecasting. One challenge of using deep learning models for finance forecasting is the shortage of available training data when using small datasets. Direct trend classification using deep neural networks trained on small datasets is susceptible to the overfitting problem. In this paper, we propose to first learn compact representations from time series data, then use the learned representations to train a simpler model for predicting time series movements. We consider a class-conditioned latent variable model. We train an encoder network to maximize the mutual information between the latent variables and the trend information conditioned on the encoded observed variables. We show that conditional mutual information maximization can be approximated by a contrastive loss. Then, the problem is transformed into a classification task of determining whether two encoded representations are sampled from the same class or not. This is equivalent to performing pairwise comparisons of the training datapoints, and thus, improves the generalization ability of the encoder network. We use deep autoregressive models as our encoder to capture long-term dependencies of the sequence data. Empirical experiments indicate that our proposed method has the potential to advance state-of-the-art performance.",0
"Our article introduces a framework for representation learning that can improve financial time series forecasting. Small datasets can present a challenge when using deep learning techniques for finance forecasting due to the risk of overfitting. We suggest a two-step approach: first, learning compact representations from time series data and then using these to train a simpler model for predicting time series movements. We propose a class-conditioned latent variable model and train an encoder network to maximize the mutual information between the latent variables and the trend information. We demonstrate this can be achieved using contrastive loss, which transforms the problem into a classification task. Pairwise comparisons of training datapoints improve the generalization ability of the encoder network. Deep autoregressive models are used as our encoder to capture long-term dependencies of sequence data. Empirical experiments indicate that our proposed method could enhance state-of-the-art performance.",1
"The goal of this work is to temporally align asynchronous subtitles in sign language videos. In particular, we focus on sign-language interpreted TV broadcast data comprising (i) a video of continuous signing, and (ii) subtitles corresponding to the audio content. Previous work exploiting such weakly-aligned data only considered finding keyword-sign correspondences, whereas we aim to localise a complete subtitle text in continuous signing. We propose a Transformer architecture tailored for this task, which we train on manually annotated alignments covering over 15K subtitles that span 17.7 hours of video. We use BERT subtitle embeddings and CNN video representations learned for sign recognition to encode the two signals, which interact through a series of attention layers. Our model outputs frame-level predictions, i.e., for each video frame, whether it belongs to the queried subtitle or not. Through extensive evaluations, we show substantial improvements over existing alignment baselines that do not make use of subtitle text embeddings for learning. Our automatic alignment model opens up possibilities for advancing machine translation of sign languages via providing continuously synchronized video-text data.",0
"The aim of this study is to synchronize subtitles in sign language videos that are not temporally aligned. The study specifically focuses on TV broadcasts that include continuous signing video and corresponding audio subtitles. Previous research focused only on finding keyword-sign matches in such weakly-aligned data. However, this study aims to locate complete subtitle texts in continuous signing. To achieve this goal, the study proposes a Transformer architecture that is trained on more than 15K manually annotated alignments covering 17.7 hours of video. The two signals, BERT subtitle embeddings, and CNN video representations learned for sign recognition are encoded and interact through attention layers. The model provides frame-level predictions, indicating whether each video frame belongs to the queried subtitle or not. The study shows significant improvement compared to existing alignment baselines that do not use subtitle text embeddings for learning. The automatic alignment model has the potential to advance machine translation of sign languages by providing synchronized video-text data.",1
"We present a novel LSTM cell architecture capable of learning both intra- and inter-perspective relationships available in visual sequences captured from multiple perspectives. Our architecture adopts a novel recurrent joint learning strategy that uses additional gates and memories at the cell level. We demonstrate that by using the proposed cell to create a network, more effective and richer visual representations are learned for recognition tasks. We validate the performance of our proposed architecture in the context of two multi-perspective visual recognition tasks namely lip reading and face recognition. Three relevant datasets are considered and the results are compared against fusion strategies, other existing multi-input LSTM architectures, and alternative recognition solutions. The experiments show the superior performance of our solution over the considered benchmarks, both in terms of recognition accuracy and complexity. We make our code publicly available at https://github.com/arsm/MPLSTM.",0
"Our study introduces a cutting-edge LSTM cell structure that can acquire intra- and inter-perspective connections from visual sequences captured from multiple viewpoints. Our design employs an innovative recurrent joint learning method that employs additional gates and memories at the cell level. By utilizing our suggested cell to construct a network, we demonstrate that more efficient and deeper visual representations can be learned for identification tasks. We evaluate the effectiveness of our architecture in the context of two multi-perspective visual identification tasks, specifically lip reading and face recognition, using three pertinent datasets. We compare the results with fusion techniques, other existing multi-input LSTM architectures, and other identification methods. Our experiments show that our solution outperforms the compared benchmarks in terms of recognition accuracy and complexity. The code is available for public use at https://github.com/arsm/MPLSTM.",1
"We propose a highly data-efficient classification and active learning framework for classifying chest X-rays. It is based on (1) unsupervised representation learning of a Convolutional Neural Network and (2) the Gaussian Process method. The unsupervised representation learning employs self-supervision that does not require class labels, and the learned features are proven to achieve label-efficient classification. GP is a kernel-based Bayesian approach that also leads to data-efficient predictions with the added benefit of estimating each decision's uncertainty. Our novel framework combines these two elements in sequence to achieve highly data and label efficient classifications. Moreover, both elements are less sensitive to the prevalent and challenging class imbalance issue, thanks to the (1) feature learned without labels and (2) the Bayesian nature of GP. The GP-provided uncertainty estimates enable active learning by ranking samples based on the uncertainty and selectively labeling samples showing higher uncertainty. We apply this novel combination to the data-deficient and severely imbalanced case of COVID-19 chest X-ray classification. We demonstrate that only $\sim 10\%$ of the labeled data is needed to reach the accuracy from training all available labels. Its application to the COVID-19 data in a fully supervised classification scenario shows that our model, with a generic ResNet backbone, outperforms (COVID-19 case by 4\%) the state-of-the-art model with a highly tuned architecture. Our model architecture and proposed framework are general and straightforward to apply to a broader class of datasets, with expected success.",0
"Our proposed framework for chest X-ray classification is highly efficient in terms of data and labels. It is a combination of unsupervised representation learning using a Convolutional Neural Network and the Gaussian Process method. The self-supervision employed in the unsupervised representation learning does not require class labels, and the learned features are proven to achieve label-efficient classification. The GP method is a kernel-based Bayesian approach that provides data-efficient predictions and estimates the uncertainty of each decision. Our framework is less sensitive to class imbalance issues due to the features learned without labels and the Bayesian nature of GP. We use GP-provided uncertainty estimates to enable active learning by selectively labeling samples showing higher uncertainty. We apply this framework to COVID-19 chest X-ray classification, and it demonstrates that only $\sim 10\%$ of labeled data is needed to reach the accuracy of training all available labels. Our model with a generic ResNet backbone outperforms the state-of-the-art model with a highly tuned architecture by 4\% in a fully supervised classification scenario. Our framework is applicable to a wide range of datasets and is expected to achieve success.",1
"The potential for machine learning systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. Much recent work has focused on developing algorithmic tools to assess and mitigate such unfairness. However, there is little work on enhancing fairness in graph algorithms. Here, we develop a simple, effective and general method, CrossWalk, that enhances fairness of various graph algorithms, including influence maximization, link prediction and node classification, applied to node embeddings. CrossWalk is applicable to any random walk based node representation learning algorithm, such as DeepWalk and Node2Vec. The key idea is to bias random walks to cross group boundaries, by upweighting edges which (1) are closer to the groups' peripheries or (2) connect different groups in the network. CrossWalk pulls nodes that are near groups' peripheries towards their neighbors from other groups in the embedding space, while preserving the necessary structural information from the graph. Extensive experiments show the effectiveness of our algorithm to enhance fairness in various graph algorithms, including influence maximization, link prediction and node classification in synthetic and real networks, with only a very small decrease in performance.",0
"There is growing concern among both the general public and academia about how machine learning systems may exacerbate social inequalities and injustices. While some recent research has focused on creating algorithmic tools to identify and address these issues, little attention has been given to improving fairness in graph algorithms. In response, we have developed a straightforward and versatile method called CrossWalk that can enhance the fairness of a range of graph algorithms, including those used for influence maximization, link prediction, and node classification in node embeddings. CrossWalk is compatible with any node representation learning algorithm based on random walks, such as DeepWalk and Node2Vec. The basic concept behind CrossWalk is to adjust random walks so that they cross group boundaries more frequently. We accomplish this by increasing the weight of edges that are closer to the periphery of different groups or that connect different groups in the network. CrossWalk is able to pull nodes that are near the periphery of groups toward their neighbors in other groups in the embedding space, while still maintaining the structural information of the graph. Our algorithm has been extensively tested and shown to effectively enhance fairness in various graph algorithms, with only a minimal reduction in performance, in both synthetic and real networks.",1
"Inspired by the fact that human eyes continue to develop tracking ability in early and middle childhood, we propose to use tracking as a proxy task for a computer vision system to learn the visual representations. Modelled on the Catch game played by the children, we design a Catch-the-Patch (CtP) game for a 3D-CNN model to learn visual representations that would help with video-related tasks. In the proposed pretraining framework, we cut an image patch from a given video and let it scale and move according to a pre-set trajectory. The proxy task is to estimate the position and size of the image patch in a sequence of video frames, given only the target bounding box in the first frame. We discover that using multiple image patches simultaneously brings clear benefits. We further increase the difficulty of the game by randomly making patches invisible. Extensive experiments on mainstream benchmarks demonstrate the superior performance of CtP against other video pretraining methods. In addition, CtP-pretrained features are less sensitive to domain gaps than those trained by a supervised action recognition task. When both trained on Kinetics-400, we are pleasantly surprised to find that CtP-pretrained representation achieves much higher action classification accuracy than its fully supervised counterpart on Something-Something dataset. Code is available online: github.com/microsoft/CtP.",0
"Our proposal is to utilize tracking as a means for a computer vision system to develop visual representations. We were inspired by the development of tracking ability in children during early and middle childhood. To achieve this, we created a game called Catch-the-Patch (CtP) based on the Catch game played by children. We utilized a 3D-CNN model and designed the CtP game to train the model in visual representations that would aid in video-related tasks. In our pretraining framework, we extracted an image patch from a video and let it move and scale according to a predetermined path. The objective was to predict the position and size of the image patch in a sequence of video frames based solely on the target bounding box in the first frame. Our experiments showed that using multiple image patches simultaneously produced better results. We also added a level of difficulty by randomly making patches invisible. Our CtP game outperformed other video pretraining methods in mainstream benchmarks. Additionally, CtP-pretrained features were less affected by domain gaps than those trained by a supervised action recognition task. We found that CtP-pretrained representation achieved higher action classification accuracy than its fully supervised counterpart on the Something-Something dataset. Our code is available online at github.com/microsoft/CtP.",1
"From the beginning of zero-shot learning research, visual attributes have been shown to play an important role. In order to better transfer attribute-based knowledge from known to unknown classes, we argue that an image representation with integrated attribute localization ability would be beneficial for zero-shot learning. To this end, we propose a novel zero-shot representation learning framework that jointly learns discriminative global and local features using only class-level attributes. While a visual-semantic embedding layer learns global features, local features are learned through an attribute prototype network that simultaneously regresses and decorrelates attributes from intermediate features. We show that our locality augmented image representations achieve a new state-of-the-art on three zero-shot learning benchmarks. As an additional benefit, our model points to the visual evidence of the attributes in an image, e.g. for the CUB dataset, confirming the improved attribute localization ability of our image representation.",0
"Visual attributes have been found to be crucial in zero-shot learning research. To enhance the transfer of attribute-based knowledge from known to unknown classes, we suggest that an image representation that incorporates attribute localization would be advantageous. In light of this, we present a novel zero-shot representation learning framework that simultaneously learns discriminative global and local features using only class-level attributes. While a visual-semantic embedding layer learns global features, the attribute prototype network learns local features by regressing and decorrelating attributes from intermediate features. Our approach achieves a new state-of-the-art on three zero-shot learning benchmarks and also improves attribute localization ability, as demonstrated by the visual evidence of attributes in images from the CUB dataset.",1
"Face anti-spoofing approach based on domain generalization(DG) has drawn growing attention due to its robustness forunseen scenarios. Existing DG methods assume that the do-main label is known.However, in real-world applications, thecollected dataset always contains mixture domains, where thedomain label is unknown. In this case, most of existing meth-ods may not work. Further, even if we can obtain the domainlabel as existing methods, we think this is just a sub-optimalpartition. To overcome the limitation, we propose domain dy-namic adjustment meta-learning (D2AM) without using do-main labels, which iteratively divides mixture domains viadiscriminative domain representation and trains a generaliz-able face anti-spoofing with meta-learning. Specifically, wedesign a domain feature based on Instance Normalization(IN) and propose a domain representation learning module(DRLM) to extract discriminative domain features for cluster-ing. Moreover, to reduce the side effect of outliers on cluster-ing performance, we additionally utilize maximum mean dis-crepancy (MMD) to align the distribution of sample featuresto a prior distribution, which improves the reliability of clus tering. Extensive experiments show that the proposed methodoutperforms conventional DG-based face anti-spoofing meth-ods, including those utilizing domain labels. Furthermore, weenhance the interpretability through visualizatio",0
"The use of a Face anti-spoofing approach based on domain generalization (DG) has gained popularity because of its ability to handle unforeseen scenarios. However, current DG methods rely on knowing the domain label, which is not always available in real-world applications where collected datasets may contain mixture domains with unknown labels. This presents a challenge for existing methods. Even when the domain label is obtained, it may not be an optimal partition. To overcome these limitations, we introduce domain dynamic adjustment meta-learning (D2AM), which iteratively divides mixture domains through discriminative domain representation and trains a generalizable face anti-spoofing model using meta-learning. Our approach utilizes Instance Normalization (IN) to design a domain feature and proposes a domain representation learning module (DRLM) to extract discriminative domain features for clustering. Additionally, we utilize maximum mean discrepancy (MMD) to align the distribution of sample features to a prior distribution, improving clustering reliability. Our experiments demonstrate that D2AM outperforms conventional DG-based face anti-spoofing methods, including those that rely on domain labels. Moreover, we provide interpretability through visualization.",1
"Representation learning has been widely studied in the context of meta-learning, enabling rapid learning of new tasks through shared representations. Recent works such as MAML have explored using fine-tuning-based metrics, which measure the ease by which fine-tuning can achieve good performance, as proxies for obtaining representations. We present a theoretical framework for analyzing representations derived from a MAML-like algorithm, assuming the available tasks use approximately the same underlying representation. We then provide risk bounds on the best predictor found by fine-tuning via gradient descent, demonstrating that the algorithm can provably leverage the shared structure. The upper bound applies to general function classes, which we demonstrate by instantiating the guarantees of our framework in the logistic regression and neural network settings. In contrast, we establish the existence of settings where any algorithm, using a representation trained with no consideration for task-specific fine-tuning, performs as well as a learner with no access to source tasks in the worst case. This separation result underscores the benefit of fine-tuning-based methods, such as MAML, over methods with ""frozen representation"" objectives in few-shot learning.",0
"Representation learning has been extensively researched for meta-learning, facilitating swift learning of new tasks through shared representations. Recent studies, such as MAML, have explored using fine-tuning-based metrics as proxies for obtaining representations, measuring the ease with which fine-tuning can achieve good performance. We propose a theoretical framework for analyzing MAML-like algorithm-derived representations, assuming that the available tasks use nearly the same underlying representation. We then provide risk bounds on the best predictor obtained by fine-tuning via gradient descent, demonstrating that the algorithm can leverage the shared structure. Our upper bound applies to general function classes, which we prove by instantiating the guarantees in the logistic regression and neural network settings. On the other hand, we establish that in some settings, any algorithm that uses a representation trained with no consideration for task-specific fine-tuning performs as well as a learner with no access to source tasks, highlighting the benefits of fine-tuning-based methods like MAML over ""frozen representation"" methods in few-shot learning.",1
"We study how representation learning can improve the efficiency of bandit problems. We study the setting where we play $T$ linear bandits with dimension $d$ concurrently, and these $T$ bandit tasks share a common $k (\ll d)$ dimensional linear representation. For the finite-action setting, we present a new algorithm which achieves $\widetilde{O}(T\sqrt{kN} + \sqrt{dkNT})$ regret, where $N$ is the number of rounds we play for each bandit. When $T$ is sufficiently large, our algorithm significantly outperforms the naive algorithm (playing $T$ bandits independently) that achieves $\widetilde{O}(T\sqrt{d N})$ regret. We also provide an $\Omega(T\sqrt{kN} + \sqrt{dkNT})$ regret lower bound, showing that our algorithm is minimax-optimal up to poly-logarithmic factors. Furthermore, we extend our algorithm to the infinite-action setting and obtain a corresponding regret bound which demonstrates the benefit of representation learning in certain regimes. We also present experiments on synthetic and real-world data to illustrate our theoretical findings and demonstrate the effectiveness of our proposed algorithms.",0
"Our focus is on exploring how representation learning can enhance the efficiency of bandit problems. Specifically, we examine a scenario where $T$ linear bandits with dimension $d$ are played concurrently. These tasks share a common $k (\ll d)$ dimensional linear representation. We propose a novel algorithm for the finite-action setting that achieves $\widetilde{O}(T\sqrt{kN} + \sqrt{dkNT})$ regret, where $N$ denotes the number of rounds for each bandit. Our algorithm significantly outperforms the naive approach of playing $T$ bandits independently, which achieves $\widetilde{O}(T\sqrt{d N})$ regret when $T$ is sufficiently large. We also establish an $\Omega(T\sqrt{kN} + \sqrt{dkNT})$ regret lower bound, indicating that our algorithm is minimax-optimal up to poly-logarithmic factors. We extend our algorithm to the infinite-action setting and obtain a corresponding regret bound, demonstrating the advantages of representation learning in certain circumstances. Finally, we provide experimental results on synthetic and real-world data to illustrate our theoretical findings and the effectiveness of our proposed algorithms.",1
"Face parsing aims to predict pixel-wise labels for facial components of a target face in an image. Existing approaches usually crop the target face from the input image with respect to a bounding box calculated during pre-processing, and thus can only parse inner facial Regions of Interest~(RoIs). Peripheral regions like hair are ignored and nearby faces that are partially included in the bounding box can cause distractions. Moreover, these methods are only trained and evaluated on near-frontal portrait images and thus their performance for in-the-wild cases has been unexplored. To address these issues, this paper makes three contributions. First, we introduce iBugMask dataset for face parsing in the wild, which consists of 21,866 training images and 1,000 testing images. The training images are obtained by augmenting an existing dataset with large face poses. The testing images are manually annotated with $11$ facial regions and there are large variations in sizes, poses, expressions and background. Second, we propose RoI Tanh-polar transform that warps the whole image to a Tanh-polar representation with a fixed ratio between the face area and the context, guided by the target bounding box. The new representation contains all information in the original image, and allows for rotation equivariance in the convolutional neural networks~(CNNs). Third, we propose a hybrid residual representation learning block, coined HybridBlock, that contains convolutional layers in both the Tanh-polar space and the Tanh-Cartesian space, allowing for receptive fields of different shapes in CNNs. Through extensive experiments, we show that the proposed method improves the state-of-the-art for face parsing in the wild and does not require facial landmarks for alignment.",0
"The goal of face parsing is to predict labels for specific facial features in an image at the pixel level. Current approaches typically crop the target face based on pre-calculated bounding boxes, limiting their ability to parse peripheral regions like hair and potentially causing distractions from nearby faces partially included in the bounding box. These methods are also only evaluated on near-frontal portrait images, leaving their performance in real-world scenarios untested. To address these limitations, this paper presents three contributions. Firstly, the iBugMask dataset is introduced, consisting of 21,866 training images and 1,000 testing images with varied poses, expressions, and backgrounds. Secondly, a RoI Tanh-polar transform is proposed, which warps the whole image to a Tanh-polar representation with a fixed face-to-context ratio guided by the target bounding box. This new representation enables rotation equivariance in CNNs and preserves all information from the original image. Thirdly, a HybridBlock is introduced, which contains convolutional layers in both the Tanh-polar and Tanh-Cartesian space, allowing for receptive fields of different shapes in CNNs. Extensive experiments show that this approach outperforms current state-of-the-art methods for face parsing in real-world scenarios and does not require facial landmarks for alignment.",1
"Face representation learning using datasets with massive number of identities requires appropriate training methods. Softmax-based approach, currently the state-of-the-art in face recognition, in its usual ""full softmax"" form is not suitable for datasets with millions of persons. Several methods, based on the ""sampled softmax"" approach, were proposed to remove this limitation. These methods, however, have a set of disadvantages. One of them is a problem of ""prototype obsolescence"": classifier weights (prototypes) of the rarely sampled classes, receive too scarce gradients and become outdated and detached from the current encoder state, resulting in an incorrect training signals. This problem is especially serious in ultra-large-scale datasets. In this paper, we propose a novel face representation learning model called Prototype Memory, which alleviates this problem and allows training on a dataset of any size. Prototype Memory consists of the limited-size memory module for storing recent class prototypes and employs a set of algorithms to update it in appropriate way. New class prototypes are generated on the fly using exemplar embeddings in the current mini-batch. These prototypes are enqueued to the memory and used in a role of classifier weights for usual softmax classification-based training. To prevent obsolescence and keep the memory in close connection with encoder, prototypes are regularly refreshed, and oldest ones are dequeued and disposed. Prototype Memory is computationally efficient and independent of dataset size. It can be used with various loss functions, hard example mining algorithms and encoder architectures. We prove the effectiveness of the proposed model by extensive experiments on popular face recognition benchmarks.",0
"When learning face representation with datasets containing a large number of identities, appropriate training methods are necessary. The usual ""full softmax"" form of the current state-of-the-art approach, which is softmax-based, is not suitable for datasets with millions of people. To overcome this limitation, several methods have been proposed based on the ""sampled softmax"" approach. However, these methods have their own disadvantages, such as the problem of ""prototype obsolescence,"" which occurs when classifier weights of rarely sampled classes become outdated and detached from the current encoder state, leading to incorrect training signals. This problem is particularly severe in ultra-large-scale datasets. In this paper, we introduce a new face representation learning model called Prototype Memory, which addresses this issue and enables training on datasets of any size. Prototype Memory includes a limited-size memory module for storing recent class prototypes and uses algorithms to update it appropriately. New class prototypes are generated on the fly using exemplar embeddings in the current mini-batch, and these prototypes are enqueued to the memory and used as classifier weights for usual softmax classification-based training. To prevent obsolescence, prototypes are regularly refreshed, and the oldest ones are dequeued and disposed of. Prototype Memory is both computationally efficient and independent of dataset size, making it compatible with various loss functions, hard example mining algorithms, and encoder architectures. We demonstrate the effectiveness of this model through extensive experiments on popular face recognition benchmarks.",1
"We present Mixture of Contrastive Experts (MiCE), a unified probabilistic clustering framework that simultaneously exploits the discriminative representations learned by contrastive learning and the semantic structures captured by a latent mixture model. Motivated by the mixture of experts, MiCE employs a gating function to partition an unlabeled dataset into subsets according to the latent semantics and multiple experts to discriminate distinct subsets of instances assigned to them in a contrastive learning manner. To solve the nontrivial inference and learning problems caused by the latent variables, we further develop a scalable variant of the Expectation-Maximization (EM) algorithm for MiCE and provide proof of the convergence. Empirically, we evaluate the clustering performance of MiCE on four widely adopted natural image datasets. MiCE achieves significantly better results than various previous methods and a strong contrastive learning baseline.",0
"MiCE is a unified probabilistic clustering framework that leverages both the discriminative representations derived from contrastive learning and the semantic structures captured by a latent mixture model. The approach is inspired by the mixture of experts and involves using a gating function to partition an unlabeled dataset into subsets based on latent semantics, and employing multiple experts to differentiate between distinct subsets using contrastive learning. To address the challenges posed by latent variables, we have developed a scalable variant of the Expectation-Maximization (EM) algorithm for MiCE and demonstrated its convergence. We evaluated MiCE on four natural image datasets and found that it outperformed previous methods and a strong contrastive learning baseline.",1
"In this paper, we propose a subspace representation learning (SRL) framework to tackle few-shot image classification tasks. It exploits a subspace in local CNN feature space to represent an image, and measures the similarity between two images according to a weighted subspace distance (WSD). When K images are available for each class, we develop two types of template subspaces to aggregate K-shot information: the prototypical subspace (PS) and the discriminative subspace (DS). Based on the SRL framework, we extend metric learning based techniques from vector to subspace representation. While most previous works adopted global vector representation, using subspace representation can effectively preserve the spatial structure, and diversity within an image. We demonstrate the effectiveness of the SRL framework on three public benchmark datasets: MiniImageNet, TieredImageNet and Caltech-UCSD Birds-200-2011 (CUB), and the experimental results illustrate competitive/superior performance of our method compared to the previous state-of-the-art.",0
"This paper presents a solution for few-shot image classification tasks through the use of a subspace representation learning (SRL) framework. The SRL framework utilizes a subspace in the local CNN feature space to represent images and measures the similarity between two images based on a weighted subspace distance (WSD). When K images are available for each class, the framework develops two types of template subspaces: the prototypical subspace (PS) and the discriminative subspace (DS) to aggregate K-shot information. The metric learning based techniques are extended from vector to subspace representation. Unlike previous works that use global vector representation, the use of subspace representation preserves the spatial structure and diversity within an image. The SRL framework's effectiveness is demonstrated on three public benchmark datasets, namely MiniImageNet, TieredImageNet, and Caltech-UCSD Birds-200-2011 (CUB). The experimental results show that the proposed method outperforms the previous state-of-the-art approaches.",1
"Graph convolution networks have recently garnered a lot of attention for representation learning on non-Euclidean feature spaces. Recent research has focused on stacking multiple layers like in convolutional neural networks for the increased expressive power of graph convolution networks. However, simply stacking multiple graph convolution layers lead to issues like vanishing gradient, over-fitting and over-smoothing. Such problems are much less when using shallower networks, even though the shallow networks have lower expressive power. In this work, we propose a novel Multipath Graph convolutional neural network that aggregates the output of multiple different shallow networks. We train and test our model on various benchmarks datasets for the task of node property prediction. Results show that the proposed method not only attains increased test accuracy but also requires fewer training epochs to converge. The full implementation is available at https://github.com/rangan2510/MultiPathGCN",0
"Recently, there has been significant interest in using graph convolution networks to learn representations on non-Euclidean feature spaces. Researchers have explored stacking multiple layers, similar to convolutional neural networks, to enhance the expressive power of these networks. However, this approach often leads to issues such as vanishing gradient, over-fitting, and over-smoothing. Shallow networks, while less powerful, are more resilient to these problems. Thus, we introduce a new approach, the Multipath Graph Convolutional Neural Network, which aggregates the output of several shallow networks. We evaluate our model on various benchmarks for node property prediction and demonstrate improved test accuracy and faster convergence with fewer training epochs. The full implementation can be found at https://github.com/rangan2510/MultiPathGCN.",1
"Recent years have witnessed an upsurge of research interests and applications of machine learning on graphs. Automated machine learning (AutoML) on graphs is on the horizon to automatically design the optimal machine learning algorithm for a given graph task. However, none of the existing libraries can fully support AutoML on graphs. To fill this gap, we present Automated Graph Learning (AutoGL), the first library for automated machine learning on graphs. AutoGL is open-source, easy to use, and flexible to be extended. Specifically, we propose an automated machine learning pipeline for graph data containing four modules: auto feature engineering, model training, hyper-parameter optimization, and auto ensemble. For each module, we provide numerous state-of-the-art methods and flexible base classes and APIs, which allow easy customization. We further provide experimental results to showcase the usage of our AutoGL library.",0
"Machine learning on graphs has become increasingly popular in recent years, leading to the development of automated machine learning (AutoML) for graph-related tasks. However, current libraries do not fully support this technology, leaving a gap in the market. To address this issue, we have created Automated Graph Learning (AutoGL), the first open-source library for AutoML on graphs. Our library offers a four-module pipeline for graph data, including auto feature engineering, model training, hyper-parameter optimization, and auto ensemble. We have also included multiple state-of-the-art methods and APIs for customization. Our experimental results demonstrate the effectiveness of AutoGL.",1
"In this paper, we focus on deep clustering and unsupervised representation learning for images. Recent advances in deep clustering and unsupervised representation learning are based on the idea that different views of an input image (generated through data augmentation techniques) must be closer in the representation space (exemplar consistency), and/or similar images have a similar cluster assignment (population consistency). We define an additional notion of consistency, consensus consistency, which ensures that representations are learnt to induce similar partitions for variations in the representation space, different clustering algorithms or different initializations of a clustering algorithm. We define a clustering loss by performing variations in the representation space and seamlessly integrate all three consistencies (consensus, exemplar and population) into an end-to-end learning framework. The proposed algorithm, Consensus Clustering using Unsupervised Representation Learning (ConCURL) improves the clustering performance over state-of-the art methods on four out of five image datasets. Further, we extend the evaluation procedure for clustering to reflect the challenges in real world clustering tasks, such as clustering performance in the case of distribution shift. We also perform a detailed ablation study for a deeper understanding of the algorithm.",0
"The focus of this paper is on the application of deep clustering and unsupervised representation learning to images. Recent developments in these areas have emphasized the importance of exemplar consistency, meaning that different views of an image should be represented similarly, and population consistency, meaning that similar images should be clustered together. The authors propose a new type of consistency, consensus consistency, which ensures that representations induce similar partitions for variations in the representation space, clustering algorithms, and initializations. To achieve this, they introduce a clustering loss that integrates all three types of consistency into a single end-to-end learning framework. The proposed algorithm, ConCURL, outperforms state-of-the-art methods on four out of five image datasets and is evaluated using a procedure that reflects real-world clustering challenges. An ablation study is also conducted to provide a deeper understanding of the algorithm.",1
"While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. In this work, we show that the robustness of neural networks can be greatly improved through the use of random convolutions as data augmentation. Random convolutions are approximately shape-preserving and may distort local textures. Intuitively, randomized convolutions create an infinite number of new domains with similar global shapes but random local textures. Therefore, we explore using outputs of multi-scale random convolutions as new images or mixing them with the original images during training. When applying a network trained with our approach to unseen domains, our method consistently improves the performance on domain generalization benchmarks and is scalable to ImageNet. In particular, in the challenging scenario of generalizing to the sketch domain in PACS and to ImageNet-Sketch, our method outperforms state-of-art methods by a large margin. More interestingly, our method can benefit downstream tasks by providing a more robust pretrained visual representation.",0
"Although deep neural networks have demonstrated success in various computer vision tasks, they have been found to be susceptible to texture style shifts and small perturbations, unlike humans who are more robust to these changes. This study proposes that the use of random convolutions as data augmentation can significantly enhance the robustness of neural networks. Random convolutions are roughly shape-preserving but can distort local textures, generating countless new domains with similar shapes but random local textures. To improve the network's performance on domain generalization benchmarks, the study explores using the outputs of multi-scale random convolutions as new images or blending them with the original images during training. When applied to unseen domains, this approach consistently improves performance and is scalable to ImageNet. Notably, the approach outperforms state-of-the-art methods by a wide margin in the challenging scenario of generalizing to the sketch domain in PACS and ImageNet-Sketch. Furthermore, the study shows that the method can benefit downstream tasks by providing a more robust pretrained visual representation.",1
"Hypergraph, an expressive structure with flexibility to model the higher-order correlations among entities, has recently attracted increasing attention from various research domains. Despite the success of Graph Neural Networks (GNNs) for graph representation learning, how to adapt the powerful GNN-variants directly into hypergraphs remains a challenging problem. In this paper, we propose UniGNN, a unified framework for interpreting the message passing process in graph and hypergraph neural networks, which can generalize general GNN models into hypergraphs. In this framework, meticulously-designed architectures aiming to deepen GNNs can also be incorporated into hypergraphs with the least effort. Extensive experiments have been conducted to demonstrate the effectiveness of UniGNN on multiple real-world datasets, which outperform the state-of-the-art approaches with a large margin. Especially for the DBLP dataset, we increase the accuracy from 77.4\% to 88.8\% in the semi-supervised hypernode classification task. We further prove that the proposed message-passing based UniGNN models are at most as powerful as the 1-dimensional Generalized Weisfeiler-Leman (1-GWL) algorithm in terms of distinguishing non-isomorphic hypergraphs. Our code is available at \url{https://github.com/OneForward/UniGNN}.",0
"Recently, there has been a growing interest in hypergraphs, a versatile structure that can model higher-order correlations among entities. Despite the success of Graph Neural Networks (GNNs) in learning graph representations, adapting GNN variants to hypergraphs is still a challenging task. In this paper, we introduce UniGNN, a unified framework that can generalize GNN models to hypergraphs. This framework incorporates carefully designed architectures that deepen GNNs with minimal effort. Through extensive experiments on various real-world datasets, we demonstrate the superior performance of UniGNN over state-of-the-art approaches, especially in the semi-supervised hypernode classification task, where we achieve an accuracy boost from 77.4% to 88.8% on the DBLP dataset. Moreover, we show that our message-passing based UniGNN models are as powerful as the 1-dimensional Generalized Weisfeiler-Leman algorithm in distinguishing non-isomorphic hypergraphs. Our code is publicly available at \url{https://github.com/OneForward/UniGNN}.",1
"Graph autoencoders are very efficient at embedding graph-based complex data sets. However, most of the autoencoders have shallow depths and their efficiency tends to decrease with the increase of layer depth. In this paper, we study the effect of adding residual connections to shallow and deep graph variational and vanilla autoencoders. We show that residual connections improve the accuracy of the deep graph-based autoencoders. Furthermore, we propose Res-VGAE, a graph variational autoencoder with different residual connections. Our experiments show that our model achieves superior results when compared with other autoencoder-based models for the link prediction task.",0
"The efficiency of graph autoencoders in embedding complex data sets based on graphs is notable. However, the majority of autoencoders have shallow depths and their effectiveness diminishes as the layer depth increases. This study investigates the impact of introducing residual connections to both shallow and deep graph vanilla and variational autoencoders. The findings suggest that residual connections enhance the accuracy of deep graph autoencoders. Additionally, the paper introduces Res-VGAE, a graph variational autoencoder with distinct residual connections. The experiments demonstrate that Res-VGAE outperforms other autoencoder models for the link prediction task.",1
"Hypergraphs are a generalized data structure of graphs to model higher-order correlations among entities, which have been successfully adopted into various research domains. Meanwhile, HyperGraph Neural Network (HGNN) is currently the de-facto method for hypergraph representation learning. However, HGNN aims at single hypergraph learning and uses a pre-concatenation approach when confronting multi-modal datasets, which leads to sub-optimal exploitation of the inter-correlations of multi-modal hypergraphs. HGNN also suffers the over-smoothing issue, that is, its performance drops significantly when layers are stacked up. To resolve these issues, we propose the Residual enhanced Multi-Hypergraph Neural Network, which can not only fuse multi-modal information from each hypergraph effectively, but also circumvent the over-smoothing issue associated with HGNN. We conduct experiments on two 3D benchmarks, the NTU and the ModelNet40 datasets, and compare against multiple state-of-the-art methods. Experimental results demonstrate that both the residual hypergraph convolutions and the multi-fusion architecture can improve the performance of the base model and the combined model achieves a new state-of-the-art. Code is available at \url{https://github.com/OneForward/ResMHGNN}.",0
"Hypergraphs, a generalized form of graphs, are used to represent higher-order correlations among entities in various research domains. The HyperGraph Neural Network (HGNN) is currently the most widely used method for hypergraph representation learning. However, HGNN has limitations when dealing with multi-modal datasets, as it uses a pre-concatenation approach, leading to sub-optimal exploitation of inter-correlations. Additionally, HGNN suffers from the over-smoothing issue when layers are stacked up. To address these challenges, we propose the Residual enhanced Multi-Hypergraph Neural Network. This approach effectively fuses multi-modal information from each hypergraph and avoids the over-smoothing problem. We evaluate our approach on two 3D benchmarks and compare it against multiple state-of-the-art methods. Experimental results demonstrate that our method outperforms the base model and achieves a new state-of-the-art. Our code is available at \url{https://github.com/OneForward/ResMHGNN}.",1
"Existing learning models often utilise CT-scan images to predict lung diseases. These models are posed by high uncertainties that affect lung segmentation and visual feature learning. We introduce MARL, a novel Multimodal Attentional Representation Learning model architecture that learns useful features from multimodal data under uncertainty. We feed the proposed model with both the lung CT-scan images and their perspective historical patients' biological records collected over times. Such rich data offers to analyse both spatial and temporal aspects of the disease. MARL employs Fuzzy-based image spatial segmentation to overcome uncertainties in CT-scan images. We then utilise a pre-trained Convolutional Neural Network (CNN) to learn visual representation vectors from images. We augment patients' data with statistical features from the segmented images. We develop a Long Short-Term Memory (LSTM) network to represent the augmented data and learn sequential patterns of disease progressions. Finally, we inject both CNN and LSTM feature vectors to an attention layer to help focus on the best learning features. We evaluated MARL on regression of lung disease progression and status classification. MARL outperforms state-of-the-art CNN architectures, such as EfficientNet and DenseNet, and baseline prediction models. It achieves a 91% R^2 score, which is higher than the other models by a range of 8% to 27%. Also, MARL achieves 97% and 92% accuracy for binary and multi-class classification, respectively. MARL improves the accuracy of state-of-the-art CNN models with a range of 19% to 57%. The results show that combining spatial and sequential temporal features produces better discriminative feature.",0
"Learning models currently use CT-scan images to predict lung diseases, but these models have high levels of uncertainty that affect lung segmentation and visual feature learning. To address these issues, we introduce MARL, a Multimodal Attentional Representation Learning model that learns useful features from multimodal data under uncertainty. In addition to lung CT-scan images, we also incorporate historical biological records of patients to analyze both spatial and temporal aspects of the disease. MARL utilizes Fuzzy-based image spatial segmentation to overcome uncertainties in CT-scan images and a pre-trained Convolutional Neural Network (CNN) to learn visual representation vectors. Patients' data is augmented with statistical features from the segmented images, and a Long Short-Term Memory (LSTM) network is used to represent the augmented data and learn sequential patterns of disease progressions. An attention layer is then employed to focus on the best learning features. MARL outperforms state-of-the-art CNN architectures, such as EfficientNet and DenseNet, achieving a 91% R^2 score and 97% and 92% accuracy for binary and multi-class classification, respectively. The results highlight the effectiveness of combining spatial and sequential temporal features for better discriminative features.",1
"In this work, we propose a Cross-view Contrastive Learning framework for unsupervised 3D skeleton-based action Representation (CrosSCLR), by leveraging multi-view complementary supervision signal. CrosSCLR consists of both single-view contrastive learning (SkeletonCLR) and cross-view consistent knowledge mining (CVC-KM) modules, integrated in a collaborative learning manner. It is noted that CVC-KM works in such a way that high-confidence positive/negative samples and their distributions are exchanged among views according to their embedding similarity, ensuring cross-view consistency in terms of contrastive context, i.e., similar distributions. Extensive experiments show that CrosSCLR achieves remarkable action recognition results on NTU-60 and NTU-120 datasets under unsupervised settings, with observed higher-quality action representations. Our code is available at https://github.com/LinguoLi/CrosSCLR.",0
"Our proposal is the CrosSCLR framework, which utilizes multi-view complementary supervision signals for unsupervised 3D skeleton-based action representation. This framework integrates the SkeletonCLR and CVC-KM modules, which perform single-view contrastive learning and cross-view consistent knowledge mining, respectively. CVC-KM ensures cross-view consistency by exchanging high-confidence positive/negative samples and their distributions among views based on embedding similarity. CrosSCLR achieves impressive action recognition results on the NTU-60 and NTU-120 datasets under unsupervised settings, with higher-quality action representations. Our code can be accessed at https://github.com/LinguoLi/CrosSCLR.",1
"Graph Neural Networks (GNNs) have received significant attention due to their state-of-the-art performance on various graph representation learning tasks. However, recent studies reveal that GNNs are vulnerable to adversarial attacks, i.e. an attacker is able to fool the GNNs by perturbing the graph structure or node features deliberately. While being able to successfully decrease the performance of GNNs, most existing attacking algorithms require access to either the model parameters or the training data, which is not practical in the real world.   In this paper, we develop deeper insights into the Mettack algorithm, which is a representative grey-box attacking method, and then we propose a gradient-based black-box attacking algorithm. Firstly, we show that the Mettack algorithm will perturb the edges unevenly, thus the attack will be highly dependent on a specific training set. As a result, a simple yet useful strategy to defense against Mettack is to train the GNN with the validation set. Secondly, to overcome the drawbacks, we propose the Black-Box Gradient Attack (BBGA) algorithm. Extensive experiments demonstrate that out proposed method is able to achieve stable attack performance without accessing the training sets of the GNNs. Further results shows that our proposed method is also applicable when attacking against various defense methods.",0
"GNNs have gained attention for their exceptional performance in graph representation learning tasks, but recent research highlights their susceptibility to adversarial attacks where an attacker can manipulate the graph structure or node features to deceive the GNNs. Despite the ability to reduce GNN performance, current attacking algorithms require access to model parameters or training data, which is not feasible in real-world scenarios. This study delves deeper into the Mettack algorithm, a representative grey-box attack, and proposes a gradient-based black-box attack algorithm. The analysis reveals that Mettack perturbs edges unevenly, making the attack highly dependent on a specific training set. Therefore, training the GNN with the validation set can serve as a simple yet effective defense strategy against Mettack. To address these limitations, the Black-Box Gradient Attack (BBGA) algorithm is proposed, which achieves stable attack performance without accessing GNN training sets. Results show that BBGA is effective against various defense methods.",1
"As one of the most fundamental tasks in graph theory, subgraph matching is a crucial task in many fields, ranging from information retrieval, computer vision, biology, chemistry and natural language processing. Yet subgraph matching problem remains to be an NP-complete problem. This study proposes an end-to-end learning-based approximate method for subgraph matching task, called subgraph matching network (Sub-GMN). The proposed Sub-GMN firstly uses graph representation learning to map nodes to node-level embedding. It then combines metric learning and attention mechanisms to model the relationship between matched nodes in the data graph and query graph. To test the performance of the proposed method, we applied our method on two databases. We used two existing methods, GNN and FGNN as baseline for comparison. Our experiment shows that, on dataset 1, on average the accuracy of Sub-GMN are 12.21\% and 3.2\% higher than that of GNN and FGNN respectively. On average running time Sub-GMN runs 20-40 times faster than FGNN. In addition, the average F1-score of Sub-GMN on all experiments with dataset 2 reached 0.95, which demonstrates that Sub-GMN outputs more correct node-to-node matches.   Comparing with the previous GNNs-based methods for subgraph matching task, our proposed Sub-GMN allows varying query and data graphes in the test/application stage, while most previous GNNs-based methods can only find a matched subgraph in the data graph during the test/application for the same query graph used in the training stage. Another advantage of our proposed Sub-GMN is that it can output a list of node-to-node matches, while most existing end-to-end GNNs based methods cannot provide the matched node pairs.",0
"Subgraph matching is a crucial task in fields such as information retrieval, computer vision, biology, chemistry and natural language processing. However, it remains an NP-complete problem. This study introduces a learning-based method, called Sub-GMN, for approximating subgraph matching. Sub-GMN uses graph representation learning to map nodes to node-level embedding, and combines metric learning and attention mechanisms to model the relationship between matched nodes. To evaluate performance, Sub-GMN was tested on two databases and compared to two existing methods, GNN and FGNN. Results showed that Sub-GMN had higher accuracy and faster running times than the two baseline methods. Additionally, Sub-GMN allows for varying query and data graphs in the test stage, and can output a list of node-to-node matches, while most existing GNN-based methods cannot.",1
"Labeling videos at scale is impractical. Consequently, self-supervised visual representation learning is key for efficient video analysis. Recent success in learning image representations suggests contrastive learning is a promising framework to tackle this challenge. However, when applied to real-world videos, contrastive learning may unknowingly lead to the separation of instances that contain semantically similar events. In our work, we introduce a cooperative variant of contrastive learning to utilize complementary information across views and address this issue. We use data-driven sampling to leverage implicit relationships between multiple input video views, whether observed (e.g. RGB) or inferred (e.g. flow, segmentation masks, poses). We are one of the firsts to explore exploiting inter-instance relationships to drive learning. We experimentally evaluate our representations on the downstream task of action recognition. Our method achieves competitive performance on standard benchmarks (UCF101, HMDB51, Kinetics400). Furthermore, qualitative experiments illustrate that our models can capture higher-order class relationships.",0
"Labeling videos in large quantities is not practical, which is why self-supervised visual representation learning is crucial for efficient video analysis. Contrastive learning has shown promise for learning image representations, but it may unintentionally separate instances that contain similar events when applied to real-world videos. To address this issue, we introduce a cooperative variant of contrastive learning that uses data-driven sampling to incorporate complementary information across multiple input video views, whether observed (e.g. RGB) or inferred (e.g. flow, segmentation masks, poses). We are among the first to explore the use of inter-instance relationships for driving learning. We evaluate our method on the task of action recognition and achieve competitive performance on standard benchmarks (UCF101, HMDB51, Kinetics400). Additionally, our models can capture higher-order class relationships, as demonstrated through qualitative experiments.",1
"We present a large-scale study on unsupervised spatiotemporal representation learning from videos. With a unified perspective on four recent image-based frameworks, we study a simple objective that can easily generalize all these methods to space-time. Our objective encourages temporally-persistent features in the same video, and in spite of its simplicity, it works surprisingly well across: (i) different unsupervised frameworks, (ii) pre-training datasets, (iii) downstream datasets, and (iv) backbone architectures. We draw a series of intriguing observations from this study, e.g., we discover that encouraging long-spanned persistency can be effective even if the timespan is 60 seconds. In addition to state-of-the-art results in multiple benchmarks, we report a few promising cases in which unsupervised pre-training can outperform its supervised counterpart. Code is made available at https://github.com/facebookresearch/SlowFast",0
"Our study focuses on unsupervised spatiotemporal representation learning from videos on a large scale. We explore four recent image-based frameworks from a unified perspective and examine a straightforward objective that can be applied to all of these methods in space-time. Our objective emphasizes the creation of temporally-persistent features within the same video. Despite its simplicity, this technique performs remarkably well across various unsupervised frameworks, pre-training datasets, downstream datasets, and backbone architectures. Our study reveals several intriguing observations, such as the effectiveness of encouraging long-spanned persistency even with a timespan of 60 seconds. We also present several cases where unsupervised pre-training surpasses its supervised counterpart, in addition to state-of-the-art results in multiple benchmarks. The code is available at https://github.com/facebookresearch/SlowFast.",1
"Graph representation learning has become a ubiquitous component in many scenarios, ranging from social network analysis to energy forecasting in smart grids. In several applications, ensuring the fairness of the node (or graph) representations with respect to some protected attributes is crucial for their correct deployment. Yet, fairness in graph deep learning remains under-explored, with few solutions available. In particular, the tendency of similar nodes to cluster on several real-world graphs (i.e., homophily) can dramatically worsen the fairness of these procedures. In this paper, we propose a biased edge dropout algorithm (FairDrop) to counter-act homophily and improve fairness in graph representation learning. FairDrop can be plugged in easily on many existing algorithms, is efficient, adaptable, and can be combined with other fairness-inducing solutions. After describing the general algorithm, we demonstrate its application on two benchmark tasks, specifically, as a random walk model for producing node embeddings, and to a graph convolutional network for link prediction. We prove that the proposed algorithm can successfully improve the fairness of all models up to a small or negligible drop in accuracy, and compares favourably with existing state-of-the-art solutions. In an ablation study, we demonstrate that our algorithm can flexibly interpolate between biasing towards fairness and an unbiased edge dropout. Furthermore, to better evaluate the gains, we propose a new dyadic group definition to measure the bias of a link prediction task when paired with group-based fairness metrics. In particular, we extend the metric used to measure the bias in the node embeddings to take into account the graph structure.",0
"Graph representation learning is widely used in various applications, such as social network analysis and energy forecasting in smart grids. In many cases, it is crucial to ensure fairness in the node or graph representations concerning protected attributes for appropriate deployment. However, fairness in graph deep learning is not explored much, and there are only a few available solutions. The homophily tendency of similar nodes to cluster in real-world graphs can worsen the fairness of these procedures significantly. To address this issue, we propose an algorithm called FairDrop, which is a biased edge dropout algorithm that can improve fairness in graph representation learning. FairDrop is efficient, adaptable, and can be integrated easily with existing algorithms and other fairness-inducing solutions. We demonstrate the effectiveness of our approach on two benchmark tasks: producing node embeddings using a random walk model and link prediction using a graph convolutional network. Our algorithm successfully improves the fairness of all models with a small or negligible drop in accuracy, and it outperforms existing state-of-the-art solutions. We also demonstrate that our algorithm can flexibly interpolate between biasing towards fairness and an unbiased edge dropout. Furthermore, we propose a new dyadic group definition to measure the bias of a link prediction task when paired with group-based fairness metrics and extend the metric used to measure the bias in the node embeddings to account for the graph structure.",1
"Self-supervised learning has been widely used to obtain transferrable representations from unlabeled images. Especially, recent contrastive learning methods have shown impressive performances on downstream image classification tasks. While these contrastive methods mainly focus on generating invariant global representations at the image-level under semantic-preserving transformations, they are prone to overlook spatial consistency of local representations and therefore have a limitation in pretraining for localization tasks such as object detection and instance segmentation. Moreover, aggressively cropped views used in existing contrastive methods can minimize representation distances between the semantically different regions of a single image.   In this paper, we propose a spatially consistent representation learning algorithm (SCRL) for multi-object and location-specific tasks. In particular, we devise a novel self-supervised objective that tries to produce coherent spatial representations of a randomly cropped local region according to geometric translations and zooming operations. On various downstream localization tasks with benchmark datasets, the proposed SCRL shows significant performance improvements over the image-level supervised pretraining as well as the state-of-the-art self-supervised learning methods.   Code is available at https://github.com/kakaobrain/scrl",0
"The use of self-supervised learning has become widespread in obtaining transferable representations from unlabeled images. Recent contrastive learning methods have shown remarkable results in downstream image classification tasks, mainly by generating invariant global representations at the image-level under semantic-preserving transformations. However, these methods tend to overlook the spatial consistency of local representations, which limits their effectiveness in pretraining for localization tasks such as object detection and instance segmentation. Furthermore, the aggressively cropped views used in existing contrastive methods can reduce representation distances between semantically different regions of a single image. This paper proposes a spatially consistent representation learning algorithm (SCRL) for multi-object and location-specific tasks. The authors devise a novel self-supervised objective that aims to produce coherent spatial representations of a randomly cropped local region based on geometric translations and zooming operations. The proposed SCRL significantly outperforms image-level supervised pretraining and state-of-the-art self-supervised learning methods on various downstream localization tasks with benchmark datasets. The code is available at https://github.com/kakaobrain/scrl.",1
"Ultra-wideband (UWB) is the state-of-the-art and most popular technology for wireless localization. Nevertheless, precise ranging and localization in non-line-of-sight (NLoS) conditions is still an open research topic. Indeed, multipath effects, reflections, refractions, and complexity of the indoor radio environment can easily introduce a positive bias in the ranging measurement, resulting in highly inaccurate and unsatisfactory position estimation. This article proposes an efficient representation learning methodology that exploits the latest advancement in deep learning and graph optimization techniques to achieve effective ranging error mitigation at the edge. Channel Impulse Response (CIR) signals are directly exploited to extract high semantic features to estimate corrections in either NLoS or LoS conditions. Extensive experimentation with different settings and configurations has proved the effectiveness of our methodology and demonstrated the feasibility of a robust and low computational power UWB range error mitigation.",0
"Although Ultra-wideband (UWB) is currently the most advanced and widely used technology for wireless localization, achieving precise ranging and localization in non-line-of-sight (NLoS) conditions remains a subject of ongoing research. The indoor radio environment is complex, and multipath effects, reflections, and refractions can introduce errors in ranging measurements, resulting in inaccurate and unsatisfactory position estimation. To address this issue, this article proposes a representation learning methodology that utilizes the latest advancements in deep learning and graph optimization techniques to mitigate ranging errors at the edge. The methodology employs Channel Impulse Response (CIR) signals to extract high semantic features for estimating corrections in both NLoS and LoS conditions. Extensive experimentation with various settings and configurations has demonstrated the effectiveness and feasibility of our approach, enabling robust and low computational power UWB range error mitigation.",1
"Remarkable performance from Transformer networks in Natural Language Processing promote the development of these models in dealing with computer vision tasks such as image recognition and segmentation. In this paper, we introduce a novel framework, called Multi-level Multi-scale Point Transformer (MLMSPT) that works directly on the irregular point clouds for representation learning. Specifically, a point pyramid transformer is investigated to model features with diverse resolutions or scales we defined, followed by a multi-level transformer module to aggregate contextual information from different levels of each scale and enhance their interactions. While a multi-scale transformer module is designed to capture the dependencies among representations across different scales. Extensive evaluation on public benchmark datasets demonstrate the effectiveness and the competitive performance of our methods on 3D shape classification, part segmentation and semantic segmentation tasks.",0
"The impressive performance of Transformer networks in Natural Language Processing has led to their application in computer vision tasks such as image recognition and segmentation. This paper presents a new approach, the Multi-level Multi-scale Point Transformer (MLMSPT), which is designed for representation learning on irregular point clouds. The framework includes a point pyramid transformer to model features at different resolutions, a multi-level transformer module to combine contextual information from different scales, and a multi-scale transformer module to capture dependencies among representations. Our methods are evaluated on public benchmark datasets for 3D shape classification, part segmentation, and semantic segmentation tasks, and show competitive performance.",1
"Graph neural networks (GNNs) are a powerful inductive bias for modelling algorithmic reasoning procedures and data structures. Their prowess was mainly demonstrated on tasks featuring Markovian dynamics, where querying any associated data structure depends only on its latest state. For many tasks of interest, however, it may be highly beneficial to support efficient data structure queries dependent on previous states. This requires tracking the data structure's evolution through time, placing significant pressure on the GNN's latent representations. We introduce Persistent Message Passing (PMP), a mechanism which endows GNNs with capability of querying past state by explicitly persisting it: rather than overwriting node representations, it creates new nodes whenever required. PMP generalises out-of-distribution to more than 2x larger test inputs on dynamic temporal range queries, significantly outperforming GNNs which overwrite states.",0
"Graph neural networks (GNNs) are a strong tool for modelling algorithmic reasoning procedures and data structures. They have been primarily successful in tasks involving Markovian dynamics, where data structures can be queried based only on their most recent state. However, for many tasks, it would be advantageous to efficiently query data structures based on previous states. This puts pressure on the GNN's latent representations as they must track the data structure's evolution over time. To address this issue, we propose Persistent Message Passing (PMP), which enables GNNs to query past states by creating new nodes instead of overwriting existing ones. PMP outperforms GNNs that overwrite states and demonstrates robustness to larger test inputs on dynamic temporal range queries.",1
"Social Media Platforms (SMPs) like Facebook, Twitter, Instagram etc. have large user base all around the world that generates huge amount of data every second. This includes a lot of posts by fake and spam users, typically used by many organisations around the globe to have competitive edge over others. In this work, we aim at detecting such user accounts in Twitter using a novel approach. We show how to distinguish between Genuine and Spam accounts in Twitter using a combination of Graph Representation Learning and Natural Language Processing techniques.",0
"Social Media Platforms (SMPs) such as Facebook, Twitter, and Instagram have a vast global user base that produces vast amounts of data every second. This data includes numerous posts generated by fake and spam users, which many organizations worldwide use to gain a competitive advantage over others. Our objective is to identify such user accounts on Twitter using a unique method. We demonstrate how to differentiate between genuine and spam accounts on Twitter by utilizing a blend of Graph Representation Learning and Natural Language Processing techniques.",1
"For many of the 700 million illiterate people around the world, speech recognition technology could provide a bridge to valuable information and services. Yet, those most in need of this technology are often the most underserved by it. In many countries, illiterate people tend to speak only low-resource languages, for which the datasets necessary for speech technology development are scarce. In this paper, we investigate the effectiveness of unsupervised speech representation learning on noisy radio broadcasting archives, which are abundant even in low-resource languages. We make three core contributions. First, we release two datasets to the research community. The first, West African Radio Corpus, contains 142 hours of audio in more than 10 languages with a labeled validation subset. The second, West African Virtual Assistant Speech Recognition Corpus, consists of 10K labeled audio clips in four languages. Next, we share West African wav2vec, a speech encoder trained on the noisy radio corpus, and compare it with the baseline Facebook speech encoder trained on six times more data of higher quality. We show that West African wav2vec performs similarly to the baseline on a multilingual speech recognition task, and significantly outperforms the baseline on a West African language identification task. Finally, we share the first-ever speech recognition models for Maninka, Pular and Susu, languages spoken by a combined 10 million people in over seven countries, including six where the majority of the adult population is illiterate. Our contributions offer a path forward for ethical AI research to serve the needs of those most disadvantaged by the digital divide.",0
"Speech recognition technology has the potential to provide valuable information and services for the 700 million illiterate individuals worldwide. However, those who would benefit the most from this technology are often the ones who are underserved by it. The reason for this is that in many countries, illiterate people tend to speak low-resource languages, which have limited datasets available for speech technology development. This paper aims to explore the effectiveness of unsupervised speech representation learning on noisy radio broadcasting archives, which are more abundant even in low-resource languages. The study makes three primary contributions. Firstly, it introduces two datasets to the research community - West African Radio Corpus, which contains 142 hours of audio in over 10 languages with a labeled validation subset, and West African Virtual Assistant Speech Recognition Corpus, which consists of 10K labeled audio clips in four languages. Secondly, it shares West African wav2vec, a speech encoder that has been trained on the noisy radio corpus and is compared to the baseline Facebook speech encoder trained on six times more data of higher quality. The study demonstrates that West African wav2vec performs similarly to the baseline on a multilingual speech recognition task and significantly outperforms the baseline on a West African language identification task. Finally, the study shares the first-ever speech recognition models for Maninka, Pular, and Susu, languages spoken by over ten million people in seven countries, including six where the majority of the adult population is illiterate. These contributions pave the way for ethical AI research to serve the needs of those most disadvantaged by the digital divide.",1
"Self-attention mechanism recently achieves impressive advancement in Natural Language Processing (NLP) and Image Processing domains. And its permutation invariance property makes it ideally suitable for point cloud processing. Inspired by this remarkable success, we propose an end-to-end architecture, dubbed Cross-Level Cross-Scale Cross-Attention Network (CLCSCANet), for point cloud representation learning. First, a point-wise feature pyramid module is introduced to hierarchically extract features from different scales or resolutions. Then a cross-level cross-attention is designed to model long-range inter-level and intra-level dependencies. Finally, we develop a cross-scale cross-attention module to capture interactions between-and-within scales for representation enhancement. Compared with state-of-the-art approaches, our network can obtain competitive performance on challenging 3D object classification, point cloud segmentation tasks via comprehensive experimental evaluation.",0
"The self-attention mechanism has made significant progress in the areas of Natural Language Processing (NLP) and Image Processing. Its permutation invariance feature makes it an ideal fit for point cloud processing. Our proposed Cross-Level Cross-Scale Cross-Attention Network (CLCSCANet) utilizes this success to create an end-to-end architecture for learning point cloud representation. Initially, a point-wise feature pyramid module is implemented to extract features hierarchically from varying scales or resolutions. Subsequently, a cross-level cross-attention module is designed to model long-range inter-level and intra-level dependencies. Finally, we develop a cross-scale cross-attention module to enhance representation through interactions between-and-within scales. Our network outperforms state-of-the-art approaches in challenging 3D object classification and point cloud segmentation tasks based on comprehensive empirical assessments.",1
"This paper studies the problem of novel category discovery on single- and multi-modal data with labels from different but relevant categories. We present a generic, end-to-end framework to jointly learn a reliable representation and assign clusters to unlabelled data. To avoid over-fitting the learnt embedding to labelled data, we take inspiration from self-supervised representation learning by noise-contrastive estimation and extend it to jointly handle labelled and unlabelled data. In particular, we propose using category discrimination on labelled data and cross-modal discrimination on multi-modal data to augment instance discrimination used in conventional contrastive learning approaches. We further employ Winner-Take-All (WTA) hashing algorithm on the shared representation space to generate pairwise pseudo labels for unlabelled data to better predict cluster assignments. We thoroughly evaluate our framework on large-scale multi-modal video benchmarks Kinetics-400 and VGG-Sound, and image benchmarks CIFAR10, CIFAR100 and ImageNet, obtaining state-of-the-art results.",0
"This article examines the challenge of discovering new categories in data that has labels from relevant but distinct categories, regardless of whether it is single- or multi-modal. We propose an all-inclusive approach that not only learns a reliable representation but also assigns clusters to unlabeled information. To prevent the learned embedding from overfitting to the labeled data, we adopt the self-supervised representation learning method of noise-contrastive estimation and expand it to handle both labeled and unlabeled data. Specifically, we suggest employing category differentiation on labeled data and cross-modal differentiation on multi-modal data to supplement the instance differentiation used in traditional contrastive learning methods. We also use the Winner-Take-All (WTA) hashing algorithm on the shared representation space to create pairwise pseudo labels for unlabeled data, which aids in better predicting cluster assignments. We comprehensively evaluate our methodology on large-scale multi-modal video datasets Kinetics-400 and VGG-Sound, as well as image datasets CIFAR10, CIFAR100, and ImageNet, achieving superior outcomes.",1
"Unsupervised attributed graph representation learning is challenging since both structural and feature information are required to be represented in the latent space. Existing methods concentrate on learning latent representation via reconstruction tasks, but cannot directly optimize representation and are prone to oversmoothing, thus limiting the applications on downstream tasks. To alleviate these issues, we propose a novel graph embedding framework named Deep Manifold Attributed Graph Embedding (DMAGE). A node-to-node geodesic similarity is proposed to compute the inter-node similarity between the data space and the latent space and then use Bergman divergence as loss function to minimize the difference between them. We then design a new network structure with fewer aggregation to alleviate the oversmoothing problem and incorporate graph structure augmentation to improve the representation's stability. Our proposed DMAGE surpasses state-of-the-art methods by a significant margin on three downstream tasks: unsupervised visualization, node clustering, and link prediction across four popular datasets.",0
"Learning representations for attributed graphs in an unsupervised manner is a challenging task that requires incorporating both structural and feature information into the latent space. Existing methods rely on reconstructive tasks to learn latent representations, which are not directly optimized and often lead to oversmoothing, limiting their applicability to downstream tasks. To address these limitations, we propose a novel graph embedding framework called Deep Manifold Attributed Graph Embedding (DMAGE). Our approach computes inter-node similarity using a node-to-node geodesic similarity measure and minimizes the difference between the data space and latent space using the Bergman divergence as the loss function. To overcome oversmoothing, we introduce a new network structure with fewer aggregations and incorporate graph structure augmentation to enhance representation stability. Our proposed DMAGE outperforms state-of-the-art methods significantly on three downstream tasks, including unsupervised visualization, node clustering, and link prediction, across four popular datasets.",1
"Following the tremendous success of transformer in natural language processing and image understanding tasks, in this paper, we present a novel point cloud representation learning architecture, named Dual Transformer Network (DTNet), which mainly consists of Dual Point Cloud Transformer (DPCT) module. Specifically, by aggregating the well-designed point-wise and channel-wise multi-head self-attention models simultaneously, DPCT module can capture much richer contextual dependencies semantically from the perspective of position and channel. With the DPCT module as a fundamental component, we construct the DTNet for performing point cloud analysis in an end-to-end manner. Extensive quantitative and qualitative experiments on publicly available benchmarks demonstrate the effectiveness of our proposed transformer framework for the tasks of 3D point cloud classification and segmentation, achieving highly competitive performance in comparison with the state-of-the-art approaches.",0
"In this paper, we introduce the Dual Transformer Network (DTNet), a groundbreaking architecture for point cloud representation learning. After the success of transformer in natural language processing and image analysis, we developed a new module called Dual Point Cloud Transformer (DPCT) that combines point-wise and channel-wise multi-head self-attention models. The DPCT module can capture more contextual dependencies in terms of position and channel, resulting in a richer semantic understanding. Using the DPCT module, we created DTNet, an end-to-end system for point cloud analysis. Our experiments on publicly available benchmarks show that our transformer framework achieves high performance in 3D point cloud classification and segmentation, surpassing state-of-the-art approaches.",1
"Virtually all of deep learning literature relies on the assumption of large amounts of available training data. Indeed, even the majority of few-shot learning methods rely on a large set of ""base classes"" for pretraining. This assumption, however, does not always hold. For some tasks, annotating a large number of classes can be infeasible, and even collecting the images themselves can be a challenge in some scenarios. In this paper, we study this problem and call it ""Small Data"" setting, in contrast to ""Big Data"". To unlock the full potential of small data, we propose to augment the models with annotations for other related tasks, thus increasing their generalization abilities. In particular, we use the richly annotated scene parsing dataset ADE20K to construct our realistic Long-tail Recognition with Diverse Supervision (LRDS) benchmark by splitting the object categories into head and tail based on their distribution. Following the standard few-shot learning protocol, we use the head classes for representation learning and the tail classes for evaluation. Moreover, we further subsample the head categories and images to generate two novel settings which we call ""Scarce-Class"" and ""Scarce-Image"", respectively corresponding to the shortage of samples for rare classes and training images. Finally, we analyze the effect of applying various additional supervision sources under the proposed settings. Our experiments demonstrate that densely labeling a small set of images can indeed largely remedy the small data constraints.",0
"Most of the deep learning research assumes the availability of abundant training data. Even few-shot learning methods depend on a substantial number of ""base classes"" for pretraining. However, this assumption is not always valid, as some tasks may require annotating too many classes or collecting images, which can be challenging. This study investigates this issue, which we call ""Small Data,"" in contrast to ""Big Data."" To maximize the potential of small data, we suggest enhancing the models with annotations for related tasks to enhance their generalization abilities. We use the ADE20K scene parsing dataset to develop our realistic Long-tail Recognition with Diverse Supervision (LRDS) benchmark by separating object categories into head and tail based on their distribution. We use the head classes for representation learning and the tail classes for evaluation, following the standard few-shot learning protocol. We also create two novel settings, ""Scarce-Class"" and ""Scarce-Image,"" by further subsampling the head categories and images, respectively, to reflect the scarcity of samples for rare classes and training images. Finally, we assess the impact of applying diverse additional supervision sources under the proposed settings. Our experiments reveal that densely labeling a small set of images can significantly alleviate the small data limitations.",1
"We develop an approach to learning visual representations that embraces multimodal data, driven by a combination of intra- and inter-modal similarity preservation objectives. Unlike existing visual pre-training methods, which solve a proxy prediction task in a single domain, our method exploits intrinsic data properties within each modality and semantic information from cross-modal correlation simultaneously, hence improving the quality of learned visual representations. By including multimodal training in a unified framework with different types of contrastive losses, our method can learn more powerful and generic visual features. We first train our model on COCO and evaluate the learned visual representations on various downstream tasks including image classification, object detection, and instance segmentation. For example, the visual representations pre-trained on COCO by our method achieve state-of-the-art top-1 validation accuracy of $55.3\%$ on ImageNet classification, under the common transfer protocol. We also evaluate our method on the large-scale Stock images dataset and show its effectiveness on multi-label image tagging, and cross-modal retrieval tasks.",0
"Our method for learning visual representations incorporates multimodal data and a combination of intra- and inter-modal similarity preservation objectives. Unlike existing visual pre-training techniques that address a proxy prediction task in a single domain, our approach leverages intrinsic data properties within each modality and semantic information from cross-modal correlation simultaneously, leading to higher quality learned visual representations. By integrating multimodal training into a unified framework that employs various types of contrastive losses, our method can generate more robust and general visual features. We initially train the model on COCO and then assess the learned visual representations on different downstream tasks such as object detection, image classification, and instance segmentation. Our method's pre-trained visual representations for COCO achieve a top-1 validation accuracy of $55.3\%$ on ImageNet classification under the common transfer protocol, demonstrating its superiority. We also demonstrate the effectiveness of our technique on the large-scale Stock images dataset, highlighting its capability for cross-modal retrieval tasks and multi-label image tagging.",1
"Deep models have improved state-of-the-art for both supervised and unsupervised learning. For example, deep embedded clustering (DEC) has greatly improved the unsupervised clustering performance, by using stacked autoencoders for representation learning. However, one weakness of deep modeling is that the local neighborhood structure in the original space is not necessarily preserved in the latent space. To preserve local geometry, various methods have been proposed in the supervised and semi-supervised learning literature (e.g., spectral clustering and label propagation) using graph Laplacian regularization. In this paper, we combine the strength of deep representation learning with measure propagation (MP), a KL-divergence based graph regularization method originally used in the semi-supervised scenario. The main assumption of MP is that if two data points are close in the original space, they are likely to belong to the same class, measured by KL-divergence of class membership distribution. By taking the same assumption in the unsupervised learning scenario, we propose our Deep Embedded Clustering Aided by Measure Propagation (DECAMP) model. We evaluate DECAMP on short text clustering tasks. On three public datasets, DECAMP performs competitively with other state-of-the-art baselines, including baselines using additional data to generate word embeddings used in the clustering process. As an example, on the Stackoverflow dataset, DECAMP achieved a clustering accuracy of 79%, which is about 5% higher than all existing baselines. These empirical results suggest that DECAMP is a very effective method for unsupervised learning.",0
"The application of deep models has led to significant improvements in both supervised and unsupervised learning. For instance, stacked autoencoders in deep embedded clustering (DEC) have enhanced unsupervised clustering performance. However, one limitation of the deep modeling approach is that the local neighborhood structure in the original space may not be preserved in the latent space. To address this issue, several approaches have been proposed in the literature, such as spectral clustering and label propagation, which use graph Laplacian regularization to maintain local geometry. This paper proposes the Deep Embedded Clustering Aided by Measure Propagation (DECAMP) model, which combines deep representation learning with measure propagation (MP), a KL-divergence based graph regularization method originally used in the semi-supervised scenario. DECAMP assumes that two data points close in the original space are likely to belong to the same class, as measured by KL-divergence of class membership distribution. We evaluate DECAMP on short text clustering tasks and find that it performs competitively with other state-of-the-art baselines, achieving a clustering accuracy of 79% on the Stackoverflow dataset, which is about 5% higher than all existing baselines. These results suggest that DECAMP is a highly effective method for unsupervised learning.",1
"We present a collaborative learning method called Mutual Contrastive Learning (MCL) for general visual representation learning. The core idea of MCL is to perform mutual interaction and transfer of contrastive distributions among a cohort of models. Benefiting from MCL, each model can learn extra contrastive knowledge from others, leading to more meaningful feature representations for visual recognition tasks. We emphasize that MCL is conceptually simple yet empirically powerful. It is a generic framework that can be applied to both supervised and self-supervised representation learning. Experimental results on supervised and self-supervised image classification, transfer learning and few-shot learning show that MCL can lead to consistent performance gains, demonstrating that MCL can guide the network to generate better feature representations.",0
"Our article introduces Mutual Contrastive Learning (MCL), a method for visual representation learning that involves collaboration among models. MCL involves the transfer and mutual interaction of contrastive distributions among a group of models, allowing each one to gain extra knowledge from others, leading to more effective feature representations for visual recognition tasks. MCL is a straightforward but highly effective framework that can be applied to both supervised and self-supervised representation learning. Our experiments on supervised and self-supervised image classification, transfer learning, and few-shot learning demonstrate that MCL consistently improves performance, indicating that it can guide networks to generate better feature representations.",1
"Temporal networks serve as abstractions of many real-world dynamic systems. These networks typically evolve according to certain laws, such as the law of triadic closure, which is universal in social networks. Inductive representation learning of temporal networks should be able to capture such laws and further be applied to systems that follow the same laws but have not been unseen during the training stage. Previous works in this area depend on either network node identities or rich edge attributes and typically fail to extract these laws. Here, we propose Causal Anonymous Walks (CAWs) to inductively represent a temporal network. CAWs are extracted by temporal random walks and work as automatic retrieval of temporal network motifs to represent network dynamics while avoiding the time-consuming selection and counting of those motifs. CAWs adopt a novel anonymization strategy that replaces node identities with the hitting counts of the nodes based on a set of sampled walks to keep the method inductive, and simultaneously establish the correlation between motifs. We further propose a neural-network model CAW-N to encode CAWs, and pair it with a CAW sampling strategy with constant memory and time cost to support online training and inference. CAW-N is evaluated to predict links over 6 real temporal networks and uniformly outperforms previous SOTA methods by averaged 10% AUC gain in the inductive setting. CAW-N also outperforms previous methods in 4 out of the 6 networks in the transductive setting.",0
"Temporal networks are abstractions used to represent dynamic systems in the real world. These networks follow specific laws, such as the law of triadic closure, which is universal in social networks. Inductive representation learning of temporal networks must capture these laws and be applicable to systems that follow the same laws but were not seen during training. Current methods rely on network node identities or rich edge attributes, which fail to extract these laws. To address this, we propose Causal Anonymous Walks (CAWs), which represent a temporal network by extracting motifs using temporal random walks. CAWs adopt an anonymization strategy to keep the method inductive and establish the correlation between motifs. We introduce a neural-network model, CAW-N, to encode CAWs, which is evaluated to predict links over 6 real temporal networks. CAW-N outperforms previous methods in both the inductive and transductive settings. Additionally, our proposed CAW sampling strategy supports online training and inference with constant memory and time cost.",1
"In this paper, a novel unsupervised low-rank representation model, i.e., Auto-weighted Low-Rank Representation (ALRR), is proposed to construct a more favorable similarity graph (SG) for clustering. In particular, ALRR enhances the discriminability of SG by capturing the multi-subspace structure and extracting the salient features simultaneously. Specifically, an auto-weighted penalty is introduced to learn a similarity graph by highlighting the effective features, and meanwhile, overshadowing the disturbed features. Consequently, ALRR obtains a similarity graph that can preserve the intrinsic geometrical structures within the data by enforcing a smaller similarity on two dissimilar samples. Moreover, we employ a block-diagonal regularizer to guarantee the learned graph contains $k$ diagonal blocks. This can facilitate a more discriminative representation learning for clustering tasks. Extensive experimental results on synthetic and real databases demonstrate the superiority of ALRR over other state-of-the-art methods with a margin of 1.8\%$\sim$10.8\%.",0
"The paper introduces a new approach called Auto-weighted Low-Rank Representation (ALRR) for creating a more optimal similarity graph (SG) for clustering. ALRR improves the SG's ability to distinguish between data points by simultaneously capturing the multi-subspace structure and extracting essential features. To achieve this, an auto-weighted penalty is applied to prioritize effective features and suppress the disturbed ones when constructing the SG. As a result, the SG generated by ALRR is capable of preserving the intrinsic geometrical structures of the data by reducing similarity between dissimilar samples. Furthermore, a block-diagonal regularizer is used to ensure that the learned graph has k diagonal blocks, which helps in creating a more discriminating representation for clustering tasks. The paper presents extensive experimental results on synthetic and actual databases that demonstrate the superiority of ALRR over other state-of-the-art methods, with a margin of 1.8% to 10.8%.",1
"The common self-supervised pre-training practice requires collecting massive unlabeled data together and then trains a representation model, dubbed \textbf{joint training}. However, in real-world scenarios where data are collected in a streaming fashion, the joint training scheme is usually storage-heavy and time-consuming. A more efficient alternative is to train a model continually with streaming data, dubbed \textbf{sequential training}. Nevertheless, it is unclear how well sequential self-supervised pre-training performs with streaming data. In this paper, we conduct thorough experiments to investigate self-supervised pre-training with streaming data. Specifically, we evaluate the transfer performance of sequential self-supervised pre-training with four different data sequences on three different downstream tasks and make comparisons with joint self-supervised pre-training. Surprisingly, we find sequential self-supervised learning exhibits almost the same performance as the joint training when the distribution shifts within streaming data are mild. Even for data sequences with large distribution shifts, sequential self-supervised training with simple techniques, e.g., parameter regularization or data replay, still performs comparably to joint training. Based on our findings, we recommend using sequential self-supervised training as a \textbf{more efficient yet performance-competitive} representation learning practice for real-world applications.",0
"The typical method of self-supervised pre-training involves collecting vast amounts of unlabeled data and training a representation model, known as joint training. This approach, however, can be impractical in real-world scenarios where data is continuously acquired. Sequential training, an alternative method that involves continually training a model with streaming data, is more efficient but its performance with streaming data is unclear. To address this, we conducted experiments to evaluate the transfer performance of sequential self-supervised pre-training with different data sequences on various downstream tasks and compared it with joint self-supervised pre-training. Surprisingly, we found that sequential self-supervised learning achieves similar performance to joint training, even with data sequences that have significant distribution shifts, with the use of simple techniques such as parameter regularization or data replay. Therefore, we recommend using sequential self-supervised training as a more efficient yet performance-competitive approach for real-world applications.",1
"Recently, a considerable literature has grown up around the theme of Graph Convolutional Network (GCN). How to effectively leverage the rich structural information in complex graphs, such as knowledge graphs with heterogeneous types of entities and relations, is a primary open challenge in the field. Most GCN methods are either restricted to graphs with a homogeneous type of edges (e.g., citation links only), or focusing on representation learning for nodes only instead of jointly propagating and updating the embeddings of both nodes and edges for target-driven objectives. This paper addresses these limitations by proposing a novel framework, namely the Knowledge Embedding based Graph Convolutional Network (KE-GCN), which combines the power of GCNs in graph-based belief propagation and the strengths of advanced knowledge embedding (a.k.a. knowledge graph embedding) methods, and goes beyond. Our theoretical analysis shows that KE-GCN offers an elegant unification of several well-known GCN methods as specific cases, with a new perspective of graph convolution. Experimental results on benchmark datasets show the advantageous performance of KE-GCN over strong baseline methods in the tasks of knowledge graph alignment and entity classification.",0
"The theme of Graph Convolutional Network (GCN) has sparked a considerable amount of literature in recent times. However, effectively leveraging the abundant structural information in complex graphs, such as knowledge graphs with diverse types of entities and relations, remains a significant challenge in the field. Many GCN methods are limited to graphs with homogeneous edges, like citation links, or focus solely on node representation learning instead of jointly propagating and updating embeddings of both nodes and edges for target-driven objectives. This paper proposes a novel framework called Knowledge Embedding based Graph Convolutional Network (KE-GCN) that overcomes these limitations. It combines the power of GCNs in graph-based belief propagation with the strengths of advanced knowledge embedding methods and offers a fresh perspective on graph convolution. Our theoretical analysis demonstrates that KE-GCN unifies several well-known GCN methods as specific cases. The experimental results on benchmark datasets demonstrate that KE-GCN outperforms strong baseline methods in the tasks of knowledge graph alignment and entity classification.",1
"Several regularization methods have recently been introduced which force the latent activations of an autoencoder or deep neural network to conform to either a Gaussian or hyperspherical distribution, or to minimize the implicit rank of the distribution in latent space. In the present work, we introduce a novel regularizing loss function which simulates a pairwise repulsive force between items and an attractive force of each item toward the origin. We show that minimizing this loss function in isolation achieves a hyperspherical distribution. Moreover, when used as a regularizing term, the scaling factor can be adjusted to allow greater flexibility and tolerance of eccentricity, thus allowing the latent variables to be stratified according to their relative importance, while still promoting diversity. We apply this method of Eccentric Regularization to an autoencoder, and demonstrate its effectiveness in image generation, representation learning and downstream classification tasks.",0
"Recently, new regularization techniques have emerged that compel autoencoder or deep neural network's latent activations to adhere to a Gaussian or hyperspherical distribution or to reduce the implicit rank of the distribution in latent space. Our study introduces an innovative regularizing loss function that emulates a pairwise repulsive force among items and an attractive force of each item towards the origin. This loss function minimization alone produces a hyperspherical distribution. When used as a regularizing term, the scaling factor can be adjusted to allow greater flexibility and tolerance of eccentricity, which stratifies latent variables according to their relative importance while still promoting diversity. We implemented this Eccentric Regularization method to an autoencoder and demonstrated its effectiveness in generating images, learning representations, and downstream classification tasks.",1
"We consider two popular Graph Representation Learning (GRL) methods: message passing for node classification and network embedding for link prediction. For each, we pick a popular model that we: (i) linearize and (ii) and switch its training objective to Frobenius norm error minimization. These simplifications can cast the training into finding the optimal parameters in closed-form. We program in TensorFlow a functional form of Truncated Singular Value Decomposition (SVD), such that, we could decompose a dense matrix $\mathbf{M}$, without explicitly computing $\mathbf{M}$. We achieve competitive performance on popular GRL tasks while providing orders of magnitude speedup. We open-source our code at http://github.com/samihaija/tf-fsvd",0
"In this study, we examine two widely used methods for Graph Representation Learning (GRL): message passing for node classification and network embedding for link prediction. For each method, we select a popular model and simplify it by (i) linearizing and (ii) changing its training objective to minimize the Frobenius norm error. These simplifications allow us to find optimal parameters in closed-form. We use TensorFlow to program a functional form of Truncated Singular Value Decomposition (SVD) that can decompose a dense matrix $\mathbf{M}$ without explicitly computing it. By doing so, we achieve competitive performance on popular GRL tasks and significant speedup. Our code is available for public use on http://github.com/samihaija/tf-fsvd.",1
"Having access to multi-modal cues (e.g. vision and audio) empowers some cognitive tasks to be done faster compared to learning from a single modality. In this work, we propose to transfer knowledge across heterogeneous modalities, even though these data modalities may not be semantically correlated. Rather than directly aligning the representations of different modalities, we compose audio, image, and video representations across modalities to uncover richer multi-modal knowledge. Our main idea is to learn a compositional embedding that closes the cross-modal semantic gap and captures the task-relevant semantics, which facilitates pulling together representations across modalities by compositional contrastive learning. We establish a new, comprehensive multi-modal distillation benchmark on three video datasets: UCF101, ActivityNet, and VGGSound. Moreover, we demonstrate that our model significantly outperforms a variety of existing knowledge distillation methods in transferring audio-visual knowledge to improve video representation learning. Code is released here: https://github.com/yanbeic/CCL.",0
"The use of multiple cues such as vision and audio can improve the speed of cognitive tasks compared to relying on one modality. In this study, we propose transferring knowledge between different modalities, even if they are not semantically related. Rather than aligning the representations of each modality, we combine audio, image, and video representations to gain more comprehensive multi-modal knowledge. Our main idea is to learn a compositional embedding that bridges the semantic gap and captures relevant task semantics, enabling us to bring together representations from different modalities through compositional contrastive learning. We introduce a new multi-modal distillation benchmark on three video datasets: UCF101, ActivityNet, and VGGSound. Additionally, our model surpasses various existing knowledge distillation techniques in transferring audio-visual knowledge and enhancing video representation learning. Our code is available at https://github.com/yanbeic/CCL.",1
"Self-supervised learning has shown great potentials in improving the video representation ability of deep neural networks by getting supervision from the data itself. However, some of the current methods tend to cheat from the background, i.e., the prediction is highly dependent on the video background instead of the motion, making the model vulnerable to background changes. To mitigate the model reliance towards the background, we propose to remove the background impact by adding the background. That is, given a video, we randomly select a static frame and add it to every other frames to construct a distracting video sample. Then we force the model to pull the feature of the distracting video and the feature of the original video closer, so that the model is explicitly restricted to resist the background influence, focusing more on the motion changes. We term our method as \emph{Background Erasing} (BE). It is worth noting that the implementation of our method is so simple and neat and can be added to most of the SOTA methods without much efforts. Specifically, BE brings 16.4% and 19.1% improvements with MoCo on the severely biased datasets UCF101 and HMDB51, and 14.5% improvement on the less biased dataset Diving48.",0
"Deep neural networks have been enhanced in their ability to represent video through self-supervised learning, wherein the data itself provides supervision. However, current methods are prone to relying excessively on the video background, rather than the motion, rendering the model vulnerable to background changes. To address this problem, we propose the addition of the background to video samples, such that the model is forced to resist background influence and focus on motion changes. This method, called Background Erasing (BE), is easily implemented and can be added to most state-of-the-art (SOTA) methods with minimal effort. BE has shown significant improvements of 16.4% and 19.1% with MoCo on the biased datasets UCF101 and HMDB51, and 14.5% on the less biased dataset Diving48.",1
"Graphs have been widely used to represent complex data in many applications. Efficient and effective analysis of graphs is important for graph-based applications. However, most graph analysis tasks are combinatorial optimization (CO) problems, which are NP-hard. Recent studies have focused a lot on the potential of using machine learning (ML) to solve graph-based CO problems. Most recent methods follow the two-stage framework. The first stage is graph representation learning, which embeds the graphs into low-dimension vectors. The second stage uses ML to solve the CO problems using the embeddings of the graphs learned in the first stage. The works for the first stage can be classified into two categories, graph embedding (GE) methods and end-to-end (E2E) learning methods. For GE methods, learning graph embedding has its own objective, which may not rely on the CO problems to be solved. The CO problems are solved by independent downstream tasks. For E2E learning methods, the learning of graph embeddings does not have its own objective and is an intermediate step of the learning procedure of solving the CO problems. The works for the second stage can also be classified into two categories, non-autoregressive methods and autoregressive methods. Non-autoregressive methods predict a solution for a CO problem in one shot. A non-autoregressive method predicts a matrix that denotes the probability of each node/edge being a part of a solution of the CO problem. The solution can be computed from the matrix. Autoregressive methods iteratively extend a partial solution step by step. At each step, an autoregressive method predicts a node/edge conditioned to current partial solution, which is used to its extension. In this survey, we provide a thorough overview of recent studies of the graph learning-based CO methods. The survey ends with several remarks on future research directions.",0
"The use of graphs to represent complex data in various applications is widespread. Efficiently analyzing graphs is crucial for graph-based applications, but most graph analysis tasks are NP-hard combinatorial optimization (CO) problems. Recently, machine learning (ML) has been explored as a potential solution for graph-based CO problems, with most methods following a two-stage framework. The first stage involves graph representation learning to embed graphs into low-dimensional vectors. The second stage uses ML to solve CO problems with the learned embeddings. There are two categories of methods for the first stage: graph embedding (GE) methods and end-to-end (E2E) learning methods. GE methods have their own objective for learning graph embeddings, while E2E methods use graph embeddings as an intermediate step for solving CO problems. The second stage can also be classified into two categories: non-autoregressive and autoregressive methods. Non-autoregressive methods predict a solution for CO problems in one shot, while autoregressive methods extend a partial solution iteratively. This survey provides a comprehensive overview of recent studies on graph learning-based CO methods, ending with suggestions for future research directions.",1
"Homography estimation is often an indispensable step in many computer vision tasks. The existing approaches, however, are not robust to illumination and/or larger viewpoint changes. In this paper, we propose bidirectional implicit Homography Estimation (biHomE) loss for unsupervised homography estimation. biHomE minimizes the distance in the feature space between the warped image from the source viewpoint and the corresponding image from the target viewpoint. Since we use a fixed pre-trained feature extractor and the only learnable component of our framework is the homography network, we effectively decouple the homography estimation from representation learning. We use an additional photometric distortion step in the synthetic COCO dataset generation to better represent the illumination variation of the real-world scenarios. We show that biHomE achieves state-of-the-art performance on synthetic COCO dataset, which is also comparable or better compared to supervised approaches. Furthermore, the empirical results demonstrate the robustness of our approach to illumination variation compared to existing methods.",0
"Many computer vision tasks require homography estimation, but current methods are not reliable when faced with changes in illumination or viewpoint. This paper introduces the bidirectional implicit Homography Estimation (biHomE) loss for unsupervised homography estimation. biHomE minimizes the distance between the warped image from the source viewpoint and the corresponding image from the target viewpoint in the feature space. Our method decouples homography estimation from representation learning by using a fixed pre-trained feature extractor and a learnable homography network. Additionally, we use a photometric distortion step in the synthetic COCO dataset generation to better represent real-world illumination variation. Our approach achieves state-of-the-art performance on the synthetic COCO dataset, comparable or better than supervised approaches, and demonstrates robustness to illumination variation compared to existing methods.",1
"We present a new generic method for shadow-aware multi-view satellite photogrammetry of Earth Observation scenes. Our proposed method, the Shadow Neural Radiance Field (S-NeRF) follows recent advances in implicit volumetric representation learning. For each scene, we train S-NeRF using very high spatial resolution optical images taken from known viewing angles. The learning requires no labels or shape priors: it is self-supervised by an image reconstruction loss. To accommodate for changing light source conditions both from a directional light source (the Sun) and a diffuse light source (the sky), we extend the NeRF approach in two ways. First, direct illumination from the Sun is modeled via a local light source visibility field. Second, indirect illumination from a diffuse light source is learned as a non-local color field as a function of the position of the Sun. Quantitatively, the combination of these factors reduces the altitude and color errors in shaded areas, compared to NeRF. The S-NeRF methodology not only performs novel view synthesis and full 3D shape estimation, it also enables shadow detection, albedo synthesis, and transient object filtering, without any explicit shape supervision.",0
"A new technique for multi-view satellite photogrammetry that accounts for shadows in Earth Observation scenes is introduced. The proposed method, called Shadow Neural Radiance Field (S-NeRF), employs implicit volumetric representation learning and does not require labels or shape priors. S-NeRF is trained using high-resolution optical images taken from known viewing angles and can adapt to changes in the directional and diffuse light sources. It models direct illumination from the Sun using a local light source visibility field and indirect illumination from the sky using a non-local color field that depends on the Sun's position. S-NeRF reduces altitude and color errors in shaded areas compared to NeRF and enables shadow detection, albedo synthesis, and transient object filtering without explicit shape supervision. In addition to providing novel view synthesis and full 3D shape estimation, S-NeRF offers several other benefits.",1
"Recently, there has been great success in applying deep neural networks on graph structured data. Most work, however, focuses on either node- or graph-level supervised learning, such as node, link or graph classification or node-level unsupervised learning (e.g. node clustering). Despite its wide range of possible applications, graph-level unsupervised learning has not received much attention yet. This might be mainly attributed to the high representation complexity of graphs, which can be represented by n! equivalent adjacency matrices, where n is the number of nodes. In this work we address this issue by proposing a permutation-invariant variational autoencoder for graph structured data. Our proposed model indirectly learns to match the node ordering of input and output graph, without imposing a particular node ordering or performing expensive graph matching. We demonstrate the effectiveness of our proposed model on various graph reconstruction and generation tasks and evaluate the expressive power of extracted representations for downstream graph-level classification and regression.",0
"Deep neural networks have shown significant progress in dealing with graph structured data. However, current research mainly focuses on supervised learning at either node or graph levels, such as node clustering or node, link, and graph classification. Graph-level unsupervised learning has not yet received much attention due to the high complexity of graph representation, which can have n! equivalent adjacency matrices, where n is the number of nodes. Our research addresses this issue by proposing a permutation-invariant variational autoencoder that indirectly learns to match the node ordering of input and output graphs without imposing a specific node ordering or performing expensive graph matching. Our model has shown effectiveness in various graph reconstruction and generation tasks, and we have evaluated the extracted representations' expressive power for downstream graph-level classification and regression.",1
"Object-centric representations have recently enabled significant progress in tackling relational reasoning tasks. By building a strong object-centric inductive bias into neural architectures, recent efforts have improved generalization and data efficiency of machine learning algorithms for these problems. One problem class involving relational reasoning that still remains under-explored is multi-agent reinforcement learning (MARL). Here we investigate whether object-centric representations are also beneficial in the fully cooperative MARL setting. Specifically, we study two ways of incorporating an agent-centric inductive bias into our RL algorithm: 1. Introducing an agent-centric attention module with explicit connections across agents 2. Adding an agent-centric unsupervised predictive objective (i.e. not using action labels), to be used as an auxiliary loss for MARL, or as the basis of a pre-training step. We evaluate these approaches on the Google Research Football environment as well as DeepMind Lab 2D. Empirically, agent-centric representation learning leads to the emergence of more complex cooperation strategies between agents as well as enhanced sample efficiency and generalization.",0
"Relational reasoning tasks have seen significant progress with the use of object-centric representations. Recent efforts have improved the data efficiency and generalization of machine learning algorithms by incorporating a strong object-centric inductive bias into neural architectures. However, multi-agent reinforcement learning (MARL) involving relational reasoning remains under-explored. This study investigates the benefits of object-centric representations in the fully cooperative MARL setting. Two approaches are studied: introducing an agent-centric attention module with explicit connections across agents, and adding an agent-centric unsupervised predictive objective. The study evaluates these approaches on the Google Research Football environment and DeepMind Lab 2D. The results show that agent-centric representation learning leads to more complex cooperation strategies, improved sample efficiency, and enhanced generalization.",1
"We present a novel method to learn temporally consistent 3D reconstruction of clothed people from a monocular video. Recent methods for 3D human reconstruction from monocular video using volumetric, implicit or parametric human shape models, produce per frame reconstructions giving temporally inconsistent output and limited performance when applied to video. In this paper, we introduce an approach to learn temporally consistent features for textured reconstruction of clothed 3D human sequences from monocular video by proposing two advances: a novel temporal consistency loss function; and hybrid representation learning for implicit 3D reconstruction from 2D images and coarse 3D geometry. The proposed advances improve the temporal consistency and accuracy of both the 3D reconstruction and texture prediction from a monocular video. Comprehensive comparative performance evaluation on images of people demonstrates that the proposed method significantly outperforms the state-of-the-art learning-based single image 3D human shape estimation approaches achieving significant improvement of reconstruction accuracy, completeness, quality and temporal consistency.",0
"A new technique is presented for achieving a consistent 3D reconstruction of clothed individuals from a single video. Previous methods for creating 3D human models using volumetric, implicit or parametric shapes resulted in temporally inconsistent output and limited performance when applied to video. The proposed approach addresses these issues by introducing a novel temporal consistency loss function and hybrid representation learning for implicit 3D reconstruction. These advances improve the temporal consistency and accuracy of both the 3D reconstruction and texture prediction from a monocular video. Comparative evaluations show that this method outperforms existing single image 3D human shape estimation techniques in terms of accuracy, completeness, quality and temporal consistency.",1
"Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code is publicly available at https://github.com/zhoudaquan/dvit_repo.",0
"Recently, Vision transformers (ViTs) have been used successfully for image classification tasks. However, unlike convolution neural networks (CNNs), adding more convolutional layers does not enhance the performance of ViTs as they reach saturation quickly when scaled deeper. This scaling difficulty is due to the attention collapse issue, where the attention maps become similar in deeper layers of the transformer, resulting in identical feature maps in the top layers of deep ViT models. This issue hinders the self-attention mechanism's ability to learn effective concepts for representation learning, leading to a lack of expected performance gains. To tackle this issue, we propose a simple and effective method called Re-attention, which generates diverse attention maps at different layers with minor computation and memory cost. This method enables the training of deeper ViT models with consistent performance improvements, even up to 1.6% Top-1 classification accuracy on ImageNet with 32 transformer blocks. The code is publicly available at https://github.com/zhoudaquan/dvit_repo.",1
"Deep neural network-based classifiers trained with the categorical cross-entropy (CCE) loss are sensitive to label noise in the training data. One common type of method that can mitigate the impact of label noise can be viewed as supervised robust methods; one can simply replace the CCE loss with a loss that is robust to label noise, or re-weight training samples and down-weight those with higher loss values. Recently, another type of method using semi-supervised learning (SSL) has been proposed, which augments these supervised robust methods to exploit (possibly) noisy samples more effectively. Although supervised robust methods perform well across different data types, they have been shown to be inferior to the SSL methods on image classification tasks under label noise. Therefore, it remains to be seen that whether these supervised robust methods can also perform well if they can utilize the unlabeled samples more effectively. In this paper, we show that by initializing supervised robust methods using representations learned through contrastive learning leads to significantly improved performance under label noise. Surprisingly, even the simplest method (training a classifier with the CCE loss) can outperform the state-of-the-art SSL method by more than 50\% under high label noise when initialized with contrastive learning. Our implementation will be publicly available at {\url{https://github.com/arghosh/noisy_label_pretrain}}.",0
"Classifiers based on deep neural networks and trained with categorical cross-entropy loss are vulnerable to label noise during training. One way to address this is through supervised robust methods, which involve replacing the CCE loss with a loss that is less sensitive to label noise, or down-weighting training samples with higher loss values. However, recent research has shown that semi-supervised learning methods are more effective than supervised robust methods for image classification under label noise. It is unclear whether supervised robust methods can perform better if they incorporate unlabeled samples more effectively. This study demonstrates that initializing supervised robust methods with representations learned through contrastive learning can significantly improve their performance under label noise. Surprisingly, even the simplest method of training a classifier with CCE loss can outperform state-of-the-art SSL methods by over 50% under high label noise when initialized with contrastive learning. The implementation of this approach will be publicly available at {\url{https://github.com/arghosh/noisy_label_pretrain}}.",1
"Adversarial examples causing evasive predictions are widely used to evaluate and improve the robustness of machine learning models. However, current studies on adversarial examples focus on supervised learning tasks, relying on the ground-truth data label, a targeted objective, or supervision from a trained classifier. In this paper, we propose a framework of generating adversarial examples for unsupervised models and demonstrate novel applications to data augmentation. Our framework exploits a mutual information neural estimator as an information-theoretic similarity measure to generate adversarial examples without supervision. We propose a new MinMax algorithm with provable convergence guarantees for efficient generation of unsupervised adversarial examples. Our framework can also be extended to supervised adversarial examples. When using unsupervised adversarial examples as a simple plug-in data augmentation tool for model retraining, significant improvements are consistently observed across different unsupervised tasks and datasets, including data reconstruction, representation learning, and contrastive learning. Our results show novel methods and advantages in studying and improving robustness of unsupervised learning problems via adversarial examples. Our codes are available at https://github.com/IBM/UAE.",0
"The use of adversarial examples to test and enhance the resilience of machine learning models is widespread. However, current research on adversarial examples focuses on supervised learning tasks, which rely on ground-truth labels, targeted objectives, or supervision from a trained classifier. This article introduces a framework for generating adversarial examples for unsupervised models and presents innovative applications for data augmentation. By utilizing a mutual information neural estimator, our framework generates unsupervised adversarial examples without supervision, and we propose a MinMax algorithm with guaranteed convergence for efficient generation. Our approach can also be adapted for supervised adversarial examples. Our results demonstrate the effectiveness of using unsupervised adversarial examples as a data augmentation tool for model retraining, with consistent improvements observed across various unsupervised tasks and datasets, including data reconstruction, representation learning, and contrastive learning. Our study highlights the potential benefits of using adversarial examples to enhance the robustness of unsupervised learning problems. Our code is available on GitHub at https://github.com/IBM/UAE.",1
"We propose a new model, the Neighbor Mixture Model (NMM), for modeling node labels in a graph. This model aims to capture correlations between the labels of nodes in a local neighborhood. We carefully design the model so it could be an alternative to a Markov Random Field but with more affordable computations. In particular, drawing samples and evaluating marginal probabilities of single labels can be done in linear time. To scale computations to large graphs, we devise a variational approximation without introducing extra parameters. We further use graph neural networks (GNNs) to parameterize the NMM, which reduces the number of learnable parameters while allowing expressive representation learning. The proposed model can be either fit directly to large observed graphs or used to enable scalable inference that preserves correlations for other distributions such as deep generative graph models. Across a diverse set of node classification, image denoising, and link prediction tasks, we show our proposed NMM advances the state-of-the-art in modeling real-world labeled graphs.",0
"Our suggestion is a new method, called the Neighbor Mixture Model (NMM), to model node labels in a graph. The goal of this model is to capture correlations between the labels of nodes within a local neighborhood. We have carefully designed the model to be a more affordable and efficient alternative to a Markov Random Field. Specifically, we can draw samples and evaluate marginal probabilities of single labels in linear time. To handle large graphs, we have created a variational approximation without introducing extra parameters. Additionally, we have used graph neural networks (GNNs) to parameterize the NMM, which reduces the number of learnable parameters while still allowing expressive representation learning. The proposed method can be directly applied to large observed graphs or used to enable scalable inference that preserves correlations for other distributions such as deep generative graph models. We have demonstrated the effectiveness of the NMM across various tasks, such as node classification, image denoising, and link prediction, and have shown that it surpasses the current state-of-the-art in modeling real-world labeled graphs.",1
"Time series with missing data are signals encountered in important settings for machine learning. Some of the most successful prior approaches for modeling such time series are based on recurrent neural networks that transform the input and previous state to account for the missing observations, and then treat the transformed signal in a standard manner.   In this paper, we introduce a single unifying framework, Recursive Input and State Estimation (RISE), for this general approach and reformulate existing models as specific instances of this framework. We then explore additional novel variations within the RISE framework to improve the performance of any instance. We exploit representation learning techniques to learn latent representations of the signals used by RISE instances. We discuss and develop various encoding techniques to learn latent signal representations. We benchmark instances of the framework with various encoding functions on three data imputation datasets, observing that RISE instances always benefit from encoders that learn representations for numerical values from the digits into which they can be decomposed.",0
"Signals with missing data in time series are frequently encountered in machine learning settings. Recurrent neural networks have been successful in modeling these time series by transforming the input and previous state to account for the missing observations and then treating the transformed signal conventionally. This paper introduces a unified framework called Recursive Input and State Estimation (RISE) for this approach and reformulates existing models as specific instances of the framework. Additional variations within the RISE framework are explored to enhance the performance of any instance, including representation learning techniques to learn latent signal representations. Various encoding techniques are also discussed and developed to learn these latent signal representations. The framework is benchmarked with various encoding functions on three data imputation datasets, revealing that encoders that learn representations for numerical values from the digits into which they can be decomposed always benefit RISE instances.",1
"Many unsupervised representation learning methods belong to the class of similarity learning models. While various modality-specific approaches exist for different types of data, a core property of many methods is that representations of similar inputs are close under some similarity function. We propose EMDE (Efficient Manifold Density Estimator) - a framework utilizing arbitrary vector representations with the property of local similarity to succinctly represent smooth probability densities on Riemannian manifolds. Our approximate representation has the desirable properties of being fixed-size and having simple additive compositionality, thus being especially amenable to treatment with neural networks - both as input and output format, producing efficient conditional estimators. We generalize and reformulate the problem of multi-modal recommendations as conditional, weighted density estimation on manifolds. Our approach allows for trivial inclusion of multiple interaction types, modalities of data as well as interaction strengths for any recommendation setting. Applying EMDE to both top-k and session-based recommendation settings, we establish new state-of-the-art results on multiple open datasets in both uni-modal and multi-modal settings.",0
"Several unsupervised representation learning techniques fall under the category of similarity learning models, with modality-specific approaches available for various data types. The common feature of these methods is that comparable inputs have representations that are close under certain similarity functions. Our proposal, the Efficient Manifold Density Estimator (EMDE), is a framework that creates concise representations of smooth probability densities on Riemannian manifolds using arbitrary vector representations with local similarity properties. Our approximate representation is of a fixed size and has straightforward additive compositionality, making it ideal for use with neural networks as both input and output formats. This produces efficient conditional estimators. We reframe the issue of multi-modal recommendations as conditional, weighted density estimation on manifolds using our approach. It allows for easy integration of multiple interaction types, data modalities, and interaction strengths in any recommendation setting. We demonstrate the effectiveness of EMDE in both top-k and session-based recommendation settings, achieving state-of-the-art results on several open datasets in both uni- and multi-modal settings.",1
"This paper studies active learning (AL) on graphs, whose purpose is to discover the most informative nodes to maximize the performance of graph neural networks (GNNs). Previously, most graph AL methods focus on learning node representations from a carefully selected labeled dataset with large amount of unlabeled data neglected. Motivated by the success of contrastive learning (CL), we propose a novel paradigm that seamlessly integrates graph AL with CL. While being able to leverage the power of abundant unlabeled data in a self-supervised manner, nodes selected by AL further provide semantic information that can better guide representation learning. Besides, previous work measures the informativeness of nodes without considering the neighborhood propagation scheme of GNNs, so that noisy nodes may be selected. We argue that due to the smoothing nature of GNNs, the central nodes from homophilous subgraphs should benefit the model training most. To this end, we present a minimax selection scheme that explicitly harnesses neighborhood information and discover homophilous subgraphs to facilitate active selection. Comprehensive, confounding-free experiments on five public datasets demonstrate the superiority of our method over state-of-the-arts.",0
"The aim of this research is to examine active learning (AL) on graphs in order to identify the most informative nodes for optimizing the performance of graph neural networks (GNNs). Previous methods for graph AL have concentrated on learning node representations from a labeled dataset, with less attention given to unlabeled data. Drawing on the success of contrastive learning (CL), the authors propose a new approach that combines graph AL with CL, enabling the use of abundant self-supervised unlabeled data and providing semantic information to guide representation learning. Existing methods for measuring node informativeness do not consider the neighborhood propagation scheme of GNNs, which may result in the selection of noisy nodes. To address this, the authors suggest that central nodes from homophilous subgraphs are the most beneficial for model training due to the smoothing nature of GNNs. As such, they introduce a minimax selection scheme that explicitly incorporates neighborhood information and identifies homophilous subgraphs to facilitate active selection. The authors conducted comprehensive experiments on five public datasets to demonstrate the superiority of their approach over existing state-of-the-art methods.",1
"Graph neural networks (GNNs) have been widely used in deep learning on graphs. They can learn effective node representations that achieve superior performances in graph analysis tasks such as node classification and node clustering. However, most methods ignore the heterogeneity in real-world graphs. Methods designed for heterogeneous graphs, on the other hand, fail to learn complex semantic representations because they only use meta-paths instead of meta-graphs. Furthermore, they cannot fully capture the content-based correlations between nodes, as they either do not use the self-attention mechanism or only use it to consider the immediate neighbors of each node, ignoring the higher-order neighbors. We propose a novel Higher-order Attribute-Enhancing (HAE) framework that enhances node embedding in a layer-by-layer manner. Under the HAE framework, we propose a Higher-order Attribute-Enhancing Graph Neural Network (HAEGNN) for heterogeneous network representation learning. HAEGNN simultaneously incorporates meta-paths and meta-graphs for rich, heterogeneous semantics, and leverages the self-attention mechanism to explore content-based nodes interactions. The unique higher-order architecture of HAEGNN allows examining the first-order as well as higher-order neighborhoods. Moreover, HAEGNN shows good explainability as it learns the importances of different meta-paths and meta-graphs. HAEGNN is also memory-efficient, for it avoids per meta-path based matrix calculation. Experimental results not only show HAEGNN superior performance against the state-of-the-art methods in node classification, node clustering, and visualization, but also demonstrate its superiorities in terms of memory efficiency and explainability.",0
"Deep learning on graphs has been revolutionized by Graph Neural Networks (GNNs), which are widely utilized to learn effective node representations that outperform traditional methods in graph analysis tasks such as node classification and clustering. However, existing methods often overlook the heterogeneity present in real-world graphs. On the other hand, methods designed for heterogeneous graphs focus solely on meta-paths, which fail to learn complex semantic representations and are unable to fully capture content-based correlations between nodes. To address these limitations, we propose a novel Higher-order Attribute-Enhancing (HAE) framework that enhances node embedding layer by layer. This framework enables us to create a Higher-order Attribute-Enhancing Graph Neural Network (HAEGNN) for learning heterogeneous network representations. Our proposed model combines meta-paths and meta-graphs to capture rich, heterogeneous semantics and leverages the self-attention mechanism to explore content-based node interactions. Additionally, the higher-order architecture of HAEGNN allows for the examination of both first-order and higher-order neighborhoods, while maintaining good explainability through the learning of importance weights for different meta-paths and meta-graphs. Furthermore, HAEGNN is memory-efficient, eliminating the need for per meta-path based matrix calculation. Our experimental results demonstrate that HAEGNN outperforms existing state-of-the-art methods in node classification, clustering, and visualization, while also demonstrating superior memory efficiency and explainability.",1
"Graph Neural Networks (GNNs) have been widely used for the representation learning of various structured graph data, typically through message passing among nodes by aggregating their neighborhood information via different operations. While promising, most existing GNNs oversimplified the complexity and diversity of the edges in the graph, and thus inefficient to cope with ubiquitous heterogeneous graphs, which are typically in the form of multi-relational graph representations. In this paper, we propose RioGNN, a novel Reinforced, recursive and flexible neighborhood selection guided multi-relational Graph Neural Network architecture, to navigate complexity of neural network structures whilst maintaining relation-dependent representations. We first construct a multi-relational graph, according to the practical task, to reflect the heterogeneity of nodes, edges, attributes and labels. To avoid the embedding over-assimilation among different types of nodes, we employ a label-aware neural similarity measure to ascertain the most similar neighbors based on node attributes. A reinforced relation-aware neighbor selection mechanism is developed to choose the most similar neighbors of a targeting node within a relation before aggregating all neighborhood information from different relations to obtain the eventual node embedding. Particularly, to improve the efficiency of neighbor selecting, we propose a new recursive and scalable reinforcement learning framework with estimable depth and width for different scales of multi-relational graphs. RioGNN can learn more discriminative node embedding with enhanced explainability due to the recognition of individual importance of each relation via the filtering threshold mechanism.",0
"The use of Graph Neural Networks (GNNs) has become widespread for the purpose of learning representations of various structured graph data. This is typically achieved through the exchange of messages among nodes, by collecting information from their immediate surroundings through various operations. Despite the potential benefits of GNNs, most existing models oversimplify the complexity and diversity of the edges in graphs, making them less efficient in dealing with ubiquitous heterogeneous graphs, which are usually represented as multi-relational graphs. In this study, we introduce RioGNN, a novel Graph Neural Network architecture that employs reinforced, recursive, and flexible neighborhood selection to guide the representation learning process for multi-relational graphs. We first construct a multi-relational graph that reflects the heterogeneity of nodes, edges, attributes, and labels, based on the practical task at hand. To prevent embedding over-assimilation among different types of nodes, we use a label-aware neural similarity measure to determine the most similar neighbors based on node attributes. We then develop a reinforced relation-aware neighbor selection mechanism to choose the most similar neighbors of a node within a relation, before aggregating all neighborhood information from different relations to obtain the final node embedding. To improve the efficiency of neighbor selection, we propose a new recursive and scalable reinforcement learning framework with estimable depth and width for different scales of multi-relational graphs. RioGNN can learn more discriminative node embeddings with enhanced explainability, as it recognizes the individual importance of each relation through the filtering threshold mechanism.",1
"Representation learning has significantly been developed with the advance of contrastive learning methods. Most of those methods have benefited from various data augmentations that are carefully designated to maintain their identities so that the images transformed from the same instance can still be retrieved. However, those carefully designed transformations limited us to further explore the novel patterns exposed by other transformations. Meanwhile, as found in our experiments, the strong augmentations distorted the images' structures, resulting in difficult retrieval. Thus, we propose a general framework called Contrastive Learning with Stronger Augmentations~(CLSA) to complement current contrastive learning approaches. Here, the distribution divergence between the weakly and strongly augmented images over the representation bank is adopted to supervise the retrieval of strongly augmented queries from a pool of instances. Experiments on the ImageNet dataset and downstream datasets showed the information from the strongly augmented images can significantly boost the performance. For example, CLSA achieves top-1 accuracy of 76.2% on ImageNet with a standard ResNet-50 architecture with a single-layer classifier fine-tuned, which is almost the same level as 76.5% of supervised results. The code and pre-trained models are available in https://github.com/maple-research-lab/CLSA.",0
"The development of contrastive learning methods has greatly advanced representation learning. These methods have benefited from carefully selected data augmentations that maintain the identity of the transformed images, allowing for retrieval of images from the same instance. However, these limited transformations prevent exploration of novel patterns that may be exposed by other transformations. Additionally, strong augmentations can distort image structures, making retrieval difficult. To address these issues, we propose a framework called Contrastive Learning with Stronger Augmentations (CLSA) to complement current contrastive learning approaches. The distribution divergence between weakly and strongly augmented images is utilized to supervise retrieval of strongly augmented queries from a pool of instances. Experiments on ImageNet and downstream datasets show that information from strongly augmented images can significantly boost performance. CLSA achieves a top-1 accuracy of 76.2% on ImageNet with a standard ResNet-50 architecture with a single-layer classifier fine-tuned, nearly matching supervised results. The code and pre-trained models are available on https://github.com/maple-research-lab/CLSA.",1
"Recent work learns contextual representations of source code by reconstructing tokens from their context. For downstream semantic understanding tasks like summarizing code in English, these representations should ideally capture program functionality. However, we show that the popular reconstruction-based BERT model is sensitive to source code edits, even when the edits preserve semantics. We propose ContraCode: a contrastive pre-training task that learns code functionality, not form. ContraCode pre-trains a neural network to identify functionally similar variants of a program among many non-equivalent distractors. We scalably generate these variants using an automated source-to-source compiler as a form of data augmentation. Contrastive pre-training improves JavaScript summarization and TypeScript type inference accuracy by 2% to 13%. We also propose a new zero-shot JavaScript code clone detection dataset, showing that ContraCode is both more robust and semantically meaningful. On it, we outperform RoBERTa by 39% AUROC in an adversarial setting and up to 5% on natural code.",0
"The current approach to understanding source code involves reconstructing tokens in their context to create contextual representations. However, these representations should ideally capture program functionality for semantic understanding tasks like summarizing code in English. Yet, the reconstruction-based BERT model is shown to be sensitive to source code edits, even when the edits maintain semantics. To solve this issue, we introduce ContraCode, a contrastive pre-training task that focuses on learning code functionality rather than form. ContraCode trains a neural network to identify functionally similar versions of a program from numerous non-equivalent alternatives. We generate these variants using an automated source-to-source compiler as data augmentation. Contrastive pre-training results in a 2% to 13% improvement in JavaScript summarization and TypeScript type inference accuracy. We also introduce a new zero-shot JavaScript code clone detection dataset, demonstrating ContraCode's superior robustness and semantic meaning. In an adversarial setting, we outperform RoBERTa by 39% AUROC and up to 5% on natural code.",1
"Recent works for attributed network clustering utilize graph convolution to obtain node embeddings and simultaneously perform clustering assignments on the embedding space. It is effective since graph convolution combines the structural and attributive information for node embedding learning. However, a major limitation of such works is that the graph convolution only incorporates the attribute information from the local neighborhood of nodes but fails to exploit the mutual affinities between nodes and attributes. In this regard, we propose a variational co-embedding learning model for attributed network clustering (VCLANC). VCLANC is composed of dual variational auto-encoders to simultaneously embed nodes and attributes. Relying on this, the mutual affinity information between nodes and attributes could be reconstructed from the embedding space and served as extra self-supervised knowledge for representation learning. At the same time, trainable Gaussian mixture model is used as priors to infer the node clustering assignments. To strengthen the performance of the inferred clusters, we use a mutual distance loss on the centers of the Gaussian priors and a clustering assignment hardening loss on the node embeddings. Experimental results on four real-world attributed network datasets demonstrate the effectiveness of the proposed VCLANC for attributed network clustering.",0
"Attributed network clustering has recently utilized graph convolution for node embeddings and clustering assignments in the embedding space. This approach effectively combines structural and attributive information for node embedding learning. However, it has a limitation since graph convolution only considers attribute information from local neighborhoods of nodes and fails to leverage mutual affinities between nodes and attributes. To address this issue, a variational co-embedding learning model for attributed network clustering (VCLANC) is proposed. VCLANC uses dual variational auto-encoders to simultaneously embed nodes and attributes, allowing for mutual affinity information between nodes and attributes to be reconstructed from the embedding space. Additionally, a trainable Gaussian mixture model is used as priors to infer node clustering assignments. To improve the performance of the inferred clusters, a mutual distance loss on the centers of the Gaussian priors and a clustering assignment hardening loss on the node embeddings are utilized. Experimental results on four real-world attributed network datasets demonstrate the effectiveness of VCLANC for attributed network clustering.",1
"Self-supervised learning is an empirically successful approach to unsupervised learning based on creating artificial supervised learning problems. A popular self-supervised approach to representation learning is contrastive learning, which leverages naturally occurring pairs of similar and dissimilar data points, or multiple views of the same data. This work provides a theoretical analysis of contrastive learning in the multi-view setting, where two views of each datum are available. The main result is that linear functions of the learned representations are nearly optimal on downstream prediction tasks whenever the two views provide redundant information about the label.",0
"Self-supervised learning is a technique for unsupervised learning that has achieved empirical success by constructing artificial supervised learning problems. Contrastive learning is a prevalent approach to representation learning within self-supervised learning, which utilizes naturally formed pairs of similar and dissimilar data points or multiple perspectives of the same data. This study presents a theoretical examination of contrastive learning in the multi-view context, where two views of each datum are accessible. The primary outcome is that linear functions of the learned representations are close to optimal on downstream prediction tasks if the two views offer overlapping information about the label.",1
"Existing CNNs-Based RGB-D Salient Object Detection (SOD) networks are all required to be pre-trained on the ImageNet to learn the hierarchy features which can help to provide a good initialization. However, the collection and annotation of large-scale datasets are time-consuming and expensive. In this paper, we utilize Self-Supervised Representation Learning (SSL) to design two pretext tasks: the cross-modal auto-encoder and the depth-contour estimation. Our pretext tasks require only a few and unlabeled RGB-D datasets to perform pre-training, which makes the network capture rich semantic contexts and reduce the gap between two modalities, thereby providing an effective initialization for the downstream task. In addition, for the inherent problem of cross-modal fusion in RGB-D SOD, we propose a consistency-difference aggregation (CDA) module that splits a single feature fusion into multi-path fusion to achieve an adequate perception of consistent and differential information. The CDA module is general and suitable for both cross-modal and cross-level feature fusion. Extensive experiments on six benchmark RGB-D SOD datasets, our model pre-trained on the RGB-D dataset ($6,392$ without any annotations) can perform favorably against most state-of-the-art RGB-D methods pre-trained on ImageNet ($1,280,000$ with image-level annotations).",0
"Presently, RGB-D Salient Object Detection (SOD) networks based on Convolutional Neural Networks (CNNs) require pre-training on ImageNet for learning hierarchical features, which is time-consuming and expensive due to the need for collecting and annotating large-scale datasets. This paper proposes using Self-Supervised Representation Learning (SSL) to create two pretext tasks, namely the cross-modal auto-encoder and depth-contour estimation, which require only a few unlabeled RGB-D datasets for pre-training. This approach enables the network to capture rich semantic contexts and reduce the gap between modalities, providing an effective initialization for downstream tasks. Additionally, a consistency-difference aggregation (CDA) module is proposed to address cross-modal fusion issues in RGB-D SOD. The CDA module can split a single feature fusion into multi-path fusion for achieving an adequate perception of consistent and differential information, and is suitable for both cross-modal and cross-level feature fusion. Extensive experiments on six benchmark RGB-D SOD datasets show that our model pre-trained on an RGB-D dataset of 6,392 without any annotations can perform favorably against most state-of-the-art RGB-D methods pre-trained on ImageNet with image-level annotations of 1,280,000.",1
"Simplicial complexes form an important class of topological spaces that are frequently used to in many applications areas such as computer-aided design, computer graphics, and simulation. The representation learning on graphs, which are just 1-d simplicial complexes, has witnessed a great attention and success in the past few years. Due to the additional complexity higher dimensional simplicial hold, there has not been enough effort to extend representation learning to these objects especially when it comes to learn entire-simplicial complex representation. In this work, we propose a method for simplicial complex-level representation learning that embeds a simplicial complex to a universal embedding space in a way that complex-to-complex proximity is preserved. Our method utilizes a simplex-level embedding induced by a pre-trained simplicial autoencoder to learn an entire simplicial complex representation. To the best of our knowledge, this work presents the first method for learning simplicial complex-level representation.",0
"Simplicial complexes are a significant type of topological spaces that find frequent use in various application areas, such as computer graphics, computer-aided design, and simulation. In recent years, representation learning on graphs (1-d simplicial complexes) has gained immense attention and success. However, extending this learning to higher dimensional simplicial complexes has been challenging due to their added complexity. Consequently, there has been a lack of effort to learn an entire-simplicial complex representation. In this study, we propose a method for learning a simplicial complex-level representation that preserves complex-to-complex proximity. Our approach employs a simplex-level embedding produced by a pre-trained simplicial autoencoder to learn the representation of an entire simplicial complex. This work presents the first method for learning a simplicial complex-level representation.",1
"Graph representation learning has long been an important yet challenging task for various real-world applications. However, their downstream tasks are mainly performed in the settings of supervised or semi-supervised learning. Inspired by recent advances in unsupervised contrastive learning, this paper is thus motivated to investigate how the node-wise contrastive learning could be performed. Particularly, we respectively resolve the class collision issue and the imbalanced negative data distribution issue. Extensive experiments are performed on three real-world datasets and the proposed approach achieves the SOTA model performance.",0
"For numerous practical applications, the process of learning graph representations has been a significant but difficult undertaking. Although their subsequent tasks are predominantly carried out in supervised or semi-supervised learning scenarios. This study is encouraged by recent progress in unsupervised contrastive learning and aims to explore how node-wise contrastive learning can be accomplished. Specifically, we tackle the problems of class collision and uneven distribution of negative data. We conduct extensive experiments on three actual datasets, and the proposed method attains a state-of-the-art model performance.",1
"Graph neural networks (GNN) have been proven to be mature enough for handling graph-structured data on node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation is critical yet still challenging. Existing pooling methods either struggle to capture the local substructure or fail to effectively utilize high-order dependency, thus diminishing the expression capability. In this paper we propose HAP, a hierarchical graph-level representation learning framework, which is adaptively sensitive to graph structures, i.e., HAP clusters local substructures incorporating with high-order dependencies. HAP utilizes a novel cross-level attention mechanism MOA to naturally focus more on close neighborhood while effectively capture higher-order dependency that may contain crucial information. It also learns a global graph content GCont that extracts the graph pattern properties to make the pre- and post-coarsening graph content maintain stable, thus providing global guidance in graph coarsening. This novel innovation also facilitates generalization across graphs with the same form of features. Extensive experiments on fourteen datasets show that HAP significantly outperforms twelve popular graph pooling methods on graph classification task with an maximum accuracy improvement of 22.79%, and exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.5% and 16.7%.",0
"Graph neural networks (GNN) have achieved maturity in handling graph-structured data on node-level graph representation learning tasks. However, the challenge of learning expressive graph-level representation via graph pooling techniques remains critical. Existing pooling methods struggle to capture local substructure or effectively utilize high-order dependency, thus limiting their expression capability. In this paper, we introduce a novel hierarchical graph-level representation learning framework called HAP. HAP is sensitive to graph structures and clusters local substructures with high-order dependencies. It employs a cross-level attention mechanism, MOA, to naturally focus on close neighborhoods and capture crucial higher-order dependency information. HAP also learns a global graph content, GCont, that extracts graph pattern properties to maintain stable pre- and post-coarsening graph content and provide global guidance in graph coarsening. This innovation facilitates generalization across graphs with the same feature form. Our extensive experiments on fourteen datasets demonstrate that HAP significantly outperforms twelve popular graph pooling methods in graph classification tasks, with a maximum accuracy improvement of 22.79%. Furthermore, HAP exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.5% and 16.7%, respectively.",1
"This work concerns video-language pre-training and representation learning. In this now ubiquitous training scheme, a model first performs pre-training on paired videos and text (e.g., video clips and accompanied subtitles) from a large uncurated source corpus, before transferring to specific downstream tasks. This two-stage training process inevitably raises questions about the generalization ability of the pre-trained model, which is particularly pronounced when a salient domain gap exists between source and target data (e.g., instructional cooking videos vs. movies). In this paper, we first bring to light the sensitivity of pre-training objectives (contrastive vs. reconstructive) to domain discrepancy. Then, we propose a simple yet effective framework, CUPID, to bridge this domain gap by filtering and adapting source data to the target data, followed by domain-focused pre-training. Comprehensive experiments demonstrate that pre-training on a considerably small subset of domain-focused data can effectively close the source-target domain gap and achieve significant performance gain, compared to random sampling or even exploiting the full pre-training dataset. CUPID yields new state-of-the-art performance across multiple video-language and video tasks, including text-to-video retrieval [72, 37], video question answering [36], and video captioning [72], with consistent performance lift over different pre-training methods.",0
"This article is about video-language pre-training and representation learning. The training process involves a model pre-training on paired videos and text from a large uncurated source corpus before moving on to specific tasks. However, this two-stage process raises questions about the generalization ability of the pre-trained model, especially when there is a domain gap between the source and target data. In this paper, the authors explore the sensitivity of pre-training objectives to domain discrepancy and propose a framework called CUPID to bridge this gap. They demonstrate that pre-training on a small subset of domain-focused data using CUPID can effectively close the source-target domain gap and achieve significant performance gain. The results show that CUPID outperforms other pre-training methods across multiple video-language and video tasks.",1
"Jointly identifying a mixture of discrete and continuous factors of variability without supervision is a key problem in unraveling complex phenomena. Variational inference has emerged as a promising method to learn interpretable mixture representations. However, posterior approximation in high-dimensional latent spaces, particularly for discrete factors remains challenging. Here, we propose an unsupervised variational framework using multiple interacting networks called cpl-mixVAE that scales well to high-dimensional discrete settings. In this framework, the mixture representation of each network is regularized by imposing a consensus constraint on the discrete factor. We justify the use of this framework by providing both theoretical and experimental results. Finally, we use the proposed method to jointly uncover discrete and continuous factors of variability describing gene expression in a single-cell transcriptomic dataset profiling more than a hundred cortical neuron types.",0
"Identifying a combination of discrete and continuous factors of variation without supervision is a crucial challenge in understanding complex phenomena. Variational inference has shown promise in creating understandable mixture representations. However, approximating the posterior in high-dimensional latent spaces, especially for discrete factors, is still problematic. To address this, we introduce an unsupervised variational framework called cpl-mixVAE that uses multiple interacting networks and scales well to high-dimensional discrete settings. The mixture representation of each network is regularized by imposing a consensus constraint on the discrete factor. We support this framework with both theoretical and experimental evidence. As a final step, we apply our method to a single-cell transcriptomic dataset with over a hundred cortical neuron types to jointly reveal discrete and continuous factors of variability that describe gene expression.",1
"In recent years, graph neural networks (GNNs) have been widely adopted in the representation learning of graph-structured data and provided state-of-the-art performance in various applications such as link prediction, node classification, and recommendation. Motivated by recent advances of self-supervision for representation learning in natural language processing and computer vision, self-supervised learning has been recently studied to leverage unlabeled graph-structured data. However, employing self-supervision tasks as auxiliary tasks to assist a primary task has been less explored in the literature on graphs. In this paper, we propose a novel self-supervised auxiliary learning framework to effectively learn graph neural networks. Moreover, this work is the first study showing that a meta-path prediction is beneficial as a self-supervised auxiliary task for heterogeneous graphs. Our method is learning to learn a primary task with various auxiliary tasks to improve generalization performance. The proposed method identifies an effective combination of auxiliary tasks and automatically balances them to improve the primary task. Our methods can be applied to any graph neural network in a plug-in manner without manual labeling or additional data. Also, it can be extended to any other auxiliary tasks. Our experiments demonstrate that the proposed method consistently improves the performance of node classification and link prediction.",0
"Graph neural networks (GNNs) have gained widespread popularity in recent years for their ability to learn representations of graph-structured data and deliver top-notch results in applications like node classification, link prediction, and recommendation. While self-supervised learning has proven effective in natural language processing and computer vision, its potential for leveraging unlabeled graph-structured data has only recently been explored. However, the use of self-supervision tasks as auxiliary tasks to enhance primary task performance has not been extensively studied in the graph literature. This paper introduces a novel self-supervised auxiliary learning framework for effectively training GNNs, highlighting the benefits of meta-path prediction as a self-supervised auxiliary task for heterogeneous graphs. Our approach learns to learn primary tasks with multiple auxiliary tasks, automatically balancing them to improve primary task performance. It can be applied to any GNN in a plug-in manner without manual labeling or additional data, and can be extended to other auxiliary tasks. Our experiments illustrate that our method consistently boosts the performance of node classification and link prediction.",1
"This paper introduces Relative Predictive Coding (RPC), a new contrastive representation learning objective that maintains a good balance among training stability, minibatch size sensitivity, and downstream task performance. The key to the success of RPC is two-fold. First, RPC introduces the relative parameters to regularize the objective for boundedness and low variance. Second, RPC contains no logarithm and exponential score functions, which are the main cause of training instability in prior contrastive objectives. We empirically verify the effectiveness of RPC on benchmark vision and speech self-supervised learning tasks. Lastly, we relate RPC with mutual information (MI) estimation, showing RPC can be used to estimate MI with low variance.",0
"A new method of contrastive representation learning called Relative Predictive Coding (RPC) is introduced in this paper. This method maintains a good balance between training stability, minibatch size sensitivity, and downstream task performance. RPC's success is due to two factors. The first is the use of relative parameters to regulate the objective's boundedness and low variance. The second is the absence of logarithm and exponential score functions, which have been the main cause of training instability in previous contrastive objectives. The effectiveness of RPC is empirically proven using benchmark vision and speech self-supervised learning tasks. Additionally, RPC is related to mutual information (MI) estimation, as it can estimate MI with low variance.",1
"Real-world imagery is often characterized by a significant imbalance of the number of images per class, leading to long-tailed distributions. An effective and simple approach to long-tailed visual recognition is to learn feature representations and a classifier separately, with instance and class-balanced sampling, respectively. In this work, we introduce a new framework, by making the key observation that a feature representation learned with instance sampling is far from optimal in a long-tailed setting. Our main contribution is a new training method, referred to as Class-Balanced Distillation (CBD), that leverages knowledge distillation to enhance feature representations. CBD allows the feature representation to evolve in the second training stage, guided by the teacher learned in the first stage. The second stage uses class-balanced sampling, in order to focus on under-represented classes. This framework can naturally accommodate the usage of multiple teachers, unlocking the information from an ensemble of models to enhance recognition capabilities. Our experiments show that the proposed technique consistently outperforms the state of the art on long-tailed recognition benchmarks such as ImageNet-LT, iNaturalist17 and iNaturalist18. The experiments also show that our method does not sacrifice the accuracy of head classes to improve the performance of tail classes, unlike most existing work.",0
"Long-tailed distributions are common in real-world imagery due to an imbalance in the number of images per class. To address this issue in visual recognition, it is effective to learn feature representations and classifiers separately using instance and class-balanced sampling, respectively. However, our work introduces a new framework that improves upon this method by recognizing that a feature representation learned through instance sampling is suboptimal in a long-tailed setting. Our contribution is a new training method called Class-Balanced Distillation (CBD), which utilizes knowledge distillation to enhance feature representations. CBD allows the feature representation to evolve in the second training stage with the guidance of a teacher model learned in the first stage, using class-balanced sampling to focus on under-represented classes. This framework can use multiple teachers to improve recognition capabilities. Our experiments demonstrate that our technique outperforms the state of the art on long-tailed recognition benchmarks such as ImageNet-LT, iNaturalist17, and iNaturalist18, without sacrificing the accuracy of head classes to improve the performance of tail classes, as most existing methods do.",1
"Graph Neural Networks (GNNs) have proved to be an effective representation learning framework for graph-structured data, and have achieved state-of-the-art performance on many practical predictive tasks, such as node classification, link prediction and graph classification. Among the variants of GNNs, Graph Attention Networks (GATs) learn to assign dense attention coefficients over all neighbors of a node for feature aggregation, and improve the performance of many graph learning tasks. However, real-world graphs are often very large and noisy, and GATs are prone to overfitting if not regularized properly. Even worse, the local aggregation mechanism of GATs may fail on disassortative graphs, where nodes within local neighborhood provide more noise than useful information for feature aggregation. In this paper, we propose Sparse Graph Attention Networks (SGATs) that learn sparse attention coefficients under an $L_0$-norm regularization, and the learned sparse attentions are then used for all GNN layers, resulting in an edge-sparsified graph. By doing so, we can identify noisy/task-irrelevant edges, and thus perform feature aggregation on most informative neighbors. Extensive experiments on synthetic and real-world graph learning benchmarks demonstrate the superior performance of SGATs. In particular, SGATs can remove about 50\%-80\% edges from large assortative graphs, while retaining similar classification accuracies. On disassortative graphs, SGATs prune majority of noisy edges and outperform GATs in classification accuracies by significant margins. Furthermore, the removed edges can be interpreted intuitively and quantitatively. To the best of our knowledge, this is the first graph learning algorithm that shows significant redundancies in graphs and edge-sparsified graphs can achieve similar or sometimes higher predictive performances than original graphs.",0
"Graph Neural Networks (GNNs) have proven to be an effective way of learning representations for graph-structured data, achieving state-of-the-art performance on tasks such as node classification, link prediction, and graph classification. Within the family of GNNs, Graph Attention Networks (GATs) have shown particular promise by learning to assign dense attention coefficients across a node's neighbors for feature aggregation. However, real-world graphs are often large and noisy, making GATs prone to overfitting when not properly regularized. Additionally, GATs may not perform well on disassortative graphs where nodes within the local neighborhood provide more noise than useful information for feature aggregation. To address these issues, we propose Sparse Graph Attention Networks (SGATs), which learn sparse attention coefficients under an $L_0$-norm regularization. The learned sparse attentions are then used for all GNN layers, resulting in an edge-sparsified graph. This allows the identification of noisy or task-irrelevant edges, and feature aggregation can be performed on the most informative neighbors. Our experiments on synthetic and real-world graph learning benchmarks demonstrate the superior performance of SGATs. Notably, SGATs can remove 50\%-80\% of edges from large assortative graphs while maintaining similar classification accuracies. On disassortative graphs, SGATs prune the majority of noisy edges, outperforming GATs in classification accuracies by significant margins. Furthermore, we can interpret and quantify the removed edges. To our knowledge, this is the first graph learning algorithm to show significant redundancies in graphs, and edge-sparsified graphs can achieve similar or higher predictive performances than original graphs.",1
"With the remarkable success of representation learning in providing powerful predictions and data insights, we have witnessed a rapid expansion of representation learning techniques into modeling, analysis, and learning with networks. Biomedical networks are universal descriptors of systems of interacting elements, from protein interactions to disease networks, all the way to healthcare systems and scientific knowledge. In this review, we put forward an observation that long-standing principles of network biology and medicine -- while often unspoken in machine learning research -- can provide the conceptual grounding for representation learning, explain its current successes and limitations, and inform future advances. We synthesize a spectrum of algorithmic approaches that, at their core, leverage topological features to embed networks into compact vector spaces. We also provide a taxonomy of biomedical areas that are likely to benefit most from algorithmic innovation. Representation learning techniques are becoming essential for identifying causal variants underlying complex traits, disentangling behaviors of single cells and their impact on health, and diagnosing and treating diseases with safe and effective medicines.",0
"Representation learning has shown remarkable success in offering powerful predictions and data insights, leading to a rapid expansion of its techniques in modeling, analysis, and learning with networks. Biomedical networks are universal descriptors of systems with interacting elements, from protein interactions to disease networks, healthcare systems, and scientific knowledge. This review proposes an observation that long-standing principles of network biology and medicine can provide the conceptual foundation for representation learning, elucidate its current successes and limitations, and inform future advances, despite being often unspoken in machine learning research. A spectrum of algorithmic approaches is synthesized, leveraging topological features to embed networks into compact vector spaces. Additionally, a taxonomy of biomedical areas that are likely to benefit most from algorithmic innovation is provided. Representation learning techniques are becoming crucial for identifying causal variants underlying complex traits, disentangling behaviors of single cells and their impact on health, and diagnosing and treating diseases with safe and effective medicines.",1
"Drug-drug interaction(DDI) prediction is an important task in the medical health machine learning community. This study presents a new method, multi-view graph contrastive representation learning for drug-drug interaction prediction, MIRACLE for brevity, to capture inter-view molecule structure and intra-view interactions between molecules simultaneously. MIRACLE treats a DDI network as a multi-view graph where each node in the interaction graph itself is a drug molecular graph instance. We use GCNs and bond-aware attentive message passing networks to encode DDI relationships and drug molecular graphs in the MIRACLE learning stage, respectively. Also, we propose a novel unsupervised contrastive learning component to balance and integrate the multi-view information. Comprehensive experiments on multiple real datasets show that MIRACLE outperforms the state-of-the-art DDI prediction models consistently.",0
"The medical health machine learning community places significant importance on predicting drug-drug interactions (DDI). This research introduces a new approach, called MIRACLE, which utilizes multi-view graph contrastive representation learning for DDI prediction. MIRACLE captures both inter-view molecule structure and intra-view interactions between molecules simultaneously. In essence, MIRACLE treats a DDI network as a multi-view graph, with each node in the interaction graph being a drug molecular graph instance. MIRACLE employs GCNs and bond-aware attentive message passing networks to encode DDI relationships and drug molecular graphs, respectively, during the learning stage. Additionally, a novel unsupervised contrastive learning component is proposed to balance and integrate the multi-view information. Comprehensive experiments on multiple real datasets demonstrate that MIRACLE consistently outperforms state-of-the-art DDI prediction models.",1
"We propose a combination of a variational autoencoder and a transformer based model which fully utilises graph convolutional and graph pooling layers to operate directly on graphs. The transformer model implements a novel node encoding layer, replacing the position encoding typically used in transformers, to create a transformer with no position information that operates on graphs, encoding adjacent node properties into the edge generation process. The proposed model builds on graph generative work operating on graphs with edge features, creating a model that offers improved scalability with the number of nodes in a graph. In addition, our model is capable of learning a disentangled, interpretable latent space that represents graph properties through a mapping between latent variables and graph properties. In experiments we chose a benchmark task of molecular generation, given the importance of both generated node and edge features. Using the QM9 dataset we demonstrate that our model performs strongly across the task of generating valid, unique and novel molecules. Finally, we demonstrate that the model is interpretable by generating molecules controlled by molecular properties, and we then analyse and visualise the learned latent representation.",0
"Our proposal involves combining a variational autoencoder and a transformer model that utilizes graph convolutional and graph pooling layers to directly operate on graphs. The transformer model incorporates a unique node encoding layer in place of the typical position encoding found in transformers, allowing it to function on graphs by encoding adjacent node properties into the edge generation process. Our model expands on existing graph generative work by creating a more scalable model capable of handling larger graphs with improved efficiency. Additionally, our model can learn a disentangled latent space that represents graph properties through a mapping between latent variables and graph properties. To test our model, we used the QM9 dataset to generate valid, unique, and novel molecules, demonstrating the model's strong performance. Furthermore, we generated molecules controlled by molecular properties, showcasing the model's interpretability and visualizing the learned latent representation.",1
"We present GATSBI, a generative model that can transform a sequence of raw observations into a structured latent representation that fully captures the spatio-temporal context of the agent's actions. In vision-based decision-making scenarios, an agent faces complex high-dimensional observations where multiple entities interact with each other. The agent requires a good scene representation of the visual observation that discerns essential components and consistently propagates along the time horizon. Our method, GATSBI, utilizes unsupervised object-centric scene representation learning to separate an active agent, static background, and passive objects. GATSBI then models the interactions reflecting the causal relationships among decomposed entities and predicts physically plausible future states. Our model generalizes to a variety of environments where different types of robots and objects dynamically interact with each other. We show GATSBI achieves superior performance on scene decomposition and video prediction compared to its state-of-the-art counterparts.",0
"GATSBI is a generative model that transforms raw observations into a structured latent representation, capturing the spatio-temporal context of an agent's actions. In complex visual decision-making scenarios, agents encounter high-dimensional observations with multiple interacting entities. A scene representation that discerns essential components and propagates consistently along the time horizon is crucial. GATSBI uses unsupervised object-centric scene representation learning to separate active agents, static backgrounds, and passive objects. It models interactions reflecting causal relationships among decomposed entities and predicts physically plausible future states. GATSBI generalizes to various environments with different robot and object interactions, outperforming state-of-the-art models in scene decomposition and video prediction.",1
"Heterogeneous information networks(HINs) become popular in recent years for its strong capability of modelling objects with abundant information using explicit network structure. Network embedding has been proved as an effective method to convert information networks into lower-dimensional space, whereas the core information can be well preserved. However, traditional network embedding algorithms are sub-optimal in capturing rich while potentially incompatible semantics provided by HINs. To address this issue, a novel meta-path-based HIN representation learning framework named mSHINE is designed to simultaneously learn multiple node representations for different meta-paths. More specifically, one representation learning module inspired by the RNN structure is developed and multiple node representations can be learned simultaneously, where each representation is associated with one respective meta-path. By measuring the relevance between nodes with the designed objective function, the learned module can be applied in downstream link prediction tasks. A set of criteria for selecting initial meta-paths is proposed as the other module in mSHINE which is important to reduce the optimal meta-path selection cost when no prior knowledge of suitable meta-paths is available. To corroborate the effectiveness of mSHINE, extensive experimental studies including node classification and link prediction are conducted on five real-world datasets. The results demonstrate that mSHINE outperforms other state-of-the-art HIN embedding methods.",0
"In recent years, Heterogeneous Information Networks (HINs) have gained popularity due to their ability to model objects with a wealth of information using explicit network structures. While network embedding has proven to be an effective method for converting information networks into a lower-dimensional space while preserving core information, traditional network embedding algorithms are sub-optimal in capturing rich, potentially incompatible semantics provided by HINs. To address this issue, a new meta-path-based HIN representation learning framework called mSHINE has been designed. This framework simultaneously learns multiple node representations for different meta-paths, with each representation associated with a respective meta-path. The learned module can be applied in downstream link prediction tasks by measuring the relevance between nodes with the designed objective function. Additionally, a set of criteria for selecting initial meta-paths has been proposed to reduce the optimal meta-path selection cost when no prior knowledge of suitable meta-paths is available. Extensive experimental studies, including node classification and link prediction on five real-world datasets, were conducted to validate the effectiveness of mSHINE, which outperforms other state-of-the-art HIN embedding methods.",1
"End-to-end deep representation learning has achieved remarkable accuracy for monocular 3D human pose estimation, yet these models may fail for unseen poses with limited and fixed training data. This paper proposes a novel data augmentation method that: (1) is scalable for synthesizing massive amount of training data (over 8 million valid 3D human poses with corresponding 2D projections) for training 2D-to-3D networks, (2) can effectively reduce dataset bias. Our method evolves a limited dataset to synthesize unseen 3D human skeletons based on a hierarchical human representation and heuristics inspired by prior knowledge. Extensive experiments show that our approach not only achieves state-of-the-art accuracy on the largest public benchmark, but also generalizes significantly better to unseen and rare poses. Code, pre-trained models and tools are available at this HTTPS URL.",0
"Although end-to-end deep representation learning has produced impressive results for monocular 3D human pose estimation, these models may not perform well when faced with unseen poses due to limited and fixed training data. To address this issue, this study proposes a novel data augmentation method that can generate a large amount of training data, over 8 million valid 3D human poses with corresponding 2D projections, for 2D-to-3D networks. The method is scalable and can effectively reduce dataset bias. By utilizing a hierarchical human representation and heuristics inspired by prior knowledge, our approach can synthesize unseen 3D human skeletons from a limited dataset. Our extensive experiments demonstrate that our approach not only achieves state-of-the-art accuracy on the largest public benchmark but also generalizes significantly better to unseen and rare poses. Code, pre-trained models, and tools are available at this HTTPS URL.",1
"Contrastive learning has nearly closed the gap between supervised and self-supervised learning of image representations, and has also been explored for videos. However, prior work on contrastive learning for video data has not explored the effect of explicitly encouraging the features to be distinct across the temporal dimension. We develop a new temporal contrastive learning framework consisting of two novel losses to improve upon existing contrastive self-supervised video representation learning methods. The local-local temporal contrastive loss adds the task of discriminating between non-overlapping clips from the same video, whereas the global-local temporal contrastive aims to discriminate between timesteps of the feature map of an input clip in order to increase the temporal diversity of the learned features. Our proposed temporal contrastive learning framework achieves significant improvement over the state-of-the-art results in various downstream video understanding tasks such as action recognition, limited-label action classification, and nearest-neighbor video retrieval on multiple video datasets and backbones. We also demonstrate significant improvement in fine-grained action classification for visually similar classes. With the commonly used 3D ResNet-18 architecture, we achieve 82.4% (+5.1% increase over the previous best) top-1 accuracy on UCF101 and 52.9% (+5.4% increase) on HMDB51 action classification, and 56.2% (+11.7% increase) Top-1 Recall on UCF101 nearest neighbor video retrieval.",0
"The gap between supervised and self-supervised learning of image representations has almost closed with the advent of contrastive learning, which has also been applied to videos. However, previous research on contrastive learning for video data did not investigate the impact of explicitly promoting distinctiveness across the temporal dimension. To address this gap, we propose a novel temporal contrastive learning approach that comprises two new losses to enhance existing contrastive self-supervised video representation learning methods. The local-local temporal contrastive loss distinguishes between non-overlapping clips from the same video, while the global-local temporal contrastive loss discriminates between timesteps of the feature map of an input clip to increase temporal diversity. Our proposed temporal contrastive learning framework yields significant improvements in various downstream video understanding tasks, including action recognition, limited-label action classification, and nearest-neighbor video retrieval on multiple video datasets and backbones. Furthermore, we demonstrate considerable advancements in fine-grained action classification for visually similar classes. Using the 3D ResNet-18 architecture, we achieve 82.4% (+5.1% increase over the previous best) top-1 accuracy for UCF101, 52.9% (+5.4% increase) for HMDB51 action classification, and 56.2% (+11.7% increase) Top-1 Recall for UCF101 nearest neighbor video retrieval.",1
"We develop and rigorously evaluate a deep learning based system that can accurately classify skin conditions while detecting rare conditions for which there is not enough data available for training a confident classifier. We frame this task as an out-of-distribution (OOD) detection problem. Our novel approach, hierarchical outlier detection (HOD) assigns multiple abstention classes for each training outlier class and jointly performs a coarse classification of inliers vs. outliers, along with fine-grained classification of the individual classes. We demonstrate the effectiveness of the HOD loss in conjunction with modern representation learning approaches (BiT, SimCLR, MICLe) and explore different ensembling strategies for further improving the results. We perform an extensive subgroup analysis over conditions of varying risk levels and different skin types to investigate how the OOD detection performance changes over each subgroup and demonstrate the gains of our framework in comparison to baselines. Finally, we introduce a cost metric to approximate downstream clinical impact. We use this cost metric to compare the proposed method against a baseline system, thereby making a stronger case for the overall system effectiveness in a real-world deployment scenario.",0
"Our team has created and thoroughly tested a deep learning system capable of accurately identifying various skin conditions, including rare ones with limited data for training. To achieve this, we approached the task as an out-of-distribution (OOD) detection problem using a new method called hierarchical outlier detection (HOD). Our approach involves assigning multiple abstention classes for each outlier class, allowing for a coarse classification of inliers vs. outliers, as well as fine-grained classification of individual classes. We demonstrate the effectiveness of using the HOD loss in combination with modern representation learning approaches such as BiT, SimCLR, and MICLe, and explore various ensembling strategies to enhance the results. We conducted a thorough analysis of the system's performance in detecting OOD across different subgroups of skin types and risk levels and introduced a cost metric to evaluate the potential clinical impact. Through this metric, we were able to compare our system's performance against a baseline and demonstrate its effectiveness in a real-world deployment scenario.",1
"Graph Neural Networks (GNNs) have achieved tremendous success in graph representation learning. Unfortunately, current GNNs usually rely on loading the entire attributed graph into network for processing. This implicit assumption may not be satisfied with limited memory resources, especially when the attributed graph is large. In this paper, we pioneer to propose a Binary Graph Convolutional Network (Bi-GCN), which binarizes both the network parameters and input node features. Besides, the original matrix multiplications are revised to binary operations for accelerations. According to the theoretical analysis, our Bi-GCN can reduce the memory consumption by an average of ~30x for both the network parameters and input data, and accelerate the inference speed by an average of ~47x, on the citation networks. Meanwhile, we also design a new gradient approximation based back-propagation method to train our Bi-GCN well. Extensive experiments have demonstrated that our Bi-GCN can give a comparable performance compared to the full-precision baselines. Besides, our binarization approach can be easily applied to other GNNs, which has been verified in the experiments.",0
"Graph Neural Networks (GNNs) have been highly successful in learning graph representations. However, current GNNs require the entire attributed graph to be loaded into the network for processing, which may not be possible with limited memory resources, especially for large graphs. In this study, we propose the Binary Graph Convolutional Network (Bi-GCN), which binarizes both the network parameters and input node features, and uses binary operations instead of matrix multiplications for faster processing. Our theoretical analysis shows that Bi-GCN can reduce memory consumption by ~30x and accelerate inference speed by ~47x on citation networks. We also introduce a new gradient approximation based back-propagation method to train Bi-GCN effectively. Our experiments demonstrate that Bi-GCN performs comparably to full-precision baselines and can be easily applied to other GNNs.",1
"Learning temporal patterns from multivariate longitudinal data is challenging especially in cases when data is sporadic, as often seen in, e.g., healthcare applications where the data can suffer from irregularity and asynchronicity as the time between consecutive data points can vary across features and samples, hindering the application of existing deep learning models that are constructed for complete, evenly spaced data with fixed sequence lengths. In this paper, a novel deep learning-based model is developed for modeling multiple temporal features in sporadic data using an integrated deep learning architecture based on a recurrent neural network (RNN) unit and a continuous-time autoregressive (CAR) model. The proposed model, called CARRNN, uses a generalized discrete-time autoregressive model that is trainable end-to-end using neural networks modulated by time lags to describe the changes caused by the irregularity and asynchronicity. It is applied to multivariate time-series regression tasks using data provided for Alzheimer's disease progression modeling and intensive care unit (ICU) mortality rate prediction, where the proposed model based on a gated recurrent unit (GRU) achieves the lowest prediction errors among the proposed RNN-based models and state-of-the-art methods using GRUs and long short-term memory (LSTM) networks in their architecture.",0
"When dealing with multivariate longitudinal data, learning temporal patterns can be difficult, especially if the data is sporadic. This is often the case in healthcare applications, where irregularity and asynchronicity can affect the data due to varying time intervals between consecutive data points across features and samples. This can make it challenging to apply existing deep learning models that are designed for complete, evenly spaced data with fixed sequence lengths. In this study, a new deep learning-based model called CARRNN was developed to model multiple temporal features in sporadic data using an integrated architecture based on a recurrent neural network unit and a continuous-time autoregressive model. The proposed model employs a generalized discrete-time autoregressive model that is trainable end-to-end using neural networks modulated by time lags to describe the changes caused by irregularity and asynchronicity. CARRNN was tested on multivariate time-series regression tasks using data from Alzheimer's disease progression modeling and ICU mortality rate prediction. The model based on a gated recurrent unit achieved the lowest prediction errors among the proposed RNN-based models and state-of-the-art methods using GRUs and LSTM networks.",1
"Sensor and control data of modern mechatronic systems are often available as heterogeneous time series with different sampling rates and value ranges. Suitable classification and regression methods from the field of supervised machine learning already exist for predictive tasks, for example in the context of condition monitoring, but their performance scales strongly with the number of labeled training data. Their provision is often associated with high effort in the form of person-hours or additional sensors. In this paper, we present a method for unsupervised feature extraction using autoencoder networks that specifically addresses the heterogeneous nature of the database and reduces the amount of labeled training data required compared to existing methods. Three public datasets of mechatronic systems from different application domains are used to validate the results.",0
"Mechatronic systems often produce sensor and control data that are in the form of time series with varying sampling rates and value ranges. While supervised machine learning methods are available for predictive tasks, such as condition monitoring, their performance heavily depends on the amount of labeled training data available, which can require significant effort. This paper proposes an unsupervised feature extraction method that utilizes autoencoder networks to address the heterogeneity of the data and reduce the need for labeled training data. The proposed method is validated using three public datasets from different mechatronic system applications.",1
"We study joint learning of Convolutional Neural Network (CNN) and Transformer for vision-language pre-training (VLPT) which aims to learn cross-modal alignments from millions of image-text pairs. State-of-the-art approaches extract salient image regions and align regions with words step-by-step. As region-based visual features usually represent parts of an image, it is challenging for existing vision-language models to fully understand the semantics from paired natural languages. In this paper, we propose SOHO to ""See Out of tHe bOx"" that takes a whole image as input, and learns vision-language representation in an end-to-end manner. SOHO does not require bounding box annotations which enables inference 10 times faster than region-based approaches. In particular, SOHO learns to extract comprehensive yet compact image features through a visual dictionary (VD) that facilitates cross-modal understanding. VD is designed to represent consistent visual abstractions of similar semantics. It is updated on-the-fly and utilized in our proposed pre-training task Masked Visual Modeling (MVM). We conduct experiments on four well-established vision-language tasks by following standard VLPT settings. In particular, SOHO achieves absolute gains of 2.0% R@1 score on MSCOCO text retrieval 5k test split, 1.5% accuracy on NLVR$^2$ test-P split, 6.7% accuracy on SNLI-VE test split, respectively.",0
"The objective of our research is to study the joint learning of Convolutional Neural Network (CNN) and Transformer for vision-language pre-training (VLPT). This involves learning cross-modal alignments from millions of image-text pairs. Existing approaches extract salient image regions and align them with words sequentially. However, this approach presents a challenge in understanding the semantics of paired natural languages as region-based visual features typically represent only parts of an image. Our proposed solution, SOHO (See Out of tHe bOx), takes a whole image as input and learns vision-language representation in an end-to-end manner. Unlike region-based methods, SOHO does not require bounding box annotations and is 10 times faster at inference. We use a visual dictionary (VD) to extract comprehensive yet compact image features that facilitate cross-modal understanding. The VD is updated on-the-fly and used in Masked Visual Modeling (MVM), our proposed pre-training task. We evaluated SOHO on four well-established vision-language tasks, and it achieved absolute gains of 2.0% R@1 score on MSCOCO text retrieval 5k test split, 1.5% accuracy on NLVR$^2$ test-P split, and 6.7% accuracy on SNLI-VE test split.",1
"Real-world data is often unbalanced and long-tailed, but deep models struggle to recognize rare classes in the presence of frequent classes. To address unbalanced data, most studies try balancing the data, the loss, or the classifier to reduce classification bias towards head classes. Far less attention has been given to the latent representations learned with unbalanced data. We show that the feature extractor part of deep networks suffers greatly from this bias. We propose a new loss based on robustness theory, which encourages the model to learn high-quality representations for both head and tail classes. While the general form of the robustness loss may be hard to compute, we further derive an easy-to-compute upper bound that can be minimized efficiently. This procedure reduces representation bias towards head classes in the feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT, and iNaturalist long-tail benchmarks. We find that training with robustness increases recognition accuracy of tail classes while largely maintaining the accuracy of head classes. The new robustness loss can be combined with various classifier balancing techniques and can be applied to representations at several layers of the deep model.",0
"Deep models struggle to identify rare classes in the presence of frequent classes due to the unbalanced and long-tailed nature of real-world data. To tackle this issue, many studies focus on balancing the data, loss, or classifier to decrease classification bias towards head classes. However, the feature extractor part of deep networks is also affected by this bias, and its latent representations are often overlooked. Our research demonstrates that this bias significantly impacts the feature extractor, and we propose a novel robustness loss based on robustness theory to encourage high-quality representations for both head and tail classes. Although the general form of the robustness loss is complex, we provide an easy-to-compute upper bound that can be efficiently minimized. This approach reduces representation bias towards head classes in the feature space, and we achieve state-of-the-art results on CIFAR100-LT, ImageNet-LT, and iNaturalist long-tail benchmarks. Our findings show that training with robustness increases recognition accuracy of tail classes while maintaining the accuracy of head classes. Furthermore, the new robustness loss can be combined with various classifier balancing techniques and can be applied to representations at different layers of the deep model.",1
"Self-supervised learning based on instance discrimination has shown remarkable progress. In particular, contrastive learning, which regards each image as well as its augmentations as an individual class and tries to distinguish them from all other images, has been verified effective for representation learning. However, pushing away two images that are de facto similar is suboptimal for general representation. In this paper, we propose a hierarchical semantic alignment strategy via expanding the views generated by a single image to \textbf{Cross-samples and Multi-level} representation, and models the invariance to semantically similar images in a hierarchical way. This is achieved by extending the contrastive loss to allow for multiple positives per anchor, and explicitly pulling semantically similar images/patches together at different layers of the network. Our method, termed as CsMl, has the ability to integrate multi-level visual representations across samples in a robust way. CsMl is applicable to current contrastive learning based methods and consistently improves the performance. Notably, using the moco as an instantiation, CsMl achieves a \textbf{76.6\% }top-1 accuracy with linear evaluation using ResNet-50 as backbone, and \textbf{66.7\%} and \textbf{75.1\%} top-1 accuracy with only 1\% and 10\% labels, respectively. \textbf{All these numbers set the new state-of-the-art.}",0
"Significant advancements have been made in self-supervised learning through instance discrimination. Notably, contrastive learning has proven to be an effective means of representation learning. However, treating each image and its augmentations as separate classes for distinguishing them from all other images may not be optimal for general representation, as it may push away similar images. Our proposed method, CsMl, introduces a hierarchical semantic alignment strategy that expands views generated by a single image to Cross-samples and Multi-level representations, and models invariance to semantically similar images hierarchically. This is achieved by extending the contrastive loss to allow for multiple positives per anchor and explicitly pulling semantically similar images/patches together at different layers of the network. CsMl integrates multi-level visual representations across samples in a robust way and can be applied to current contrastive learning based methods, consistently improving performance. Our moco instantiation of CsMl achieves state-of-the-art results with a top-1 accuracy of 76.6% using ResNet-50 as the backbone, and top-1 accuracies of 66.7% and 75.1% with only 1% and 10% labels, respectively.",1
"Contrastive learning has delivered impressive results in many audio-visual representation learning scenarios. However, existing approaches optimize for learning either \textit{global} representations useful for tasks such as classification, or \textit{local} representations useful for tasks such as audio-visual source localization and separation. While they produce satisfactory results in their intended downstream scenarios, they often fail to generalize to tasks that they were not originally designed for. In this work, we propose a versatile self-supervised approach to learn audio-visual representations that generalize to both the tasks which require global semantic information (e.g., classification) and the tasks that require fine-grained spatio-temporal information (e.g. localization). We achieve this by optimizing two cross-modal contrastive objectives that together encourage our model to learn discriminative global-local visual information given audio signals. To show that our approach learns generalizable video representations, we evaluate it on various downstream scenarios including action/sound classification, lip reading, deepfake detection, and sound source localization.",0
"In many instances of audio-visual representation learning, Contrastive learning has produced impressive outcomes. However, current methods aim to learn either \textit{global} representations for tasks like classification or \textit{local} representations for tasks like audio-visual source localization and separation. Though satisfactory for their intended downstream scenarios, they often fail to apply to other tasks. This research proposes a self-supervised approach that is adaptable for both tasks requiring global semantic information (e.g., classification) and tasks necessitating fine-grained spatio-temporal information (e.g. localization). This is accomplished by optimizing two cross-modal contrastive objectives that promote the learning of discriminative global-local visual information given audio signals. To demonstrate the generalizability of our approach, we evaluate it on a range of downstream scenarios, including action/sound classification, lip reading, deepfake detection, and sound source localization.",1
"The Information Bottleneck (IB) provides an information theoretic principle for representation learning, by retaining all information relevant for predicting label while minimizing the redundancy. Though IB principle has been applied to a wide range of applications, its optimization remains a challenging problem which heavily relies on the accurate estimation of mutual information. In this paper, we present a new strategy, Variational Self-Distillation (VSD), which provides a scalable, flexible and analytic solution to essentially fitting the mutual information but without explicitly estimating it. Under rigorously theoretical guarantee, VSD enables the IB to grasp the intrinsic correlation between representation and label for supervised training. Furthermore, by extending VSD to multi-view learning, we introduce two other strategies, Variational Cross-Distillation (VCD) and Variational Mutual-Learning (VML), which significantly improve the robustness of representation to view-changes by eliminating view-specific and task-irrelevant information. To verify our theoretically grounded strategies, we apply our approaches to cross-modal person Re-ID, and conduct extensive experiments, where the superior performance against state-of-the-art methods are demonstrated. Our intriguing findings highlight the need to rethink the way to estimate mutual",0
"The Information Bottleneck principle is used for representation learning by minimizing redundancy while retaining relevant information for predicting labels. Despite its broad application, the optimization of IB remains challenging due to the need for accurate mutual information estimation. This paper introduces a new strategy called Variational Self-Distillation (VSD) that provides an analytic and flexible solution for fitting mutual information without explicit estimation. VSD has theoretical guarantee and helps IB to capture the correlation between representation and label for supervised training. The paper also extends VSD to multi-view learning, introducing two other strategies, Variational Cross-Distillation and Variational Mutual-Learning, which improve the robustness of representation to view-changes. The study applies these approaches to cross-modal person Re-ID and demonstrates superior performance against state-of-the-art methods. The findings shed light on the need to reconsider mutual estimation.",1
"We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that both provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.",0
"Our research focuses on how reinforcement learning can be expedited through representation learning from extensive observations, such as images. Our approach does not rely on domain knowledge or pixel-reconstruction. Our aim is to develop representations that facilitate effective downstream control and are unaffected by irrelevant task details. We propose using bisimulation metrics to measure behavioral similarity between states in continuous MDPs. This metric helps us learn robust latent representations that encode only relevant task information from observations. Our technique trains encoders to ensure that distances in latent space are equal to bisimulation distances in the state space. Our method is successful in disregarding irrelevant information in modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving State-of-the-Art performance. We also test our method in a first-person highway driving task, where our technique demonstrates invariance to clouds, weather, and time of day. Additionally, we offer generalization results based on properties of bisimulation metrics and their links to causal inference.",1
"We introduce a non-parametric approach for infinite video texture synthesis using a representation learned via contrastive learning. We take inspiration from Video Textures, which showed that plausible new videos could be generated from a single one by stitching its frames together in a novel yet consistent order. This classic work, however, was constrained by its use of hand-designed distance metrics, limiting its use to simple, repetitive videos. We draw on recent techniques from self-supervised learning to learn this distance metric, allowing us to compare frames in a manner that scales to more challenging dynamics, and to condition on other data, such as audio. We learn representations for video frames and frame-to-frame transition probabilities by fitting a video-specific model trained using contrastive learning. To synthesize a texture, we randomly sample frames with high transition probabilities to generate diverse temporally smooth videos with novel sequences and transitions. The model naturally extends to an audio-conditioned setting without requiring any finetuning. Our model outperforms baselines on human perceptual scores, can handle a diverse range of input videos, and can combine semantic and audio-visual cues in order to synthesize videos that synchronize well with an audio signal.",0
"Our approach to infinite video texture synthesis uses a non-parametric model, which is based on a representation learned through contrastive learning. We were inspired by Video Textures, which demonstrated the generation of new videos from a single one by reordering its frames in a novel yet consistent way. However, their use of hand-designed distance metrics limited the complexity of the resulting videos. To overcome this, we utilized recent self-supervised learning techniques to learn the distance metric and enable comparison of frames with more complex dynamics, as well as conditioning on other data, such as audio. We learned video frame representations and transition probabilities using a video-specific model trained through contrastive learning. To synthesize a texture, we randomly sampled frames with high transition probabilities, generating diverse and smooth videos with novel sequences and transitions. The model can also be extended to an audio-conditioned setting without any additional fine-tuning. Our model outperforms baselines on human perceptual scores and can handle a wide range of input videos while combining semantic and audio-visual cues to produce videos that are well synchronized with the audio signal.",1
"Contrastive learning has recently shown immense potential in unsupervised visual representation learning. Existing studies in this track mainly focus on intra-image invariance learning. The learning typically uses rich intra-image transformations to construct positive pairs and then maximizes agreement using a contrastive loss. The merits of inter-image invariance, conversely, remain much less explored. One major obstacle to exploit inter-image invariance is that it is unclear how to reliably construct inter-image positive pairs, and further derive effective supervision from them since there are no pair annotations available. In this work, we present a rigorous and comprehensive study on inter-image invariance learning from three main constituting components: pseudo-label maintenance, sampling strategy, and decision boundary design. Through carefully-designed comparisons and analysis, we propose a unified and generic framework that supports the integration of unsupervised intra- and inter-image invariance learning. With all the obtained recipes, our final model, namely InterCLR, shows consistent improvements over state-of-the-art intra-image invariance learning methods on multiple standard benchmarks. Codes will be released at https://github.com/open-mmlab/OpenSelfSup.",0
"Unsupervised visual representation learning has recently witnessed significant potential in contrastive learning. While previous studies have primarily focused on learning intra-image invariance, contrastive loss is utilized to maximize agreement by employing rich intra-image transformations to construct positive pairs. However, inter-image invariance has not been explored enough due to the difficulty of reliably constructing inter-image positive pairs and deriving effective supervision since there are no pair annotations available. This study presents a comprehensive examination of inter-image invariance learning by considering three main components - pseudo-label maintenance, sampling strategy, and decision boundary design. By comparing and analyzing these components, a unified and generic framework called InterCLR is proposed to integrate unsupervised intra- and inter-image invariance learning. The final model shows consistent improvements over state-of-the-art intra-image invariance learning methods on multiple standard benchmarks, and the codes are available at https://github.com/open-mmlab/OpenSelfSup.",1
"Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streaming data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space during the learning process. Additionally, continual learning does not assume the data stream to be stationary, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. As an additional contribution, we generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework. We obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.",0
"Representation learning has established the importance of prototypical features to represent class distributions. However, the challenge arises when learning prototypes online from streaming data, as the constantly changing parameter space during the learning process quickly renders them outdated. This issue is compounded by the fact that continual learning does not account for non-stationary data streams, often leading to catastrophic forgetting of previous knowledge. In this paper, we present a novel system that addresses both problems by enabling prototypes to evolve continually in a shared latent space, allowing for learning and prediction at any given time. Unlike previous work in continual learning, our system processes data streams in an online fashion without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Our approach includes a unique objective function that promotes cluster density about the class prototype and increased inter-class variance, enhancing learning. We also employ pseudo-prototypes in each batch, which are constituted by replay of exemplars from memory, to elevate the quality of the latent space. Moreover, we contribute to the existing paradigms in continual learning by introducing a two-agent learner-evaluator framework that incorporates data incremental learning from data streams. Our approach outperforms existing methods by a significant margin on eight benchmarks, including three highly imbalanced data streams.",1
"Crowd counting is a fundamental yet challenging task, which desires rich information to generate pixel-wise crowd density maps. However, most previous methods only used the limited information of RGB images and cannot well discover potential pedestrians in unconstrained scenarios. In this work, we find that incorporating optical and thermal information can greatly help to recognize pedestrians. To promote future researches in this field, we introduce a large-scale RGBT Crowd Counting (RGBT-CC) benchmark, which contains 2,030 pairs of RGB-thermal images with 138,389 annotated people. Furthermore, to facilitate the multimodal crowd counting, we propose a cross-modal collaborative representation learning framework, which consists of multiple modality-specific branches, a modality-shared branch, and an Information Aggregation-Distribution Module (IADM) to capture the complementary information of different modalities fully. Specifically, our IADM incorporates two collaborative information transfers to dynamically enhance the modality-shared and modality-specific representations with a dual information propagation mechanism. Extensive experiments conducted on the RGBT-CC benchmark demonstrate the effectiveness of our framework for RGBT crowd counting. Moreover, the proposed approach is universal for multimodal crowd counting and is also capable to achieve superior performance on the ShanghaiTechRGBD dataset. Finally, our source code and benchmark are released at {\url{http://lingboliu.com/RGBT_Crowd_Counting.html}}.",0
"Counting crowds is a challenging task that requires detailed information to create accurate crowd density maps. However, previous methods have only utilized limited RGB image data and have struggled to detect pedestrians in complex scenarios. In this study, we discovered that combining optical and thermal data can greatly improve pedestrian recognition. To encourage further research, we created the RGBT Crowd Counting (RGBT-CC) benchmark, which includes 2,030 pairs of RGB-thermal images with 138,389 annotated individuals. Additionally, we developed a cross-modal collaborative representation learning framework that includes multiple modality-specific branches, a modality-shared branch, and an Information Aggregation-Distribution Module (IADM). Our IADM features two collaborative information transfers that dynamically enhance both modality-specific and modality-shared representations. Our experiments on the RGBT-CC benchmark demonstrate the effectiveness of our approach for multimodal crowd counting, and our method also shows superior performance on the ShanghaiTechRGBD dataset. We have made our benchmark and source code available at {\url{http://lingboliu.com/RGBT_Crowd_Counting.html}}.",1
"We present a self-supervised Contrastive Video Representation Learning (CVRL) method to learn spatiotemporal visual representations from unlabeled videos. Our representations are learned using a contrastive loss, where two augmented clips from the same short video are pulled together in the embedding space, while clips from different videos are pushed away. We study what makes for good data augmentations for video self-supervised learning and find that both spatial and temporal information are crucial. We carefully design data augmentations involving spatial and temporal cues. Concretely, we propose a temporally consistent spatial augmentation method to impose strong spatial augmentations on each frame of the video while maintaining the temporal consistency across frames. We also propose a sampling-based temporal augmentation method to avoid overly enforcing invariance on clips that are distant in time. On Kinetics-600, a linear classifier trained on the representations learned by CVRL achieves 70.4% top-1 accuracy with a 3D-ResNet-50 (R3D-50) backbone, outperforming ImageNet supervised pre-training by 15.7% and SimCLR unsupervised pre-training by 18.8% using the same inflated R3D-50. The performance of CVRL can be further improved to 72.9% with a larger R3D-152 (2x filters) backbone, significantly closing the gap between unsupervised and supervised video representation learning. Our code and models will be available at https://github.com/tensorflow/models/tree/master/official/.",0
"To obtain spatiotemporal visual representations from unlabeled videos, we introduce a method called Contrastive Video Representation Learning (CVRL) that is self-supervised. Our approach uses a contrastive loss to train the representations, where two augmented clips from the same short video are brought together in the embedding space while clips from different videos are separated. We examine the qualities that make for effective data augmentations for video self-supervised learning and discover that spatial and temporal information are both essential. We create data augmentations that involve spatial and temporal cues by designing a temporally consistent spatial augmentation method and a sampling-based temporal augmentation method. On Kinetics-600, we demonstrate that a linear classifier trained on the CVRL representations attains 70.4% top-1 accuracy with a 3D-ResNet-50 (R3D-50) backbone, surpassing ImageNet supervised pre-training by 15.7% and SimCLR unsupervised pre-training by 18.8% using the same inflated R3D-50. The performance of CVRL can be enhanced to 72.9% with a larger R3D-152 (2x filters) backbone, which significantly narrows the gap between unsupervised and supervised video representation learning. Our code and models will be accessible at https://github.com/tensorflow/models/tree/master/official/.",1
"Face representation learning solutions have recently achieved great success for various applications such as verification and identification. However, face recognition approaches that are based purely on RGB images rely solely on intensity information, and therefore are more sensitive to facial variations, notably pose, occlusions, and environmental changes such as illumination and background. A novel depth-guided attention mechanism is proposed for deep multi-modal face recognition using low-cost RGB-D sensors. Our novel attention mechanism directs the deep network ""where to look"" for visual features in the RGB image by focusing the attention of the network using depth features extracted by a Convolution Neural Network (CNN). The depth features help the network focus on regions of the face in the RGB image that contains more prominent person-specific information. Our attention mechanism then uses this correlation to generate an attention map for RGB images from the depth features extracted by CNN. We test our network on four public datasets, showing that the features obtained by our proposed solution yield better results on the Lock3DFace, CurtinFaces, IIIT-D RGB-D, and KaspAROV datasets which include challenging variations in pose, occlusion, illumination, expression, and time-lapse. Our solution achieves average (increased) accuracies of 87.3\% (+5.0\%), 99.1\% (+0.9\%), 99.7\% (+0.6\%) and 95.3\%(+0.5\%) for the four datasets respectively, thereby improving the state-of-the-art. We also perform additional experiments with thermal images, instead of depth images, showing the high generalization ability of our solution when adopting other modalities for guiding the attention mechanism instead of depth information",0
"Recently, face representation learning solutions have been highly successful in various applications such as verification and identification. However, face recognition approaches that solely rely on intensity information from RGB images are more sensitive to facial variations, including pose, occlusions, illumination, and background changes. To address this issue, we propose a novel depth-guided attention mechanism for deep multi-modal face recognition that employs low-cost RGB-D sensors. This attention mechanism directs the deep network to focus on regions of the face that contain more prominent person-specific information by using depth features extracted by a CNN. Our attention mechanism generates an attention map for the RGB images based on the correlation between depth features and regions of the face with more salient features. We evaluate our proposed solution on four public datasets, including Lock3DFace, CurtinFaces, IIIT-D RGB-D, and KaspAROV, which contain challenging variations in pose, occlusion, illumination, expression, and time-lapse. Our solution achieves average accuracies of 87.3%, 99.1%, 99.7%, and 95.3% for the four datasets, respectively, improving upon the state-of-the-art. Additionally, we demonstrate the high generalization ability of our solution by conducting experiments with thermal images instead of depth images, highlighting the adaptability of our attention mechanism to other modalities.",1
"Generalizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs). Unlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are less explored for GNNs. In this paper, we propose a graph contrastive learning (GraphCL) framework for learning unsupervised representations of graph data. We first design four types of graph augmentations to incorporate various priors. We then systematically study the impact of various combinations of graph augmentations on multiple datasets, in four different settings: semi-supervised, unsupervised, and transfer learning as well as adversarial attacks. The results show that, even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations of similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also investigate the impact of parameterized graph augmentation extents and patterns, and observe further performance gains in preliminary experiments. Our codes are available at https://github.com/Shen-Lab/GraphCL.",0
"Current graph neural networks (GNNs) face challenges in achieving generalizable, transferrable, and robust representation learning on graph-structured data, unlike convolutional neural networks (CNNs) for image data which have developed self-supervised learning and pre-training. In this study, we introduce the graph contrastive learning (GraphCL) framework to learn unsupervised representations of graph data. The framework utilizes four types of graph augmentations to incorporate various priors and we systematically analyze their impact on multiple datasets in four different settings. The results demonstrate that our GraphCL framework can produce graph representations with similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods without extensive augmentation extents or complex GNN architectures. We also explore the effects of parameterized graph augmentation extents and patterns, which show further performance improvements in our preliminary experiments. Our codes can be accessed at https://github.com/Shen-Lab/GraphCL.",1
"A rich set of interpretable dimensions has been shown to emerge in the latent space of the Generative Adversarial Networks (GANs) trained for synthesizing images. In order to identify such latent dimensions for image editing, previous methods typically annotate a collection of synthesized samples and train linear classifiers in the latent space. However, they require a clear definition of the target attribute as well as the corresponding manual annotations, limiting their applications in practice. In this work, we examine the internal representation learned by GANs to reveal the underlying variation factors in an unsupervised manner. In particular, we take a closer look into the generation mechanism of GANs and further propose a closed-form factorization algorithm for latent semantic discovery by directly decomposing the pre-trained weights. With a lightning-fast implementation, our approach is capable of not only finding semantically meaningful dimensions comparably to the state-of-the-art supervised methods, but also resulting in far more versatile concepts across multiple GAN models trained on a wide range of datasets.",0
"The latent space of Generative Adversarial Networks (GANs) has been found to contain a diverse range of interpretable dimensions that can be used for image synthesis. However, previous methods for identifying these dimensions for image editing have relied on manual annotations and a clear definition of the target attribute, limiting their practical applications. In this study, we investigate the internal representation of GANs and propose an unsupervised algorithm for discovering latent semantics by decomposing the pre-trained weights. Our approach is not only fast, but also capable of identifying semantically meaningful dimensions that are more versatile than those found by state-of-the-art supervised methods, across multiple GAN models trained on various datasets.",1
"The complexity and non-Euclidean structure of graph data hinder the development of data augmentation methods similar to those in computer vision. In this paper, we propose a feature augmentation method for graph nodes based on topological regularization, in which topological structure information is introduced into end-to-end model. Specifically, we first obtain topology embedding of nodes through unsupervised representation learning method based on random walk. Then, the topological embedding as additional features and the original node features are input into a dual graph neural network for propagation, and two different high-order neighborhood representations of nodes are obtained. On this basis, we propose a regularization technique to bridge the differences between the two different node representations, eliminate the adverse effects caused by the topological features of graphs directly used, and greatly improve the performance. We have carried out extensive experiments on a large number of datasets to prove the effectiveness of our model.",0
"The intricate and non-Euclidean configuration of graph data presents a challenge for creating data augmentation techniques similar to those used in computer vision. This paper suggests a topological regularization method for augmenting features of graph nodes, which brings in topological structure information into the end-to-end model. Initially, we use unsupervised representation learning, based on random walk, to acquire topology embedding of nodes. We then input the topological embedding and the original node features into a dual graph neural network for propagation, which produces two different high-order neighborhood representations of nodes. To bridge the differences between these two representations, we propose a regularization method. This eliminates the negative impact of directly using topological features of graphs and significantly enhances performance. We have conducted comprehensive experiments on multiple datasets to verify the effectiveness of our model.",1
"Given a collection of images, humans are able to discover landmarks by modeling the shared geometric structure across instances. This idea of geometric equivariance has been widely used for the unsupervised discovery of object landmark representations. In this paper, we develop a simple and effective approach by combining instance-discriminative and spatially-discriminative contrastive learning. We show that when a deep network is trained to be invariant to geometric and photometric transformations, representations emerge from its intermediate layers that are highly predictive of object landmarks. Stacking these across layers in a ""hypercolumn"" and projecting them using spatially-contrastive learning further improves their performance on matching and few-shot landmark regression tasks. We also present a unified view of existing equivariant and invariant representation learning approaches through the lens of contrastive learning, shedding light on the nature of invariances learned. Experiments on standard benchmarks for landmark learning, as well as a new challenging one we propose, show that the proposed approach surpasses prior state-of-the-art.",0
"Humans can identify landmarks in a group of images by recognizing the shared geometric structure across instances, which is known as geometric equivariance. This concept has been extensively used in discovering object landmark representations without supervision. In this study, we introduce a straightforward and efficient technique by combining instance-discriminative and spatially-discriminative contrastive learning. Our findings demonstrate that deep networks trained to be resistant to geometric and photometric transformations generate representations in their intermediate layers that are highly predictive of object landmarks. These representations are stacked across layers in a ""hypercolumn"" and projected using spatially-contrastive learning to further improve their performance on few-shot landmark regression tasks and matching. We also provide a comprehensive overview of current equivariant and invariant representation learning methods using contrastive learning, which illuminates the invariances learned. Our experiments on standard benchmarks for landmark learning, as well as a challenging new benchmark, reveal that our approach outperforms prior state-of-the-art methods.",1
"Recognizing human emotion/expressions automatically is quite an expected ability for intelligent robotics, as it can promote better communication and cooperation with humans. Current deep-learning-based algorithms may achieve impressive performance in some lab-controlled environments, but they always fail to recognize the expressions accurately for the uncontrolled in-the-wild situation. Fortunately, facial action units (AU) describe subtle facial behaviors, and they can help distinguish uncertain and ambiguous expressions. In this work, we explore the correlations among the action units and facial expressions, and devise an AU-Expression Knowledge Constrained Representation Learning (AUE-CRL) framework to learn the AU representations without AU annotations and adaptively use representations to facilitate facial expression recognition. Specifically, it leverages AU-expression correlations to guide the learning of the AU classifiers, and thus it can obtain AU representations without incurring any AU annotations. Then, it introduces a knowledge-guided attention mechanism that mines useful AU representations under the constraint of AU-expression correlations. In this way, the framework can capture local discriminative and complementary features to enhance facial representation for facial expression recognition. We conduct experiments on the challenging uncontrolled datasets to demonstrate the superiority of the proposed framework over current state-of-the-art methods. Codes and trained models are available at https://github.com/HCPLab-SYSU/AUE-CRL.",0
"The ability to automatically recognize human emotions and expressions is an important skill for advanced robotics, as it can improve communication and cooperation with humans. Although deep-learning algorithms have shown impressive performance in controlled lab environments, they struggle to accurately identify expressions in uncontrolled, real-world settings. Fortunately, facial action units (AU) can help distinguish subtle facial behaviors and uncertain expressions. This study explores the correlations between AU and facial expressions and introduces an AU-Expression Knowledge Constrained Representation Learning (AUE-CRL) framework to learn AU representations without annotations and use them to enhance facial expression recognition. The framework utilizes AU-expression correlations to guide the learning of AU classifiers and incorporates a knowledge-guided attention mechanism to mine useful AU representations under the constraint of AU-expression correlations. Experimental results demonstrate the effectiveness of the proposed framework on challenging uncontrolled datasets. The codes and trained models are available at https://github.com/HCPLab-SYSU/AUE-CRL.",1
"A key challenge in self-supervised video representation learning is how to effectively capture motion information besides context bias. While most existing works implicitly achieve this with video-specific pretext tasks (e.g., predicting clip orders, time arrows, and paces), we develop a method that explicitly decouples motion supervision from context bias through a carefully designed pretext task. Specifically, we take the keyframes and motion vectors in compressed videos (e.g., in H.264 format) as the supervision sources for context and motion, respectively, which can be efficiently extracted at over 500 fps on the CPU. Then we design two pretext tasks that are jointly optimized: a context matching task where a pairwise contrastive loss is cast between video clip and keyframe features; and a motion prediction task where clip features, passed through an encoder-decoder network, are used to estimate motion features in a near future. These two tasks use a shared video backbone and separate MLP heads. Experiments show that our approach improves the quality of the learned video representation over previous works, where we obtain absolute gains of 16.0% and 11.1% in video retrieval recall on UCF101 and HMDB51, respectively. Moreover, we find the motion prediction to be a strong regularization for video networks, where using it as an auxiliary task improves the accuracy of action recognition with a margin of 7.4%~13.8%.",0
"When it comes to self-supervised video representation learning, a major obstacle is effectively capturing motion information without being biased by context. While many current methods address this implicitly using video-specific pretext tasks, such as predicting clip orders, time arrows, and paces, we have developed a technique that explicitly separates motion supervision from context bias through a thoughtfully designed pretext task. Our approach uses keyframes and motion vectors in compressed videos, such as those found in H.264 format, as supervision sources for motion and context, respectively. These sources can be quickly extracted on the CPU at over 500 fps. We have created two pretext tasks that are optimized in tandem: a context matching task that utilizes pairwise contrastive loss between video clip and keyframe features, and a motion prediction task that estimates motion features in the near future using clip features that pass through an encoder-decoder network. Both tasks use a shared video backbone and separate MLP heads. Our experiments demonstrate that our approach outperforms previous works in terms of learned video representation quality, with absolute gains of 16.0% and 11.1% in video retrieval recall on UCF101 and HMDB51, respectively. Additionally, we have found that motion prediction serves as a robust regularizer for video networks, and using it as an auxiliary task improves the accuracy of action recognition by a margin of 7.4% to 13.8%.",1
"We present a multiview pseudo-labeling approach to video learning, a novel framework that uses complementary views in the form of appearance and motion information for semi-supervised learning in video. The complementary views help obtain more reliable pseudo-labels on unlabeled video, to learn stronger video representations than from purely supervised data. Though our method capitalizes on multiple views, it nonetheless trains a model that is shared across appearance and motion input and thus, by design, incurs no additional computation overhead at inference time. On multiple video recognition datasets, our method substantially outperforms its supervised counterpart, and compares favorably to previous work on standard benchmarks in self-supervised video representation learning.",0
"Our novel approach to video learning employs multiview pseudo-labeling, which utilizes appearance and motion information to enhance semi-supervised learning in video. This framework generates more dependable pseudo-labels for unlabeled video, allowing for the development of stronger video representations beyond traditional supervised data. Despite utilizing multiple views, our method trains a shared model for appearance and motion input, resulting in no additional computation overhead during inference. Our approach significantly outperforms its supervised counterpart on multiple video recognition datasets and compares favorably to previous work on standard benchmarks in self-supervised video representation learning.",1
"Adapting pre-trained representations has become the go-to recipe for learning new downstream tasks with limited examples. While literature has demonstrated great successes via representation learning, in this work, we show that substantial performance improvement of downstream tasks can also be achieved by appropriate designs of the adaptation process. Specifically, we propose a modular adaptation method that selectively performs multiple state-of-the-art (SOTA) adaptation methods in sequence. As different downstream tasks may require different types of adaptation, our modular adaptation enables the dynamic configuration of the most suitable modules based on the downstream task. Moreover, as an extension to existing cross-domain 5-way k-shot benchmarks (e.g., miniImageNet -> CUB), we create a new high-way (~100) k-shot benchmark with data from 10 different datasets. This benchmark provides a diverse set of domains and allows the use of stronger representations learned from ImageNet. Experimental results show that by customizing adaptation process towards downstream tasks, our modular adaptation pipeline (MAP) improves 3.1% in 5-shot classification accuracy over baselines of finetuning and Prototypical Networks.",0
"The practice of utilizing pre-trained representations has become the standard approach for acquiring knowledge on new downstream tasks with limited examples. Despite the impressive achievements in representation learning documented in literature, this study reveals that appropriate adaptation process designs can lead to significant enhancements in downstream task performance. Specifically, we introduce a modular adaptation method that performs various state-of-the-art (SOTA) adaptation methods sequentially, based on the downstream task's requirements. This modular adaptation approach enables the dynamic setup of the most appropriate modules for different downstream tasks. Additionally, we establish a new high-way (~100) k-shot benchmark utilizing data from 10 different datasets, which extends the existing cross-domain 5-way k-shot benchmarks (e.g., miniImageNet -> CUB) and allows for the utilization of more robust representations learned from ImageNet. Our experimental results demonstrate that by tailoring the adaptation process to the downstream task, our modular adaptation pipeline (MAP) boosts 5-shot classification accuracy by 3.1% compared to finetuning and Prototypical Networks baselines.",1
"Deep neural networks may perform poorly when training datasets are heavily class-imbalanced. Recently, two-stage methods decouple representation learning and classifier learning to improve performance. But there is still the vital issue of miscalibration. To address it, we design two methods to improve calibration and performance in such scenarios. Motivated by the fact that predicted probability distributions of classes are highly related to the numbers of class instances, we propose label-aware smoothing to deal with different degrees of over-confidence for classes and improve classifier learning. For dataset bias between these two stages due to different samplers, we further propose shifted batch normalization in the decoupling framework. Our proposed methods set new records on multiple popular long-tailed recognition benchmark datasets, including CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, Places-LT, and iNaturalist 2018. Code will be available at https://github.com/Jia-Research-Lab/MiSLAS.",0
"If heavily class-imbalanced training datasets are used, Deep neural networks may not perform well. However, two-stage methods have been developed to improve performance by separating representation learning and classifier learning. Despite this, there is still an important issue of miscalibration. Therefore, we have created two methods to enhance calibration and performance in such situations. Our first approach, label-aware smoothing, addresses different levels of over-confidence for classes and enhances classifier learning by considering the predicted probability distributions of classes, which are highly related to the number of class instances. Additionally, we propose shifted batch normalization in the decoupling framework to handle dataset bias between the two stages caused by different samplers. Our methods have achieved new records on various well-known long-tailed recognition benchmark datasets, including CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, Places-LT, and iNaturalist 2018. Our code will be available at https://github.com/Jia-Research-Lab/MiSLAS.",1
"Modeling time-evolving knowledge graphs (KGs) has recently gained increasing interest. Here, graph representation learning has become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to interpret their predictions. This paper provides a link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the structural dependencies and the temporal dynamics. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and by attention propagation. Our approach provides human-understandable evidence explaining the forecast. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model obtains a relative improvement of up to 20% on Hits@1 compared to the previous best KG forecasting method. We also conduct a survey with 53 respondents, and the results show that the evidence extracted by the model for link forecasting is aligned with human understanding.",0
"The interest in modeling time-evolving knowledge graphs (KGs) has recently increased. Graph representation learning has become the dominant approach for link prediction on temporal KGs, but embedding-based methods lack interpretability. This paper presents a link forecasting framework that models the structural dependencies and temporal dynamics of query-relevant subgraphs of temporal KGs. We propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide subgraph extraction and expansion. Our approach provides human-understandable evidence for link forecasting. We evaluate our model on four benchmark temporal KGs and achieve up to a 20% relative improvement on Hits@1 compared to the previous best method. We also conduct a survey showing that our model's evidence for link forecasting aligns with human understanding.",1
"Most existing CNN-based super-resolution (SR) methods are developed based on an assumption that the degradation is fixed and known (e.g., bicubic downsampling). However, these methods suffer a severe performance drop when the real degradation is different from their assumption. To handle various unknown degradations in real-world applications, previous methods rely on degradation estimation to reconstruct the SR image. Nevertheless, degradation estimation methods are usually time-consuming and may lead to SR failure due to large estimation errors. In this paper, we propose an unsupervised degradation representation learning scheme for blind SR without explicit degradation estimation. Specifically, we learn abstract representations to distinguish various degradations in the representation space rather than explicit estimation in the pixel space. Moreover, we introduce a Degradation-Aware SR (DASR) network with flexible adaption to various degradations based on the learned representations. It is demonstrated that our degradation representation learning scheme can extract discriminative representations to obtain accurate degradation information. Experiments on both synthetic and real images show that our network achieves state-of-the-art performance for the blind SR task. Code is available at: https://github.com/LongguangWang/DASR.",0
"CNN-based super-resolution (SR) methods commonly assume a fixed and known degradation, such as bicubic downsampling. However, these methods face significant performance issues when the real degradation differs from this assumption. To overcome this challenge in real-world applications, previous methods rely on degradation estimation, which can be time-consuming and lead to SR failure due to large estimation errors. Therefore, this paper proposes an unsupervised degradation representation learning approach for blind SR without explicit degradation estimation. Instead, the approach learns abstract representations in the representation space to distinguish various degradations. Additionally, a Degradation-Aware SR (DASR) network is introduced, which adapts flexibly to various degradations based on the learned representations. Results demonstrate that the proposed degradation representation learning scheme can accurately extract degradation information, achieving state-of-the-art performance for the blind SR task. Code is available at: https://github.com/LongguangWang/DASR.",1
"Vision-and-language pre-training has achieved impressive success in learning multimodal representations between vision and language. To generalize this success to non-English languages, we introduce UC2, the first machine translation-augmented framework for cross-lingual cross-modal representation learning. To tackle the scarcity problem of multilingual captions for image datasets, we first augment existing English-only datasets with other languages via machine translation (MT). Then we extend the standard Masked Language Modeling and Image-Text Matching training objectives to multilingual setting, where alignment between different languages is captured through shared visual context (i.e, using image as pivot). To facilitate the learning of a joint embedding space of images and all languages of interest, we further propose two novel pre-training tasks, namely Masked Region-to-Token Modeling (MRTM) and Visual Translation Language Modeling (VTLM), leveraging MT-enhanced translated data. Evaluation on multilingual image-text retrieval and multilingual visual question answering benchmarks demonstrates that our proposed framework achieves new state-of-the-art on diverse non-English benchmarks while maintaining comparable performance to monolingual pre-trained models on English tasks.",0
"The successful use of vision-and-language pre-training in learning multimodal representations between vision and language has been impressive. However, to extend this success to languages other than English, we have introduced UC2, the first framework that incorporates machine translation into cross-lingual cross-modal representation learning. Since multilingual captions for image datasets are scarce, we have used machine translation (MT) to augment existing English-only datasets with other languages. We then include alignment between different languages through shared visual context using the image as a pivot, and extend the standard Masked Language Modeling and Image-Text Matching training objectives to a multilingual setting. To enable the learning of a joint embedding space of images and all languages of interest, we propose two new pre-training tasks - Masked Region-to-Token Modeling (MRTM) and Visual Translation Language Modeling (VTLM) - that leverage MT-enhanced translated data. Our proposed framework achieves new state-of-the-art performance on diverse non-English benchmarks for multilingual image-text retrieval and multilingual visual question answering, while maintaining comparable performance to monolingual pre-trained models on English tasks.",1
"Unsupervised representation learning with contrastive learning achieved great success. This line of methods duplicate each training batch to construct contrastive pairs, making each training batch and its augmented version forwarded simultaneously and leading to additional computation. We propose a new jigsaw clustering pretext task in this paper, which only needs to forward each training batch itself, and reduces the training cost. Our method makes use of information from both intra- and inter-images, and outperforms previous single-batch based ones by a large margin. It is even comparable to the contrastive learning methods when only half of training batches are used.   Our method indicates that multiple batches during training are not necessary, and opens the door for future research of single-batch unsupervised methods. Our models trained on ImageNet datasets achieve state-of-the-art results with linear classification, outperforming previous single-batch methods by 2.6%. Models transferred to COCO datasets outperform MoCo v2 by 0.4% with only half of the training batches. Our pretrained models outperform supervised ImageNet pretrained models on CIFAR-10 and CIFAR-100 datasets by 0.9% and 4.1% respectively. Code is available at https://github.com/Jia-Research-Lab/JigsawClustering",0
"Contrastive learning has been successful in unsupervised representation learning, but it requires duplicating each training batch to create contrastive pairs. This increases computation as both the original batch and its augmented version need to be forwarded simultaneously. We introduce a new jigsaw clustering task that only requires forwarding the original batch, reducing training costs. Our method utilizes information from intra- and inter-images and outperforms previous single-batch methods significantly, even matching contrastive learning when using only half the training batches. Our approach suggests that multiple batches are unnecessary for training and opens the door for future research in single-batch unsupervised methods. Our ImageNet models achieve state-of-the-art results with linear classification, surpassing previous single-batch methods by 2.6%. Transferring our models to COCO datasets outperforms MoCo v2 by 0.4% with only half of the training batches. Our pretrained models surpass supervised ImageNet pretrained models by 0.9% and 4.1% on CIFAR-10 and CIFAR-100 datasets, respectively. Visit https://github.com/Jia-Research-Lab/JigsawClustering for our code.",1
"Domain adaptation (DA) is a representation learning methodology that transfers knowledge from a label-sufficient source domain to a label-scarce target domain. While most of early methods are focused on unsupervised DA (UDA), several studies on semi-supervised DA (SSDA) are recently suggested. In SSDA, a small number of labeled target images are given for training, and the effectiveness of those data is demonstrated by the previous studies. However, the previous SSDA approaches solely adopt those data for embedding ordinary supervised losses, overlooking the potential usefulness of the few yet informative clues. Based on this observation, in this paper, we propose a novel method that further exploits the labeled target images for SSDA. Specifically, we utilize labeled target images to selectively generate pseudo labels for unlabeled target images. In addition, based on the observation that pseudo labels are inevitably noisy, we apply a label noise-robust learning scheme, which progressively updates the network and the set of pseudo labels by turns. Extensive experimental results show that our proposed method outperforms other previous state-of-the-art SSDA methods.",0
"The method of domain adaptation (DA) involves learning representations that transfer knowledge from a domain with sufficient labels to one with scarce labels. In the past, unsupervised DA (UDA) has been the primary focus, but there have been recent studies on semi-supervised DA (SSDA) that use a few labeled target images for training. However, previous SSDA methods only used these labeled images for regular supervised losses, ignoring their informative potential. To address this issue, this paper proposes a new method that generates pseudo labels for unlabeled target images using the labeled images, and applies a label noise-robust learning scheme to account for the inevitable noise in the pseudo labels. Experimental results demonstrate that this approach outperforms other state-of-the-art SSDA methods.",1
"We present a self-supervised approach for learning video representations using temporal video alignment as a pretext task, while exploiting both frame-level and video-level information. We leverage a novel combination of temporal alignment loss and temporal regularization terms, which can be used as supervision signals for training an encoder network. Specifically, the temporal alignment loss (i.e., Soft-DTW) aims for the minimum cost for temporally aligning videos in the embedding space. However, optimizing solely for this term leads to trivial solutions, particularly, one where all frames get mapped to a small cluster in the embedding space. To overcome this problem, we propose a temporal regularization term (i.e., Contrastive-IDM) which encourages different frames to be mapped to different points in the embedding space. Extensive evaluations on various tasks, including action phase classification, action phase progression, and fine-grained frame retrieval, on three datasets, namely Pouring, Penn Action, and IKEA ASM, show superior performance of our approach over state-of-the-art methods for self-supervised representation learning from videos. In addition, our method provides significant performance gain where labeled data is lacking.",0
"Our study introduces a novel self-supervised method to acquire video representations by exploiting both frame-level and video-level information and utilizing temporal video alignment as a pretext task. We incorporate temporal alignment loss and temporal regularization terms, which serve as supervision signals for training an encoder network. The Soft-DTW temporal alignment loss aims to minimize the cost of aligning videos in the embedding space. However, optimizing only for this term results in trivial solutions where all frames are mapped to a small cluster in the embedding space. To address this issue, we propose a Contrastive-IDM temporal regularization term that encourages diverse frames to be mapped to distinct points in the embedding space. Our approach outperforms state-of-the-art methods for self-supervised representation learning from videos in various tasks, including action phase classification, action phase progression, and fine-grained frame retrieval, on three datasets: Pouring, Penn Action, and IKEA ASM. Furthermore, our method yields significant performance gains in cases where labeled data is scarce.",1
"Cluster assignment of large and complex images is a crucial but challenging task in pattern recognition and computer vision. In this study, we explore the possibility of employing fuzzy clustering in a deep neural network framework. Thus, we present a novel evolutionary unsupervised learning representation model with iterative optimization. It implements the deep adaptive fuzzy clustering (DAFC) strategy that learns a convolutional neural network classifier from given only unlabeled data samples. DAFC consists of a deep feature quality-verifying model and a fuzzy clustering model, where deep feature representation learning loss function and embedded fuzzy clustering with the weighted adaptive entropy is implemented. We joint fuzzy clustering to the deep reconstruction model, in which fuzzy membership is utilized to represent a clear structure of deep cluster assignments and jointly optimize for the deep representation learning and clustering. Also, the joint model evaluates current clustering performance by inspecting whether the re-sampled data from estimated bottleneck space have consistent clustering properties to progressively improve the deep clustering model. Comprehensive experiments on a variety of datasets show that the proposed method obtains a substantially better performance for both reconstruction and clustering quality when compared to the other state-of-the-art deep clustering methods, as demonstrated with the in-depth analysis in the extensive experiments.",0
"Recognizing patterns and assigning clusters to large and complex images is a challenging task in computer vision. In this study, we investigate the application of fuzzy clustering in a deep neural network framework to address this issue. Our approach involves an unsupervised learning representation model that utilizes iterative optimization and deep adaptive fuzzy clustering (DAFC) strategy. DAFC comprises a deep feature quality-verifying model and a fuzzy clustering model that implements a deep feature representation learning loss function and embedded fuzzy clustering with weighted adaptive entropy. We integrate fuzzy clustering into the deep reconstruction model, employing fuzzy membership to represent a clear structure of deep cluster assignments and optimize deep representation learning and clustering. The joint model regularly evaluates clustering performance by examining whether re-sampled data from the bottleneck space has consistent clustering properties, thus progressively improving the deep clustering model. Our experiments on various datasets demonstrate that our proposed method outperforms other state-of-the-art deep clustering methods in both reconstruction and clustering quality.",1
"Graph neural networks (GNNs) are powerful models that have been successful in various graph representation learning tasks. Whereas gradient boosted decision trees (GBDT) often outperform other machine learning methods when faced with heterogeneous tabular data. But what approach should be used for graphs with tabular node features? Previous GNN models have mostly focused on networks with homogeneous sparse features and, as we show, are suboptimal in the heterogeneous setting. In this work, we propose a novel architecture that trains GBDT and GNN jointly to get the best of both worlds: the GBDT model deals with heterogeneous features, while GNN accounts for the graph structure. Our model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN. With an extensive experimental comparison to the leading GBDT and GNN models, we demonstrate a significant increase in performance on a variety of graphs with tabular features. The code is available: https://github.com/nd7141/bgnn.",0
"Graph neural networks (GNNs) have proven to be effective in various graph representation learning tasks, while gradient boosted decision trees (GBDT) have demonstrated superiority over other machine learning methods for heterogeneous tabular data. However, the optimal approach for graphs with tabular node features remains uncertain. Previous GNN models have mainly focused on homogeneous sparse features and are shown to be suboptimal in the heterogeneous setting. In this study, we introduce a new architecture that combines GBDT and GNN, allowing for joint training to take advantage of both models' strengths. Our model benefits from end-to-end optimization, with new trees fitting the gradient updates of GNN. Through extensive experimental comparison to leading GBDT and GNN models, we demonstrate a significant performance increase on a variety of graphs with tabular features. The code for our approach is available at https://github.com/nd7141/bgnn.",1
"Knowledge Distillation refers to a class of methods that transfers the knowledge from a teacher network to a student network. In this paper, we propose Sparse Representation Matching (SRM), a method to transfer intermediate knowledge obtained from one Convolutional Neural Network (CNN) to another by utilizing sparse representation learning. SRM first extracts sparse representations of the hidden features of the teacher CNN, which are then used to generate both pixel-level and image-level labels for training intermediate feature maps of the student network. We formulate SRM as a neural processing block, which can be efficiently optimized using stochastic gradient descent and integrated into any CNN in a plug-and-play manner. Our experiments demonstrate that SRM is robust to architectural differences between the teacher and student networks, and outperforms other KD techniques across several datasets.",0
"The process of transferring knowledge from a teacher network to a student network is known as Knowledge Distillation. Sparse Representation Matching (SRM) is a new method that uses sparse representation learning to transfer intermediate knowledge from one Convolutional Neural Network (CNN) to another. The teacher CNN's sparse representations of hidden features are extracted, and these are used to generate both pixel-level and image-level labels for training the student network's intermediate feature maps. SRM is formulated as a neural processing block that can be easily optimized using stochastic gradient descent and integrated into any CNN. We conducted experiments that demonstrate SRM's robustness to architectural differences between the teacher and student networks and its superior performance over other KD techniques in multiple datasets.",1
"Scene understanding is a critical problem in computer vision. In this paper, we propose a 3D point-based scene graph generation ($\mathbf{SGG_{point}}$) framework to effectively bridge perception and reasoning to achieve scene understanding via three sequential stages, namely scene graph construction, reasoning, and inference. Within the reasoning stage, an EDGE-oriented Graph Convolutional Network ($\texttt{EdgeGCN}$) is created to exploit multi-dimensional edge features for explicit relationship modeling, together with the exploration of two associated twinning interaction mechanisms between nodes and edges for the independent evolution of scene graph representations. Overall, our integrated $\mathbf{SGG_{point}}$ framework is established to seek and infer scene structures of interest from both real-world and synthetic 3D point-based scenes. Our experimental results show promising edge-oriented reasoning effects on scene graph generation studies. We also demonstrate our method advantage on several traditional graph representation learning benchmark datasets, including the node-wise classification on citation networks and whole-graph recognition problems for molecular analysis.",0
"The comprehension of scenes is a vital issue in the field of computer vision. This paper suggests a framework for generating a scene graph in three dimensions ($\mathbf{SGG_{point}}$) that can efficiently link perception and reasoning to achieve scene understanding through three consecutive stages: scene graph construction, reasoning, and inference. The reasoning stage creates an EDGE-oriented Graph Convolutional Network ($\texttt{EdgeGCN}$) that employs multi-dimensional edge features for explicit relationship modeling. Additionally, it explores two twinning interaction mechanisms between nodes and edges to enable the independent evolution of scene graph representations. Our integrated $\mathbf{SGG_{point}}$ framework aims to identify and deduce scene structures of interest from both real-world and synthetic 3D point-based scenes. Our experimental results illustrate the promising edge-oriented reasoning effects on scene graph generation studies. Furthermore, we demonstrate the advantages of our method on several conventional graph representation learning benchmark datasets, including the node-wise classification on citation networks and whole-graph recognition problems for molecular analysis.",1
"Food recognition plays an important role in food choice and intake, which is essential to the health and well-being of humans. It is thus of importance to the computer vision community, and can further support many food-oriented vision and multimodal tasks. Unfortunately, we have witnessed remarkable advancements in generic visual recognition for released large-scale datasets, yet largely lags in the food domain. In this paper, we introduce Food2K, which is the largest food recognition dataset with 2,000 categories and over 1 million images.Compared with existing food recognition datasets, Food2K bypasses them in both categories and images by one order of magnitude, and thus establishes a new challenging benchmark to develop advanced models for food visual representation learning. Furthermore, we propose a deep progressive region enhancement network for food recognition, which mainly consists of two components, namely progressive local feature learning and region feature enhancement. The former adopts improved progressive training to learn diverse and complementary local features, while the latter utilizes self-attention to incorporate richer context with multiple scales into local features for further local feature enhancement. Extensive experiments on Food2K demonstrate the effectiveness of our proposed method. More importantly, we have verified better generalization ability of Food2K in various tasks, including food recognition, food image retrieval, cross-modal recipe retrieval, food detection and segmentation. Food2K can be further explored to benefit more food-relevant tasks including emerging and more complex ones (e.g., nutritional understanding of food), and the trained models on Food2K can be expected as backbones to improve the performance of more food-relevant tasks. We also hope Food2K can serve as a large scale fine-grained visual recognition benchmark.",0
"The identification of food is crucial to the selection and consumption of food, which is crucial to human health and well-being. As a result, the computer vision community is interested in this field, as it can support various food-oriented vision and multimodal tasks. Despite the significant advancements in generic visual recognition for large-scale datasets, there has been a lack of progress in the food domain. In this study, we present Food2K, which is the most extensive food recognition dataset with over a million images and 2,000 categories. Food2K surpasses existing food recognition datasets in both categories and images, establishing a more challenging benchmark for developing advanced models for food visual representation learning. In addition, we propose a deep progressive region enhancement network for food recognition, which includes two components: progressive local feature learning and region feature enhancement. Our approach has been shown to be effective through extensive experiments on Food2K, demonstrating better generalization ability in various tasks, including food recognition, food image retrieval, cross-modal recipe retrieval, food detection, and segmentation. Food2K can be further utilized for more complex and emerging food-related tasks, such as nutritional understanding of food. The models trained on Food2K can be used as backbones for improving the performance of more food-relevant tasks, and Food2K can serve as a benchmark for large-scale fine-grained visual recognition.",1
"We propose a local-to-global representation learning algorithm for 3D point cloud data, which is appropriate to handle various geometric transformations, especially rotation, without explicit data augmentation with respect to the transformations. Our model takes advantage of multi-level abstraction based on graph convolutional neural networks, which constructs a descriptor hierarchy to encode rotation-invariant shape information of an input object in a bottom-up manner. The descriptors in each level are obtained from a neural network based on a graph via stochastic sampling of 3D points, which is effective in making the learned representations robust to the variations of input data. The proposed algorithm presents the state-of-the-art performance on the rotation-augmented 3D object recognition and segmentation benchmarks, and we further analyze its characteristics through comprehensive ablative experiments.",0
"Our proposed algorithm for 3D point cloud data utilizes a local-to-global representation learning approach that can handle various geometric transformations, particularly rotation, without the need for explicit data augmentation for these transformations. Our model utilizes multi-level abstraction through graph convolutional neural networks to create a descriptor hierarchy that encodes rotation-invariant shape information from the input object in a bottom-up fashion. The neural network-based graph generates descriptors at each level through stochastic sampling of 3D points, which makes the learned representations robust against input data variations. Our algorithm outperforms current state-of-the-art methods in rotation-augmented 3D object recognition and segmentation benchmarks, and we conduct comprehensive ablative experiments to further analyze its characteristics.",1
"Learning robust representations to discriminate cell phenotypes based on microscopy images is important for drug discovery. Drug development efforts typically analyse thousands of cell images to screen for potential treatments. Early works focus on creating hand-engineered features from these images or learn such features with deep neural networks in a fully or weakly-supervised framework. Both require prior knowledge or labelled datasets. Therefore, subsequent works propose unsupervised approaches based on generative models to learn these representations. Recently, representations learned with self-supervised contrastive loss-based methods have yielded state-of-the-art results on various imaging tasks compared to earlier unsupervised approaches. In this work, we leverage a contrastive learning framework to learn appropriate representations from single-cell fluorescent microscopy images for the task of Mechanism-of-Action classification. The proposed work is evaluated on the annotated BBBC021 dataset, and we obtain state-of-the-art results in NSC, NCSB and drop metrics for an unsupervised approach. We observe an improvement of 10% in NCSB accuracy and 11% in NSC-NSCB drop over the previously best unsupervised method. Moreover, the performance of our unsupervised approach ties with the best supervised approach. Additionally, we observe that our framework performs well even without post-processing, unlike earlier methods. With this, we conclude that one can learn robust cell representations with contrastive learning.",0
"It is crucial to develop strong representations that can distinguish between cell phenotypes based on microscopy images for drug discovery purposes. When developing drugs, thousands of cell images are typically analysed to screen for potential treatments. Initially, researchers focused on creating hand-crafted features from these images or utilising deep neural networks to learn features in a supervised or weakly-supervised setting. However, both methods require labelled datasets or prior knowledge. Therefore, recent studies have proposed unsupervised methods that rely on generative models to learn representations. Self-supervised contrastive loss-based methods have shown superior results compared to earlier unsupervised approaches. In this study, we use a contrastive learning framework to develop appropriate representations from single-cell fluorescent microscopy images for Mechanism-of-Action classification. Our work is evaluated on the annotated BBBC021 dataset, and we achieve state-of-the-art results in NSC, NCSB, and drop metrics for an unsupervised approach. We observe a 10% increase in NCSB accuracy and an 11% increase in NSC-NSCB drop compared to the previous best unsupervised method. Furthermore, our unsupervised approach performs as well as the best supervised approach. We also find that our framework performs well without post-processing, which sets it apart from earlier methods. In summary, our study demonstrates that contrastive learning can yield robust cell representations.",1
"Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously.   Experiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.",0
"Recognizing objects relies on two primary cues, shape and texture, which are complementary. However, Convolutional Neural Networks often exhibit bias towards one of these cues, depending on their training dataset, which can degrade model performance. Our research has shown that this bias can be rectified through a simple algorithm that promotes shape-texture debiased learning. We achieve this by supplementing training data with images that conflict in shape and texture, such as a chimpanzee-shaped image with lemon texture, and providing corresponding supervisions for both cues simultaneously. Our experiments have demonstrated the success of this method in improving model performance on various image recognition benchmarks and enhancing adversarial robustness. For instance, when training on ImageNet, our approach has helped ResNet-152 achieve noteworthy enhancements in ImageNet (+1.2%), ImageNet-A (+5.2%), ImageNet-C (+8.3%), and Stylized-ImageNet (+11.1%), as well as in defending against FGSM adversarial attacks on ImageNet (+14.4%). Furthermore, our method is compatible with other advanced data augmentation strategies, such as Mixup and CutMix. The code for our algorithm is available at https://github.com/LiYingwei/ShapeTextureDebiasedTraining.",1
"We propose a method to disentangle linear-encoded facial semantics from StyleGAN without external supervision. The method derives from linear regression and sparse representation learning concepts to make the disentangled latent representations easily interpreted as well. We start by coupling StyleGAN with a stabilized 3D deformable facial reconstruction method to decompose single-view GAN generations into multiple semantics. Latent representations are then extracted to capture interpretable facial semantics. In this work, we make it possible to get rid of labels for disentangling meaningful facial semantics. Also, we demonstrate that the guided extrapolation along the disentangled representations can help with data augmentation, which sheds light on handling unbalanced data. Finally, we provide an analysis of our learned localized facial representations and illustrate that the semantic information is encoded, which surprisingly complies with human intuition. The overall unsupervised design brings more flexibility to representation learning in the wild.",0
"Our proposed method involves disentangling facial semantics that are linear-encoded within StyleGAN without the need for external supervision. To achieve this, we utilize concepts from linear regression and sparse representation learning, which enable the resulting latent representations to be easily interpreted. We begin by utilizing a stabilized 3D deformable facial reconstruction method in conjunction with StyleGAN to break down single-view GAN generations into multiple semantics. We then extract latent representations to capture interpretable facial semantics, without the need for labels. Our method also demonstrates the benefits of guided extrapolation along the disentangled representations, which can aid in data augmentation and handling unbalanced data. Furthermore, we analyze our learned localized facial representations and show that semantic information is encoded in a manner that aligns with human intuition. Our unsupervised approach enhances the flexibility of representation learning in real-world settings.",1
"Despite recent advances in representation learning in hypercomplex (HC) space, this subject is still vastly unexplored in the context of graphs. Motivated by the complex and quaternion algebras, which have been found in several contexts to enable effective representation learning that inherently incorporates a weight-sharing mechanism, we develop graph neural networks that leverage the properties of hypercomplex feature transformation. In particular, in our proposed class of models, the multiplication rule specifying the algebra itself is inferred from the data during training. Given a fixed model architecture, we present empirical evidence that our proposed model incorporates a regularization effect, alleviating the risk of overfitting. We also show that for fixed model capacity, our proposed method outperforms its corresponding real-formulated GNN, providing additional confirmation for the enhanced expressivity of HC embeddings. Finally, we test our proposed hypercomplex GNN on several open graph benchmark datasets and show that our models reach state-of-the-art performance while consuming a much lower memory footprint with 70& fewer parameters. Our implementations are available at https://github.com/bayer-science-for-a-better-life/phc-gnn.",0
"Representation learning in hypercomplex (HC) space has made recent advances, but it remains relatively unexplored in the context of graphs. We were inspired by the effectiveness of complex and quaternion algebras in enabling representation learning that includes a weight-sharing mechanism. Therefore, we developed graph neural networks that utilize hypercomplex feature transformation properties. Our proposed models infer the multiplication rule of the algebra from the data during training and provide empirical evidence of a regularization effect, reducing the risk of overfitting. Compared to real-formulated GNNs, our proposed method outperforms them with enhanced expressivity of HC embeddings. We tested our hypercomplex GNN on several open graph benchmark datasets and achieved state-of-the-art performance with significantly fewer parameters and a lower memory footprint. Our implementations are available at https://github.com/bayer-science-for-a-better-life/phc-gnn.",1
"Most successful self-supervised learning methods are trained to align the representations of two independent views from the data. State-of-the-art methods in video are inspired by image techniques, where these two views are similarly extracted by cropping and augmenting the resulting crop. However, these methods miss a crucial element in the video domain: time. We introduce BraVe, a self-supervised learning framework for video. In BraVe, one of the views has access to a narrow temporal window of the video while the other view has a broad access to the video content. Our models learn to generalise from the narrow view to the general content of the video. Furthermore, BraVe processes the views with different backbones, enabling the use of alternative augmentations or modalities into the broad view such as optical flow, randomly convolved RGB frames, audio or their combinations. We demonstrate that BraVe achieves state-of-the-art results in self-supervised representation learning on standard video and audio classification benchmarks including UCF101, HMDB51, Kinetics, ESC-50 and AudioSet.",0
"The majority of successful self-supervised learning approaches are designed to align the representations of two separate views of the data. The latest methods for video are influenced by image techniques, where the two views are obtained by cropping and augmenting the resulting crop in a similar manner. However, these methods neglect a crucial aspect in the video domain: time. We present BraVe, a self-supervised learning framework for video. In BraVe, one of the views is exposed to a narrow temporal window of the video while the other view has broader access to the video content. Our models are trained to generalize from the narrow view to the general content of the video. Additionally, BraVe employs different backbones to process the views, enabling the use of alternative augmentations or modalities such as optical flow, randomly convolved RGB frames, audio, or their combinations into the broad view. Our results show that BraVe outperforms previous state-of-the-art methods in self-supervised representation learning on standard video and audio classification benchmarks, including UCF101, HMDB51, Kinetics, ESC-50, and AudioSet.",1
"What is the best way to learn a universal face representation? Recent work on Deep Learning in the area of face analysis has focused on supervised learning for specific tasks of interest (e.g. face recognition, facial landmark localization etc.) but has overlooked the overarching question of how to find a facial representation that can be readily adapted to several facial analysis tasks and datasets. To this end, we make the following 4 contributions: (a) we introduce, for the first time, a comprehensive evaluation benchmark for facial representation learning consisting of 5 important face analysis tasks. (b) We systematically investigate two ways of large-scale representation learning applied to faces: supervised and unsupervised pre-training. Importantly, we focus our evaluations on the case of few-shot facial learning. (c) We investigate important properties of the training datasets including their size and quality (labelled, unlabelled or even uncurated). (d) To draw our conclusions, we conducted a very large number of experiments. Our main two findings are: (1) Unsupervised pre-training on completely in-the-wild, uncurated data provides consistent and, in some cases, significant accuracy improvements for all facial tasks considered. (2) Many existing facial video datasets seem to have a large amount of redundancy. We will release code, pre-trained models and data to facilitate future research.",0
"The focus of recent Deep Learning research in face analysis has been on supervised learning for specific tasks, such as face recognition and facial landmark localization. However, the question of how to develop a facial representation that can be applied to various facial analysis tasks and datasets has been overlooked. In response, we present four contributions: (a) the introduction of a comprehensive evaluation benchmark for facial representation learning consisting of five important face analysis tasks, (b) a systematic investigation of two large-scale representation learning methods, supervised and unsupervised pre-training, with a specific focus on few-shot facial learning, (c) an exploration of important properties of training datasets, including size and quality, and (d) a large number of experiments to draw conclusions. Our findings indicate that unsupervised pre-training on completely uncurated, in-the-wild data consistently improves accuracy for all facial tasks considered. Additionally, we discovered that many existing facial video datasets have significant redundancy. To facilitate future research, we will release code, pre-trained models, and data.",1
"We provide a construction for categorical representation learning and introduce the foundations of ""$\textit{categorifier}$"". The central theme in representation learning is the idea of $\textbf{everything to vector}$. Every object in a dataset $\mathcal{S}$ can be represented as a vector in $\mathbb{R}^n$ by an $\textit{encoding map}$ $E: \mathcal{O}bj(\mathcal{S})\to\mathbb{R}^n$. More importantly, every morphism can be represented as a matrix $E: \mathcal{H}om(\mathcal{S})\to\mathbb{R}^{n}_{n}$. The encoding map $E$ is generally modeled by a $\textit{deep neural network}$. The goal of representation learning is to design appropriate tasks on the dataset to train the encoding map (assuming that an encoding is optimal if it universally optimizes the performance on various tasks). However, the latter is still a $\textit{set-theoretic}$ approach. The goal of the current article is to promote the representation learning to a new level via a $\textit{category-theoretic}$ approach. As a proof of concept, we provide an example of a text translator equipped with our technology, showing that our categorical learning model outperforms the current deep learning models by 17 times. The content of the current article is part of the recent US patent proposal (patent application number: 63110906).",0
"In this article, we introduce the concept of ""$\textit{categorifier}$"" and present a method for categorical representation learning. The fundamental principle of representation learning is to convert every object in a dataset $\mathcal{S}$ into a vector in $\mathbb{R}^n$ using an $\textit{encoding map}$ $E: \mathcal{O}bj(\mathcal{S})\to\mathbb{R}^n$. Additionally, every morphism can be represented as a matrix $E: \mathcal{H}om(\mathcal{S})\to\mathbb{R}^{n}_{n}$. Typically, the encoding map $E$ is implemented using a $\textit{deep neural network}$, and the aim of representation learning is to develop appropriate tasks to train the encoding map. However, this approach is based on set theory. The focus of this article is to introduce a new approach to representation learning using category theory. We demonstrate the effectiveness of our categorical learning model by discussing an example of a text translator, which outperforms current deep learning models by 17 times. This article is part of a recent US patent proposal (patent application number: 63110906).",1
"Current semantic segmentation methods focus only on mining ""local"" context, i.e., dependencies between pixels within individual images, by context-aggregation modules (e.g., dilated convolution, neural attention) or structure-aware optimization criteria (e.g., IoU-like loss). However, they ignore ""global"" context of the training data, i.e., rich semantic relations between pixels across different images. Inspired by the recent advance in unsupervised contrastive representation learning, we propose a pixel-wise contrastive framework for semantic segmentation in the fully supervised setting. The core idea is to enforce pixel embeddings belonging to a same semantic class to be more similar than embeddings from different classes. It raises a pixel-wise metric learning paradigm for semantic segmentation, by explicitly exploring the structures of labeled pixels, which were rarely explored before. Our method can be effortlessly incorporated into existing segmentation frameworks without extra overhead during testing. We experimentally show that, with famous segmentation models (i.e., DeepLabV3, HRNet, OCR) and backbones (i.e., ResNet, HR-Net), our method brings consistent performance improvements across diverse datasets (i.e., Cityscapes, PASCAL-Context, COCO-Stuff, CamVid). We expect this work will encourage our community to rethink the current de facto training paradigm in fully supervised semantic segmentation.",0
"At present, semantic segmentation techniques concentrate solely on analyzing ""local"" context, which means accounting for the connections between pixels within individual images. This is achieved through context-aggregation modules like dilated convolution and neural attention, or structure-aware optimization criteria such as IoU-like loss. However, these methods overlook the ""global"" context of the training data, which encompasses the intricate semantic relationships between pixels across various images. Our proposal is inspired by recent developments in unsupervised contrastive representation learning. It involves a pixel-wise contrastive framework for semantic segmentation in the fully supervised setting. Essentially, our approach aims to make pixel embeddings belonging to the same semantic class more similar than embeddings from different classes. This, in turn, creates a pixel-wise metric learning paradigm for semantic segmentation that explicitly explores the structures of labeled pixels, which has not been done before. Our method can be effortlessly integrated into existing segmentation frameworks without any additional overhead during testing. We conducted experiments with famous segmentation models and backbones and across diverse datasets, including Cityscapes, PASCAL-Context, COCO-Stuff, and CamVid. Our results demonstrate that our method consistently improves performance. We hope that our work will encourage the community to reconsider the current default training paradigm in fully supervised semantic segmentation.",1
"Standard meta-learning for representation learning aims to find a common representation to be shared across multiple tasks. The effectiveness of these methods is often limited when the nuances of the tasks' distribution cannot be captured by a single representation. In this work we overcome this issue by inferring a conditioning function, mapping the tasks' side information (such as the tasks' training dataset itself) into a representation tailored to the task at hand. We study environments in which our conditional strategy outperforms standard meta-learning, such as those in which tasks can be organized in separate clusters according to the representation they share. We then propose a meta-algorithm capable of leveraging this advantage in practice. In the unconditional setting, our method yields a new estimator enjoying faster learning rates and requiring less hyper-parameters to tune than current state-of-the-art methods. Our results are supported by preliminary experiments.",0
"The objective of standard meta-learning for representation learning is to identify a shared representation that can be used across multiple tasks. However, these methods have limitations in capturing the intricacies of task distribution through a single representation. To address this limitation, our research focuses on developing a conditioning function that can map the side information of tasks (such as the training dataset) into a customized representation for each task. Our analysis shows that this conditional approach outperforms standard meta-learning, particularly when tasks can be classified into different clusters based on their shared representation. We present a meta-algorithm that can leverage this advantage in practical settings. In the unconditional context, our method offers a novel estimator that has faster learning rates and requires fewer hyper-parameters than existing state-of-the-art methods. We have conducted preliminary experiments that support our findings.",1
"Deep Reinforcement Learning (DRL) has recently achieved significant advances in various domains. However, explaining the policy of RL agents still remains an open problem due to several factors, one being the complexity of explaining neural networks decisions. Recently, a group of works have used decision-tree-based models to learn explainable policies. Soft decision trees (SDTs) and discretized differentiable decision trees (DDTs) have been demonstrated to achieve both good performance and share the benefit of having explainable policies. In this work, we further improve the results for tree-based explainable RL in both performance and explainability. Our proposal, Cascading Decision Trees (CDTs) apply representation learning on the decision path to allow richer expressivity. Empirical results show that in both situations, where CDTs are used as policy function approximators or as imitation learners to explain black-box policies, CDTs can achieve better performances with more succinct and explainable models than SDTs. As a second contribution our study reveals limitations of explaining black-box policies via imitation learning with tree-based explainable models, due to its inherent instability.",0
"Various domains have seen significant advancements in Deep Reinforcement Learning (DRL). However, there still exists an open problem of explaining the policy of RL agents, owing to the complexity of neural networks decisions. Some recent works have employed decision-tree-based models to achieve explainable policies. Soft decision trees (SDTs) and discretized differentiable decision trees (DDTs) are known to deliver good performance and explainable policies. This work aims to enhance the performance and explainability of tree-based explainable RL. To achieve this, we introduce Cascading Decision Trees (CDTs), which apply representation learning on the decision path to enable richer expressivity. Empirical results demonstrate that CDTs outperform SDTs in both situations where they function as policy function approximators or as imitation learners to explain black-box policies. Our study also highlights the limitations of explaining black-box policies via imitation learning with tree-based explainable models due to their inherent instability.",1
"This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that addresses the fundamental limitations of instance-wise contrastive learning. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.",0
"The article introduces Prototypical Contrastive Learning (PCL), an unsupervised method for learning representations that overcome the limitations of instance-wise contrastive learning. The method not only learns low-level features for instance discrimination but also implicitly encodes the data's semantic structures into the learned embedding space. The approach introduces prototypes as latent variables, which help find the maximum-likelihood estimation of the network parameters using an Expectation-Maximization framework. The E-step involves finding the distribution of prototypes via clustering, and the M-step involves optimizing the network using contrastive learning. The article proposes ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, to encourage representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks, including low-resource transfer learning. The code and pretrained models are available at https://github.com/salesforce/PCL.",1
"This paper studies few-shot learning via representation learning, where one uses $T$ source tasks with $n_1$ data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only $n_2 (\ll n_1)$ data. Specifically, we focus on the setting where there exists a good \emph{common representation} between source and target, and our goal is to understand how much of a sample size reduction is possible. First, we study the setting where this common representation is low-dimensional and provide a fast rate of $O\left(\frac{\mathcal{C}\left(\Phi\right)}{n_1T} + \frac{k}{n_2}\right)$; here, $\Phi$ is the representation function class, $\mathcal{C}\left(\Phi\right)$ is its complexity measure, and $k$ is the dimension of the representation. When specialized to linear representation functions, this rate becomes $O\left(\frac{dk}{n_1T} + \frac{k}{n_2}\right)$ where $d (\gg k)$ is the ambient input dimension, which is a substantial improvement over the rate without using representation learning, i.e. over the rate of $O\left(\frac{d}{n_2}\right)$. This result bypasses the $\Omega(\frac{1}{T})$ barrier under the i.i.d. task assumption, and can capture the desired property that all $n_1T$ samples from source tasks can be \emph{pooled} together for representation learning. Next, we consider the setting where the common representation may be high-dimensional but is capacity-constrained (say in norm); here, we again demonstrate the advantage of representation learning in both high-dimensional linear regression and neural network learning. Our results demonstrate representation learning can fully utilize all $n_1T$ samples from source tasks.",0
"The aim of this study is to investigate few-shot learning by means of representation learning. This involves using $T$ source tasks with $n_1$ data per task to develop a representation that reduces the sample complexity of a target task that has only $n_2$ data available. The focus is on identifying a good common representation between the source and target tasks, and determining the extent to which sample size reduction is achievable. The study examines two scenarios: one where the common representation is low-dimensional, and another where it may be high-dimensional but is capacity-constrained. In the former, the study provides a fast rate of $O\left(\frac{\mathcal{C}\left(\Phi\right)}{n_1T} + \frac{k}{n_2}\right)$, with a specialized rate of $O\left(\frac{dk}{n_1T} + \frac{k}{n_2}\right)$ for linear representation functions. This represents a significant improvement over the rate without representation learning, which is $O\left(\frac{d}{n_2}\right)$. In the latter scenario, the study shows that representation learning can fully utilize all $n_1T$ samples from source tasks in both high-dimensional linear regression and neural network learning.",1
"Single domain generalization is a challenging case of model generalization, where the models are trained on a single domain and tested on other unseen domains. A promising solution is to learn cross-domain invariant representations by expanding the coverage of the training domain. These methods have limited generalization performance gains in practical applications due to the lack of appropriate safety and effectiveness constraints. In this paper, we propose a novel learning framework called progressive domain expansion network (PDEN) for single domain generalization. The domain expansion subnetwork and representation learning subnetwork in PDEN mutually benefit from each other by joint learning. For the domain expansion subnetwork, multiple domains are progressively generated in order to simulate various photometric and geometric transforms in unseen domains. A series of strategies are introduced to guarantee the safety and effectiveness of the expanded domains. For the domain invariant representation learning subnetwork, contrastive learning is introduced to learn the domain invariant representation in which each class is well clustered so that a better decision boundary can be learned to improve it's generalization. Extensive experiments on classification and segmentation have shown that PDEN can achieve up to 15.28% improvement compared with the state-of-the-art single-domain generalization methods.",0
"Single domain generalization is a challenging task that involves training models on a single domain and testing them on other previously unseen domains. To enhance the generalization performance, cross-domain invariant representations can be learned by expanding the coverage of the training domain. However, existing methods lack appropriate safety and effectiveness constraints, limiting their practical applications. This paper proposes a novel learning framework called progressive domain expansion network (PDEN) that addresses this issue. The domain expansion subnetwork and representation learning subnetwork in PDEN mutually benefit from joint learning. The domain expansion subnetwork generates multiple domains progressively to simulate various photometric and geometric transforms in unseen domains, while a series of strategies guarantee their safety and effectiveness. The domain invariant representation learning subnetwork uses contrastive learning to enable well-clustered classes, improving the decision boundary and generalization performance. Experimental results on classification and segmentation demonstrate that PDEN achieves up to 15.28% improvement compared to state-of-the-art single-domain generalization methods.",1
"Most of the existing literature regarding hyperbolic embedding concentrate upon supervised learning, whereas the use of unsupervised hyperbolic embedding is less well explored. In this paper, we analyze how unsupervised tasks can benefit from learned representations in hyperbolic space. To explore how well the hierarchical structure of unlabeled data can be represented in hyperbolic spaces, we design a novel hyperbolic message passing auto-encoder whose overall auto-encoding is performed in hyperbolic space. The proposed model conducts auto-encoding the networks via fully utilizing hyperbolic geometry in message passing. Through extensive quantitative and qualitative analyses, we validate the properties and benefits of the unsupervised hyperbolic representations. Codes are available at https://github.com/junhocho/HGCAE.",0
"The majority of literature on hyperbolic embedding focuses on supervised learning, leaving unsupervised hyperbolic embedding underexplored. This study examines the potential advantages of learned representations in hyperbolic space for unsupervised tasks. We introduce a new hyperbolic message passing auto-encoder that fully utilizes hyperbolic geometry to perform auto-encoding in hyperbolic space, enabling the representation of the hierarchical structure of unlabeled data. Our proposed model is extensively analyzed through quantitative and qualitative methods, confirming the benefits of unsupervised hyperbolic representations. The codes for this study are available at https://github.com/junhocho/HGCAE.",1
"Graph Neural Networks (GNNs) have emerged as a powerful and flexible framework for representation learning on irregular data. As they generalize the operations of classical CNNs on grids to arbitrary topologies, GNNs also bring much of the implementation challenges of their Euclidean counterparts. Model size, memory footprint, and energy consumption are common concerns for many real-world applications. Network binarization allocates a single bit to parameters and activations, thus dramatically reducing the memory requirements (up to 32x compared to single-precision floating-point numbers) and maximizing the benefits of fast SIMD instructions on modern hardware for measurable speedups. However, in spite of the large body of work on binarization for classical CNNs, this area remains largely unexplored in geometric deep learning. In this paper, we present and evaluate different strategies for the binarization of graph neural networks. We show that through careful design of the models, and control of the training process, binary graph neural networks can be trained at only a moderate cost in accuracy on challenging benchmarks. In particular, we present the first dynamic graph neural network in Hamming space, able to leverage efficient k-NN search on binary vectors to speed-up the construction of the dynamic graph. We further verify that the binary models offer significant savings on embedded devices. Our code is publicly available on Github.",0
"Graph Neural Networks (GNNs) are a versatile and potent approach to representation learning on non-uniform data. While they extend the operations of traditional Convolutional Neural Networks (CNNs) to arbitrary topologies, GNNs also pose similar implementation challenges as their Euclidean counterparts. Memory footprint, model size, and energy consumption are major concerns for real-world applications. Network binarization assigns a single bit to parameters and activations, leading to a significant reduction in memory requirements (up to 32 times compared to single-precision floating-point numbers) and maximizing the benefits of fast SIMD instructions on modern hardware for measurable speedups. However, despite the extensive research on binarization for classical CNNs, this area remains largely unexplored in geometric deep learning. In this study, we propose and evaluate different approaches for the binarization of graph neural networks. We show that with careful model design and training process control, binary graph neural networks can be trained with only a moderate drop in accuracy on challenging benchmarks. We also introduce the first dynamic graph neural network in Hamming space, which can leverage efficient k-NN search on binary vectors to accelerate the construction of the dynamic graph. Furthermore, we demonstrate that binary models offer significant savings on embedded devices. Our work is open-source and available on Github.",1
"Most existing distance metric learning approaches use fully labeled data to learn the sample similarities in an embedding space. We present a self-training framework, SLADE, to improve retrieval performance by leveraging additional unlabeled data. We first train a teacher model on the labeled data and use it to generate pseudo labels for the unlabeled data. We then train a student model on both labels and pseudo labels to generate final feature embeddings. We use self-supervised representation learning to initialize the teacher model. To better deal with noisy pseudo labels generated by the teacher network, we design a new feature basis learning component for the student network, which learns basis functions of feature representations for unlabeled data. The learned basis vectors better measure the pairwise similarity and are used to select high-confident samples for training the student network. We evaluate our method on standard retrieval benchmarks: CUB-200, Cars-196 and In-shop. Experimental results demonstrate that our approach significantly improves the performance over the state-of-the-art methods.",0
"The majority of current methods for distance metric learning require fully labeled data to teach the similarities between samples in an embedding space. In contrast, we introduce a self-training framework called SLADE that enhances retrieval performance through the use of additional unlabeled data. Initially, we train a teacher model on the labeled data and utilize it to produce pseudo labels for the unlabeled data. We then train a student model on both the labels and pseudo labels to generate final feature embeddings. We employ self-supervised representation learning to initialize the teacher model. To handle the noisy pseudo labels generated by the teacher network, we develop a novel feature basis learning component for the student network. This component learns basis functions of feature representations for unlabeled data. The acquired basis vectors more accurately measure pairwise similarity and are used to select high-confidence samples for training the student network. We assess our approach on well-established retrieval benchmarks: CUB-200, Cars-196, and In-shop. Our experimental results demonstrate that our method outperforms state-of-the-art methods by a significant margin.",1
"This work investigates fundamental questions related to learning features in convolutional neural networks (CNN). Empirical findings across multiple architectures such as VGG, ResNet, Inception, DenseNet and MobileNet indicate that weights near the center of a filter are larger than weights on the outside. Current regularization schemes violate this principle. Thus, we introduce Locality-promoting Regularization (LOCO-Reg), which yields accuracy gains across multiple architectures and datasets. We also show theoretically that the empirical finding is a consequence of maximizing feature cohesion under the assumption of spatial locality.",0
"The focus of this study is to explore essential inquiries concerning the acquisition of features in convolutional neural networks (CNN). After conducting experiments on various architectures, including VGG, ResNet, Inception, DenseNet, and MobileNet, it was discovered that the weights in the middle of a filter are more significant than those on the periphery. This principle is not upheld by current regularization techniques. Therefore, a new method called Locality-promoting Regularization (LOCO-Reg) is introduced, resulting in improved accuracy across different architectures and datasets. Furthermore, the study demonstrates that the empirical finding is the outcome of maximizing feature consistency based on the presumption of spatial locality.",1
"This paper addresses the challenge of novel view synthesis for a human performer from a very sparse set of camera views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves remarkable view synthesis quality given dense input views. However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed problem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. The deformable mesh also provides geometric guidance for the network to learn 3D representations more efficiently. To evaluate our approach, we create a multi-view dataset named ZJU-MoCap that captures performers with complex motions. Experiments on ZJU-MoCap show that our approach outperforms prior works by a large margin in terms of novel view synthesis quality. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset. The code and dataset are available at https://zju3dv.github.io/neuralbody/.",0
"In this paper, the challenge of generating new views of a human performer from a limited number of camera angles is explored. While some recent studies have achieved impressive results in view synthesis by using implicit neural representations of 3D scenes with dense input views, sparse input views pose an ill-posed problem for representation learning. To address this issue, this paper proposes Neural Body, a new human body representation that integrates observations across video frames. The proposed representation assumes that learned neural representations at different frames share the same latent codes anchored to a deformable mesh, which facilitates natural integration of observations across frames and enhances the network's ability to learn 3D representations efficiently. The effectiveness of the proposed approach is evaluated using the ZJU-MoCap dataset, which captures performers with complex motions. Results show that the proposed approach outperforms prior works significantly in terms of novel view synthesis quality. Additionally, the proposed approach is demonstrated to be capable of reconstructing a moving person from a monocular video on the People-Snapshot dataset. The code and dataset are available at https://zju3dv.github.io/neuralbody/.",1
"Self-supervised visual representation learning has seen huge progress recently, but no large scale evaluation has compared the many models now available. We evaluate the transfer performance of 13 top self-supervised models on 40 downstream tasks, including many-shot and few-shot recognition, object detection, and dense prediction. We compare their performance to a supervised baseline and show that on most tasks the best self-supervised models outperform supervision, confirming the recently observed trend in the literature. We find ImageNet Top-1 accuracy to be highly correlated with transfer to many-shot recognition, but increasingly less so for few-shot, object detection and dense prediction. No single self-supervised method dominates overall, suggesting that universal pre-training is still unsolved. Our analysis of features suggests that top self-supervised learners fail to preserve colour information as well as supervised alternatives, but tend to induce better classifier calibration, and less attentive overfitting than supervised learners.",0
"Recently, there has been significant progress in self-supervised visual representation learning. However, there has been no large-scale evaluation that has compared the multitude of available models. To address this, we assessed the transfer performance of 13 leading self-supervised models on 40 downstream tasks, which included object detection, few-shot and many-shot recognition, and dense prediction. We compared the models to a supervised baseline and found that the best self-supervised models outperformed supervision in most tasks, which confirms the trend observed in the literature. Our findings showed that ImageNet Top-1 accuracy had a strong correlation with transfer to many-shot recognition; however, this correlation decreased for few-shot, object detection, and dense prediction. No single self-supervised method dominated overall, suggesting that universal pre-training remains unsolved. Our analysis of features revealed that top self-supervised learners did not preserve color information as well as supervised alternatives. However, these models induced better classifier calibration and less attentive overfitting than supervised learners.",1
"Mixup is a powerful data augmentation method that interpolates between two or more examples in the input or feature space and between the corresponding target labels. Many recent mixup methods focus on cutting and pasting two or more objects into one image, which is more about efficient processing than interpolation. However, how to best interpolate images is not well defined. In this sense, mixup has been connected to autoencoders, because often autoencoders ""interpolate well"", for instance generating an image that continuously deforms into another.   In this work, we revisit mixup from the interpolation perspective and introduce AlignMix, where we geometrically align two images in the feature space. The correspondences allow us to interpolate between two sets of features, while keeping the locations of one set. Interestingly, this gives rise to a situation where mixup retains mostly the geometry or pose of one image and the texture of the other, connecting it to style transfer. More than that, we show that an autoencoder can still improve representation learning under mixup, without the classifier ever seeing decoded images. AlignMix outperforms state-of-the-art mixup methods on five different benchmarks.",0
"The technique of mixup involves the combination of two or more examples in the input or feature space and their corresponding target labels, resulting in a powerful data augmentation method. Although some recent mixup methods involve the efficient processing of cutting and pasting objects into one image, this is not considered true interpolation. The connection between mixup and autoencoders has been made, as autoencoders have shown to interpolate well. In this study, the authors take a closer look at mixup from an interpolation perspective and introduce AlignMix, a method that aligns two images in the feature space to allow for interpolation while preserving the locations of one set of features. This results in a situation where mixup retains the geometry of one image and the texture of the other, similar to style transfer. The authors also demonstrate that an autoencoder can still improve representation learning under mixup, without requiring the classifier to see decoded images. AlignMix outperforms current mixup methods on five different benchmarks.",1
"Many recent methods for unsupervised representation learning train models to be invariant to different ""views,"" or distorted versions of an input. However, designing these views requires considerable trial and error by human experts, hindering widespread adoption of unsupervised representation learning methods across domains and modalities. To address this, we propose viewmaker networks: generative models that learn to produce useful views from a given input. Viewmakers are stochastic bounded adversaries: they produce views by generating and then adding an $\ell_p$-bounded perturbation to the input, and are trained adversarially with respect to the main encoder network. Remarkably, when pretraining on CIFAR-10, our learned views enable comparable transfer accuracy to the well-tuned SimCLR augmentations -- despite not including transformations like cropping or color jitter. Furthermore, our learned views significantly outperform baseline augmentations on speech recordings (+9% points, on average) and wearable sensor data (+17% points). Viewmakers can also be combined with handcrafted views: they improve robustness to common image corruptions and can increase transfer performance in cases where handcrafted views are less explored. These results suggest that viewmakers may provide a path towards more general representation learning algorithms -- reducing the domain expertise and effort needed to pretrain on a much wider set of domains. Code is available at https://github.com/alextamkin/viewmaker.",0
"To enhance the adoption of unsupervised representation learning methods across domains and modalities, recent techniques aim to train models to be impervious to diverse ""views"" or distorted versions of inputs. However, crafting these views involves arduous trial and error by human experts, thus impeding widespread usage. To overcome this challenge, we propose viewmaker networks, which are generative models that learn to produce valuable views from a given input. Viewmakers function as stochastic bounded adversaries, creating views by adding an $\ell_p$-bounded perturbation to the input, and are adversarially trained regarding the primary encoder network. Surprisingly, our learned views result in comparable transfer accuracy to well-tuned SimCLR augmentations when pretraining on CIFAR-10, even without transformations such as cropping or color jitter. Furthermore, our learned views outperform baseline augmentations by a significant margin in speech recordings (+9% points, on average) and wearable sensor data (+17% points). Viewmakers may also enhance handcrafted views, making them more robust to common image corruptions and improving transfer performance in domains where handcrafted views are limited. Our findings suggest that viewmakers may pave the way for more general representation learning algorithms, reducing the domain expertise and effort required to pretrain models on a broader range of domains. The code is available at https://github.com/alextamkin/viewmaker.",1
"Learning embedding spaces of suitable geometry is critical for representation learning. In order for learned representations to be effective and efficient, it is ideal that the geometric inductive bias aligns well with the underlying structure of the data. In this paper, we propose Switch Spaces, a data-driven approach for learning representations in product space. Specifically, product spaces (or manifolds) are spaces of mixed curvature, i.e., a combination of multiple euclidean and non-euclidean (hyperbolic, spherical) manifolds. To this end, we introduce sparse gating mechanisms that learn to choose, combine and switch spaces, allowing them to be switchable depending on the input data with specialization. Additionally, the proposed method is also efficient and has a constant computational complexity regardless of the model size. Experiments on knowledge graph completion and item recommendations show that the proposed switch space achieves new state-of-the-art performances, outperforming pure product spaces and recently proposed task-specific models.",0
"It is essential to learn embedding spaces with a suitable geometry for representation learning. For the learned representations to be effective and efficient, it is ideal that the geometric inductive bias aligns well with the underlying data structure. In this study, we introduce Switch Spaces, a data-driven approach that uses product spaces to learn representations. Product spaces are mixed curvature spaces that combine multiple manifolds, including euclidean and non-euclidean (hyperbolic, spherical) manifolds. We propose sparse gating mechanisms that learn to choose, combine, and switch spaces to be switchable depending on the input data's specialization. Additionally, the proposed method is efficient and has a constant computational complexity, regardless of the model size. Our experiments on knowledge graph completion and item recommendations demonstrate that the proposed switch space achieves new state-of-the-art performances, outperforming pure product spaces and recently proposed task-specific models.",1
"In recent years, representation learning has become the research focus of the machine learning community. Large-scale pre-training neural networks have become the first step to realize general intelligence. The key to the success of neural networks lies in their abstract representation capabilities for data. Several learning fields are actually discussing how to learn representations and there lacks a unified perspective. We convert the representation learning problem under multiple tasks into a ranking problem, taking the ranking problem as a unified perspective, the representation learning under different tasks is solved by optimizing the approximate NDCG loss. Experiments under different learning tasks like classification, retrieval, multi-label learning, regression, self-supervised learning prove the superiority of approximate NDCG loss. Further, under the self-supervised learning task, the training data is transformed by data augmentation method to improve the performance of the approximate NDCG loss, which proves that the approximate NDCG loss can make full use of the information of the unsupervised training data.",0
"Representation learning has become a significant area of focus for the machine learning community in recent years. To achieve general intelligence, large-scale pre-training neural networks are essential as they possess abstract representation capabilities for data. However, there is no unified perspective on how to learn these representations, and various learning fields are exploring this problem. To address this, we propose a ranking-based approach to the representation learning problem across multiple tasks. By optimizing the approximate NDCG loss, we can solve representation learning under different tasks. Our experiments demonstrate that the approximate NDCG loss is superior for different learning tasks, including classification, retrieval, multi-label learning, regression, and self-supervised learning. Moreover, we show that data augmentation techniques can improve the performance of the approximate NDCG loss under the self-supervised learning task, indicating that this approach can make the best use of unsupervised training data.",1
"Gait recognition plays a vital role in human identification since gait is a unique biometric feature that can be perceived at a distance. Although existing gait recognition methods can learn gait features from gait sequences in different ways, the performance of gait recognition suffers from insufficient labeled data, especially in some practical scenarios associated with short gait sequences or various clothing styles. It is unpractical to label the numerous gait data. In this work, we propose a self-supervised gait recognition method, termed SelfGait, which takes advantage of the massive, diverse, unlabeled gait data as a pre-training process to improve the representation abilities of spatiotemporal backbones. Specifically, we employ the horizontal pyramid mapping (HPM) and micro-motion template builder (MTB) as our spatiotemporal backbones to capture the multi-scale spatiotemporal representations. Experiments on CASIA-B and OU-MVLP benchmark gait datasets demonstrate the effectiveness of the proposed SelfGait compared with four state-of-the-art gait recognition methods. The source code has been released at https://github.com/EchoItLiu/SelfGait.",0
"The identification of humans through gait recognition is crucial as it allows biometric features to be perceived at a distance. Despite various existing methods that can learn gait features, the performance of gait recognition is limited due to the lack of labeled data, particularly in scenarios where there are short gait sequences or different clothing styles. Labeling a large amount of gait data is impractical. This study introduces SelfGait, a self-supervised gait recognition method that utilizes massive and diverse unlabeled gait data for pre-training to enhance the representation abilities of spatiotemporal backbones. The study utilizes horizontal pyramid mapping (HPM) and micro-motion template builder (MTB) as spatiotemporal backbones to capture multi-scale spatiotemporal representations. Results from experiments on benchmark gait datasets CASIA-B and OU-MVLP demonstrate the effectiveness of SelfGait compared to four state-of-the-art gait recognition methods. The source code for SelfGait has been released at https://github.com/EchoItLiu/SelfGait.",1
"Driving in a dynamic, multi-agent, and complex urban environment is a difficult task requiring a complex decision-making policy. The learning of such a policy requires a state representation that can encode the entire environment. Mid-level representations that encode a vehicle's environment as images have become a popular choice. Still, they are quite high-dimensional, limiting their use in data-hungry approaches such as reinforcement learning. In this article, we propose to learn a low-dimensional and rich latent representation of the environment by leveraging the knowledge of relevant semantic factors. To do this, we train an encoder-decoder deep neural network to predict multiple application-relevant factors such as the trajectories of other agents and the ego car. We also propose a hazard signal in addition to the learned latent representation as input to a down-stream policy. We demonstrate that using the multi-head encoder-decoder neural network results in a more informative representation than a standard single-head model. In particular, the proposed representation learning and the hazard signal help reinforcement learning to learn faster, with increased performance and less data than baseline methods.",0
"The task of driving in a complex and dynamic urban environment is challenging and requires a decision-making policy that can encode the entire environment. Although mid-level representations that use images to encode a vehicle's environment have become popular, their high dimensionality limits their use in data-intensive methods such as reinforcement learning. This article proposes a method for learning a low-dimensional and rich latent representation of the environment by leveraging relevant semantic factors. An encoder-decoder deep neural network is trained to predict multiple application-relevant factors, including the trajectories of other agents and the ego car, while also incorporating a hazard signal as input to a downstream policy. The multi-head encoder-decoder neural network results in a more informative representation than a standard single-head model. The proposed representation learning and hazard signal also aid reinforcement learning, enabling it to learn faster and with increased performance while using less data than baseline methods.",1
"Learning discriminative image representations plays a vital role in long-tailed image classification because it can ease the classifier learning in imbalanced cases. Given the promising performance contrastive learning has shown recently in representation learning, in this work, we explore effective supervised contrastive learning strategies and tailor them to learn better image representations from imbalanced data in order to boost the classification accuracy thereon. Specifically, we propose a novel hybrid network structure being composed of a supervised contrastive loss to learn image representations and a cross-entropy loss to learn classifiers, where the learning is progressively transited from feature learning to the classifier learning to embody the idea that better features make better classifiers. We explore two variants of contrastive loss for feature learning, which vary in the forms but share a common idea of pulling the samples from the same class together in the normalized embedding space and pushing the samples from different classes apart. One of them is the recently proposed supervised contrastive (SC) loss, which is designed on top of the state-of-the-art unsupervised contrastive loss by incorporating positive samples from the same class. The other is a prototypical supervised contrastive (PSC) learning strategy which addresses the intensive memory consumption in standard SC loss and thus shows more promise under limited memory budget. Extensive experiments on three long-tailed classification datasets demonstrate the advantage of the proposed contrastive learning based hybrid networks in long-tailed classification.",0
"The acquisition of discriminative image representations is crucial for successful long-tailed image classification, as it can aid in the learning process for imbalanced cases. Given the success of contrastive learning in representation learning, we investigate the implementation of effective supervised contrastive learning strategies designed to improve image representation learning from imbalanced data and enhance subsequent classification accuracy. We propose a novel hybrid network structure that combines a supervised contrastive loss for feature learning with a cross-entropy loss for classifier learning. This structure facilitates the gradual transition from feature learning to classifier learning, reflecting the notion that superior features lead to improved classifiers. We explore two variants of contrastive loss for feature learning, both of which aim to bring samples from the same class closer together in the normalized embedding space and push those from different classes further apart. The first is the supervised contrastive (SC) loss, which builds on the state-of-the-art unsupervised contrastive loss by adding positive samples from the same class. The second, a prototypical supervised contrastive (PSC) learning strategy, addresses the high memory usage of standard SC loss and shows greater promise when operating within a limited memory budget. Through extensive experiments on three long-tailed classification datasets, we demonstrate the superiority of hybrid networks that incorporate contrastive learning for long-tailed classification tasks.",1
"We introduce a novel representation learning method to disentangle pose-dependent as well as view-dependent factors from 2D human poses. The method trains a network using cross-view mutual information maximization (CV-MIM) which maximizes mutual information of the same pose performed from different viewpoints in a contrastive learning manner. We further propose two regularization terms to ensure disentanglement and smoothness of the learned representations. The resulting pose representations can be used for cross-view action recognition. To evaluate the power of the learned representations, in addition to the conventional fully-supervised action recognition settings, we introduce a novel task called single-shot cross-view action recognition. This task trains models with actions from only one single viewpoint while models are evaluated on poses captured from all possible viewpoints. We evaluate the learned representations on standard benchmarks for action recognition, and show that (i) CV-MIM performs competitively compared with the state-of-the-art models in the fully-supervised scenarios; (ii) CV-MIM outperforms other competing methods by a large margin in the single-shot cross-view setting; (iii) and the learned representations can significantly boost the performance when reducing the amount of supervised training data. Our code is made publicly available at https://github.com/google-research/google-research/tree/master/poem",0
"We have developed a new approach to learn representations that disentangle pose-dependent and view-dependent factors from 2D human poses. Our method involves training a network through cross-view mutual information maximization (CV-MIM), which maximizes mutual information of the same pose performed from different viewpoints in a contrastive learning fashion. Additionally, we introduce two regularization terms to ensure the smoothness and disentanglement of the learned representations. These pose representations can be utilized for cross-view action recognition. To assess the effectiveness of the learned representations, we introduce a new task called single-shot cross-view action recognition, where models are trained with actions from only one viewpoint and evaluated on poses captured from all possible viewpoints. Our evaluations on standard benchmarks for action recognition demonstrate that our approach performs competitively with state-of-the-art models in fully-supervised scenarios, outperforms other methods by a large margin in the single-shot cross-view setting, and significantly improves performance when reducing the amount of supervised training data. We have made our code publicly available at https://github.com/google-research/google-research/tree/master/poem.",1
"Continual learning is known for suffering from catastrophic forgetting, a phenomenon where earlier learned concepts are forgotten at the expense of more recent samples. In this work, we challenge the assumption that continual learning is inevitably associated with catastrophic forgetting by presenting a set of tasks that surprisingly do not suffer from catastrophic forgetting when learned continually. The robustness of these tasks leads to the potential of having a proxy representation learning task for continual classification. We further introduce a novel yet simple algorithm, YASS that achieves state-of-the-art performance in the class-incremental categorization learning task and provide an insight into the benefit of learning the representation continuously. Finally, we present converging evidence on the forgetting dynamics of representation learning in continual models. The codebase, dataset, and pre-trained models released with this article can be found at https://github.com/rehg-lab/CLRec.",0
"The phenomenon of catastrophic forgetting is a well-known issue in continual learning, where earlier concepts are forgotten in favor of more recent ones. However, our work challenges the belief that continual learning inevitably leads to catastrophic forgetting by presenting a set of tasks that do not suffer from this problem when learned continuously. These tasks are robust and offer the potential for a proxy representation learning task for continual classification. We introduce a new algorithm called YASS, which achieves state-of-the-art performance in the class-incremental categorization learning task and provides valuable insight into the benefits of continuous representation learning. Furthermore, we provide evidence of the forgetting dynamics of representation learning in continual models. To access the codebase, dataset, and pre-trained models, please visit https://github.com/rehg-lab/CLRec.",1
"We present a two-stage framework for deep one-class classification. We first learn self-supervised representations from one-class data, and then build one-class classifiers on learned representations. The framework not only allows to learn better representations, but also permits building one-class classifiers that are faithful to the target task. We argue that classifiers inspired by the statistical perspective in generative or discriminative models are more effective than existing approaches, such as a normality score from a surrogate classifier. We thoroughly evaluate different self-supervised representation learning algorithms under the proposed framework for one-class classification. Moreover, we present a novel distribution-augmented contrastive learning that extends training distributions via data augmentation to obstruct the uniformity of contrastive representations. In experiments, we demonstrate state-of-the-art performance on visual domain one-class classification benchmarks, including novelty and anomaly detection. Finally, we present visual explanations, confirming that the decision-making process of deep one-class classifiers is intuitive to humans. The code is available at https://github.com/google-research/deep_representation_one_class.",0
"Our approach to deep one-class classification involves a two-stage framework. Initially, we learn self-supervised representations from one-class data, followed by the creation of one-class classifiers based on the learned representations. This framework not only improves representation learning but also allows the development of one-class classifiers that accurately reflect the target task. Our argument is that classifiers based on generative or discriminative models' statistical perspective are more effective than current methods, such as using a normality score from a surrogate classifier. We perform a thorough evaluation of various self-supervised representation learning algorithms using our proposed framework for one-class classification. Additionally, we introduce distribution-augmented contrastive learning, which expands training distributions via data augmentation to hinder the uniformity of contrastive representations. Our experiments demonstrate state-of-the-art results on visual domain one-class classification benchmarks, including novelty and anomaly detection. Finally, we provide visual explanations that confirm the deep one-class classifiers' decision-making process is intuitive to humans. Interested parties can access the code at https://github.com/google-research/deep_representation_one_class.",1
"Cross-resolution face recognition (CRFR), which is important in intelligent surveillance and biometric forensics, refers to the problem of matching a low-resolution (LR) probe face image against high-resolution (HR) gallery face images. Existing shallow learning-based and deep learning-based methods focus on mapping the HR-LR face pairs into a joint feature space where the resolution discrepancy is mitigated. However, little works consider how to extract and utilize the intermediate discriminative features from the noisy LR query faces to further mitigate the resolution discrepancy due to the resolution limitations. In this study, we desire to fully exploit the multi-level deep convolutional neural network (CNN) feature set for robust CRFR. In particular, our contributions are threefold. (i) To learn more robust and discriminative features, we desire to adaptively fuse the contextual features from different layers. (ii) To fully exploit these contextual features, we design a feature set-based representation learning (FSRL) scheme to collaboratively represent the hierarchical features for more accurate recognition. Moreover, FSRL utilizes the primitive form of feature maps to keep the latent structural information, especially in noisy cases. (iii) To further promote the recognition performance, we desire to fuse the hierarchical recognition outputs from different stages. Meanwhile, the discriminability from different scales can also be fully integrated. By exploiting these advantages, the efficiency of the proposed method can be delivered. Experimental results on several face datasets have verified the superiority of the presented algorithm to the other competitive CRFR approaches.",0
"The problem of matching a low-resolution (LR) probe face image with high-resolution (HR) gallery face images, known as cross-resolution face recognition (CRFR), is crucial for intelligent surveillance and biometric forensics. Current methods use shallow or deep learning-based techniques to map HR-LR face pairs into a joint feature space to mitigate resolution discrepancies. However, few methods consider extracting and utilizing intermediate discriminative features from the noisy LR query faces to further mitigate resolution limitations. This study aims to exploit the multi-level deep convolutional neural network (CNN) feature set for robust CRFR. The proposed method has three contributions: (i) adaptively fusing contextual features from different layers to learn more robust and discriminative features, (ii) using a feature set-based representation learning (FSRL) scheme to collaboratively represent the hierarchical features for more accurate recognition, and (iii) fusing hierarchical recognition outputs from different stages to promote recognition performance and fully integrate discriminability from different scales. The proposed method outperforms other CRFR approaches on several face datasets, delivering efficiency and superior results.",1
"In this paper, we look at the problem of few-shot classification that aims to learn a classifier for previously unseen classes and domains from few labeled samples. Recent methods use adaptation networks for aligning their features to new domains or select the relevant features from multiple domain-specific feature extractors. In this work, we propose to learn a single set of universal deep representations by distilling knowledge of multiple separately trained networks after co-aligning their features with the help of adapters and centered kernel alignment. We show that the universal representations can be further refined for previously unseen domains by an efficient adaptation step in a similar spirit to distance learning methods. We rigorously evaluate our model in the recent Meta-Dataset benchmark and demonstrate that it significantly outperforms the previous methods while being more efficient. Our code will be available at https://github.com/VICO-UoE/URL.",0
"This paper addresses the issue of few-shot classification, which involves developing a classifier for new classes and domains using only a limited number of labeled samples. Current approaches use adaptation networks to align features to new domains or select relevant features from multiple domain-specific extractors. Our proposal is to learn a universal set of deep representations by distilling knowledge from multiple separately trained networks. This is achieved by co-aligning features using adapters and centered kernel alignment. The universal representations can then be refined for previously unseen domains through an efficient adaptation step. This method was rigorously evaluated in the Meta-Dataset benchmark and outperformed previous methods while being more efficient. Our code is available at https://github.com/VICO-UoE/URL.",1
"Local discriminative representation is needed in many medical image analysis tasks such as identifying sub-types of lesion or segmenting detailed components of anatomical structures. However, the commonly applied supervised representation learning methods require a large amount of annotated data, and unsupervised discriminative representation learning distinguishes different images by learning a global feature, both of which are not suitable for localized medical image analysis tasks. In order to avoid the limitations of these two methods, we introduce local discrimination into unsupervised representation learning in this work. The model contains two branches: one is an embedding branch which learns an embedding function to disperse dissimilar pixels over a low-dimensional hypersphere; and the other is a clustering branch which learns a clustering function to classify similar pixels into the same cluster. These two branches are trained simultaneously in a mutually beneficial pattern, and the learnt local discriminative representations are able to well measure the similarity of local image regions. These representations can be transferred to enhance various downstream tasks. Meanwhile, they can also be applied to cluster anatomical structures from unlabeled medical images under the guidance of topological priors from simulation or other structures with similar topological characteristics. The effectiveness and usefulness of the proposed method are demonstrated by enhancing various downstream tasks and clustering anatomical structures in retinal images and chest X-ray images.",0
"In medical image analysis, the use of local discriminative representation is necessary for tasks like detecting lesion sub-types or segmenting intricate anatomical structures. However, the commonly employed supervised representation learning methods require a considerable amount of labeled data, and unsupervised discriminative representation learning identifies different images by learning a global feature, which is not suitable for localized medical image analysis tasks. To overcome these limitations, we present a technique that incorporates local discrimination into unsupervised representation learning. Our model includes two branches: one learns an embedding function to disperse dissimilar pixels over a low-dimensional hypersphere, while the other learns a clustering function to classify similar pixels into the same cluster. These two branches are trained simultaneously and generate local discriminative representations, which can effectively measure the similarity of local image regions. These representations can be utilized to enhance various downstream tasks and cluster anatomical structures from unlabeled medical images guided by topological priors from simulation or other structures with similar topological characteristics. We demonstrate the effectiveness and usefulness of our approach by enhancing various downstream tasks and clustering anatomical structures in retinal images and chest X-ray images.",1
"Leveraging domain knowledge including fingerprints and functional groups in molecular representation learning is crucial for chemical property prediction and drug discovery. When modeling the relation between graph structure and molecular properties implicitly, existing works can hardly capture structural or property changes and complex structure, with much smaller atom vocabulary and highly frequent atoms. In this paper, we propose the Contrastive Knowledge-aware GNN (CKGNN) for self-supervised molecular representation learning to fuse domain knowledge into molecular graph representation. We explicitly encode domain knowledge via knowledge-aware molecular encoder under the contrastive learning framework, ensuring that the generated molecular embeddings equipped with chemical domain knowledge to distinguish molecules with similar chemical formula but dissimilar functions. Extensive experiments on 8 public datasets demonstrate the effectiveness of our model with a 6\% absolute improvement on average against strong competitors. Ablation study and further investigation also verify the best of both worlds: incorporation of chemical domain knowledge into self-supervised learning.",0
"The incorporation of domain knowledge, such as fingerprints and functional groups, is critical for effective molecular representation learning, which is essential for chemical property prediction and drug discovery. However, existing approaches that model the relationship between graph structure and molecular properties struggle to capture complex structures and property changes due to limited atom vocabulary and high frequency of atoms. To address this issue, we propose a self-supervised molecular representation learning method called Contrastive Knowledge-aware GNN (CKGNN), which integrates domain knowledge into molecular graph representation. Our method utilizes a knowledge-aware molecular encoder, which explicitly encodes domain knowledge within a contrastive learning framework. This ensures that the molecular embeddings generated by our method are equipped with chemical domain knowledge, allowing them to distinguish molecules with similar chemical formulas but dissimilar functions. Our experiments on eight public datasets demonstrate that our model outperforms strong competitors by an average of 6% in absolute improvement. Ablation studies and further investigations confirm that our approach is the best of both worlds, as it incorporates chemical domain knowledge into self-supervised learning.",1
"Mixup-based data augmentation has achieved great success as regularizer for deep neural networks. However, existing mixup methods require explicitly designed mixup policies. In this paper, we present a flexible, general Automatic Mixup (AutoMix) framework which utilizes discriminative features to learn a sample mixing policy adaptively. We regard mixup as a pretext task and split it into two sub-problems: mixed samples generation and mixup classification. To this end, we design a lightweight mix block to generate synthetic samples based on feature maps and mix labels. Since the two sub-problems are in the nature of Expectation-Maximization (EM), we also propose a momentum training pipeline to optimize the mixup process and mixup classification process alternatively in an end-to-end fashion. Extensive experiments on six popular classification benchmarks show that AutoMix consistently outperforms other leading mixup methods and improves generalization abilities to downstream tasks. We hope AutoMix will motivate the community to rethink the role of mixup in representation learning. The code will be released soon.",0
"The use of mixup-based data augmentation has proven to be a successful regularizer for deep neural networks. However, current mixup methods necessitate specifically designed mixup policies. In this study, we introduce a versatile and comprehensive Automatic Mixup (AutoMix) framework that leverages discriminative features to adaptively learn a sample mixing policy. Mixup is treated as a pretext task and separated into two sub-problems: mixed samples generation and mixup classification. Accordingly, we devise a lightweight mix block that utilizes feature maps and mix labels to produce synthetic samples. As the two sub-problems are EM-based, we propose a momentum training pipeline to optimize the mixup process and mixup classification process alternately in an end-to-end fashion. Through extensive experiments on six prominent classification benchmarks, we demonstrate that AutoMix consistently outperforms other leading mixup methods and enhances generalization abilities in downstream tasks. We anticipate that AutoMix will prompt the community to reconsider the role of mixup in representation learning, and the code will soon be made available.",1
"Unsupervised representation learning is an important challenge in computer vision, with self-supervised learning methods recently closing the gap to supervised representation learning. An important ingredient in high-performing self-supervised methods is the use of data augmentation by training models to place different augmented views of the same image nearby in embedding space. However, commonly used augmentation pipelines treat images holistically, disregarding the semantic relevance of parts of an image-e.g. a subject vs. a background-which can lead to the learning of spurious correlations. Our work addresses this problem by investigating a class of simple, yet highly effective ""background augmentations"", which encourage models to focus on semantically-relevant content by discouraging them from focusing on image backgrounds. Background augmentations lead to substantial improvements (+1-2% on ImageNet-1k) in performance across a spectrum of state-of-the art self-supervised methods (MoCov2, BYOL, SwAV) on a variety of tasks, allowing us to reach within 0.3% of supervised performance. We also demonstrate that background augmentations improve robustness to a number of out of distribution settings, including natural adversarial examples, the backgrounds challenge, adversarial attacks, and ReaL ImageNet.",0
"The challenge of unsupervised representation learning in computer vision is significant, but recent self-supervised learning methods have made progress in catching up to supervised representation learning. Effective self-supervised methods rely on data augmentation by training models to recognize different views of the same image that are situated closely in embedding space. However, many commonly used augmentation techniques treat images holistically, without considering the semantic relevance of image parts such as subjects versus backgrounds, which can result in spurious correlations being learned. Our research addresses this issue by exploring a type of uncomplicated yet highly successful ""background augmentation"" that encourages models to concentrate on semantically significant content by discouraging attention on image backgrounds. Background augmentations result in considerable increases in performance (+1-2% on ImageNet-1k) across numerous cutting-edge self-supervised methods (MoCov2, BYOL, SwAV) on various tasks, bringing us to within 0.3% of supervised performance. We also demonstrate that background augmentations enhance resilience in several out-of-distribution settings, including natural adversarial examples, the backgrounds challenge, adversarial attacks, and ReaL ImageNet.",1
"Graph autoencoders (GAEs) are powerful tools in representation learning for graph embedding. However, the performance of GAEs is very dependent on the quality of the graph structure, i.e., of the adjacency matrix. In other words, GAEs would perform poorly when the adjacency matrix is incomplete or be disturbed. In this paper, two novel unsupervised graph embedding methods, unsupervised graph embedding via adaptive graph learning (BAGE) and unsupervised graph embedding via variational adaptive graph learning (VBAGE) are proposed. The proposed methods expand the application range of GAEs on graph embedding, i.e, on the general datasets without graph structure. Meanwhile, the adaptive learning mechanism can initialize the adjacency matrix without be affected by the parameter. Besides that, the latent representations are embedded in the laplacian graph structure to preserve the topology structure of the graph in the vector space. Moreover, the adjacency matrix can be self-learned for better embedding performance when the original graph structure is incomplete. With adaptive learning, the proposed method is much more robust to the graph structure. Experimental studies on several datasets validate our design and demonstrate that our methods outperform baselines by a wide margin in node clustering, node classification, and graph visualization tasks.",0
"The use of Graph autoencoders (GAEs) for graph embedding in representation learning is a powerful tool. However, the quality of the adjacency matrix plays a crucial role in determining the performance of GAEs. When the adjacency matrix is incomplete or disturbed, the GAEs perform poorly. This paper proposes two novel unsupervised graph embedding methods, namely, unsupervised graph embedding via adaptive graph learning (BAGE) and unsupervised graph embedding via variational adaptive graph learning (VBAGE), which expand the application range of GAEs on graph embedding. The proposed methods use an adaptive learning mechanism that can initialize the adjacency matrix without being affected by the parameter. Additionally, the latent representations are embedded in the laplacian graph structure to preserve the topology structure of the graph in the vector space. The adjacency matrix can also be self-learned for better embedding performance when the original graph structure is incomplete. With adaptive learning, the proposed method is more robust to the graph structure. The experimental studies conducted on several datasets validate the design and demonstrate that the proposed methods outperform the baselines by a wide margin in node clustering, node classification, and graph visualization tasks.",1
"Health management is getting increasing attention all over the world. However, existing health management mainly relies on hospital examination and treatment, which are complicated and untimely. The emerging of mobile devices provides the possibility to manage people's health status in a convenient and instant way. Estimation of health status can be achieved with various kinds of data streams continuously collected from wearable sensors. However, these data streams are multi-source and heterogeneous, containing complex temporal structures with local contextual and global temporal aspects, which makes the feature learning and data joint utilization challenging. We propose to model the behavior-related multi-source data streams with a local-global graph, which contains multiple local context sub-graphs to learn short term local context information with heterogeneous graph neural networks and a global temporal sub-graph to learn long term dependency with self-attention networks. Then health status is predicted based on the structure-aware representation learned from the local-global behavior graph. We take experiments on StudentLife dataset, and extensive results demonstrate the effectiveness of our proposed model.",0
"The topic of healthcare management is receiving increasing attention worldwide. However, the current approach to healthcare management is predominantly reliant on hospital exams and treatments, which can be complicated and untimely. Fortunately, the emergence of mobile devices has opened up the possibility of managing people's health status conveniently and instantaneously. This can be achieved through the continuous collection of various kinds of data streams from wearable sensors to estimate health status. Nevertheless, these data streams are multi-source and heterogeneous, with complex temporal structures that contain both local contextual and global temporal aspects. As a result, feature learning and data joint utilization can be challenging. To overcome this challenge, we propose modeling the behavior-related multi-source data streams with a local-global graph, which includes multiple sub-graphs of local context to learn short-term local context information using heterogeneous graph neural networks. We also incorporate a global temporal sub-graph to learn long-term dependency with self-attention networks. This approach enables us to predict health status based on the structure-aware representation learned from the local-global behavior graph. Our experimental results on the StudentLife dataset demonstrate the effectiveness of our proposed model.",1
"A recent line of work in the machine learning community addresses the problem of predicting high-dimensional spatiotemporal phenomena by leveraging specific tools from the differential equations theory. Following this direction, we propose in this article a novel and general paradigm for this task based on a resolution method for partial differential equations: the separation of variables. This inspiration allows us to introduce a dynamical interpretation of spatiotemporal disentanglement. It induces a principled model based on learning disentangled spatial and temporal representations of a phenomenon to accurately predict future observations. We experimentally demonstrate the performance and broad applicability of our method against prior state-of-the-art models on physical and synthetic video datasets.",0
The machine learning community has recently been studying how to predict complex spatiotemporal phenomena using tools from differential equations theory. Our article proposes a new approach to this problem using a resolution method called separation of variables. This method allows us to interpret spatiotemporal disentanglement in a dynamic way and create a model that accurately predicts future observations by learning disentangled spatial and temporal representations. We tested our method on physical and synthetic video datasets and found that it outperforms previous state-of-the-art models.,1
"Unsupervised Domain Adaptive (UDA) person re-identification (ReID) aims at adapting the model trained on a labeled source-domain dataset to a target-domain dataset without any further annotations. Most successful UDA-ReID approaches combine clustering-based pseudo-label prediction with representation learning and perform the two steps in an alternating fashion. However, offline interaction between these two steps may allow noisy pseudo labels to substantially hinder the capability of the model. In this paper, we propose a Group-aware Label Transfer (GLT) algorithm, which enables the online interaction and mutual promotion of pseudo-label prediction and representation learning. Specifically, a label transfer algorithm simultaneously uses pseudo labels to train the data while refining the pseudo labels as an online clustering algorithm. It treats the online label refinery problem as an optimal transport problem, which explores the minimum cost for assigning M samples to N pseudo labels. More importantly, we introduce a group-aware strategy to assign implicit attribute group IDs to samples. The combination of the online label refining algorithm and the group-aware strategy can better correct the noisy pseudo label in an online fashion and narrow down the search space of the target identity. The effectiveness of the proposed GLT is demonstrated by the experimental results (Rank-1 accuracy) for Market1501$\to$DukeMTMC (82.0\%) and DukeMTMC$\to$Market1501 (92.2\%), remarkably closing the gap between unsupervised and supervised performance on person re-identification.",0
"The goal of Unsupervised Domain Adaptive (UDA) person re-identification (ReID) is to adapt a model trained on a labeled source-domain dataset to a target-domain dataset without additional annotations. Successful approaches combine clustering-based pseudo-label prediction with representation learning, but the offline interaction between these steps can result in noisy pseudo labels that limit the model's effectiveness. To address this issue, we propose a Group-aware Label Transfer (GLT) algorithm that enables online interaction and mutual promotion of pseudo-label prediction and representation learning. GLT uses pseudo labels to train the data and refines the labels as an online clustering algorithm. It treats the online label refining problem as an optimal transport problem, assigning M samples to N pseudo labels at the minimum cost. Additionally, we assign implicit attribute group IDs to samples using a group-aware strategy. The GLT algorithm corrects noisy pseudo labels in an online fashion and narrows down the search space of the target identity. Experimental results demonstrate the effectiveness of GLT, achieving Rank-1 accuracy of 82.0% for Market1501$\to$DukeMTMC and 92.2% for DukeMTMC$\to$Market1501, significantly closing the gap between unsupervised and supervised performance on person re-identification.",1
"Learning disentanglement aims at finding a low dimensional representation which consists of multiple explanatory and generative factors of the observational data. The framework of variational autoencoder (VAE) is commonly used to disentangle independent factors from observations. However, in real scenarios, factors with semantics are not necessarily independent. Instead, there might be an underlying causal structure which renders these factors dependent. We thus propose a new VAE based framework named CausalVAE, which includes a Causal Layer to transform independent exogenous factors into causal endogenous ones that correspond to causally related concepts in data. We further analyze the model identifiabitily, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments are conducted on various datasets, including synthetic and real word benchmark CelebA. Results show that the causal representations learned by CausalVAE are semantically interpretable, and their causal relationship as a Directed Acyclic Graph (DAG) is identified with good accuracy. Furthermore, we demonstrate that the proposed CausalVAE model is able to generate counterfactual data through ""do-operation"" to the causal factors.",0
"The goal of learning disentanglement is to uncover a low-dimensional representation that explains the various factors that generate observational data. The widely used method for disentangling independent factors from observations is the variational autoencoder (VAE) framework. However, in real-life situations, factors with meaning are not necessarily independent, but rather, they may be related through an underlying causal structure. To address this issue, we propose a new VAE-based framework called CausalVAE, which includes a Causal Layer that transforms independent exogenous factors into causal endogenous factors that correspond to causally related concepts in data. We also analyze the model's identifiability and demonstrate that the proposed model can accurately recover the true model from observations to some extent. We conduct experiments on various datasets, including synthetic and real-world benchmark CelebA, and find that the causal representations learned by CausalVAE are semantically interpretable, and their causal relationships are accurately identified as a Directed Acyclic Graph (DAG). Additionally, we show that the proposed CausalVAE model can generate counterfactual data by manipulating the causal factors.",1
"Advancements in machine learning algorithms have had a beneficial impact on representation learning, classification, and prediction models built using electronic health record (EHR) data. Effort has been put both on increasing models' overall performance as well as improving their interpretability, particularly regarding the decision-making process. In this study, we present a temporal deep learning model to perform bidirectional representation learning on EHR sequences with a transformer architecture to predict future diagnosis of depression. This model is able to aggregate five heterogenous and high-dimensional data sources from the EHR and process them in a temporal manner for chronic disease prediction at various prediction windows. We applied the current trend of pretraining and fine-tuning on EHR data to outperform the current state-of-the-art in chronic disease prediction, and to demonstrate the underlying relation between EHR codes in the sequence. The model generated the highest increases of precision-recall area under the curve (PRAUC) from 0.70 to 0.76 in depression prediction compared to the best baseline model. Furthermore, the self-attention weights in each sequence quantitatively demonstrated the inner relationship between various codes, which improved the model's interpretability. These results demonstrate the model's ability to utilize heterogeneous EHR data to predict depression while achieving high accuracy and interpretability, which may facilitate constructing clinical decision support systems in the future for chronic disease screening and early detection.",0
"Machine learning algorithms have positively impacted the development of representation learning, classification, and prediction models utilizing electronic health record (EHR) data. To improve performance and interpretability, a temporal deep learning model was developed using a transformer architecture to predict future diagnoses of depression. This model effectively combines and processes five distinct and complex EHR data sources in a temporal manner, resulting in superior chronic disease prediction at different time frames. By employing pretraining and fine-tuning techniques, the model outperformed current state-of-the-art methods and demonstrated the relationship between EHR codes in the sequence, improving its interpretability. The model achieved a significant increase in precision-recall area under the curve (PRAUC) from 0.70 to 0.76 for depression prediction. Its ability to utilize heterogeneous EHR data for accurate and interpretable prediction may facilitate the development of clinical decision support systems for chronic disease screening and early detection.",1
"As a subset of unsupervised representation learning, self-supervised representation learning adopts self-defined signals as supervision and uses the learned representation for downstream tasks, such as object detection and image captioning. Many proposed approaches for self-supervised learning follow naturally a multi-view perspective, where the input (e.g., original images) and the self-supervised signals (e.g., augmented images) can be seen as two redundant views of the data. Building from this multi-view perspective, this paper provides an information-theoretical framework to better understand the properties that encourage successful self-supervised learning. Specifically, we demonstrate that self-supervised learned representations can extract task-relevant information and discard task-irrelevant information. Our theoretical framework paves the way to a larger space of self-supervised learning objective design. In particular, we propose a composite objective that bridges the gap between prior contrastive and predictive learning objectives, and introduce an additional objective term to discard task-irrelevant information. To verify our analysis, we conduct controlled experiments to evaluate the impact of the composite objectives. We also explore our framework's empirical generalization beyond the multi-view perspective, where the cross-view redundancy may not be clearly observed.",0
"Self-supervised representation learning is a type of unsupervised representation learning that utilizes self-defined signals for supervision and applies the learned representation to downstream tasks, such as image captioning and object detection. Many self-supervised learning methods adopt a multi-view perspective, where the input and self-supervised signals can be viewed as two redundant views of the data. This article presents an information-theoretical framework that helps to understand the properties that contribute to successful self-supervised learning. The framework demonstrates that self-supervised learned representations can extract task-relevant information while discarding task-irrelevant information. The article proposes a composite objective that bridges the gap between prior contrastive and predictive learning objectives, and introduces an additional objective term to eliminate task-irrelevant information. To validate the analysis, the article conducts controlled experiments to evaluate the impact of the composite objectives. The article also explores the framework's empirical generalization beyond the multi-view perspective, where cross-view redundancy may not be readily apparent.",1
"We ask the following question: what training information is required to design an effective outlier/out-of-distribution (OOD) detector, i.e., detecting samples that lie far away from the training distribution? Since unlabeled data is easily accessible for many applications, the most compelling approach is to develop detectors based on only unlabeled in-distribution data. However, we observe that most existing detectors based on unlabeled data perform poorly, often equivalent to a random prediction. In contrast, existing state-of-the-art OOD detectors achieve impressive performance but require access to fine-grained data labels for supervised training. We propose SSD, an outlier detector based on only unlabeled in-distribution data. We use self-supervised representation learning followed by a Mahalanobis distance based detection in the feature space. We demonstrate that SSD outperforms most existing detectors based on unlabeled data by a large margin. Additionally, SSD even achieves performance on par, and sometimes even better, with supervised training based detectors. Finally, we expand our detection framework with two key extensions. First, we formulate few-shot OOD detection, in which the detector has access to only one to five samples from each class of the targeted OOD dataset. Second, we extend our framework to incorporate training data labels, if available. We find that our novel detection framework based on SSD displays enhanced performance with these extensions, and achieves state-of-the-art performance. Our code is publicly available at https://github.com/inspire-group/SSD.",0
"The main inquiry is what type of training information is necessary to develop a successful detector for outlying or out-of-distribution (OOD) samples, meaning samples that vary greatly from the distribution of the training set. Although obtaining unlabeled data is simple for various applications, creating detectors exclusively based on this data is not effective. Research has shown that most current detectors relying on unlabeled data perform poorly, often giving random predictions. In comparison, cutting-edge OOD detectors display impressive results but require access to detailed data labels for supervised training. In response, we suggest SSD, an outlier detector that relies solely on unlabeled in-distribution data. We utilize self-supervised representation learning followed by detection based on Mahalanobis distance in the feature space. Our findings show that SSD outperforms most current detectors relying on unlabeled data by a significant amount. Additionally, SSD matches or surpasses the performance of supervised training-based detectors. We also extend our detection framework by introducing two critical enhancements. First, we formulate few-shot OOD detection, which allows the detector to access one to five samples from each class in the targeted OOD dataset. Second, we integrate training data labels, whenever available. Our research demonstrates that our novel detection framework based on SSD exhibits superior performance with these extensions, achieving state-of-the-art results. Our code is openly available at https://github.com/inspire-group/SSD.",1
"We propose methods to strengthen the invariance properties of representations obtained by contrastive learning. While existing approaches implicitly induce a degree of invariance as representations are learned, we look to more directly enforce invariance in the encoding process. To this end, we first introduce a training objective for contrastive learning that uses a novel regularizer to control how the representation changes under transformation. We show that representations trained with this objective perform better on downstream tasks and are more robust to the introduction of nuisance transformations at test time. Second, we propose a change to how test time representations are generated by introducing a feature averaging approach that combines encodings from multiple transformations of the original input, finding that this leads to across the board performance gains. Finally, we introduce the novel Spirograph dataset to explore our ideas in the context of a differentiable generative process with multiple downstream tasks, showing that our techniques for learning invariance are highly beneficial.",0
"Our aim is to enhance the invariance characteristics of representations generated through contrastive learning. Although current methods do indirectly foster some level of invariance during representation learning, we aim to directly enforce invariance during encoding. For this purpose, we propose a new training objective for contrastive learning, which employs a unique regularizer to regulate the transformation of the representation. Our experiments demonstrate that the representations trained using this objective perform better on downstream tasks and are more resilient to nuisance transformations during testing. Additionally, we introduce a feature averaging approach to generate test time representations by blending encodings generated from various transformations of the original input. This approach leads to improved performance overall. Lastly, we use the differentiable generative process of the Spirograph dataset to explore our ideas and demonstrate that our techniques for learning invariance are exceedingly beneficial.",1
"In this work, we study the phenomenon of catastrophic forgetting in the graph representation learning scenario. The primary objective of the analysis is to understand whether classical continual learning techniques for flat and sequential data have a tangible impact on performances when applied to graph data. To do so, we experiment with a structure-agnostic model and a deep graph network in a robust and controlled environment on three different datasets. The benchmark is complemented by an investigation on the effect of structure-preserving regularization techniques on catastrophic forgetting. We find that replay is the most effective strategy in so far, which also benefits the most from the use of regularization. Our findings suggest interesting future research at the intersection of the continual and graph representation learning fields. Finally, we provide researchers with a flexible software framework to reproduce our results and carry out further experiments.",0
"The focus of our research is on catastrophic forgetting in the context of graph representation learning. We aim to determine whether traditional continual learning methods for flat and sequential data can improve performance when applied to graphs. Our experimentation involves using a structure-agnostic model and a deep graph network on three different datasets in a controlled environment. We also examine the impact of structure-preserving regularization techniques on catastrophic forgetting. Our results indicate that replay is the most effective strategy, especially when combined with regularization. These findings suggest exciting opportunities for future research that bridges the continual and graph representation learning fields. Additionally, we provide a flexible software framework to enable researchers to reproduce our results and conduct further experiments.",1
"In this paper, we propose a novel self-supervised representation learning method, Self-EMD, for object detection. Our method directly trained on unlabeled non-iconic image dataset like COCO, instead of commonly used iconic-object image dataset like ImageNet. We keep the convolutional feature maps as the image embedding to preserve spatial structures and adopt Earth Mover's Distance (EMD) to compute the similarity between two embeddings. Our Faster R-CNN (ResNet50-FPN) baseline achieves 39.8% mAP on COCO, which is on par with the state of the art self-supervised methods pre-trained on ImageNet. More importantly, it can be further improved to 40.4% mAP with more unlabeled images, showing its great potential for leveraging more easily obtained unlabeled data. Code will be made available.",0
"This paper introduces a new approach to self-supervised representation learning called Self-EMD, which is designed for object detection. Unlike traditional methods that use iconic-object image datasets like ImageNet, Self-EMD trains directly on unlabeled non-iconic image datasets such as COCO. The convolutional feature maps are retained as the image embedding to preserve spatial structures, and Earth Mover's Distance (EMD) is used to calculate the similarity between two embeddings. The Faster R-CNN (ResNet50-FPN) baseline achieves a 39.8% mAP on COCO, which is comparable to the best self-supervised methods trained on ImageNet. With more unlabeled images, the performance can be improved to 40.4% mAP, indicating its potential for utilizing more readily available unlabeled data. The code will be accessible.",1
"Representation learning methods for heterogeneous networks produce a low-dimensional vector embedding for each node that is typically fixed for all tasks involving the node. Many of the existing methods focus on obtaining a static vector representation for a node in a way that is agnostic to the downstream application where it is being used. In practice, however, downstream tasks such as link prediction require specific contextual information that can be extracted from the subgraphs related to the nodes provided as input to the task. To tackle this challenge, we develop SLiCE, a framework bridging static representation learning methods using global information from the entire graph with localized attention driven mechanisms to learn contextual node representations. We first pre-train our model in a self-supervised manner by introducing higher-order semantic associations and masking nodes, and then fine-tune our model for a specific link prediction task. Instead of training node representations by aggregating information from all semantic neighbors connected via metapaths, we automatically learn the composition of different metapaths that characterize the context for a specific task without the need for any pre-defined metapaths. SLiCE significantly outperforms both static and contextual embedding learning methods on several publicly available benchmark network datasets. We also interpret the semantic association matrix and provide its utility and relevance in making successful link predictions between heterogeneous nodes in the network.",0
"Methods for representation learning in heterogeneous networks generate a low-dimensional vector representation for each node, which typically remains unchanged across all tasks. Most existing techniques aim to obtain a stable vector representation for nodes that is independent of their downstream application. However, tasks such as link prediction require specific contextual information that can be extracted from the subgraphs related to the input nodes. To address this issue, we propose SLiCE, a framework that combines static representation learning methods with localized attention mechanisms to learn contextual node representations. Our approach involves pre-training the model in a self-supervised manner by masking nodes and introducing higher-order semantic associations, followed by fine-tuning for a specific link prediction task. Our method automatically learns the composition of different metapaths that characterizes the context for a specific task without the need for pre-defined metapaths. SLiCE performs significantly better than both static and contextual embedding learning methods on various publicly available benchmark network datasets. We also interpret the semantic association matrix and demonstrate its usefulness in making successful link predictions between heterogeneous nodes in the network.",1
"Source code (Context) and its parsed abstract syntax tree (AST; Structure) are two complementary representations of the same computer program. Traditionally, designers of machine learning models have relied predominantly either on Structure or Context. We propose a new model, which jointly learns on Context and Structure of source code. In contrast to previous approaches, our model uses only language-agnostic features, i.e., source code and features that can be computed directly from the AST. Besides obtaining state-of-the-art on monolingual code summarization on all five programming languages considered in this work, we propose the first multilingual code summarization model. We show that jointly training on non-parallel data from multiple programming languages improves results on all individual languages, where the strongest gains are on low-resource languages. Remarkably, multilingual training only from Context does not lead to the same improvements, highlighting the benefits of combining Structure and Context for representation learning on code.",0
"The source code and its parsed abstract syntax tree (AST) are two different but complementary aspects of a computer program. In the past, machine learning model designers have mostly relied on either the structure or the context. However, we have developed a new model that combines learning on both the context and structure of the source code. Our model uses language-agnostic features that can be computed directly from the AST. Our model has achieved the state-of-the-art in monolingual code summarization for all five programming languages considered in our work. We have also proposed the first multilingual code summarization model, and we have demonstrated that jointly training on non-parallel data from multiple programming languages improves results on all individual languages, especially on low-resource languages. It is noteworthy that multilingual training using only the context does not result in the same improvements, highlighting the advantages of combining the structure and context for representation learning on code.",1
