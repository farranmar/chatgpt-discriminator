"There has been appreciable progress in unsupervised network representation learning (UNRL) approaches over graphs recently with flexible random-walk approaches, new optimization objectives and deep architectures. However, there is no common ground for systematic comparison of embeddings to understand their behavior for different graphs and tasks. In this paper we theoretically group different approaches under a unifying framework and empirically investigate the effectiveness of different network representation methods. In particular, we argue that most of the UNRL approaches either explicitly or implicit model and exploit context information of a node. Consequently, we propose a framework that casts a variety of approaches -- random walk based, matrix factorization and deep learning based -- into a unified context-based optimization function. We systematically group the methods based on their similarities and differences. We study the differences among these methods in detail which we later use to explain their performance differences (on downstream tasks). We conduct a large-scale empirical study considering 9 popular and recent UNRL techniques and 11 real-world datasets with varying structural properties and two common tasks -- node classification and link prediction. We find that there is no single method that is a clear winner and that the choice of a suitable method is dictated by certain properties of the embedding methods, task and structural properties of the underlying graph. In addition we also report the common pitfalls in evaluation of UNRL methods and come up with suggestions for experimental design and interpretation of results.",0
"Recently, significant advancements have been made in unsupervised network representation learning (UNRL) for graphs with new optimization objectives, flexible random-walk approaches, and deep architectures. However, there is a lack of a standardized approach to compare embeddings and understand their behavior for different graphs and tasks. This paper proposes a unifying framework to group various UNRL approaches based on their similarities and differences, and to empirically investigate the effectiveness of different network representation methods. The proposed framework considers that most UNRL approaches model and exploit node context information. We systematically group the methods and study their differences to explain their performance on downstream tasks. A large-scale empirical study is conducted, considering nine popular and recent UNRL techniques and 11 real-world datasets with varying structural properties and two common tasks: node classification and link prediction. We found that there is no clear winner and that the choice of a suitable method depends on the properties of the embedding methods, task, and structural properties of the underlying graph. Additionally, we report common pitfalls in evaluating UNRL methods and provide suggestions for experimental design and result interpretation.",1
"We propose a new method for video object segmentation (VOS) that addresses object pattern learning from unlabeled videos, unlike most existing methods which rely heavily on extensive annotated data. We introduce a unified unsupervised/weakly supervised learning framework, called MuG, that comprehensively captures intrinsic properties of VOS at multiple granularities. Our approach can help advance understanding of visual patterns in VOS and significantly reduce annotation burden. With a carefully-designed architecture and strong representation learning ability, our learned model can be applied to diverse VOS settings, including object-level zero-shot VOS, instance-level zero-shot VOS, and one-shot VOS. Experiments demonstrate promising performance in these settings, as well as the potential of MuG in leveraging unlabeled data to further improve the segmentation accuracy.",0
"A novel approach to video object segmentation (VOS) is proposed, which differs from most existing methods that rely on extensive annotated data. Instead, our approach, called MuG, introduces a unified unsupervised/weakly supervised learning framework that captures the intrinsic properties of VOS at various granularities. This method can enhance the comprehension of visual patterns in VOS and reduce the annotation burden significantly. With a well-designed architecture and robust representation learning ability, our model can be applied to diverse VOS scenarios, including object-level zero-shot VOS, instance-level zero-shot VOS, and one-shot VOS. Experiments demonstrate that MuG performs well in these settings and has the potential to improve segmentation accuracy by leveraging unlabeled data.",1
"Our work focuses on tackling the challenging but natural visual recognition task of long-tailed data distribution (i.e., a few classes occupy most of the data, while most classes have rarely few samples). In the literature, class re-balancing strategies (e.g., re-weighting and re-sampling) are the prominent and effective methods proposed to alleviate the extreme imbalance for dealing with long-tailed problems. In this paper, we firstly discover that these re-balancing methods achieving satisfactory recognition accuracy owe to that they could significantly promote the classifier learning of deep networks. However, at the same time, they will unexpectedly damage the representative ability of the learned deep features to some extent. Therefore, we propose a unified Bilateral-Branch Network (BBN) to take care of both representation learning and classifier learning simultaneously, where each branch does perform its own duty separately. In particular, our BBN model is further equipped with a novel cumulative learning strategy, which is designed to first learn the universal patterns and then pay attention to the tail data gradually. Extensive experiments on four benchmark datasets, including the large-scale iNaturalist ones, justify that the proposed BBN can significantly outperform state-of-the-art methods. Furthermore, validation experiments can demonstrate both our preliminary discovery and effectiveness of tailored designs in BBN for long-tailed problems. Our method won the first place in the iNaturalist 2019 large scale species classification competition, and our code is open-source and available at https://github.com/Megvii-Nanjing/BBN.",0
"Our research focuses on addressing the challenging task of recognizing long-tailed data distribution, which refers to a situation where some classes have a significant number of samples, while most classes have very few. Existing research has proposed class re-balancing strategies such as re-weighting and re-sampling, which have been effective in mitigating the extreme imbalance. However, we have discovered that these methods improve classifier learning in deep networks but also harm the representative ability of the learned deep features. To address this, we propose a unified Bilateral-Branch Network (BBN) that can handle both representation learning and classifier learning simultaneously. Each branch performs its own task, and our BBN model is equipped with a novel cumulative learning strategy that learns universal patterns first before focusing on tail data. Our experiments on four benchmark datasets, including iNaturalist, demonstrate that BBN significantly outperforms state-of-the-art methods. We also validate our preliminary discovery and the effectiveness of tailored designs in BBN for long-tailed problems. Our method won first place in the iNaturalist 2019 species classification competition, and our code is available at https://github.com/Megvii-Nanjing/BBN.",1
"This paper introduces a novel perspective about error in machine learning and proposes inverse feature learning (IFL) as a representation learning approach that learns a set of high-level features based on the representation of error for classification or clustering purposes. The proposed perspective about error representation is fundamentally different from current learning methods, where in classification approaches they interpret the error as a function of the differences between the true labels and the predicted ones or in clustering approaches, in which the clustering objective functions such as compactness are used. Inverse feature learning method operates based on a deep clustering approach to obtain a qualitative form of the representation of error as features. The performance of the proposed IFL method is evaluated by applying the learned features along with the original features, or just using the learned features in different classification and clustering techniques for several data sets. The experimental results show that the proposed method leads to promising results in classification and especially in clustering. In classification, the proposed features along with the primary features improve the results of most of the classification methods on several popular data sets. In clustering, the performance of different clustering methods is considerably improved on different data sets. There are interesting results that show some few features of the representation of error capture highly informative aspects of primary features. We hope this paper helps to utilize the error representation learning in different feature learning domains.",0
"This article presents a new approach to error in machine learning, called inverse feature learning (IFL), which uses error representation to create high-level features for classification and clustering purposes. Unlike current methods that view error as a function of the differences between true and predicted labels, or use objective functions like compactness in clustering, IFL uses a deep clustering approach to obtain a qualitative representation of error as features. The effectiveness of IFL was tested by applying the learned features with the original features or using only the learned features in various classification and clustering techniques on multiple datasets. The results showed that IFL improved classification and clustering performance significantly, with some error representation features capturing highly informative aspects of primary features. Overall, this method offers a valuable contribution to error representation learning in feature learning domains.",1
"This paper proposes inverse feature learning as a novel supervised feature learning technique that learns a set of high-level features for classification based on an error representation approach. The key contribution of this method is to learn the representation of error as high-level features, while current representation learning methods interpret error by loss functions which are obtained as a function of differences between the true labels and the predicted ones. One advantage of such learning method is that the learned features for each class are independent of learned features for other classes; therefore, this method can learn simultaneously meaning that it can learn new classes without retraining. Error representation learning can also help with generalization and reduce the chance of over-fitting by adding a set of impactful features to the original data set which capture the relationships between each instance and different classes through an error generation and analysis process. This method can be particularly effective in data sets, where the instances of each class have diverse feature representations or the ones with imbalanced classes. The experimental results show that the proposed method results in significantly better performance compared to the state-of-the-art classification techniques for several popular data sets. We hope this paper can open a new path to utilize the proposed perspective of error representation learning in different feature learning domains.",0
"The proposed technique in this paper is called inverse feature learning, which is a novel form of supervised feature learning. It aims to learn high-level features for classification based on an error representation approach. Unlike current representation learning methods that interpret error through loss functions, this method learns the representation of error as high-level features. One advantage of this approach is that the learned features for each class are independent from those of other classes, allowing for simultaneous learning of new classes without retraining. Error representation learning also helps with generalization and reduces over-fitting by adding impactful features to the original data set. This method is particularly effective for data sets with diverse feature representations or imbalanced classes. The experimental results demonstrate that this method outperforms existing classification techniques for several popular data sets. The authors hope that this paper will present a new perspective on error representation learning for feature learning domains.",1
"Deep neural networks are composed of layers of parametrised linear operations intertwined with non linear activations. In basic models, such as the multi-layer perceptron, a linear layer operates on a simple input vector embedding of the instance being processed, and produces an output vector embedding by straight multiplication by a matrix parameter. In more complex models, the input and output are structured and their embeddings are higher order tensors. The parameter of each linear operation must then be controlled so as not to explode with the complexity of the structures involved. This is essentially the role of convolution models, which exist in many flavours dependent on the type of structure they deal with (grids, networks, time series etc.). We present here a unified framework which aims at capturing the essence of these diverse models, allowing a systematic analysis of their properties and their mutual enrichment. We also show that attention models naturally fit in the same framework: attention is convolution in which the structure itself is adaptive, and learnt, instead of being given a priori.",0
"Deep neural networks consist of layers that combine linear operations with non-linear activations. In simpler models, like the multi-layer perceptron, the linear layer operates on an input vector and produces an output vector by multiplying it with a matrix parameter. However, in more complex models, the input and output are structured, and their embeddings are higher order tensors. To prevent the parameter of each linear operation from becoming too complex, convolution models are used, which can deal with different types of structures like grids, networks, and time series. Our unified framework captures the essence of these diverse models, allowing for a systematic analysis of their properties and mutual enrichment. Further, attention models fit naturally into this framework, as attention is a convolution where the structure itself is adaptive and learned, rather than being given a priori.",1
"Scalability in terms of object density in a scene is a primary challenge in unsupervised sequential object-oriented representation learning. Most of the previous models have been shown to work only on scenes with a few objects. In this paper, we propose SCALOR, a probabilistic generative world model for learning SCALable Object-oriented Representation of a video. With the proposed spatially-parallel attention and proposal-rejection mechanisms, SCALOR can deal with orders of magnitude larger numbers of objects compared to the previous state-of-the-art models. Additionally, we introduce a background module that allows SCALOR to model complex dynamic backgrounds as well as many foreground objects in the scene. We demonstrate that SCALOR can deal with crowded scenes containing up to a hundred objects while jointly modeling complex dynamic backgrounds. Importantly, SCALOR is the first unsupervised object representation model shown to work for natural scenes containing several tens of moving objects.",0
"The major issue in unsupervised sequential object-oriented representation learning is the ability to scale according to the density of objects in a scene. Previous models have only been effective for scenes with a small number of objects. This paper introduces SCALOR, a probabilistic generative world model that addresses this challenge by utilizing spatially-parallel attention and proposal-rejection mechanisms, allowing it to handle significantly more objects than previous models. With the addition of a background module, SCALOR can also model complex dynamic backgrounds in combination with numerous foreground objects. The effectiveness of SCALOR is demonstrated through its ability to handle crowded scenes with up to a hundred objects while simultaneously modeling complex dynamic backgrounds. Furthermore, SCALOR is the first unsupervised object representation model to successfully work with natural scenes containing several tens of moving objects.",1
"Predictions and predictive knowledge have seen recent success in improving not only robot control but also other applications ranging from industrial process control to rehabilitation. A property that makes these predictive approaches well suited for robotics is that they can be learned online and incrementally through interaction with the environment. However, a remaining challenge for many prediction-learning approaches is an appropriate choice of prediction-learning parameters, especially parameters that control the magnitude of a learning machine's updates to its predictions (the learning rate or step size). To begin to address this challenge, we examine the use of online step-size adaptation using a sensor-rich robotic arm. Our method of choice, Temporal-Difference Incremental Delta-Bar-Delta (TIDBD), learns and adapts step sizes on a feature level; importantly, TIDBD allows step-size tuning and representation learning to occur at the same time. We show that TIDBD is a practical alternative for classic Temporal-Difference (TD) learning via an extensive parameter search. Both approaches perform comparably in terms of predicting future aspects of a robotic data stream. Furthermore, the use of a step-size adaptation method like TIDBD appears to allow a system to automatically detect and characterize common sensor failures in a robotic application. Together, these results promise to improve the ability of robotic devices to learn from interactions with their environments in a robust way, providing key capabilities for autonomous agents and robots.",0
"The success of predictions and predictive knowledge has been observed in various applications, including robot control, industrial process control, and rehabilitation. One advantage of predictive approaches for robotics is their ability to learn incrementally and online through interaction with the environment. However, the challenge of selecting appropriate prediction-learning parameters, particularly those controlling a machine's learning rate, remains. To address this challenge, we investigated the use of online step-size adaptation with a sensor-rich robotic arm using Temporal-Difference Incremental Delta-Bar-Delta (TIDBD). This approach allows for feature-level step-size tuning and representation learning simultaneously. We found that TIDBD is a practical alternative to classic Temporal-Difference (TD) learning and performs comparably in predicting future aspects of robotic data streams. Additionally, using a step-size adaptation method like TIDBD can automatically detect and characterize common sensor failures in robotic applications. These findings have the potential to enhance the robustness of robotic devices' ability to learn autonomously from their environments.",1
"Contrastive learning is an approach to representation learning that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, we prove that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. We apply this procedure in a semi-supervised setup and demonstrate empirically that linear classifiers with these representations perform well in document classification tasks with very few training examples.",0
"The utilization of naturally occurring similar and dissimilar data point pairs is the foundation of contrastive learning, an approach to representation learning. It is possible to discover beneficial data embeddings using this method. In the context of topic modeling assumptions for document classification, we have demonstrated that contrastive learning can retrieve a document representation that exposes its underlying topic posterior information to linear models. By applying this technique in a semi-supervised setup, we have provided empirical evidence that linear classifiers utilizing these representations excel in document classification tasks with minimal training examples.",1
"To take full advantage of fast-growing unlabeled networked data, this paper introduces a novel self-supervised strategy for graph representation learning by exploiting natural supervision provided by the data itself. Inspired by human social behavior, we assume that the global context of each node is composed of all nodes in the graph since two arbitrary entities in a connected network could interact with each other via paths of varying length. Based on this, we investigate whether the global context can be a source of free and effective supervisory signals for learning useful node representations. Specifically, we randomly select pairs of nodes in a graph and train a well-designed neural net to predict the contextual position of one node relative to the other. Our underlying hypothesis is that the representations learned from such within-graph context would capture the global topology of the graph and finely characterize the similarity and differentiation between nodes, which is conducive to various downstream learning tasks. Extensive benchmark experiments including node classification, clustering, and link prediction demonstrate that our approach outperforms many state-of-the-art unsupervised methods and sometimes even exceeds the performance of supervised counterparts.",0
"This paper proposes a new method for learning graph representations using self-supervision. By utilizing the natural supervision present in unlabeled networked data, we develop a technique inspired by human social behavior. Our approach assumes that the global context of each node in the graph is composed of all other nodes, as any two entities in a connected network can interact through paths of varying lengths. We investigate whether this global context can provide effective supervisory signals for learning node representations. Our method involves selecting pairs of nodes at random and training a neural net to predict the contextual position of one node relative to the other. We hypothesize that these representations will capture the global topology of the graph and enable various downstream learning tasks. Our experiments, including node classification, clustering, and link prediction, show that our approach outperforms many state-of-the-art unsupervised methods and sometimes even surpasses supervised methods.",1
"Graph representation learning is a ubiquitous task in machine learning where the goal is to embed each vertex into a low-dimensional vector space. We consider the bipartite graph and formalize its representation learning problem as a statistical estimation problem of parameters in a semiparametric exponential family distribution. The bipartite graph is assumed to be generated by a semiparametric exponential family distribution, whose parametric component is given by the proximity of outputs of two one-layer neural networks, while nonparametric (nuisance) component is the base measure. Neural networks take high-dimensional features as inputs and output embedding vectors. In this setting, the representation learning problem is equivalent to recovering the weight matrices. The main challenges of estimation arise from the nonlinearity of activation functions and the nonparametric nuisance component of the distribution. To overcome these challenges, we propose a pseudo-likelihood objective based on the rank-order decomposition technique and focus on its local geometry. We show that the proposed objective is strongly convex in a neighborhood around the ground truth, so that a gradient descent-based method achieves linear convergence rate. Moreover, we prove that the sample complexity of the problem is linear in dimensions (up to logarithmic factors), which is consistent with parametric Gaussian models. However, our estimator is robust to any model misspecification within the exponential family, which is validated in extensive experiments.",0
"In machine learning, graph representation learning is a common task that aims to embed each vertex into a low-dimensional vector space. Our focus is on the bipartite graph, and we formalize its representation learning problem as a statistical estimation challenge involving semiparametric exponential family distribution parameters. We assume that the bipartite graph is generated by a semiparametric exponential family distribution, with its parametric component being the proximity of two one-layer neural network outputs, and the nonparametric component being the base measure. The neural networks take high-dimensional features as inputs and produce embedding vectors as outputs. The goal of the representation learning problem is to recover the weight matrices. However, the nonlinearity of activation functions and the nonparametric nuisance component of the distribution pose significant challenges to estimation. To address these challenges, we propose a pseudo-likelihood objective based on the rank-order decomposition technique and focus on its local geometry. We demonstrate that the proposed objective is strongly convex in a neighborhood around the ground truth, allowing a gradient descent-based method to achieve a linear convergence rate. We also prove that the sample complexity of the problem is linear in dimensions, similar to parametric Gaussian models. However, our estimator is robust to any model misspecification within the exponential family, as shown in our extensive experiments.",1
"We present a novel variational generative adversarial network (VGAN) based on Wasserstein loss to learn a latent representation from a face image that is invariant to identity but preserves head-pose information. This facilitates synthesis of a realistic face image with the same head pose as a given input image, but with a different identity. One application of this network is in privacy-sensitive scenarios; after identity replacement in an image, utility, such as head pose, can still be recovered. Extensive experimental validation on synthetic and real human-face image datasets performed under 3 threat scenarios confirms the ability of the proposed network to preserve head pose of the input image, mask the input identity, and synthesize a good-quality realistic face image of a desired identity. We also show that our network can be used to perform pose-preserving identity morphing and identity-preserving pose morphing. The proposed method improves over a recent state-of-the-art method in terms of quantitative metrics as well as synthesized image quality.",0
"Our research introduces a new variational generative adversarial network (VGAN) that utilizes Wasserstein loss to acquire a latent representation from a facial image. This representation is invariant to identity but retains head-pose information, allowing for the production of a convincing facial image with the same head pose as the original input, but with a different identity. This network is particularly useful for situations where privacy is a concern, as the original image's utility, such as head pose, can still be retrieved after identity substitution. We conducted extensive experiments on synthetic and genuine human-face image datasets to verify the network's ability to maintain head pose, obscure the input identity, and create a realistic facial image of a target identity. We also demonstrated that our network can be used for pose-preserving identity morphing and identity-preserving pose morphing. Our approach outperforms a recent state-of-the-art method in terms of quantitative metrics and synthesized image quality.",1
"Recent advances in deep learning have achieved promising performance for medical image analysis, while in most cases ground-truth annotations from human experts are necessary to train the deep model. In practice, such annotations are expensive to collect and can be scarce for medical imaging applications. Therefore, there is significant interest in learning representations from unlabelled raw data. In this paper, we propose a self-supervised learning approach to learn meaningful and transferable representations from medical imaging video without any type of human annotation. We assume that in order to learn such a representation, the model should identify anatomical structures from the unlabelled data. Therefore we force the model to address anatomy-aware tasks with free supervision from the data itself. Specifically, the model is designed to correct the order of a reshuffled video clip and at the same time predict the geometric transformation applied to the video clip. Experiments on fetal ultrasound video show that the proposed approach can effectively learn meaningful and strong representations, which transfer well to downstream tasks like standard plane detection and saliency prediction.",0
"Advancements in deep learning have shown promise in the analysis of medical images, but often require ground-truth annotations from human experts to train the model. However, obtaining these annotations can be costly and difficult for medical imaging applications. Therefore, there is a growing interest in learning meaningful representations from unlabelled raw data. This paper proposes a self-supervised learning approach to learn transferable representations from medical imaging video without human annotation. The proposed approach aims to identify anatomical structures from unlabelled data by forcing the model to address anatomy-aware tasks with free supervision from the data itself. Specifically, the model is designed to correct the order of a reshuffled video clip and predict the geometric transformation applied to the clip. Experiments on fetal ultrasound video demonstrate that the proposed approach can effectively learn meaningful and transferable representations, which can be used for downstream tasks such as standard plane detection and saliency prediction.",1
"This paper employs a formal connection of machine learning with thermodynamics to characterize the quality of learnt representations for transfer learning. We discuss how information-theoretic functional such as rate, distortion and classification loss of a model lie on a convex, so-called equilibrium surface.We prescribe dynamical processes to traverse this surface under constraints, e.g., an iso-classification process that trades off rate and distortion to keep the classification loss unchanged. We demonstrate how this process can be used for transferring representations from a source dataset to a target dataset while keeping the classification loss constant. Experimental validation of the theoretical results is provided on standard image-classification datasets.",0
"In this paper, we utilize a formal association between thermodynamics and machine learning to evaluate the efficacy of acquired representations for transfer learning. Our focus is on examining how information-based functions, such as rate, distortion, and classification loss, of a model are situated on a convex equilibrium surface. We propose a set of dynamic procedures that can navigate this surface under specified limitations, such as an iso-classification process that balances rate and distortion while maintaining a constant classification loss. Our research showcases how this approach can be implemented to transfer acquired representations from a source dataset to a target dataset while keeping the classification loss unaffected. We also present experimental evidence that supports our theoretical findings on established image-classification datasets.",1
"Self-supervised learning is showing great promise for monocular depth estimation, using geometry as the only source of supervision. Depth networks are indeed capable of learning representations that relate visual appearance to 3D properties by implicitly leveraging category-level patterns. In this work we investigate how to leverage more directly this semantic structure to guide geometric representation learning, while remaining in the self-supervised regime. Instead of using semantic labels and proxy losses in a multi-task approach, we propose a new architecture leveraging fixed pretrained semantic segmentation networks to guide self-supervised representation learning via pixel-adaptive convolutions. Furthermore, we propose a two-stage training process to overcome a common semantic bias on dynamic objects via resampling. Our method improves upon the state of the art for self-supervised monocular depth prediction over all pixels, fine-grained details, and per semantic categories.",0
"The use of self-supervised learning in monocular depth estimation using only geometry as a source of supervision has shown promising results. Depth networks have the ability to learn representations that connect visual appearance to 3D properties by implicitly utilizing category-level patterns. To further enhance the learning process, we aim to incorporate semantic structure to guide geometric representation learning while still adhering to the self-supervised regime. Instead of a multi-task approach using semantic labels and proxy losses, we propose a new architecture that utilizes pretrained semantic segmentation networks to guide self-supervised representation learning through pixel-adaptive convolutions. Additionally, we present a two-stage training process to overcome a common semantic bias on dynamic objects through resampling. Our method surpasses the current state-of-the-art in self-supervised monocular depth prediction in terms of all pixels, fine-grained details, and per semantic categories.",1
"Self-supervised representation learning targets to learn convnet-based image representations from unlabeled data. Inspired by the success of NLP methods in this area, in this work we propose a self-supervised approach based on spatially dense image descriptions that encode discrete visual concepts, here called visual words. To build such discrete representations, we quantize the feature maps of a first pre-trained self-supervised convnet, over a k-means based vocabulary. Then, as a self-supervised task, we train another convnet to predict the histogram of visual words of an image (i.e., its Bag-of-Words representation) given as input a perturbed version of that image. The proposed task forces the convnet to learn perturbation-invariant and context-aware image features, useful for downstream image understanding tasks. We extensively evaluate our method and demonstrate very strong empirical results, e.g., our pre-trained self-supervised representations transfer better on detection task and similarly on classification over classes ""unseen"" during pre-training, when compared to the supervised case.   This also shows that the process of image discretization into visual words can provide the basis for very powerful self-supervised approaches in the image domain, thus allowing further connections to be made to related methods from the NLP domain that have been extremely successful so far.",0
"The objective of self-supervised representation learning is to acquire convnet-based image representations from unlabeled data. Drawing inspiration from the effectiveness of NLP techniques in this area, we suggest a self-supervised approach that is based on spatially dense image descriptions encoding discrete visual concepts, referred to as visual words. We generate such discrete representations by quantizing the feature maps of a first pre-trained self-supervised convnet, using a k-means based vocabulary. We then train another convnet to predict the histogram of visual words of an image (i.e., its Bag-of-Words representation) in a self-supervised task, given a perturbed version of that image as input. The proposed task motivates the convnet to learn perturbation-invariant and context-aware image features, which are valuable for understanding images. Our method is extensively evaluated, showing strong empirical results. For instance, our pre-trained self-supervised representations outperform the supervised case in transfer learning on detection tasks and classification over classes ""unseen"" during pre-training. This indicates that the process of image discretization into visual words can offer a solid foundation for powerful self-supervised approaches in the image domain, thereby establishing further connections to related techniques from the successful NLP domain.",1
"This work exploits action equivariance for representation learning in reinforcement learning. Equivariance under actions states that transitions in the input space are mirrored by equivalent transitions in latent space, while the map and transition functions should also commute. We introduce a contrastive loss function that enforces action equivariance on the learned representations. We prove that when our loss is zero, we have a homomorphism of a deterministic Markov Decision Process (MDP). Learning equivariant maps leads to structured latent spaces, allowing us to build a model on which we plan through value iteration. We show experimentally that for deterministic MDPs, the optimal policy in the abstract MDP can be successfully lifted to the original MDP. Moreover, the approach easily adapts to changes in the goal states. Empirically, we show that in such MDPs, we obtain better representations in fewer epochs compared to representation learning approaches using reconstructions, while generalizing better to new goals than model-free approaches.",0
"In this work, we utilize the concept of action equivariance to enhance representation learning in reinforcement learning. Action equivariance requires that transitions in the input space should correspond to equivalent transitions in the latent space, while the map and transition functions must also commute. To enforce this property on the learned representations, we introduce a contrastive loss function. We demonstrate that when this loss function is zero, we achieve a homomorphism of a deterministic Markov Decision Process. By learning equivariant maps, we can create structured latent spaces that enable us to plan using value iteration. Our experiments reveal that for deterministic MDPs, the optimal policy in the abstract MDP can be successfully transferred to the original MDP. Additionally, our approach is adaptable to changes in the goal states. Empirically, we demonstrate that our method outperforms reconstruction-based representation learning approaches in terms of better representations obtained in fewer epochs and better generalization to new goals compared to model-free methods.",1
"For a robotic grasping task in which diverse unseen target objects exist in a cluttered environment, some deep learning-based methods have achieved state-of-the-art results using visual input directly. In contrast, actor-critic deep reinforcement learning (RL) methods typically perform very poorly when grasping diverse objects, especially when learning from raw images and sparse rewards. To make these RL techniques feasible for vision-based grasping tasks, we employ state representation learning (SRL), where we encode essential information first for subsequent use in RL. However, typical representation learning procedures are unsuitable for extracting pertinent information for learning the grasping skill, because the visual inputs for representation learning, where a robot attempts to grasp a target object in clutter, are extremely complex. We found that preprocessing based on the disentanglement of a raw input image is the key to effectively capturing a compact representation. This enables deep RL to learn robotic grasping skills from highly varied and diverse visual inputs. We demonstrate the effectiveness of this approach with varying levels of disentanglement in a realistic simulated environment.",0
"Several deep learning-based methods have been successful in achieving state-of-the-art results for a robotic grasping task involving diverse target objects in a cluttered environment by using visual input directly. However, actor-critic deep reinforcement learning methods have performed poorly in grasping diverse objects, especially when learning from raw images and sparse rewards. To make these RL techniques feasible for vision-based grasping tasks, state representation learning is employed, wherein essential information is encoded first for subsequent use in RL. However, typical representation learning procedures are unsuitable for extracting pertinent information necessary for learning grasping skills because of the complexity of visual inputs. To capture a compact representation, preprocessing based on the disentanglement of a raw input image is crucial, enabling deep RL to learn robotic grasping skills from highly varied and diverse visual inputs. The effectiveness of this approach is demonstrated in a realistic simulated environment with varying levels of disentanglement.",1
"Recognizing wild faces is extremely hard as they appear with all kinds of variations. Traditional methods either train with specifically annotated variation data from target domains, or by introducing unlabeled target variation data to adapt from the training data. Instead, we propose a universal representation learning framework that can deal with larger variation unseen in the given training data without leveraging target domain knowledge. We firstly synthesize training data alongside some semantically meaningful variations, such as low resolution, occlusion and head pose. However, directly feeding the augmented data for training will not converge well as the newly introduced samples are mostly hard examples. We propose to split the feature embedding into multiple sub-embeddings, and associate different confidence values for each sub-embedding to smooth the training procedure. The sub-embeddings are further decorrelated by regularizing variation classification loss and variation adversarial loss on different partitions of them. Experiments show that our method achieves top performance on general face recognition datasets such as LFW and MegaFace, while significantly better on extreme benchmarks such as TinyFace and IJB-S.",0
"Identifying wild faces is a challenging task due to the wide range of variations they may have. Traditional methods for training either rely on annotated variation data from the target domain or incorporate unlabeled variation data from the target domain to adapt from the training data. However, we propose a universal representation learning framework that can handle larger variations that are not present in the provided training data without relying on any knowledge of the target domain. To begin, we generate training data that includes semantically meaningful variations such as occlusion, low resolution, and head pose. Yet, training with the augmented data alone does not lead to optimal outcomes since the newly introduced samples are often difficult examples. To address this issue, we divide the feature embedding into multiple sub-embeddings and assign distinct confidence values to each sub-embedding to smooth the training process. Furthermore, we use variation classification loss and variation adversarial loss to decorrelate the sub-embeddings on different partitions. Our approach demonstrates superior performance on various face recognition datasets, including LFW and MegaFace, and outperforms other methods on challenging benchmarks such as TinyFace and IJB-S.",1
"We seek to learn a representation on a large annotated data source that generalizes to a target domain using limited new supervision. Many prior approaches to this problem have focused on learning ""disentangled"" representations so that as individual factors vary in a new domain, only a portion of the representation need be updated. In this work, we seek the generalization power of disentangled representations, but relax the requirement of explicit latent disentanglement and instead encourage linearity of individual factors of variation by requiring them to be manipulable by learned linear transformations. We dub these transformations latent canonicalizers, as they aim to modify the value of a factor to a pre-determined (but arbitrary) canonical value (e.g., recoloring the image foreground to black). Assuming a source domain with access to meta-labels specifying the factors of variation within an image, we demonstrate experimentally that our method helps reduce the number of observations needed to generalize to a similar target domain when compared to a number of supervised baselines.",0
"Our objective is to acquire a representation from an extensively annotated data set that can be applied to a target domain with minimal supervision. Prior solutions to this issue have emphasized the learning of ""disentangled"" representations to enable partial updates to the representation when individual factors vary in a new domain. In this research, we aim for the generalization capability of disentangled representations but ease the requirement of explicit latent disentanglement. Instead, we stimulate linearity of individual factors of variation by making them manipulable through learned linear transformations, which we term latent canonicalizers. These transformations modify a factor's value to a pre-determined canonical value, such as changing the color of the image foreground to black. Assuming a source domain with access to meta-labels that specify the image's variation factors, our method reduces the number of observations required to generalize to a similar target domain. We demonstrate this experimentally and compare our approach to several supervised baselines.",1
"We present a new method to learn video representations from large-scale unlabeled video data. Ideally, this representation will be generic and transferable, directly usable for new tasks such as action recognition and zero or few-shot learning. We formulate unsupervised representation learning as a multi-modal, multi-task learning problem, where the representations are shared across different modalities via distillation. Further, we introduce the concept of loss function evolution by using an evolutionary search algorithm to automatically find optimal combination of loss functions capturing many (self-supervised) tasks and modalities. Thirdly, we propose an unsupervised representation evaluation metric using distribution matching to a large unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised constraint, which is not guided by any labeling, produces similar results to weakly-supervised, task-specific ones. The proposed unsupervised representation learning results in a single RGB network and outperforms previous methods. Notably, it is also more effective than several label-based methods (e.g., ImageNet), with the exception of large, fully labeled video datasets.",0
"A new approach for acquiring video representations from extensive unlabeled video data is presented. The objective is to create a generic and transferable representation that can be used directly for new tasks like zero or few-shot learning and action recognition. The method treats unsupervised representation learning as a multi-modal, multi-task learning problem where distillation is used to share representations across various modalities. Additionally, the concept of loss function evolution is introduced, whereby an evolutionary search algorithm is used to automatically identify the optimal combination of loss functions that captures multiple self-supervised tasks and modalities. Furthermore, an unsupervised representation evaluation metric is proposed, which uses distribution matching to a vast unlabeled dataset as a prior constraint based on Zipf's law. This unsupervised constraint produces comparable results to weakly-supervised, task-specific ones. The proposed unsupervised representation learning method yields a single RGB network and surpasses previous methods. It is also more effective than various label-based methods, except for large, fully labeled video datasets.",1
"One popular trend in meta-learning is to learn from many training tasks a common initialization for a gradient-based method that can be used to solve a new task with few samples. The theory of meta-learning is still in its early stages, with several recent learning-theoretic analyses of methods such as Reptile [Nichol et al., 2018] being for convex models. This work shows that convex-case analysis might be insufficient to understand the success of meta-learning, and that even for non-convex models it is important to look inside the optimization black-box, specifically at properties of the optimization trajectory. We construct a simple meta-learning instance that captures the problem of one-dimensional subspace learning. For the convex formulation of linear regression on this instance, we show that the new task sample complexity of any initialization-based meta-learning algorithm is $\Omega(d)$, where $d$ is the input dimension. In contrast, for the non-convex formulation of a two layer linear network on the same instance, we show that both Reptile and multi-task representation learning can have new task sample complexity of $\mathcal{O}(1)$, demonstrating a separation from convex meta-learning. Crucially, analyses of the training dynamics of these methods reveal that they can meta-learn the correct subspace onto which the data should be projected.",0
"Learning a common initialization for solving new tasks with few samples is a popular trend in meta-learning. However, the theory of meta-learning is still in its early stages and recent analyses have only focused on convex models, such as Reptile. This limited approach may not fully explain the success of meta-learning, especially for non-convex models. Therefore, it is important to examine the optimization trajectory of the learning process. To explore this, we created a simple meta-learning instance for one-dimensional subspace learning. For linear regression, we found that any initialization-based meta-learning algorithm has a new task sample complexity of $\Omega(d)$ for the convex formulation. However, for the non-convex formulation of a two layer linear network on the same instance, both Reptile and multi-task representation learning have a new task sample complexity of $\mathcal{O}(1)$. This demonstrates a clear separation between convex and non-convex meta-learning. Furthermore, analyzing the training dynamics of these methods revealed that they were able to meta-learn the correct subspace for projecting the data.",1
"We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon's information theory that takes into account the modeling power and computational constraints of the observer. The resulting \emph{predictive $\mathcal{V}$-information} encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon's mutual information and in violation of the data processing inequality, $\mathcal{V}$-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, $\mathcal{V}$-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive $\mathcal{V}$-information is more effective than mutual information for structure learning and fair representation learning.",0
"Our proposal is to introduce a novel framework for determining information in intricate systems. Our approach is based on an extension of Shannon's information theory, which considers the observer's computational limits and modeling abilities. This approach, known as \emph{predictive $\mathcal{V}$-information}, encompasses various aspects of informativeness, such as mutual information and the coefficient of determination. Unlike mutual information, predictive $\mathcal{V}$-information can be generated through computation and does not adhere to the data processing inequality. This is consistent with deep neural networks, which progressively extract more informative features in representation learning. Furthermore, we demonstrate that predictive $\mathcal{V}$-information can be accurately estimated from high-dimensional data with PAC-style guarantees by incorporating computational constraints. Empirically, we show that predictive $\mathcal{V}$-information is more effective than mutual information in structure learning and fair representation learning.",1
"Graph representation learning embeds nodes in large graphs as low-dimensional vectors and is of great benefit to many downstream applications. Most embedding frameworks, however, are inherently transductive and unable to generalize to unseen nodes or learn representations across different graphs. Although inductive approaches can generalize to unseen nodes, they neglect different contexts of nodes and cannot learn node embeddings dually. In this paper, we present a context-aware unsupervised dual encoding framework, \textbf{CADE}, to generate representations of nodes by combining real-time neighborhoods with neighbor-attentioned representation, and preserving extra memory of known nodes. We exhibit that our approach is effective by comparing to state-of-the-art methods.",0
"The process of graph representation learning involves transforming the nodes within large graphs into low-dimensional vectors, which can be extremely useful for a variety of downstream applications. However, most existing embedding frameworks are transductive in nature, meaning they are incapable of generalizing to new nodes or creating representations that span multiple graphs. Although inductive approaches can generalize to unseen nodes, they often overlook important contextual information and cannot generate dual node embeddings. To address these challenges, we propose a novel unsupervised dual encoding framework called \textbf{CADE}. Our approach leverages real-time neighborhood information and neighbor-attentioned representation, while also preserving memory of known nodes. We demonstrate the effectiveness of our method through comparison with state-of-the-art approaches.",1
"Learning rich representation from data is an important task for deep generative models such as variational auto-encoder (VAE). However, by extracting high-level abstractions in the bottom-up inference process, the goal of preserving all factors of variations for top-down generation is compromised. Motivated by the concept of ""starting small"", we present a strategy to progressively learn independent hierarchical representations from high- to low-levels of abstractions. The model starts with learning the most abstract representation, and then progressively grow the network architecture to introduce new representations at different levels of abstraction. We quantitatively demonstrate the ability of the presented model to improve disentanglement in comparison to existing works on two benchmark data sets using three disentanglement metrics, including a new metric we proposed to complement the previously-presented metric of mutual information gap. We further present both qualitative and quantitative evidence on how the progression of learning improves disentangling of hierarchical representations. By drawing on the respective advantage of hierarchical representation learning and progressive learning, this is to our knowledge the first attempt to improve disentanglement by progressively growing the capacity of VAE to learn hierarchical representations.",0
"Deep generative models, like VAE, need to learn rich representations from data. However, this can compromise the preservation of all variations when extracting high-level abstractions in the bottom-up inference process for top-down generation. To address this, we propose a strategy of ""starting small"" to progressively learn independent hierarchical representations from high to low levels of abstraction. The model begins by learning the most abstract representation and gradually grows the network architecture to introduce new representations at different levels of abstraction. Our model improves disentanglement compared to existing works on two benchmark data sets using three disentanglement metrics, including a new metric we proposed to complement the previously-presented metric of mutual information gap. We provide both qualitative and quantitative evidence of how the progression of learning enhances disentangling of hierarchical representations. This is the first attempt to improve disentanglement by progressively growing the capacity of VAE to learn hierarchical representations, drawing on the respective advantages of hierarchical representation learning and progressive learning.",1
"A common strategy in modern learning systems is to learn a representation that is useful for many tasks, a.k.a. representation learning. We study this strategy in the imitation learning setting for Markov decision processes (MDPs) where multiple experts' trajectories are available. We formulate representation learning as a bi-level optimization problem where the ""outer"" optimization tries to learn the joint representation and the ""inner"" optimization encodes the imitation learning setup and tries to learn task-specific parameters. We instantiate this framework for the imitation learning settings of behavior cloning and observation-alone. Theoretically, we show using our framework that representation learning can provide sample complexity benefits for imitation learning in both settings. We also provide proof-of-concept experiments to verify our theory.",0
"Representation learning is a popular approach in modern learning systems, which involves acquiring a representation that is beneficial for various tasks. In this study, we investigate this strategy in the context of imitation learning for Markov decision processes (MDPs), where we have access to multiple experts' trajectories. Our approach involves formulating representation learning as a bi-level optimization problem, where the ""outer"" optimization aims to learn the joint representation, and the ""inner"" optimization focuses on encoding the imitation learning setup and learning task-specific parameters. We apply this framework to the behavior cloning and observation-alone imitation learning settings. Our theoretical analysis demonstrates that representation learning can result in sample complexity advantages in both settings. Additionally, we conduct proof-of-concept experiments to validate our theory.",1
"Learning compact discrete representations of data is a key task on its own or for facilitating subsequent processing of data. In this paper we present a model that produces Discrete InfoMax Codes (DIMCO); we learn a probabilistic encoder that yields k-way d-dimensional codes associated with input data. Our model's learning objective is to maximize the mutual information between codes and labels with a regularization, which enforces entries of a codeword to be as independent as possible. We show that the infomax principle also justifies previous loss functions (e.g., cross-entropy) as its special cases. Our analysis also shows that using shorter codes, as DIMCO does, reduces overfitting in the context of few-shot classification. Through experiments in various domains, we observe this implicit meta-regularization effect of DIMCO. Furthermore, we show that the codes learned by DIMCO are efficient in terms of both memory and retrieval time compared to previous methods.",0
"Acquiring condensed and distinct representations of data is a crucial task, either as an independent objective or as a means of facilitating subsequent data processing. This article introduces a Discrete InfoMax Codes (DIMCO) model, which trains a probabilistic encoder to generate k-way d-dimensional codes that correspond to input data. Our model aims to maximize mutual information between codes and labels, with a regularization that promotes code entries to be as independent as possible. We demonstrate that the infomax principle validates previous loss functions, such as cross-entropy, and that DIMCO's shorter codes can reduce overfitting in few-shot classification scenarios. Through experiments in various domains, we observe DIMCO's implicit meta-regularization effect. Additionally, we prove that DIMCO's codes are more efficient in terms of both storage and retrieval time than previous techniques.",1
"In this paper, we consider the source of Deep Reinforcement Learning (DRL)'s sample complexity, asking how much derives from the requirement of learning useful representations of environment states and how much is due to the sample complexity of learning a policy. While for DRL agents, the distinction between representation and policy may not be clear, we seek new insight through a set of transfer learning experiments. In each experiment, we retain some fraction of layers trained on either the same game or a related game, comparing the benefits of transfer learning to learning a policy from scratch. Interestingly, we find that benefits due to transfer are highly variable in general and non-symmetric across pairs of tasks. Our experiments suggest that perhaps transfer from simpler environments can boost performance on more complex downstream tasks and that the requirements of learning a useful representation can range from negligible to the majority of the sample complexity, based on the environment. Furthermore, we find that fine-tuning generally outperforms training with the transferred layers frozen, confirming an insight first noted in the classification setting.",0
"The objective of this study is to investigate the factors responsible for the sample complexity in Deep Reinforcement Learning (DRL). This includes analyzing the proportion of complexity derived from learning valuable representations of environment states versus the complexity of learning a policy. Although DRL agents may not easily distinguish between representation and policy, we conduct a set of transfer learning experiments to gain new insights. These experiments involve retaining some layers trained on the same game or a related game, comparing the benefits of transfer learning to learning a policy from scratch. Our findings reveal that the benefits of transfer learning are inconsistent and asymmetrical across task pairs. We observed that transfer from simpler environments may enhance performance on more complex downstream tasks, and the sample complexity required for learning a valuable representation may vary depending on the environment. Moreover, we confirmed that fine-tuning generally outperforms training with the transferred layers frozen, as previously noted in the classification setting.",1
"Deep learning (DL) models have achieved paradigm-changing performance in many fields with high dimensional data, such as images, audio, and text. However, the black-box nature of deep neural networks is a barrier not just to adoption in applications such as medical diagnosis, where interpretability is essential, but also impedes diagnosis of under performing models. The task of diagnosing or explaining DL models requires the computation of additional artifacts, such as activation values and gradients. These artifacts are large in volume, and their computation, storage, and querying raise significant data management challenges.   In this paper, we articulate DL diagnosis as a data management problem, and we propose a general, yet representative, set of queries to evaluate systems that strive to support this new workload. We further develop a novel data sampling technique that produce approximate but accurate results for these model debugging queries. Our sampling technique utilizes the lower dimension representation learned by the DL model and focuses on model decision boundaries for the data in this lower dimensional space. We evaluate our techniques on one standard computer vision and one scientific data set and demonstrate that our sampling technique outperforms a variety of state-of-the-art alternatives in terms of query accuracy.",0
"DL models have made significant advancements in fields that involve high dimensional data, such as images, audio, and text. However, the complex and opaque nature of deep neural networks creates challenges when trying to interpret and diagnose underperforming models, which is particularly problematic in applications like medical diagnosis. In order to diagnose or explain DL models, additional artifacts such as activation values and gradients need to be computed, but the volume of these artifacts presents significant data management challenges. This paper proposes a solution to this problem by framing DL diagnosis as a data management issue and presenting a set of representative queries to evaluate systems that support this workload. The paper also introduces a novel data sampling technique that utilizes the lower dimension representation learned by the DL model and focuses on model decision boundaries. This technique outperforms current state-of-the-art alternatives in terms of query accuracy, as demonstrated through evaluation on standard computer vision and scientific data sets.",1
"Representation learning promises to unlock deep learning for the long tail of vision tasks without expensive labelled datasets. Yet, the absence of a unified evaluation for general visual representations hinders progress. Popular protocols are often too constrained (linear classification), limited in diversity (ImageNet, CIFAR, Pascal-VOC), or only weakly related to representation quality (ELBO, reconstruction error). We present the Visual Task Adaptation Benchmark (VTAB), which defines good representations as those that adapt to diverse, unseen tasks with few examples. With VTAB, we conduct a large-scale study of many popular publicly-available representation learning algorithms. We carefully control confounders such as architecture and tuning budget. We address questions like: How effective are ImageNet representations beyond standard natural datasets? How do representations trained via generative and discriminative models compare? To what extent can self-supervision replace labels? And, how close are we to general visual representations?",0
"The potential of representation learning to enable deep learning for a wider range of vision tasks without the need for costly labelled datasets is hindered by the lack of a standardized evaluation for general visual representations. The current popular evaluation protocols are either too limited in scope, lacking diversity, or only tangentially related to the quality of the representations. To address this, we introduce the Visual Task Adaptation Benchmark (VTAB), which assesses the effectiveness of representations in adapting to diverse, previously unseen tasks with minimal training data. Using VTAB, we conduct a comprehensive study of many widely used publicly available representation learning algorithms, controlling for variables such as architecture and tuning budget. Our investigation seeks to answer important questions such as the effectiveness of ImageNet representations beyond standard natural datasets, the comparison of representations trained by generative and discriminative models, the potential of self-supervision to replace labels, and the degree of progress towards achieving truly general visual representations.",1
"We introduce a parameterization method called Neural Bayes which allows computing statistical quantities that are in general difficult to compute and opens avenues for formulating new objectives for unsupervised representation learning. Specifically, given an observed random variable $\mathbf{x}$ and a latent discrete variable $z$, we can express $p(\mathbf{x}|z)$, $p(z|\mathbf{x})$ and $p(z)$ in closed form in terms of a sufficiently expressive function (Eg. neural network) using our parameterization without restricting the class of these distributions. To demonstrate its usefulness, we develop two independent use cases for this parameterization:   1. Mutual Information Maximization (MIM): MIM has become a popular means for self-supervised representation learning. Neural Bayes allows us to compute mutual information between observed random variables $\mathbf{x}$ and latent discrete random variables $z$ in closed form. We use this for learning image representations and show its usefulness on downstream classification tasks.   2. Disjoint Manifold Labeling: Neural Bayes allows us to formulate an objective which can optimally label samples from disjoint manifolds present in the support of a continuous distribution. This can be seen as a specific form of clustering where each disjoint manifold in the support is a separate cluster. We design clustering tasks that obey this formulation and empirically show that the model optimally labels the disjoint manifolds. Our code is available at \url{https://github.com/salesforce/NeuralBayes}",0
"The Neural Bayes parameterization method is presented as a solution for computing challenging statistical quantities and creating new objectives for unsupervised representation learning. By utilizing our parameterization without limiting the class of distributions, we can express the closed form of $p(\mathbf{x}|z)$, $p(z|\mathbf{x})$, and $p(z)$ through an adequately expressive function, such as a neural network. This method is demonstrated through two distinct use cases: Mutual Information Maximization (MIM) and Disjoint Manifold Labeling. MIM enables self-supervised representation learning, and the closed-form mutual information computation between $\mathbf{x}$ and $z$ is used to learn image representations effectively. Disjoint Manifold Labeling is formulated by Neural Bayes to optimally label samples from separate manifolds in the support of a continuous distribution, which can be seen as a unique clustering approach. We designed clustering tasks to follow this formulation and showed that the model optimally labels the disjoint manifolds. Our code can be found at \url{https://github.com/salesforce/NeuralBayes}.",1
"Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products of a retail website; and the nodes are connected based on a context window. In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress. It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.",0
"Recently, representation learning has been effectively used to generate vector representations of entities for language learning, recommender systems, and similarity learning. Graph embeddings utilize the local structure of a graph to create embeddings for nodes, which can represent items such as words or products, that are connected based on their context. This paper focuses on graph embeddings that use an accurate associative learning update rule to model the embedding vector of a node as a non-convex Gaussian mixture of the embeddings of neighboring nodes, with a constant variance that decreases as iterations progress. Our algorithm is easily parallelizable without the need for shared memory, allowing for its use on large graphs with higher dimensional embeddings. We evaluate the effectiveness of our proposed method on benchmark datasets and compare it favorably to state-of-the-art techniques. Additionally, we apply our method to generate relevant recommendations for a large retailer.",1
"We introduce a sparse scattering deep convolutional neural network, which provides a simple model to analyze properties of deep representation learning for classification. Learning a single dictionary matrix with a classifier yields a higher classification accuracy than AlexNet over the ImageNet 2012 dataset. The network first applies a scattering transform that linearizes variabilities due to geometric transformations such as translations and small deformations. A sparse $\ell^1$ dictionary coding reduces intra-class variability while preserving class separation through projections over unions of linear spaces. It is implemented in a deep convolutional network with a homotopy algorithm having an exponential convergence. A convergence proof is given in a general framework that includes ALISTA. Classification results are analyzed on ImageNet.",0
"A simple model for analyzing deep representation learning for classification is presented in this article through the introduction of a sparse scattering deep convolutional neural network. Learning a single dictionary matrix with a classifier achieves a higher classification accuracy than AlexNet on the ImageNet 2012 dataset. The network utilizes a scattering transform to linearize variabilities caused by geometric transformations, followed by a sparse $\ell^1$ dictionary coding to reduce intra-class variability while preserving class separation through projections over unions of linear spaces. An exponential convergence homotopy algorithm is employed in a deep convolutional network, and a convergence proof is provided in a general framework that includes ALISTA. Classification results on ImageNet are also analyzed.",1
"Recent advancements in graph representation learning have led to the emergence of condensed encodings that capture the main properties of a graph. However, even though these abstract representations are powerful for downstream tasks, they are not equally suitable for visualisation purposes. In this work, we merge Mapper, an algorithm from the field of Topological Data Analysis (TDA), with the expressive power of Graph Neural Networks (GNNs) to produce hierarchical, topologically-grounded visualisations of graphs. These visualisations do not only help discern the structure of complex graphs but also provide a means of understanding the models applied to them for solving various tasks. We further demonstrate the suitability of Mapper as a topological framework for graph pooling by mathematically proving an equivalence with Min-Cut and Diff Pool. Building upon this framework, we introduce a novel pooling algorithm based on PageRank, which obtains competitive results with state of the art methods on graph classification benchmarks.",0
"The progress made in graph representation learning has resulted in the development of condensed encodings that capture essential graph properties. Although these encodings are useful for downstream tasks, they are not ideal for visualization purposes. This study combines Mapper, a Topological Data Analysis algorithm, with Graph Neural Networks to generate hierarchical, topologically-grounded visualizations of graphs. These visualizations not only aid in identifying the complex graph structure but also assist in comprehending the models used to solve various tasks. Additionally, Mapper serves as a suitable topological framework for graph pooling, as demonstrated by proving equivalence with Min-Cut and Diff Pool. A novel pooling algorithm based on PageRank is introduced, which produces competitive results with state of the art methods in graph classification benchmarks.",1
"Combining clustering and representation learning is one of the most promising approaches for unsupervised learning of deep neural networks. However, doing so naively leads to ill posed learning problems with degenerate solutions. In this paper, we propose a novel and principled learning formulation that addresses these issues. The method is obtained by maximizing the information between labels and input data indices. We show that this criterion extends standard crossentropy minimization to an optimal transport problem, which we solve efficiently for millions of input images and thousands of labels using a fast variant of the Sinkhorn-Knopp algorithm. The resulting method is able to self-label visual data so as to train highly competitive image representations without manual labels. Our method achieves state of the art representation learning performance for AlexNet and ResNet-50 on SVHN, CIFAR-10, CIFAR-100 and ImageNet and yields the first self-supervised AlexNet that outperforms the supervised Pascal VOC detection baseline. Code and models are available.",0
"The combination of clustering and representation learning shows great promise for unsupervised learning of deep neural networks. However, this approach can lead to poorly defined learning problems with degenerate solutions if done in a naive manner. In this paper, we propose a new and principled learning formulation that addresses these issues by maximizing the information between labels and input data indices. We demonstrate that this criterion expands standard crossentropy minimization to an optimal transport problem, which we efficiently solve for millions of input images and thousands of labels using a fast variant of the Sinkhorn-Knopp algorithm. Our resulting method can self-label visual data to train highly competitive image representations without the need for manual labels. We achieve state-of-the-art representation learning performance for AlexNet and ResNet-50 on SVHN, CIFAR-10, CIFAR-100, and ImageNet. Additionally, our method yields the first self-supervised AlexNet that surpasses the supervised Pascal VOC detection baseline. We have made our code and models available for others to use.",1
"We investigate the effect of the dimensionality of the representations learned in Deep Neural Networks (DNNs) on their robustness to input perturbations, both adversarial and random. To achieve low dimensionality of learned representations, we propose an easy-to-use, end-to-end trainable, low-rank regularizer (LR) that can be applied to any intermediate layer representation of a DNN. This regularizer forces the feature representations to (mostly) lie in a low-dimensional linear subspace. We perform a wide range of experiments that demonstrate that the LR indeed induces low rank on the representations, while providing modest improvements to accuracy as an added benefit. Furthermore, the learned features make the trained model significantly more robust to input perturbations such as Gaussian and adversarial noise (even without adversarial training). Lastly, the low-dimensionality means that the learned features are highly compressible; thus discriminative features of the data can be stored using very little memory. Our experiments indicate that models trained using the LR learn robust classifiers by discovering subspaces that avoid non-robust features. Algorithmically, the LR is scalable, generic, and straightforward to implement into existing deep learning frameworks.",0
"Our study aims to explore how the dimensionality of Deep Neural Network (DNN) representations affects their ability to withstand input perturbations, whether they are random or adversarial. We propose a low-rank regularizer (LR) that can be easily applied to any intermediate layer representation in a DNN to achieve a low dimensionality of learned representations. By imposing a constraint on the feature representations to lie mostly in a low-dimensional linear subspace, the LR induces low rank on the representations and slightly improves accuracy. Additionally, the learned features make the model significantly more robust to input perturbations, such as Gaussian and adversarial noise, even without adversarial training. The low dimensionality of the learned features also enables highly compressible discriminative features, requiring minimal memory storage. Our experiments reveal that the LR leads to robust classifiers by avoiding non-robust features, and its algorithmic scalability, generality, and ease of integration into existing deep learning frameworks make it a valuable tool.",1
"Designing a single neural network architecture that performs competitively across a range of molecule property prediction tasks remains largely an open challenge, and its solution may unlock a widespread use of deep learning in the drug discovery industry. To move towards this goal, we propose Molecule Attention Transformer (MAT). Our key innovation is to augment the attention mechanism in Transformer using inter-atomic distances and the molecular graph structure. Experiments show that MAT performs competitively on a diverse set of molecular prediction tasks. Most importantly, with a simple self-supervised pretraining, MAT requires tuning of only a few hyperparameter values to achieve state-of-the-art performance on downstream tasks. Finally, we show that attention weights learned by MAT are interpretable from the chemical point of view.",0
"Creating a neural network architecture that can effectively handle a variety of molecule property prediction tasks is a challenge that has yet to be fully resolved. Success in this area could greatly expand the use of deep learning in the drug discovery industry. To address this challenge, we introduce the Molecule Attention Transformer (MAT), which enhances the Transformer's attention mechanism with inter-atomic distances and molecular graph structure. Our experiments demonstrate that MAT performs well across a range of molecular prediction tasks. Most notably, with a straightforward self-supervised pretraining, MAT only requires small adjustments to hyperparameters to achieve top-notch results in downstream tasks. Additionally, we demonstrate that the attention weights learned by MAT are easily interpreted from a chemical perspective.",1
"The long-tail distribution of the visual world poses great challenges for deep learning based classification models on how to handle the class imbalance problem. Existing solutions usually involve class-balancing strategies, e.g., by loss re-weighting, data re-sampling, or transfer learning from head- to tail-classes, but most of them adhere to the scheme of jointly learning representations and classifiers. In this work, we decouple the learning procedure into representation learning and classification, and systematically explore how different balancing strategies affect them for long-tailed recognition. The findings are surprising: (1) data imbalance might not be an issue in learning high-quality representations; (2) with representations learned with the simplest instance-balanced (natural) sampling, it is also possible to achieve strong long-tailed recognition ability by adjusting only the classifier. We conduct extensive experiments and set new state-of-the-art performance on common long-tailed benchmarks like ImageNet-LT, Places-LT and iNaturalist, showing that it is possible to outperform carefully designed losses, sampling strategies, even complex modules with memory, by using a straightforward approach that decouples representation and classification. Our code is available at https://github.com/facebookresearch/classifier-balancing.",0
"Deep learning based classification models face significant challenges in handling the class imbalance problem caused by the long-tail distribution of the visual world. Common solutions involve class-balancing strategies such as data re-sampling, loss re-weighting, or transfer learning from head- to tail-classes. However, these strategies usually involve jointly learning representations and classifiers. In this study, we have decoupled the learning process into representation learning and classification and explored how different balancing strategies affect them in long-tailed recognition. Our findings suggest that data imbalance is not an issue in learning high-quality representations, and it is possible to achieve strong long-tailed recognition ability by adjusting only the classifier with representations learned with the simplest instance-balanced (natural) sampling. We have conducted extensive experiments and set new state-of-the-art performance on common long-tailed benchmarks like ImageNet-LT, Places-LT, and iNaturalist, demonstrating that a straightforward approach that decouples representation and classification can outperform carefully designed losses, sampling strategies, and even complex modules with memory. Our code is available at https://github.com/facebookresearch/classifier-balancing.",1
"Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes as well as capturing temporal patterns. The node embeddings, which are now functions of time, should represent both the static node features and the evolving topological structures. Moreover, node and topological features can be temporal as well, whose patterns the node embeddings should also capture. We propose the temporal graph attention (TGAT) layer to efficiently aggregate temporal-topological neighborhood features as well as to learn the time-feature interactions. For TGAT, we use the self-attention mechanism as building block and develop a novel functional time encoding technique based on the classical Bochner's theorem from harmonic analysis. By stacking TGAT layers, the network recognizes the node embeddings as functions of time and is able to inductively infer embeddings for both new and observed nodes as the graph evolves. The proposed approach handles both node classification and link prediction task, and can be naturally extended to include the temporal edge features. We evaluate our method with transductive and inductive tasks under temporal settings with two benchmark and one industrial dataset. Our TGAT model compares favorably to state-of-the-art baselines as well as the previous temporal graph embedding approaches.",0
"To enable scalable machine learning on real-world dynamic networks, it is crucial to perform inductive representation learning on temporal graphs. Such graphs require the ability to handle new nodes and capture temporal patterns, with node embeddings representing both static node features and evolving topological structures. These features can also be temporal, necessitating the capturing of their patterns in node embeddings. To facilitate this, we propose the use of the temporal graph attention (TGAT) layer, which can efficiently aggregate temporal-topological neighborhood features and learn time-feature interactions. This is achieved through the use of a self-attention mechanism as a building block and a novel functional time encoding technique based on Bochner's theorem from harmonic analysis. By stacking TGAT layers, the network can recognize node embeddings as functions of time and infer embeddings for both new and observed nodes as the graph evolves. This approach can handle both node classification and link prediction tasks and can be extended to include temporal edge features. We evaluated our method on two benchmark and one industrial dataset using transductive and inductive tasks under temporal settings and found that our TGAT model outperforms state-of-the-art baselines and previous temporal graph embedding approaches.",1
"An important problem in multiview representation learning is finding the optimal combination of views with respect to the specific task at hand. To this end, we introduce NAM: a Neural Attentive Multiview machine that learns multiview item representations and similarity by employing a novel attention mechanism. NAM harnesses multiple information sources and automatically quantifies their relevancy with respect to a supervised task. Finally, a very practical advantage of NAM is its robustness to the case of dataset with missing views. We demonstrate the effectiveness of NAM for the task of movies and app recommendations. Our evaluations indicate that NAM outperforms single view models as well as alternative multiview methods on item recommendations tasks, including cold-start scenarios.",0
"Finding the optimal combination of views is a crucial challenge in multiview representation learning for achieving specific goals. In order to tackle this issue, we have developed NAM, a Neural Attentive Multiview system that employs a unique attention mechanism to acquire multiview item representations and similarity. NAM leverages several information sources and evaluates their significance in relation to a supervised task. Additionally, NAM has the practical advantage of being resilient to missing views in datasets. In our experiments, we demonstrate the efficacy of NAM for making movie and app recommendations. Our findings indicate that NAM surpasses single view models and other multiview techniques in item recommendation tasks, including cold-start scenarios.",1
"The information bottleneck principle provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label while minimizing the amount of other, excess information in the representation. The original formulation, however, requires labeled data to identify the superfluous information. In this work, we extend this ability to the multi-view unsupervised setting, where two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and label-limited versions of the MIR-Flickr dataset. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to common unsupervised approaches for representation learning.",0
"The principle of the information bottleneck is a method for learning how to represent information by training an encoder to keep only the relevant information for predicting a label while minimizing the extra information in the representation. However, the original formulation requires labeled data to identify the unnecessary information. In this study, we expand this ability to the unsupervised multi-view setting, where two views of the same entity are given but the label is unknown. This allows us to identify excess information as that which is not shared by both views. A new multi-view model is defined based on theoretical analysis that produces excellent results on the Sketchy dataset and label-limited versions of the MIR-Flickr dataset. We also extend our theory to the single-view setting by utilizing standard data augmentation techniques, demonstrating improved generalization abilities compared to common unsupervised approaches for representation learning.",1
"Most existing 3D CNNs for video representation learning are clip-based methods, and thus do not consider video-level temporal evolution of spatio-temporal features. In this paper, we propose Video-level 4D Convolutional Neural Networks, referred as V4D, to model the evolution of long-range spatio-temporal representation with 4D convolutions, and at the same time, to preserve strong 3D spatio-temporal representation with residual connections. Specifically, we design a new 4D residual block able to capture inter-clip interactions, which could enhance the representation power of the original clip-level 3D CNNs. The 4D residual blocks can be easily integrated into the existing 3D CNNs to perform long-range modeling hierarchically. We further introduce the training and inference methods for the proposed V4D. Extensive experiments are conducted on three video recognition benchmarks, where V4D achieves excellent results, surpassing recent 3D CNNs by a large margin.",0
"The majority of current 3D CNNs utilized for video representation learning are based on clips and do not take into account the temporal evolution of spatio-temporal features at the video level. Our paper introduces Video-level 4D Convolutional Neural Networks (V4D), which utilize 4D convolutions to model the evolution of long-range spatio-temporal representation while maintaining strong 3D spatio-temporal representation through residual connections. We have developed a new 4D residual block that captures inter-clip interactions to enhance the representation power of clip-level 3D CNNs. These blocks can be easily integrated into existing 3D CNNs for hierarchical long-range modeling. We also present the training and inference methods for V4D. We conducted extensive experiments on three video recognition benchmarks, where V4D outperformed recent 3D CNNs by a significant margin.",1
"Metric-based meta-learning techniques have successfully been applied to few-shot classification problems. In this paper, we propose to leverage cross-modal information to enhance metric-based few-shot learning methods. Visual and semantic feature spaces have different structures by definition. For certain concepts, visual features might be richer and more discriminative than text ones. While for others, the inverse might be true. Moreover, when the support from visual information is limited in image classification, semantic representations (learned from unsupervised text corpora) can provide strong prior knowledge and context to help learning. Based on these two intuitions, we propose a mechanism that can adaptively combine information from both modalities according to new image categories to be learned. Through a series of experiments, we show that by this adaptive combination of the two modalities, our model outperforms current uni-modality few-shot learning methods and modality-alignment methods by a large margin on all benchmarks and few-shot scenarios tested. Experiments also show that our model can effectively adjust its focus on the two modalities. The improvement in performance is particularly large when the number of shots is very small.",0
"Few-shot classification problems have been successfully tackled using metric-based meta-learning techniques. This paper proposes an approach to enhance these methods by exploiting cross-modal information. Visual and semantic feature spaces differ in structure, with some concepts being better represented by visual features and others by text features. When visual information is limited, semantic representations learned from unsupervised text corpora can provide valuable context and prior knowledge for learning. To combine these modalities, we introduce a mechanism that adapts to new image categories. Our experiments demonstrate that our approach outperforms uni-modality few-shot learning methods and modality-alignment methods by a large margin across all benchmarks and few-shot scenarios. Our model is also effective in adjusting its focus on the two modalities, with significant improvements in performance when the number of shots is minimal.",1
"Unsupervised text encoding models have recently fueled substantial progress in NLP. The key idea is to use neural networks to convert words in texts to vector space representations based on word positions in a sentence and their contexts, which are suitable for end-to-end training of downstream tasks. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as POIs into models. A general-purpose representation model for space is valuable for a multitude of tasks. However, no such general model exists to date beyond simply applying discretization or feed-forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for location encoding and is critical for recognizing places and for path-integration. Therefore, we propose a representation learning model called Space2Vec to encode the absolute positions and spatial relationships of places. We conduct experiments on two real-world geographic data for two different tasks: 1) predicting types of POIs given their positions and context, 2) image classification leveraging their geo-locations. Results show that because of its multi-scale representations, Space2Vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed-forward nets, and tile embedding approaches for location modeling and image classification tasks. Detailed analysis shows that all baselines can at most well handle distribution at one scale but show poor performances in other scales. In contrast, Space2Vec's multi-scale representation can handle distributions at different scales.",0
"Recent progress in NLP has been significantly influenced by unsupervised text encoding models that use neural networks to convert words in texts to vector space representations. These representations are based on word positions in a sentence and their contexts, and they are suitable for end-to-end training of downstream tasks. Spatial analysis also faces a similar situation, where absolute positions and spatial contexts of geographic objects need to be considered for modeling. Despite the value of a general-purpose representation model for space, no such model exists to date beyond discretization or feed-forward nets to coordinates. Little effort has been put into jointly modeling distributions with vastly different characteristics that commonly emerge from GIS data. However, Nobel Prize-winning Neuroscience research suggests that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for location encoding and is critical for recognizing places and for path-integration. Based on this, we propose a representation learning model called Space2Vec that can encode the absolute positions and spatial relationships of places. Our experiments on two real-world geographic datasets for two different tasks demonstrate that Space2Vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed-forward nets, and tile embedding approaches for location modeling and image classification tasks. This is because Space2Vec's multi-scale representation can handle distributions at different scales, whereas all baselines can only handle distributions at one scale at most.",1
"Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al. (2019) demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. However, in many practical settings, one might have access to a limited amount of supervision, for example through manual labeling of (some) factors of variation in a few training examples. In this paper, we investigate the impact of such supervision on state-of-the-art disentanglement methods and perform a large scale study, training over 52000 models under well-defined and reproducible experimental conditions. We observe that a small number of labeled examples (0.01--0.5\% of the data set), with potentially imprecise and incomplete labels, is sufficient to perform model selection on state-of-the-art unsupervised models. Further, we investigate the benefit of incorporating supervision into the training process. Overall, we empirically validate that with little and imprecise supervision it is possible to reliably learn disentangled representations.",0
"The challenge of acquiring disentangled representations is a fundamental issue in representation learning. A recent study by Locatello et al. (2019) established that unsupervised disentanglement learning without inductive biases cannot be achieved, and current unsupervised techniques and inductive biases do not consistently produce disentangled representations. However, practical situations may offer limited supervision, such as manual labeling of some factors of variation in a few training instances. This study explores the impact of such supervision on cutting-edge disentanglement techniques, conducting a comprehensive investigation with over 52,000 models trained under well-defined and reproducible experimental conditions. Results show that a small number of labeled examples (0.01-0.5% of the dataset), even with imprecise and incomplete labels, is sufficient for model selection on state-of-the-art unsupervised models. Additionally, the study evaluates the benefits of incorporating supervision into the training process. Taken together, the findings provide empirical evidence that disentangled representations can be reliably learned with little and imprecise supervision.",1
"Disentangled representations have recently been shown to improve fairness, data efficiency and generalisation in simple supervised and reinforcement learning tasks. To extend the benefits of disentangled representations to more complex domains and practical applications, it is important to enable hyperparameter tuning and model selection of existing unsupervised approaches without requiring access to ground truth attribute labels, which are not available for most datasets. This paper addresses this problem by introducing a simple yet robust and reliable method for unsupervised disentangled model selection. Our approach, Unsupervised Disentanglement Ranking (UDR), leverages the recent theoretical results that explain why variational autoencoders disentangle (Rolinek et al, 2019), to quantify the quality of disentanglement by performing pairwise comparisons between trained model representations. We show that our approach performs comparably to the existing supervised alternatives across 5,400 models from six state of the art unsupervised disentangled representation learning model classes. Furthermore, we show that the ranking produced by our approach correlates well with the final task performance on two different domains.",0
"The utilization of disentangled representations has demonstrated advantages in fairness, data efficiency, and generalization in simple supervised and reinforcement learning tasks. However, to apply this technique to more complex domains and practical applications, it is crucial to allow hyperparameter tuning and model selection of existing unsupervised approaches. Unfortunately, most datasets lack ground truth attribute labels, making this task impractical. This paper presents a solution to this problem by proposing a straightforward, robust, and reliable method for unsupervised disentangled model selection called Unsupervised Disentanglement Ranking (UDR). UDR leverages the recent theoretical findings that explain why variational autoencoders disentangle to quantify the quality of disentanglement through pairwise comparisons between trained model representations. The study demonstrates that UDR performs similarly to the existing supervised alternatives across 5,400 models from six state-of-the-art unsupervised disentangled representation learning model classes. Additionally, the ranking generated by UDR correlates well with the final task performance on two different domains.",1
"We propose the first qualitative hypothesis characterizing the behavior of visual transformation based self-supervision, called the VTSS hypothesis. Given a dataset upon which a self-supervised task is performed while predicting instantiations of a transformation, the hypothesis states that if the predicted instantiations of the transformations are already present in the dataset, then the representation learned will be less useful. The hypothesis was derived by observing a key constraint in the application of self-supervision using a particular transformation. This constraint, which we term the transformation conflict for this paper, forces a network learn degenerative features thereby reducing the usefulness of the representation. The VTSS hypothesis helps us identify transformations that have the potential to be effective as a self-supervision task. Further, it helps to generally predict whether a particular transformation based self-supervision technique would be effective or not for a particular dataset. We provide extensive evaluations on CIFAR 10, CIFAR 100, SVHN and FMNIST confirming the hypothesis and the trends it predicts. We also propose novel cost-effective self-supervision techniques based on translation and scale, which when combined with rotation outperforms all transformations applied individually. Overall, this paper aims to shed light on the phenomenon of visual transformation based self-supervision.",0
"The VTSS hypothesis is our qualitative proposal for characterizing the behavior of self-supervision based on visual transformation. Essentially, the hypothesis suggests that if a self-supervised task predicts instantiations of a transformation that are already present in the dataset, the learned representation will be less useful. This hypothesis is based on our observation of a constraint that we call the transformation conflict, which causes the network to learn degenerative features, ultimately reducing the usefulness of the representation. By identifying effective transformations for self-supervision and predicting the effectiveness of a self-supervision technique for a particular dataset, the VTSS hypothesis can guide the development of cost-effective self-supervision techniques. To confirm the hypothesis, we conducted extensive evaluations on CIFAR 10, CIFAR 100, SVHN, and FMNIST, proposing new self-supervision techniques based on translation and scale that outperform individual transformations when combined with rotation. Our paper aims to illuminate the phenomenon of visual transformation based self-supervision.",1
"Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN (Geometric Graph Convolutional Networks), to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs. Code is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.",0
"MPNNs have been successful in learning representations on graphs for various real-world applications. However, MPNNs' aggregators have two fundamental weaknesses that limit their ability to represent graph-structured data: the loss of structural information of nodes in neighborhoods and the inability to capture long-range dependencies in disassortative graphs. These weaknesses have been overlooked by most studies. Using observations from classical neural networks and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome these limitations. The underlying idea is that aggregation on a graph can benefit from a continuous space. Our proposed scheme is permutation-invariant and comprises three modules: node embedding, structural neighborhood, and bi-level aggregation. We also introduce Geom-GCN (Geometric Graph Convolutional Networks), an implementation of the scheme in graph convolutional networks, for transductive learning on graphs. Experimental results demonstrate that Geom-GCN achieved state-of-the-art performance on various open datasets of graphs. Our code is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.",1
"Supervised classification and representation learning are two widely used classes of methods to analyze multivariate images. Although complementary, these methods have been scarcely considered jointly in a hierarchical modeling. In this paper, a method coupling these two approaches is designed using a matrix cofactorization formulation. Each task is modeled as a factorization matrix problem and a term relating both coding matrices is then introduced to drive an appropriate coupling. The link can be interpreted as a clustering operation over a low-dimensional representation vectors. The attribution vectors of the clustering are then used as features vectors for the classification task, i.e., the coding vectors of the corresponding factorization problem. A proximal gradient descent algorithm, ensuring convergence to a critical point of the objective function, is then derived to solve the resulting non-convex non-smooth optimization problem. An evaluation of the proposed method is finally conducted both on synthetic and real data in the specific context of hyperspectral image interpretation, unifying two standard analysis techniques, namely unmixing and classification.",0
"Two prevalent methods used to analyze multivariate images are supervised classification and representation learning. However, these methods have rarely been combined in a hierarchical modeling. This paper presents a new approach that integrates these two methods using a matrix cofactorization formulation. Each task is represented as a factorization matrix problem, and a term that connects the coding matrices is introduced to facilitate coupling. This link can be interpreted as a clustering operation over low-dimensional representation vectors. The clustering's attribution vectors are then utilized as feature vectors for the classification task, i.e., the coding vectors of the corresponding factorization problem. A proximal gradient descent algorithm is developed to solve the resulting non-convex non-smooth optimization problem, which guarantees convergence to a critical point of the objective function. Finally, the proposed method is evaluated on both synthetic and real data in the context of hyperspectral image interpretation, merging two standard analysis techniques, unmixing and classification.",1
"Graph convolutional networks (GCNs) are a widely used method for graph representation learning. We investigate the power of GCNs, as a function of their number of layers, to distinguish between different random graph models on the basis of the embeddings of their sample graphs. In particular, the graph models that we consider arise from graphons, which are the most general possible parameterizations of infinite exchangeable graph models and which are the central objects of study in the theory of dense graph limits. We exhibit an infinite class of graphons that are well-separated in terms of cut distance and are indistinguishable by a GCN with nonlinear activation functions coming from a certain broad class if its depth is at least logarithmic in the size of the sample graph. These results theoretically match empirical observations of several prior works. Finally, we show a converse result that for pairs of graphons satisfying a degree profile separation property, a very simple GCN architecture suffices for distinguishability. To prove our results, we exploit a connection to random walks on graphs.",0
"The use of graph convolutional networks (GCNs) for learning graph representations is widespread. Our study aims to determine the effectiveness of GCNs in distinguishing between various random graph models based on their sample graph embeddings, as a function of the number of layers in the network. The graph models under consideration are derived from graphons, which are the most all-encompassing parameterizations of infinite exchangeable graph models and are fundamental in dense graph limits theory. We present a set of graphons that are well-separated in terms of cut distance and cannot be distinguished by a GCN, provided that the depth of the network is at least logarithmic with respect to the sample graph size. These findings align with previous empirical observations. We also demonstrate a converse result, where graphons possessing a degree profile separation characteristic can be distinguished using a simple GCN architecture. Random walks on graphs serve as the basis for our proof.",1
"Clustering is a fundamental task in data analysis. Recently, deep clustering, which derives inspiration primarily from deep learning approaches, achieves state-of-the-art performance and has attracted considerable attention. Current deep clustering methods usually boost the clustering results by means of the powerful representation ability of deep learning, e.g., autoencoder, suggesting that learning an effective representation for clustering is a crucial requirement. The strength of deep clustering methods is to extract the useful representations from the data itself, rather than the structure of data, which receives scarce attention in representation learning. Motivated by the great success of Graph Convolutional Network (GCN) in encoding the graph structure, we propose a Structural Deep Clustering Network (SDCN) to integrate the structural information into deep clustering. Specifically, we design a delivery operator to transfer the representations learned by autoencoder to the corresponding GCN layer, and a dual self-supervised mechanism to unify these two different deep neural architectures and guide the update of the whole model. In this way, the multiple structures of data, from low-order to high-order, are naturally combined with the multiple representations learned by autoencoder. Furthermore, we theoretically analyze the delivery operator, i.e., with the delivery operator, GCN improves the autoencoder-specific representation as a high-order graph regularization constraint and autoencoder helps alleviate the over-smoothing problem in GCN. Through comprehensive experiments, we demonstrate that our propose model can consistently perform better over the state-of-the-art techniques.",0
"Data analysis involves a crucial task known as clustering. Deep clustering, which draws inspiration from deep learning techniques, has recently gained significant attention due to its state-of-the-art performance. Typically, deep clustering methods use the robust representation capabilities of deep learning, such as the autoencoder. This highlights the importance of learning effective representations for clustering. The strength of deep clustering methods lies in their ability to extract useful representations from the data itself, rather than focusing on the data structure, which is often neglected in representation learning. To incorporate the graph structure of data, we propose the Structural Deep Clustering Network (SDCN), which builds on the success of the Graph Convolutional Network (GCN). We achieve this by designing a delivery operator that transfers the autoencoder's learned representations to the appropriate GCN layer and a dual self-supervised mechanism that harmonizes the two neural architectures and guides the model's update. Our approach seamlessly combines the data's multiple structures, from low-order to high-order, with the multiple representations learned by the autoencoder. We also provide a theoretical analysis of the delivery operator, demonstrating how GCN enhances the autoencoder-specific representation as a high-order graph regularization constraint, and how the autoencoder helps mitigate the over-smoothing issue in GCN. Our comprehensive experiments show that our proposed model consistently outperforms state-of-the-art techniques.",1
"Anomaly detection on attributed networks aims at finding nodes whose patterns deviate significantly from the majority of reference nodes, which is pervasive in many applications such as network intrusion detection and social spammer detection. However, most existing methods neglect the complex cross-modality interactions between network structure and node attribute. In this paper, we propose a deep joint representation learning framework for anomaly detection through a dual autoencoder (AnomalyDAE), which captures the complex interactions between network structure and node attribute for high-quality embeddings. Specifically, AnomalyDAE consists of a structure autoencoder and an attribute autoencoder to learn both node embedding and attribute embedding jointly in latent space. Moreover, attention mechanism is employed in structure encoder to learn the importance between a node and its neighbors for an effective capturing of structure pattern, which is important to anomaly detection. Besides, by taking both the node embedding and attribute embedding as inputs of attribute decoder, the cross-modality interactions between network structure and node attribute are learned during the reconstruction of node attribute. Finally, anomalies can be detected by measuring the reconstruction errors of nodes from both the structure and attribute perspectives. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed method.",0
"The detection of anomalies in attributed networks is crucial in various applications, such as network intrusion detection and social spammer detection. However, current methods often overlook the intricate interactions between the network structure and node attribute. In this study, we introduce a deep joint representation learning framework for anomaly detection using a dual autoencoder (AnomalyDAE). This approach captures the complex interactions between network structure and node attribute to generate high-quality embeddings. AnomalyDAE comprises a structure autoencoder and an attribute autoencoder, which jointly learn the node embedding and attribute embedding in the latent space. Additionally, the structure encoder includes an attention mechanism to determine the significance of a node and its neighbors for effective pattern capturing. The attribute decoder uses both the node embedding and attribute embedding as inputs to learn the cross-modality interactions between the network structure and node attribute while reconstructing node attribute. The detection of anomalies is achieved by calculating the reconstruction errors of nodes from both the structure and attribute perspectives. The effectiveness of the proposed method is validated through extensive experiments on real-world datasets.",1
"Considering the inherent stochasticity and uncertainty, predicting future video frames is exceptionally challenging. In this work, we study the problem of video prediction by combining interpretability of stochastic state space models and representation learning of deep neural networks. Our model builds upon an variational encoder which transforms the input video into a latent feature space and a Luenberger-type observer which captures the dynamic evolution of the latent features. This enables the decomposition of videos into static features and dynamics in an unsupervised manner. By deriving the stability theory of the nonlinear Luenberger-type observer, the hidden states in the feature space become insensitive with respect to the initial values, which improves the robustness of the overall model. Furthermore, the variational lower bound on the data log-likelihood can be derived to obtain the tractable posterior prediction distribution based on the variational principle. Finally, the experiments such as the Bouncing Balls dataset and the Pendulum dataset are provided to demonstrate the proposed model outperforms concurrent works.",0
"Forecasting future video frames is an extremely difficult task due to the inherent unpredictability and uncertainty. This study combines the interpretability of stochastic state space models with the deep neural network's representation learning to address the problem of video prediction. Our model uses a variational encoder to transform the input video into a latent feature space, and a Luenberger-type observer to capture the dynamic evolution of latent features. This unsupervised process enables us to break down videos into static features and dynamics. The model's robustness is enhanced by deriving the stability theory of the nonlinear Luenberger-type observer, allowing the hidden states in the feature space to become insensitive to initial values. The variational principle is used to derive the variational lower bound on the data log-likelihood, which provides the tractable posterior prediction distribution. We provide experiments, including the Bouncing Balls dataset and the Pendulum dataset, to demonstrate that our proposed model outperforms other concurrent works.",1
"In this paper, we propose a new video representation learning method, named Temporal Squeeze (TS) pooling, which can extract the essential movement information from a long sequence of video frames and map it into a set of few images, named Squeezed Images. By embedding the Temporal Squeeze pooling as a layer into off-the-shelf Convolution Neural Networks (CNN), we design a new video classification model, named Temporal Squeeze Network (TeSNet). The resulting Squeezed Images contain the essential movement information from the video frames, corresponding to the optimization of the video classification task. We evaluate our architecture on two video classification benchmarks, and the results achieved are compared to the state-of-the-art.",0
"The objective of this paper is to present a novel technique for learning video representation, called Temporal Squeeze (TS) pooling. This method effectively captures the crucial movement information from a lengthy sequence of video frames and converts it into a small set of images, known as Squeezed Images. By adding the Temporal Squeeze pooling as a layer to pre-existing Convolution Neural Networks (CNN), we have created a new video classification model, named Temporal Squeeze Network (TeSNet). The Squeezed Images obtained from this process accurately capture the essential movement information from the video frames, leading to optimal video classification results. We tested our approach on two video classification benchmarks and compared our results with the state-of-the-art.",1
"Despite the success of Generative Adversarial Networks (GANs) in image synthesis, there lacks enough understanding on what generative models have learned inside the deep generative representations and how photo-realistic images are able to be composed of the layer-wise stochasticity introduced in recent GANs. In this work, we show that highly-structured semantic hierarchy emerges as variation factors from synthesizing scenes from the generative representations in state-of-the-art GAN models, like StyleGAN and BigGAN. By probing the layer-wise representations with a broad set of semantics at different abstraction levels, we are able to quantify the causality between the activations and semantics occurring in the output image. Such a quantification identifies the human-understandable variation factors learned by GANs to compose scenes. The qualitative and quantitative results further suggest that the generative representations learned by the GANs with layer-wise latent codes are specialized to synthesize different hierarchical semantics: the early layers tend to determine the spatial layout and configuration, the middle layers control the categorical objects, and the later layers finally render the scene attributes as well as color scheme. Identifying such a set of manipulatable latent variation factors facilitates semantic scene manipulation.",0
"Although Generative Adversarial Networks (GANs) have been successful in generating realistic images, there is still a lack of understanding regarding the deep generative representations of these models and how they create photo-realistic images from stochastic layers. Our study demonstrates that GAN models such as StyleGAN and BigGAN produce highly-structured semantic hierarchies when synthesizing scenes from generative representations. By analyzing the layer-wise representations with a variety of semantics at different levels of abstraction, we can determine the causal relationship between the activations and semantics in the output image. This quantification reveals the variation factors learned by GANs to compose scenes, which can be manipulated to modify the semantics of the generated scene. Our results show that GANs with layer-wise latent codes specialize in synthesizing different hierarchical semantics, with early layers determining spatial layout, middle layers controlling categorical objects, and later layers rendering scene attributes and color schemes. Identifying these manipulatable latent variation factors enables semantic scene manipulation.",1
"Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance. Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.",0
"Formulating real-world sequential decision-making problems as optimal control with high-dimensional observations and unknown dynamics is common. A promising strategy is to embed high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, and use it for control in the latent space. A significant challenge is to learn a representation suitable for existing control algorithms. This paper focuses on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR), by formulating and analyzing the representation learning problem from an optimal control perspective. Three principles underlie the learned representation: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles correspond to a loss function that comprises three terms: prediction, consistency, and curvature (PCC). To make PCC tractable, an amortized variational bound for the PCC loss function is derived. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm offers significantly more stable and reproducible training and leads to superior control performance. Further ablation studies support the importance of all three PCC components for learning a good latent space for control.",1
"Canonical Correlation Analysis (CCA) is a linear representation learning method that seeks maximally correlated variables in multi-view data. Non-linear CCA extends this notion to a broader family of transformations, which are more powerful in many real-world applications. Given the joint probability, the Alternating Conditional Expectation (ACE) algorithm provides an optimal solution to the non-linear CCA problem. However, it suffers from limited performance and an increasing computational burden when only a finite number of samples is available. In this work we introduce an information-theoretic compressed representation framework for the non-linear CCA problem (CRCCA), which extends the classical ACE approach. Our suggested framework seeks compact representations of the data that allow a maximal level of correlation. This way we control the trade-off between the flexibility and the complexity of the model. CRCCA provides theoretical bounds and optimality conditions, as we establish fundamental connections to rate-distortion theory, the information bottleneck and remote source coding. In addition, it allows a soft dimensionality reduction, as the compression level is determined by the mutual information between the original noisy data and the extracted signals. Finally, we introduce a simple implementation of the CRCCA framework, based on lattice quantization.",0
"Canonical Correlation Analysis (CCA) is a technique used in linear representation learning to identify variables that are highly correlated in multi-view data. Non-linear CCA expands this concept to include a wider range of transformations, making it more effective in various real-world scenarios. The Alternating Conditional Expectation (ACE) algorithm is used to solve the non-linear CCA problem by determining the optimal solution from joint probability data. However, its performance is limited when working with a finite number of samples, and it becomes computationally intensive. To address this, we present an information-theoretic compressed representation framework for non-linear CCA (CRCCA), which builds on the ACE approach. The framework aims to achieve maximum correlation by generating compact representations of the data, balancing flexibility and complexity. CRCCA has theoretical bounds and optimality conditions, and is connected to rate-distortion theory, information bottleneck, and remote source coding. Furthermore, it allows for soft dimensionality reduction by adjusting the compression level based on the mutual information between the original noisy data and the extracted signals. Finally, we propose a simple implementation of the CRCCA framework using lattice quantization.",1
"In existing visual representation learning tasks, deep convolutional neural networks (CNNs) are often trained on images annotated with single tags, such as ImageNet. However, a single tag cannot describe all important contents of one image, and some useful visual information may be wasted during training. In this work, we propose to train CNNs from images annotated with multiple tags, to enhance the quality of visual representation of the trained CNN model. To this end, we build a large-scale multi-label image database with 18M images and 11K categories, dubbed Tencent ML-Images. We efficiently train the ResNet-101 model with multi-label outputs on Tencent ML-Images, taking 90 hours for 60 epochs, based on a large-scale distributed deep learning framework,i.e.,TFplus. The good quality of the visual representation of the Tencent ML-Images checkpoint is verified through three transfer learning tasks, including single-label image classification on ImageNet and Caltech-256, object detection on PASCAL VOC 2007, and semantic segmentation on PASCAL VOC 2012. The Tencent ML-Images database, the checkpoints of ResNet-101, and all the training codehave been released at https://github.com/Tencent/tencent-ml-images. It is expected to promote other vision tasks in the research and industry community.",0
"In current tasks that involve learning visual representations, deep convolutional neural networks (CNNs) are typically trained on images annotated with only one tag, such as ImageNet. However, a single tag is insufficient to describe all the important contents of an image and useful visual information may be overlooked during training. This study proposes to enhance the quality of visual representation of trained CNN models by training them on images annotated with multiple tags. To achieve this, a large-scale multi-label image database called Tencent ML-Images was created, comprising 18M images and 11K categories. The ResNet-101 model was efficiently trained using multi-label outputs on this database, taking 90 hours for 60 epochs, using a large-scale distributed deep learning framework called TFplus. The visual representation quality was verified through three transfer learning tasks, including single-label image classification on ImageNet and Caltech-256, object detection on PASCAL VOC 2007, and semantic segmentation on PASCAL VOC 2012. The Tencent ML-Images database, ResNet-101 checkpoints, and all training code are available at https://github.com/Tencent/tencent-ml-images, and are expected to benefit vision tasks in the research and industry communities.",1
"In graph instance representation learning, both the diverse graph instance sizes and the graph node orderless property have been the major obstacles that render existing representation learning models fail to work. In this paper, we will examine the effectiveness of GRAPH-BERT on graph instance representation learning, which was designed for node representation learning tasks originally. To adapt GRAPH-BERT to the new problem settings, we re-design it with a segmented architecture instead, which is also named as SEG-BERT (Segmented GRAPH-BERT) for reference simplicity in this paper. SEG-BERT involves no node-order-variant inputs or functional components anymore, and it can handle the graph node orderless property naturally. What's more, SEG-BERT has a segmented architecture and introduces three different strategies to unify the graph instance sizes, i.e., full-input, padding/pruning and segment shifting, respectively. SEG-BERT is pre-trainable in an unsupervised manner, which can be further transferred to new tasks directly or with necessary fine-tuning. We have tested the effectiveness of SEG-BERT with experiments on seven graph instance benchmark datasets, and SEG-BERT can out-perform the comparison methods on six out of them with significant performance advantages.",0
"The main challenges in graph instance representation learning are the varying sizes of the instances and the unordered nature of the graph nodes, which existing representation learning models struggle to address. This study aims to evaluate the effectiveness of GRAPH-BERT, initially designed for node representation learning tasks, in graph instance representation learning. To cater to the new problem settings, we present SEG-BERT (Segmented GRAPH-BERT), a modified version of GRAPH-BERT with a segmented architecture that handles the graph node orderless property naturally. SEG-BERT adopts three different strategies to unify the graph instance sizes: full-input, padding/pruning, and segment shifting. It is pre-trainable in an unsupervised manner and can be transferred to new tasks with fine-tuning. In experiments conducted on seven graph instance benchmark datasets, SEG-BERT outperforms comparison methods on six of them with significant performance advantages.",1
"Graph neural networks (GNNs) have been widely used in representation learning on graphs and achieved state-of-the-art performance in tasks such as node classification and link prediction. However, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. In this paper, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which involve identifying useful connections between unconnected nodes on the original graph, while learning effective node representation on the new graphs in an end-to-end fashion. Graph Transformer layer, a core layer of GTNs, learns a soft selection of edge types and composite relations for generating useful multi-hop connections so-called meta-paths. Our experiments show that GTNs learn new graph structures, based on data and tasks without domain knowledge, and yield powerful node representation via convolution on the new graphs. Without domain-specific graph preprocessing, GTNs achieved the best performance in all three benchmark node classification tasks against the state-of-the-art methods that require pre-defined meta-paths from domain knowledge.",0
"Representation learning on graphs has been revolutionized by Graph Neural Networks (GNNs), which have achieved outstanding performance in tasks like node classification and link prediction. However, most GNNs are designed to learn node representations on fixed and homogeneous graphs, which poses limitations when learning representations on misspecified or heterogeneous graphs. Addressing this issue, this paper proposes Graph Transformer Networks (GTNs) that can generate new graph structures by identifying useful connections between unconnected nodes on the original graph. GTNs learn effective node representations on new graphs in an end-to-end manner, where the Graph Transformer layer plays a central role in learning soft selections of edge types and composite relations for generating useful multi-hop connections called meta-paths. Our experiments demonstrate that GTNs learn new graph structures without domain knowledge and achieve powerful node representation via convolution on the new graphs, outperforming state-of-the-art methods that require pre-defined meta-paths from domain knowledge in all three benchmark node classification tasks, without requiring domain-specific graph preprocessing.",1
"Massive Open Online Courses (MOOCs) have become popular platforms for online learning. While MOOCs enable students to study at their own pace, this flexibility makes it easy for students to drop out of class. In this paper, our goal is to predict if a learner is going to drop out within the next week, given clickstream data for the current week. To this end, we present a multi-layer representation learning solution based on branch and bound (BB) algorithm, which learns from low-level clickstreams in an unsupervised manner, produces interpretable results, and avoids manual feature engineering. In experiments on Coursera data, we show that our model learns a representation that allows a simple model to perform similarly well to more complex, task-specific models, and how the BB algorithm enables interpretable results. In our analysis of the observed limitations, we discuss promising future directions.",0
"Online learning has grown in popularity due to Massive Open Online Courses (MOOCs). Although MOOCs provide students with the freedom to learn at their own pace, this flexibility also increases the likelihood of student dropouts. The objective of this study is to forecast whether a learner will withdraw from the course in the upcoming week, using clickstream data from the current week. We propose a multi-layer representation learning approach that employs the branch and bound (BB) algorithm. This method learns from low-level clickstreams in an unsupervised manner, avoids the need for manual feature engineering, and produces interpretable outcomes. Our experiments on Coursera data reveal that our model generates a representation that enables a simple model to perform similarly well as more complex, task-specific models. Furthermore, the BB algorithm facilitates the production of interpretable results. Finally, we discuss future directions to address the observed limitations.",1
"Recent findings in neuroscience suggest that the human brain represents information in a geometric structure (for instance, through conceptual spaces). In order to communicate, we flatten the complex representation of entities and their attributes into a single word or a sentence. In this paper we use graph convolutional networks to support the evolution of language and cooperation in multi-agent systems. Motivated by an image-based referential game, we propose a graph referential game with varying degrees of complexity, and we provide strong baseline models that exhibit desirable properties in terms of language emergence and cooperation. We show that the emerged communication protocol is robust, that the agents uncover the true factors of variation in the game, and that they learn to generalize beyond the samples encountered during training.",0
"New insights from neuroscience suggest that the human brain employs a geometric structure, such as conceptual spaces, to represent information. However, when communicating, we simplify this complex representation by condensing it into singular words or sentences. This study leverages graph convolutional networks to facilitate language and cooperation development in multi-agent systems. Using an image-based referential game as inspiration, we propose a graph referential game with varying complexities and establish strong baseline models that demonstrate favorable attributes in terms of language emergence and cooperation. Our results indicate that the communication protocol created is resilient, agents uncover the genuine factors of variation in the game, and they learn to generalize beyond the samples seen during training.",1
"The richness in the content of various information networks such as social networks and communication networks provides the unprecedented potential for learning high-quality expressive representations without external supervision. This paper investigates how to preserve and extract the abundant information from graph-structured data into embedding space in an unsupervised manner. To this end, we propose a novel concept, Graphical Mutual Information (GMI), to measure the correlation between input graphs and high-level hidden representations. GMI generalizes the idea of conventional mutual information computations from vector space to the graph domain where measuring mutual information from two aspects of node features and topological structure is indispensable. GMI exhibits several benefits: First, it is invariant to the isomorphic transformation of input graphs---an inevitable constraint in many existing graph representation learning algorithms; Besides, it can be efficiently estimated and maximized by current mutual information estimation methods such as MINE; Finally, our theoretical analysis confirms its correctness and rationality. With the aid of GMI, we develop an unsupervised learning model trained by maximizing GMI between the input and output of a graph neural encoder. Considerable experiments on transductive as well as inductive node classification and link prediction demonstrate that our method outperforms state-of-the-art unsupervised counterparts, and even sometimes exceeds the performance of supervised ones.",0
"This paper explores the potential of learning expressive representations from rich content available in various information networks, including social networks and communication networks, without external supervision. The focus is on extracting and preserving abundant information from graph-structured data in an unsupervised manner. The proposed concept, Graphical Mutual Information (GMI), measures the correlation between input graphs and high-level hidden representations. GMI extends the conventional mutual information computations from vector space to the graph domain, where measuring mutual information from node features and topological structure is crucial. GMI has several benefits, including invariance to isomorphic transformation and efficient estimation and maximization using current mutual information estimation methods. The unsupervised learning model developed using GMI is trained by maximizing GMI between the input and output of a graph neural encoder. The experimental results demonstrate that the proposed method outperforms state-of-the-art unsupervised counterparts and sometimes even exceeds the performance of supervised ones, in tasks such as transductive and inductive node classification and link prediction.",1
"In this paper, we tackle for the first time, the problem of self-supervised representation learning for free-hand sketches. This importantly addresses a common problem faced by the sketch community -- that annotated supervisory data are difficult to obtain. This problem is very challenging in that sketches are highly abstract and subject to different drawing styles, making existing solutions tailored for photos unsuitable. Key for the success of our self-supervised learning paradigm lies with our sketch-specific designs: (i) we propose a set of pretext tasks specifically designed for sketches that mimic different drawing styles, and (ii) we further exploit the use of a textual convolution network (TCN) in a dual-branch architecture for sketch feature learning, as means to accommodate the sequential stroke nature of sketches. We demonstrate the superiority of our sketch-specific designs through two sketch-related applications (retrieval and recognition) on a million-scale sketch dataset, and show that the proposed approach outperforms the state-of-the-art unsupervised representation learning methods, and significantly narrows the performance gap between with supervised representation learning.",0
"The focus of this paper is on solving the problem of self-supervised representation learning for free-hand sketches, which has not been addressed before. This is particularly important for the sketch community as obtaining annotated supervisory data is difficult. The challenge lies in the fact that sketches are highly abstract and have different drawing styles, making existing solutions designed for photos unsuitable. Our success in self-supervised learning is due to our sketch-specific designs, which include proposing pretext tasks that mimic different drawing styles and using a textual convolution network in a dual-branch architecture to accommodate the sequential stroke nature of sketches. Through two sketch-related applications, retrieval and recognition, on a million-scale sketch dataset, we demonstrate the superiority of our sketch-specific designs. We show that our proposed approach outperforms the state-of-the-art unsupervised representation learning methods and significantly narrows the performance gap between supervised and unsupervised representation learning.",1
"With the prevalence of RGB-D cameras, multi-modal video data have become more available for human action recognition. One main challenge for this task lies in how to effectively leverage their complementary information. In this work, we propose a Modality Compensation Network (MCN) to explore the relationships of different modalities, and boost the representations for human action recognition. We regard RGB/optical flow videos as source modalities, skeletons as auxiliary modality. Our goal is to extract more discriminative features from source modalities, with the help of auxiliary modality. Built on deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, our model bridges data from source and auxiliary modalities by a modality adaptation block to achieve adaptive representation learning, that the network learns to compensate for the loss of skeletons at test time and even at training time. We explore multiple adaptation schemes to narrow the distance between source and auxiliary modal distributions from different levels, according to the alignment of source and auxiliary data in training. In addition, skeletons are only required in the training phase. Our model is able to improve the recognition performance with source data when testing. Experimental results reveal that MCN outperforms state-of-the-art approaches on four widely-used action recognition benchmarks.",0
"Multi-modal video data has become more readily available for human action recognition due to the prevalence of RGB-D cameras. However, effectively utilizing the complementary information from these modalities is a major challenge. To address this, our work proposes a Modality Compensation Network (MCN) that explores the relationships between different modalities and enhances representations for human action recognition. We consider RGB/optical flow videos as source modalities and skeletons as an auxiliary modality to extract more discriminative features from the source modalities. Our model is built on deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, which bridge the data from the source and auxiliary modalities via a modality adaptation block to achieve adaptive representation learning. Our model learns to compensate for the loss of skeletons at test time and even at training time. We explore various adaptation schemes to narrow the distance between the source and auxiliary modal distributions at different levels, based on the alignment of source and auxiliary data during training. Furthermore, our model only requires skeletons in the training phase. Our results demonstrate that MCN outperforms state-of-the-art approaches on four widely-used action recognition benchmarks by improving recognition performance with source data during testing.",1
"Have you ever wondered how a song might sound if performed by a different artist? In this work, we propose SCM-GAN, an end-to-end non-parallel song conversion system powered by generative adversarial and transfer learning that allows users to listen to a selected target singer singing any song. SCM-GAN first separates songs into vocals and instrumental music using a U-Net network, then converts the vocal segments to the target singer using advanced CycleGAN-VC, before merging the converted vocals with their corresponding background music. SCM-GAN is first initialized with feature representations learned from a state-of-the-art voice-to-voice conversion and then trained on a dataset of non-parallel songs. Furthermore, SCM-GAN is evaluated against a set of metrics including global variance GV and modulation spectra MS on the 24 Mel-cepstral coefficients (MCEPs). Transfer learning improves the GV by 35% and the MS by 13% on average. A subjective comparison is conducted to test the user satisfaction with the quality and the naturalness of the conversion. Results show above par similarity between SCM-GAN's output and the target (70\% on average) as well as great naturalness of the converted songs.",0
"Have you ever pondered how a song would sound if sung by a different artist? Our proposal, SCM-GAN, is a non-parallel song conversion system that utilizes generative adversarial and transfer learning to provide users with the ability to listen to any song sung by a selected target singer. SCM-GAN utilizes a U-Net network to separate songs into vocals and instrumental music, then uses advanced CycleGAN-VC to convert the vocal segments to the target singer before merging them with their corresponding background music. The system is initialized with feature representations learned from a state-of-the-art voice-to-voice conversion and trained on a dataset of non-parallel songs. SCM-GAN is evaluated using metrics such as global variance GV and modulation spectra MS on the 24 Mel-cepstral coefficients (MCEPs). Transfer learning enhances the GV by 35% and the MS by 13% on average. To test user satisfaction with the quality and naturalness of the conversion, a subjective comparison is conducted. Results demonstrate above average similarity between SCM-GAN's output and the target (70% on average), as well as excellent naturalness of the converted songs.",1
"For a robot to perform complex manipulation tasks, it is necessary for it to have a good grasping ability. However, vision based robotic grasp detection is hindered by the unavailability of sufficient labelled data. Furthermore, the application of semi-supervised learning techniques to grasp detection is under-explored. In this paper, a semi-supervised learning based grasp detection approach has been presented, which models a discrete latent space using a Vector Quantized Variational AutoEncoder (VQ-VAE). To the best of our knowledge, this is the first time a Variational AutoEncoder (VAE) has been applied in the domain of robotic grasp detection. The VAE helps the model in generalizing beyond the Cornell Grasping Dataset (CGD) despite having a limited amount of labelled data by also utilizing the unlabelled data. This claim has been validated by testing the model on images, which are not available in the CGD. Along with this, we augment the Generative Grasping Convolutional Neural Network (GGCNN) architecture with the decoder structure used in the VQ-VAE model with the intuition that it should help to regress in the vector-quantized latent space. Subsequently, the model performs significantly better than the existing approaches which do not make use of unlabelled images to improve the grasp.",0
"To perform complex manipulation tasks, a robot must possess excellent grasping abilities. However, vision-based detection of robotic grasping is limited by the lack of adequate labeled data and the underexplored application of semi-supervised learning techniques. This paper presents a semi-supervised learning approach to grasp detection that utilizes a Vector Quantized Variational AutoEncoder (VQ-VAE) to model a discrete latent space. This is the first time a Variational AutoEncoder (VAE) has been employed in robotic grasp detection, and it helps the model generalize beyond the Cornell Grasping Dataset (CGD) by also utilizing unlabelled data. The model's efficacy has been verified by testing it on images not present in the CGD. Additionally, the Generative Grasping Convolutional Neural Network (GGCNN) architecture has been augmented with the decoder structure used in the VQ-VAE model to help regress in the vector-quantized latent space, resulting in superior performance compared to existing approaches that do not utilize unlabelled images to improve the grasp.",1
"This work combines Convolutional Neural Networks (CNNs), clustering via Self-Organizing Maps (SOMs) and Hebbian Learning to propose the building blocks of Convolutional Self-Organizing Neural Networks (CSNNs), which learn representations in an unsupervised and Backpropagation-free manner. Our approach replaces the learning of traditional convolutional layers from CNNs with the competitive learning procedure of SOMs and simultaneously learns local masks between those layers with separate Hebbian-like learning rules to overcome the problem of disentangling factors of variation when filters are learned through clustering. We investigate the learned representation by designing two simple models with our building blocks, achieving comparable performance to many methods which use Backpropagation, while we reach comparable performance on Cifar10 and give baseline performances on Cifar100, Tiny ImageNet and a small subset of ImageNet for Backpropagation-free methods.",0
"The aim of this work is to introduce Convolutional Self-Organizing Neural Networks (CSNNs), which are composed of Convolutional Neural Networks (CNNs), Self-Organizing Maps (SOMs) clustering, and Hebbian Learning. This approach enables the unsupervised learning of representations without the need for Backpropagation. Unlike traditional CNNs, our approach employs the competitive learning process of SOMs to replace the convolutional layers' learning. Additionally, separate Hebbian-like learning rules are used to learn local masks between the layers. This approach overcomes the issue of disentangling factors of variation when filters are learned through clustering. Our research includes designing two simple models using our building blocks, which achieve comparable performance to many Backpropagation methods. We also demonstrate that our method performs similarly to Backpropagation-based methods on Cifar10 and provides baseline results on Cifar100, Tiny ImageNet, and a small subset of ImageNet for Backpropagation-free methods.",1
"Network representation learning (NRL) is a powerful technique for learning low-dimensional vector representation of high-dimensional and sparse graphs. Most studies explore the structure and metadata associated with the graph using random walks and employ an unsupervised or semi-supervised learning schemes. Learning in these methods is context-free, because only a single representation per node is learned. Recently studies have argued on the sufficiency of a single representation and proposed a context-sensitive approach that proved to be highly effective in applications such as link prediction and ranking.   However, most of these methods rely on additional textual features that require RNNs or CNNs to capture high-level features or rely on a community detection algorithm to identify multiple contexts of a node.   In this study, without requiring additional features nor a community detection algorithm, we propose a novel context-sensitive algorithm called GAP that learns to attend on different parts of a node's neighborhood using attentive pooling networks. We show the efficacy of GAP using three real-world datasets on link prediction and node clustering tasks and compare it against 10 popular and state-of-the-art (SOTA) baselines. GAP consistently outperforms them and achieves up to ~9% and ~20% gain over the best performing methods on link prediction and clustering tasks, respectively.",0
"Network representation learning (NRL) is a highly effective method of acquiring low-dimensional vector representations of high-dimensional and sparse graphs. Typically, researchers analyze the graph's structure and metadata by utilizing random walks and employing unsupervised or semi-supervised learning approaches. These methods are context-free as they only learn a single representation per node. Recently, however, scholars have proposed a context-sensitive approach that has been particularly efficacious in link prediction and ranking applications. Unfortunately, most of these methods necessitate additional textual features that require RNNs or CNNs to capture high-level features or rely on a community detection algorithm to identify multiple contexts of a node. In this study, we propose a novel context-sensitive algorithm dubbed GAP that utilizes attentive pooling networks to learn how to attend to various parts of a node's neighborhood without requiring additional features or a community detection algorithm. We demonstrate the effectiveness of GAP on three real-world datasets for link prediction and node clustering tasks and compare it to ten popular and state-of-the-art (SOTA) baselines. GAP consistently outperforms these baselines, achieving gains of up to ~9% and ~20% on link prediction and clustering tasks, respectively.",1
"The use of drug combinations often leads to polypharmacy side effects (POSE). A recent method formulates POSE prediction as a link prediction problem on a graph of drugs and proteins, and solves it with Graph Convolutional Networks (GCNs). However, due to the complex relationships in POSE, this method has high computational cost and memory demand. This paper proposes a flexible Tri-graph Information Propagation (TIP) model that operates on three subgraphs to learn representations progressively by propagation from protein-protein graph to drug-drug graph via protein-drug graph. Experiments show that TIP improves accuracy by 7%+, time efficiency by 83$\times$, and space efficiency by 3$\times$.",0
"Polypharmacy side effects (POSE) often occur when combining drugs, which can be problematic. A recent approach used Graph Convolutional Networks (GCNs) to predict POSE by treating the problem as a link prediction task on a graph of drugs and proteins. However, this method is computationally expensive and memory-intensive due to the complex relationships involved in POSE. To address this, a new flexible model called Tri-graph Information Propagation (TIP) is proposed. TIP operates on three subgraphs and progressively learns representations by propagating from a protein-protein graph to a drug-drug graph through a protein-drug graph. Experiments show that TIP achieves a 7%+ improvement in accuracy, 83$\times$ faster computation time, and requires 3$\times$ less memory space.",1
"We propose a novel deep learning method for local self-supervised representation learning that does not require labels nor end-to-end backpropagation but exploits the natural order in data instead. Inspired by the observation that biological neural networks appear to learn without backpropagating a global error signal, we split a deep neural network into a stack of gradient-isolated modules. Each module is trained to maximally preserve the information of its inputs using the InfoNCE bound from Oord et al. [2018]. Despite this greedy training, we demonstrate that each module improves upon the output of its predecessor, and that the representations created by the top module yield highly competitive results on downstream classification tasks in the audio and visual domain. The proposal enables optimizing modules asynchronously, allowing large-scale distributed training of very deep neural networks on unlabelled datasets.",0
"Our innovative approach to local self-supervised representation learning utilizes deep learning without the need for labels or end-to-end backpropagation. Instead, we leverage the inherent data order to train a deep neural network split into gradient-isolated modules. Each module follows the InfoNCE bound from Oord et al. [2018] to retain maximum input information. Despite the modular training, our results show that each module outperforms its predecessor, and the top module generates highly competitive representations for audio and visual classification tasks. Our proposal also allows asynchronous optimization of modules, facilitating large-scale distributed training of deep neural networks on unlabeled datasets. Inspired by biological neural networks that learn without global error signals, our method presents a promising alternative to traditional backpropagation approaches.",1
"""Deep Archetypal Analysis"" generates latent representations of high-dimensional datasets in terms of fractions of intuitively understandable basic entities called archetypes. The proposed method is an extension of linear ""Archetypal Analysis"" (AA), an unsupervised method to represent multivariate data points as sparse convex combinations of extremal elements of the dataset. Unlike the original formulation of AA, ""Deep AA"" can also handle side information and provides the ability for data-driven representation learning which reduces the dependence on expert knowledge. Our method is motivated by studies of evolutionary trade-offs in biology where archetypes are species highly adapted to a single task. Along these lines, we demonstrate that ""Deep AA"" also lends itself to the supervised exploration of chemical space, marking a distinct starting point for de novo molecular design. In the unsupervised setting we show how ""Deep AA"" is used on CelebA to identify archetypal faces. These can then be superimposed in order to generate new faces which inherit dominant traits of the archetypes they are based on.",0
"""Deep Archetypal Analysis"" is a technique used to create latent representations of complex datasets by breaking them down into basic entities known as archetypes, which are easily understood. This approach is an extension of ""Archetypal Analysis"" (AA), which uses a linear method to represent multivariate data points as sparse combinations of extreme dataset elements. However, ""Deep AA"" goes beyond AA by allowing for side information and data-driven representation learning, reducing the need for expert knowledge. The motivation for this method comes from studies of evolutionary trade-offs in biology, where archetypes are species highly adapted to specific tasks. ""Deep AA"" can be used in the supervised exploration of chemical space for de novo molecular design. In the unsupervised setting, it identifies archetypal faces in CelebA, which can be combined to create new faces with dominant traits inherited from the archetypes.",1
"Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.",0
"Numerous modern techniques for unsupervised or self-supervised representation learning aim to train feature extractors by maximizing the estimated mutual information (MI) between various data views. However, there are several immediate issues with this approach, such as the difficulty of estimating MI and the possibility of highly tangled representations due to its invariance under arbitrary invertible transformations. Despite these challenges, these methods have consistently demonstrated superior performance. This paper contends, with supporting empirical evidence, that the success of these techniques is not solely due to the properties of MI but also heavily reliant on the inductive bias in selecting feature extractor architectures and the parametrization of MI estimators. Additionally, the paper establishes a correlation to deep metric learning and proposes this interpretation as a plausible explanation for the recent method's triumph.",1
"Autoencoder-based learning has emerged as a staple for disciplining representations in unsupervised and semi-supervised settings. This paper analyzes a framework for improving generalization in a purely supervised setting, where the target space is high-dimensional. We motivate and formalize the general framework of target-embedding autoencoders (TEA) for supervised prediction, learning intermediate latent representations jointly optimized to be both predictable from features as well as predictive of targets---encoding the prior that variations in targets are driven by a compact set of underlying factors. As our theoretical contribution, we provide a guarantee of generalization for linear TEAs by demonstrating uniform stability, interpreting the benefit of the auxiliary reconstruction task as a form of regularization. As our empirical contribution, we extend validation of this approach beyond existing static classification applications to multivariate sequence forecasting, verifying their advantage on both linear and nonlinear recurrent architectures---thereby underscoring the further generality of this framework beyond feedforward instantiations.",0
"Autoencoder-based learning has become a fundamental technique for regulating representations in unsupervised and semi-supervised environments. In this study, we investigate a method for enhancing generalization in a purely supervised context, where the target space is high-dimensional. We introduce the concept of target-embedding autoencoders (TEA) as a general framework for supervised prediction. TEA learns intermediate latent representations that are optimized to be both predictable from features and predictive of targets, which encodes the prior that variations in targets are caused by a compact set of underlying factors. We provide a theoretical guarantee of generalization for linear TEAs by demonstrating uniform stability, and we interpret the auxiliary reconstruction task's benefit as a form of regularization. We also extend the validation of this approach beyond existing static classification applications to multivariate sequence forecasting and verify its advantage on both linear and nonlinear recurrent architectures. This underscores the further universality of this framework beyond feedforward instantiations.",1
We propose a representation learning framework for medical diagnosis domain. It is based on heterogeneous network-based model of diagnostic data as well as modified metapath2vec algorithm for learning latent node representation. We compare the proposed algorithm with other representation learning methods in two practical case studies: symptom/disease classification and disease prediction. We observe a significant performance boost in these task resulting from learning representations of domain data in a form of heterogeneous network.,0
Our proposal is a framework for representation learning in the medical diagnosis domain. The framework utilizes a heterogeneous network-based model of diagnostic data in combination with a modified metapath2vec algorithm to learn latent node representations. We conducted two practical case studies - symptom/disease classification and disease prediction - and compared our algorithm's performance with other representation learning methods. Our findings indicate that learning representations of domain data in the form of a heterogeneous network leads to a significant performance boost in these tasks.,1
"Uncertainty quantification for deep learning is a challenging open problem. Bayesian statistics offer a mathematically grounded framework to reason about uncertainties; however, approximate posteriors for modern neural networks still require prohibitive computational costs. We propose a family of algorithms which split the classification task into two stages: representation learning and uncertainty estimation. We compare four specific instances, where uncertainty estimation is performed via either an ensemble of Stochastic Gradient Descent or Stochastic Gradient Langevin Dynamics snapshots, an ensemble of bootstrapped logistic regressions, or via a number of Monte Carlo Dropout passes. We evaluate their performance in terms of \emph{selective} classification (risk-coverage), and their ability to detect out-of-distribution samples. Our experiments suggest there is limited value in adding multiple uncertainty layers to deep classifiers, and we observe that these simple methods strongly outperform a vanilla point-estimate SGD in some complex benchmarks like ImageNet.",0
"It is difficult to quantify uncertainty in deep learning, and this remains an unresolved issue. Bayesian statistics provide a solid framework for assessing uncertainties, but it is still incredibly computationally expensive to obtain approximate posteriors for modern neural networks. To address this problem, we present a series of algorithms that divide the classification task into two stages: representation learning and uncertainty estimation. We explore four different approaches for estimating uncertainty, including using an ensemble of Stochastic Gradient Descent or Stochastic Gradient Langevin Dynamics snapshots, an ensemble of bootstrapped logistic regressions, or a number of Monte Carlo Dropout passes. Our evaluation focuses on both selective classification (risk-coverage) and the ability to detect out-of-distribution samples. Our findings indicate that adding multiple uncertainty layers to deep classifiers may not be particularly useful, and that simple methods can outperform a vanilla point-estimate SGD in complex benchmarks like ImageNet.",1
"Existing graph neural networks may suffer from the ""suspended animation problem"" when the model architecture goes deep. Meanwhile, for some graph learning scenarios, e.g., nodes with text/image attributes or graphs with long-distance node correlations, deep graph neural networks will be necessary for effective graph representation learning. In this paper, we propose a new graph neural network, namely DIFNET (Graph Diffusive Neural Network), for graph representation learning and node classification. DIFNET utilizes both neural gates and graph residual learning for node hidden state modeling, and includes an attention mechanism for node neighborhood information diffusion. Extensive experiments will be done in this paper to compare DIFNET against several state-of-the-art graph neural network models. The experimental results can illustrate both the learning performance advantages and effectiveness of DIFNET, especially in addressing the ""suspended animation problem"".",0
"When graph neural networks become deep, they may experience the ""suspended animation problem"". However, in certain graph learning scenarios where nodes possess text/image attributes or graphs feature long-distance node correlations, deep graph neural networks become necessary for successful graph representation learning. This paper introduces DIFNET (Graph Diffusive Neural Network), a novel graph neural network for node classification and graph representation learning. DIFNET leverages neural gates and graph residual learning to model node hidden states, and an attention mechanism for node neighborhood information diffusion. Extensive experiments are conducted in this paper to compare DIFNET with various state-of-the-art graph neural network models. Through these experiments, we demonstrate the performance advantages and effectiveness of DIFNET, particularly in addressing the ""suspended animation problem"".",1
"How can we enable machines to make sense of the world, and become better at learning? To approach this goal, I believe viewing intelligence in terms of many integral aspects, and also a universal two-term tradeoff between task performance and complexity, provides two feasible perspectives. In this thesis, I address several key questions in some aspects of intelligence, and study the phase transitions in the two-term tradeoff, using strategies and tools from physics and information. Firstly, how can we make the learning models more flexible and efficient, so that agents can learn quickly with fewer examples? Inspired by how physicists model the world, we introduce a paradigm and an AI Physicist agent for simultaneously learning many small specialized models (theories) and the domain they are accurate, which can then be simplified, unified and stored, facilitating few-shot learning in a continual way. Secondly, for representation learning, when can we learn a good representation, and how does learning depend on the structure of the dataset? We approach this question by studying phase transitions when tuning the tradeoff hyperparameter. In the information bottleneck, we theoretically show that these phase transitions are predictable and reveal structure in the relationships between the data, the model, the learned representation and the loss landscape. Thirdly, how can agents discover causality from observations? We address part of this question by introducing an algorithm that combines prediction and minimizing information from the input, for exploratory causal discovery from observational time series. Fourthly, to make models more robust to label noise, we introduce Rank Pruning, a robust algorithm for classification with noisy labels. I believe that building on the work of my thesis we will be one step closer to enable more intelligent machines that can make sense of the world.",0
"In order to improve machine learning capabilities and allow for a better understanding of the world, there are two feasible perspectives to consider: the integral aspects of intelligence and the universal tradeoff between task performance and complexity. This thesis examines these perspectives by addressing several key questions related to intelligence and studying the phase transitions in the tradeoff using tools from physics and information. The thesis explores ways to improve learning models by introducing a paradigm and an AI Physicist agent that can quickly learn accurate theories and simplify them to facilitate few-shot learning. The thesis also investigates representation learning by studying the relationship between the data, model, learned representation, and loss landscape. Additionally, the thesis addresses causal discovery through an algorithm that combines prediction and minimizing information from input. Finally, the thesis introduces a robust algorithm for classification with noisy labels. Through this work, the goal is to bring us one step closer to creating more intelligent machines that can make sense of the world.",1
"The problem of finding the sparsest vector (direction) in a low dimensional subspace can be considered as a homogeneous variant of the sparse recovery problem, which finds applications in robust subspace recovery, dictionary learning, sparse blind deconvolution, and many other problems in signal processing and machine learning. However, in contrast to the classical sparse recovery problem, the most natural formulation for finding the sparsest vector in a subspace is usually nonconvex. In this paper, we overview recent advances on global nonconvex optimization theory for solving this problem, ranging from geometric analysis of its optimization landscapes, to efficient optimization algorithms for solving the associated nonconvex optimization problem, to applications in machine intelligence, representation learning, and imaging sciences. Finally, we conclude this review by pointing out several interesting open problems for future research.",0
"The problem of identifying the least dense vector in a low-dimensional subspace can be viewed as a uniform version of the sparse recovery problem, which has practical applications in fields such as robust subspace recovery, dictionary learning, and sparse blind deconvolution, as well as in signal processing and machine learning. Unlike the traditional sparse recovery problem, the quest for the sparsest vector in a subspace typically involves a non-convex formulation. This paper presents an overview of recent developments in global non-convex optimization theory for addressing this challenge. These advancements include analyzing the optimization landscapes using geometric methods, creating efficient algorithms for solving the associated non-convex optimization problem, and exploring how these techniques can be applied to machine intelligence, representation learning, and imaging sciences. The review concludes by highlighting several intriguing areas for future research.",1
"Effective modeling of electronic health records (EHR) is rapidly becoming an important topic in both academia and industry. A recent study showed that using the graphical structure underlying EHR data (e.g. relationship between diagnoses and treatments) improves the performance of prediction tasks such as heart failure prediction. However, EHR data do not always contain complete structure information. Moreover, when it comes to claims data, structure information is completely unavailable to begin with. Under such circumstances, can we still do better than just treating EHR data as a flat-structured bag-of-features? In this paper, we study the possibility of jointly learning the hidden structure of EHR while performing supervised prediction tasks on EHR data. Specifically, we discuss that Transformer is a suitable basis model to learn the hidden EHR structure, and propose Graph Convolutional Transformer, which uses data statistics to guide the structure learning process. The proposed model consistently outperformed previous approaches empirically, on both synthetic data and publicly available EHR data, for various prediction tasks such as graph reconstruction and readmission prediction, indicating that it can serve as an effective general-purpose representation learning algorithm for EHR data.",0
"The modeling of electronic health records (EHR) has become a crucial topic in academia and industry. A recent study found that utilizing the graphical structure underlying EHR data (like the relationship between diagnoses and treatments) enhances the performance of prediction tasks such as heart failure prediction. However, EHR data may not always have complete structure information. Moreover, structure information is entirely unavailable when it comes to claims data. In light of these circumstances, can we do better than simply treating EHR data as a flat-structured bag-of-features? This paper explores the feasibility of learning the hidden EHR structure while executing supervised prediction tasks on EHR data. We argue that Transformer is an appropriate base model for learning the hidden EHR structure and introduce Graph Convolutional Transformer. This model uses data statistics to guide the structure learning process. Empirically, the proposed model consistently outperforms previous approaches on both synthetic data and publicly available EHR data for various prediction tasks such as graph reconstruction and readmission prediction. This suggests that it can serve as an effective general-purpose representation learning algorithm for EHR data.",1
"This paper proposes a novel deep subspace clustering approach which uses convolutional autoencoders to transform input images into new representations lying on a union of linear subspaces. The first contribution of our work is to insert multiple fully-connected linear layers between the encoder layers and their corresponding decoder layers to promote learning more favorable representations for subspace clustering. These connection layers facilitate the feature learning procedure by combining low-level and high-level information for generating multiple sets of self-expressive and informative representations at different levels of the encoder. Moreover, we introduce a novel loss minimization problem which leverages an initial clustering of the samples to effectively fuse the multi-level representations and recover the underlying subspaces more accurately. The loss function is then minimized through an iterative scheme which alternatively updates the network parameters and produces new clusterings of the samples. Experiments on four real-world datasets demonstrate that our approach exhibits superior performance compared to the state-of-the-art methods on most of the subspace clustering problems.",0
"A new technique for deep subspace clustering is presented in this paper, utilizing convolutional autoencoders to convert input images into fresh representations situated on a collection of linear subspaces. The first novelty of this research is the addition of multiple fully-connected linear layers between the encoder and decoder layers, which enhances the learning of more desirable representations for subspace clustering. These intermediate layers allow for the combination of low-level and high-level data, generating numerous self-expressive and informative representations at various encoder levels. Additionally, a unique loss minimization problem is introduced, integrating an initial clustering of samples to more accurately restore the underlying subspaces by fusing the multi-level representations. An iterative approach is employed to minimize the loss function by updating network parameters and generating new sample clusters. Our method demonstrates superior performance on most subspace clustering problems compared to state-of-the-art methods, as shown through experiments on four real-world datasets.",1
"Few-shot learning algorithms aim to learn model parameters capable of adapting to unseen classes with the help of only a few labeled examples. A recent regularization technique - Manifold Mixup focuses on learning a general-purpose representation, robust to small changes in the data distribution. Since the goal of few-shot learning is closely linked to robust representation learning, we study Manifold Mixup in this problem setting. Self-supervised learning is another technique that learns semantically meaningful features, using only the inherent structure of the data. This work investigates the role of learning relevant feature manifold for few-shot tasks using self-supervision and regularization techniques. We observe that regularizing the feature manifold, enriched via self-supervised techniques, with Manifold Mixup significantly improves few-shot learning performance. We show that our proposed method S2M2 beats the current state-of-the-art accuracy on standard few-shot learning datasets like CIFAR-FS, CUB, mini-ImageNet and tiered-ImageNet by 3-8 %. Through extensive experimentation, we show that the features learned using our approach generalize to complex few-shot evaluation tasks, cross-domain scenarios and are robust against slight changes to data distribution.",0
"The objective of few-shot learning algorithms is to acquire model parameters that can adjust to new categories with minimal labeled examples. A new regularization technique, known as Manifold Mixup, emphasizes on developing a versatile representation that can withstand minor variations in the data distribution. Given that few-shot learning aims to achieve strong and resilient representation learning, we analyze the effectiveness of Manifold Mixup in this context. Self-supervised learning is an additional method that learns semantically relevant features by relying solely on the intrinsic structure of the data. This study explores the significance of learning pertinent feature manifolds for few-shot tasks using self-supervision and regularization techniques. Our findings reveal that by enriching the feature manifold via self-supervised methods and using Manifold Mixup as regularization, few-shot learning performance improves significantly. Our proposed approach, S2M2, surpasses the existing state-of-the-art accuracy on common few-shot learning datasets such as CIFAR-FS, CUB, mini-ImageNet, and tiered-ImageNet by 3-8 %. Our extensive experimentation demonstrates that the features learned using our method generalize well to complex few-shot evaluation tasks and cross-domain scenarios and remain robust even when there are slight changes in the data distribution.",1
"This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (e.g. graph2vec) but they tend to only consider certain substructures (e.g. subtrees) as graph representatives. Inspired by recent progress of unsupervised representation learning, in this paper we proposed a novel method called InfoGraph for learning graph-level representations. We maximize the mutual information between the graph-level representation and the representations of substructures of different scales (e.g., nodes, edges, triangles). By doing so, the graph-level representations encode aspects of the data that are shared across different scales of substructures. Furthermore, we further propose InfoGraph*, an extension of InfoGraph for semi-supervised scenarios. InfoGraph* maximizes the mutual information between unsupervised graph representations learned by InfoGraph and the representations learned by existing supervised methods. As a result, the supervised encoder learns from unlabeled data while preserving the latent semantic space favored by the current supervised task. Experimental results on the tasks of graph classification and molecular property prediction show that InfoGraph is superior to state-of-the-art baselines and InfoGraph* can achieve performance competitive with state-of-the-art semi-supervised models.",0
"The focus of this paper is on the acquisition of representations for entire graphs in scenarios that are either unsupervised or semi-supervised. These types of representations are crucial for many practical applications, such as predicting the characteristics of molecules and analyzing social networks. While traditional graph kernel methods are effective, they suffer from poor generalization due to their manual designs. More recent approaches, like graph2vec, which are based on language models, tend to only consider certain substructures, such as subtrees, as graph representatives. The authors propose a novel method called InfoGraph, which uses unsupervised representation learning to encode aspects of the data that are shared across different scales of substructures. They also propose an extension of InfoGraph, called InfoGraph*, for semi-supervised scenarios. This method maximizes the mutual information between unsupervised graph representations and the representations learned by existing supervised methods. The experimental results demonstrate that InfoGraph outperforms current baselines, while InfoGraph* achieves competitive performance with state-of-the-art semi-supervised models in graph classification and molecular property prediction tasks.",1
"We present Spectral Inference Networks, a framework for learning eigenfunctions of linear operators by stochastic optimization. Spectral Inference Networks generalize Slow Feature Analysis to generic symmetric operators, and are closely related to Variational Monte Carlo methods from computational physics. As such, they can be a powerful tool for unsupervised representation learning from video or graph-structured data. We cast training Spectral Inference Networks as a bilevel optimization problem, which allows for online learning of multiple eigenfunctions. We show results of training Spectral Inference Networks on problems in quantum mechanics and feature learning for videos on synthetic datasets. Our results demonstrate that Spectral Inference Networks accurately recover eigenfunctions of linear operators and can discover interpretable representations from video in a fully unsupervised manner.",0
"Spectral Inference Networks are introduced as an approach for acquiring eigenfunctions of linear operators through stochastic optimization. This method extends Slow Feature Analysis to various symmetric operators and is closely linked to Variational Monte Carlo techniques used in computational physics, making it a promising technique for unsupervised representation learning from graph-structured or video data. Training Spectral Inference Networks is treated as a bilevel optimization issue, enabling the online learning of numerous eigenfunctions. Our study demonstrates the effectiveness of Spectral Inference Networks in accurately recovering eigenfunctions of linear operators and discovering meaningful representations from video in a fully unsupervised manner. We present findings from training Spectral Inference Networks on quantum mechanics problems and synthetic datasets for video feature learning.",1
"For subspace recovery, most existing low-rank representation (LRR) models performs in the original space in single-layer mode. As such, the deep hierarchical information cannot be learned, which may result in inaccurate recoveries for complex real data. In this paper, we explore the deep multi-subspace recovery problem by designing a multilayer architecture for latent LRR. Technically, we propose a new Multilayer Collabora-tive Low-Rank Representation Network model termed DeepLRR to discover deep features and deep subspaces. In each layer (>2), DeepLRR bilinearly reconstructs the data matrix by the collabo-rative representation with low-rank coefficients and projection matrices in the previous layer. The bilinear low-rank reconstruc-tion of previous layer is directly fed into the next layer as the input and low-rank dictionary for representation learning, and is further decomposed into a deep principal feature part, a deep salient feature part and a deep sparse error. As such, the coher-ence issue can be also resolved due to the low-rank dictionary, and the robustness against noise can also be enhanced in the feature subspace. To recover the sparse errors in layers accurately, a dynamic growing strategy is used, as the noise level will be-come smaller for the increase of layers. Besides, a neighborhood reconstruction error is also included to encode the locality of deep salient features by deep coefficients adaptively in each layer. Extensive results on public databases show that our DeepLRR outperforms other related models for subspace discovery and clustering.",0
"Most current low-rank representation (LRR) models used for subspace recovery operate in a single-layer mode in the original space. However, this approach does not allow for the learning of deep hierarchical information, which may cause inaccuracies when recovering complex real data. This paper proposes a solution to this issue by introducing a multilayer architecture for latent LRR, called DeepLRR. This model utilizes a new Multilayer Collaborative Low-Rank Representation Network to discover deep features and subspaces. Each layer (>2) of DeepLRR reconstructs the data matrix bilinearly by using collaborative representation with low-rank coefficients and projection matrices from the previous layer. The bilinear low-rank reconstruction from the previous layer is then used as input and a low-rank dictionary for representation learning. It is further decomposed into a deep principal feature part, a deep salient feature part, and a deep sparse error. This approach resolves issues of coherence and enhances the robustness against noise in the feature subspace. To accurately recover sparse errors in each layer, a dynamic growing strategy is used, and a neighborhood reconstruction error is included to encode the locality of deep salient features adaptively in each layer. Experimental results on public databases demonstrate that DeepLRR outperforms other related models for subspace discovery and clustering.",1
"Both the Dictionary Learning (DL) and Convolutional Neural Networks (CNN) are powerful image representation learning systems based on different mechanisms and principles, however whether we can seamlessly integrate them to improve the per-formance is noteworthy exploring. To address this issue, we propose a novel generalized end-to-end representation learning architecture, dubbed Convolutional Dictionary Pair Learning Network (CDPL-Net) in this paper, which integrates the learning schemes of the CNN and dictionary pair learning into a unified framework. Generally, the architecture of CDPL-Net includes two convolutional/pooling layers and two dictionary pair learn-ing (DPL) layers in the representation learning module. Besides, it uses two fully-connected layers as the multi-layer perception layer in the nonlinear classification module. In particular, the DPL layer can jointly formulate the discriminative synthesis and analysis representations driven by minimizing the batch based reconstruction error over the flatted feature maps from the convolution/pooling layer. Moreover, DPL layer uses l1-norm on the analysis dictionary so that sparse representation can be delivered, and the embedding process will also be robust to noise. To speed up the training process of DPL layer, the efficient stochastic gradient descent is used. Extensive simulations on real databases show that our CDPL-Net can deliver enhanced performance over other state-of-the-art methods.",0
"The Dictionary Learning (DL) and Convolutional Neural Networks (CNN) are two different but powerful image representation learning systems. The possibility of combining these systems to improve overall performance is worth exploring. This paper proposes a new architecture called Convolutional Dictionary Pair Learning Network (CDPL-Net) that integrates the learning mechanisms of both CNN and dictionary pair learning. The CDPL-Net includes two convolutional/pooling layers and two dictionary pair learning (DPL) layers in the representation learning module, and two fully-connected layers in the nonlinear classification module. The DPL layer can jointly formulate discriminative synthesis and analysis representations. It uses l1-norm on the analysis dictionary to deliver sparse representation and make the embedding process robust to noise. Efficient stochastic gradient descent is used to speed up the training process of DPL layer. The simulations on real databases show that CDPL-Net outperforms other state-of-the-art methods.",1
"A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.",0
"Representation learning poses a key inquiry about the feasibility of reconstructing the genuine latent variables of a highly intricate generative process. Khemakhem et al.'s (2019) recent groundbreaking research on nonlinear ICA has successfully addressed this issue for a wide range of conditional generative processes. Our study builds on this significant finding to make it applicable to real-world data. Firstly, we expand the theory to incorporate scenarios where the intrinsic problem dimension is unknown. We establish that in some specific but not overly restrictive cases, an estimating model can automatically segregate informative latent variables from noise. Moreover, the recovered informative latent variables map one-to-one with the true latent variables of the generative process, except for a trivial component-wise transformation. Secondly, we introduce the General Incompressible-flow Network (GIN), a modification of the RealNVP invertible neural network architecture (Dinh et al., 2016), which is well-suited for this type of problem. Our experiments on artificial data and EMNIST confirm the theoretical predictions, and we present a comprehensive set of precisely 22 informative latent variables extracted from EMNIST.",1
"In this paper we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",0
"This article explores the use of self-supervised representation learning in reinforcement learning (RL) to improve its sample efficiency. Our proposed approach involves using a forward prediction objective to simultaneously learn embeddings of states and action sequences. These embeddings capture the dynamics of the environment, resulting in efficient policy learning. We show that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on low-dimensional state control. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in just 1-2 million environment steps.",1
"Audio-visual learning, aimed at exploiting the relationship between audio and visual modalities, has drawn considerable attention since deep learning started to be used successfully. Researchers tend to leverage these two modalities either to improve the performance of previously considered single-modality tasks or to address new challenging problems. In this paper, we provide a comprehensive survey of recent audio-visual learning development. We divide the current audio-visual learning tasks into four different subfields: audio-visual separation and localization, audio-visual correspondence learning, audio-visual generation, and audio-visual representation learning. State-of-the-art methods as well as the remaining challenges of each subfield are further discussed. Finally, we summarize the commonly used datasets and performance metrics.",0
"Since the successful implementation of deep learning, there has been a significant interest in audio-visual learning, which exploits the relationship between audio and visual modes. Researchers have utilized these modalities to enhance the performance of previously single-modality tasks or tackle new, more complex problems. This comprehensive paper offers a survey of recent developments in audio-visual learning, categorizing current tasks into four subfields: audio-visual separation and localization, audio-visual correspondence learning, audio-visual generation, and audio-visual representation learning. The latest techniques in each subfield and the challenges that remain are discussed. Additionally, commonly used datasets and performance metrics are summarized.",1
"Learning disentangled representation of data without supervision is an important step towards improving the interpretability of generative models. Despite recent advances in disentangled representation learning, existing approaches often suffer from the trade-off between representation learning and generation performance i.e. improving generation quality sacrifices disentanglement performance). We propose an Information-Distillation Generative Adversarial Network (ID-GAN), a simple yet generic framework that easily incorporates the existing state-of-the-art models for both disentanglement learning and high-fidelity synthesis. Our method learns disentangled representation using VAE-based models, and distills the learned representation with an additional nuisance variable to the separate GAN-based generator for high-fidelity synthesis. To ensure that both generative models are aligned to render the same generative factors, we further constrain the GAN generator to maximize the mutual information between the learned latent code and the output. Despite the simplicity, we show that the proposed method is highly effective, achieving comparable image generation quality to the state-of-the-art methods using the disentangled representation. We also show that the proposed decomposition leads to an efficient and stable model design, and we demonstrate photo-realistic high-resolution image synthesis results (1024x1024 pixels) for the first time using the disentangled representations.",0
"Improving the interpretability of generative models requires unsupervised learning of disentangled data representations. However, current approaches often face a trade-off between representation learning and generation performance, such that enhancing generation quality comes at the expense of disentanglement performance. This study introduces an Information-Distillation Generative Adversarial Network (ID-GAN) framework that combines state-of-the-art models for disentanglement learning and high-fidelity synthesis. Using VAE-based models, the proposed method learns disentangled representation and distills it with a nuisance variable to a separate GAN-based generator for high-fidelity synthesis. To ensure that both generative models are aligned, the GAN generator is constrained to maximize the mutual information between the learned latent code and the output. Despite its simplicity, the proposed method achieves comparable image generation quality to state-of-the-art methods, and leads to efficient and stable model design. Furthermore, the proposed decomposition enables photo-realistic high-resolution image synthesis results (1024x1024 pixels) using disentangled representations for the first time.",1
"Self-supervised (SS) learning is a powerful approach for representation learning using unlabeled data. Recently, it has been applied to Generative Adversarial Networks (GAN) training. Specifically, SS tasks were proposed to address the catastrophic forgetting issue in the GAN discriminator. In this work, we perform an in-depth analysis to understand how SS tasks interact with learning of generator. From the analysis, we identify issues of SS tasks which allow a severely mode-collapsed generator to excel the SS tasks. To address the issues, we propose new SS tasks based on a multi-class minimax game. The competition between our proposed SS tasks in the game encourages the generator to learn the data distribution and generate diverse samples. We provide both theoretical and empirical analysis to support that our proposed SS tasks have better convergence property. We conduct experiments to incorporate our proposed SS tasks into two different GAN baseline models. Our approach establishes state-of-the-art FID scores on CIFAR-10, CIFAR-100, STL-10, CelebA, Imagenet $32\times32$ and Stacked-MNIST datasets, outperforming existing works by considerable margins in some cases. Our unconditional GAN model approaches performance of conditional GAN without using labeled data. Our code: https://github.com/tntrung/msgan",0
"The utilization of Self-supervised (SS) learning has become a dominant method in representation learning via unlabeled data. Recently, it has been employed in the training of Generative Adversarial Networks (GAN) to tackle the issue of catastrophic forgetting in the GAN discriminator. In this study, we conduct a thorough analysis to comprehend the interaction between SS tasks and the generator's learning process. We have identified certain drawbacks of the SS tasks that enable a generator with severe mode collapse to outperform them. To address this issue, we propose new SS tasks based on a multi-class minimax game. The competition between our proposed SS tasks in the game encourages the generator to learn the data distribution and produce diverse samples. We provide both theoretical and empirical analysis to support that our proposed SS tasks have better convergence properties. We conduct experiments to integrate our proposed SS tasks into two different GAN baseline models. Our approach has achieved state-of-the-art FID scores on various datasets, including CIFAR-10, CIFAR-100, STL-10, CelebA, Imagenet $32\times32$ and Stacked-MNIST, surpassing existing works in some cases. Our unconditional GAN model performs as well as a conditional GAN without the use of labeled data. Our code is available on https://github.com/tntrung/msgan.",1
"A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world down-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better down-stream performance. In particular, they enable quicker learning using fewer samples.",0
"Independently encoding information about the significant factors of variation in the data, a disentangled representation is believed to be beneficial in solving various practical tasks. However, there is insufficient empirical evidence to support this claim. To address this, we conducted a comprehensive study to investigate whether disentangled representations are more appropriate for abstract reasoning tasks. We created two tasks similar to Raven's Progressive Matrices and assessed the usefulness of representations derived from 360 cutting-edge unsupervised disentanglement models. Using these representations, we trained 3600 abstract reasoning models and found that disentangled representations led to better performance in downstream tasks. Specifically, they facilitated faster learning using fewer samples.",1
"Experimental reproducibility and replicability are critical topics in machine learning. Authors have often raised concerns about their lack in scientific publications to improve the quality of the field. Recently, the graph representation learning field has attracted the attention of a wide research community, which resulted in a large stream of works. As such, several Graph Neural Network models have been developed to effectively tackle graph classification. However, experimental procedures often lack rigorousness and are hardly reproducible. Motivated by this, we provide an overview of common practices that should be avoided to fairly compare with the state of the art. To counter this troubling trend, we ran more than 47000 experiments in a controlled and uniform framework to re-evaluate five popular models across nine common benchmarks. Moreover, by comparing GNNs with structure-agnostic baselines we provide convincing evidence that, on some datasets, structural information has not been exploited yet. We believe that this work can contribute to the development of the graph learning field, by providing a much needed grounding for rigorous evaluations of graph classification models.",0
"The importance of experimental reproducibility and replicability in the field of machine learning cannot be overstated. Many authors have expressed their concerns about the lack of these qualities in scientific publications, which can negatively affect the quality of the field. The field of graph representation learning has recently gained a lot of attention, resulting in a plethora of works. Several Graph Neural Network models have been developed to tackle graph classification effectively, but their experimental procedures often lack rigor and reproducibility. To address this issue, we present a summary of common practices that should be avoided to ensure a fair comparison with the state-of-the-art. To combat this trend, we conducted over 47,000 experiments in a controlled and uniform framework to re-evaluate five popular models across nine common benchmarks. Furthermore, by comparing GNNs with structure-agnostic baselines, we provide compelling evidence that structural information has yet to be exploited in some datasets. We are confident that our work will contribute to the development of the graph learning field by providing a much-needed foundation for rigorous evaluations of graph classification models.",1
"Variational autoencoders (VAEs) have ushered in a new era of unsupervised learning methods for complex distributions. Although these techniques are elegant in their approach, they are typically not useful for representation learning. In this work, we propose a simple yet powerful class of VAEs that simultaneously result in meaningful learned representations. Our solution is to combine traditional VAEs with mutual information maximization, with the goal to enhance amortized inference in VAEs using Information Theoretic techniques. We call this approach InfoMax-VAE, and such an approach can significantly boost the quality of learned high-level representations. We realize this through the explicit maximization of information measures associated with the representation. Using extensive experiments on varied datasets and setups, we show that InfoMax-VAE outperforms contemporary popular approaches, including Info-VAE and $\beta$-VAE.",0
"VAEs have revolutionized unsupervised learning methods for intricate distributions, although they tend to be insufficient for representation learning. The proposed solution is a new class of VAEs, named InfoMax-VAE, which integrates traditional VAEs with mutual information maximization. This approach aims to improve the amortized inference in VAEs using Information Theoretic techniques and simultaneously results in meaningful learned representations. Through the explicit maximization of information measures linked with the representation, InfoMax-VAE can considerably enhance the quality of learned high-level representations. Extensive experiments on diverse datasets and setups demonstrate that InfoMax-VAE surpasses other popular approaches, including Info-VAE and $\beta$-VAE.",1
"In the Information Bottleneck (IB), when tuning the relative strength between compression and prediction terms, how do the two terms behave, and what's their relationship with the dataset and the learned representation? In this paper, we set out to answer these questions by studying multiple phase transitions in the IB objective: $\text{IB}_\beta[p(z|x)] = I(X; Z) - \beta I(Y; Z)$ defined on the encoding distribution p(z|x) for input $X$, target $Y$ and representation $Z$, where sudden jumps of $dI(Y; Z)/d \beta$ and prediction accuracy are observed with increasing $\beta$. We introduce a definition for IB phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. Using second-order calculus of variations, we derive a formula that provides a practical condition for IB phase transitions, and draw its connection with the Fisher information matrix for parameterized models. We provide two perspectives to understand the formula, revealing that each IB phase transition is finding a component of maximum (nonlinear) correlation between $X$ and $Y$ orthogonal to the learned representation, in close analogy with canonical-correlation analysis (CCA) in linear settings. Based on the theory, we present an algorithm for discovering phase transition points. Finally, we verify that our theory and algorithm accurately predict phase transitions in categorical datasets, predict the onset of learning new classes and class difficulty in MNIST, and predict prominent phase transitions in CIFAR10.",0
"The goal of this paper is to explore the behavior and relationship between the compression and prediction terms in the Information Bottleneck (IB) objective, as well as their connection with the dataset and learned representation. To achieve this, we investigate multiple phase transitions in the IB objective, which is defined as $\text{IB}_\beta[p(z|x)] = I(X; Z) - \beta I(Y; Z)$, where sudden jumps of $dI(Y; Z)/d \beta$ and prediction accuracy are observed with increasing $\beta$. We define IB phase transitions as a qualitative change in the IB loss landscape and relate them to the onset of learning new classes. Using second-order calculus of variations, we derive a formula that provides a practical condition for IB phase transitions and draw its connection with the Fisher information matrix for parameterized models. We explain that each IB phase transition finds a component of maximum correlation between $X$ and $Y$ orthogonal to the learned representation, similar to canonical-correlation analysis (CCA) in linear settings. We present an algorithm that uses our theory to accurately predict phase transition points and verify our results on categorical datasets, MNIST, and CIFAR10.",1
"A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations.",0
"Human cognition relies heavily on a structured understanding of the world, including objects, relations, and hierarchies. However, learning such models from raw sensory data is a challenge. To address this challenge, we present Contrastively-trained Structured World Models (C-SWMs), which use a contrastive approach to learn representations in environments with compositional structure. Each state embedding is structured as a set of object representations and their relations, modeled by a graph neural network, enabling the discovery of objects from raw pixel observations without direct supervision. We evaluate C-SWMs on compositional environments, simple Atari games, and a multi-object physics simulation, demonstrating their ability to overcome limitations of pixel reconstruction models and outperform typical representatives of this model class in highly structured environments. Additionally, C-SWMs learn interpretable object-based representations.",1
"Trajectory owner prediction is the basis for many applications such as personalized recommendation, urban planning. Although much effort has been put on this topic, the results archived are still not good enough. Existing methods mainly employ RNNs to model trajectories semantically due to the inherent sequential attribute of trajectories. However, these approaches are weak at Point of Interest (POI) representation learning and trajectory feature detection. Thus, the performance of existing solutions is far from the requirements of practical applications. In this paper, we propose a novel CNN-based Trajectory Owner Prediction (CNNTOP) method. Firstly, we connect all POI according to trajectories from all users. The result is a connected graph that can be used to generate more informative POI sequences than other approaches. Secondly, we employ the Node2Vec algorithm to encode each POI into a low-dimensional real value vector. Then, we transform each trajectory into a fixed-dimensional matrix, which is similar to an image. Finally, a CNN is designed to detect features and predict the owner of a given trajectory. The CNN can extract informative features from the matrix representations of trajectories by convolutional operations, Batch normalization, and $K$-max pooling operations. Extensive experiments on real datasets demonstrate that CNNTOP substantially outperforms existing solutions in terms of macro-Precision, macro-Recall, macro-F1, and accuracy.",0
"The prediction of trajectory owners is essential for various applications, such as personalized recommendations and urban planning. Despite much research in this area, the achieved results are still inadequate. Existing methods use RNNs to semantically model trajectories due to their sequential nature. However, these methods struggle with Point of Interest (POI) representation learning and trajectory feature detection, resulting in poor performance for practical applications. To address this issue, we propose a new method called CNN-based Trajectory Owner Prediction (CNNTOP). Firstly, we connect all POIs based on user trajectories to create a more informative POI sequence. Then, we use the Node2Vec algorithm to encode each POI into a low-dimensional vector. We transform each trajectory into a fixed-dimensional matrix, similar to an image, and use a CNN to detect features and predict the owner. The CNN uses convolutional operations, Batch normalization, and $K$-max pooling operations to extract essential features from trajectory matrix representations. Our experimental results on real datasets show that CNNTOP outperforms existing methods significantly in terms of macro-Precision, macro-Recall, macro-F1, and accuracy.",1
"Time series constitute a challenging data type for machine learning algorithms, due to their highly variable lengths and sparse labeling in practice. In this paper, we tackle this challenge by proposing an unsupervised method to learn universal embeddings of time series. Unlike previous works, it is scalable with respect to their length and we demonstrate the quality, transferability and practicability of the learned representations with thorough experiments and comparisons. To this end, we combine an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining general-purpose representations for variable length and multivariate time series.",0
"Machine learning algorithms face difficulty in dealing with time series data due to their varying lengths and limited labeling in real-world scenarios. In this study, we present an unsupervised approach to generating universal embeddings for time series, which overcomes these challenges. Our method is scalable with regards to length and provides high-quality, transferable, and practical representations, as evidenced by comprehensive experiments and comparisons. We achieve this by combining a causal dilated convolution encoder with a novel triplet loss that utilizes time-based negative sampling, resulting in general-purpose representations suitable for multivariate and variable length time series.",1
"Domain gaps of sensor modalities pose a challenge for the design of autonomous robots. Taking a step towards closing this gap, we propose two unsupervised training frameworks for finding a common representation of LiDAR and camera data. The first method utilizes a double Siamese training structure to ensure consistency in the results. The second method uses a Canny edge image guiding the networks towards a desired representation. All networks are trained in an unsupervised manner, leaving room for scalability. The results are evaluated using common computer vision applications, and the limitations of the proposed approaches are outlined.",0
"The design of autonomous robots faces a challenge due to the gaps in sensor modalities within domains. In an attempt to overcome this hindrance, we suggest two unsupervised training frameworks that aim to discover a shared representation of LiDAR and camera data. To ensure consistency in the results, the first approach employs a double Siamese training structure, while the second method uses a Canny edge image to guide the networks towards the desired representation. The scalability of these unsupervised training methods is maintained since they do not require supervision. The outcomes are assessed using common computer vision applications, and the limitations of the suggested techniques are highlighted.",1
"We propose a novel self-supervised method, referred to as Video Cloze Procedure (VCP), to learn rich spatial-temporal representations. VCP first generates ""blanks"" by withholding video clips and then creates ""options"" by applying spatio-temporal operations on the withheld clips. Finally, it fills the blanks with ""options"" and learns representations by predicting the categories of operations applied on the clips. VCP can act as either a proxy task or a target task in self-supervised learning. As a proxy task, it converts rich self-supervised representations into video clip operations (options), which enhances the flexibility and reduces the complexity of representation learning. As a target task, it can assess learned representation models in a uniform and interpretable manner. With VCP, we train spatial-temporal representation models (3D-CNNs) and apply such models on action recognition and video retrieval tasks. Experiments on commonly used benchmarks show that the trained models outperform the state-of-the-art self-supervised models with significant margins.",0
"Our proposed method for learning spatial-temporal representations, called Video Cloze Procedure (VCP), is innovative and self-supervised. VCP generates ""blanks"" by withholding video clips and uses spatio-temporal operations to create ""options."" It then fills the blanks with ""options"" and predicts the categories of operations applied on the clips to learn representations. VCP can serve as a proxy or target task in self-supervised learning. As a proxy task, it enhances the flexibility and reduces the complexity of representation learning by converting rich self-supervised representations into video clip operations (options). As a target task, it can uniformly and interpretably assess learned representation models. We train spatial-temporal representation models (3D-CNNs) using VCP and apply them to action recognition and video retrieval tasks. Experiments on commonly used benchmarks demonstrate that our trained models outperform the state-of-the-art self-supervised models by significant margins.",1
"Central to all machine learning algorithms is data representation. For multi-agent systems, selecting a representation which adequately captures the interactions among agents is challenging due to the latent group structure which tends to vary depending on context. However, in multi-agent systems with strong group structure, we can simultaneously learn this structure and map a set of agents to a consistently ordered representation for further learning. In this paper, we present a dynamic alignment method which provides a robust ordering of structured multi-agent data enabling representation learning to occur in a fraction of the time of previous methods. We demonstrate the value of this approach using a large amount of soccer tracking data from a professional league.",0
"The core of all machine learning algorithms is the way data is presented. For multi-agent systems, finding a representation that accurately captures the interactions between agents is difficult because the group structure is often latent and subject to change depending on the context. However, in multi-agent systems with a strong group structure, we can learn the structure and map a set of agents to a consistently ordered representation that can be used for further learning. This paper introduces a dynamic alignment method that quickly and robustly orders structured multi-agent data, allowing representation learning to occur much faster than with previous methods. The effectiveness of this method is demonstrated using a large quantity of soccer tracking data from a professional league.",1
"Unsupervised learning of disentangled representations involves uncovering of different factors of variations that contribute to the data generation process. Total correlation penalization has been a key component in recent methods towards disentanglement. However, Kullback-Leibler (KL) divergence-based total correlation is metric-agnostic and sensitive to data samples. In this paper, we introduce Wasserstein total correlation in both variational autoencoder and Wasserstein autoencoder settings to learn disentangled latent representations. A critic is adversarially trained along with the main objective to estimate the Wasserstein total correlation term. We discuss the benefits of using Wasserstein distance over KL divergence to measure independence and conduct quantitative and qualitative experiments on several data sets. Moreover, we introduce a new metric to measure disentanglement. We show that the proposed approach has comparable performances on disentanglement with smaller sacrifices in reconstruction abilities.",0
"Disentangling representations through unsupervised learning involves identifying various factors that contribute to the generation of data. Recent methods for disentanglement have used total correlation penalization, but the Kullback-Leibler (KL) divergence-based total correlation is insensitive to metrics and subject to data samples. This study introduces the use of Wasserstein total correlation in both variational autoencoder and Wasserstein autoencoder settings to learn disentangled latent representations. To estimate the Wasserstein total correlation term, a critic is trained adversarially along with the main objective. The benefits of using Wasserstein distance over KL divergence to measure independence are discussed, and quantitative and qualitative experiments are conducted on several data sets. Additionally, a new metric is introduced to measure disentanglement, and it is demonstrated that the proposed approach has similar disentanglement performance while sacrificing less in reconstruction abilities.",1
"Canonical correlation analysis (CCA) is a popular technique for learning representations that are maximally correlated across multiple views in data. In this paper, we extend the CCA based framework for learning a multiview mixture model. We show that the proposed model and a set of simple heuristics yield improvements over standard CCA, as measured in terms of performance on downstream tasks. Our experimental results show that our correlation-based objective meaningfully generalizes the CCA objective to a mixture of CCA models.",0
The widely used method for acquiring correlated representations across various data views is Canonical Correlation Analysis (CCA). The paper introduces an expansion of the CCA-based framework to learn a multiview mixture model. The study demonstrates that the suggested model and straightforward heuristics enhance the standard CCA as gauged by the results of downstream tasks. The experimental findings indicate that the correlation-based objective of our proposed model meaningfully extends the CCA objective to a combination of CCA models.,1
"In this paper, we investigate the unsupervised deep representation learning issue and technically propose a novel framework called Deep Self-representative Concept Factorization Network (DSCF-Net), for clustering deep features. To improve the representation and clustering abilities, DSCF-Net explicitly considers discovering hidden deep semantic features, enhancing the robustness proper-ties of the deep factorization to noise and preserving the local man-ifold structures of deep features. Specifically, DSCF-Net seamlessly integrates the robust deep concept factorization, deep self-expressive representation and adaptive locality preserving feature learning into a unified framework. To discover hidden deep repre-sentations, DSCF-Net designs a hierarchical factorization architec-ture using multiple layers of linear transformations, where the hierarchical representation is performed by formulating the prob-lem as optimizing the basis concepts in each layer to improve the representation indirectly. DSCF-Net also improves the robustness by subspace recovery for sparse error correction firstly and then performs the deep factorization in the recovered visual subspace. To obtain locality-preserving representations, we also present an adaptive deep self-representative weighting strategy by using the coefficient matrix as the adaptive reconstruction weights to keep the locality of representations. Extensive comparison results with several other related models show that DSCF-Net delivers state-of-the-art performance on several public databases.",0
"This paper explores the problem of unsupervised deep representation learning and introduces a new framework, known as the Deep Self-representative Concept Factorization Network (DSCF-Net), for clustering deep features. DSCF-Net aims to enhance representation and clustering abilities by considering hidden deep semantic features, improving robustness to noise, and preserving local manifold structures of deep features. It achieves this through a combination of robust deep concept factorization, deep self-expressive representation, and adaptive locality preserving feature learning. DSCF-Net uses a hierarchical factorization architecture with multiple layers of linear transformations to discover hidden deep representations. It also improves robustness by performing subspace recovery for sparse error correction before deep factorization. To obtain locality-preserving representations, DSCF-Net uses an adaptive deep self-representative weighting strategy with the coefficient matrix as the adaptive reconstruction weights. Comparison results with other related models demonstrate that DSCF-Net outperforms them on several public databases.",1
"Existing vision-language methods typically support two languages at a time at most. In this paper, we present a modular approach which can easily be incorporated into existing vision-language methods in order to support many languages. We accomplish this by learning a single shared Multimodal Universal Language Embedding (MULE) which has been visually-semantically aligned across all languages. Then we learn to relate MULE to visual data as if it were a single language. Our method is not architecture specific, unlike prior work which typically learned separate branches for each language, enabling our approach to easily be adapted to many vision-language methods and tasks. Since MULE learns a single language branch in the multimodal model, we can also scale to support many languages, and languages with fewer annotations can take advantage of the good representation learned from other (more abundant) language data. We demonstrate the effectiveness of MULE on the bidirectional image-sentence retrieval task, supporting up to four languages in a single model. In addition, we show that Machine Translation can be used for data augmentation in multilingual learning, which, combined with MULE, improves mean recall by up to 21.9% on a single-language compared to prior work, with the most significant gains seen on languages with relatively few annotations. Our code is publicly available.",0
"Current vision-language methods usually only accommodate a maximum of two languages simultaneously. This study introduces a modular approach that can easily integrate with existing vision-language methods, enabling support for multiple languages. This is achieved by training a unified Multimodal Universal Language Embedding (MULE) that has been aligned semantically and visually across all languages, and then establishing the relationship between MULE and visual data as if it were a single language. This approach is not architecture-specific, unlike previous methods that required separate branches for each language, making it adaptable to various vision-language methods and tasks. MULE is advantageous because it learns a single language branch in the multimodal model, allowing it to support multiple languages and less annotated languages to benefit from the representation learned from more abundant language data. The effectiveness of MULE is demonstrated in the bidirectional image-sentence retrieval task, supporting up to four languages in a single model. In addition, the study shows that Machine Translation can enhance multilingual learning through data augmentation, resulting in a maximum mean recall improvement of 21.9% on a single language compared to prior work, with the most significant gains on languages with relatively few annotations. The code is publicly available.",1
"Knowledge representation of graph-based systems is fundamental across many disciplines. To date, most existing methods for representation learning primarily focus on networks with simplex labels, yet real-world objects (nodes) are inherently complex in nature and often contain rich semantics or labels, e.g., a user may belong to diverse interest groups of a social network, resulting in multi-label networks for many applications. The multi-label network nodes not only have multiple labels for each node, such labels are often highly correlated making existing methods ineffective or fail to handle such correlation for node representation learning. In this paper, we propose a novel multi-label graph convolutional network (ML-GCN) for learning node representation for multi-label networks. To fully explore label-label correlation and network topology structures, we propose to model a multi-label network as two Siamese GCNs: a node-node-label graph and a label-label-node graph. The two GCNs each handle one aspect of representation learning for nodes and labels, respectively, and they are seamlessly integrated under one objective function. The learned label representations can effectively preserve the inner-label interaction and node label properties, and are then aggregated to enhance the node representation learning under a unified training framework. Experiments and comparisons on multi-label node classification validate the effectiveness of our proposed approach.",0
"The representation of knowledge in graph-based systems is essential in various fields. However, current methods for representation learning mainly concentrate on networks with simple labels, while real-life objects (nodes) are complex in nature and typically have rich semantics or labels. For instance, a user can belong to various interest groups in a social network, leading to multi-label networks for several applications. Multi-label network nodes have multiple labels that are often highly correlated, making existing methods insufficient or incapable of handling such correlation for node representation learning. This paper suggests a new approach, the multi-label graph convolutional network (ML-GCN), to learn node representation for multi-label networks. To fully explore label-label correlation and network topology structures, the authors propose modeling a multi-label network as two Siamese GCNs: a node-node-label graph and a label-label-node graph. The two GCNs each handle one aspect of representation learning for nodes and labels, respectively, and they are seamlessly integrated under one objective function. The learned label representations can effectively preserve the inner-label interaction and node label properties, and are then aggregated to enhance the node representation learning under a unified training framework. The proposed approach's effectiveness is validated through experiments and comparisons on multi-label node classification.",1
"Numerous control and learning problems face the situation where sequences of high-dimensional highly dependent data are available but no or little feedback is provided to the learner, which makes any inference rather challenging. To address this challenge, we formulate the following problem. Given a series of observations $X_0,\dots,X_n$ coming from a large (high-dimensional) space $\mathcal X$, find a representation function $f$ mapping $\mathcal X$ to a finite space $\mathcal Y$ such that the series $f(X_0),\dots,f(X_n)$ preserves as much information as possible about the original time-series dependence in $X_0,\dots,X_n$. We show that, for stationary time series, the function $f$ can be selected as the one maximizing a certain information criterion that we call time-series information. Some properties of this functions are investigated, including its uniqueness and consistency of its empirical estimates.   Implications for the problem of optimal control are presented.",0
"When working with sequences of data that are highly dependent and have a high dimension, it can be challenging to make inferences without feedback. To tackle this issue, we have formulated a problem in which we aim to find a function called a representation function, denoted as $f$, that maps the large space $\mathcal X$ to a finite space $\mathcal Y$. The objective of $f$ is to preserve as much information as possible about the original time-series dependence in $X_0,\dots,X_n$, given a series of observations. We propose selecting $f$ as the function that maximizes a specific information criterion, which we refer to as time-series information, for stationary time series. We investigate several properties of this function, including its uniqueness and the consistency of its empirical estimates, and discuss its implications for the problem of optimal control.",1
"Adversarial representation learning is a promising paradigm for obtaining data representations that are invariant to certain sensitive attributes while retaining the information necessary for predicting target attributes. Existing approaches solve this problem through iterative adversarial minimax optimization and lack theoretical guarantees. In this paper, we first study the ""linear"" form of this problem i.e., the setting where all the players are linear functions. We show that the resulting optimization problem is both non-convex and non-differentiable. We obtain an exact closed-form expression for its global optima through spectral learning and provide performance guarantees in terms of analytical bounds on the achievable utility and invariance. We then extend this solution and analysis to non-linear functions through kernel representation. Numerical experiments on UCI, Extended Yale B and CIFAR-100 datasets indicate that, (a) practically, our solution is ideal for ""imparting"" provable invariance to any biased pre-trained data representation, and (b) empirically, the trade-off between utility and invariance provided by our solution is comparable to iterative minimax optimization of existing deep neural network based approaches. Code is available at https://github.com/human-analysis/Kernel-ARL",0
"The idea of adversarial representation learning shows promise in creating data representations that are unaffected by specific sensitive attributes while still retaining the necessary information for predicting target attributes. However, current methods rely on iterative adversarial minimax optimization without any theoretical guarantees. This paper focuses on the ""linear"" version of this problem, where all players are linear functions. The resulting optimization problem is non-convex and non-differentiable, but we solve it using spectral learning and provide analytical bounds on achievable utility and invariance. We extend this solution to non-linear functions through kernel representation and conduct numerical experiments on several datasets. Our results show that our solution is practical for imparting provable invariance to biased pre-trained data representations and that the trade-off between utility and invariance is comparable to existing deep neural network based approaches. Code for our method is available at https://github.com/human-analysis/Kernel-ARL.",1
"Graph Neural Networks (GNNs), which generalize deep neural networks to graph-structured data, have drawn considerable attention and achieved state-of-the-art performance in numerous graph related tasks. However, existing GNN models mainly focus on designing graph convolution operations. The graph pooling (or downsampling) operations, that play an important role in learning hierarchical representations, are usually overlooked. In this paper, we propose a novel graph pooling operator, called Hierarchical Graph Pooling with Structure Learning (HGP-SL), which can be integrated into various graph neural network architectures. HGP-SL incorporates graph pooling and structure learning into a unified module to generate hierarchical representations of graphs. More specifically, the graph pooling operation adaptively selects a subset of nodes to form an induced subgraph for the subsequent layers. To preserve the integrity of graph's topological information, we further introduce a structure learning mechanism to learn a refined graph structure for the pooled graph at each layer. By combining HGP-SL operator with graph neural networks, we perform graph level representation learning with focus on graph classification task. Experimental results on six widely used benchmarks demonstrate the effectiveness of our proposed model.",0
"Graph Neural Networks (GNNs) have gained significant attention for their ability to extend deep neural networks to graph-structured data, leading to state-of-the-art performance in various graph-related tasks. Despite this, current GNN models primarily concentrate on creating graph convolution operations while neglecting the significance of graph pooling (or downsampling) operations, which are crucial for learning hierarchical representations. This study introduces a new graph pooling operator called Hierarchical Graph Pooling with Structure Learning (HGP-SL), which can be incorporated into different GNN architectures. HGP-SL combines graph pooling and structure learning into a single module to generate hierarchical representations of graphs. The graph pooling operation selects a subset of nodes to form an induced subgraph for subsequent layers, while a structure learning mechanism preserves the graph's topological information by refining the graph structure at each layer. Using HGP-SL with GNNs, this study focuses on graph classification tasks and demonstrates the effectiveness of the proposed model on six widely used benchmarks.",1
"How much does having visual priors about the world (e.g. the fact that the world is 3D) assist in learning to perform downstream motor tasks (e.g. navigating a complex environment)? What are the consequences of not utilizing such visual priors in learning? We study these questions by integrating a generic perceptual skill set (a distance estimator, an edge detector, etc.) within a reinforcement learning framework (see Fig. 1). This skill set (""mid-level vision"") provides the policy with a more processed state of the world compared to raw images.   Our large-scale study demonstrates that using mid-level vision results in policies that learn faster, generalize better, and achieve higher final performance, when compared to learning from scratch and/or using state-of-the-art visual and non-visual representation learning methods. We show that conventional computer vision objectives are particularly effective in this regard and can be conveniently integrated into reinforcement learning frameworks. Finally, we found that no single visual representation was universally useful for all downstream tasks, hence we computationally derive a task-agnostic set of representations optimized to support arbitrary downstream tasks.",0
"By incorporating a set of perceptual skills, such as distance estimation and edge detection, into a reinforcement learning framework, we investigate the benefits of having visual priors when learning motor tasks, such as navigating complex environments. We also examine the consequences of not utilizing such visual priors. Our study reveals that using a ""mid-level vision"" skill set results in quicker learning, better generalization, and superior final performance compared to learning from scratch or using state-of-the-art representation learning methods. Moreover, we discovered that no single visual representation was universally effective for all downstream tasks, and thus, we developed a task-agnostic representation set optimized for various motor tasks. Conventional computer vision objectives were found to be particularly advantageous and can be conveniently integrated into reinforcement learning frameworks.",1
"Acquiring ground truth labels for unlabelled data can be a costly procedure, since it often requires manual labour that is error-prone. Consequently, the available amount of labelled data is increasingly reduced due to the limitations of manual data labelling. It is possible to increase the amount of labelled data samples by performing automated labelling or crowd-sourcing the annotation procedure. However, they often introduce noise or uncertainty in the labelset, that leads to decreased performance of supervised deep learning methods. On the other hand, weak supervision methods remain robust during noisy labelsets or can be effective even with low amounts of labelled data. In this paper we evaluate the effectiveness of a representation learning method that uses external categorical evidence called ""Evidence Transfer"", against low amount of corresponding evidence termed as incomplete evidence. Evidence transfer is a robust solution against external unknown categorical evidence that can introduce noise or uncertainty. In our experimental evaluation, evidence transfer proves to be effective and robust against different levels of incompleteness, for two types of incomplete evidence.",0
"Obtaining accurate labels for unlabelled data can be an expensive process, as it often involves manual labour that is prone to errors. As a result, the amount of available labelled data is decreasing due to the limitations of manual labelling. To increase the number of labelled data samples, automated labelling or crowd-sourcing annotation procedures can be used, but these methods often produce noisy or uncertain labels that can reduce the performance of supervised deep learning techniques. However, weak supervision methods can remain robust even in the presence of noisy labels or with limited labelled data. This study evaluates the effectiveness of a representation learning technique called ""Evidence Transfer"" that uses external categorical evidence to address incomplete evidence. Evidence transfer is a resilient solution against external unknown categorical evidence that may introduce noise or uncertainty. In the experimental evaluation, evidence transfer proves to be effective and robust against different levels of incompleteness for two types of incomplete evidence.",1
"Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.",0
"Federated learning is a method that enhances data privacy and efficiency in machine learning conducted across networks of distributed devices, like mobile phones, IoT and wearable devices. However, models trained through federated learning may not be able to generalize to new devices due to domain shift - the occurrence of statistical differences between the labeled data collected by source nodes and the unlabeled data of the target node. This paper proposes a structured solution to the problem of federated domain adaptation, which intends to align the representations learned by different nodes with the target node's data distribution. Our approach is an extension of adversarial adaptation techniques that adapts to the constraints of the federated setting. We also utilize a dynamic attention mechanism and feature disentanglement to enhance knowledge transfer. We conducted various experiments on image and text classification tasks and achieved promising results under the unsupervised federated domain adaptation setting.",1
"In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). In order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.",0
"The objective of our research is to examine the significance of locality and compositionality in the development of representations for Zero Shot Learning (ZSL). To ascertain the relevance of these properties, we introduce an additional restriction that contrasts with the majority of recent ZSL studies, in which no pre-training on other datasets such as ImageNet is conducted. Through our experiments, we discovered that both locality, which pertains to small aspects of the input, and compositionality, which entails how effectively the learned representations can be expressed as a function of a smaller vocabulary, play a crucial role in generalization. These findings emphasize the need for future research in representation learning to prioritize models that are more aware of local features.",1
"How to learn a discriminative fine-grained representation is a key point in many computer vision applications, such as person re-identification, fine-grained classification, fine-grained image retrieval, etc. Most of the previous methods focus on learning metrics or ensemble to derive better global representation, which are usually lack of local information. Based on the considerations above, we propose a novel Attribute-Aware Attention Model ($A^3M$), which can learn local attribute representation and global category representation simultaneously in an end-to-end manner. The proposed model contains two attention models: attribute-guided attention module uses attribute information to help select category features in different regions, at the same time, category-guided attention module selects local features of different attributes with the help of category cues. Through this attribute-category reciprocal process, local and global features benefit from each other. Finally, the resulting feature contains more intrinsic information for image recognition instead of the noisy and irrelevant features. Extensive experiments conducted on Market-1501, CompCars, CUB-200-2011 and CARS196 demonstrate the effectiveness of our $A^3M$. Code is available at https://github.com/iamhankai/attribute-aware-attention.",0
"Learning a discriminative fine-grained representation is crucial for numerous computer vision applications, such as fine-grained classification, person re-identification, and fine-grained image retrieval. However, previous methods have focused on learning metrics or ensembles to improve global representation, lacking local information. In response, we propose the Attribute-Aware Attention Model ($A^3M$), which simultaneously learns local attribute representation and global category representation in an end-to-end manner. The $A^3M$ comprises two attention models: the attribute-guided attention module selects category features in different regions using attribute information, while the category-guided attention module chooses local features of different attributes with the aid of category cues. This reciprocal process between attribute and category ensures that local and global features benefit from each other, resulting in a more informative feature for image recognition. We conducted extensive experiments on Market-1501, CUB-200-2011, CompCars, and CARS196, demonstrating the effectiveness of our proposed model. Our code is available at https://github.com/iamhankai/attribute-aware-attention.",1
"Probably the most important problem in machine learning is the preliminary biasing of a learner's hypothesis space so that it is small enough to ensure good generalisation from reasonable training sets, yet large enough that it contains a good solution to the problem being learnt. In this paper a mechanism for {\em automatically} learning or biasing the learner's hypothesis space is introduced. It works by first learning an appropriate {\em internal representation} for a learning environment and then using that representation to bias the learner's hypothesis space for the learning of future tasks drawn from the same environment.   An internal representation must be learnt by sampling from {\em many similar tasks}, not just a single task as occurs in ordinary machine learning. It is proved that the number of examples $m$ {\em per task} required to ensure good generalisation from a representation learner obeys $m = O(a+b/n)$ where $n$ is the number of tasks being learnt and $a$ and $b$ are constants. If the tasks are learnt independently ({\em i.e.} without a common representation) then $m=O(a+b)$. It is argued that for learning environments such as speech and character recognition $b\gg a$ and hence representation learning in these environments can potentially yield a drastic reduction in the number of examples required per task. It is also proved that if $n = O(b)$ (with $m=O(a+b/n)$) then the representation learnt will be good for learning novel tasks from the same environment, and that the number of examples required to generalise well on a novel task will be reduced to $O(a)$ (as opposed to $O(a+b)$ if no representation is used).   It is shown that gradient descent can be used to train neural network representations and experiment results are reported providing strong qualitative support for the theoretical results.",0
"One of the most significant challenges in machine learning is to limit the learner's hypothesis space to ensure that it is not too large but contains a suitable solution for the problem being learned. This paper presents a method for automatically biasing the learner's hypothesis space by first learning an appropriate internal representation of the learning environment and then using it to bias the hypothesis space for future tasks from the same environment. To learn an internal representation, multiple similar tasks must be sampled, unlike traditional machine learning that only uses a single task. The paper proves that the number of examples required per task to ensure good generalization from a representation learner is proportional to $O(a+b/n)$, where $n$ is the number of tasks being learned, and $a$ and $b$ are constants. If the tasks are learned independently, then the number of examples required is proportional to $O(a+b)$. The paper argues that in environments such as speech and character recognition, $b$ is significantly larger than $a$, and representation learning can significantly reduce the number of examples required per task. Additionally, if $n=O(b)$, the learned representation can generalize well for new tasks from the same environment, and the number of examples required to generalize well on a new task will be reduced to $O(a)$. The paper also shows that gradient descent can train neural network representations, and experimental results support the theoretical results.",1
"In this paper, we focus on graph representation learning of heterogeneous information network (HIN), in which various types of vertices are connected by various types of relations. Most of the existing methods conducted on HIN revise homogeneous graph embedding models via meta-paths to learn low-dimensional vector space of HIN. In this paper, we propose a novel Heterogeneous Graph Structural Attention Neural Network (HetSANN) to directly encode structural information of HIN without meta-path and achieve more informative representations. With this method, domain experts will not be needed to design meta-path schemes and the heterogeneous information can be processed automatically by our proposed model. Specifically, we implicitly represent heterogeneous information using the following two methods: 1) we model the transformation between heterogeneous vertices through a projection in low-dimensional entity spaces; 2) afterwards, we apply the graph neural network to aggregate multi-relational information of projected neighborhood by means of attention mechanism. We also present three extensions of HetSANN, i.e., voices-sharing product attention for the pairwise relationships in HIN, cycle-consistency loss to retain the transformation between heterogeneous entity spaces, and multi-task learning with full use of information. The experiments conducted on three public datasets demonstrate that our proposed models achieve significant and consistent improvements compared to state-of-the-art solutions.",0
"The main focus of this paper is on learning graph representations of heterogeneous information networks (HINs), which consist of various types of vertices connected by different types of relations. Existing methods have mainly used meta-paths to modify homogeneous graph embedding models and learn low-dimensional vector spaces of HINs. However, in this paper, we propose a new approach called the Heterogeneous Graph Structural Attention Neural Network (HetSANN), which directly encodes the structural information of HINs without the use of meta-paths, resulting in more informative representations. This eliminates the need for domain experts to design meta-path schemes, allowing our model to automatically process heterogeneous information. Our approach involves two methods: 1) modeling the transformation between heterogeneous vertices using a low-dimensional entity space projection, and 2) using a graph neural network to aggregate multi-relational information of projected neighborhoods through an attention mechanism. We also present three HetSANN extensions: a voices-sharing product attention for pairwise relationships in HINs, cycle-consistency loss to preserve the transformation between heterogeneous entity spaces, and multi-task learning to make full use of information. Our experiments on three public datasets show that our proposed models outperform state-of-the-art solutions with significant and consistent improvements.",1
"Constructing high-quality generative models for 3D shapes is a fundamental task in computer vision with diverse applications in geometry processing, engineering, and design. Despite the recent progress in deep generative modelling, synthesis of finely detailed 3D surfaces, such as high-resolution point clouds, from scratch has not been achieved with existing approaches. In this work, we propose to employ the latent-space Laplacian pyramid representation within a hierarchical generative model for 3D point clouds. We combine the recently proposed latent-space GAN and Laplacian GAN architectures to form a multi-scale model capable of generating 3D point clouds at increasing levels of detail. Our evaluation demonstrates that our model outperforms the existing generative models for 3D point clouds.",0
"Creating top-notch generative models for 3D shapes is a crucial undertaking in computer vision, having a wide range of applications in geometry processing, engineering, and design. Despite the strides made in deep generative modelling, existing methods have failed to generate finely-detailed 3D surfaces, including high-resolution point clouds, from scratch. To address this, we propose incorporating the latent-space Laplacian pyramid representation into a hierarchical generative model for 3D point clouds. Through combining latent-space GAN and Laplacian GAN architectures, we develop a multi-scale model that can generate 3D point clouds at progressively more intricate levels. Our model surpasses existing generative models for 3D point clouds, as demonstrated by our evaluation.",1
"Graph Convolutional Networks (GCNs) have gained significant developments in representation learning on graphs. However, current GCNs suffer from two common challenges: 1) GCNs are only effective with shallow structures; stacking multiple GCN layers will lead to over-smoothing. 2) GCNs do not scale well with large, dense graphs due to the recursive neighborhood expansion. We generalize the propagation strategies of current GCNs as a \emph{""Sink$\to$Source""} mode, which seems to be an underlying cause of the two challenges. To address these issues intrinsically, in this paper, we study the information propagation mechanism in a \emph{""Source$\to$Sink""} mode. We introduce a new concept ""information flow path"" that explicitly defines where information originates and how it diffuses. Then a novel framework, namely Flow Graph Network (FlowGN), is proposed to learn node representations. FlowGN is computationally efficient and flexible in propagation strategies. Moreover, FlowGN decouples the layer structure from the information propagation process, removing the interior constraint of applying deep structures in traditional GCNs. Further experiments on public datasets demonstrate the superiority of FlowGN against state-of-the-art GCNs.",0
"Significant progress has been made in representation learning on graphs with Graph Convolutional Networks (GCNs). However, two common challenges affect current GCNs. Firstly, they are only effective with shallow structures, and stacking multiple GCN layers leads to over-smoothing. Secondly, they do not scale well with large, dense graphs due to recursive neighborhood expansion. These challenges are caused by the ""Sink$\to$Source"" mode of information propagation in current GCNs. To address these issues, we propose a new ""Source$\to$Sink"" mode of information propagation in our novel framework, Flow Graph Network (FlowGN). The FlowGN framework uses the concept of an ""information flow path"" to define the origin and diffusion of information. It is computationally efficient, flexible, and allows for deep structures without constraints. Our experiments on public datasets show that FlowGN outperforms state-of-the-art GCNs.",1
"Normalization methods are essential components in convolutional neural networks (CNNs). They either standardize or whiten data using statistics estimated in predefined sets of pixels. Unlike existing works that design normalization techniques for specific tasks, we propose Switchable Whitening (SW), which provides a general form unifying different whitening methods as well as standardization methods. SW learns to switch among these operations in an end-to-end manner. It has several advantages. First, SW adaptively selects appropriate whitening or standardization statistics for different tasks (see Fig.1), making it well suited for a wide range of tasks without manual design. Second, by integrating benefits of different normalizers, SW shows consistent improvements over its counterparts in various challenging benchmarks. Third, SW serves as a useful tool for understanding the characteristics of whitening and standardization techniques. We show that SW outperforms other alternatives on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For example, without bells and whistles, we achieve state-of-the-art performance with 45.33% mIoU on the ADE20K dataset. Code is available at https://github.com/XingangPan/Switchable-Whitening.",0
"Convolutional neural networks (CNNs) rely on normalization methods to standardize or whiten data using statistics derived from predefined sets of pixels. While previous works have developed normalization techniques tailored for specific tasks, our proposed Switchable Whitening (SW) offers a general form that unites various whitening and standardization methods. SW learns to alternate between these operations in an end-to-end process, providing several benefits. Firstly, SW automatically selects the appropriate whitening or standardization statistics for different tasks, making it versatile without requiring manual design. Secondly, by integrating the advantages of different normalizers, SW consistently outperforms its counterparts in challenging benchmarks. Thirdly, SW serves as a useful tool for understanding the attributes of whitening and standardization techniques. Our results demonstrate that SW achieves state-of-the-art performance in image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For instance, we achieved a 45.33% mIoU on the ADE20K dataset without any additional features. Our code is accessible at https://github.com/XingangPan/Switchable-Whitening.",1
"Human activities often occur in specific scene contexts, e.g., playing basketball on a basketball court. Training a model using existing video datasets thus inevitably captures and leverages such bias (instead of using the actual discriminative cues). The learned representation may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning. Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) a human mask confusion loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, including action classification, temporal localization, and spatio-temporal action detection. Our results show consistent improvement over the baseline model without debiasing.",0
"When humans engage in activities, they typically do so within a specific context, such as playing basketball on a basketball court. However, when models are trained using existing video datasets, they may inadvertently incorporate this bias rather than focusing on the actual cues that distinguish between different actions. As a result, the representations learned by these models may not be suitable for new action classes or tasks. To address this issue, we propose a method to mitigate scene bias in video representation learning. This involves modifying the standard cross-entropy loss for action classification by adding an adversarial loss for scene types and a human mask confusion loss for videos where human actors are obscured. By doing so, we encourage the model to learn representations that cannot predict scene types or correct actions in the absence of evidence. We demonstrate the effectiveness of our approach by applying our pre-trained model to three different tasks, and show that our method consistently outperforms the baseline model that does not incorporate debiasing.",1
"This work proposes kernel transform learning. The idea of dictionary learning is well known; it is a synthesis formulation where a basis is learnt along with the coefficients so as to generate or synthesize the data. Transform learning is its analysis equivalent; the transforms operates or analyses on the data to generate the coefficients. The concept of kernel dictionary learning has been introduced in the recent past, where the dictionary is represented as a linear combination of non-linear version of the data. Its success has been showcased in feature extraction. In this work we propose to kernelize transform learning on line similar to kernel dictionary learning. An efficient solution for kernel transform learning has been proposed especially for problems where the number of samples is much larger than the dimensionality of the input samples making the kernel matrix very high dimensional. Kernel transform learning has been compared with other representation learning tools like autoencoder, restricted Boltzmann machine as well as with dictionary learning (and its kernelized version). Our proposed kernel transform learning yields better results than all the aforesaid techniques; experiments have been carried out on benchmark databases.",0
"The presented study introduces kernel transform learning, which is the analytical counterpart of the well-known dictionary learning. Dictionary learning involves learning a basis and its coefficients to synthesize data, while transform learning operates on data to generate coefficients. Recently, kernel dictionary learning has gained popularity as it represents a dictionary as a linear combination of non-linear versions of data and has shown success in feature extraction. This work proposes a kernelized version of transform learning, specifically for problems where the number of samples is much greater than the dimensionality of input samples. Comparison with other representation learning techniques such as autoencoder, restricted Boltzmann machine, and dictionary learning (including its kernelized version) demonstrates that kernel transform learning outperforms all of them, as confirmed by experiments on benchmark databases.",1
"Conventionally, autoencoders are unsupervised representation learning tools. In this work, we propose a novel discriminative autoencoder. Use of supervised discriminative learning ensures that the learned representation is robust to variations commonly encountered in image datasets. Using the basic discriminating autoencoder as a unit, we build a stacked architecture aimed at extracting relevant representation from the training data. The efficiency of our feature extraction algorithm ensures a high classification accuracy with even simple classification schemes like KNN (K-nearest neighbor). We demonstrate the superiority of our model for representation learning by conducting experiments on standard datasets for character/image recognition and subsequent comparison with existing supervised deep architectures like class sparse stacked autoencoder and discriminative deep belief network.",0
"Traditionally, autoencoders are employed for unsupervised representation learning. However, our research introduces a distinctive discriminative autoencoder. The use of supervised discriminative learning guarantees that the learned representation is robust enough to tolerate common variations in image datasets. We construct a stacked architecture using the basic discriminating autoencoder as a unit, which aims to extract relevant representation from the training data. Our feature extraction algorithm is highly efficient and results in high classification accuracy, even with simple classification schemes like KNN (K-nearest neighbor). By conducting experiments on standard datasets for character/image recognition, we demonstrate the superiority of our model for representation learning when compared to existing supervised deep architectures like class sparse stacked autoencoder and discriminative deep belief network.",1
"As deep neural networks become more adept at traditional tasks, many of the most exciting new challenges concern multimodality---observations that combine diverse types, such as image and text. In this paper, we introduce a family of multimodal deep generative models derived from variational bounds on the evidence (data marginal likelihood). As part of our derivation we find that many previous multimodal variational autoencoders used objectives that do not correctly bound the joint marginal likelihood across modalities. We further generalize our objective to work with several types of deep generative model (VAE, GAN, and flow-based), and allow use of different model types for different modalities. We benchmark our models across many image, label, and text datasets, and find that our multimodal VAEs excel with and without weak supervision. Additional improvements come from use of GAN image models with VAE language models. Finally, we investigate the effect of language on learned image representations through a variety of downstream tasks, such as compositionally, bounding box prediction, and visual relation prediction. We find evidence that these image representations are more abstract and compositional than equivalent representations learned from only visual data.",0
"The increasing proficiency of deep neural networks in conventional tasks has led to a growing interest in multimodality, which involves combining varied types of observations, including image and text. This paper presents a range of multimodal deep generative models that are developed from variational bounds on the evidence or data marginal likelihood. During our research, we discovered that several prior multimodal variational autoencoders used objectives that did not accurately bound the joint marginal likelihood across modalities. We extended our objective to work with various deep generative model types, like VAE, GAN, and flow-based, allowing the use of distinct model types for various modalities. We evaluated our models on numerous datasets, including image, label, and text, and discovered that our multimodal VAEs outperformed with and without weak supervision. Additionally, incorporating GAN image models with VAE language models provided further enhancements. Ultimately, we investigated the effect of language on learned image representations across various downstream tasks, such as compositionally, bounding box prediction, and visual relation prediction. We found that these image representations are more abstract and compositional than equivalent representations learned from only visual data.",1
"Recent progress of self-supervised visual representation learning has achieved remarkable success on many challenging computer vision benchmarks. However, whether these techniques can be used for domain adaptation has not been explored. In this work, we propose a generic method for self-supervised domain adaptation, using object recognition and semantic segmentation of urban scenes as use cases. Focusing on simple pretext/auxiliary tasks (e.g. image rotation prediction), we assess different learning strategies to improve domain adaptation effectiveness by self-supervision. Additionally, we propose two complementary strategies to further boost the domain adaptation accuracy on semantic segmentation within our method, consisting of prediction layer alignment and batch normalization calibration. The experimental results show adaptation levels comparable to most studied domain adaptation methods, thus, bringing self-supervision as a new alternative for reaching domain adaptation. The code is available at https://github.com/Jiaolong/self-supervised-da.",0
"Many computer vision benchmarks have seen remarkable success through recent progress in self-supervised visual representation learning. However, the possibility of using these techniques for domain adaptation has yet to be explored. This study proposes a generic method for self-supervised domain adaptation, using object recognition and semantic segmentation of urban scenes as examples. The study focuses on simple pretext/auxiliary tasks, such as image rotation prediction, to assess different learning strategies for enhancing domain adaptation effectiveness through self-supervision. In addition, the study suggests two complementary strategies, prediction layer alignment and batch normalization calibration, to further improve the accuracy of domain adaptation in semantic segmentation. The experimental results demonstrate that the adaptation levels achieved by the proposed method are comparable to those of most existing domain adaptation techniques. Thus, self-supervision can be considered a new alternative for achieving domain adaptation. The code is available at https://github.com/Jiaolong/self-supervised-da.",1
"We study nonconvex optimization landscapes for learning overcomplete representations, including learning (i) sparsely used overcomplete dictionaries and (ii) convolutional dictionaries, where these unsupervised learning problems find many applications in high-dimensional data analysis. Despite the empirical success of simple nonconvex algorithms, theoretical justifications of why these methods work so well are far from satisfactory. In this work, we show these problems can be formulated as $\ell^4$-norm optimization problems with spherical constraint, and study the geometric properties of their nonconvex optimization landscapes. For both problems, we show the nonconvex objectives have benign (global) geometric structures, in the sense that every local minimizer is close to one of the target solutions and every saddle point exhibits negative curvature. This discovery enables the development of guaranteed global optimization methods using simple initializations. For both problems, we show the nonconvex objectives have benign geometric structures -- every local minimizer is close to one of the target solutions and every saddle point exhibits negative curvature -- either in the entire space or within a sufficiently large region. This discovery ensures local search algorithms (such as Riemannian gradient descent) with simple initializations approximately find the target solutions. Finally, numerical experiments justify our theoretical discoveries.",0
"Our focus is on exploring nonconvex optimization landscapes for the purpose of learning overcomplete representations, including those involving sparsely used overcomplete dictionaries and convolutional dictionaries. These unsupervised learning problems have many applications in high-dimensional data analysis, and straightforward nonconvex algorithms have demonstrated empirical success. However, theoretical justifications for their effective performance are yet to be satisfactory. Our research aims to address this gap by formulating these problems as $\ell^4$-norm optimization problems with spherical constraints and analyzing the geometric properties of their nonconvex optimization landscapes. Our findings reveal that both problems have benign geometric structures, with every local minimizer being close to one of the target solutions and every saddle point exhibiting negative curvature. This discovery has enabled us to develop global optimization methods with simple initializations. We have also conducted numerical experiments to validate our theoretical discoveries.",1
"Feature selection methods have an important role on the readability of data and the reduction of complexity of learning algorithms. In recent years, a variety of efforts are investigated on feature selection problems based on unsupervised viewpoint due to the laborious labeling task on large datasets. In this paper, we propose a novel approach on unsupervised feature selection initiated from the subspace clustering to preserve the similarities by representation learning of low dimensional subspaces among the samples. A self-expressive model is employed to implicitly learn the cluster similarities in an adaptive manner. The proposed method not only maintains the sample similarities through subspace clustering, but it also captures the discriminative information based on a regularized regression model. In line with the convergence analysis of the proposed method, the experimental results on benchmark datasets demonstrate the effectiveness of our approach as compared with the state of the art methods.",0
"The readability of data and the complexity of learning algorithms can be improved through the use of feature selection methods. However, labeling large datasets can be a laborious task, leading to efforts in unsupervised feature selection. This paper presents a new approach to unsupervised feature selection using subspace clustering to maintain similarities between samples. A self-expressive model is used to adaptively learn cluster similarities, while also capturing discriminative information through a regularized regression model. Our approach is shown to be effective through convergence analysis and experimental results on benchmark datasets, outperforming state of the art methods.",1
"Sparse representations have been shown to be useful in deep reinforcement learning for mitigating catastrophic interference and improving the performance of agents in terms of cumulative reward. Previous results were based on a two step process were the representation was learned offline and the action-value function was learned online afterwards. In this paper, we investigate if it is possible to learn a sparse representation and the action-value function simultaneously and incrementally. We investigate this question by employing several regularization techniques and observing how they affect sparsity of the representation learned by a DQN agent in two different benchmark domains. Our results show that with appropriate regularization it is possible to increase the sparsity of the representations learned by DQN agents. Moreover, we found that learning sparse representations also resulted in improved performance in terms of cumulative reward. Finally, we found that the performance of the agents that learned a sparse representation was more robust to the size of the experience replay buffer. This last finding supports the long standing hypothesis that the overlap in representations learned by deep neural networks is the leading cause of catastrophic interference.",0
"The usefulness of sparse representations in deep reinforcement learning has been demonstrated to reduce catastrophic interference and enhance agent performance in cumulative rewards. Previous research used a two-step process, where the representation was learned offline and the action-value function was learned online. This study investigates the possibility of learning both the sparse representation and action-value function simultaneously and incrementally. By implementing various regularization techniques, the DQN agent's learned sparse representation was observed in two benchmark domains. Results indicate that appropriate regularization techniques can increase the sparsity of DQN agents' learned representations and improve cumulative rewards. Additionally, agents that learned sparse representations showed more robust performance in relation to the size of the experience replay buffer. This supports the hypothesis that deep neural network overlap in representations is the primary cause of catastrophic interference.",1
"It is not until recently that graph neural networks (GNNs) are adopted to perform graph representation learning, among which, those based on the aggregation of features within the neighborhood of a node achieved great success. However, despite such achievements, GNNs illustrate defects in identifying some common structural patterns which, unfortunately, play significant roles in various network phenomena. In this paper, we propose GraLSP, a GNN framework which explicitly incorporates local structural patterns into the neighborhood aggregation through random anonymous walks. Specifically, we capture local graph structures via random anonymous walks, powerful and flexible tools that represent structural patterns. The walks are then fed into the feature aggregation, where we design various mechanisms to address the impact of structural features, including adaptive receptive radius, attention and amplification. In addition, we design objectives that capture similarities between structures and are optimized jointly with node proximity objectives. With the adequate leverage of structural patterns, our model is able to outperform competitive counterparts in various prediction tasks in multiple datasets.",0
"Until recently, graph neural networks (GNNs) have not been widely used for graph representation learning. However, GNNs that rely on feature aggregation within the neighborhood of a node have achieved great success. Despite this success, GNNs have limitations in identifying common structural patterns that play significant roles in many network phenomena. We propose GraLSP, a GNN framework that incorporates local structural patterns into the neighborhood aggregation through random anonymous walks. We use these walks to capture local graph structures, which are flexible and powerful tools for representing structural patterns. We then introduce various mechanisms to address the impact of structural features, including adaptive receptive radius, attention, and amplification. Additionally, we design objectives that measure similarities between structures and optimize them jointly with node proximity objectives. With the inclusion of structural patterns, our model outperforms competitive counterparts in various prediction tasks across multiple datasets.",1
"Pre-training convolutional neural networks with weakly-supervised and self-supervised strategies is becoming increasingly popular for several computer vision tasks. However, due to the lack of strong discriminative signals, these learned representations may overfit to the pre-training objective (e.g., hashtag prediction) and not generalize well to downstream tasks. In this work, we present a simple strategy - ClusterFit (CF) to improve the robustness of the visual representations learned during pre-training. Given a dataset, we (a) cluster its features extracted from a pre-trained network using k-means and (b) re-train a new network from scratch on this dataset using cluster assignments as pseudo-labels. We empirically show that clustering helps reduce the pre-training task-specific information from the extracted features thereby minimizing overfitting to the same. Our approach is extensible to different pre-training frameworks -- weak- and self-supervised, modalities -- images and videos, and pre-training tasks -- object and action classification. Through extensive transfer learning experiments on 11 different target datasets of varied vocabularies and granularities, we show that ClusterFit significantly improves the representation quality compared to the state-of-the-art large-scale (millions / billions) weakly-supervised image and video models and self-supervised image models.",0
"The use of weakly-supervised and self-supervised strategies to pre-train convolutional neural networks is gaining popularity for various computer vision tasks. However, the lack of strong discriminative signals may cause the learned representations to overfit to the pre-training objective, such as hashtag prediction, and not perform well in downstream tasks. This paper proposes a simple strategy called ClusterFit (CF) to enhance the robustness of visual representations learned during pre-training. CF involves clustering the features of a dataset extracted from a pre-trained network using k-means, and then re-training a new network from scratch using cluster assignments as pseudo-labels. The authors demonstrate that clustering helps to reduce the pre-training task-specific information from the features, thereby reducing the risk of overfitting. CF is applicable to different pre-training frameworks, modalities, and tasks. The authors conduct extensive transfer learning experiments on 11 different target datasets with varying vocabularies and granularities, and show that ClusterFit significantly improves the quality of representations compared to state-of-the-art weakly-supervised and self-supervised models trained on millions or billions of images or videos.",1
"We tackle the challenge of disentangled representation learning in generative adversarial networks (GANs) from the perspective of regularized optimal transport (OT). Specifically, a smoothed OT loss gives rise to an implicit transportation plan between the latent space and the data space. Based on this theoretical observation, we exploit a structured regularization on the transportation plan to encourage a prescribed latent subspace to be informative. This yields the formulation of a novel informative OT-based GAN. By convex duality, we obtain the equivalent view that this leads to perturbed ground costs favoring sparsity in the informative latent dimensions. Practically, we devise a stable training algorithm for the proposed informative GAN. Our experiments support the hypothesis that such regularizations effectively yield the discovery of disentangled and interpretable latent representations. Our work showcases potential power of a regularized OT framework in the context of generative modeling through its access to the transport plan. Further challenges are addressed in this line.",0
"We approach the problem of disentangling representation learning in GANs using regularized optimal transport (OT) techniques. By using a smoothed OT loss, we can create a transportation plan between the latent and data spaces. This allows us to apply a structured regularization to the transportation plan, which promotes informative latent subspaces. This results in a new informative OT-based GAN that promotes sparsity in the informative latent dimensions. We have created a stable training algorithm for this new GAN and our experiments suggest that this approach can lead to the discovery of disentangled and interpretable latent representations. Our work demonstrates how a regularized OT framework can be used in generative modeling by accessing the transport plan, and we address further challenges in this area.",1
"The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations for a large training set of images. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced as ""pearl"") that learns invariant representations based on pretext tasks. We use PIRL with a commonly used pretext task that involves solving jigsaw puzzles. We find that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in self-supervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised learning of image representations with good invariance properties.",0
"The aim of self-supervised learning from images is to create image representations with semantic meaning, using pretext tasks that don't require semantic annotations for a large set of training images. Many pretext tasks produce representations that are covariant with image transformations, but we believe that semantic representations should be invariant to such transformations. Hence, we present Pretext-Invariant Representation Learning (PIRL), which learns invariant representations based on pretext tasks. We apply PIRL to the popular pretext task of solving jigsaw puzzles and observe a significant improvement in semantic quality of the learned image representations. Our approach achieves state-of-the-art results in self-supervised learning from images on various benchmarks. Remarkably, PIRL outperforms supervised pre-training in learning image representations for object detection, despite being unsupervised. These results highlight the potential of self-supervised learning for creating image representations with robust invariance properties.",1
"The ability of a graph neural network (GNN) to leverage both the graph topology and graph labels is fundamental to building discriminative node and graph embeddings. Building on previous work, we theoretically show that edGNN, our model for directed labeled graphs, is as powerful as the Weisfeiler-Lehman algorithm for graph isomorphism. Our experiments support our theoretical findings, confirming that graph neural networks can be used effectively for inference problems on directed graphs with both node and edge labels. Code available at https://github.com/guillaumejaume/edGNN.",0
"To construct discerning node and graph embeddings, it is crucial for a graph neural network (GNN) to effectively use both graph topology and labels. Our model, edGNN, for directed labeled graphs, is proven to be as potent as the Weisfeiler-Lehman algorithm for graph isomorphism, building on previous research. We have substantiated our theoretical discoveries with experiments that demonstrate the successful use of graph neural networks in inference tasks on directed graphs with both node and edge labels. The code can be accessed at https://github.com/guillaumejaume/edGNN.",1
"A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient search mechanisms implemented by attention mechanisms.",0
"A novel prior has been proposed for acquiring representations of high-level concepts that are commonly used in language. This prior can be used in conjunction with other priors to aid in disentangling abstract factors from one another. The inspiration for this approach is derived from cognitive neuroscience theories of consciousness, which view it as a bottleneck that selects only a few elements from a wider pool. These elements are then broadcast and subsequently processed by both perception and decision-making. The selected elements are thought to form a low-dimensional conscious state, which combines a few concepts to create a conscious thought. This thought is what one is immediately aware of at a particular moment. The proposed architectural and information-processing constraint is based on assumptions about the joint distribution of high-level concepts, which are generally true and consistent with natural language. A low-dimensional thought or conscious state is akin to a sentence, which involves few variables but has a high probability of being true. This is consistent with a joint distribution that has the form of a sparse factor graph, where each factor involves only a few variables, but creates a strong dip in the overall energy function. The consciousness prior also enables natural language utterances to be mapped to conscious states or to express classical AI knowledge in a form similar to facts and rules, while incorporating uncertainty and efficient search mechanisms provided by attention mechanisms.",1
"Representations of data that are invariant to changes in specified factors are useful for a wide range of problems: removing potential biases in prediction problems, controlling the effects of covariates, and disentangling meaningful factors of variation. Unfortunately, learning representations that exhibit invariance to arbitrary nuisance factors yet remain useful for other tasks is challenging. Existing approaches cast the trade-off between task performance and invariance in an adversarial way, using an iterative minimax optimization. We show that adversarial training is unnecessary and sometimes counter-productive; we instead cast invariant representation learning as a single information-theoretic objective that can be directly optimized. We demonstrate that this approach matches or exceeds performance of state-of-the-art adversarial approaches for learning fair representations and for generative modeling with controllable transformations.",0
"The ability to maintain consistent data representations despite alterations in specific factors is valuable for a variety of applications, such as eliminating potential biases in predictive situations, managing the impact of covariates, and separating significant factors of variability. However, developing representations that remain invariant to irrelevant variables while still being useful for other tasks is difficult. Current methods involve a trade-off between invariance and task performance through adversarial learning, but this approach can be counterproductive. Our research proposes an alternative method that utilizes a single information-based objective for invariant representation learning, resulting in performance that matches or surpasses state-of-the-art adversarial approaches for both fair representation learning and generative modeling with adjustable transformations.",1
"Supervised machine learning models often associate irrelevant nuisance factors with the prediction target, which hurts generalization. We propose a framework for training robust neural networks that induces invariance to nuisances through learning to discover and separate predictive and nuisance factors of data. We present an information theoretic formulation of our approach, from which we derive training objectives and its connections with previous methods. Empirical results on a wide array of datasets show that the proposed framework achieves state-of-the-art performance, without requiring nuisance annotations during training.",0
"The prediction accuracy of supervised machine learning models is affected by irrelevant factors, negatively impacting their generalization. In order to address this issue, we introduce a framework for training neural networks that ensures robustness by identifying and separating predictive and nuisance factors of data. Our approach is formulated using an information theoretic approach and incorporates training objectives and links to previous methods. Our framework yields state-of-the-art performance on various datasets, without necessitating nuisance annotations during training.",1
"A new semi-supervised ensemble algorithm called XGBOD (Extreme Gradient Boosting Outlier Detection) is proposed, described and demonstrated for the enhanced detection of outliers from normal observations in various practical datasets. The proposed framework combines the strengths of both supervised and unsupervised machine learning methods by creating a hybrid approach that exploits each of their individual performance capabilities in outlier detection. XGBOD uses multiple unsupervised outlier mining algorithms to extract useful representations from the underlying data that augment the predictive capabilities of an embedded supervised classifier on an improved feature space. The novel approach is shown to provide superior performance in comparison to competing individual detectors, the full ensemble and two existing representation learning based algorithms across seven outlier datasets.",0
"The article introduces a novel algorithm named XGBOD, which enhances the detection of outliers in practical datasets. The algorithm is a combination of supervised and unsupervised machine learning methods, which exploit their individual performance capabilities. XGBOD uses multiple unsupervised outlier mining algorithms to extract useful representations from the data, which augment the predictive capabilities of a supervised classifier. This approach has been proven to provide superior performance in comparison to other individual detectors, the full ensemble and two existing representation learning based algorithms across seven outlier datasets.",1
"Representations used for Facial Expression Recognition (FER) usually contain expression information along with identity features. In this paper, we propose a novel Disentangled Expression learning-Generative Adversarial Network (DE-GAN) which combines the concept of disentangled representation learning with residue learning to explicitly disentangle facial expression representation from identity information. In this method the facial expression representation is learned by reconstructing an expression image employing an encoder-decoder based generator. Unlike previous works using only expression residual learning for facial expression recognition, our method learns the disentangled expression representation along with the expressive component recorded by the encoder of DE-GAN. In order to improve the quality of synthesized expression images and the effectiveness of the learned disentangled expression representation, expression and identity classification is performed by the discriminator of DE-GAN. Experiments performed on widely used datasets (CK+, MMI, Oulu-CASIA) show that the proposed technique produces comparable or better results than state-of-the-art methods.",0
"Typically, facial expression recognition (FER) representations include both expression and identity features. However, our paper introduces a new approach, the Disentangled Expression learning-Generative Adversarial Network (DE-GAN), which utilizes disentangled representation learning and residue learning to explicitly separate facial expression representation from identity information. With this method, the facial expression representation is learned through an encoder-decoder based generator that reconstructs an expression image. Unlike previous methods that solely employ expression residual learning for FER, our approach also captures the disentangled expression representation along with the expressive component recorded by the encoder of DE-GAN. Additionally, the discriminator of DE-GAN performs expression and identity classification to enhance the quality of synthesized expression images and the effectiveness of the learned disentangled expression representation. Through experiments conducted on widely used datasets (CK+, MMI, Oulu-CASIA), the proposed technique proves to be comparable or even more effective than current state-of-the-art methods.",1
"Convolution is an integral operation that defines how the shape of one function is modified by another function. This powerful concept forms the basis of hierarchical feature learning in deep neural networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere ($\mathbb{S}^2$) or a unit ball ($\mathbb{B}^3$)---entails unique challenges. In this work, we propose a novel `\emph{volumetric convolution}' operation that can effectively model and convolve arbitrary functions in $\mathbb{B}^3$. We develop a theoretical framework for \emph{volumetric convolution} based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer in deep networks. By construction, our formulation leads to the derivation of a novel formula to measure the symmetry of a function in $\mathbb{B}^3$ around an arbitrary axis, that is useful in function analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on one viable use case i.e., 3D object recognition.",0
"The operation of convolution involves modifying the shape of one function with another function and is a crucial concept in deep neural networks for hierarchical feature learning. While performing convolution in Euclidean geometries is simple, extending it to other topological spaces such as a sphere or a unit ball presents unique challenges. To address this, we introduce a new operation called `\emph{volumetric convolution}' that effectively models and convolves arbitrary functions in $\mathbb{B}^3$. We utilize Zernike polynomials to develop a theoretical framework for volumetric convolution and implement it as a differentiable layer in deep networks. Our formulation also provides a novel formula for measuring the symmetry of a function around any axis, which is useful in function analysis tasks. We demonstrate the effectiveness of volumetric convolution in 3D object recognition.",1
"Sequential modelling with self-attention has achieved cutting edge performances in natural language processing. With advantages in model flexibility, computation complexity and interpretability, self-attention is gradually becoming a key component in event sequence models. However, like most other sequence models, self-attention does not account for the time span between events and thus captures sequential signals rather than temporal patterns. Without relying on recurrent network structures, self-attention recognizes event orderings via positional encoding. To bridge the gap between modelling time-independent and time-dependent event sequence, we introduce a functional feature map that embeds time span into high-dimensional spaces. By constructing the associated translation-invariant time kernel function, we reveal the functional forms of the feature map under classic functional function analysis results, namely Bochner's Theorem and Mercer's Theorem. We propose several models to learn the functional time representation and the interactions with event representation. These methods are evaluated on real-world datasets under various continuous-time event sequence prediction tasks. The experiments reveal that the proposed methods compare favorably to baseline models while also capturing useful time-event interactions.",0
"Natural language processing has seen significant advancements in sequential modelling with self-attention, offering advantages in terms of model flexibility, computation complexity, and interpretability. As a result, self-attention is emerging as a critical component in event sequence models. However, like other sequence models, self-attention falls short in accounting for the time span between events, focusing more on sequential signals rather than temporal patterns. Despite not relying on recurrent network structures, self-attention recognizes event orderings using positional encoding. To address the gap between modelling time-independent and time-dependent event sequences, we propose a functional feature map that embeds time span into high-dimensional spaces. By constructing a translation-invariant time kernel function, we reveal the functional forms of the feature map using classic functional function analysis results such as Bochner's Theorem and Mercer's Theorem. We introduce several models to learn the functional time representation and its interactions with event representation, which are evaluated on real-world datasets with various continuous-time event sequence prediction tasks. The experiments show that our proposed methods outperform baseline models while also capturing useful time-event interactions.",1
"In this paper, we propose a new product knowledge graph (PKG) embedding approach for learning the intrinsic product relations as product knowledge for e-commerce. We define the key entities and summarize the pivotal product relations that are critical for general e-commerce applications including marketing, advertisement, search ranking and recommendation. We first provide a comprehensive comparison between PKG and ordinary knowledge graph (KG) and then illustrate why KG embedding methods are not suitable for PKG learning. We construct a self-attention-enhanced distributed representation learning model for learning PKG embeddings from raw customer activity data in an end-to-end fashion. We design an effective multi-task learning schema to fully leverage the multi-modal e-commerce data. The Poincare embedding is also employed to handle complex entity structures. We use a real-world dataset from grocery.walmart.com to evaluate the performances on knowledge completion, search ranking and recommendation. The proposed approach compares favourably to baselines in knowledge completion and downstream tasks.",0
"This article introduces a novel approach to embedding a product knowledge graph (PKG) for e-commerce that focuses on learning intrinsic product relations. The paper outlines the crucial entities and summarizes pivotal product relationships that are fundamental for various e-commerce applications, such as marketing, advertising, search ranking, and recommendation. To begin, the authors compare PKG and ordinary knowledge graphs (KG), highlighting why KG embedding methods are not suitable for learning PKGs. Then, the authors describe a self-attention-enhanced distributed representation learning model, which learns PKG embeddings based on raw customer activity data through an end-to-end process. The authors also introduce a multi-task learning schema to effectively leverage the multi-modal e-commerce data and employ Poincare embedding to handle complex entity structures. Finally, the proposed approach is evaluated using real-world grocery.walmart.com data, with a focus on knowledge completion, search ranking, and recommendation. The results show that the proposed approach outperforms baselines in knowledge completion and downstream tasks.",1
"Deep learning-based health status representation learning and clinical prediction have raised much research interest in recent years. Existing models have shown superior performance, but there are still several major issues that have not been fully taken into consideration. First, the historical variation pattern of the biomarker in diverse time scales plays a vital role in indicating the health status, but it has not been explicitly extracted by existing works. Second, key factors that strongly indicate the health risk are different among patients. It is still challenging to adaptively make use of the features for patients in diverse conditions. Third, using prediction models as the black box will limit the reliability in clinical practice. However, none of the existing works can provide satisfying interpretability and meanwhile achieve high prediction performance. In this work, we develop a general health status representation learning model, named AdaCare. It can capture the long and short-term variations of biomarkers as clinical features to depict the health status in multiple time scales. It also models the correlation between clinical features to enhance the ones which strongly indicate the health status and thus can maintain a state-of-the-art performance in terms of prediction accuracy while providing qualitative interpretability. We conduct a health risk prediction experiment on two real-world datasets. Experiment results indicate that AdaCare outperforms state-of-the-art approaches and provides effective interpretability, which is verifiable by clinical experts.",0
"Recent years have seen a growing interest in deep learning-based health status representation learning and clinical prediction. While existing models have demonstrated superior performance, there are still significant issues that remain unaddressed. Firstly, the historical variation pattern of the biomarker in different time scales is crucial in indicating health status, but this has not been explicitly extracted by current works. Secondly, key factors that indicate health risk vary among patients, making it challenging to adaptively use features for those in diverse conditions. Thirdly, relying on prediction models as a black box limits their reliability in clinical practice, yet none of the existing models provide satisfactory interpretability while achieving high prediction performance. In this study, we propose a general health status representation learning model, called AdaCare, that captures both long and short-term biomarker variations as clinical features to depict health status across multiple time scales. AdaCare models the correlation between clinical features to enhance those that strongly indicate health status, maintaining state-of-the-art prediction accuracy while providing qualitative interpretability. We conducted a health risk prediction experiment using two real-world datasets, and our results show that AdaCare outperforms existing approaches and provides effective interpretability that clinical experts can verify.",1
"Optimizing the execution time of tensor program, e.g., a convolution, involves finding its optimal configuration. Searching the configuration space exhaustively is typically infeasible in practice. In line with recent research using TVM, we propose to learn a surrogate model to overcome this issue. The model is trained on an acyclic graph called an abstract syntax tree, and utilizes a graph convolutional network to exploit structure in the graph. We claim that a learnable graph-based data processing is a strong competitor to heuristic-based feature extraction. We present a new dataset of graphs corresponding to configurations and their execution time for various tensor programs. We provide baselines for a runtime prediction task.",0
"To enhance the execution time of a tensor program, such as a convolution, the optimal setup must be determined. However, exhaustively searching the configuration space is impractical. To address this challenge, we suggest adopting a surrogate model, as seen in recent TVM research. This model is trained on an abstract syntax tree, which is a cycle-free graph, and incorporates a graph convolutional network to benefit from the graph's structure. We argue that a trainable graph-based data processing approach is a formidable contender to heuristic-based feature extraction. Additionally, we introduce a fresh collection of graphs that correspond to various tensor program configurations and their execution times. We present benchmarks for a runtime prediction task.",1
"Most previous studies on multi-agent reinforcement learning focus on deriving decentralized and cooperative policies to maximize a common reward and rarely consider the transferability of trained policies to new tasks. This prevents such policies from being applied to more complex multi-agent tasks. To resolve these limitations, we propose a model that conducts both representation learning for multiple agents using hierarchical graph attention network and policy learning using multi-agent actor-critic. The hierarchical graph attention network is specially designed to model the hierarchical relationships among multiple agents that either cooperate or compete with each other to derive more advanced strategic policies. Two attention networks, the inter-agent and inter-group attention layers, are used to effectively model individual and group level interactions, respectively. The two attention networks have been proven to facilitate the transfer of learned policies to new tasks with different agent compositions and allow one to interpret the learned strategies. Empirically, we demonstrate that the proposed model outperforms existing methods in several mixed cooperative and competitive tasks.",0
"Previous research on multi-agent reinforcement learning has primarily focused on developing decentralized and cooperative policies that aim to maximize a common reward, with little consideration given to the transferability of these policies to new tasks. This lack of transferability has limited the potential application of such policies to more complex multi-agent tasks. To address this issue, we propose a model that incorporates both representation learning for multiple agents using a hierarchical graph attention network and policy learning using a multi-agent actor-critic approach. The hierarchical graph attention network is specifically designed to model the hierarchical relationships among multiple agents that either cooperate or compete with each other, allowing for the development of more advanced strategic policies. The model also includes two attention networks, the inter-agent and inter-group attention layers, which effectively model individual and group level interactions, respectively. These attention networks have been shown to enhance the transfer of learned policies to new tasks with varying agent compositions and enable the interpretation of the learned strategies. Our empirical results demonstrate that the proposed model outperforms existing methods in several mixed cooperative and competitive tasks.",1
"Disentangled encoding is an important step towards a better representation learning. However, despite the numerous efforts, there still is no clear winner that captures the independent features of the data in an unsupervised fashion. In this work we empirically evaluate the performance of six unsupervised disentanglement approaches on the mpi3d toy dataset curated and released for the NeurIPS 2019 Disentanglement Challenge. The methods investigated in this work are Beta-VAE, Factor-VAE, DIP-I-VAE, DIP-II-VAE, Info-VAE, and Beta-TCVAE. The capacities of all models were progressively increased throughout the training and the hyper-parameters were kept intact across experiments. The methods were evaluated based on five disentanglement metrics, namely, DCI, Factor-VAE, IRS, MIG, and SAP-Score. Within the limitations of this study, the Beta-TCVAE approach was found to outperform its alternatives with respect to the normalized sum of metrics. However, a qualitative study of the encoded latents reveal that there is not a consistent correlation between the reported metrics and the disentanglement potential of the model.",0
"Enhancing representation learning requires effective disentangled encoding, which is still an ongoing challenge despite many attempts. This study assesses six unsupervised disentanglement techniques on the mpi3d toy dataset, which was specifically developed for the NeurIPS 2019 Disentanglement Challenge. The models used in the experiment were Beta-VAE, Factor-VAE, DIP-I-VAE, DIP-II-VAE, Info-VAE, and Beta-TCVAE, and were trained with increasing capacities and constant hyper-parameters. The evaluation criteria consisted of five disentanglement metrics, including DCI, Factor-VAE, IRS, MIG, and SAP-Score. The results revealed that Beta-TCVAE performed the best in terms of the normalized sum of metrics, but the correlation between the reported metrics and the disentanglement potential of the models was inconsistent based on the qualitative analysis of the encoded latents.",1
"We address the problem of disentangled representation learning with independent latent factors in graph convolutional networks (GCNs). The current methods usually learn node representation by describing its neighborhood as a perceptual whole in a holistic manner while ignoring the entanglement of the latent factors. However, a real-world graph is formed by the complex interaction of many latent factors (e.g., the same hobby, education or work in social network). While little effort has been made toward exploring the disentangled representation in GCNs. In this paper, we propose a novel Independence Promoted Graph Disentangled Networks (IPGDN) to learn disentangled node representation while enhancing the independence among node representations. In particular, we firstly present disentangled representation learning by neighborhood routing mechanism, and then employ the Hilbert-Schmidt Independence Criterion (HSIC) to enforce independence between the latent representations, which is effectively integrated into a graph convolutional framework as a regularizer at the output layer. Experimental studies on real-world graphs validate our model and demonstrate that our algorithms outperform the state-of-the-arts by a wide margin in different network applications, including semi-supervised graph classification, graph clustering and graph visualization.",0
"The problem of learning disentangled representations with independent latent factors in graph convolutional networks (GCNs) is addressed in this paper. Current methods tend to describe the neighborhood of a node as a holistic entity, without considering the entanglement of the latent factors that form a real-world graph. The proposed Independence Promoted Graph Disentangled Networks (IPGDN) aim to overcome this limitation by introducing a novel disentangled representation learning mechanism based on neighborhood routing, and enforcing independence among node representations through the Hilbert-Schmidt Independence Criterion (HSIC), which is integrated into a graph convolutional framework as a regularizer at the output layer. The experiments conducted on real-world graphs demonstrate that the proposed model outperforms state-of-the-art methods in various network applications, including semi-supervised graph classification, graph clustering and graph visualization.",1
"Learning representations of data is an important problem in statistics and machine learning. While the origin of learning representations can be traced back to factor analysis and multidimensional scaling in statistics, it has become a central theme in deep learning with important applications in computer vision and computational neuroscience. In this article, we review recent advances in learning representations from a statistical perspective. In particular, we review the following two themes: (a) unsupervised learning of vector representations and (b) learning of both vector and matrix representations.",0
"The task of acquiring data representations is a significant issue in the fields of statistics and machine learning. Although the concept of acquiring representations can be traced back to factor analysis and multidimensional scaling in statistics, it has now become a fundamental aspect in deep learning and has essential applications in computer vision and computational neuroscience. This paper examines the most recent developments in acquiring representations from a statistical standpoint. The two primary themes that are reviewed are unsupervised learning of vector representations and the acquisition of both vector and matrix representations.",1
"The (variational) graph auto-encoder and its variants have been popularly used for representation learning on graph-structured data. While the encoder is often a powerful graph convolutional network, the decoder reconstructs the graph structure by only considering two nodes at a time, thus ignoring possible interactions among edges. On the other hand, structured prediction, which considers the whole graph simultaneously, is computationally expensive. In this paper, we utilize the well-known triadic closure property which is exhibited in many real-world networks. We propose the triad decoder, which considers and predicts the three edges involved in a local triad together. The triad decoder can be readily used in any graph-based auto-encoder. In particular, we incorporate this to the (variational) graph auto-encoder. Experiments on link prediction, node clustering and graph generation show that the use of triads leads to more accurate prediction, clustering and better preservation of the graph characteristics.",0
"Representation learning on graph-structured data is commonly achieved through the use of the (variational) graph auto-encoder and its variations. Although the encoder is usually a robust graph convolutional network, the decoder reconstructs the graph structure by only analyzing two nodes at a time, disregarding possible interactions among edges. Meanwhile, structured prediction, which considers the entire graph simultaneously, is computationally demanding. To address this, we introduce the triad decoder, which utilizes the well-known triadic closure property prevalent in many real-world networks. Our proposed decoder predicts and considers all three edges involved in a local triad, and is easily incorporated into any graph-based auto-encoder, including the (variational) graph auto-encoder. Our experiments on link prediction, node clustering, and graph generation demonstrate that the use of triads results in more precise predictions, better clustering, and improved preservation of graph characteristics.",1
"Learning meaningful and compact representations with disentangled semantic aspects is considered to be of key importance in representation learning. Since real-world data is notoriously costly to collect, many recent state-of-the-art disentanglement models have heavily relied on synthetic toy data-sets. In this paper, we propose a novel data-set which consists of over one million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position. In order to be able to control all the factors of variation precisely, we built an experimental platform where the objects are being moved by a robotic arm. In addition, we provide two more datasets which consist of simulations of the experimental setup. These datasets provide for the first time the possibility to systematically investigate how well different disentanglement methods perform on real data in comparison to simulation, and how simulated data can be leveraged to build better representations of the real world. We provide a first experimental study of these questions and our results indicate that learned models transfer poorly, but that model and hyperparameter selection is an effective means of transferring information to the real world.",0
"The acquisition of meaningful and concise representations that possess disentangled semantic aspects is deemed crucial in the realm of representation learning. Due to the exorbitant cost of collecting real-world data, several recent disentanglement models have extensively relied on synthetic toy data-sets, however. Our paper presents a unique data-set that encompasses more than one million images of physical 3D objects that exhibit seven factors of variation, such as object color, shape, size, and position. To precisely regulate all the factors of variation, we established an experimental platform that employs a robotic arm to move the objects. Furthermore, we offer two more data-sets that comprise simulations of the experimental setup. These data-sets offer an opportunity to systematically investigate the performance of different disentanglement methods on real data in comparison to simulation, and how simulated data can be utilized to construct better representations of the actual world. Our preliminary study reveals that learned models do not transfer well, but that the selection of model and hyperparameters is an effective means of transferring information to the real world.",1
"Existing deep learning methods for action recognition in videos require a large number of labeled videos for training, which is labor-intensive and time-consuming. For the same action, the knowledge learned from different media types, e.g., videos and images, may be related and complementary. However, due to the domain shifts and heterogeneous feature representations between videos and images, the performance of classifiers trained on images may be dramatically degraded when directly deployed to videos. In this paper, we propose a novel method, named Deep Image-to-Video Adaptation and Fusion Networks (DIVAFN), to enhance action recognition in videos by transferring knowledge from images using video keyframes as a bridge. The DIVAFN is a unified deep learning model, which integrates domain-invariant representations learning and cross-modal feature fusion into a unified optimization framework. Specifically, we design an efficient cross-modal similarities metric to reduce the modality shift among images, keyframes and videos. Then, we adopt an autoencoder architecture, whose hidden layer is constrained to be the semantic representations of the action class names. In this way, when the autoencoder is adopted to project the learned features from different domains to the same space, more compact, informative and discriminative representations can be obtained. Finally, the concatenation of the learned semantic feature representations from these three autoencoders are used to train the classifier for action recognition in videos. Comprehensive experiments on four real-world datasets show that our method outperforms some state-of-the-art domain adaptation and action recognition methods.",0
"Action recognition in videos using deep learning methods currently requires a significant amount of labeled videos for training, which can be a time-consuming and labor-intensive process. Although the knowledge learned from various media types such as images and videos might be related and complementary, the performance of classifiers trained on images may be significantly reduced when directly applied to videos due to domain shifts and heterogeneous feature representations. Thus, this paper proposes a new method called Deep Image-to-Video Adaptation and Fusion Networks (DIVAFN) to improve action recognition in videos by transferring knowledge from images through video keyframes. The DIVAFN model combines domain-invariant representations learning and cross-modal feature fusion in one optimization framework. This is achieved by using an efficient cross-modal similarities metric to minimize the modality shift between images, keyframes, and videos. Additionally, an autoencoder architecture is used, and its hidden layer is constrained to be the semantic representations of the action class names. This allows the autoencoder to project the learned features from different domains to the same space, resulting in more informative and discriminative representations. Finally, the semantic feature representations learned from the three autoencoders are concatenated and used to train the classifier for action recognition. The proposed method outperforms some state-of-the-art domain adaptation and action recognition methods on four real-world datasets, as demonstrated by comprehensive experiments.",1
"(Very early draft)Traditional supervised learning keeps pushing convolution neural network(CNN) achieving state-of-art performance. However, lack of large-scale annotation data is always a big problem due to the high cost of it, even ImageNet dataset is over-fitted by complex models now. The success of unsupervised learning method represented by the Bert model in natural language processing(NLP) field shows its great potential. And it makes that unlimited training samples becomes possible and the great universal generalization ability changes NLP research direction directly. In this article, we purpose a novel unsupervised learning method based on contrastive predictive coding. Under that, we are able to train model with any non-annotation images and improve model's performance to reach state-of-art performance at the same level of model complexity. Beside that, since the number of training images could be unlimited amplification, an universal large-scale pre-trained computer vision model is possible in the future.",0
"The traditional approach to supervised learning has been to use convolutional neural networks (CNNs) to achieve state-of-the-art performance. However, obtaining large-scale annotated data has always been a challenge due to its high cost. Even the ImageNet dataset is now over-fitted by complex models. On the other hand, the success of unsupervised learning methods like the Bert model in the field of natural language processing (NLP) has shown great potential. It has made unlimited training samples possible and has led to a change in the direction of NLP research by providing great universal generalization ability. This article proposes a novel unsupervised learning method based on contrastive predictive coding, which can train models with non-annotated images and improve performance to reach state-of-the-art levels at the same level of model complexity. Moreover, the number of training images can be amplified without limit, making it possible to create a universal pre-trained computer vision model in the future.",1
"Massive electronic health records (EHRs) enable the success of learning accurate patient representations to support various predictive health applications. In contrast, doctor representation was not well studied despite that doctors play pivotal roles in healthcare. How to construct the right doctor representations? How to use doctor representation to solve important health analytic problems? In this work, we study the problem on {\it clinical trial recruitment}, which is about identifying the right doctors to help conduct the trials based on the trial description and patient EHR data of those doctors. We propose doctor2vec which simultaneously learns 1) doctor representations from EHR data and 2) trial representations from the description and categorical information about the trials. In particular, doctor2vec utilizes a dynamic memory network where the doctor's experience with patients are stored in the memory bank and the network will dynamically assign weights based on the trial representation via an attention mechanism. Validated on large real-world trials and EHR data including 2,609 trials, 25K doctors and 430K patients, doctor2vec demonstrated improved performance over the best baseline by up to $8.7\%$ in PR-AUC. We also demonstrated that the doctor2vec embedding can be transferred to benefit data insufficiency settings including trial recruitment in less populated/newly explored country with $13.7\%$ improvement or for rare diseases with $8.1\%$ improvement in PR-AUC.",0
"The success of learning accurate patient representations to support various predictive health applications is enabled by massive electronic health records (EHRs). However, the role of doctors in healthcare is pivotal, yet doctor representation has not been well studied. This work aims to address the issue of constructing the right doctor representations and how to use them to solve important health analytic problems, specifically the problem of clinical trial recruitment. The proposed solution, doctor2vec, simultaneously learns doctor representations from EHR data and trial representations from the trial description and categorical information. Doctor2vec utilizes a dynamic memory network that stores the doctor's experience with patients in the memory bank and assigns weights dynamically based on the trial representation using an attention mechanism. Doctor2vec was validated on large real-world trials and EHR data, including 2,609 trials, 25K doctors, and 430K patients, and demonstrated up to an 8.7% improvement in PR-AUC over the best baseline. The embedding of doctor2vec can also be transferred to benefit data insufficiency settings, such as trial recruitment in less populated/newly explored countries with a 13.7% improvement or for rare diseases with an 8.1% improvement in PR-AUC.",1
"Most machine learning theory and practice is concerned with learning a single task. In this thesis it is argued that in general there is insufficient information in a single task for a learner to generalise well and that what is required for good generalisation is information about many similar learning tasks. Similar learning tasks form a body of prior information that can be used to constrain the learner and make it generalise better. Examples of learning scenarios in which there are many similar tasks are handwritten character recognition and spoken word recognition.   The concept of the environment of a learner is introduced as a probability measure over the set of learning problems the learner might be expected to learn. It is shown how a sample from the environment may be used to learn a representation, or recoding of the input space that is appropriate for the environment. Learning a representation can equivalently be thought of as learning the appropriate features of the environment. Bounds are derived on the sample size required to ensure good generalisation from a representation learning process. These bounds show that under certain circumstances learning a representation appropriate for $n$ tasks reduces the number of examples required of each task by a factor of $n$.   Once a representation is learnt it can be used to learn novel tasks from the same environment, with the result that far fewer examples are required of the new tasks to ensure good generalisation. Bounds are given on the number of tasks and the number of samples from each task required to ensure that a representation will be a good one for learning novel tasks.   The results on representation learning are generalised to cover any form of automated hypothesis space bias.",0
"The majority of machine learning theory and practice focuses on learning a single task. However, this thesis argues that a single task does not provide enough information for a learner to generalize effectively. To achieve good generalization, information about many similar learning tasks is necessary. These similar tasks provide a prior information that can be used to limit the learner and improve generalization. Examples of scenarios with many similar tasks include handwritten character recognition and spoken word recognition.   The concept of the learner's environment is introduced as a probability measure over the set of learning problems that the learner may encounter. A sample from the environment can be used to learn a representation or recoding of the input space that is suitable for the environment. Learning a representation is equivalent to learning the appropriate features of the environment. Bounds are established on the sample size required to ensure good generalization from a representation learning process. These bounds indicate that under certain circumstances, learning a representation suitable for n tasks reduces the number of examples required for each task by a factor of n.   Once a representation is learned, it can be used to learn new tasks from the same environment with fewer examples needed to ensure good generalization. Bounds are provided on the number of tasks and the number of samples from each task required to ensure that a representation is suitable for learning novel tasks.   The results on representation learning are extended to cover any form of automated hypothesis space bias.",1
"Images or videos always contain multiple objects or actions. Multi-label recognition has been witnessed to achieve pretty performance attribute to the rapid development of deep learning technologies. Recently, graph convolution network (GCN) is leveraged to boost the performance of multi-label recognition. However, what is the best way for label correlation modeling and how feature learning can be improved with label system awareness are still unclear. In this paper, we propose a label graph superimposing framework to improve the conventional GCN+CNN framework developed for multi-label recognition in the following two aspects. Firstly, we model the label correlations by superimposing label graph built from statistical co-occurrence information into the graph constructed from knowledge priors of labels, and then multi-layer graph convolutions are applied on the final superimposed graph for label embedding abstraction. Secondly, we propose to leverage embedding of the whole label system for better representation learning. In detail, lateral connections between GCN and CNN are added at shallow, middle and deep layers to inject information of label system into backbone CNN for label-awareness in the feature learning process. Extensive experiments are carried out on MS-COCO and Charades datasets, showing that our proposed solution can greatly improve the recognition performance and achieves new state-of-the-art recognition performance.",0
"The use of images and videos often involves multiple objects and actions, which can be recognized through the use of multi-label recognition and deep learning technologies. While the graph convolution network (GCN) has been used to enhance multi-label recognition, it remains unclear how to model label correlations and improve feature learning with label system awareness. This paper proposes a label graph superimposing framework that improves the conventional GCN+CNN framework by incorporating label correlations through statistical co-occurrence information and knowledge priors, and by leveraging the whole label system for better representation learning. Lateral connections between GCN and CNN are added to inject label system information at shallow, middle, and deep layers in the feature learning process. Experiments on MS-COCO and Charades datasets demonstrate significant improvements in recognition performance and new state-of-the-art results.",1
"Representing networks in a low dimensional latent space is a crucial task with many interesting applications in graph learning problems, such as link prediction and node classification. A widely applied network representation learning paradigm is based on the combination of random walks for sampling context nodes and the traditional \textit{Skip-Gram} model to capture center-context node relationships. In this paper, we emphasize on exponential family distributions to capture rich interaction patterns between nodes in random walk sequences. We introduce the generic \textit{exponential family graph embedding} model, that generalizes random walk-based network representation learning techniques to exponential family conditional distributions. We study three particular instances of this model, analyzing their properties and showing their relationship to existing unsupervised learning models. Our experimental evaluation on real-world datasets demonstrates that the proposed techniques outperform well-known baseline methods in two downstream machine learning tasks.",0
"The task of representing networks in a low dimensional latent space has significant importance in graph learning problems, including link prediction and node classification. One commonly used method for network representation learning involves combining random walks for context node sampling with the traditional \textit{Skip-Gram} model to capture center-context node relationships. In this paper, we focus on the use of exponential family distributions to capture complex interaction patterns between nodes in random walk sequences. We introduce the \textit{exponential family graph embedding} model, which extends random walk-based network representation learning techniques to exponential family conditional distributions. We analyze the properties of three specific instances of this model and demonstrate their relationship to existing unsupervised learning models. Our experimental evaluation on real-world datasets shows that our proposed methods outperform well-known baseline methods in two downstream machine learning tasks.",1
"Graph neural network (GNN) is a deep model for graph representation learning. One advantage of graph neural network is its ability to incorporate node features into the learning process. However, this prevents graph neural network from being applied into featureless graphs. In this paper, we first analyze the effects of node features on the performance of graph neural network. We show that GNNs work well if there is a strong correlation between node features and node labels. Based on these results, we propose new feature initialization methods that allows to apply graph neural network to non-attributed graphs. Our experimental results show that the artificial features are highly competitive with real features.",0
"The Graph Neural Network (GNN) is a deep model designed for learning graph representations. One of its benefits is the ability to integrate node features into the learning process. However, this feature also limits its use on graphs without any attributes. This research examines the impact of node features on GNN performance and concludes that GNNs perform well if there is a significant correlation between node features and labels. Consequently, the study introduces new feature initialization techniques to allow GNN to be applied to non-attributed graphs. The experiment results reveal that synthetic features are just as effective as actual features.",1
"In this paper, we propose a structured Robust Adaptive Dic-tionary Pair Learning (RA-DPL) framework for the discrim-inative sparse representation learning. To achieve powerful representation ability of the available samples, the setting of RA-DPL seamlessly integrates the robust projective dictionary pair learning, locality-adaptive sparse representations and discriminative coding coefficients learning into a unified learning framework. Specifically, RA-DPL improves existing projective dictionary pair learning in four perspectives. First, it applies a sparse l2,1-norm based metric to encode the recon-struction error to deliver the robust projective dictionary pairs, and the l2,1-norm has the potential to minimize the error. Sec-ond, it imposes the robust l2,1-norm clearly on the analysis dictionary to ensure the sparse property of the coding coeffi-cients rather than using the costly l0/l1-norm. As such, the robustness of the data representation and the efficiency of the learning process are jointly considered to guarantee the effi-cacy of our RA-DPL. Third, RA-DPL conceives a structured reconstruction weight learning paradigm to preserve the local structures of the coding coefficients within each class clearly in an adaptive manner, which encourages to produce the locality preserving representations. Fourth, it also considers improving the discriminating ability of coding coefficients and dictionary by incorporating a discriminating function, which can ensure high intra-class compactness and inter-class separation in the code space. Extensive experiments show that our RA-DPL can obtain superior performance over other state-of-the-arts.",0
"The proposed framework in this paper is the Robust Adaptive Dictionary Pair Learning (RA-DPL) which aims to enhance the discriminative sparse representation learning by integrating robust projective dictionary pair learning, locality-adaptive sparse representations, and discriminative coding coefficients learning into a unified system. RA-DPL improves the existing projective dictionary pair learning by implementing a sparse l2,1-norm based metric to encode the reconstruction error, ensuring the robustness of the data representation and the efficiency of the learning process. It also uses a structured reconstruction weight learning paradigm to preserve the local structures of the coding coefficients within each class in an adaptive manner, which encourages locality preserving representations. Lastly, RA-DPL incorporates a discriminating function to improve the discriminating ability of coding coefficients and dictionary, ensuring high intra-class compactness and inter-class separation in the code space. The experiments conducted show that RA-DPL outperforms other state-of-the-art methods.",1
"Although deep learning has been applied to successfully address many data mining problems, relatively limited work has been done on deep learning for anomaly detection. Existing deep anomaly detection methods, which focus on learning new feature representations to enable downstream anomaly detection methods, perform indirect optimization of anomaly scores, leading to data-inefficient learning and suboptimal anomaly scoring. Also, they are typically designed as unsupervised learning due to the lack of large-scale labeled anomaly data. As a result, they are difficult to leverage prior knowledge (e.g., a few labeled anomalies) when such information is available as in many real-world anomaly detection applications.   This paper introduces a novel anomaly detection framework and its instantiation to address these problems. Instead of representation learning, our method fulfills an end-to-end learning of anomaly scores by a neural deviation learning, in which we leverage a few (e.g., multiple to dozens) labeled anomalies and a prior probability to enforce statistically significant deviations of the anomaly scores of anomalies from that of normal data objects in the upper tail. Extensive results show that our method can be trained substantially more data-efficiently and achieves significantly better anomaly scoring than state-of-the-art competing methods.",0
"While deep learning has been successful in solving numerous data mining problems, its application to anomaly detection has been limited. Current methods focus on learning new feature representations for downstream anomaly detection, resulting in indirect optimization of anomaly scores and inefficient learning. These methods are typically unsupervised due to the lack of labeled anomaly data, making it difficult to leverage prior knowledge in real-world applications. This paper presents a new anomaly detection framework that addresses these issues by using neural deviation learning to achieve end-to-end learning of anomaly scores. By leveraging a few labeled anomalies and a prior probability, this method enforces statistically significant deviations of anomaly scores from normal data objects in the upper tail. Results demonstrate that this method is more data-efficient and achieves better anomaly scoring than current competing methods.",1
"We study the problem of learning permutation invariant representations that can capture ""flexible"" notions of containment. We formalize this problem via a measure theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations.",0
"Our focus is on understanding how to acquire permutation invariant representations with the ability to capture versatile concepts of containment. Through a measure theoretic characterization of multisets, we formalize this aim and introduce a learning model that is grounded in theory. Our proposal involves training the model to anticipate the size of the symmetric difference or intersection of multisets, a unique task. Our findings indicate that our model surpasses DeepSet-based approaches with unconstrained object representations by predicting containment relations and estimating the sizes of symmetric differences and intersections with greater accuracy. Furthermore, the model acquires significant representations.",1
"Humans excel in continuously learning with small data without forgetting how to solve old problems. However, neural networks require large datasets to compute latent representations across different tasks while minimizing a loss function. For example, a natural language understanding (NLU) system will often deal with emerging entities during its deployment as interactions with users in realistic scenarios will generate new and infrequent names, events, and locations. Here, we address this scenario by introducing an RL trainable controller that disentangles the representation learning of a neural encoder from its memory management role.   Our proposed solution is straightforward and simple: we train a controller to execute an optimal sequence of reading and writing operations on an external memory with the goal of leveraging diverse activations from the past and provide accurate predictions. Our approach is named Learning to Control (LTC) and allows few-shot learning with two degrees of memory plasticity. We experimentally show that our system obtains accurate results for few-shot learning of entity recognition in the Stanford Task-Oriented Dialogue dataset.",0
"While humans are exceptional at learning from limited data and retaining the ability to solve previous problems, neural networks necessitate extensive datasets to compute latent representations across different tasks and minimize loss. In the case of natural language understanding systems, new and infrequent names, events, and locations may arise during real-world interactions with users. To address this issue, we have introduced an RL trainable controller that separates the representation learning of a neural encoder from its memory management role. Our solution, named Learning to Control (LTC), involves teaching a controller to execute an optimal sequence of reading and writing operations on external memory, leveraging diverse past activations to provide precise predictions. Our approach enables few-shot learning with two degrees of memory plasticity and has been experimentally shown to achieve accurate results for entity recognition on the Stanford Task-Oriented Dialogue dataset.",1
"Graph representation learning resurges as a trending research subject owing to the widespread use of deep learning for Euclidean data, which inspire various creative designs of neural networks in the non-Euclidean domain, particularly graphs. With the success of these graph neural networks (GNN) in the static setting, we approach further practical scenarios where the graph dynamically evolves. Existing approaches typically resort to node embeddings and use a recurrent neural network (RNN, broadly speaking) to regulate the embeddings and learn the temporal dynamics. These methods require the knowledge of a node in the full time span (including both training and testing) and are less applicable to the frequent change of the node set. In some extreme scenarios, the node sets at different time steps may completely differ. To resolve this challenge, we propose EvolveGCN, which adapts the graph convolutional network (GCN) model along the temporal dimension without resorting to node embeddings. The proposed approach captures the dynamism of the graph sequence through using an RNN to evolve the GCN parameters. Two architectures are considered for the parameter evolution. We evaluate the proposed approach on tasks including link prediction, edge classification, and node classification. The experimental results indicate a generally higher performance of EvolveGCN compared with related approaches. The code is available at \url{https://github.com/IBM/EvolveGCN}.",0
"The use of deep learning for Euclidean data has made graph representation learning a popular research subject. This has led to the development of creative neural network designs for non-Euclidean data, particularly graphs. While graph neural networks have been successful in static settings, the challenge lies in handling dynamically evolving graphs. Existing approaches rely on node embeddings and recurrent neural networks, which require knowledge of nodes throughout the entire time span and may not be suitable for frequent changes in the node set. To address this issue, we propose EvolveGCN, a method that adapts the graph convolutional network model along the temporal dimension without using node embeddings. Our approach uses an RNN to evolve the GCN parameters and captures the dynamism of the graph sequence. We evaluate the proposed approach on different tasks and find that it outperforms related approaches. The code can be accessed at \url{https://github.com/IBM/EvolveGCN}.",1
"Learning robust representations that allow to reliably establish relations between images is of paramount importance for virtually all of computer vision. Annotating the quadratic number of pairwise relations between training images is simply not feasible, while unsupervised inference is prone to noise, thus leaving the vast majority of these relations to be unreliable. To nevertheless find those relations which can be reliably utilized for learning, we follow a divide-and-conquer strategy: We find reliable similarities by extracting compact groups of images and reliable dissimilarities by partitioning these groups into subsets, converting the complicated overall problem into few reliable local subproblems. For each of the subsets we obtain a representation by learning a mapping to a target feature space so that their reliable relations are kept. Transitivity relations between the subsets are then exploited to consolidate the local solutions into a concerted global representation. While iterating between grouping, partitioning, and learning, we can successively use more and more reliable relations which, in turn, improves our image representation. In experiments, our approach shows state-of-the-art performance on unsupervised classification on ImageNet with 46.0% and competes favorably on different transfer learning tasks on PASCAL VOC.",0
"Developing strong representations that enable the establishment of relationships between images with reliability is crucial in the field of computer vision. It is not practical to annotate the numerous pairwise relationships between training images, and unsupervised inference is susceptible to inaccuracies, thus rendering most of these connections unreliable. To tackle this issue, a divide-and-conquer approach is utilized, where compact groups of images are extracted to find reliable similarities and these groups are partitioned into subsets to identify dependable dissimilarities. This strategy transforms the complex problem into a few reliable local subproblems. A representation is obtained for each subset by learning a mapping to a target feature space that maintains their reliable relationships. The transitivity relations between the subsets are then utilized to merge the local solutions into a cohesive global representation. By iterating between grouping, partitioning, and learning, more and more reliable connections can be employed, which improves the image representation. This method achieves excellent results on unsupervised classification on ImageNet with 46.0% and performs competitively on various transfer learning tasks on PASCAL VOC.",1
"Person re-identification aims to associate images of the same person over multiple non-overlapping camera views at different times. Depending on the human operator, manual re-identification in large camera networks is highly time consuming and erroneous. Automated person re-identification is required due to the extensive quantity of visual data produced by rapid inflation of large scale distributed multi-camera systems. The state-of-the-art works focus on learning and factorize person appearance features into latent discriminative factors at multiple semantic levels. We propose Deep Parallel Feature Consensus Network (DeepPFCN), a novel network architecture that learns multi-scale person appearance features using convolutional neural networks. This model factorizes the visual appearance of a person into latent discriminative factors at multiple semantic levels. Finally consensus is built. The feature representations learned by DeepPFCN are more robust for the person re-identification task, as we learn discriminative scale-specific features and maximize multi-scale feature fusion selections in multi-scale image inputs. We further exploit average and max pooling in separate scale for person-specific task to discriminate features globally and locally. We demonstrate the re-identification advantages of the proposed DeepPFCN model over the state-of-the-art re-identification methods on three benchmark datasets: Market1501, DukeMTMCreID, and CUHK03. We have achieved mAP results of 75.8%, 64.3%, and 52.6% respectively on these benchmark datasets.",0
"The goal of person re-identification is to match pictures of the same individual across different camera views and times. Manual re-identification in large camera networks can be time-consuming and prone to errors. As a result, automated person re-identification is necessary due to the increasing amount of visual data produced by distributed multi-camera systems. Current research focuses on learning person appearance features at multiple semantic levels to improve accuracy. To address this, we present Deep Parallel Feature Consensus Network, a novel network that uses convolutional neural networks to learn multi-scale person appearance features. This approach factorizes visual appearance into discriminative factors at multiple semantic levels and builds consensus. Our model's feature representations are more robust for person re-identification because of the discriminative scale-specific features and multi-scale feature fusion selections. Additionally, we use average and max pooling in separate scales for person-specific tasks to globally and locally discriminate features. Our proposed model, DeepPFCN, outperforms state-of-the-art re-identification methods on three benchmark datasets: Market1501, DukeMTMCreID, and CUHK03, with mAP results of 75.8%, 64.3%, and 52.6%, respectively.",1
"Self-supervised learning by predicting transformations has demonstrated outstanding performances in both unsupervised and (semi-)supervised tasks. Among the state-of-the-art methods is the AutoEncoding Transformations (AET) by decoding transformations from the learned representations of original and transformed images. Both deterministic and probabilistic AETs rely on the Euclidean distance to measure the deviation of estimated transformations from their groundtruth counterparts. However, this assumption is questionable as a group of transformations often reside on a curved manifold rather staying in a flat Euclidean space. For this reason, we should use the geodesic to characterize how an image transform along the manifold of a transformation group, and adopt its length to measure the deviation between transformations. Particularly, we present to autoencode a Lie group of homography transformations PG(2) to learn image representations. For this, we make an estimate of the intractable Riemannian logarithm by projecting PG(2) to a subgroup of rotation transformations SO(3) that allows the closed-form expression of geodesic distances. Experiments demonstrate the proposed AETv2 model outperforms the previous version as well as the other state-of-the-art self-supervised models in multiple tasks.",0
"Predicting transformations through self-supervised learning has shown exceptional results in unsupervised and (semi-)supervised tasks. One of the leading methods is AutoEncoding Transformations (AET), which decodes transformations from the original and transformed images' learned representations. The deterministic and probabilistic AETs use the Euclidean distance to measure the estimated transformations' deviation from their groundtruth counterparts. However, this assumption is questionable as a group of transformations often exists on a curved manifold rather than a flat Euclidean space. Therefore, we should use the geodesic to determine how an image transforms along the manifold of a transformation group and use its length to measure the deviation between transformations. Our proposed approach involves autoencoding a Lie group of homography transformations PG(2) to learn image representations. To achieve this, we estimate the intractable Riemannian logarithm by projecting PG(2) to a subgroup of rotation transformations SO(3), which allows for a closed-form expression of geodesic distances. Experiments show that our proposed AETv2 model outperforms the previous version and other state-of-the-art self-supervised models in multiple tasks.",1
"For nonconvex optimization in machine learning, this article proves that every local minimum achieves the globally optimal value of the perturbable gradient basis model at any differentiable point. As a result, nonconvex machine learning is theoretically as supported as convex machine learning with a handcrafted basis in terms of the loss at differentiable local minima, except in the case when a preference is given to the handcrafted basis over the perturbable gradient basis. The proofs of these results are derived under mild assumptions. Accordingly, the proven results are directly applicable to many machine learning models, including practical deep neural networks, without any modification of practical methods. Furthermore, as special cases of our general results, this article improves or complements several state-of-the-art theoretical results on deep neural networks, deep residual networks, and overparameterized deep neural networks with a unified proof technique and novel geometric insights. A special case of our results also contributes to the theoretical foundation of representation learning.",0
"This article demonstrates that for nonconvex optimization in machine learning, every local minimum attains the globally optimal value of the perturbable gradient basis model at any differentiable point. Therefore, nonconvex machine learning is theoretically equivalent to convex machine learning with a handcrafted basis, except when the latter is given preference over the former. The proofs of these results are based on modest assumptions, making them applicable to various machine learning models, including practical deep neural networks, without any modifications. Additionally, as a special case of the general results, this article enhances or supplements several cutting-edge theoretical results on deep neural networks, deep residual networks, and overparameterized deep neural networks, using a unified proof method and original geometric insights. A specific instance of these results also contributes to the theoretical basis of representation learning.",1
"Given the importance of remote sensing, surprisingly little attention has been paid to it by the representation learning community. To address it and to establish baselines and a common evaluation protocol in this domain, we provide simplified access to 5 diverse remote sensing datasets in a standardized form. Specifically, we investigate in-domain representation learning to develop generic remote sensing representations and explore which characteristics are important for a dataset to be a good source for remote sensing representation learning. The established baselines achieve state-of-the-art performance on these datasets.",0
"Despite the significant role of remote sensing, it is striking that the representation learning field has not given it much consideration. To tackle this issue and establish benchmarks and uniform evaluation procedures in this area, we have made available simplified access to 5 distinct remote sensing datasets in a standardized format. Our focus is on in-domain representation learning to produce universal remote sensing representations and to determine the essential features of a dataset that make it an ideal resource for remote sensing representation learning. Our set benchmarks have achieved the highest level of performance on these datasets.",1
"An open research problem in automatic signature verification is the skilled forgery attacks. However, the skilled forgeries are very difficult to acquire for representation learning. To tackle this issue, this paper proposes to learn dynamic signature representations through ranking synthesized signatures. First, a neuromotor inspired signature synthesis method is proposed to synthesize signatures with different distortion levels for any template signature. Then, given the templates, we construct a lightweight one-dimensional convolutional network to learn to rank the synthesized samples, and directly optimize the average precision of the ranking to exploit relative and fine-grained signature similarities. Finally, after training, fixed-length representations can be extracted from dynamic signatures of variable lengths for verification. One highlight of our method is that it requires neither skilled nor random forgeries for training, yet it surpasses the state-of-the-art by a large margin on two public benchmarks.",0
"The skilled forgery attacks pose a challenge in automatic signature verification, as obtaining these forgeries for representation learning is exceedingly difficult. To address this issue, this study proposes a method for learning dynamic signature representations by ranking synthesized signatures. The first step involves a neuromotor-inspired signature synthesis technique, which produces signatures with varying distortion levels for any given template signature. Next, a lightweight one-dimensional convolutional network is created to rank the synthesized samples based on the templates, optimizing the average precision of the ranking to exploit relative and fine-grained similarities between signatures. Following training, fixed-length representations can be extracted from variable-length dynamic signatures for verification. Notably, this approach does not require skilled or random forgeries for training and outperforms the current state-of-the-art by a significant margin on two public benchmarks.",1
"There is a recent surge of interest in cross-modal representation learning corresponding to images and text. The main challenge lies in mapping images and text to a shared latent space where the embeddings corresponding to a similar semantic concept lie closer to each other than the embeddings corresponding to different semantic concepts, irrespective of the modality. Ranking losses are commonly used to create such shared latent space -- however, they do not impose any constraints on inter-class relationships resulting in neighboring clusters to be completely unrelated. The works in the domain of visual semantic embeddings address this problem by first constructing a semantic embedding space based on some external knowledge and projecting image embeddings onto this fixed semantic embedding space. These works are confined only to image domain and constraining the embeddings to a fixed space adds additional burden on learning. This paper proposes a novel method, HUSE, to learn cross-modal representation with semantic information. HUSE learns a shared latent space where the distance between any two universal embeddings is similar to the distance between their corresponding class embeddings in the semantic embedding space. HUSE also uses a classification objective with a shared classification layer to make sure that the image and text embeddings are in the same shared latent space. Experiments on UPMC Food-101 show our method outperforms previous state-of-the-art on retrieval, hierarchical precision and classification results.",0
"Recently, there has been an increase in interest in cross-modal representation learning, specifically for images and text. The main challenge is to map images and text to a common latent space where embeddings for similar semantic concepts are closer to each other than embeddings for different semantic concepts, regardless of modality. Although ranking losses are commonly used for creating such a shared latent space, they do not impose constraints on inter-class relationships, leading to unrelated neighboring clusters. To address this problem, previous works in visual semantic embeddings have constructed a semantic embedding space based on external knowledge and projected image embeddings onto this fixed space. However, this approach is limited to the image domain and adds an additional learning burden. In this paper, we propose HUSE, a novel method for learning cross-modal representation with semantic information. HUSE learns a shared latent space where the distance between any two universal embeddings is similar to the distance between their corresponding class embeddings in the semantic space. HUSE also employs a classification objective with a shared classification layer to ensure that the image and text embeddings are in the same shared latent space. Our experiments on UPMC Food-101 demonstrate that our method outperforms the previous state-of-the-art in retrieval, hierarchical precision, and classification results.",1
"Compression is at the heart of effective representation learning. However, lossy compression is typically achieved through simple parametric models like Gaussian noise to preserve analytic tractability, and the limitations this imposes on learning are largely unexplored. Further, the Gaussian prior assumptions in models such as variational autoencoders (VAEs) provide only an upper bound on the compression rate in general. We introduce a new noise channel, \emph{Echo noise}, that admits a simple, exact expression for mutual information for arbitrary input distributions. The noise is constructed in a data-driven fashion that does not require restrictive distributional assumptions. With its complex encoding mechanism and exact rate regularization, Echo leads to improved bounds on log-likelihood and dominates $\beta$-VAEs across the achievable range of rate-distortion trade-offs. Further, we show that Echo noise can outperform flow-based methods without the need to train additional distributional transformations.",0
"Effective representation learning relies on compression, but the commonly used lossy compression techniques, such as Gaussian noise, have limitations that have not been fully explored. These techniques are employed for their simplicity and analytic tractability, but they only provide an upper bound on compression rates. In this study, we introduce a new noise channel called \emph{Echo noise}, which allows for an exact expression of mutual information for any input distribution without the need for restrictive assumptions. Echo noise is data-driven and has a complex encoding mechanism, which leads to improved log-likelihood bounds and outperforms $\beta$-VAEs over a wide range of rate-distortion trade-offs. Additionally, Echo noise is shown to outperform flow-based methods without the need for additional distributional transformations.",1
"We present an algorithm, HOMER, for exploration and reinforcement learning in rich observation environments that are summarizable by an unknown latent state space. The algorithm interleaves representation learning to identify a new notion of kinematic state abstraction with strategic exploration to reach new states using the learned abstraction. The algorithm provably explores the environment with sample complexity scaling polynomially in the number of latent states and the time horizon, and, crucially, with no dependence on the size of the observation space, which could be infinitely large. This exploration guarantee further enables sample-efficient global policy optimization for any reward function. On the computational side, we show that the algorithm can be implemented efficiently whenever certain supervised learning problems are tractable. Empirically, we evaluate HOMER on a challenging exploration problem, where we show that the algorithm is exponentially more sample efficient than standard reinforcement learning baselines.",0
"An algorithm named HOMER has been developed for exploration and reinforcement learning in complex observation environments, where the underlying latent state space is unknown. The algorithm involves a combination of representation learning and strategic exploration to identify a new kinematic state abstraction and reach new states using the learned abstraction. HOMER has been proven to explore the environment with sample complexity that increases polynomially with the number of latent states and the time horizon, without being dependent on the size of the observation space. This exploration guarantee facilitates sample-efficient global policy optimization for any reward function. The computational efficiency of the algorithm is demonstrated with the tractability of certain supervised learning problems. In an empirical evaluation, HOMER was found to be exponentially more sample efficient than standard reinforcement learning baselines in a challenging exploration problem.",1
"The supervised learning paradigm is limited by the cost - and sometimes the impracticality - of data collection and labeling in multiple domains. Self-supervised learning, a paradigm which exploits the structure of unlabeled data to create learning problems that can be solved with standard supervised approaches, has shown great promise as a pretraining or feature learning approach in fields like computer vision and time series processing. In this work, we present self-supervision strategies that can be used to learn informative representations from multivariate time series. One successful approach relies on predicting whether time windows are sampled from the same temporal context or not. As demonstrated on a clinically relevant task (sleep scoring) and with two electroencephalography datasets, our approach outperforms a purely supervised approach in low data regimes, while capturing important physiological information without any access to labels.",0
"The supervised learning model is constrained by the expenses and impracticality of collecting and labeling data in different areas. Self-supervised learning is a model that utilizes the configuration of unlabeled data to construct learning problems that can be tackled with standard supervised methods. It has demonstrated great potential as a pretraining or feature learning strategy in fields like computer vision and time series processing. Our paper introduces self-supervision techniques that can be utilized to obtain informative representations from multivariate time series. One successful technique involves predicting whether time windows belong to the same temporal context or not. Our approach surpasses a purely supervised method in low data environments and captures important physiological information without any label access, as demonstrated on a clinically relevant task (sleep scoring) and two electroencephalography datasets.",1
"Disentangled representation learning finds compact, independent and easy-to-interpret factors of the data. Learning such has been shown to require an inductive bias, which we explicitly encode in a generative model of images. Specifically, we propose a model with two latent spaces: one that represents spatial transformations of the input data, and another that represents the transformed data. We find that the latter naturally captures the intrinsic appearance of the data. To realize the generative model, we propose a Variationally Inferred Transformational Autoencoder (VITAE) that incorporates a spatial transformer into a variational autoencoder. We show how to perform inference in the model efficiently by carefully designing the encoders and restricting the transformation class to be diffeomorphic. Empirically, our model separates the visual style from digit type on MNIST, separates shape and pose in images of human bodies and facial features from facial shape on CelebA.",0
"The process of disentangled representation learning involves identifying concise, separate, and easily understandable components of data. To achieve this, we have discovered that there is a need for an inductive bias, which we have included in a generative image model. Our suggested model consists of two latent spaces: one that represents changes in the data's spatial orientation, and another that represents the transformed data itself. We have found that the latter space is capable of capturing the intrinsic appearance of the data. Our proposed solution, the Variationally Inferred Transformational Autoencoder (VITAE), incorporates a spatial transformer into a variational autoencoder to generate the model. We have successfully demonstrated how to perform inference in the model effectively by meticulously designing the encoders and restricting the transformation class to be diffeomorphic. Through experimentation, we have shown that our model can separate visual style from digit type in MNIST, as well as distinguish between shape and pose in images of human bodies and facial features from facial shape on CelebA.",1
"The information bottleneck principle is an elegant and useful approach to representation learning. In this paper, we investigate the problem of representation learning in the context of reinforcement learning using the information bottleneck framework, aiming at improving the sample efficiency of the learning algorithms. %by accelerating the process of discarding irrelevant information when the %input states are extremely high-dimensional. We analytically derive the optimal conditional distribution of the representation, and provide a variational lower bound. Then, we maximize this lower bound with the Stein variational (SV) gradient method. We incorporate this framework in the advantageous actor critic algorithm (A2C) and the proximal policy optimization algorithm (PPO). Our experimental results show that our framework can improve the sample efficiency of vanilla A2C and PPO significantly. Finally, we study the information bottleneck (IB) perspective in deep RL with the algorithm called mutual information neural estimation(MINE) . We experimentally verify that the information extraction-compression process also exists in deep RL and our framework is capable of accelerating this process. We also analyze the relationship between MINE and our method, through this relationship, we theoretically derive an algorithm to optimize our IB framework without constructing the lower bound.",0
"This paper explores representation learning in the context of reinforcement learning using the information bottleneck framework, which is a useful and elegant approach. The goal is to enhance the efficiency of learning algorithms by discarding irrelevant information when input states are high-dimensional. The optimal conditional distribution of the representation is analytically derived, and a variational lower bound is provided. The Stein Variational (SV) gradient method is used to maximize this lower bound. The advantageous actor critic algorithm (A2C) and the proximal policy optimization algorithm (PPO) are integrated into this framework. Experimental results show that the sample efficiency of vanilla A2C and PPO can be significantly improved. The information bottleneck (IB) perspective in deep RL is studied using the mutual information neural estimation (MINE) algorithm. The information extraction-compression process in deep RL is analyzed, and the framework is shown to accelerate this process. The relationship between MINE and the IB method is also analyzed, and an algorithm is theoretically derived to optimize the IB framework without constructing the lower bound.",1
"Machine learning on graph structured data has attracted much research interest due to its ubiquity in real world data. However, how to efficiently represent graph data in a general way is still an open problem. Traditional methods use handcraft graph features in a tabular form but suffer from the defects of domain expertise requirement and information loss. Graph representation learning overcomes these defects by automatically learning the continuous representations from graph structures, but they require abundant training labels, which are often hard to fulfill for graph-level prediction problems. In this work, we demonstrate that, if available, the domain expertise used for designing handcraft graph features can improve the graph-level representation learning when training labels are scarce. Specifically, we proposed a multi-task knowledge distillation method. By incorporating network-theory-based graph metrics as auxiliary tasks, we show on both synthetic and real datasets that the proposed multi-task learning method can improve the prediction performance of the original learning task, especially when the training data size is small.",0
"The prevalence of graph structured data in real world scenarios has made machine learning on this type of data a topic of great interest among researchers. However, there is still an unresolved issue regarding how to efficiently represent graph data in a generalized way. Current methods involve the use of manually designed graph features in a tabular format, but this approach has limitations due to the need for expert knowledge and information loss. Graph representation learning has emerged as a solution to these problems by enabling automatic continuous representations from graph structures. However, this method requires extensive training labels, which are often difficult to obtain for graph-level prediction tasks. In this study, we propose a multi-task knowledge distillation approach that leverages domain expertise in the design of handcrafted graph features to enhance graph-level representation learning when training labels are limited. Our results demonstrate that this method can significantly improve prediction performance, particularly when training data size is small, as evidenced by both synthetic and real datasets.",1
"Many video enhancement algorithms rely on optical flow to register frames in a video sequence. Precise flow estimation is however intractable; and optical flow itself is often a sub-optimal representation for particular video processing tasks. In this paper, we propose task-oriented flow (TOFlow), a motion representation learned in a self-supervised, task-specific manner. We design a neural network with a trainable motion estimation component and a video processing component, and train them jointly to learn the task-oriented flow. For evaluation, we build Vimeo-90K, a large-scale, high-quality video dataset for low-level video processing. TOFlow outperforms traditional optical flow on standard benchmarks as well as our Vimeo-90K dataset in three video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.",0
"Numerous video enhancement algorithms use optical flow to align frames in a video sequence. However, obtaining accurate flow estimation is challenging, and optical flow may not be the most effective representation for specific video processing tasks. This research proposes ""task-oriented flow"" (TOFlow), a motion representation learned through self-supervised, task-specific techniques. The study develops a neural network with a trainable motion estimation component and a video processing component, which work together to learn TOFlow. To evaluate its effectiveness, the researchers create Vimeo-90K, a large, high-quality video dataset for low-level video processing. In three video processing tasks - frame interpolation, video denoising/deblocking, and video super-resolution - TOFlow outperforms traditional optical flow on standard benchmarks and the Vimeo-90K dataset.",1
"For embodied agents to infer representations of the underlying 3D physical world they inhabit, they should efficiently combine multisensory cues from numerous trials, e.g., by looking at and touching objects. Despite its importance, multisensory 3D scene representation learning has received less attention compared to the unimodal setting. In this paper, we propose the Generative Multisensory Network (GMN) for learning latent representations of 3D scenes which are partially observable through multiple sensory modalities. We also introduce a novel method, called the Amortized Product-of-Experts, to improve the computational efficiency and the robustness to unseen combinations of modalities at test time. Experimental results demonstrate that the proposed model can efficiently infer robust modality-invariant 3D-scene representations from arbitrary combinations of modalities and perform accurate cross-modal generation. To perform this exploration, we also develop the Multisensory Embodied 3D-Scene Environment (MESE).",0
"Efficiently combining multisensory cues, such as looking and touching objects, is crucial for embodied agents to infer representations of the 3D physical world they inhabit. However, compared to the unimodal setting, multisensory 3D scene representation learning has received less attention. This paper proposes the Generative Multisensory Network (GMN) to learn latent representations of 3D scenes that are observable through multiple sensory modalities. The paper also introduces a novel method, the Amortized Product-of-Experts, to improve computational efficiency and robustness to unseen combinations of modalities during testing. Experimental results demonstrate that the proposed model can efficiently infer robust modality-invariant 3D-scene representations and perform accurate cross-modal generation, which were explored using the Multisensory Embodied 3D-Scene Environment (MESE).",1
"Graph representation learning for hypergraphs can be used to extract patterns among higher-order interactions that are critically important in many real world problems. Current approaches designed for hypergraphs, however, are unable to handle different types of hypergraphs and are typically not generic for various learning tasks. Indeed, models that can predict variable-sized heterogeneous hyperedges have not been available. Here we develop a new self-attention based graph neural network called Hyper-SAGNN applicable to homogeneous and heterogeneous hypergraphs with variable hyperedge sizes. We perform extensive evaluations on multiple datasets, including four benchmark network datasets and two single-cell Hi-C datasets in genomics. We demonstrate that Hyper-SAGNN significantly outperforms the state-of-the-art methods on traditional tasks while also achieving great performance on a new task called outsider identification. Hyper-SAGNN will be useful for graph representation learning to uncover complex higher-order interactions in different applications.",0
"The use of graph representation learning in hypergraphs is crucial for identifying patterns among higher-order interactions in real-world problems. Nonetheless, current approaches are limited in their ability to handle various hypergraph types and are not universally applicable to different learning tasks. Moreover, there is a lack of models capable of predicting variable-sized heterogeneous hyperedges. In response, we have developed a novel self-attention based graph neural network, named Hyper-SAGNN, which can effectively address homogeneous and heterogeneous hypergraphs with varying hyperedge sizes. Through extensive evaluations on multiple datasets, including four benchmark network datasets and two single-cell Hi-C datasets in genomics, we have demonstrated that our Hyper-SAGNN model surpasses state-of-the-art methods on traditional tasks while also achieving remarkable performance on a new task called outsider identification. The integration of Hyper-SAGNN into graph representation learning will enable the discovery of intricate higher-order interactions across diverse applications.",1
"While a wide range of interpretable generative procedures for graphs exist, matching observed graph topologies with such procedures and choices for its parameters remains an open problem. Devising generative models that closely reproduce real-world graphs requires domain knowledge and time-consuming simulation. While existing deep learning approaches rely on less manual modelling, they offer little interpretability. This work approaches graph generation (decoding) as the inverse of graph compression (encoding). We show that in a disentanglement-focused deep autoencoding framework, specifically Beta-Variational Autoencoders (Beta-VAE), choices of generative procedures and their parameters arise naturally in the latent space. Our model is capable of learning disentangled, interpretable latent variables that represent the generative parameters of procedurally generated random graphs and real-world graphs. The degree of disentanglement is quantitatively measured using the Mutual Information Gap (MIG). When training our Beta-VAE model on ER random graphs, its latent variables have a near one-to-one mapping to the ER random graph parameters n and p. We deploy the model to analyse the correlation between graph topology and node attributes measuring their mutual dependence without handpicking topological properties.",0
"Although there are many interpretable generative procedures available for graphs, it is still a challenge to find the appropriate procedures and parameters to match observed graph topologies. Developing generative models that accurately replicate real-world graphs requires significant time and domain knowledge. While current deep learning methods rely less on manual modelling, they are not very interpretable. This study proposes an alternative approach to graph generation by viewing it as the inverse of graph compression. Using a disentanglement-focused deep autoencoding framework called Beta-Variational Autoencoders (Beta-VAE), the study demonstrates that generative procedures and their parameters can be naturally obtained in the latent space. The model can learn disentangled and interpretable latent variables that represent the generative parameters of procedurally generated random graphs and real-world graphs. The degree of disentanglement is measured quantitatively using the Mutual Information Gap (MIG). When training the Beta-VAE model on ER random graphs, the latent variables have a nearly one-to-one mapping to the ER random graph parameters n and p. The model is used to analyze the correlation between graph topology and node attributes, measuring their mutual dependence without handpicking topological properties.",1
"Multi-focus noisy image fusion represents an important task in the field of image fusion which generates a single, clear and focused image from all source images. In this paper, we propose a novel multi-focus noisy image fusion method based on low-rank representation (LRR) which is a powerful tool in representation learning. A multi-scale transform framework is adopted in which source images are decomposed into low frequency and high frequency coefficients, respectively. For low frequency coefficients, the fused low frequency coefficients are determined by a spatial frequency strategy, while the high frequency coefficients are fused by the LRR-based fusion strategy. Finally, the fused image is reconstructed by inverse multi-scale transforms with fused coefficients. Experimental results demonstrate that the proposed algorithm offers state-of-the-art performance even when the source images contain noise. The Code of our fusion method is available at https://github.com/hli1221/imagefusion_noisy_lrr",0
"Generating a single, clear and focused image from multiple source images is a crucial task in image fusion. Our paper introduces a novel method for multi-focus noisy image fusion using low-rank representation (LRR), which is a powerful tool for representation learning. To achieve this, we adopt a multi-scale transform framework that decomposes source images into low and high frequency coefficients. For low frequency coefficients, we use a spatial frequency strategy to determine the fused coefficients, while the LRR-based fusion strategy is used for high frequency coefficients. The fused image is then reconstructed using inverse multi-scale transforms with the fused coefficients. Our experimental results demonstrate that our proposed algorithm outperforms existing methods, even in cases where the source images are noisy. The code for our fusion method is available at https://github.com/hli1221/imagefusion_noisy_lrr.",1
"Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation. Pretrained BigBiGAN models -- including image generators and encoders -- are available on TensorFlow Hub (https://tfhub.dev/s?publisher=deepmind&q=bigbigan).",0
"GANs, which are generative models trained adversarially, have produced impressive results in image synthesis. However, while GANs were initially hailed for their use in unsupervised representation learning, they have since been replaced by self-supervised approaches. In this study, we prove that advancements in image generation quality can lead to drastically improved representation learning performance. Our method, called BigBiGAN, builds on the highly advanced BigGAN model by introducing an encoder and modifying the discriminator to facilitate representation learning. We extensively evaluate the BigBiGAN models' representation learning and generation abilities, revealing that these generation-based models are the current leader in unsupervised representation learning on ImageNet and unconditional image generation. Pretrained BigBiGAN models, including both image generators and encoders, are now available on TensorFlow Hub (https://tfhub.dev/s?publisher=deepmind&q=bigbigan).",1
"Despite their renowned predictive power on i.i.d. data, convolutional neural networks are known to rely more on high-frequency patterns that humans deem superficial than on low-frequency patterns that agree better with intuitions about what constitutes category membership. This paper proposes a method for training robust convolutional networks by penalizing the predictive power of the local representations learned by earlier layers. Intuitively, our networks are forced to discard predictive signals such as color and texture that can be gleaned from local receptive fields and to rely instead on the global structures of the image. Across a battery of synthetic and benchmark domain adaptation tasks, our method confers improved generalization out of the domain. Also, to evaluate cross-domain transfer, we introduce ImageNet-Sketch, a new dataset consisting of sketch-like images, that matches the ImageNet classification validation set in categories and scale.",0
"Despite being highly accurate in predicting i.i.d. data, convolutional neural networks have a tendency to rely more on high-frequency patterns that are superficial to humans, rather than low-frequency patterns that align better with our understanding of category membership. This paper presents a technique for training convolutional networks to be more robust by penalizing the predictive power of local representations learned in earlier layers. This forces the networks to prioritize global image structures rather than signals like color and texture that are gleaned from local receptive fields. Using synthetic and benchmark domain adaptation tasks, we demonstrate that our method improves generalization outside of the domain. Additionally, we introduce a new dataset, ImageNet-Sketch, which consists of sketch-like images that match the ImageNet classification validation set in categories and scale, to evaluate cross-domain transfer.",1
"This paper considers the problem of Phase Identification in power distribution systems. In particular, it focuses on improving supervised learning accuracies by focusing on exploiting some of the problem's information theoretic properties. This focus, along with recent advances in Information Theoretic Machine Learning (ITML), helps us to create two new techniques. The first transforms a bound on information losses into a data selection technique. This is important because phase identification data labels are difficult to obtain in practice. The second interprets the properties of distribution systems in the terms of ITML. This allows us to obtain an improvement in the representation learned by any classifier applied to the problem. We tested these two techniques experimentally on real datasets and have found that they yield phenomenal performance in every case. In the most extreme case, they improve phase identification accuracy from $51.7\%$ to $97.3\%$. Furthermore, since many problems share the physical properties of phase identification exploited in this paper, the techniques can be applied to a wide range of similar problems.",0
"This article explores Phase Identification in power distribution systems and proposes techniques to enhance supervised learning accuracy by utilizing the problem's information theoretic properties. With the aid of Information Theoretic Machine Learning (ITML), two new methods are developed. The first method transforms an information loss bound into a data selection technique, which is valuable as obtaining phase identification data labels in practice can be challenging. The second method interprets the properties of distribution systems using ITML, resulting in an improved representation learned by any classifier applied to the problem. Experimentation on actual datasets demonstrates the effectiveness of these techniques, achieving remarkable performance in all cases. In the most extreme scenario, phase identification accuracy is increased from 51.7% to 97.3%. Additionally, since many problems share similar physical properties with phase identification, these methods can be applied to a broad range of related issues.",1
"Automatic speech emotion recognition provides computers with critical context to enable user understanding. While methods trained and tested within the same dataset have been shown successful, they often fail when applied to unseen datasets. To address this, recent work has focused on adversarial methods to find more generalized representations of emotional speech. However, many of these methods have issues converging, and only involve datasets collected in laboratory conditions. In this paper, we introduce Adversarial Discriminative Domain Generalization (ADDoG), which follows an easier to train ""meet in the middle"" approach. The model iteratively moves representations learned for each dataset closer to one another, improving cross-dataset generalization. We also introduce Multiclass ADDoG, or MADDoG, which is able to extend the proposed method to more than two datasets, simultaneously. Our results show consistent convergence for the introduced methods, with significantly improved results when not using labels from the target dataset. We also show how, in most cases, ADDoG and MADDoG can be used to improve upon baseline state-of-the-art methods when target dataset labels are added and in-the-wild data are considered. Even though our experiments focus on cross-corpus speech emotion, these methods could be used to remove unwanted factors of variation in other settings.",0
"Computers require context to understand users, and automatic speech emotion recognition is a crucial tool for this purpose. However, methods that are successful within a dataset often fail when applied to new datasets, prompting recent research on adversarial approaches to develop more generalized representations of emotional speech. These methods have faced challenges in convergence and are limited to laboratory datasets. In this study, we present a simpler approach called Adversarial Discriminative Domain Generalization (ADDoG), which gradually brings learned representations of each dataset closer to each other to enhance cross-dataset generalization. We also introduce Multiclass ADDoG (MADDoG) for multiple datasets. Our results demonstrate consistent convergence and significant improvement when target dataset labels are absent or in-the-wild data is included. Although our focus is on cross-corpus speech emotion, these techniques can be applied to eliminate undesirable variation in other contexts.",1
"In this paper, we aim to tackle the one-shot person re-identification problem where only one image is labelled for each person, while other images are unlabelled. This task is challenging due to the lack of sufficient labelled training data. To tackle this problem, we propose to iteratively guess pseudo labels for the unlabeled image samples, which are later used to update the re-identification model together with the labelled samples. A new sampling mechanism is designed to select unlabeled samples to pseudo labelled samples based on the distance matrix, and to form a training triplet batch including both labelled samples and pseudo labelled samples. We also design an HSoften-Triplet-Loss to soften the negative impact of the incorrect pseudo label, considering the unreliable nature of pseudo labelled samples. Finally, we deploy an adversarial learning method to expand the image samples to different camera views. Our experiments show that our framework achieves a new state-of-the-art one-shot Re-ID performance on Market-1501 (mAP 42.7%) and DukeMTMC-Reid dataset (mAP 40.3%). Code will be available soon.",0
"The purpose of this paper is to address the difficulty of identifying individuals with only one labelled image, leaving the rest unlabelled. This challenge arises from the lack of adequate labelled training data. To overcome this issue, we propose a method of iteratively guessing pseudo labels for the unlabelled images, which are then used to update the re-identification model in conjunction with the labelled samples. We have designed a new sampling mechanism to select unlabelled samples and create a training triplet batch that includes both labelled and pseudo labelled samples based on the distance matrix. Additionally, we have developed an HSoften-Triplet-Loss to alleviate the negative impact of incorrect pseudo labels, considering their unreliable nature. Finally, we have deployed an adversarial learning approach to expand the image samples to various camera views. Our experimental results demonstrate that our framework outperforms the existing state-of-the-art one-shot Re-ID performance on Market-1501 (mAP 42.7%) and DukeMTMC-Reid dataset (mAP 40.3%). The code will be available shortly.",1
"Nonlinear independent component analysis (ICA) is a general framework for unsupervised representation learning, and aimed at recovering the latent variables in data. Recent practical methods perform nonlinear ICA by solving a series of classification problems based on logistic regression. However, it is well-known that logistic regression is vulnerable to outliers, and thus the performance can be strongly weakened by outliers. In this paper, we first theoretically analyze nonlinear ICA models in the presence of outliers. Our analysis implies that estimation in nonlinear ICA can be seriously hampered when outliers exist on the tails of the (noncontaminated) target density, which happens in a typical case of contamination by outliers. We develop two robust nonlinear ICA methods based on the {\gamma}-divergence, which is a robust alternative to the KL-divergence in logistic regression. The proposed methods are shown to have desired robustness properties in the context of nonlinear ICA. We also experimentally demonstrate that the proposed methods are very robust and outperform existing methods in the presence of outliers. Finally, the proposed method is applied to ICA-based causal discovery and shown to find a plausible causal relationship on fMRI data.",0
"The goal of nonlinear independent component analysis (ICA) is to learn unsupervised representations and uncover the underlying variables in data. Current approaches to nonlinear ICA rely on logistic regression, which is known to be vulnerable to outliers and can negatively impact performance. This paper presents a theoretical analysis of nonlinear ICA models in the presence of outliers. The analysis reveals that estimating nonlinear ICA can be significantly hindered when outliers exist on the tails of the target density. To address this issue, the paper proposes two robust nonlinear ICA methods that use the -divergence as a more resilient alternative to the KL-divergence in logistic regression. The proposed methods demonstrate superior robustness and outperform existing techniques when dealing with outliers. Additionally, the proposed approach is applied to ICA-based causal discovery and achieves promising results on fMRI data.",1
"Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.",0
"The goal of continual learning is to enhance the capacity of modern learning systems to handle non-stationary distributions by sequentially learning a series of tasks. Previous research has predominantly focused on supervised or reinforcement learning tasks, and has assumed complete knowledge of task labels and boundaries. This paper presents CURL, an approach that addresses a more comprehensive issue known as unsupervised continual learning. The emphasis is on learning representations without any prior knowledge of task identity, while considering scenarios involving abrupt changes between tasks, smooth transitions, or shuffled data. The CURL approach incorporates task inference within the model, can dynamically expand to capture new concepts, and includes rehearsal-based techniques to mitigate catastrophic forgetting. We demonstrate the effectiveness of CURL in an unsupervised learning environment using MNIST and Omniglot datasets, where the lack of labels ensures no task-related information is leaked. Additionally, we show superior performance compared to previous research in an i.i.d setting or when applied to supervised tasks like incremental class learning.",1
"In this work, we move beyond the traditional complex-valued representations, introducing more expressive hypercomplex representations to model entities and relations for knowledge graph embeddings. More specifically, quaternion embeddings, hypercomplex-valued embeddings with three imaginary components, are utilized to represent entities. Relations are modelled as rotations in the quaternion space. The advantages of the proposed approach are: (1) Latent inter-dependencies (between all components) are aptly captured with Hamilton product, encouraging a more compact interaction between entities and relations; (2) Quaternions enable expressive rotation in four-dimensional space and have more degree of freedom than rotation in complex plane; (3) The proposed framework is a generalization of ComplEx on hypercomplex space while offering better geometrical interpretations, concurrently satisfying the key desiderata of relational representation learning (i.e., modeling symmetry, anti-symmetry and inversion). Experimental results demonstrate that our method achieves state-of-the-art performance on four well-established knowledge graph completion benchmarks.",0
"This study goes beyond the conventional use of complex-valued representations by introducing more expressive hypercomplex representations to model entities and relations for knowledge graph embeddings. The approach involves the use of quaternion embeddings, which are hypercomplex-valued embeddings with three imaginary components, to represent entities. Relations are represented as rotations in the quaternion space. The advantages of this approach include the apt capturing of latent inter-dependencies with Hamilton product, expressive rotation in four-dimensional space, and a generalization of ComplEx on hypercomplex space with better geometrical interpretations. The proposed framework also satisfies the key desiderata of relational representation learning, such as modeling symmetry, anti-symmetry, and inversion. Experimental results show that this method achieves state-of-the-art performance on four established knowledge graph completion benchmarks.",1
"Deep generative models for graphs have shown great promise in the area of drug design, but have so far found little application beyond generating graph-structured molecules. In this work, we demonstrate a proof of concept for the challenging task of road network extraction from image data. This task can be framed as image-conditioned graph generation, for which we develop the Generative Graph Transformer (GGT), a deep autoregressive model that makes use of attention mechanisms for image conditioning and the recurrent generation of graphs. We benchmark GGT on the application of road network extraction from semantic segmentation data. For this, we introduce the Toulouse Road Network dataset, based on real-world publicly-available data. We further propose the StreetMover distance: a metric based on the Sinkhorn distance for effectively evaluating the quality of road network generation. The code and dataset are publicly available.",0
"Although deep generative models for graphs have been successful in drug design, their application has been limited to generating graph-structured molecules. This study aims to demonstrate the potential of these models in a new area: road network extraction from image data. We introduce the Generative Graph Transformer (GGT), an autoregressive model that uses attention mechanisms for image conditioning and recurrent graph generation. We evaluate GGT on the Toulouse Road Network dataset, a real-world collection of semantic segmentation data, and propose a new metric called StreetMover distance to effectively evaluate the quality of road network generation. Our code and dataset are available to the public.",1
"We explore the impact of learning paradigms on training deep neural networks for the Travelling Salesman Problem. We design controlled experiments to train supervised learning (SL) and reinforcement learning (RL) models on fixed graph sizes up to 100 nodes, and evaluate them on variable sized graphs up to 500 nodes. Beyond not needing labelled data, our results reveal favorable properties of RL over SL: RL training leads to better emergent generalization to variable graph sizes and is a key component for learning scale-invariant solvers for novel combinatorial problems.",0
"Our study delves into how learning paradigms affect the training of deep neural networks for the Travelling Salesman Problem. We conduct controlled experiments, training supervised learning (SL) and reinforcement learning (RL) models on graph sizes of up to 100 nodes, and assess their performance on variable sized graphs of up to 500 nodes. In addition to not requiring labelled data, our findings demonstrate that RL has advantageous characteristics over SL. Specifically, RL training results in superior generalization abilities for variable graph sizes and is crucial for developing scale-invariant solvers for new combinatorial problems.",1
"A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite---they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. We show that it is possible to learn naturally sparse representations that are more effective for online updating. Moreover, our algorithm is complementary to existing continual learning strategies, such as MER and GEM. Finally, we demonstrate that a basic online updating strategy on representations learned by OML is competitive with rehearsal based methods for continual learning. We release an implementation of our method at https://github.com/khurramjaved96/mrcl .",0
"The ability of a continual learning agent to learn quickly from new data while minimizing forgetfulness depends on its ability to build on existing knowledge. However, current intelligent systems that use neural network function approximators tend to forget what they have learned and are not equipped to facilitate future learning. This is because they learn from a representation that is not explicitly trained to achieve these objectives. To address this issue, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. Our approach enables the learning of naturally sparse representations that are more effective for online updating and can be used in conjunction with existing strategies such as MER and GEM. Finally, we demonstrate that our method is competitive with rehearsal-based methods for continual learning, and provide an implementation on https://github.com/khurramjaved96/mrcl.",1
"We introduce a framework for dynamic adversarial discovery of information (DADI), motivated by a scenario where information (a feature set) is used by third parties with unknown objectives. We train a reinforcement learning agent to sequentially acquire a subset of the information while balancing accuracy and fairness of predictors downstream. Based on the set of already acquired features, the agent decides dynamically to either collect more information from the set of available features or to stop and predict using the information that is currently available. Building on previous work exploring adversarial representation learning, we attain group fairness (demographic parity) by rewarding the agent with the adversary's loss, computed over the final feature set. Importantly, however, the framework provides a more general starting point for fair or private dynamic information discovery. Finally, we demonstrate empirically, using two real-world datasets, that we can trade-off fairness and predictive performance",0
"Our framework, known as Dynamic Adversarial Discovery of Information (DADI), was developed to address situations where third parties may utilize information without clear objectives. Through training a reinforcement learning agent, we are able to selectively acquire information while taking into account downstream predictor accuracy and fairness. The agent can intelligently determine whether to expand the feature set or rely on existing data for predictions. Our approach builds on previous work in adversarial representation learning to achieve group fairness via the adversary's loss, which is computed using the final feature set. Notably, our framework can be applied to a range of fair or private information discovery scenarios. We conducted empirical testing on two real-world datasets and discovered that we could balance predictive performance and fairness.",1
"Gaussian processes are flexible function approximators, with inductive biases controlled by a covariance kernel. Learning the kernel is the key to representation learning and strong predictive performance. In this paper, we develop functional kernel learning (FKL) to directly infer functional posteriors over kernels. In particular, we place a transformed Gaussian process over a spectral density, to induce a non-parametric distribution over kernel functions. The resulting approach enables learning of rich representations, with support for any stationary kernel, uncertainty over the values of the kernel, and an interpretable specification of a prior directly over kernels, without requiring sophisticated initialization or manual intervention. We perform inference through elliptical slice sampling, which is especially well suited to marginalizing posteriors with the strongly correlated priors typical to function space modelling. We develop our approach for non-uniform, large-scale, multi-task, and multidimensional data, and show promising performance in a wide range of settings, including interpolation, extrapolation, and kernel recovery experiments.",0
"The covariance kernel controls the inductive biases of Gaussian processes, making them versatile function approximators. To achieve strong predictive performance and representation learning, it is essential to learn the kernel. This paper presents functional kernel learning (FKL), which infers functional posteriors over kernels directly. The approach involves placing a transformed Gaussian process over a spectral density to induce a non-parametric distribution of kernel functions. FKL allows for the learning of rich representations, support for any stationary kernel, and uncertainty over kernel values, with an interpretable prior specification. Inference is done using elliptical slice sampling, which is well-suited for function space modelling. The proposed method is applicable to non-uniform, large-scale, multi-task, and multidimensional data, and exhibits promising results in interpolation, extrapolation, and kernel recovery experiments. No sophisticated initialization or manual intervention is required.",1
"One of the challenges in training generative models such as the variational auto encoder (VAE) is avoiding posterior collapse. When the generator has too much capacity, it is prone to ignoring latent code. This problem is exacerbated when the dataset is small, and the latent dimension is high. The root of the problem is the ELBO objective, specifically the Kullback-Leibler (KL) divergence term in objective function \citep{zhao2019infovae}. This paper proposes a new objective function to replace the KL term with one that emulates the maximum mean discrepancy (MMD) objective. It also introduces a new technique, named latent clipping, that is used to control distance between samples in latent space. A probabilistic autoencoder model, named $\mu$-VAE, is designed and trained on MNIST and MNIST Fashion datasets, using the new objective function and is shown to outperform models trained with ELBO and $\beta$-VAE objective. The $\mu$-VAE is less prone to posterior collapse, and can generate reconstructions and new samples in good quality. Latent representations learned by $\mu$-VAE are shown to be good and can be used for downstream tasks such as classification.",0
"The avoidance of posterior collapse is a significant challenge in training generative models, including the variational auto encoder (VAE). When the generator's capacity is excessive, the latent code may be ignored. This problem is more pronounced when the dataset is small and the latent dimension is high, and it originates from the KL divergence term in the ELBO objective function. To overcome this issue, the authors of \citep{zhao2019infovae} propose a new objective function that replaces the KL term with the maximum mean discrepancy (MMD) objective. They also introduce a technique called latent clipping to manage the distance between samples in latent space. The probabilistic autoencoder model, named $\mu$-VAE, is designed using the new objective function and trained on MNIST and MNIST Fashion datasets. The authors demonstrate that $\mu$-VAE outperforms ELBO and $\beta$-VAE models, is less prone to posterior collapse, and generates high-quality reconstructions and new samples. Furthermore, the latent representations learned by $\mu$-VAE are effective for downstream tasks like classification.",1
"We examine Generative Adversarial Networks (GANs) through the lens of deep Energy Based Models (EBMs), with the goal of exploiting the density model that follows from this formulation. In contrast to a traditional view where the discriminator learns a constant function when reaching convergence, here we show that it can provide useful information for downstream tasks, e.g., feature extraction for classification. To be concrete, in the EBM formulation, the discriminator learns an unnormalized density function (i.e., the negative energy term) that characterizes the data manifold. We propose to evaluate both the generator and the discriminator by deriving corresponding Fisher Score and Fisher Information from the EBM. We show that by assuming that the generated examples form an estimate of the learned density, both the Fisher Information and the normalized Fisher Vectors are easy to compute. We also show that we are able to derive a distance metric between examples and between sets of examples. We conduct experiments showing that the GAN-induced Fisher Vectors demonstrate competitive performance as unsupervised feature extractors for classification and perceptual similarity tasks. Code is available at \url{https://github.com/apple/ml-afv}.",0
"Our exploration of Generative Adversarial Networks (GANs) focuses on their relationship with deep Energy Based Models (EBMs) in order to utilize the density model that arises from this framework. Rather than the traditional understanding that the discriminator learns a constant function upon convergence, we demonstrate that it can offer valuable insights for downstream tasks such as classification feature extraction. Specifically, in the EBM formulation, the discriminator acquires knowledge of an unnormalized density function (i.e., the negative energy term) that characterizes the data manifold. We propose to evaluate both the generator and the discriminator by utilizing corresponding Fisher Scores and Fisher Information from the EBM. By assuming that the generated examples provide an estimate of the learned density, we can compute both the Fisher Information and the normalized Fisher Vectors with ease. We also introduce a distance metric for both individual examples and sets of examples. Our experiments show that the GAN-generated Fisher Vectors perform competitively as unsupervised feature extractors for classification and perceptual similarity tasks. Code for this work can be found at \url{https://github.com/apple/ml-afv}.",1
"RGB-Infrared (IR) person re-identification is an important and challenging task due to large cross-modality variations between RGB and IR images. Most conventional approaches aim to bridge the cross-modality gap with feature alignment by feature representation learning. Different from existing methods, in this paper, we propose a novel and end-to-end Alignment Generative Adversarial Network (AlignGAN) for the RGB-IR RE-ID task. The proposed model enjoys several merits. First, it can exploit pixel alignment and feature alignment jointly. To the best of our knowledge, this is the first work to model the two alignment strategies jointly for the RGB-IR RE-ID problem. Second, the proposed model consists of a pixel generator, a feature generator, and a joint discriminator. By playing a min-max game among the three components, our model is able to not only alleviate the cross-modality and intra-modality variations but also learn identity-consistent features. Extensive experimental results on two standard benchmarks demonstrate that the proposed model performs favorably against state-of-the-art methods. Especially, on SYSU-MM01 dataset, our model can achieve an absolute gain of 15.4% and 12.9% in terms of Rank-1 and mAP.",0
"Person re-identification through RGB-Infrared (IR) imaging is a difficult task due to the significant differences between the two modalities. Current approaches focus on feature alignment through representation learning to bridge this gap. However, this paper presents a unique approach through an end-to-end Alignment Generative Adversarial Network (AlignGAN) for RGB-IR RE-ID. This model combines pixel and feature alignment strategies, which is a first in this field. The AlignGAN model is made up of a pixel generator, feature generator, and joint discriminator, which work together to reduce cross-modality and intra-modality variations while learning consistent features. Results from experiments on two standard benchmarks show that the proposed model outperforms state-of-the-art methods, with a significant 15.4% and 12.9% increase in Rank-1 and mAP on the SYSU-MM01 dataset.",1
"Learning from graph-structured data is an important task in machine learning and artificial intelligence, for which Graph Neural Networks (GNNs) have shown great promise. Motivated by recent advances in geometric representation learning, we propose a novel GNN architecture for learning representations on Riemannian manifolds with differentiable exponential and logarithmic maps. We develop a scalable algorithm for modeling the structural properties of graphs, comparing Euclidean and hyperbolic geometry. In our experiments, we show that hyperbolic GNNs can lead to substantial improvements on various benchmark datasets.",0
Graph Neural Networks (GNNs) have demonstrated significant potential in the important task of learning from graph-structured data in the field of machine learning and artificial intelligence. Our proposed GNN architecture leverages recent advancements in geometric representation learning and is designed for learning representations on Riemannian manifolds through the use of differentiable exponential and logarithmic maps. We have created a scalable algorithm for modeling the structural properties of graphs and have compared Euclidean and hyperbolic geometry. Our experiments show that hyperbolic GNNs provide substantial improvements on various benchmark datasets.,1
"Deep neural networks require collecting and annotating large amounts of data to train successfully. In order to alleviate the annotation bottleneck, we propose a novel self-supervised representation learning approach for spatiotemporal features extracted from videos. We introduce Skip-Clip, a method that utilizes temporal coherence in videos, by training a deep model for future clip order ranking conditioned on a context clip as a surrogate objective for video future prediction. We show that features learned using our method are generalizable and transfer strongly to downstream tasks. For action recognition on the UCF101 dataset, we obtain 51.8% improvement over random initialization and outperform models initialized using inflated ImageNet parameters. Skip-Clip also achieves results competitive with state-of-the-art self-supervision methods.",0
"To train deep neural networks effectively, a large amount of data must be collected and annotated. To overcome the bottleneck caused by annotation, we present a new method of self-supervised representation learning for spatiotemporal features extracted from videos. Our approach, called Skip-Clip, utilizes temporal coherence in videos and trains a deep model for future clip order ranking based on a context clip. This serves as a surrogate objective for video future prediction. Our method produces generalizable features that transfer well to downstream tasks. When tested on the UCF101 dataset for action recognition, Skip-Clip showed a 51.8% improvement over random initialization and outperformed models initialized using inflated ImageNet parameters. Additionally, our results were competitive with state-of-the-art self-supervision methods.",1
"Deep neural networks progressively transform their inputs across multiple processing layers. What are the geometrical properties of the representations learned by these networks? Here we study the intrinsic dimensionality (ID) of data-representations, i.e. the minimal number of parameters needed to describe a representation. We find that, in a trained network, the ID is orders of magnitude smaller than the number of units in each layer. Across layers, the ID first increases and then progressively decreases in the final layers. Remarkably, the ID of the last hidden layer predicts classification accuracy on the test set. These results can neither be found by linear dimensionality estimates (e.g., with principal component analysis), nor in representations that had been artificially linearized. They are neither found in untrained networks, nor in networks that are trained on randomized labels. This suggests that neural networks that can generalize are those that transform the data into low-dimensional, but not necessarily flat manifolds.",0
"The inputs of deep neural networks undergo multiple processing layers that transform them. Our focus is on exploring the geometrical properties of the learned representations. Specifically, we look into the intrinsic dimensionality (ID) of these data-representations, which refers to the minimum number of parameters required to describe a representation. Our findings indicate that the ID in trained networks is significantly lower than the number of units in each layer. Furthermore, the ID increases across layers and then gradually decreases in the final layers. Interestingly, the ID of the final hidden layer can predict the accuracy of classification on the test set. Notably, these results cannot be obtained through linear dimensionality estimates or artificially linearized representations. They are also not found in untrained networks or networks trained on randomized labels. This suggests that neural networks capable of generalization transform data into low-dimensional, non-flat manifolds.",1
"Deep representation learning using triplet network for classification suffers from a lack of theoretical foundation and difficulty in tuning both the network and classifiers for performance. To address the problem, local-margin triplet loss along with local positive and negative mining strategy is proposed with theory on how the strategy integrate nearest-neighbor hyper-parameter with triplet learning to increase subsequent classification performance. Results in experiments with 2 public datasets, MNIST and Cifar-10, and 2 small medical image datasets demonstrate that proposed strategy outperforms end-to-end softmax and typical triplet loss in settings without data augmentation while maintaining utility of transferable feature for related tasks. The method serves as a good performance baseline where end-to-end methods encounter difficulties such as small sample data with limited allowable data augmentation.",0
"The use of triplet networks for deep representation learning in classification faces challenges due to a lack of theoretical foundation and the complexity of tuning both the network and classifiers for optimal performance. To overcome this, a local-margin triplet loss approach is proposed, along with a local positive and negative mining strategy. The theoretical framework outlines how this new strategy incorporates the nearest-neighbor hyper-parameter with triplet learning to enhance subsequent classification performance. Experimental results with MNIST and Cifar-10 datasets, as well as two medical image datasets, show that the proposed strategy outperforms end-to-end softmax and typical triplet loss approaches in settings without data augmentation. Moreover, the transferable feature remains useful for related tasks. This method serves as a reliable performance baseline in situations where end-to-end methods struggle, such as with limited data augmentation and small sample sizes.",1
"Studies show that the representations learned by deep neural networks can be transferred to similar prediction tasks in other domains for which we do not have enough labeled data. However, as we transition to higher layers in the model, the representations become more task-specific and less generalizable. Recent research on deep domain adaptation proposed to mitigate this problem by forcing the deep model to learn more transferable feature representations across domains. This is achieved by incorporating domain adaptation methods into deep learning pipeline. The majority of existing models learn the transferable feature representations which are highly correlated with the outcome. However, correlations are not always transferable. In this paper, we propose a novel deep causal representation learning framework for unsupervised domain adaptation, in which we propose to learn domain-invariant causal representations of the input from the source domain. We simulate a virtual target domain using reweighted samples from the source domain and estimate the causal effect of features on the outcomes. The extensive comparative study demonstrates the strengths of the proposed model for unsupervised domain adaptation via causal representations.",0
"Research has shown that deep neural networks can transfer learned representations to similar prediction tasks in different domains where labeled data is limited. However, the representations become more specific to the task and less generalizable as we move to higher layers in the model. To address this issue, recent studies suggest incorporating domain adaptation methods into the deep learning pipeline, which help the model learn more transferable feature representations across domains. Most existing models learn transferable feature representations that are strongly associated with the outcome, but not always transferable. In this study, we introduce a novel framework for unsupervised domain adaptation called deep causal representation learning, which aims to learn domain-invariant causal representations of the input from the source domain. We create a virtual target domain using reweighted samples from the source domain and estimate the causal effect of features on the outcomes. Our extensive comparative study demonstrates the effectiveness of our model for unsupervised domain adaptation via causal representations.",1
"Modern neural network training relies on piece-wise (sub-)differentiable functions in order to use backpropagation to update model parameters. In this work, we introduce a novel method to allow simple non-differentiable functions at intermediary layers of deep neural networks. We do so by training with a differentiable approximation bridge (DAB) neural network which approximates the non-differentiable forward function and provides gradient updates during backpropagation. We present strong empirical results (performing over 600 experiments) in four different domains: unsupervised (image) representation learning, variational (image) density estimation, image classification, and sequence sorting to demonstrate that our proposed method improves state of the art performance. We demonstrate that training with DAB aided discrete non-differentiable functions improves image reconstruction quality and posterior linear separability by 10% against the Gumbel-Softmax relaxed estimator [37, 26] as well as providing a 9% improvement in the test variational lower bound in comparison to the state of the art RELAX [16] discrete estimator. We also observe an accuracy improvement of 77% in neural sequence sorting and a 25% improvement against the straight-through estimator [5] in an image classification setting. The DAB network is not used for inference and expands the class of functions that are usable in neural networks.",0
"To update model parameters in neural network training, backpropagation relies on piece-wise (sub-)differentiable functions. However, in this study, we introduce a novel approach that enables the use of simple non-differentiable functions in intermediary deep neural network layers. This is accomplished through training with a differentiable approximation bridge (DAB) neural network, which approximates the non-differentiable forward function and facilitates gradient updates during backpropagation. We conducted over 600 experiments in four different domains, including unsupervised (image) representation learning, variational (image) density estimation, image classification, and sequence sorting, and found that our proposed method significantly improves performance compared to state-of-the-art approaches. Specifically, we observed a 10% improvement in image reconstruction quality and posterior linear separability, a 9% improvement in the test variational lower bound, a 77% accuracy improvement in neural sequence sorting, and a 25% improvement in image classification compared to other estimators. The DAB network is not involved in inference and expands the range of functions that can be used in neural networks.",1
"Graph Attention Networks (GATs) are the state-of-the-art neural architecture for representation learning with graphs. GATs learn attention functions that assign weights to nodes so that different nodes have different influences in the feature aggregation steps. In practice, however, induced attention functions are prone to over-fitting due to the increasing number of parameters and the lack of direct supervision on attention weights. GATs also suffer from over-smoothing at the decision boundary of nodes. Here we propose a framework to address their weaknesses via margin-based constraints on attention during training. We first theoretically demonstrate the over-smoothing behavior of GATs and then develop an approach using constraint on the attention weights according to the class boundary and feature aggregation pattern. Furthermore, to alleviate the over-fitting problem, we propose additional constraints on the graph structure. Extensive experiments and ablation studies on common benchmark datasets demonstrate the effectiveness of our method, which leads to significant improvements over the previous state-of-the-art graph attention methods on all datasets.",0
"The most advanced neural architecture for learning with graphs is known as Graph Attention Networks (GATs). These networks are capable of learning attention functions that assign different weights to nodes, giving them varying degrees of influence in the feature aggregation process. However, GATs have a tendency to over-fit due to the increasing number of parameters and the lack of supervision on attention weights. Additionally, GATs tend to smooth out at the decision boundaries of nodes. To address these issues, we propose a new framework that incorporates margin-based constraints during training. Our approach utilizes class boundaries and feature aggregation patterns to constrain attention weights, while also including constraints on the graph structure to mitigate over-fitting. Through extensive experimentation and ablation studies, we demonstrate the effectiveness of our approach, which outperforms previous state-of-the-art graph attention methods on all benchmark datasets.",1
"Domain adaptation aims to exploit the knowledge in source domain to promote the learning tasks in target domain, which plays a critical role in real-world applications. Recently, lots of deep learning approaches based on autoencoders have achieved a significance performance in domain adaptation. However, most existing methods focus on minimizing the distribution divergence by putting the source and target data together to learn global feature representations, while they do not consider the local relationship between instances in the same category from different domains. To address this problem, we propose a novel Semi-Supervised Representation Learning framework via Dual Autoencoders for domain adaptation, named SSRLDA. More specifically, we extract richer feature representations by learning the global and local feature representations simultaneously using two novel autoencoders, which are referred to as marginalized denoising autoencoder with adaptation distribution (MDAad) and multi-class marginalized denoising autoencoder (MMDA) respectively. Meanwhile, we make full use of label information to optimize feature representations. Experimental results show that our proposed approach outperforms several state-of-the-art baseline methods.",0
"The goal of domain adaptation is to leverage knowledge from a source domain to improve learning outcomes in a target domain, which is crucial for practical applications. Autoencoder-based deep learning methods have recently demonstrated significant success in domain adaptation. However, existing approaches primarily focus on minimizing distribution divergence by combining source and target data to learn global feature representations, without considering the local relationships among instances in the same category from different domains. To tackle this issue, we propose a new framework for domain adaptation called Semi-Supervised Representation Learning via Dual Autoencoders (SSRLDA). Our approach simultaneously learns global and local feature representations using two novel autoencoders, namely the marginalized denoising autoencoder with adaptation distribution (MDAad) and the multi-class marginalized denoising autoencoder (MMDA). Additionally, we utilize label information to optimize feature representations. Our experimental results demonstrate that our approach outperforms several state-of-the-art baseline methods.",1
"Existing methods for person re-identification (Re-ID) are mostly based on supervised learning which requires numerous manually labeled samples across all camera views for training. Such a paradigm suffers the scalability issue since in real-world Re-ID application, it is difficult to exhaustively label abundant identities over multiple disjoint camera views. To this end, we propose a progressive deep learning method for unsupervised person Re-ID in the wild by Tracklet Association with Spatio-Temporal Regularization (TASTR). In our approach, we first collect tracklet data within each camera by automatic person detection and tracking. Then, an initial Re-ID model is trained based on within-camera triplet construction for person representation learning. After that, based on the person visual feature and spatio-temporal constraint, we associate cross-camera tracklets to generate cross-camera triplets and update the Re-ID model. Lastly, with the refined Re-ID model, better visual feature of person can be extracted, which further promote the association of cross-camera tracklets. The last two steps are iterated multiple times to progressively upgrade the Re-ID model.",0
"Most methods for person re-identification (Re-ID) rely on supervised learning, which demands a large number of manually labeled examples from all camera angles for training. However, this model lacks scalability because labeling vast numbers of identities across multiple, separate camera angles is challenging in real-world Re-ID situations. To address this, we propose a novel unsupervised person Re-ID method through Tracklet Association with Spatio-Temporal Regularization (TASTR) using progressive deep learning. Our method first collects tracklet data from each camera using automated person detection and tracking before training an initial Re-ID model based on within-camera triplet construction for person representation learning. Next, cross-camera tracklets are associated based on person visual features and spatio-temporal constraints to generate cross-camera triplets and update the Re-ID model. Further iterations of the last two steps progressively enhance the Re-ID model by extracting better visual features of people, which, in turn, promotes the association of cross-camera tracklets.",1
"Revealing latent structure in data is an active field of research, having introduced exciting technologies such as variational autoencoders and adversarial networks, and is essential to push machine learning towards unsupervised knowledge discovery. However, a major challenge is the lack of suitable benchmarks for an objective and quantitative evaluation of learned representations. To address this issue we introduce Morpho-MNIST, a framework that aims to answer: ""to what extent has my model learned to represent specific factors of variation in the data?"" We extend the popular MNIST dataset by adding a morphometric analysis enabling quantitative comparison of trained models, identification of the roles of latent variables, and characterisation of sample diversity. We further propose a set of quantifiable perturbations to assess the performance of unsupervised and supervised methods on challenging tasks such as outlier detection and domain adaptation. Data and code are available at https://github.com/dccastro/Morpho-MNIST.",0
"The exploration of hidden patterns in data is a dynamic area of study that has introduced innovative technologies like variational autoencoders and adversarial networks. This is crucial for advancing machine learning towards independent knowledge discovery. Nonetheless, there is a significant difficulty in finding suitable benchmarks for an objective and quantitative assessment of acquired representations. Our solution to this issue is the Morpho-MNIST framework, which seeks to answer the question of whether a model has learned to represent specific variables in the data. By expanding the widely used MNIST dataset with morphometric analysis, we enable the quantitative comparison of trained models, identification of latent variable roles, and the characterization of sample diversity. We also suggest measurable perturbations to evaluate the effectiveness of both unsupervised and supervised methods on challenging tasks like outlier detection and domain adaptation. You can find the data and code at https://github.com/dccastro/Morpho-MNIST.",1
"The estimation of an f-divergence between two probability distributions based on samples is a fundamental problem in statistics and machine learning. Most works study this problem under very weak assumptions, in which case it is provably hard. We consider the case of stronger structural assumptions that are commonly satisfied in modern machine learning, including representation learning and generative modelling with autoencoder architectures. Under these assumptions we propose and study an estimator that can be easily implemented, works well in high dimensions, and enjoys faster rates of convergence. We verify the behavior of our estimator empirically in both synthetic and real-data experiments, and discuss its direct implications for total correlation, entropy, and mutual information estimation.",0
"The fundamental problem in statistics and machine learning is the estimation of an f-divergence between two probability distributions based on samples. However, this problem is often studied under very weak assumptions, which makes it difficult to solve. Our research focuses on stronger structural assumptions that are commonly satisfied in modern machine learning, including representation learning and generative modelling with autoencoder architectures. We propose an estimator that is easy to implement, performs well in high dimensions, and has faster rates of convergence. We conducted empirical experiments using synthetic and real data to verify the behavior of our estimator and discuss its implications for estimating total correlation, entropy, and mutual information.",1
In this work we approach the task of learning multilingual word representations in an offline manner by fitting a generative latent variable model to a multilingual dictionary. We model equivalent words in different languages as different views of the same word generated by a common latent variable representing their latent lexical meaning. We explore the task of alignment by querying the fitted model for multilingual embeddings achieving competitive results across a variety of tasks. The proposed model is robust to noise in the embedding space making it a suitable method for distributed representations learned from noisy corpora.,0
"Our work focuses on learning multilingual word representations offline through the use of a generative latent variable model applied to a multilingual dictionary. Our approach considers equivalent words in different languages as different perspectives of the same word, which are generated by a shared latent variable representing their lexical meaning. We investigate alignment by extracting multilingual embeddings from the fitted model, and our results are competitive across various tasks. Additionally, our model is resilient to noise in the embedding space, making it a viable option for distributed representations learned from noisy corpora.",1
"Endowing robots with human-like physical reasoning abilities remains challenging. We argue that existing methods often disregard spatio-temporal relations and by using Graph Neural Networks (GNNs) that incorporate a relational inductive bias, we can shift the learning process towards exploiting relations. In this work, we learn action-conditional forward dynamics models of a simulated manipulation task from visual observations involving cluttered and irregularly shaped objects. We investigate two GNN approaches and empirically assess their capability to generalize to scenarios with novel and an increasing number of objects. The first, Graph Networks (GN) based approach, considers explicitly defined edge attributes and not only does it consistently underperform an auto-encoder baseline that we modified to predict future states, our results indicate how different edge attributes can significantly influence the predictions. Consequently, we develop the Auto-Predictor that does not rely on explicitly defined edge attributes. It outperforms the baseline and the GN-based models. Overall, our results show the sensitivity of GNN-based approaches to the task representation, the efficacy of relational inductive biases and advocate choosing lightweight approaches that implicitly reason about relations over ones that leave these decisions to human designers.",0
"It is still a difficult task to give robots human-like physical reasoning abilities. Our argument is that current methods tend to overlook spatio-temporal relationships. By using Graph Neural Networks (GNNs) that incorporate a relational inductive bias, we can focus on exploiting relationships during the learning process. In this study, we train action-conditional forward dynamics models for a simulated manipulation task using visual observations of cluttered and irregularly shaped objects. We test two GNN approaches and evaluate their ability to generalize to scenarios with new and increasing numbers of objects. The first approach, based on Graph Networks (GN), explicitly defines edge attributes. However, it consistently underperforms a modified auto-encoder baseline that predicts future states. We find that different edge attributes can significantly affect the predictions. As a result, we introduce the Auto-Predictor, which does not rely on explicitly defined edge attributes and outperforms both the baseline and GN-based models. Our results demonstrate the importance of task representation in GNN-based approaches, the effectiveness of relational inductive biases, and the advantages of choosing lightweight approaches that implicitly reason about relations instead of leaving these decisions to human designers.",1
"Multimodalities provide promising performance than unimodality in most tasks. However, learning the semantic of the representations from multimodalities efficiently is extremely challenging. To tackle this, we propose the Transformer based Cross-modal Translator (TCT) to learn unimodal sequence representations by translating from other related multimodal sequences on a supervised learning method. Combined TCT with Multimodal Transformer Network (MTN), we evaluate MTN-TCT on the video-grounded dialogue which uses multimodality. The proposed method reports new state-of-the-art performance on video-grounded dialogue which indicates representations learned by TCT are more semantics compared to directly use unimodality.",0
"In most tasks, multimodalities show more promise in performance than unimodality. However, efficiently learning the semantic of representations from multimodalities presents a significant challenge. To address this issue, we suggest using the Transformer based Cross-modal Translator (TCT) to learn unimodal sequence representations through supervised learning by translating from other related multimodal sequences. When combined with the Multimodal Transformer Network (MTN), we examine MTN-TCT in video-grounded dialogue which employs multimodality. The proposed approach achieves a new state-of-the-art performance in video-grounded dialogue, indicating that the representations learned by TCT are more semantically rich compared to those derived from unimodality alone.",1
"Reconstructing visual stimulus (image) only from human brain activity measured with functional Magnetic Resonance Imaging (fMRI) is a significant and meaningful task in Human-AI collaboration. However, the inconsistent distribution and representation between fMRI signals and visual images cause the heterogeneity gap. Moreover, the fMRI data is often extremely high-dimensional and contains a lot of visually-irrelevant information. Existing methods generally suffer from these issues so that a satisfactory reconstruction is still challenging. In this paper, we show that it is possible to overcome these challenges by learning visually-guided cognitive latent representations from the fMRI signals, and inversely decoding them to the image stimuli. The resulting framework is called Dual-Variational Autoencoder/ Generative Adversarial Network (D-VAE/GAN), which combines the advantages of adversarial representation learning with knowledge distillation. In addition, we introduce a novel three-stage learning approach which enables the (cognitive) encoder to gradually distill useful knowledge from the paired (visual) encoder during the learning process. Extensive experimental results on both artificial and natural images have demonstrated that our method could achieve surprisingly good results and outperform all other alternatives.",0
"A significant and meaningful task in Human-AI collaboration involves reconstructing visual stimulus (image) solely from human brain activity measured by functional Magnetic Resonance Imaging (fMRI). However, the heterogeneity gap arises due to the inconsistent distribution and representation between fMRI signals and visual images. Additionally, the fMRI data is often high-dimensional and contains visually-irrelevant information, making satisfactory reconstruction challenging for existing methods. To address these issues, we propose a solution that involves learning visually-guided cognitive latent representations from the fMRI signals and decoding them to the image stimuli. Our approach, called Dual-Variational Autoencoder/ Generative Adversarial Network (D-VAE/GAN), combines adversarial representation learning with knowledge distillation. We also introduce a three-stage learning approach that enables the encoder to gradually distill useful knowledge from the paired encoder during the learning process. Our method achieves surprisingly good results and outperforms all other alternatives, as demonstrated by extensive experimental results on both artificial and natural images.",1
"We propose a condition-adaptive representation learning framework for the driver drowsiness detection based on 3D-deep convolutional neural network. The proposed framework consists of four models: spatio-temporal representation learning, scene condition understanding, feature fusion, and drowsiness detection. The spatio-temporal representation learning extracts features that can describe motions and appearances in video simultaneously. The scene condition understanding classifies the scene conditions related to various conditions about the drivers and driving situations such as statuses of wearing glasses, illumination condition of driving, and motion of facial elements such as head, eye, and mouth. The feature fusion generates a condition-adaptive representation using two features extracted from above models. The detection model recognizes drivers drowsiness status using the condition-adaptive representation. The condition-adaptive representation learning framework can extract more discriminative features focusing on each scene condition than the general representation so that the drowsiness detection method can provide more accurate results for the various driving situations. The proposed framework is evaluated with the NTHU Drowsy Driver Detection video dataset. The experimental results show that our framework outperforms the existing drowsiness detection methods based on visual analysis.",0
"Our proposed framework for detecting driver drowsiness based on 3D-deep convolutional neural network is designed to adapt to various conditions. It includes four models: spatio-temporal representation learning, scene condition understanding, feature fusion, and drowsiness detection. The spatio-temporal representation learning extracts features that capture both motion and appearance in video. The scene condition understanding identifies the different conditions related to drivers and driving situations, such as glasses-wearing, illumination, and facial motion. The feature fusion combines two extracted features to generate a condition-adaptive representation. The detection model uses this representation to identify the driver's drowsiness status. By focusing on each scene condition, our framework can extract more distinct features than a general representation, leading to more accurate results for various driving situations. We evaluate our framework using the NTHU Drowsy Driver Detection video dataset and show that it outperforms existing drowsiness detection methods based on visual analysis.",1
"Generative Adversarial Networks (GANs) have been used extensively and quite successfully for unsupervised learning. As GANs don't approximate an explicit probability distribution, it's an interesting study to inspect the latent space representations learned by GANs. The current work seeks to push the boundaries of such inspection methods to further understand in more detail the manifold being learned by GANs. Various interpolation and extrapolation techniques along with vector arithmetic is used to understand the learned manifold. We show through experiments that GANs indeed learn a data probability distribution rather than memorize images/data. Further, we prove that GANs encode semantically relevant information in the learned probability distribution. The experiments have been performed on two publicly available datasets - Large Scale Scene Understanding (LSUN) and CelebA.",0
"Unsupervised learning has extensively and successfully utilized Generative Adversarial Networks (GANs). Since GANs do not approximate an explicit probability distribution, it is interesting to examine the latent space representations they learn. This study aims to expand inspection methods to better comprehend the manifold being learned by GANs, using various interpolation and extrapolation techniques along with vector arithmetic. Through experimentation on publicly available datasets such as Large Scale Scene Understanding (LSUN) and CelebA, we demonstrate that GANs learn a data probability distribution and encode semantically relevant information in it, rather than merely memorizing images or data.",1
"Recent studies on the adversarial vulnerability of neural networks have shown that models trained with the objective of minimizing an upper bound on the worst-case loss over all possible adversarial perturbations improve robustness against adversarial attacks. Beside exploiting adversarial training framework, we show that by enforcing a Deep Neural Network (DNN) to be linear in transformed input and feature space improves robustness significantly. We also demonstrate that by augmenting the objective function with Local Lipschitz regularizer boost robustness of the model further. Our method outperforms most sophisticated adversarial training methods and achieves state of the art adversarial accuracy on MNIST, CIFAR10 and SVHN dataset. In this paper, we also propose a novel adversarial image generation method by leveraging Inverse Representation Learning and Linearity aspect of an adversarially trained deep neural network classifier.",0
"New research has revealed that minimizing an upper bound on the worst-case loss over all possible adversarial perturbations during training enhances the resilience of neural networks to adversarial attacks. In addition to utilizing this adversarial training framework, our study demonstrates that enforcing linearity in both transformed input and feature space of a Deep Neural Network (DNN) further improves robustness. We also show that incorporating a Local Lipschitz regularizer into the objective function enhances the model's robustness. Our method surpasses the majority of sophisticated adversarial training techniques and achieves state-of-the-art adversarial accuracy on datasets such as MNIST, CIFAR10, and SVHN. Furthermore, we introduce a novel adversarial image generation technique that utilizes Inverse Representation Learning and the Linearity aspect of an adversarially trained deep neural network classifier.",1
"This paper proposes a method to ease the unsupervised learning of object landmark detectors. Similarly to previous methods, our approach is fully unsupervised in a sense that it does not require or make any use of annotated landmarks for the target object category. Contrary to previous works, we do however assume that a landmark detector, which has already learned a structured representation for a given object category in a fully supervised manner, is available. Under this setting, our main idea boils down to adapting the given pre-trained network to the target object categories in a fully unsupervised manner. To this end, our method uses the pre-trained network as a core which remains frozen and does not get updated during training, and learns, in an unsupervised manner, only a projection matrix to perform the adaptation to the target categories. By building upon an existing structured representation learned in a supervised manner, the optimization problem solved by our method is much more constrained with significantly less parameters to learn which seems to be important for the case of unsupervised learning. We show that our method surpasses fully unsupervised techniques trained from scratch as well as a strong baseline based on fine-tuning, and produces state-of-the-art results on several datasets. Code can be found at https://github.com/ESanchezLozano/SAIC-Unsupervised-landmark-detection-NeurIPS2019 .",0
"In this paper, a technique is proposed to facilitate the unsupervised learning of object landmark detectors. Similar to previous methods, the approach is entirely unsupervised and does not rely on or utilize annotated landmarks for the target object category. However, unlike previous works, the method assumes the availability of a landmark detector that has already learned a structured representation for a specific object category in a fully supervised manner. The primary idea is to adapt the pre-trained network to the target object categories in a fully unsupervised manner by using it as a core that remains frozen and only learning a projection matrix for adaptation. By building upon an existing structured representation learned in a supervised manner, the optimization problem solved by the method is more constrained, with significantly fewer parameters to learn, which is crucial for unsupervised learning. The method outperforms fully unsupervised techniques trained from scratch and a strong baseline based on fine-tuning, producing state-of-the-art results on multiple datasets. The code can be found at https://github.com/ESanchezLozano/SAIC-Unsupervised-landmark-detection-NeurIPS2019.",1
"Drug repositioning is an attractive cost-efficient strategy for the development of treatments for human diseases. Here, we propose an interpretable model that learns disease self-representations for drug repositioning. Our self-representation model represents each disease as a linear combination of a few other diseases. We enforce proximity in the learnt representations in a way to preserve the geometric structure of the human phenome network - a domain-specific knowledge that naturally adds relational inductive bias to the disease self-representations. We prove that our method is globally optimal and show results outperforming state-of-the-art drug repositioning approaches. We further show that the disease self-representations are biologically interpretable.",0
"The use of drug repositioning is a cost-effective approach for developing treatments for human illnesses. In this study, we present a comprehensible model that acquires disease self-representations for drug repositioning. Our self-representation model expresses each disease as a combination of other diseases. We ensure closeness in the learned descriptions to maintain the structure of the human phenome network, which provides domain-specific knowledge and adds relational inductive bias to the disease self-representations. Our approach is globally optimal and produces better results than current drug repositioning methods. In addition, we demonstrate that the disease self-representations have biological significance.",1
"We present two instances, L-GAE and L-VGAE, of the variational graph auto-encoding family (VGAE) based on separating feature propagation operations from graph convolution layers typically found in graph learning methods to a single linear matrix computation made prior to input in standard auto-encoder architectures. This decoupling enables the independent and fixed design of the auto-encoder without requiring additional GCN layers for every desired increase in the size of a node's local receptive field. Fixing the auto-encoder enables a fairer assessment on the size of a nodes receptive field in building representations. Furthermore a by-product of fixing the auto-encoder design often results in substantially smaller networks than their VGAE counterparts especially as we increase the number of feature propagations. A comparative downstream evaluation on link prediction tasks show comparable state of the art performance to similar VGAE arrangements despite considerable simplification. We also show the simple application of our methodology to more challenging representation learning scenarios such as spatio-temporal graph representation learning.",0
"We introduce two types of the VGAE called L-GAE and L-VGAE. These models separate feature propagation operations from graph convolution layers, which are commonly found in graph learning methods. Instead, they utilize a single linear matrix computation before inputting it into a standard auto-encoder architecture. This decoupling allows for the independent and fixed design of the auto-encoder, eliminating the need for additional GCN layers to increase a node's local receptive field. This fixed design allows for a more equitable assessment of a node's receptive field size when building representations. Additionally, this methodology often results in smaller networks compared to their VGAE counterparts, particularly as the number of feature propagations increases. Despite this simplification, our comparative evaluation on link prediction tasks shows competitive performance compared to similar VGAE arrangements. We also demonstrate the application of our approach to more challenging representation learning scenarios, such as spatio-temporal graph representation learning.",1
"Generative models of graph structure have applications in biology and social sciences. The state of the art is GraphRNN, which decomposes the graph generation process into a series of sequential steps. While effective for modest sizes, it loses its permutation invariance for larger graphs. Instead, we present a permutation invariant latent-variable generative model relying on graph embeddings to encode structure. Using tools from the random graph literature, our model is highly scalable to large graphs with likelihood evaluation and generation in $O(|V | + |E|)$.",0
"Applications in biology and social sciences utilize generative models of graph structure. The current leading model is GraphRNN, which breaks down the generation process of graphs into a sequence of steps. However, it is less effective for larger graphs as it loses its permutation invariance. In contrast, our model presents a permutation invariant latent-variable generative approach that utilizes graph embeddings to encode structure. Leveraging random graph techniques, our scalable model can evaluate likelihood and generate large graphs in $O(|V | + |E|)$.",1
"Inferring the structural properties of a protein from its amino acid sequence is a challenging yet important problem in biology. Structures are not known for the vast majority of protein sequences, but structure is critical for understanding function. Existing approaches for detecting structural similarity between proteins from sequence are unable to recognize and exploit structural patterns when sequences have diverged too far, limiting our ability to transfer knowledge between structurally related proteins. We newly approach this problem through the lens of representation learning. We introduce a framework that maps any protein sequence to a sequence of vector embeddings --- one per amino acid position --- that encode structural information. We train bidirectional long short-term memory (LSTM) models on protein sequences with a two-part feedback mechanism that incorporates information from (i) global structural similarity between proteins and (ii) pairwise residue contact maps for individual proteins. To enable learning from structural similarity information, we define a novel similarity measure between arbitrary-length sequences of vector embeddings based on a soft symmetric alignment (SSA) between them. Our method is able to learn useful position-specific embeddings despite lacking direct observations of position-level correspondence between sequences. We show empirically that our multi-task framework outperforms other sequence-based methods and even a top-performing structure-based alignment method when predicting structural similarity, our goal. Finally, we demonstrate that our learned embeddings can be transferred to other protein sequence problems, improving the state-of-the-art in transmembrane domain prediction.",0
"Detecting the structural properties of a protein from its amino acid sequence is a challenging task in the field of biology. Although the majority of protein sequences do not have a known structure, understanding the structure is crucial to comprehend its function. The current methods for detecting structural similarity between proteins from their sequences cannot identify and utilize structural patterns when the sequences are too diverse, which hinders our ability to transfer knowledge between structurally related proteins. We have introduced a new approach to tackle this problem using representation learning. Our framework involves mapping any protein sequence to a sequence of vector embeddings that encode structural information for each amino acid position. We have trained bidirectional long short-term memory (LSTM) models on protein sequences using a two-part feedback mechanism that incorporates information from global structural similarity between proteins and pairwise residue contact maps for individual proteins. To enable learning from structural similarity information, we have defined a novel measure of similarity between arbitrary-length sequences of vector embeddings based on a soft symmetric alignment (SSA) between them. Our method can learn useful position-specific embeddings without direct observations of position-level correspondence between sequences. We have demonstrated through experiments that our multi-task framework outperforms other sequence-based methods and even a top-performing structure-based alignment method when predicting structural similarity. We have also shown that our learned embeddings can be transferred to other protein sequence problems, improving the state-of-the-art in transmembrane domain prediction.",1
"Learning useful representations is a key ingredient to the success of modern machine learning. Currently, representation learning mostly relies on embedding data into Euclidean space. However, recent work has shown that data in some domains is better modeled by non-euclidean metric spaces, and inappropriate geometry can result in inferior performance. In this paper, we aim to eliminate the inductive bias imposed by the embedding space geometry. Namely, we propose to map data into more general non-vector metric spaces: a weighted graph with a shortest path distance. By design, such graphs can model arbitrary geometry with a proper configuration of edges and weights. Our main contribution is PRODIGE: a method that learns a weighted graph representation of data end-to-end by gradient descent. Greater generality and fewer model assumptions make PRODIGE more powerful than existing embedding-based approaches. We confirm the superiority of our method via extensive experiments on a wide range of tasks, including classification, compression, and collaborative filtering.",0
"The success of modern machine learning largely depends on acquiring useful representations. Currently, representation learning primarily involves embedding data into Euclidean space. However, recent studies have revealed that non-Euclidean metric spaces may be more suitable for modeling data in certain domains, and the use of inappropriate geometry can lead to subpar performance. The objective of this paper is to eliminate the inductive bias imposed by embedding space geometry by mapping data into more flexible non-vector metric spaces, such as a weighted graph with a shortest path distance. Such graphs can represent any geometry by configuring edges and weights appropriately. Our main contribution is PRODIGE, a method that utilizes gradient descent to learn a weighted graph representation of data end-to-end. PRODIGE is more effective than existing embedding-based approaches due to its greater generality and fewer model assumptions. We demonstrate the superiority of our method through extensive experiments on various tasks, including classification, compression, and collaborative filtering.",1
"Classical supervised classification tasks search for a nonlinear mapping that maps each encoded feature directly to a probability mass over the labels. Such a learning framework typically lacks the intuition that encoded features from the same class tend to be similar and thus has little interpretability for the learned features. In this paper, we propose a novel supervised learning model named Supervised-Encoding Quantizer (SEQ). The SEQ applies a quantizer to cluster and classify the encoded features. We found that the quantizer provides an interpretable graph where each cluster in the graph represents a class of data samples that have a particular style. We also trained a decoder that can decode convex combinations of the encoded features from similar and different clusters and provide guidance on style transfer between sub-classes.",0
"Classical supervised classification tasks aim to find a nonlinear mapping that directly maps encoded features to a probability mass over labels. However, this framework lacks the understanding that encoded features from the same class tend to be similar, which makes the learned features difficult to interpret. To address this, we introduce a new supervised learning model called Supervised-Encoding Quantizer (SEQ). This approach applies a quantizer to cluster and classify the encoded features, resulting in an interpretable graph where each cluster represents a specific class of data samples with a particular style. Additionally, we train a decoder that can decode combinations of encoded features from similar and different clusters, giving guidance on style transfer between sub-classes.",1
"Distributed representations have been used to support downstream tasks in healthcare recently. Healthcare data (e.g., electronic health records) contain multiple modalities of data from heterogeneous sources that can provide complementary information, alongside an added dimension to learning personalized patient representations. To this end, in this paper we propose a novel unsupervised encoder-decoder model, namely Mixed Pooling Multi-View Attention Autoencoder (MPVAA), that generates patient representations encapsulating a holistic view of their medical profile. Specifically, by first learning personalized graph embeddings pertaining to each patient's heterogeneous healthcare data, it then integrates the non-linear relationships among them into a unified representation through multi-view attention mechanism. Additionally, a mixed pooling strategy is incorporated in the encoding step to learn diverse information specific to each data modality. Experiments conducted for multiple tasks demonstrate the effectiveness of the proposed model over the state-of-the-art representation learning methods in healthcare.",0
"Recently, distributed representations have been utilized to aid in healthcare-related downstream tasks. Healthcare data, such as electronic health records, consist of diverse data modalities from various sources that can offer supplementary information, as well as an extra dimension for developing personalized patient representations. In this research, we introduce a new unsupervised encoder-decoder model called the Mixed Pooling Multi-View Attention Autoencoder (MPVAA). This model creates patient representations that provide a comprehensive view of their medical profile. By first acquiring personalized graph embeddings from a patient's heterogeneous healthcare data, the model then integrates non-linear linkages between them into a consolidated representation, utilizing a multi-view attention mechanism. Moreover, a mixed pooling approach is integrated into the encoding step to acquire diverse information that is specific to each data modality. Results from multiple experiments demonstrate the superiority of the proposed model over the current state-of-the-art representation learning methods in healthcare.",1
"Representation learning (RL) plays an important role in extracting proper representations from complex medical data for various analyzing tasks, such as patient grouping, clinical endpoint prediction and medication recommendation. Medical data can be divided into two typical categories, outpatient and inpatient, that have different data characteristics. However, few of existing RL methods are specially designed for inpatients data, which have strong temporal relations and consistent diagnosis. In addition, for unordered medical activity set, existing medical RL methods utilize a simple pooling strategy, which would result in indistinguishable contributions among the activities for learning. In this work, weproposeInpatient2Vec, anovelmodel for learning three kinds of representations for inpatient, including medical activity, hospital day and diagnosis. A multi-layer self-attention mechanism with two training tasks is designed to capture the inpatient data characteristics and process the unordered set. Using a real-world dataset, we demonstrate that the proposed approach outperforms the competitive baselines on semantic similarity measurement and clinical events prediction tasks.",0
"The significance of Representation Learning (RL) in analyzing complex medical data cannot be overstated. RL helps to extract appropriate representations for patient grouping, clinical endpoint prediction, and medication recommendation. Medical data can be categorized as outpatient and inpatient, each with distinct data characteristics. However, current RL techniques are not tailored to inpatient data, which have consistent diagnosis and strong temporal relations. Furthermore, existing RL methods use a simple pooling strategy for unordered medical activity sets, resulting in indistinguishable contributions among activities for learning. To address these issues, we introduce Inpatient2Vec, an innovative model that learns three forms of representations for inpatients: medical activity, hospital day, and diagnosis. Our approach employs a multi-layer self-attention mechanism with two training tasks to capture the distinct characteristics of inpatient data and process unordered sets. We demonstrate the superiority of our model over competitive baselines using a real-world dataset, achieving better results in semantic similarity measurement and clinical event prediction tasks.",1
"Gaussian Processes (GPs) with an appropriate kernel are known to provide accurate predictions and uncertainty estimates even with very small amounts of labeled data. However, GPs are generally unable to learn a good representation that can encode intricate structures in high dimensional data. The representation power of GPs depends heavily on kernel functions used to quantify the similarity between data points. Traditional GP kernels are not very effective at capturing similarity between high dimensional data points, while methods that use deep neural networks to learn a kernel are not sample-efficient. To overcome these drawbacks, we propose deep probabilistic kernels which use a probabilistic neural network to map high-dimensional data to a probability distribution in a low dimensional subspace, and leverage the rich work on kernels between distributions to capture the similarity between these distributions. Experiments on a variety of datasets show that building a GP using this covariance kernel solves the conflicting problems of representation learning and sample efficiency. Our model can be extended beyond GPs to other small-data paradigms such as few-shot classification where we show competitive performance with state-of-the-art models on the mini-Imagenet dataset.",0
"Accurate predictions and uncertainty estimates can be achieved with Gaussian Processes (GPs) when an appropriate kernel is utilized, even when limited labeled data is available. However, GPs have limitations in their ability to learn an effective representation capable of encoding complex structures in high dimensional data. The efficacy of GPs is heavily reliant on the kernel functions that measure similarity between data points. Traditional GP kernels are not effective at capturing similarity between high dimensional data points, and deep neural network-based methods are not sample-efficient. To overcome these limitations, we propose using deep probabilistic kernels, which utilize a probabilistic neural network to map high-dimensional data to a probability distribution in a low dimensional subspace. This approach leverages existing work on kernels between distributions to capture similarity. Our experiments on various datasets demonstrate that this covariance kernel approach resolves the challenges of representation learning and sample efficiency. This model can extend to other small-data paradigms, such as few-shot classification, where we demonstrate competitive performance with state-of-the-art models on the mini-Imagenet dataset.",1
"It is well established that temporal organization is critical to memory, and that the ability to temporally organize information is fundamental to many perceptual, cognitive, and motor processes. While our understanding of how the brain processes the spatial context of memories has advanced considerably, our understanding of their temporal organization lags far behind. In this paper, we propose a new approach for elucidating the neural basis of complex behaviors and temporal organization of memories. More specifically, we focus on neural decoding - the prediction of behavioral or experimental conditions based on observed neural data. In general, this is a challenging classification problem, which is of immense interest in neuroscience. Our goal is to develop a new framework that not only improves the overall accuracy of decoding, but also provides a clear latent representation of the decoding process. To accomplish this, our approach uses a Variational Auto-encoder (VAE) model with a diversity-encouraging prior based on determinantal point processes (DPP) to improve latent representation learning by avoiding redundancy in the latent space. We apply our method to data collected from a novel rat experiment that involves presenting repeated sequences of odors at a single port and testing the rats' ability to identify each odor. We show that our method leads to substantially higher accuracy rate for neural decoding and allows to discover novel biological phenomena by providing a clear latent representation of the decoding process.",0
"The importance of temporal organization in memory is widely recognized, as it plays a fundamental role in various perceptual, cognitive, and motor functions. Although research on the neural processing of spatial context in memory has progressed, understanding of temporal organization remains limited. This paper proposes a novel approach to investigating the neural basis of complex behaviors and temporal organization of memories using neural decoding, which predicts behavioral or experimental conditions from observed neural data. This is a challenging classification problem that is of great interest in neuroscience. The aim is to develop a framework that not only improves decoding accuracy but also provides a clear latent representation of the process. To achieve this, a Variational Auto-encoder (VAE) model is used with a diversity-encouraging prior based on determinantal point processes (DPP) to improve learning and avoid redundancy in the latent space. The method is applied to data from a rat experiment involving repeated sequences of odors at a single port, and it is shown to significantly improve the accuracy of neural decoding and reveal new biological phenomena by providing a clear latent representation of the decoding process.",1
"The problem of identifying geometric structure in heterogeneous, high-dimensional data is a cornerstone of representation learning. While there exists a large body of literature on the embeddability of canonical graphs, such as lattices or trees, the heterogeneity of the relational data typically encountered in practice limits the applicability of these classical methods. In this paper, we propose a combinatorial approach to evaluating embeddability, i.e., to decide whether a data set is best represented in Euclidean, Hyperbolic or Spherical space. Our method analyzes nearest-neighbor structures and local neighborhood growth rates to identify the geometric priors of suitable embedding spaces. For canonical graphs, the algorithm's prediction provably matches classical results. As for large, heterogeneous graphs, we introduce an efficiently computable statistic that approximates the algorithm's decision rule. We validate our method over a range of benchmark data sets and compare with recently published optimization-based embeddability methods.",0
"Representation learning faces the challenge of recognizing geometric structure in heterogeneous, high-dimensional data. Though canonical graphs like lattices or trees have been studied extensively, they are not always useful for real-world relational data due to its diversity. To overcome this problem, we suggest a combinatorial technique to determine embeddability, which determines whether a dataset is most appropriately represented in Euclidean, Hyperbolic, or Spherical space. Our approach examines nearest-neighbor structures and local neighborhood growth rates to determine the geometric priors of suitable embedding spaces. This method agrees with classical outcomes for canonical graphs. For large, heterogeneous graphs, we present a computationally efficient statistic that approximates the algorithm's decision rule. We validate our approach on various benchmark datasets and compare it to recently published optimization-based embeddability methods.",1
"Deep video action recognition models have been highly successful in recent years but require large quantities of manually annotated data, which are expensive and laborious to obtain. In this work, we investigate the generation of synthetic training data for video action recognition, as synthetic data have been successfully used to supervise models for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation, physics models and other components of modern game engines. With this model we generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for ""Procedural Human Action Videos"". PHAV contains a total of 39,982 videos, with more than 1,000 examples for each of 35 action categories. Our video generation approach is not limited to existing motion capture sequences: 14 of these 35 categories are procedurally defined synthetic actions. In addition, each video is represented with 6 different data modalities, including RGB, optical flow and pixel-level semantic labels. These modalities are generated almost simultaneously using the Multiple Render Targets feature of modern GPUs. In order to leverage PHAV, we introduce a deep multi-task (i.e. that considers action classes from multiple datasets) representation learning architecture that is able to simultaneously learn from synthetic and real video datasets, even when their action categories differ. Our experiments on the UCF-101 and HMDB-51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance. Our approach also significantly outperforms video representations produced by fine-tuning state-of-the-art unsupervised generative models of videos.",0
"Although highly successful, deep video action recognition models require a substantial amount of manually annotated data, which can be costly and time-consuming to obtain. This study explores the use of synthetic training data for video action recognition, as synthetic data has been effective in supervising models for various computer vision tasks. The researchers present a parametric generative model of human action videos that utilizes procedural generation, physics models, and components of modern game engines. This model generates a realistic and diverse dataset of human action videos called ""Procedural Human Action Videos"" (PHAV), consisting of 39,982 videos with over 1,000 examples for each of 35 action categories. PHAV is not limited to existing motion capture sequences, as 14 of the 35 categories are procedurally defined synthetic actions. Each video is represented in six different data modalities, including RGB, optical flow, and pixel-level semantic labels, generated almost simultaneously using modern GPUs' Multiple Render Targets feature. The study also introduces a deep multi-task representation learning architecture that can simultaneously learn from synthetic and real video datasets, even when their action categories differ. The experiments conducted on the UCF-101 and HMDB-51 benchmarks indicate that combining the large set of synthetic videos with small real-world datasets can improve recognition performance. Furthermore, the approach outperforms video representations produced by fine-tuning state-of-the-art unsupervised generative models of videos.",1
"With the rising interest in graph representation learning, a variety of approaches have been proposed to effectively capture a graph's properties. While these approaches have improved performance in graph machine learning tasks compared to traditional graph techniques, they are still perceived as techniques with limited insight into the information encoded in these representations. In this work, we explore methods to interpret node embeddings and propose the creation of a robust evaluation framework for comparing graph representation learning algorithms and hyperparameters. We test our methods on graphs with different properties and investigate the relationship between embedding training parameters and the ability of the produced embedding to recover the structure of the original graph in a downstream task.",0
"Due to the growing interest in graph representation learning, various techniques have been suggested to accurately capture a graph's characteristics. Although these techniques have improved graph machine learning performance compared to traditional approaches, they are considered to have limited understanding of the information contained in these representations. This study aims to examine ways to interpret node embeddings and establish a dependable evaluation system for comparing graph representation learning algorithms and hyperparameters. The efficacy of our methods is tested on graphs with diverse properties, and we examine the correlation between embedding training parameters and the embedding's ability to restore the initial graph's structure in a downstream task.",1
"Recent researches use margin theory to analyze the generalization performance for deep neural networks. The main results are based on the spectrally-normalized minimum margin. However, optimizing the minimum margin ignores a mass of information about margin distribution which is crucial to generalization performance. In this paper, we prove a generalization bound dominated by a ratio of the margin standard deviation to the margin mean, where the huge magnitude of spectral norms is reduced. Compared with the spectral norm terms in the existing results, the margin ratio term in our bound is orders of magnitude better in practice. On the other hand, our bound inspires us to optimize the margin ratio. We utilize a convex margin distribution loss function on the deep neural networks to validate our theoretical results. Experiments and visualizations confirm the effectiveness of our approach in terms of performance and representation learning ability.",0
"Margin theory has been used in recent research to analyze the generalization performance of deep neural networks. While previous studies have focused on the spectrally-normalized minimum margin, this approach overlooks important information about the margin distribution, which is crucial for generalization performance. To address this issue, we present a new generalization bound that is dominated by a ratio of the margin standard deviation to the margin mean. This approach reduces the magnitude of spectral norms and is much more effective in practice than existing methods. Our bound also inspires us to optimize the margin ratio, which we achieve by using a convex margin distribution loss function on deep neural networks. Our experimental results and visualizations demonstrate the effectiveness of our approach for performance and representation learning ability.",1
"Automatic representation learning of key entities in electronic health record (EHR) data is a critical step for healthcare informatics that turns heterogeneous medical records into structured and actionable information. Here we propose ME2Vec, an algorithmic framework for learning low-dimensional vectors of the most common entities in EHR: medical services, doctors, and patients. ME2Vec leverages diverse graph embedding techniques to cater for the unique characteristic of each medical entity. Using real-world clinical data, we demonstrate the efficacy of ME2Vec over competitive baselines on disease diagnosis prediction.",0
"The process of automatically learning the key entities in electronic health record (EHR) data is crucial for healthcare informatics as it transforms diverse medical records into structured and useful information. Our proposed algorithmic framework, ME2Vec, is designed for this purpose and generates low-dimensional vectors for the most common entities in EHR, namely medical services, doctors, and patients. By utilizing various graph embedding techniques, ME2Vec can accommodate the distinctive features of each medical entity. We have evaluated ME2Vec's effectiveness using actual clinical data and found that it outperforms other comparable methods in disease diagnosis prediction.",1
"Graph kernels are kernel methods measuring graph similarity and serve as a standard tool for graph classification. However, the use of kernel methods for node classification, which is a related problem to graph representation learning, is still ill-posed and the state-of-the-art methods are heavily based on heuristics. Here, we present a novel theoretical kernel-based framework for node classification that can bridge the gap between these two representation learning problems on graphs. Our approach is motivated by graph kernel methodology but extended to learn the node representations capturing the structural information in a graph. We theoretically show that our formulation is as powerful as any positive semidefinite kernels. To efficiently learn the kernel, we propose a novel mechanism for node feature aggregation and a data-driven similarity metric employed during the training phase. More importantly, our framework is flexible and complementary to other graph-based deep learning models, e.g., Graph Convolutional Networks (GCNs). We empirically evaluate our approach on a number of standard node classification benchmarks, and demonstrate that our model sets the new state of the art.",0
"The use of graph kernels as a standard tool for graph classification is well-established in kernel methods. However, when it comes to node classification, which is closely related to graph representation learning, the state-of-the-art methods are still heavily reliant on heuristics, making it an ill-posed problem. In this study, we introduce a novel kernel-based framework for node classification that bridges the gap between these two representation learning problems on graphs. Our approach is inspired by graph kernel methodology but expanded to learn node representations that capture the structural information in a graph. We demonstrate that our formulation is as powerful as any positive semidefinite kernels by providing theoretical evidence. To learn the kernel efficiently, we suggest a new mechanism for node feature aggregation and a data-driven similarity metric during the training phase. Furthermore, our framework is flexible and complements other graph-based deep learning models, such as Graph Convolutional Networks (GCNs). Our model is evaluated on various standard node classification benchmarks, and it is shown that it outperforms the current state-of-the-art models.",1
"Deep learning generates state-of-the-art semantic segmentation provided that a large number of images together with pixel-wise annotations are available. To alleviate the expensive data collection process, we propose a semi-supervised domain adaptation method for the specific case of images with similar semantic content but different pixel distributions. A network trained with supervision on a past dataset is finetuned on the new dataset to conserve its features maps. The domain adaptation becomes a simple regression between feature maps and does not require annotations on the new dataset. This method reaches performances similar to classic transfer learning on the PASCAL VOC dataset with synthetic transformations.",0
"If a significant amount of images with pixel-wise annotations is accessible, deep learning can produce cutting-edge semantic segmentation. However, collecting such data can be costly. To address this issue, our proposed solution is a semi-supervised domain adaptation technique for images that share similar semantic content but have varying pixel distributions. Our approach involves fine-tuning a network trained with supervision on a previous dataset on the new dataset to retain its feature maps. The domain adaptation process is a straightforward regression between feature maps and does not necessitate annotations on the new dataset. Our approach yields results comparable to classic transfer learning on the PASCAL VOC dataset with synthetic transformations.",1
"Approximating distributions over complicated manifolds, such as natural images, are conceptually attractive. The deep latent variable model, trained using variational autoencoders and generative adversarial networks, is now a key technique for representation learning. However, it is difficult to unify these two models for exact latent-variable inference and parallelize both reconstruction and sampling, partly due to the regularization under the latent variables, to match a simple explicit prior distribution. These approaches are prone to be oversimplified, and can only characterize a few modes of the true distribution. Based on the recently proposed Wasserstein autoencoder (WAE) with a new regularization as an optimal transport. The paper proposes a stacked Wasserstein autoencoder (SWAE) to learn a deep latent variable model. SWAE is a hierarchical model, which relaxes the optimal transport constraints at two stages. At the first stage, the SWAE flexibly learns a representation distribution, i.e., the encoded prior; and at the second stage, the encoded representation distribution is approximated with a latent variable model under the regularization encouraging the latent distribution to match the explicit prior. This model allows us to generate natural textual outputs as well as perform manipulations in the latent space to induce changes in the output space. Both quantitative and qualitative results demonstrate the superior performance of SWAE compared with the state-of-the-art approaches in terms of faithful reconstruction and generation quality.",0
"The idea of approximating distributions over intricate manifolds, like those found in natural images, is appealing. The deep latent variable model, which utilizes variational autoencoders and generative adversarial networks, is a crucial method for representation learning. However, it is challenging to combine these two models to achieve exact latent-variable inference and parallelize both reconstruction and sampling. This is partly due to the difficulty in matching a simple explicit prior distribution with the regularization under the latent variables, resulting in oversimplified approaches that can only characterize a few modes of the true distribution. To address this issue, the paper introduces a stacked Wasserstein autoencoder (SWAE) that builds on the recently proposed Wasserstein autoencoder (WAE) with a new regularization as an optimal transport. The SWAE is a hierarchical model that relaxes the optimal transport constraints at two stages, enabling flexible learning of a representation distribution at the first stage, and approximation of the encoded representation distribution with a latent variable model at the second stage. This approach allows for the generation of natural textual outputs and manipulation of the latent space to induce changes in the output space. Both quantitative and qualitative results demonstrate the superior performance of SWAE compared to state-of-the-art methods in terms of faithful reconstruction and generation quality.",1
"Effective modeling of electronic health records presents many challenges as they contain large amounts of irregularity most of which are due to the varying procedures and diagnosis a patient may have. Despite the recent progress in machine learning, unsupervised learning remains largely at open, especially in the healthcare domain. In this work, we present a two-step unsupervised representation learning scheme to summarize the multi-modal clinical time series consisting of signals and medical codes into a patient status vector. First, an auto-encoder step is used to reduce sparse medical codes and clinical time series into a distributed representation. Subsequently, the concatenation of the distributed representations is further fine-tuned using a forecasting task. We evaluate the usefulness of the representation on two downstream tasks: mortality and readmission. Our proposed method shows improved generalization performance for both short duration ICU visits and long duration ICU visits.",0
"It can be challenging to effectively model electronic health records due to the vast amount of irregularities caused by varying procedures and diagnoses for patients. While machine learning has made progress, unsupervised learning is still largely unexplored in healthcare. This study introduces a two-step unsupervised representation learning method to summarize clinical time series and medical codes into a patient status vector. The first step uses an auto-encoder to reduce sparse data into a distributed representation, which is then fine-tuned using a forecasting task. The resulting representation is evaluated for its usefulness in predicting mortality and readmission, showing improved performance for both short and long ICU visits.",1
"Graph representation learning, aiming to learn low-dimensional representations which capture the geometric dependencies between nodes in the original graph, has gained increasing popularity in a variety of graph analysis tasks, including node classification and link prediction. Existing representation learning methods based on graph neural networks and their variants rely on the aggregation of neighborhood information, which makes it sensitive to noises in the graph. In this paper, we propose Graph Denoising Policy Network (short for GDPNet) to learn robust representations from noisy graph data through reinforcement learning. GDPNet first selects signal neighborhoods for each node, and then aggregates the information from the selected neighborhoods to learn node representations for the down-stream tasks. Specifically, in the signal neighborhood selection phase, GDPNet optimizes the neighborhood for each target node by formulating the process of removing noisy neighborhoods as a Markov decision process and learning a policy with task-specific rewards received from the representation learning phase. In the representation learning phase, GDPNet aggregates features from signal neighbors to generate node representations for down-stream tasks, and provides task-specific rewards to the signal neighbor selection phase. These two phases are jointly trained to select optimal sets of neighbors for target nodes with maximum cumulative task-specific rewards, and to learn robust representations for nodes. Experimental results on node classification task demonstrate the effectiveness of GDNet, outperforming the state-of-the-art graph representation learning methods on several well-studied datasets. Additionally, GDPNet is mathematically equivalent to solving the submodular maximizing problem, which theoretically guarantees the best approximation to the optimal solution with GDPNet.",0
"Learning low-dimensional representations that capture the geometric dependencies between nodes in the original graph, known as graph representation learning, has become increasingly popular in various graph analysis tasks such as node classification and link prediction. However, current representation learning methods based on graph neural networks rely on the aggregation of neighborhood information, making them sensitive to noisy graphs. To address this, we propose the Graph Denoising Policy Network (GDPNet) that uses reinforcement learning to learn robust representations from noisy graph data. GDPNet selects signal neighborhoods for each node and aggregates the information from these neighborhoods to learn node representations for downstream tasks. The signal neighborhood selection phase optimizes the neighborhood for each target node by formulating the process of removing noisy neighborhoods as a Markov decision process and learning a policy with task-specific rewards. In the representation learning phase, GDPNet aggregates features from signal neighbors to generate node representations for downstream tasks and provides task-specific rewards to the signal neighbor selection phase. These two phases are jointly trained to select optimal sets of neighbors for target nodes with maximum cumulative task-specific rewards and to learn robust representations for nodes. Experimental results on node classification show that GDPNet outperforms state-of-the-art graph representation learning methods on several datasets. Furthermore, GDPNet is mathematically equivalent to solving the submodular maximizing problem, which guarantees the best approximation to the optimal solution with GDPNet.",1
"We introduce the Mutual Information Machine (MIM), a novel formulation of representation learning, using a joint distribution over the observations and latent state in an encoder/decoder framework. Our key principles are symmetry and mutual information, where symmetry encourages the encoder and decoder to learn different factorizations of the same underlying distribution, and mutual information, to encourage the learning of useful representations for downstream tasks. Our starting point is the symmetric Jensen-Shannon divergence between the encoding and decoding joint distributions, plus a mutual information encouraging regularizer. We show that this can be bounded by a tractable cross entropy loss function between the true model and a parameterized approximation, and relate this to the maximum likelihood framework. We also relate MIM to variational autoencoders (VAEs) and demonstrate that MIM is capable of learning symmetric factorizations, with high mutual information that avoids posterior collapse.",0
"The Mutual Information Machine (MIM) is a new way of learning representations through a joint distribution over observations and latent state in an encoder/decoder framework. Our approach emphasizes symmetry and mutual information. Symmetry encourages the encoder and decoder to learn different factorizations of the same underlying distribution, while mutual information encourages the learning of useful representations for downstream tasks. To achieve this, we use the symmetric Jensen-Shannon divergence between the encoding and decoding joint distributions, along with a regularizer that encourages mutual information. We show that this can be bounded by a cross entropy loss function between the true model and a parameterized approximation, which is related to the maximum likelihood framework. We also compare MIM to variational autoencoders (VAEs) and demonstrate that MIM can learn symmetric factorizations with high mutual information, avoiding posterior collapse.",1
"Gradient-based meta-learning has proven to be highly effective at learning model initializations, representations, and update rules that allow fast adaptation from a few samples. The core idea behind these approaches is to use fast adaptation and generalization -- two second-order metrics -- as training signals on a meta-training dataset. However, little attention has been given to other possible second-order metrics. In this paper, we investigate a different training signal -- robustness to catastrophic interference -- and demonstrate that representations learned by directing minimizing interference are more conducive to incremental learning than those learned by just maximizing fast adaptation.",0
"The effectiveness of gradient-based meta-learning in acquiring model initializations, representations, and update rules for rapid adaptation from a few samples is well-established. These approaches utilize fast adaptation and generalization as training signals based on second-order metrics. However, there has been little exploration of alternative second-order metrics. This paper examines robustness to catastrophic interference as a different training signal and shows that minimizing interference leads to better representations for incremental learning compared to only maximizing fast adaptation.",1
"Graph autoencoders (AE) and variational autoencoders (VAE) recently emerged as powerful node embedding methods, with promising performances on challenging tasks such as link prediction and node clustering. Graph AE, VAE and most of their extensions rely on graph convolutional networks (GCN) to learn vector space representations of nodes. In this paper, we propose to replace the GCN encoder by a simple linear model w.r.t. the adjacency matrix of the graph. For the two aforementioned tasks, we empirically show that this approach consistently reaches competitive performances w.r.t. GCN-based models for numerous real-world graphs, including the widely used Cora, Citeseer and Pubmed citation networks that became the de facto benchmark datasets for evaluating graph AE and VAE. This result questions the relevance of repeatedly using these three datasets to compare complex graph AE and VAE models. It also emphasizes the effectiveness of simple node encoding schemes for many real-world applications.",0
"Recently, Graph autoencoders (AE) and Variational autoencoders (VAE) have emerged as node embedding techniques that deliver promising performances on difficult tasks like link prediction and node clustering. These techniques, along with their variations, rely on graph convolutional networks (GCN) to learn vector space node representations. However, we propose replacing the GCN encoder with a basic linear model in relation to the graph's adjacency matrix. We empirically demonstrate that this approach consistently achieves competitive performances with GCN-based models for various real-world graphs, including the commonly used Cora, Citeseer, and Pubmed citation networks that serve as standard benchmark datasets for evaluating graph AE and VAE. This finding challenges the validity of solely using these three datasets to compare complex graph AE and VAE models and emphasizes the efficacy of simple node encoding methods for many real-world applications.",1
"All previous methods for audio-driven talking head generation assume the input audio to be clean with a neutral tone. As we show empirically, one can easily break these systems by simply adding certain background noise to the utterance or changing its emotional tone (to such as sad). To make talking head generation robust to such variations, we propose an explicit audio representation learning framework that disentangles audio sequences into various factors such as phonetic content, emotional tone, background noise and others. We conduct experiments to validate that conditioned on disentangled content representation, the generated mouth movement by our model is significantly more accurate than previous approaches (without disentangled learning) in the presence of noise and emotional variations. We further demonstrate that our framework is compatible with current state-of-the-art approaches by replacing their original audio learning component with ours. To our best knowledge, this is the first work which improves the performance of talking head generation from disentangled audio representation perspective, which is important for many real-world applications.",0
"Previous methods for generating talking heads based on audio input assumed that the audio was clean and had a neutral tone. However, our empirical research has shown that these systems can easily be disrupted by adding background noise or altering the emotional tone of the audio. To address this issue, we propose a framework for explicit audio representation learning that disentangles audio sequences into various factors such as phonetic content, emotional tone, and background noise. Through experiments, we have validated that our framework produces mouth movements that are significantly more accurate than previous approaches when faced with noise and emotional variations. We have also demonstrated that our framework is compatible with current state-of-the-art approaches by replacing their original audio learning component with ours. This work represents a significant improvement in talking head generation performance from a disentangled audio representation perspective, which has important implications for real-world applications.",1
"This paper introduces equivariant hamiltonian flows, a method for learning expressive densities that are invariant with respect to a known Lie-algebra of local symmetry transformations while providing an equivariant representation of the data. We provide proof of principle demonstrations of how such flows can be learnt, as well as how the addition of symmetry invariance constraints can improve data efficiency and generalisation. Finally, we make connections to disentangled representation learning and show how this work relates to a recently proposed definition.",0
"The aim of this paper is to present a technique called equivariant Hamiltonian flows, which enables the learning of densities capable of expressing invariance with respect to a known Lie-algebra of local symmetry transformations. Additionally, this method provides an equivariant representation of data. The paper includes demonstrations of how to learn such flows, as well as the benefits of incorporating symmetry invariance constraints in terms of data efficiency and generalisation. Lastly, the paper highlights the relationship between this work and the recently proposed definition of disentangled representation learning.",1
"Network representation learning has exploded recently. However, existing studies usually reconstruct networks as sequences or matrices, which may cause information bias or sparsity problem during model training. Inspired by a cognitive model of human memory, we propose a network representation learning scheme. In this scheme, we learn node embeddings by adjusting the proximity of nodes traversing the spreading structure of the network. Our proposed method shows a significant improvement in multiple analysis tasks based on various real-world networks, ranging from semantic networks to protein interaction networks, international trade networks, human behavior networks, etc. In particular, our model can effectively discover the hierarchical structures in networks. The well-organized model training speeds up the convergence to only a small number of iterations, and the training time is linear with respect to the edge numbers.",0
"Recently, network representation learning has experienced significant growth. However, current research often constructs networks as sequences or matrices, which can result in information bias or sparsity issues during model training. Drawing inspiration from a human memory cognitive model, we propose a new scheme for network representation learning. Our approach involves adjusting node embeddings based on the proximity of nodes traversing the network's spreading structure. Our proposed method has shown to be effective in various real-world networks, including semantic networks, protein interaction networks, international trade networks, and human behavior networks. Our model can also uncover hierarchical structures in networks. The well-organized model training accelerates convergence to only a few iterations, and the training time scales linearly with the number of edges.",1
"Increasing volume of Electronic Health Records (EHR) in recent years provides great opportunities for data scientists to collaborate on different aspects of healthcare research by applying advanced analytics to these EHR clinical data. A key requirement however is obtaining meaningful insights from high dimensional, sparse and complex clinical data. Data science approaches typically address this challenge by performing feature learning in order to build more reliable and informative feature representations from clinical data followed by supervised learning. In this paper, we propose a predictive modeling approach based on deep learning based feature representations and word embedding techniques. Our method uses different deep architectures (stacked sparse autoencoders, deep belief network, adversarial autoencoders and variational autoencoders) for feature representation in higher-level abstraction to obtain effective and robust features from EHRs, and then build prediction models on top of them. Our approach is particularly useful when the unlabeled data is abundant whereas labeled data is scarce. We investigate the performance of representation learning through a supervised learning approach. Our focus is to present a comparative study to evaluate the performance of different deep architectures through supervised learning and provide insights in the choice of deep feature representation techniques. Our experiments demonstrate that for small data sets, stacked sparse autoencoder demonstrates a superior generality performance in prediction due to sparsity regularization whereas variational autoencoders outperform the competing approaches for large data sets due to its capability of learning the representation distribution.",0
"The increasing number of Electronic Health Records (EHR) available presents an opportunity for data scientists to collaborate on healthcare research by applying advanced analytics to these clinical data. However, the complexity of the data requires meaningful insights to be obtained. Data science approaches can overcome this challenge by performing feature learning to create more informative feature representations from clinical data. In this paper, we propose a predictive modeling approach based on deep learning and word embedding techniques. Our method uses different deep architectures for feature representation to obtain effective and robust features from EHRs, particularly when labeled data is scarce. We evaluate the performance of different deep architectures through supervised learning and provide insights into their choice for deep feature representation techniques. Our experiments show superior generality performance for small data sets using stacked sparse autoencoders due to sparsity regularization, while variational autoencoders outperform other approaches for large data sets due to their ability to learn the representation distribution.",1
"The representation used for Facial Expression Recognition (FER) usually contain expression information along with other variations such as identity and illumination. In this paper, we propose a novel Disentangled Expression learning-Generative Adversarial Network (DE-GAN) to explicitly disentangle facial expression representation from identity information. In this learning by reconstruction method, facial expression representation is learned by reconstructing an expression image employing an encoder-decoder based generator. This expression representation is disentangled from identity component by explicitly providing the identity code to the decoder part of DE-GAN. The process of expression image reconstruction and disentangled expression representation learning is improved by performing expression and identity classification in the discriminator of DE-GAN. The disentangled facial expression representation is then used for facial expression recognition employing simple classifiers like SVM or MLP. The experiments are performed on publicly available and widely used face expression databases (CK+, MMI, Oulu-CASIA). The experimental results show that the proposed technique produces comparable results with state-of-the-art methods.",0
"Facial Expression Recognition (FER) representations typically include expression information, as well as identity and illumination variations. This study introduces a new method, called Disentangled Expression learning-Generative Adversarial Network (DE-GAN), which explicitly separates facial expression representation from identity information. The approach involves using an encoder-decoder based generator to learn facial expression representation through image reconstruction. The identity code is then provided to the decoder part of DE-GAN to disentangle the expression representation. The discriminator of DE-GAN performs expression and identity classification to improve the process of expression image reconstruction and disentangled expression representation learning. The resulting disentangled facial expression representation can be used for facial expression recognition with simple classifiers like SVM or MLP. The technique is evaluated on publicly available and widely used face expression databases (CK+, MMI, Oulu-CASIA) and produces results comparable to state-of-the-art methods.",1
"Deep learning models have achieved huge success in numerous fields, such as computer vision and natural language processing. However, unlike such fields, it is hard to apply traditional deep learning models on the graph data due to the 'node-orderless' property. Normally, adjacency matrices will cast an artificial and random node-order on the graphs, which renders the performance of deep models on graph classification tasks extremely erratic, and the representations learned by such models lack clear interpretability. To eliminate the unnecessary node-order constraint, we propose a novel model named Isomorphic Neural Network (IsoNN), which learns the graph representation by extracting its isomorphic features via the graph matching between input graph and templates. IsoNN has two main components: graph isomorphic feature extraction component and classification component. The graph isomorphic feature extraction component utilizes a set of subgraph templates as the kernel variables to learn the possible subgraph patterns existing in the input graph and then computes the isomorphic features. A set of permutation matrices is used in the component to break the node-order brought by the matrix representation. Three fully-connected layers are used as the classification component in IsoNN. Extensive experiments are conducted on benchmark datasets, the experimental results can demonstrate the effectiveness of ISONN, especially compared with both classic and state-of-the-art graph classification methods.",0
"Numerous fields, including computer vision and natural language processing, have seen immense success with deep learning models. However, applying traditional deep learning models to graph data is challenging due to the ""node-orderless"" property. The use of adjacency matrices imposes an artificial and random node-order on graphs, resulting in erratic performance and lack of interpretability. To address this constraint, a new model called Isomorphic Neural Network (IsoNN) has been proposed, which extracts isomorphic features via graph matching between input graphs and templates. IsoNN comprises two main components, namely the graph isomorphic feature extraction component and the classification component. The former uses a set of subgraph templates to learn possible patterns in input graphs and compute isomorphic features. Permutation matrices are used to break the node-order, while the latter utilizes three fully-connected layers. Extensive experiments have been conducted on benchmark datasets, demonstrating the effectiveness of IsoNN in comparison to classic and state-of-the-art graph classification methods.",1
"An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at https://git.io/robust-reps .",0
"The aim of deep learning is to acquire flexible and advanced feature representations of input data. However, traditional networks' representations possess drawbacks that prevent them from accomplishing this objective. The authors of this study demonstrate that robust optimization can be utilized to impose priors on the features acquired by deep neural networks, resulting in representations that overcome the aforementioned flaws and make significant strides towards high-level encoding of inputs. These representations are approximately invertible and allow for the direct visualization and manipulation of crucial input features. The findings suggest that adversarial robustness may be a promising approach to improve learned representations. The code and models for replicating these results are available at https://git.io/robust-reps.",1
"Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and resource-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. We show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. We evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, we show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.",0
"The use of collaborative personalization, such as through embeddings, can greatly enhance the accuracy of neural-network-based models. Our proposed approach, Federated User Representation Learning (FURL), is both simple and scalable, while also preserving privacy and being resource-efficient in the Federated Learning (FL) setting. FURL splits model parameters into federated and private parameters. Local training is used for private parameters, such as private user embeddings, which are not transferred to or averaged on the server, unlike federated parameters. We demonstrate that this parameter split does not affect most model personalization approaches. Storing user embeddings locally improves memory locality and enhances user privacy. Our evaluation of FURL on two datasets shows a significant improvement in model quality with performance increases of 8% and 51%, respectively. Additionally, FURL achieves the same level of performance as centralized training with only 0% and 4% reductions. We also show that user embeddings learned in FL and the centralized setting have a similar structure, indicating that FURL can learn collaboratively while preserving user privacy.",1
"The objective of this paper is self-supervised learning of spatio-temporal embeddings from video, suitable for human action recognition. We make three contributions: First, we introduce the Dense Predictive Coding (DPC) framework for self-supervised representation learning on videos. This learns a dense encoding of spatio-temporal blocks by recurrently predicting future representations; Second, we propose a curriculum training scheme to predict further into the future with progressively less temporal context. This encourages the model to only encode slowly varying spatial-temporal signals, therefore leading to semantic representations; Third, we evaluate the approach by first training the DPC model on the Kinetics-400 dataset with self-supervised learning, and then finetuning the representation on a downstream task, i.e. action recognition. With single stream (RGB only), DPC pretrained representations achieve state-of-the-art self-supervised performance on both UCF101(75.7% top1 acc) and HMDB51(35.7% top1 acc), outperforming all previous learning methods by a significant margin, and approaching the performance of a baseline pre-trained on ImageNet.",0
"In this paper, we aim to achieve self-supervised learning of spatio-temporal embeddings from video, which are suitable for human action recognition. Our contributions are three-fold. Firstly, we introduce the Dense Predictive Coding (DPC) framework, which facilitates self-supervised representation learning on videos by recurrently predicting future representations. This results in a dense encoding of spatio-temporal blocks. Secondly, we propose a curriculum training scheme that encourages the model to only encode slowly varying spatial-temporal signals. This is achieved by predicting further into the future with progressively less temporal context, leading to semantic representations. Thirdly, we evaluate our approach by training the DPC model on the Kinetics-400 dataset with self-supervised learning, and then finetuning the representation on a downstream task, i.e. action recognition. Our results show that DPC pretrained representations achieve state-of-the-art self-supervised performance on both UCF101(75.7% top1 acc) and HMDB51(35.7% top1 acc), outperforming all previous learning methods by a significant margin, and approaching the performance of a baseline pre-trained on ImageNet.",1
"Incompleteness is a common problem for existing knowledge graphs (KGs), and the completion of KG which aims to predict links between entities is challenging. Most existing KG completion methods only consider the direct relation between nodes and ignore the relation paths which contain useful information for link prediction. Recently, a few methods take relation paths into consideration but pay less attention to the order of relations in paths which is important for reasoning. In addition, these path-based models always ignore nonlinear contributions of path features for link prediction. To solve these problems, we propose a novel KG completion method named OPTransE. Instead of embedding both entities of a relation into the same latent space as in previous methods, we project the head entity and the tail entity of each relation into different spaces to guarantee the order of relations in the path. Meanwhile, we adopt a pooling strategy to extract nonlinear and complex features of different paths to further improve the performance of link prediction. Experimental results on two benchmark datasets show that the proposed model OPTransE performs better than state-of-the-art methods.",0
"Existing knowledge graphs (KGs) commonly face incompleteness issues, and predicting links between entities to complete KGs is a challenging task. The majority of current KG completion methods overlook relation paths, which contain valuable information for link prediction, and only consider direct relations between nodes. Although a few methods consider relation paths, they do not prioritize the order of relations in paths, which is critical for reasoning. Furthermore, these path-based models neglect the nonlinear contributions of path features for link prediction. To address these concerns, we propose a new KG completion method called OPTransE. Instead of embedding both entities of a relation into the same latent space, we project the head and tail entities of each relation into different spaces to maintain the order of relations in the path. We also use a pooling strategy to extract nonlinear and complex features of various paths to further enhance the link prediction performance. Experimental results on two benchmark datasets demonstrate that OPTransE performs better than state-of-the-art methods.",1
"Hashing has been widely applied to multimodal retrieval on large-scale multimedia data due to its efficiency in computation and storage. Particularly, deep hashing has received unprecedented research attention in recent years, owing to its perfect retrieval performance. However, most of existing deep hashing methods learn binary hash codes by preserving the similarity relationship while without exploiting the semantic labels of data points, which result in suboptimal binary codes. In this work, we propose a novel Deep Semantic Multimodal Hashing Network for scalable multimodal retrieval. In DSMHN, two sets of modality-specific hash functions are jointly learned by explicitly preserving both the inter-modality similarities and the intra-modality semantic labels. Specifically, with the assumption that the learned hash codes should be optimal for task-specific classification, two stream networks are jointly trained to learn the hash functions by embedding the semantic labels on the resultant hash codes. Different from previous deep hashing methods, which are tied to some particular forms of loss functions, the proposed deep hashing framework can be flexibly integrated with different types of loss functions. In addition, the bit balance property is investigated to generate binary codes with each bit having 50% probability to be 1 or -1. Moreover, a unified deep multimodal hashing framework is proposed to learn compact and high-quality hash codes by exploiting the feature representation learning, inter-modality similarity preserving learning, semantic label preserving learning and hash functions learning with bit balanced constraint simultaneously. We conduct extensive experiments for both unimodal and cross-modal retrieval tasks on three widely-used multimodal retrieval datasets. The experimental result demonstrates that DSMHN significantly outperforms state-of-the-art methods.",0
"Due to its computational and storage efficiency, hashing has been widely used for multimodal retrieval on large-scale multimedia data. Deep hashing, in particular, has gained significant research attention in recent years due to its superior retrieval performance. However, existing deep hashing methods primarily rely on preserving similarity relationships without utilizing the semantic labels of data points, leading to suboptimal binary codes. To address this issue, we propose the Deep Semantic Multimodal Hashing Network (DSMHN) for scalable multimodal retrieval. DSMHN learns two sets of modality-specific hash functions that preserve both inter-modality similarities and intra-modality semantic labels. Two stream networks are jointly trained to learn the hash functions by embedding the semantic labels on the resultant hash codes. Unlike previous deep hashing methods, DSMHN can flexibly integrate with different types of loss functions. Additionally, the bit balance property is investigated to generate binary codes with each bit having a 50% probability to be 1 or -1. We propose a unified deep multimodal hashing framework that learns compact and high-quality hash codes by simultaneously exploiting feature representation learning, inter-modality similarity preserving learning, semantic label preserving learning, and hash functions learning with bit balanced constraint. We evaluate DSMHN on three widely-used multimodal retrieval datasets and demonstrate its superior performance compared to state-of-the-art methods.",1
"The goal of this work is to present a systematic solution for RGB-D salient object detection, which addresses the following three aspects with a unified framework: modal-specific representation learning, complementary cue selection and cross-modal complement fusion. To learn discriminative modal-specific features, we propose a hierarchical cross-modal distillation scheme, in which the well-learned source modality provides supervisory signals to facilitate the learning process for the new modality. To better extract the complementary cues, we formulate a residual function to incorporate complements from the paired modality adaptively. Furthermore, a top-down fusion structure is constructed for sufficient cross-modal interactions and cross-level transmissions. The experimental results demonstrate the effectiveness of the proposed cross-modal distillation scheme in zero-shot saliency detection and pre-training on a new modality, as well as the advantages in selecting and fusing cross-modal/cross-level complements.",0
"The objective of this study is to introduce a comprehensive approach for detecting RGB-D salient objects. This approach deals with three key aspects using a unified framework, namely, modal-specific representation learning, complementary cue selection, and cross-modal complement fusion. To attain distinctive modal-specific features, a hierarchical cross-modal distillation scheme is suggested, where the source modality is used as a supervisory signal to aid in the learning process for the new modality. To extract complementary cues more effectively, a residual function is formulated to include complementary information from the paired modality adaptively. Additionally, a top-down fusion structure is constructed to ensure adequate cross-modal interactions and cross-level transmissions. The study's findings demonstrate the effectiveness of the proposed cross-modal distillation scheme in zero-shot saliency detection and pre-training on a new modality, as well as the advantages of selecting and fusing cross-modal/cross-level complements.",1
"Increasing volume of Electronic Health Records (EHR) in recent years provides great opportunities for data scientists to collaborate on different aspects of healthcare research by applying advanced analytics to these EHR clinical data. A key requirement however is obtaining meaningful insights from high dimensional, sparse and complex clinical data. Data science approaches typically address this challenge by performing feature learning in order to build more reliable and informative feature representations from clinical data followed by supervised learning. In this paper, we propose a predictive modeling approach based on deep learning based feature representations and word embedding techniques. Our method uses different deep architectures (stacked sparse autoencoders, deep belief network, adversarial autoencoders and variational autoencoders) for feature representation in higher-level abstraction to obtain effective and robust features from EHRs, and then build prediction models on top of them. Our approach is particularly useful when the unlabeled data is abundant whereas labeled data is scarce. We investigate the performance of representation learning through a supervised learning approach. Our focus is to present a comparative study to evaluate the performance of different deep architectures through supervised learning and provide insights in the choice of deep feature representation techniques. Our experiments demonstrate that for small data sets, stacked sparse autoencoder demonstrates a superior generality performance in prediction due to sparsity regularization whereas variational autoencoders outperform the competing approaches for large data sets due to its capability of learning the representation distribution",0
"The increase in the number of Electronic Health Records (EHR) has opened up opportunities for collaboration between data scientists and healthcare researchers. However, a challenge is obtaining useful insights from the complex, sparse, and high-dimensional clinical data. To address this, data science approaches use feature learning to create more reliable and informative representations of clinical data, followed by supervised learning. Our paper proposes a predictive modeling approach that uses deep learning and word embedding techniques to create effective and robust features from EHRs. We use various deep architectures to obtain higher-level abstractions of EHRs, and then build prediction models. Our approach is particularly useful when there is abundant unlabeled data and scarce labeled data. We evaluate the performance of different deep architectures through supervised learning and our experiments show that stacked sparse autoencoder is more effective for small data sets, while variational autoencoders perform better for large data sets due to their ability to learn the representation distribution.",1
"Scene graphs have become an important form of structured knowledge for tasks such as for image generation, visual relation detection, visual question answering, and image retrieval. While visualizing and interpreting word embeddings is well understood, scene graph embeddings have not been fully explored. In this work, we train scene graph embeddings in a layout generation task with different forms of supervision, specifically introducing triplet super-vision and data augmentation. We see a significant performance increase in both metrics that measure the goodness of layout prediction, mean intersection-over-union (mIoU)(52.3% vs. 49.2%) and relation score (61.7% vs. 54.1%),after the addition of triplet supervision and data augmentation. To understand how these different methods affect the scene graph representation, we apply several new visualization and evaluation methods to explore the evolution of the scene graph embedding. We find that triplet supervision significantly improves the embedding separability, which is highly correlated with the performance of the layout prediction model.",0
"Structured knowledge in the form of scene graphs has become important for various tasks such as image generation, visual relation detection, visual question answering, and image retrieval. However, while the visualization and interpretation of word embeddings is well-understood, scene graph embeddings have not been fully explored. In this study, we enhance the performance of scene graph embeddings in a layout generation task by introducing triplet supervision and data augmentation. Our results show a significant increase in performance metrics such as mean intersection-over-union (mIoU) and relation score. We also apply new visualization and evaluation methods to understand how these methods affect the scene graph representation and find that triplet supervision improves the embedding separability, which correlates to the performance of the layout prediction model.",1
"Multimodal datasets contain an enormous amount of relational information, which grows exponentially with the introduction of new modalities. Learning representations in such a scenario is inherently complex due to the presence of multiple heterogeneous information channels. These channels can encode both (a) inter-relations between the items of different modalities and (b) intra-relations between the items of the same modality. Encoding multimedia items into a continuous low-dimensional semantic space such that both types of relations are captured and preserved is extremely challenging, especially if the goal is a unified end-to-end learning framework. The two key challenges that need to be addressed are: 1) the framework must be able to merge complex intra and inter relations without losing any valuable information and 2) the learning model should be invariant to the addition of new and potentially very different modalities. In this paper, we propose a flexible framework which can scale to data streams from many modalities. To that end we introduce a hypergraph-based model for data representation and deploy Graph Convolutional Networks to fuse relational information within and across modalities. Our approach provides an efficient solution for distributing otherwise extremely computationally expensive or even unfeasible training processes across multiple-GPUs, without any sacrifices in accuracy. Moreover, adding new modalities to our model requires only an additional GPU unit keeping the computational time unchanged, which brings representation learning to truly multimodal datasets. We demonstrate the feasibility of our approach in the experiments on multimedia datasets featuring second, third and fourth order relations.",0
"Multimodal datasets contain vast amounts of relational information that increases exponentially with the addition of new modalities, making it challenging to learn representations due to the presence of multiple heterogeneous information channels that encode both inter and intra-relations. Encoding multimedia items into a low-dimensional semantic space that captures and preserves both types of relations is exceptionally difficult, especially if the goal is a unified end-to-end learning framework. The framework must be able to merge complex intra and inter relations without losing valuable information, and the learning model should be invariant to the addition of new modalities. In this paper, we present a flexible framework that can handle data streams from many modalities, using a hypergraph-based model for data representation and Graph Convolutional Networks to fuse relational information within and across modalities. Our approach provides an efficient solution for distributing computationally expensive training processes across multiple-GPUs without sacrificing accuracy. Additionally, adding new modalities only requires an additional GPU unit, keeping the computational time unchanged, making representation learning possible for truly multimodal datasets. Our experiments demonstrate the feasibility of our approach on multimedia datasets with second, third, and fourth order relations.",1
"Information in electronic health records (EHR), such as clinical narratives, examination reports, lab measurements, demographics, and other patient encounter entries, can be transformed into appropriate data representations that can be used for downstream clinical machine learning tasks using representation learning. Learning better representations is critical to improve the performance of downstream tasks. Due to the advances in machine learning, we now can learn better and meaningful representations from EHR through disentangling the underlying factors inside data and distilling large amounts of information and knowledge from heterogeneous EHR sources. In this chapter, we first introduce the background of learning representations and reasons why we need good EHR representations in machine learning for medicine and healthcare in Section 1. Next, we explain the commonly-used machine learning and evaluation methods for representation learning using a deep learning approach in Section 2. Following that, we review recent related studies of learning patient state representation from EHR for clinical machine learning tasks in Section 3. Finally, in Section 4 we discuss more techniques, studies, and challenges for learning natural language representations when free texts, such as clinical notes, examination reports, or biomedical literature are used. We also discuss challenges and opportunities in these rapidly growing research fields.",0
"Transformation of information from electronic health records (EHR) includes clinical narratives, examination reports, lab measurements, demographics, and other patient encounter entries. These can be converted into appropriate data representations using representation learning, which is critical for improving the performance of downstream clinical machine learning tasks. Disentangling the underlying factors in data and distilling knowledge from heterogeneous EHR sources allows us to learn better and more meaningful representations from EHR. This chapter presents the background and importance of learning representations for machine learning in medicine and healthcare, followed by an explanation of commonly-used machine learning and evaluation methods for representation learning. Recent studies on learning patient state representation from EHR for clinical machine learning tasks are also reviewed. Additionally, techniques, studies, and challenges for learning natural language representations from free texts, such as clinical notes, examination reports, or biomedical literature, are discussed in Section 4, along with opportunities and challenges in these rapidly growing research fields.",1
"Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, we demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. We open-source the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data.",0
"Deep neural networks (DNNs) have become the primary tool for machine learning in a variety of fields, such as vision, NLP, and speech. However, their effectiveness in handling heterogeneous tabular data compared to shallow models remains questionable. Gradient boosting decision trees (GBDT) are often the preferred method for tabular problems, and there is insufficient evidence that deep learning methods outperform them. This paper introduces Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture that can handle any tabular data. The NODE architecture is a generalization of ensembles of oblivious decision trees, benefiting from both end-to-end gradient-based optimization and multi-layer hierarchical representation learning. Through extensive experimental comparisons with leading GBDT packages on various tabular datasets, we demonstrate the superiority of the NODE architecture, which outperforms competitors on most tasks. We have made the PyTorch implementation of NODE open source and believe it will become a universal framework for machine learning on tabular data.",1
"Finding a generally accepted formal definition of a disentangled representation in the context of an agent behaving in an environment is an important challenge towards the construction of data-efficient autonomous agents. Higgins et al. recently proposed Symmetry-Based Disentangled Representation Learning, a definition based on a characterization of symmetries in the environment using group theory. We build on their work and make observations, theoretical and empirical, that lead us to argue that Symmetry-Based Disentangled Representation Learning cannot only be based on static observations: agents should interact with the environment to discover its symmetries. Our experiments can be reproduced in Colab and the code is available on GitHub.",0
"Constructing data-efficient autonomous agents requires addressing the challenge of establishing a formal definition of a disentangled representation that is widely accepted in the context of an agent's behavior in an environment. Recently, Higgins et al. proposed a definition called Symmetry-Based Disentangled Representation Learning, which is based on using group theory to characterize symmetries in the environment. We build upon their work and present theoretical and empirical observations that suggest Symmetry-Based Disentangled Representation Learning should not rely solely on static observations, but rather require agents to interact with the environment to discover its symmetries. We conducted experiments that can be replicated in Colab, and the code is available on GitHub.",1
"To learn disentangled representations of facial images, we present a Dual Encoder-Decoder based Generative Adversarial Network (DED-GAN). In the proposed method, both the generator and discriminator are designed with deep encoder-decoder architectures as their backbones. To be more specific, the encoder-decoder structured generator is used to learn a pose disentangled face representation, and the encoder-decoder structured discriminator is tasked to perform real/fake classification, face reconstruction, determining identity and estimating face pose. We further improve the proposed network architecture by minimising the additional pixel-wise loss defined by the Wasserstein distance at the output of the discriminator so that the adversarial framework can be better trained. Additionally, we consider face pose variation to be continuous, rather than discrete in existing literature, to inject richer pose information into our model. The pose estimation task is formulated as a regression problem, which helps to disentangle identity information from pose variations. The proposed network is evaluated on the tasks of pose-invariant face recognition (PIFR) and face synthesis across poses. An extensive quantitative and qualitative evaluation carried out on several controlled and in-the-wild benchmarking datasets demonstrates the superiority of the proposed DED-GAN method over the state-of-the-art approaches.",0
"We introduce a Dual Encoder-Decoder based Generative Adversarial Network (DED-GAN) for obtaining disentangled representations of facial images. This involves using deep encoder-decoder structures for both the generator and discriminator components of the network. The generator learns a pose disentangled face representation, while the discriminator performs real/fake classification, face reconstruction, identity determination, and face pose estimation using an encoder-decoder architecture. We further enhance the network by incorporating a pixel-wise loss defined by the Wasserstein distance at the output of the discriminator. Additionally, we consider face pose variation to be continuous, and use regression to disentangle identity information from pose variations. The proposed method is evaluated on pose-invariant face recognition (PIFR) and face synthesis across poses, and is shown to outperform existing state-of-the-art approaches through comprehensive quantitative and qualitative evaluations on various benchmarking datasets.",1
"Systems that can associate images with their spoken audio captions are an important step towards visually grounded language learning. We describe a scalable method to automatically generate diverse audio for image captioning datasets. This supports pretraining deep networks for encoding both audio and images, which we do via a dual encoder that learns to align latent representations from both modalities. We show that a masked margin softmax loss for such models is superior to the standard triplet loss. We fine-tune these models on the Flickr8k Audio Captions Corpus and obtain state-of-the-art results---improving recall in the top 10 from 29.6% to 49.5%. We also obtain human ratings on retrieval outputs to better assess the impact of incidentally matching image-caption pairs that were not associated in the data, finding that automatic evaluation substantially underestimates the quality of the retrieved results.",0
"Developing systems that can link spoken audio captions with images is a crucial step in advancing language learning based on visual context. We present a method that can generate varied audio for datasets used in image captioning, which can be scaled up efficiently. This method facilitates the pretraining of deep neural networks that can encode both audio and images using a dual encoder, which aligns latent representations from both modalities. We demonstrate that a masked margin softmax loss is more effective than the standard triplet loss for these models. We fine-tuned these models using the Flickr8k Audio Captions Corpus and achieved state-of-the-art results, increasing the recall in the top 10 from 29.6% to 49.5%. We also conducted human evaluations of the retrieval outputs to better understand the impact of incidental matches between image-caption pairs that were not originally associated in the data. Our findings suggest that the quality of the retrieved results is underestimated by automatic evaluation methods.",1
"Cross-domain sentiment analysis is currently a hot topic in the research and engineering areas. One of the most popular frameworks in this field is the domain-invariant representation learning (DIRL) paradigm, which aims to learn a distribution-invariant feature representation across domains. However, in this work, we find out that applying DIRL may harm domain adaptation when the label distribution $\rm{P}(\rm{Y})$ changes across domains. To address this problem, we propose a modification to DIRL, obtaining a novel weighted domain-invariant representation learning (WDIRL) framework. We show that it is easy to transfer existing SOTA DIRL models to WDIRL. Empirical studies on extensive cross-domain sentiment analysis tasks verified our statements and showed the effectiveness of our proposed solution.",0
"The current focus in research and engineering is on cross-domain sentiment analysis. The domain-invariant representation learning (DIRL) paradigm is widely used in this field to learn a distribution-invariant feature representation across domains. However, our research demonstrates that applying DIRL can hinder domain adaptation when the label distribution changes across domains. To solve this issue, we introduce the weighted domain-invariant representation learning (WDIRL) framework, which is a modification of DIRL. Our proposed solution is easy to implement as it can transfer existing DIRL models to WDIRL. We conducted empirical studies on various cross-domain sentiment analysis tasks, which confirmed the effectiveness of our approach.",1
"Metadata are general characteristics of the data in a well-curated and condensed format, and have been proven to be useful for decision making, knowledge discovery, and also heterogeneous data organization of biobank. Among all data types in the biobank, pathology is the key component of the biobank and also serves as the gold standard of diagnosis. To maximize the utility of biobank and allow the rapid progress of biomedical science, it is essential to organize the data with well-populated pathology metadata. However, manual annotation of such information is tedious and time-consuming. In the study, we develop a multimodal multitask learning framework to predict four major slide-level metadata of pathology images. The framework learns generalizable representations across tissue slides, pathology reports, and case-level structured data. We demonstrate improved performance across all four tasks with the proposed method compared to a single modal single task baseline on two test sets, one external test set from a distinct data source (TCGA) and one internal held-out test set (TTH). In the test sets, the performance improvements on the averaged area under receiver operating characteristic curve across the four tasks are 16.48% and 9.05% on TCGA and TTH, respectively. Such pathology metadata prediction system may be adopted to mitigate the effort of expert annotation and ultimately accelerate the data-driven research by better utilization of the pathology biobank.",0
"Metadata is a condensed and curated form of data that is useful for decision-making, knowledge discovery, and organizing heterogeneous data in biobanks. Pathology is the most significant data type in biobanks and serves as the gold standard for diagnosis. To optimize the use of biobanks and accelerate biomedical research, organizing data with well-populated pathology metadata is crucial. However, manual annotation of this information is time-consuming and tedious. In this study, we introduce a multimodal multitask learning framework that predicts four major slide-level metadata of pathology images. The framework generates generalizable representations across tissue slides, pathology reports, and case-level structured data. Our proposed method demonstrates improved performance on two test sets, an external test set from a distinct data source (TCGA) and an internal held-out test set (TTH), compared to a single modal single task baseline. The performance improvements for the averaged area under the receiver operating characteristic curve across the four tasks are 16.48% and 9.05% on TCGA and TTH, respectively. A pathology metadata prediction system could be implemented to reduce the effort of expert annotation and ultimately enhance data-driven research by better utilizing the pathology biobank.",1
"Aiming towards human-level generalization, there is a need to explore adaptable representation learning methods with greater transferability. Most existing approaches independently address task-transferability and cross-domain adaptation, resulting in limited generalization. In this paper, we propose UM-Adapt - a unified framework to effectively perform unsupervised domain adaptation for spatially-structured prediction tasks, simultaneously maintaining a balanced performance across individual tasks in a multi-task setting. To realize this, we propose two novel regularization strategies; a) Contour-based content regularization (CCR) and b) exploitation of inter-task coherency using a cross-task distillation module. Furthermore, avoiding a conventional ad-hoc domain discriminator, we re-utilize the cross-task distillation loss as output of an energy function to adversarially minimize the input domain discrepancy. Through extensive experiments, we demonstrate superior generalizability of the learned representations simultaneously for multiple tasks under domain-shifts from synthetic to natural environments. UM-Adapt yields state-of-the-art transfer learning results on ImageNet classification and comparable performance on PASCAL VOC 2007 detection task, even with a smaller backbone-net. Moreover, the resulting semi-supervised framework outperforms the current fully-supervised multi-task learning state-of-the-art on both NYUD and Cityscapes dataset.",0
"To achieve human-level generalization, it is necessary to explore adaptable representation learning methods that offer greater transferability. Many current approaches focus solely on task-transferability and cross-domain adaptation, resulting in limited generalization. In this paper, we introduce UM-Adapt - a unified framework that effectively performs unsupervised domain adaptation for spatially-structured prediction tasks, while maintaining a balanced performance across individual tasks in a multi-task setting. To achieve this, we propose two innovative regularization strategies: Contour-based content regularization (CCR) and exploitation of inter-task coherency using a cross-task distillation module. Additionally, we avoid a conventional ad-hoc domain discriminator by re-utilizing the cross-task distillation loss as the output of an energy function to adversarially minimize input domain discrepancy. Through extensive experiments, we demonstrate the superior generalizability of the learned representations for multiple tasks under domain-shifts, from synthetic to natural environments. UM-Adapt produces state-of-the-art transfer learning results on ImageNet classification and comparable performance on the PASCAL VOC 2007 detection task, even with a smaller backbone-net. Furthermore, the resulting semi-supervised framework outperforms the current fully-supervised multi-task learning state-of-the-art on both the NYUD and Cityscapes datasets.",1
"Typically, a medical image offers spatial information on the anatomy (and pathology) modulated by imaging specific characteristics. Many imaging modalities including Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) can be interpreted in this way. We can venture further and consider that a medical image naturally factors into some spatial factors depicting anatomy and factors that denote the imaging characteristics. Here, we explicitly learn this decomposed (disentangled) representation of imaging data, focusing in particular on cardiac images. We propose Spatial Decomposition Network (SDNet), which factorises 2D medical images into spatial anatomical factors and non-spatial modality factors. We demonstrate that this high-level representation is ideally suited for several medical image analysis tasks, such as semi-supervised segmentation, multi-task segmentation and regression, and image-to-image synthesis. Specifically, we show that our model can match the performance of fully supervised segmentation models, using only a fraction of the labelled images. Critically, we show that our factorised representation also benefits from supervision obtained either when we use auxiliary tasks to train the model in a multi-task setting (e.g. regressing to known cardiac indices), or when aggregating multimodal data from different sources (e.g. pooling together MRI and CT data). To explore the properties of the learned factorisation, we perform latent-space arithmetic and show that we can synthesise CT from MR and vice versa, by swapping the modality factors. We also demonstrate that the factor holding image specific information can be used to predict the input modality with high accuracy. Code will be made available at https://github.com/agis85/anatomy_modality_decomposition.",0
"Medical images typically provide information about the spatial aspects of anatomy and pathology, which are influenced by specific imaging characteristics. Various imaging methods, such as MRI and CT, can be interpreted in this manner. Our study focuses on cardiac images and aims to decompose medical images into spatial anatomical factors and non-spatial modality factors using the Spatial Decomposition Network (SDNet). This disentangled representation of imaging data is ideal for medical image analysis tasks, including semi-supervised segmentation, multi-task segmentation and regression, and image-to-image synthesis. Our results demonstrate that our model can achieve the performance of fully supervised segmentation models using only a fraction of the labelled images. Additionally, we show that our factorised representation benefits from supervision obtained either through auxiliary tasks or by pooling together multimodal data from different sources. We also show that we can synthesise CT from MR by swapping the modality factors and predict the input modality with high accuracy using the image-specific factor. The code for our study is available at https://github.com/agis85/anatomy_modality_decomposition.",1
"This paper proposes a new deep-learning method to construct test statistics by computer vision and metrics learning. The application highlighted in this paper is applying computer vision on Q-Q plot to construct a new test statistic for normality test. To the best of our knowledge, there is no similar application documented in the literature. Traditionally, there are two families of approaches for verifying the probability distribution of a random variable. Researchers either subjectively assess the Q-Q plot or objectively use a mathematical formula, such as Kolmogorov-Smirnov test, to formally conduct a normality test. Graphical assessment by human beings is not rigorous whereas normality test statistics may not be accurate enough when the uniformly most powerful test does not exist. It may take tens of years for statistician to develop a new test statistic that is more powerful statistically. Our proposed method integrates four components based on deep learning: an image representation learning component of a Q-Q plot, a dimension reduction component, a metrics learning component that best quantifies the differences between two Q-Q plots for normality test, and a new normality hypothesis testing process. Our experimentation results show that the machine-learning-based test statistics can outperform several widely-used traditional normality tests. This study provides convincing evidence that the proposed method could objectively create a powerful test statistic based on Q-Q plots and this method could be modified to construct many more powerful test statistics for other applications in the future.",0
"In this paper, a novel approach utilizing deep learning is presented for creating test statistics through computer vision and metrics learning. Specifically, the application focused on in this paper involves implementing computer vision on Q-Q plots to generate a new test statistic for normality testing, which has not been previously reported in literature. Typically, there are two methods for verifying the probability distribution of a random variable: graphical assessment by humans, which lacks rigor, and mathematical formulas, such as the Kolmogorov-Smirnov test, which may not be accurate enough if the uniformly most powerful test does not exist and can take many years to develop. Our proposed method comprises four deep learning components: an image representation learning component for Q-Q plots, a dimension reduction component, a metrics learning component that quantifies differences between two Q-Q plots for normality testing, and a new normality hypothesis testing process. Our experimental results demonstrate that the machine-learning-based test statistics outperform several traditional normality tests. This study provides evidence that our proposed method could create powerful test statistics using Q-Q plots and can be adapted for other applications in the future.",1
"Dashboard cameras capture a tremendous amount of driving scene video each day. These videos are purposefully coupled with vehicle sensing data, such as from the speedometer and inertial sensors, providing an additional sensing modality for free. In this work, we leverage the large-scale unlabeled yet naturally paired data for visual representation learning in the driving scenario. A representation is learned in an end-to-end self-supervised framework for predicting dense optical flow from a single frame with paired sensing data. We postulate that success on this task requires the network to learn semantic and geometric knowledge in the ego-centric view. For example, forecasting a future view to be seen from a moving vehicle requires an understanding of scene depth, scale, and movement of objects. We demonstrate that our learned representation can benefit other tasks that require detailed scene understanding and outperforms competing unsupervised representations on semantic segmentation.",0
"Every day, dashboard cameras capture an enormous amount of driving scene video, which is intentionally paired with vehicle sensing data from speedometers and inertial sensors to provide an additional sensing modality at no extra cost. In this study, we take advantage of this large-scale, unlabeled, yet naturally paired data for visual representation learning in driving scenarios. We use an end-to-end self-supervised framework to learn a representation for predicting dense optical flow from a single frame paired with sensing data. Our hypothesis is that the network must acquire semantic and geometric knowledge in the ego-centric perspective to succeed in this task. For instance, forecasting a future view from a moving vehicle necessitates an understanding of scene depth, scale, and object movement. We demonstrate that our learned representation can benefit other tasks that require detailed scene comprehension and outperforms competing unsupervised representations in semantic segmentation.",1
"Pedestrian detection in crowded scenes is a challenging problem, because occlusion happens frequently among different pedestrians. In this paper, we propose an effective and efficient detection network to hunt pedestrians in crowd scenes. The proposed method, namely PedHunter, introduces strong occlusion handling ability to existing region-based detection networks without bringing extra computations in the inference stage. Specifically, we design a mask-guided module to leverage the head information to enhance the feature representation learning of the backbone network. Moreover, we develop a strict classification criterion by improving the quality of positive samples during training to eliminate common false positives of pedestrian detection in crowded scenes. Besides, we present an occlusion-simulated data augmentation to enrich the pattern and quantity of occlusion samples to improve the occlusion robustness. As a consequent, we achieve state-of-the-art results on three pedestrian detection datasets including CityPersons, Caltech-USA and CrowdHuman. To facilitate further studies on the occluded pedestrian detection in surveillance scenes, we release a new pedestrian dataset, called SUR-PED, with a total of over 162k high-quality manually labeled instances in 10k images. The proposed dataset, source codes and trained models will be released.",0
"Detecting pedestrians in crowded scenes is a complex task due to frequent occlusion between individuals. In this study, we introduce PedHunter, an efficient and effective detection network for identifying pedestrians in crowded environments. PedHunter enhances existing region-based detection networks by incorporating a mask-guided module that utilizes head information to improve feature representation learning without adding extra computations during inference. We also improve the quality of positive samples during training to eliminate common false positives. Additionally, we use occlusion-simulated data augmentation to enrich the quantity and pattern of occlusion samples, thereby improving occlusion robustness. Results from CityPersons, Caltech-USA, and CrowdHuman datasets show that PedHunter achieves state-of-the-art performance. To support further research on occluded pedestrian detection, we release the SUR-PED dataset, containing over 162k manually labeled instances in 10k images, as well as source codes and trained models.",1
"In recent years, representation learning approaches have disrupted many multimedia computing tasks. Among those approaches, deep convolutional neural networks (CNNs) have notably reached human level expertise on some constrained image classification tasks. Nonetheless, training CNNs from scratch for new task or simply new data turns out to be complex and time-consuming. Recently, transfer learning has emerged as an effective methodology for adapting pre-trained CNNs to new data and classes, by only retraining the last classification layer. This paper focuses on improving this process, in order to better transfer knowledge between CNN architectures for faster trainings in the case of fine tuning for image classification. This is achieved by combining and transfering supplementary weights, based on similarity considerations between source and target classes. The study includes a comparison between semantic and content-based similarities, and highlights increased initial performances and training speed, along with superior long term performances when limited training samples are available.",0
"Representation learning methods have revolutionized various multimedia computing tasks in recent times. Deep convolutional neural networks (CNNs) have achieved a commendable level of proficiency in some specific image classification tasks, comparable to humans. However, training CNNs from scratch for new tasks or data is a challenging and time-consuming process. Transfer learning has emerged as a feasible approach to adapt pre-trained CNNs to new data and classes by retraining only the final classification layer. This paper aims to enhance this process for faster training in fine-tuning for image classification by transferring supplementary weights between CNN architectures. This involves considering similarity factors between source and target classes, which can be either semantic or content-based. The study reports improved initial performances and faster training speeds, along with superior long-term performances, particularly when the training samples are limited.",1
"Efficient representation of patients is very important in the healthcare domain and can help with many tasks such as medical risk prediction. Many existing methods, such as diagnostic Cost Groups (DCG), rely on expert knowledge to build patient representation from medical data, which is resource consuming and non-scalable. Unsupervised machine learning algorithms are a good choice for automating the representation learning process. However, there is very little research focusing on onpatient-level representation learning directly from medical claims. In this paper, weproposed a novel patient vector learning architecture that learns high quality,fixed-length patient representation from claims data. We conducted several experiments to test the quality of our learned representation, and the empirical results show that our learned patient vectors are superior to vectors learned through other methods including a popular commercial model. Lastly, we provide potential clinical interpretation for using our representation on predictive tasks, as interpretability is vital in the healthcare domain",0
"Efficiently representing patients is crucial in healthcare and can aid in medical risk prediction. Traditional methods, like Diagnostic Cost Groups (DCG), rely on expert knowledge to construct patient representation from medical data, which is resource-intensive and non-scalable. Utilizing unsupervised machine learning algorithms can automate the representation learning process. However, there is limited research on patient-level representation learning from medical claims. In this study, we propose a novel architecture for patient vector learning that generates high-quality, fixed-length patient representation from claims data. We conducted experiments to evaluate the quality of our learned representation and found that our patient vectors outperformed those generated from other methods, including a popular commercial model. We also provide potential clinical interpretation for using our representation in predictive tasks, as interpretability is crucial in the healthcare domain.",1
"Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.",0
"Utilizing the vast amount of unlabeled data present on platforms such as YouTube, self-supervised learning has gained significant importance. Most existing methods concentrate on learning low-level features. Our proposal, on the other hand, is a joint visual-linguistic model that learns high-level features without explicit supervision. We build on the BERT model's recent success in language modeling to learn bidirectional joint distributions over sequences of visual and linguistic tokens. These tokens are derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. The VideoBERT model is used in multiple tasks, such as action classification and video captioning. Our study shows that it can be directly applied to open-vocabulary classification. We confirm that a large amount of training data and cross-modal information is critical to performance. Moreover, we surpass the existing state-of-the-art in video captioning, and our quantitative outcomes confirm that the model learns high-level semantic features.",1
"We study on weakly-supervised object detection (WSOD) which plays a vital role in relieving human involvement from object-level annotations. Predominant works integrate region proposal mechanisms with convolutional neural networks (CNN). Although CNN is proficient in extracting discriminative local features, grand challenges still exist to measure the likelihood of a bounding box containing a complete object (i.e., ""objectness""). In this paper, we propose a novel WSOD framework with Objectness Distillation (i.e., WSOD^2) by designing a tailored training mechanism for weakly-supervised object detection. Multiple regression targets are specifically determined by jointly considering bottom-up (BU) and top-down (TD) objectness from low-level measurement and CNN confidences with an adaptive linear combination. As bounding box regression can facilitate a region proposal learning to approach its regression target with high objectness during training, deep objectness representation learned from bottom-up evidences can be gradually distilled into CNN by optimization. We explore different adaptive training curves for BU/TD objectness, and show that the proposed WSOD^2 can achieve state-of-the-art results.",0
"Our focus is on studying weakly-supervised object detection (WSOD) as a means of reducing human involvement in object-level annotations. Previous research has integrated region proposal mechanisms with convolutional neural networks (CNN) to extract discriminative local features. However, accurately measuring the likelihood of a bounding box containing a complete object (i.e., ""objectness"") remains a challenge. To address this issue, we introduce a novel WSOD framework with Objectness Distillation (WSOD^2) that utilizes a tailored training mechanism for weakly-supervised object detection. We determine multiple regression targets by jointly considering bottom-up (BU) and top-down (TD) objectness from low-level measurement and CNN confidences with an adaptive linear combination. During training, bounding box regression facilitates region proposal learning to approach its regression target with high objectness. As a result, deep objectness representation learned from bottom-up evidence is gradually distilled into CNN through optimization. We experiment with different adaptive training curves for BU/TD objectness and demonstrate that our proposed WSOD^2 achieves state-of-the-art results.",1
"We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g.\ phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the VQ-VAE, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.",0
"Our objective is to use autoencoding neural networks to extract meaningful latent representations of speech without supervision. The aim is to learn a representation that can capture high-level semantic content such as phoneme identities while ignoring low-level details like background noise and pitch contour. Since our focus is solely on phonetic content, we use a high-capacity WaveNet decoder to infer information that is discarded by the encoder from previous samples. We compare three variants of the autoencoder models, namely a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE (VQ-VAE), as their behavior depends on the kind of constraint applied to the latent representation. We evaluate the quality of learned representations based on speaker independence, capability to predict phonetic content, and the accuracy of reconstructing individual spectrogram frames. For the VQ-VAE, we measure the ease of mapping the discrete encodings to phonemes. We introduce a regularization scheme to ensure that the representations focus on phonetic content and report results that are comparable to the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.",1
"Graph neural networks (GNN) has been successfully applied to operate on the graph-structured data. Given a specific scenario, rich human expertise and tremendous laborious trials are usually required to identify a suitable GNN architecture. It is because the performance of a GNN architecture is significantly affected by the choice of graph convolution components, such as aggregate function and hidden dimension. Neural architecture search (NAS) has shown its potential in discovering effective deep architectures for learning tasks in image and language modeling. However, existing NAS algorithms cannot be directly applied to the GNN search problem. First, the search space of GNN is different from the ones in existing NAS work. Second, the representation learning capacity of GNN architecture changes obviously with slight architecture modifications. It affects the search efficiency of traditional search methods. Third, widely used techniques in NAS such as parameter sharing might become unstable in GNN.   To bridge the gap, we propose the automated graph neural networks (AGNN) framework, which aims to find an optimal GNN architecture within a predefined search space. A reinforcement learning based controller is designed to greedily validate architectures via small steps. AGNN has a novel parameter sharing strategy that enables homogeneous architectures to share parameters, based on a carefully-designed homogeneity definition. Experiments on real-world benchmark datasets demonstrate that the GNN architecture identified by AGNN achieves the best performance, comparing with existing handcrafted models and tradistional search methods.",0
"The successful application of Graph Neural Networks (GNN) on graph-structured data requires expert input and exhaustive trials to identify an appropriate GNN architecture. This is because the choice of graph convolution components, such as aggregate function and hidden dimension, significantly affects the performance of a GNN architecture. While Neural Architecture Search (NAS) has shown potential in discovering effective deep architectures for image and language modeling, it cannot be directly applied to the GNN search problem due to differences in search space, representation learning capacity, and instability of commonly used techniques such as parameter sharing. To address this gap, we propose the Automated Graph Neural Networks (AGNN) framework, which uses a reinforcement learning based controller to validate architectures via small steps within a predefined search space. AGNN also employs a novel parameter sharing strategy based on a carefully-designed homogeneity definition. Our experiments on benchmark datasets demonstrate that AGNN identifies the best GNN architecture compared to existing models and traditional search methods.",1
"To explore underlying complementary information from multiple views, in this paper, we propose a novel Latent Multi-view Semi-Supervised Classification (LMSSC) method. Unlike most existing multi-view semi-supervised classification methods that learn the graph using original features, our method seeks an underlying latent representation and performs graph learning and label propagation based on the learned latent representation. With the complementarity of multiple views, the latent representation could depict the data more comprehensively than every single view individually, accordingly making the graph more accurate and robust as well. Finally, LMSSC integrates latent representation learning, graph construction, and label propagation into a unified framework, which makes each subtask optimized. Experimental results on real-world benchmark datasets validate the effectiveness of our proposed method.",0
"This paper proposes a novel approach called Latent Multi-view Semi-Supervised Classification (LMSSC) to investigate complementary information from various perspectives. Unlike conventional methods that use original features to learn the graph, LMSSC discovers an underlying latent representation and conducts graph learning and label propagation based on it. The latent representation is more comprehensive than each individual view, enhancing the accuracy and robustness of the graph. Additionally, LMSSC combines latent representation learning, graph construction, and label propagation into a cohesive framework, optimizing each subtask. Empirical evidence from real-world benchmark datasets confirms the efficacy of our approach.",1
"We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.",0
"It is possible that achieving adversarial robustness and standard generalization may have conflicting objectives. The process of training robust models not only requires more resources but can also result in a decrease in standard accuracy. Our research demonstrates that this compromise between a model's standard accuracy and its ability to resist adversarial perturbations is a provable concept in a simple and natural context. These findings support similar observations made empirically in more intricate settings. Moreover, we suggest that this phenomenon occurs because robust classifiers learn distinct feature representations from standard classifiers. These differences can lead to unexpected benefits, such as robust models learning representations that align more closely with relevant data characteristics and human perception.",1
"One of the most prevalent symptoms among the elderly population, dementia, can be detected by classifiers trained on linguistic features extracted from narrative transcripts. However, these linguistic features are impacted in a similar but different fashion by the normal aging process. Aging is therefore a confounding factor, whose effects have been hard for machine learning classifiers (especially deep neural network based models) to ignore. We show DNN models are capable of estimating ages based on linguistic features. Predicting dementia based on this aging bias could lead to potentially non-generalizable accuracies on clinical datasets, if not properly deconfounded.   In this paper, we propose to address this deconfounding problem with fair representation learning. We build neural network classifiers that learn low-dimensional representations reflecting the impacts of dementia yet discarding the effects of age. To evaluate these classifiers, we specify a model-agnostic score $\Delta_{eo}^{(N)}$ measuring how classifier results are deconfounded from age. Our best models compromise accuracy by only 2.56\% and 1.54\% on two clinical datasets compared to DNNs, and their $\Delta_{eo}^{(2)}$ scores are better than statistical (residulization and inverse probability weight) adjustments.",0
"Classifiers trained on linguistic features extracted from narrative transcripts can detect dementia, which is a prevalent symptom among the elderly population. However, the similar but different impact of the normal aging process on these linguistic features makes aging a confounding factor that is difficult for machine learning classifiers, especially deep neural network-based models, to ignore. DNN models can estimate ages based on linguistic features, but predicting dementia based on this aging bias could lead to inaccuracies on clinical datasets that are not generalizable if not properly deconfounded. In this paper, we propose to address this deconfounding problem with fair representation learning. We build neural network classifiers that learn low-dimensional representations that reflect the impacts of dementia while discarding the effects of age. To evaluate these classifiers, we use a model-agnostic score $\Delta_{eo}^{(N)}$ that measures how classifier results are deconfounded from age. Our best models compromise accuracy by only 2.56\% and 1.54\% on two clinical datasets compared to DNNs, and their $\Delta_{eo}^{(2)}$ scores are better than statistical adjustments such as residulization and inverse probability weight.",1
"In representation learning and non-linear dimension reduction, there is a huge interest to learn the 'disentangled' latent variables, where each sub-coordinate almost uniquely controls a facet of the observed data. While many regularization approaches have been proposed on variational autoencoders, heuristic tuning is required to balance between disentanglement and loss in reconstruction accuracy -- due to the unsupervised nature, there is no principled way to find an optimal weight for regularization. Motivated to completely bypass regularization, we consider a projection strategy: modifying the canonical Gaussian encoder, we add a layer of scaling and rotation to the Gaussian mean, such that the marginal correlations among latent sub-coordinates become exactly zero. This achieves a theoretically maximal disentanglement, as guaranteed by zero cross-correlation between one latent sub-coordinate and the observed varying with the rest. Unlike regularizations, the extra projection layer does not impact the flexibility of the previous encoder layers, leading to almost no loss in expressiveness. This approach is simple to implement in practice. Our numerical experiments demonstrate very good performance, with no tuning required.",0
"There is significant interest in learning ""disentangled"" latent variables in representation learning and non-linear dimension reduction. These variables allow for each sub-coordinate to control a specific aspect of the observed data. Although various regularization approaches have been proposed for variational autoencoders, balancing disentanglement and reconstruction accuracy requires heuristic tuning since there is no principled way to determine an optimal weight for regularization in the absence of supervision. In order to circumvent the need for regularization, we propose a projection strategy that involves modifying the Gaussian encoder by adding a scaling and rotation layer to the Gaussian mean. This ensures that the marginal correlations among latent sub-coordinates are exactly zero, resulting in maximal disentanglement. Unlike regularizations, this additional projection layer does not compromise the expressiveness of the previous encoder layers, thereby minimizing any loss in expressiveness. The approach is straightforward to implement and our numerical experiments demonstrate excellent performance without the need for tuning.",1
"Existing deep learning models may encounter great challenges in handling graph structured data. In this paper, we introduce a new deep learning model for graph data specifically, namely the deep loopy neural network. Significantly different from the previous deep models, inside the deep loopy neural network, there exist a large number of loops created by the extensive connections among nodes in the input graph data, which makes model learning an infeasible task. To resolve such a problem, in this paper, we will introduce a new learning algorithm for the deep loopy neural network specifically. Instead of learning the model variables based on the original model, in the proposed learning algorithm, errors will be back-propagated through the edges in a group of extracted spanning trees. Extensive numerical experiments have been done on several real-world graph datasets, and the experimental results demonstrate the effectiveness of both the proposed model and the learning algorithm in handling graph data.",0
"Dealing with graph structured data can pose serious difficulties for existing deep learning models. This study presents a new deep learning model, named the deep loopy neural network, that is specifically designed for graph data. Unlike previous deep models, the deep loopy neural network contains numerous loops created by the extensive connections among nodes in the input graph data, making model learning an infeasible task. To solve this issue, a new learning algorithm is introduced in this paper for the deep loopy neural network. Instead of learning the model variables based on the original model, errors are back-propagated through the edges in a group of extracted spanning trees. Various numerical experiments have been conducted on several real-world graph datasets, and the experimental results demonstrate the effectiveness of both the proposed model and the learning algorithm in handling graph data.",1
"Most popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action---many are common across multiple actions---pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To identify these useful features, we resort to a negative bag consisting of features that are known to be irrelevant, for example, they are sampled either from datasets that are unrelated to our actions of interest or are CNN features produced via random noise as input. With the features from the video as a positive bag and the irrelevant features as the negative bag, we cast an objective to learn a (nonlinear) hyperplane that separates the unknown useful features from the rest in a multiple instance learning formulation within a support vector machine setup. We use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they can be treated as a weighted average pooling of the features from the bags, with zero weights given to non-support vectors. Our pooling scheme is end-to-end trainable within a deep learning framework. We report results from experiments on eight computer vision benchmark datasets spanning a variety of video-related tasks and demonstrate state-of-the-art performance across these tasks.",0
"The most widely used deep models for recognizing actions in videos produce separate predictions for short clips, which are then combined using heuristic pooling to label the entire video segment. However, this method may assign equal importance to all frames, even though some frames may not accurately represent the action being performed. To address this issue, we propose a discriminative pooling approach that identifies the useful features among the deep features generated from all short clips. To achieve this, we use a negative bag of features that are known to be irrelevant, such as those from unrelated datasets or produced via random noise input, and learn a hyperplane that separates the unknown features from the relevant ones in a multiple instance learning formulation. We use the hyperplane parameters as a descriptor for the entire video segment, which can be treated as a weighted average pooling of the features from the bags. Our approach is end-to-end trainable within a deep learning framework and outperforms existing methods on eight benchmark datasets covering various video-related tasks.",1
"To improve the ability of VAE to disentangle in the latent space, existing works mostly focus on enforcing independence among the learned latent factors. However, the ability of these models to disentangle often decreases as the complexity of the generative factors increases. In this paper, we investigate the little-explored effect of the modeling capacity of a posterior density on the disentangling ability of the VAE. We note that the independence within and the complexity of the latent density are two different properties we constrain when regularizing the posterior density: while the former promotes the disentangling ability of VAE, the latter -- if overly limited -- creates an unnecessary competition with the data reconstruction objective in VAE. Therefore, if we preserve the independence but allow richer modeling capacity in the posterior density, we will lift this competition and thereby allow improved independence and data reconstruction at the same time. We investigate this theoretical intuition with a VAE that utilizes a non-parametric latent factor model, the Indian Buffet Process (IBP), as a latent density that is able to grow with the complexity of the data. Across three widely-used benchmark data sets and two clinical data sets little explored for disentangled learning, we qualitatively and quantitatively demonstrated the improved disentangling performance of IBP-VAE over the state of the art. In the latter two clinical data sets riddled with complex factors of variations, we further demonstrated that unsupervised disentangling of nuisance factors via IBP-VAE -- when combined with a supervised objective -- can not only improve task accuracy in comparison to relevant supervised deep architectures but also facilitate knowledge discovery related to task decision-making. A shorter version of this work will appear in the ICDM 2019 conference proceedings.",0
"Previous research has focused on enforcing independence among the latent factors in Variational Autoencoder (VAE) models to improve their ability to disentangle. However, this approach is limited by the complexity of the generative factors. This paper explores the impact of the modeling capacity of the posterior density on the disentangling ability of VAE. The authors note that constraining the complexity of the latent density can hinder the data reconstruction objective, while promoting independence can improve disentangling ability. By allowing richer modeling capacity in the posterior density while preserving independence, the authors demonstrate improved disentangling performance of VAE with the Indian Buffet Process (IBP) model on benchmark and clinical datasets. This approach also facilitates knowledge discovery and improves task accuracy when combined with a supervised objective.",1
"Research on graph representation learning has received a lot of attention in recent years since many data in real-world applications come in form of graphs. High-dimensional graph data are often in irregular form, which makes them more difficult to analyze than image/video/audio data defined on regular lattices. Various graph embedding techniques have been developed to convert the raw graph data into a low-dimensional vector representation while preserving the intrinsic graph properties. In this review, we first explain the graph embedding task and its challenges. Next, we review a wide range of graph embedding techniques with insights. Then, we evaluate several state-of-the-art methods against small and large datasets and compare their performance. Finally, potential applications and future directions are presented.",0
"The study of graph representation learning has become a major focus of research in recent years due to the prevalence of graph data in real-world scenarios. These high-dimensional graph datasets are often irregular in nature, making them more challenging to analyze than data in the form of images, videos, or audio files that are defined on regular grids. To address this issue, a variety of methods for graph embedding have been developed that allow for the conversion of raw graph data into a low-dimensional vector representation while preserving the inherent properties of the graph. This review article provides an overview of the graph embedding task and its associated challenges, followed by a detailed examination of a wide range of graph embedding techniques. The performance of various state-of-the-art methods is then evaluated using small and large datasets, and potential applications and future directions for this field are discussed.",1
"When trained effectively, the Variational Autoencoder (VAE) is both a powerful language model and an effective representation learning framework. In practice, however, VAEs are trained with the evidence lower bound (ELBO) as a surrogate objective to the intractable marginal data likelihood. This approach to training yields unstable results, frequently leading to a disastrous local optimum known as posterior collapse. In this paper, we investigate a simple fix for posterior collapse which yields surprisingly effective results. The combination of two known heuristics, previously considered only in isolation, substantially improves held-out likelihood, reconstruction, and latent representation learning when compared with previous state-of-the-art methods. More interestingly, while our experiments demonstrate superiority on these principle evaluations, our method obtains a worse ELBO. We use these results to argue that the typical surrogate objective for VAEs may not be sufficient or necessarily appropriate for balancing the goals of representation learning and data distribution modeling.",0
"The Variational Autoencoder (VAE) can be a powerful language model and a useful representation learning framework if trained correctly. However, VAEs are typically trained using the evidence lower bound (ELBO) as a substitute objective due to the intractable marginal data likelihood, which often leads to unstable results and posterior collapse. In this study, we propose a straightforward solution to posterior collapse using two well-known heuristics that were previously used independently. Our approach significantly improves held-out likelihood, reconstruction, and latent representation learning compared to previous methods. Interestingly, our method results in a worse ELBO, suggesting that the surrogate objective commonly used for VAEs may not be sufficient for balancing representation learning and data distribution modeling.",1
"Residual representation learning simplifies the optimization problem of learning complex functions and has been widely used by traditional convolutional neural networks. However, it has not been applied to deep neural decision forest (NDF). In this paper we incorporate residual learning into NDF and the resulting model achieves state-of-the-art level accuracy on three public age estimation benchmarks while requiring less memory and computation. We further employ gradient-based technique to visualize the decision-making process of NDF and understand how it is influenced by facial image inputs. The code and pre-trained models will be available at https://github.com/Nicholasli1995/VisualizingNDF.",0
"The utilization of residual representation learning has simplified the optimization problem involved in learning intricate functions and has been widely implemented in conventional convolutional neural networks. Despite its popularity, it has not yet been utilized in deep neural decision forest (NDF). In this study, we have integrated residual learning into NDF and the resulting model has achieved the highest level of accuracy on three age estimation benchmarks available to the public. Additionally, this model requires less memory and computation. We have also employed gradient-based techniques to visualize the decision-making process of NDF and gain insight into how facial image inputs influence it. Those interested can access the code and pre-trained models at https://github.com/Nicholasli1995/VisualizingNDF.",1
"For many computer vision applications such as image captioning, visual question answering, and person search, learning discriminative feature representations at both image and text level is an essential yet challenging problem. Its challenges originate from the large word variance in the text domain as well as the difficulty of accurately measuring the distance between the features of the two modalities. Most prior work focuses on the latter challenge, by introducing loss functions that help the network learn better feature representations but fail to account for the complexity of the textual input. With that in mind, we introduce TIMAM: a Text-Image Modality Adversarial Matching approach that learns modality-invariant feature representations using adversarial and cross-modal matching objectives. In addition, we demonstrate that BERT, a publicly-available language model that extracts word embeddings, can successfully be applied in the text-to-image matching domain. The proposed approach achieves state-of-the-art cross-modal matching performance on four widely-used publicly-available datasets resulting in absolute improvements ranging from 2% to 5% in terms of rank-1 accuracy.",0
"Generating descriptive captions, answering visual questions, and searching for individuals through computer vision applications require the acquisition of discriminative feature representations at both the image and text level. However, accomplishing this task is difficult due to the vast range of words in the text domain and the challenge of accurately measuring the distance between the two modalities' features. While prior work has focused on improving feature representations, they have failed to address the complexity of textual input. To address this issue, we propose TIMAM: a Text-Image Modality Adversarial Matching approach that utilizes adversarial and cross-modal matching objectives to develop modality-invariant feature representations. Furthermore, we employ BERT, a publicly-available language model that extracts word embeddings, to demonstrate its effectiveness in the text-to-image matching domain. Our proposed approach achieves state-of-the-art cross-modal matching performance on four publicly-available datasets, resulting in rank-1 accuracy improvements ranging from 2% to 5%.",1
"Using touch devices to navigate in virtual 3D environments such as computer assisted design (CAD) models or geographical information systems (GIS) is inherently difficult for humans, as the 3D operations have to be performed by the user on a 2D touch surface. This ill-posed problem is classically solved with a fixed and handcrafted interaction protocol, which must be learned by the user. We propose to automatically learn a new interaction protocol allowing to map a 2D user input to 3D actions in virtual environments using reinforcement learning (RL). A fundamental problem of RL methods is the vast amount of interactions often required, which are difficult to come by when humans are involved. To overcome this limitation, we make use of two collaborative agents. The first agent models the human by learning to perform the 2D finger trajectories. The second agent acts as the interaction protocol, interpreting and translating to 3D operations the 2D finger trajectories from the first agent. We restrict the learned 2D trajectories to be similar to a training set of collected human gestures by first performing state representation learning, prior to reinforcement learning. This state representation learning is addressed by projecting the gestures into a latent space learned by a variational auto encoder (VAE).",0
"The use of touch devices to navigate virtual 3D environments, such as CAD models or GIS, poses a challenge for humans as the 3D actions must be performed on a 2D surface. Typically, a fixed and learned interaction protocol is used, which can be difficult for users to master. Our proposed solution involves using reinforcement learning (RL) to automatically learn a new interaction protocol that maps 2D user input to 3D actions. However, RL methods require a vast amount of interactions, which can be difficult to obtain when humans are involved. To address this issue, we use two collaborative agents. The first agent models human behavior by learning 2D finger trajectories, while the second agent serves as the interaction protocol, interpreting and translating the 2D finger trajectories into 3D actions. To ensure consistency with human gestures, we use state representation learning by projecting the gestures into a latent space learned by a VAE.",1
"In this paper, we propose a novel self-supervised representation learning by taking advantage of a neighborhood-relational encoding (NRE) among the training data. Conventional unsupervised learning methods only focused on training deep networks to understand the primitive characteristics of the visual data, mainly to be able to reconstruct the data from a latent space. They often neglected the relation among the samples, which can serve as an important metric for self-supervision. Different from the previous work, NRE aims at preserving the local neighborhood structure on the data manifold. Therefore, it is less sensitive to outliers. We integrate our NRE component with an encoder-decoder structure for learning to represent samples considering their local neighborhood information. Such discriminative and unsupervised representation learning scheme is adaptable to different computer vision tasks due to its independence from intense annotation requirements. We evaluate our proposed method for different tasks, including classification, detection, and segmentation based on the learned latent representations. In addition, we adopt the auto-encoding capability of our proposed method for applications like defense against adversarial example attacks and video anomaly detection. Results confirm the performance of our method is better or at least comparable with the state-of-the-art for each specific application, but with a generic and self-supervised approach.",0
"This paper introduces a new approach to self-supervised representation learning that utilizes neighborhood-relational encoding (NRE) to train deep networks. Unlike traditional unsupervised methods that only focus on understanding the basic characteristics of visual data, NRE preserves the local neighborhood structure on the data manifold, making it less sensitive to outliers. By integrating NRE with an encoder-decoder structure, our approach can learn to represent samples while considering their local neighborhood information. This discriminative and unsupervised representation learning scheme can be adapted to various computer vision tasks without requiring intensive annotations. We evaluate our approach on classification, detection, and segmentation tasks, and also utilize its auto-encoding capability for defense against adversarial attacks and video anomaly detection. Our results demonstrate that our approach is as good as or better than state-of-the-art methods for each specific application, while being more generic and self-supervised.",1
"In this paper, we study the problem of node representation learning with graph neural networks. We present a graph neural network class named recurrent graph neural network (RGNN), that address the shortcomings of prior methods. By using recurrent units to capture the long-term dependency across layers, our methods can successfully identify important information during recursive neighborhood expansion. In our experiments, we show that our model class achieves state-of-the-art results on three benchmarks: the Pubmed, Reddit, and PPI network datasets. Our in-depth analyses also demonstrate that incorporating recurrent units is a simple yet effective method to prevent noisy information in graphs, which enables a deeper graph neural network.",0
"The objective of this paper is to investigate the issue of node representation learning through the utilization of graph neural networks. We introduce the recurrent graph neural network (RGNN) as a type of graph neural network that addresses the defects of prior methods. Our approach uses recurrent units to capture long-term dependencies across layers, which enables the identification of significant information during recursive neighborhood expansion. Our experiments reveal that our model class outperforms the competition on three benchmarks: the Pubmed, Reddit, and PPI network datasets. Furthermore, our thorough analyses demonstrate that integrating recurrent units is a straightforward yet effective technique for preventing noisy information in graphs, which facilitates a deeper graph neural network.",1
"Recent binary representation learning models usually require sophisticated binary optimization, similarity measure or even generative models as auxiliaries. However, one may wonder whether these non-trivial components are needed to formulate practical and effective hashing models. In this paper, we answer the above question by proposing an embarrassingly simple approach to binary representation learning. With a simple classification objective, our model only incorporates two additional fully-connected layers onto the top of an arbitrary backbone network, whilst complying with the binary constraints during training. The proposed model lower-bounds the Information Bottleneck (IB) between data samples and their semantics, and can be related to many recent `learning to hash' paradigms. We show that, when properly designed, even such a simple network can generate effective binary codes, by fully exploring data semantics without any held-out alternating updating steps or auxiliary models. Experiments are conducted on conventional large-scale benchmarks, i.e., CIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms the state-of-the-art methods.",0
"Sophisticated binary optimization, similarity measure, and generative models are often required for recent binary representation learning models. However, it is debatable whether these complex components are necessary for practical and efficient hashing models. In this article, we propose a straightforward approach to binary representation learning that answers this question. Our model has a simple classification objective and only adds two fully-connected layers to any backbone network while adhering to binary constraints during training. The proposed model is a lower-bound on the Information Bottleneck (IB) between data samples and their semantics, and is comparable to recent 'learning to hash' paradigms. We demonstrate that even such a simple network can produce effective binary codes by fully exploring data semantics without the need for held-out alternating updating steps or auxiliary models. We conducted experiments on conventional large-scale benchmarks including CIFAR-10, NUS-WIDE, and ImageNet, and the results indicate that our simple model outperforms state-of-the-art methods.",1
"Automatic estimation of the number of people in unconstrained crowded scenes is a challenging task and one major difficulty stems from the huge scale variation of people. In this paper, we propose a novel Deep Structured Scale Integration Network (DSSINet) for crowd counting, which addresses the scale variation of people by using structured feature representation learning and hierarchically structured loss function optimization. Unlike conventional methods which directly fuse multiple features with weighted average or concatenation, we first introduce a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features mutually with a message passing mechanism. In this module, each scale-specific feature is considered as a continuous random variable and passes complementary information to refine the features at other scales. Second, we utilize a Dilated Multiscale Structural Similarity loss to enforce our DSSINet to learn the local correlation of people's scales within regions of various size, thus yielding high-quality density maps. Extensive experiments on four challenging benchmarks well demonstrate the effectiveness of our method. Specifically, our DSSINet achieves improvements of 9.5% error reduction on Shanghaitech dataset and 24.9% on UCF-QNRF dataset against the state-of-the-art methods.",0
"The estimation of the number of people in crowded scenes that lack constraints is a difficult task due to the significant scale variation of people. This paper proposes a new approach to crowd counting called Deep Structured Scale Integration Network (DSSINet), which addresses the challenge of scale variation by utilizing structured feature representation learning and hierarchically structured loss function optimization. Unlike traditional methods that fuse multiple features using weighted averages or concatenation, this approach employs a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features by passing complementary information to features at other scales. Furthermore, a Dilated Multiscale Structural Similarity loss is used to enforce the DSSINet to learn the local correlation of people's scales within regions of various sizes, leading to high-quality density maps. Extensive experiments on four challenging benchmarks demonstrate the effectiveness of this method, with the DSSINet achieving error reductions of 9.5% and 24.9% on the Shanghaitech and UCF-QNRF datasets, respectively, compared to state-of-the-art methods.",1
"In this paper, we propose a robust representation learning model called Adaptive Structure-constrained Low-Rank Coding (AS-LRC) for the latent representation of data. To recover the underlying subspaces more accurately, AS-LRC seamlessly integrates an adaptive weighting based block-diagonal structure-constrained low-rank representation and the group sparse salient feature extraction into a unified framework. Specifically, AS-LRC performs the latent decomposition of given data into a low-rank reconstruction by a block-diagonal codes matrix, a group sparse locality-adaptive salient feature part and a sparse error part. To enforce the block-diagonal structures adaptive to different real datasets for the low-rank recovery, AS-LRC clearly computes an auto-weighting matrix based on the locality-adaptive features and multiplies by the low-rank coefficients for direct minimization at the same time. This encourages the codes to be block-diagonal and can avoid the tricky issue of choosing optimal neighborhood size or kernel width for the weight assignment, suffered in most local geometrical structures-preserving low-rank coding methods. In addition, our AS-LRC selects the L2,1-norm on the projection for extracting group sparse features rather than learning low-rank features by Nuclear-norm regularization, which can make learnt features robust to noise and outliers in samples, and can also make the feature coding process efficient. Extensive visualizations and numerical results demonstrate the effectiveness of our AS-LRC for image representation and recovery.",0
"The paper introduces a new model for learning representations of data called Adaptive Structure-constrained Low-Rank Coding (AS-LRC). The model combines a block-diagonal structure-constrained low-rank representation and group sparse feature extraction to improve the accuracy of recovering underlying subspaces. AS-LRC decomposes data into a low-rank reconstruction, a group sparse salient feature part, and a sparse error part. To adapt to different datasets, AS-LRC computes an auto-weighting matrix based on locality-adaptive features to enforce block-diagonal structures. This avoids the need to determine an optimal neighborhood size or kernel width for weight assignment, which is a common issue in other low-rank coding methods. Also, AS-LRC uses the L2,1-norm for extracting group sparse features instead of Nuclear-norm regularization, making the feature coding process efficient and robust to noise and outliers in samples. The paper presents visualizations and numerical results that demonstrate the effectiveness of AS-LRC for image representation and recovery.",1
"Designing a registration framework for images that do not share the same probability distribution is a major challenge in modern image analytics yet trivial task for the human visual system (HVS). Discrepancies in probability distributions, also known as \emph{drifts}, can occur due to various reasons including, but not limited to differences in sequences and modalities (e.g., MRI T1-T2 and MRI-CT registration), or acquisition settings (e.g., multisite, inter-subject, or intra-subject registrations). The popular assumption about the working of HVS is that it exploits a communal feature subspace exists between the registering images or fields-of-view that encompasses key drift-invariant features. Mimicking the approach that is potentially adopted by the HVS, herein, we present a representation learning technique of this invariant communal subspace that is shared by registering domains. The proposed communal domain learning (CDL) framework uses a set of hierarchical nonlinear transforms to learn the communal subspace that minimizes the probability differences and maximizes the amount of shared information between the registering domains. Similarity metric and parameter optimization calculations for registration are subsequently performed in the drift-minimized learned communal subspace. This generic registration framework is applied to register multisequence (MR: T1, T2) and multimodal (MR, CT) images. Results demonstrated generic applicability, consistent performance, and statistically significant improvement for both multi-sequence and multi-modal data using the proposed approach ($p$-value$<0.001$; Wilcoxon rank sum test) over baseline methods.",0
"Creating a registration framework for images with different probability distributions is a challenging task in modern image analytics, but it is effortless for the human visual system (HVS). Probability distribution discrepancies, also known as ""drifts,"" can arise due to various reasons such as differences in sequences and modalities (e.g., MRI T1-T2 and MRI-CT registration) or acquisition settings (e.g., multisite, inter-subject, or intra-subject registrations). The HVS is believed to work by exploiting a communal feature subspace that exists between the registering images or fields-of-view, which includes key features that are invariant to drift. To imitate the HVS approach, we propose a representation learning technique that learns the invariant communal subspace shared by registering domains. The communal domain learning (CDL) framework uses hierarchical nonlinear transforms to learn the communal subspace that reduces the probability differences and maximizes the amount of shared information between the registering domains. Subsequently, similarity metric and parameter optimization calculations for registration are performed in the learned communal subspace, which minimizes drift. We apply this generic registration framework to multisequence (MR: T1, T2) and multimodal (MR, CT) images. Results show that the proposed approach has generic applicability, consistent performance, and statistically significant improvement for both multi-sequence and multi-modal data compared to baseline methods ($p$-value$<0.001$; Wilcoxon rank sum test).",1
"Manifold learning seeks a low dimensional representation that faithfully captures the essence of data. Current methods can successfully learn such representations, but do not provide a meaningful set of operations that are associated with the representation. Working towards operational representation learning, we endow the latent space of a large class of generative models with a random Riemannian metric, which provides us with elementary operators. As computational tools are unavailable for random Riemannian manifolds, we study deterministic approximations and derive tight error bounds on expected distances.",0
"The goal of manifold learning is to find a lower dimensional representation that accurately reflects the data. Although current methods can achieve this, they lack a useful set of associated operations. To address this, we introduce a random Riemannian metric to the latent space of many generative models, which allows for the creation of elementary operators. As it is not possible to use computational tools for random Riemannian manifolds, we explore deterministic approximations and provide precise error limits for expected distances.",1
"Recognizing multiple labels of images is a practical and challenging task, and significant progress has been made by searching semantic-aware regions and modeling label dependency. However, current methods cannot locate the semantic regions accurately due to the lack of part-level supervision or semantic guidance. Moreover, they cannot fully explore the mutual interactions among the semantic regions and do not explicitly model the label co-occurrence. To address these issues, we propose a Semantic-Specific Graph Representation Learning (SSGRL) framework that consists of two crucial modules: 1) a semantic decoupling module that incorporates category semantics to guide learning semantic-specific representations and 2) a semantic interaction module that correlates these representations with a graph built on the statistical label co-occurrence and explores their interactions via a graph propagation mechanism. Extensive experiments on public benchmarks show that our SSGRL framework outperforms current state-of-the-art methods by a sizable margin, e.g. with an mAP improvement of 2.5%, 2.6%, 6.7%, and 3.1% on the PASCAL VOC 2007 & 2012, Microsoft-COCO and Visual Genome benchmarks, respectively. Our codes and models are available at https://github.com/HCPLab-SYSU/SSGRL.",0
"The task of recognizing multiple labels of images is both practical and challenging. Progress has been made in this area through the search for semantic-aware regions and modeling label dependency. However, current methods are not able to accurately locate semantic regions due to the lack of part-level supervision or semantic guidance. Additionally, these methods do not fully explore the mutual interactions among the semantic regions nor explicitly model the label co-occurrence. To overcome these issues, we propose the Semantic-Specific Graph Representation Learning (SSGRL) framework, which comprises two crucial modules. The first module, the semantic decoupling module, incorporates category semantics to guide the learning of semantic-specific representations. The second module, the semantic interaction module, correlates these representations with a graph built on statistical label co-occurrence and explores their interactions via a graph propagation mechanism. Extensive experiments on public benchmarks show that our SSGRL framework outperforms current state-of-the-art methods by a significant margin, with improvements in mAP of 2.5%, 2.6%, 6.7%, and 3.1% on the PASCAL VOC 2007 & 2012, Microsoft-COCO, and Visual Genome benchmarks, respectively. Our codes and models are available at https://github.com/HCPLab-SYSU/SSGRL.",1
"Unsupervised representation learning has succeeded with excellent results in many applications. It is an especially powerful tool to learn a good representation of environments with partial or noisy observations. In partially observable domains it is important for the representation to encode a belief state, a sufficient statistic of the observations seen so far. In this paper, we investigate whether it is possible to learn such a belief representation using modern neural architectures. Specifically, we focus on one-step frame prediction and two variants of contrastive predictive coding (CPC) as the objective functions to learn the representations. To evaluate these learned representations, we test how well they can predict various pieces of information about the underlying state of the environment, e.g., position of the agent in a 3D maze. We show that all three methods are able to learn belief representations of the environment, they encode not only the state information, but also its uncertainty, a crucial aspect of belief states. We also find that for CPC multi-step predictions and action-conditioning are critical for accurate belief representations in visually complex environments. The ability of neural representations to capture the belief information has the potential to spur new advances for learning and planning in partially observable domains, where leveraging uncertainty is essential for optimal decision making.",0
"Unsupervised representation learning has been highly successful in numerous applications, particularly in learning representations of environments that have partial or noisy observations. When dealing with partially observable domains, it is crucial for the representation to capture a belief state that serves as a sufficient statistic of the observations made so far. This study investigates whether it is feasible to acquire such a belief representation through modern neural architectures. The study focuses on one-step frame prediction and two variations of contrastive predictive coding (CPC) as the objective functions to learn the representations. To evaluate these learned representations, the researchers examine their ability to predict various pieces of information about the environment's underlying state, such as the agent's position in a 3D maze. The study shows that all three methods can learn belief representations of the environment that encode not only state information but also uncertainty, a critical aspect of belief states. In visually complex environments, multi-step predictions and action-conditioning are crucial for accurate belief representations in CPC. The ability of neural representations to capture belief information has the potential to stimulate new advances in learning and planning in partially observable domains, where uncertainty is necessary for optimal decision-making.",1
"A key challenge for autonomous driving is safe trajectory planning in cluttered, urban environments with dynamic obstacles, such as pedestrians, bicyclists, and other vehicles. A reliable prediction of the future environment, including the behavior of dynamic agents, would allow planning algorithms to proactively generate a trajectory in response to a rapidly changing environment. We present a novel framework that predicts the future occupancy state of the local environment surrounding an autonomous agent by learning a motion model from occupancy grid data using a neural network. We take advantage of the temporal structure of the grid data by utilizing a convolutional long-short term memory network in the form of the PredNet architecture. This method is validated on the KITTI dataset and demonstrates higher accuracy and better predictive power than baseline methods.",0
"One of the main obstacles facing autonomous driving is planning a safe trajectory in busy urban areas where there are moving obstacles like pedestrians, cyclists, and other vehicles. To overcome this challenge, it's crucial to have a reliable prediction of the future environment, including the behavior of other moving objects. This allows planning algorithms to generate a trajectory in response to changes in the environment. To address this issue, we introduce a unique framework that predicts the future occupancy state of the surrounding environment by training a neural network on occupancy grid data. We utilize a convolutional long-short term memory network in the form of the PredNet architecture to take advantage of the temporal structure of the grid data. Our approach is tested on the KITTI dataset and proves to be more accurate and powerful than baseline methods.",1
"With emergence of blockchain technologies and the associated cryptocurrencies, such as Bitcoin, understanding network dynamics behind Blockchain graphs has become a rapidly evolving research direction. Unlike other financial networks, such as stock and currency trading, blockchain based cryptocurrencies have the entire transaction graph accessible to the public (i.e., all transactions can be downloaded and analyzed). A natural question is then to ask whether the dynamics of the transaction graph impacts the price of the underlying cryptocurrency. We show that standard graph features such as degree distribution of the transaction graph may not be sufficient to capture network dynamics and its potential impact on fluctuations of Bitcoin price. In contrast, the new graph associated topological features computed using the tools of persistent homology, are found to exhibit a high utility for predicting Bitcoin price dynamics. %explain higher order interactions among the nodes in Blockchain graphs and can be used to build much more accurate price prediction models. Using the proposed persistent homology-based techniques, we offer a new elegant, easily extendable and computationally light approach for graph representation learning on Blockchain.",0
"The emergence of blockchain technologies and related cryptocurrencies, like Bitcoin, has led to a growing interest in studying the network dynamics of blockchain graphs. Unlike other financial networks, where transactions are private, blockchain-based cryptocurrencies make their entire transaction graph public for download and analysis. This raises the question of whether the dynamics of the transaction graph affect the price of the cryptocurrency. However, we found that standard graph features, such as the degree distribution, may not be sufficient to capture network dynamics and predict changes in Bitcoin price. Instead, we propose using persistent homology-based techniques to analyze the topological features of the graph, which can reveal higher order interactions among nodes and offer more accurate price predictions. Our approach is elegant, computationally light, and easily extendable for future research on graph representation learning in blockchain.",1
"The advancement of machine learning algorithms has opened a wide scope for vibration-based SHM (Structural Health Monitoring). Vibration-based SHM is based on the fact that damage will alter the dynamic properties viz., structural response, frequencies, mode shapes, etc of the structure. The responses measured using sensors, which are high dimensional in nature, can be intelligently analyzed using machine learning techniques for damage assessment. Neural networks employing multilayer architectures are expressive models capable of capturing complex relationships between input-output pairs but do not account for uncertainty in network outputs. A BNN (Bayesian Neural Network) refers to extending standard networks with posterior inference. It is a neural network with a prior distribution on its weights. Deep learning architectures like CNN (Convolutional neural network) and LSTM(Long Short Term Memory) are good candidates for representation learning from high dimensional data. The advantage of using CNN over multi-layer neural networks is that they are good feature extractors as well as classifiers, which eliminates the need for generating hand-engineered features. LSTM networks are mainly used for sequence modeling. This paper presents both a Bayesian multi-layer perceptron and deep learning-based approach for damage detection and location identification in beam-like structures. Raw frequency response data simulated using finite element analysis is fed as the input of the network. As part of this, frequency response was generated for a series of simulations in the cantilever beam involving different damage scenarios. This case study shows the effectiveness of the above approaches to predict bending rigidity with an acceptable error rate.",0
"The progress in machine learning algorithms has expanded the potential for Structural Health Monitoring (SHM) based on vibrations. Vibration-based SHM relies on the fact that damage changes the dynamic properties of a structure, such as its response, frequencies, and mode shapes. High-dimensional sensor measurements can be intelligently analyzed using machine learning techniques for damage assessment. Although expressive, multilayer neural networks lack the ability to account for uncertainty in network outputs. Bayesian Neural Networks (BNNs) extend standard networks with posterior inference and have a prior distribution on their weights. Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks are promising for representation learning from high-dimensional data. CNNs are feature extractors and classifiers, which eliminates the need for generating hand-engineered features. LSTM networks are mainly used for sequence modeling. This paper presents a Bayesian multi-layer perceptron and deep learning-based approach for identifying damage and location in beam-like structures using raw frequency response data. The frequency response was generated for a series of simulations in a cantilever beam with different damage scenarios. The effectiveness of the proposed approaches to predict bending rigidity with an acceptable error rate was demonstrated in this case study.",1
"Network alignment is a critical task to a wide variety of fields. Many existing works leverage on representation learning to accomplish this task without eliminating domain representation bias induced by domain-dependent features, which yield inferior alignment performance. This paper proposes a unified deep architecture (DANA) to obtain a domain-invariant representation for network alignment via an adversarial domain classifier. Specifically, we employ the graph convolutional networks to perform network embedding under the domain adversarial principle, given a small set of observed anchors. Then, the semi-supervised learning framework is optimized by maximizing a posterior probability distribution of observed anchors and the loss of a domain classifier simultaneously. We also develop a few variants of our model, such as, direction-aware network alignment, weight-sharing for directed networks and simplification of parameter space. Experiments on three real-world social network datasets demonstrate that our proposed approaches achieve state-of-the-art alignment results.",0
"Network alignment is a crucial task in various fields, but existing works often fail to eliminate domain representation bias caused by domain-dependent features. This bias leads to poor alignment performance. To overcome this, the authors propose a unified deep architecture called DANA that uses an adversarial domain classifier to obtain a domain-invariant representation for network alignment. They use graph convolutional networks to embed networks and optimize the semi-supervised learning framework by maximizing the posterior probability distribution of observed anchors and the loss of a domain classifier simultaneously. The authors also explore different variants of their model, including direction-aware network alignment, weight-sharing for directed networks, and simplification of the parameter space. Experiments on three real-world social network datasets show that their proposed approaches achieve better alignment results than existing methods.",1
"We treat shape co-segmentation as a representation learning problem and introduce BAE-NET, a branched autoencoder network, for the task. The unsupervised BAE-NET is trained with a collection of un-segmented shapes, using a shape reconstruction loss, without any ground-truth labels. Specifically, the network takes an input shape and encodes it using a convolutional neural network, whereas the decoder concatenates the resulting feature code with a point coordinate and outputs a value indicating whether the point is inside/outside the shape. Importantly, the decoder is branched: each branch learns a compact representation for one commonly recurring part of the shape collection, e.g., airplane wings. By complementing the shape reconstruction loss with a label loss, BAE-NET is easily tuned for one-shot learning. We show unsupervised, weakly supervised, and one-shot learning results by BAE-NET, demonstrating that using only a couple of exemplars, our network can generally outperform state-of-the-art supervised methods trained on hundreds of segmented shapes. Code is available at https://github.com/czq142857/BAE-NET.",0
"Our approach to shape co-segmentation involves treating it as a form of representation learning. To achieve this, we have developed BAE-NET, a branched autoencoder network, which is trained in an unsupervised manner using a collection of un-segmented shapes. We accomplish this by using a shape reconstruction loss, without relying on any ground-truth labels. The network utilizes a convolutional neural network to encode the input shape, while the decoder combines the resulting feature code with a point coordinate to produce an output indicating whether the point is within the shape or not. The decoder is branched, with each branch learning a compact representation for a commonly recurring part of the shape collection, such as airplane wings. By adding a label loss to the shape reconstruction loss, we can easily tune BAE-NET for one-shot learning. Our results demonstrate that BAE-NET outperforms state-of-the-art supervised methods trained on hundreds of segmented shapes, even when using only a few exemplars. The code for BAE-NET is available at https://github.com/czq142857/BAE-NET.",1
"Network embedding has proved extremely useful in a variety of network analysis tasks such as node classification, link prediction, and network visualization. Almost all the existing network embedding methods learn to map the node IDs to their corresponding node embeddings. This design principle, however, hinders the existing methods from being applied in real cases. Node ID is not generalizable and, thus, the existing methods have to pay great effort in cold-start problem. The heterogeneous network usually requires extra work to encode node types, as node type is not able to be identified by node ID. Node ID carries rare information, resulting in the criticism that the existing methods are not robust to noise.   To address this issue, we introduce Compositional Network Embedding, a general inductive network representation learning framework that generates node embeddings by combining node features based on the principle of compositionally. Instead of directly optimizing an embedding lookup based on arbitrary node IDs, we learn a composition function that infers node embeddings by combining the corresponding node attribute embeddings through a graph-based loss. For evaluation, we conduct the experiments on link prediction under four different settings. The results verified the effectiveness and generalization ability of compositional network embeddings, especially on unseen nodes.",0
"Various network analysis tasks, such as node classification, link prediction, and network visualization, have benefitted greatly from network embedding. However, most existing network embedding methods only map node IDs to node embeddings, which limits their applicability. Node ID lacks generalizability, leading to challenges in addressing the cold-start problem. Additionally, encoding node types in a heterogeneous network requires extra effort, as node type cannot be identified by node ID. This has resulted in criticism of the existing methods for their lack of robustness to noise. To overcome these issues, we propose Compositional Network Embedding, a framework that uses composition principles to generate node embeddings by combining node features. Instead of optimizing an embedding lookup based on node IDs, we learn a composition function that infers node embeddings by combining attribute embeddings through a graph-based loss. We evaluate our approach through link prediction experiments in four settings, demonstrating the effectiveness and generalizability of compositional network embeddings, particularly on unseen nodes.",1
"RGB-Thermal object tracking attempt to locate target object using complementary visual and thermal infrared data. Existing RGB-T trackers fuse different modalities by robust feature representation learning or adaptive modal weighting. However, how to integrate dual attention mechanism for visual tracking is still a subject that has not been studied yet. In this paper, we propose two visual attention mechanisms for robust RGB-T object tracking. Specifically, the local attention is implemented by exploiting the common visual attention of RGB and thermal data to train deep classifiers. We also introduce the global attention, which is a multi-modal target-driven attention estimation network. It can provide global proposals for the classifier together with local proposals extracted from previous tracking result. Extensive experiments on two RGB-T benchmark datasets validated the effectiveness of our proposed algorithm.",0
"The aim of RGB-Thermal object tracking is to locate the target object by utilizing visual and thermal infrared data. Current RGB-T trackers utilize robust feature representation learning or adaptive modal weighting to fuse different modalities. However, the incorporation of a dual attention mechanism for visual tracking has not yet been explored. This paper proposes two visual attention mechanisms for robust RGB-T object tracking. The first is the local attention mechanism, which trains deep classifiers by exploiting the common visual attention of RGB and thermal data. The second is the global attention mechanism, which estimates multi-modal target-driven attention and provides both global and local proposals for the classifier. Through extensive experiments on two RGB-T benchmark datasets, our proposed algorithm has been proven to be effective.",1
"Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require machine agents to interpret natural language instructions and learn to act in visually realistic environments to achieve navigation goals. The overall task requires competence in several perception problems: successful agents combine spatio-temporal, vision and language understanding to produce appropriate action sequences. Our approach adapts pre-trained vision and language representations to relevant in-domain tasks making them more effective for VLN. Specifically, the representations are adapted to solve both a cross-modal sequence alignment and sequence coherence task. In the sequence alignment task, the model determines whether an instruction corresponds to a sequence of visual frames. In the sequence coherence task, the model determines whether the perceptual sequences are predictive sequentially in the instruction-conditioned latent space. By transferring the domain-adapted representations, we improve competitive agents in R2R as measured by the success rate weighted by path length (SPL) metric.",0
"The task of Vision-and-Language Navigation (VLN), such as Room-to-Room (R2R), requires machine agents to understand natural language instructions and operate in visually realistic environments to achieve navigation objectives. To perform this task effectively, agents must demonstrate proficiency in several perception challenges, combining spatio-temporal, vision, and language comprehension to generate appropriate action sequences. Our approach involves modifying pre-existing vision and language representations to better suit VLN tasks. Specifically, we adapt these representations to tackle both cross-modal sequence alignment and sequence coherence tasks. These tasks involve determining whether an instruction corresponds to a sequence of visual frames and whether perceptual sequences are sequentially predictive in an instruction-based latent space. By transferring the adapted representations to agents, we enhance their performance in R2R, as demonstrated by the success rate weighted by path length (SPL) metric.",1
"Unsupervised cross-domain person re-identification (Re-ID) faces two key issues. One is the data distribution discrepancy between source and target domains, and the other is the lack of labelling information in target domain. They are addressed in this paper from the perspective of representation learning. For the first issue, we highlight the presence of camera-level sub-domains as a unique characteristic of person Re-ID, and develop camera-aware domain adaptation to reduce the discrepancy not only between source and target domains but also across these sub-domains. For the second issue, we exploit the temporal continuity in each camera of target domain to create discriminative information. This is implemented by dynamically generating online triplets within each batch, in order to maximally take advantage of the steadily improved feature representation in training process. Together, the above two methods give rise to a novel unsupervised deep domain adaptation framework for person Re-ID. Experiments and ablation studies on benchmark datasets demonstrate its superiority and interesting properties.",0
"This paper addresses two main challenges in unsupervised cross-domain person re-identification (Re-ID). The first issue is the difference in data distribution between the source and target domains, as well as the presence of camera-level sub-domains unique to person Re-ID. To combat this, the authors have developed a camera-aware domain adaptation approach that reduces the discrepancy between the source and target domains, as well as across these sub-domains. The second issue is the lack of labeling information in the target domain, which is addressed through the use of temporal continuity and the creation of discriminative information. This is achieved by generating online triplets within each batch, maximizing the use of feature representation during the training process. The combination of these methods results in a novel unsupervised deep domain adaptation framework for person Re-ID, with experiments and ablation studies demonstrating its effectiveness and interesting properties.",1
"We consider an information theoretic approach to address the problem of identifying fake digital images. We propose an innovative method to formulate the issue of localizing manipulated regions in an image as a deep representation learning problem using the Information Bottleneck (IB), which has recently gained popularity as a framework for interpreting deep neural networks. Tampered images pose a serious predicament since digitized media is a ubiquitous part of our lives. These are facilitated by the easy availability of image editing software and aggravated by recent advances in deep generative models such as GANs. We propose InfoPrint, a computationally efficient solution to the IB formulation using approximate variational inference and compare it to a numerical solution that is computationally expensive. Testing on a number of standard datasets, we demonstrate that InfoPrint outperforms the state-of-the-art and the numerical solution. Additionally, it also has the ability to detect alterations made by inpainting GANs.",0
"Our focus is on tackling the issue of identifying counterfeit digital images using an information theoretic approach. We introduce a novel technique that employs the Information Bottleneck (IB) as a framework for deep representation learning, enabling us to identify manipulated areas in an image. The high availability of image editing software, coupled with advancements in deep generative models like GANs, make it difficult to detect tampered images, posing a significant challenge. Our solution, InfoPrint, is a computationally efficient approach that utilizes approximate variational inference to solve the IB formulation, and we compare it to a more computationally expensive numerical solution. Our experiments on various standard datasets demonstrate that InfoPrint outperforms both the state-of-the-art and the numerical solution and can detect changes made by inpainting GANs.",1
"We propose a symmetric graph convolutional autoencoder which produces a low-dimensional latent representation from a graph. In contrast to the existing graph autoencoders with asymmetric decoder parts, the proposed autoencoder has a newly designed decoder which builds a completely symmetric autoencoder form. For the reconstruction of node features, the decoder is designed based on Laplacian sharpening as the counterpart of Laplacian smoothing of the encoder, which allows utilizing the graph structure in the whole processes of the proposed autoencoder architecture. In order to prevent the numerical instability of the network caused by the Laplacian sharpening introduction, we further propose a new numerically stable form of the Laplacian sharpening by incorporating the signed graphs. In addition, a new cost function which finds a latent representation and a latent affinity matrix simultaneously is devised to boost the performance of image clustering tasks. The experimental results on clustering, link prediction and visualization tasks strongly support that the proposed model is stable and outperforms various state-of-the-art algorithms.",0
"Our proposal is a graph convolutional autoencoder that creates a low-dimensional latent representation from a graph. Unlike existing graph autoencoders with asymmetrical decoder parts, our autoencoder features a newly designed decoder that achieves a completely symmetric autoencoder form. To reconstruct node features, we utilize Laplacian sharpening, which complements Laplacian smoothing used in the encoder, thereby enabling the graph structure to be utilized throughout the autoencoder architecture. To address numerical instability caused by introducing Laplacian sharpening, we introduce a new numerically stable form that incorporates signed graphs. We also introduce a novel cost function that concurrently finds a latent representation and a latent affinity matrix to enhance performance for image clustering tasks. Our experimental results on clustering, link prediction, and visualization tasks demonstrate the stability and superior performance of our model compared to various state-of-the-art algorithms.",1
"Architectures for sparse hierarchical representation learning have recently been proposed for graph-structured data, but so far assume the absence of edge features in the graph. We close this gap and propose a method to pool graphs with edge features, inspired by the hierarchical nature of chemistry. In particular, we introduce two types of pooling layers compatible with an edge-feature graph-convolutional architecture and investigate their performance for molecules relevant to drug discovery on a set of two classification and two regression benchmark datasets of MoleculeNet. We find that our models significantly outperform previous benchmarks on three of the datasets and reach state-of-the-art results on the fourth benchmark, with pooling improving performance for three out of four tasks, keeping performance stable on the fourth task, and generally speeding up the training process.",0
"Recently, there have been proposals for sparse hierarchical representation learning architectures for graph-structured data. However, these proposals assume that the graph has no edge features. To address this limitation, we present a method for pooling graphs with edge features, drawing inspiration from the hierarchical structure of chemistry. We introduce two types of pooling layers that are compatible with an edge-feature graph-convolutional architecture. We evaluate the performance of our models on two classification and two regression benchmark datasets of MoleculeNet, which are relevant to drug discovery. Our findings reveal that our models outperform previous benchmarks on three of the datasets and achieve state-of-the-art results on the fourth benchmark. Furthermore, pooling enhances performance for three out of four tasks, maintains performance stability on the fourth task, and accelerates the training process.",1
"This paper tackles the problem of large-scale image-based localization (IBL) where the spatial location of a query image is determined by finding out the most similar reference images in a large database. For solving this problem, a critical task is to learn discriminative image representation that captures informative information relevant for localization. We propose a novel representation learning method having higher location-discriminating power. It provides the following contributions: 1) we represent a place (location) as a set of exemplar images depicting the same landmarks and aim to maximize similarities among intra-place images while minimizing similarities among inter-place images; 2) we model a similarity measure as a probability distribution on L_2-metric distances between intra-place and inter-place image representations; 3) we propose a new Stochastic Attraction and Repulsion Embedding (SARE) loss function minimizing the KL divergence between the learned and the actual probability distributions; 4) we give theoretical comparisons between SARE, triplet ranking and contrastive losses. It provides insights into why SARE is better by analyzing gradients. Our SARE loss is easy to implement and pluggable to any CNN. Experiments show that our proposed method improves the localization performance on standard benchmarks by a large margin. Demonstrating the broad applicability of our method, we obtained the third place out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our code and model are available at https://github.com/Liumouliu/deepIBL.",0
"The issue of determining the spatial location of an image through finding the most similar reference images in a large database is addressed in this paper, known as large-scale image-based localization (IBL). The key to solving this problem is to learn a discriminative image representation that captures relevant informative information for localization. A novel representation learning method is proposed in this paper, which has higher location-discriminating power. The following contributions are provided: 1) location is represented as a set of exemplar images of the same landmarks, aiming to maximize similarities among intra-place images while minimizing similarities among inter-place images; 2) a probability distribution is modeled as a similarity measure on L_2-metric distances between intra-place and inter-place image representations; 3) the Stochastic Attraction and Repulsion Embedding (SARE) loss function is proposed, minimizing the KL divergence between the learned and actual probability distributions; 4) theoretical comparisons between SARE, triplet ranking, and contrastive losses are given, providing insights into why SARE is superior by analyzing gradients. Our SARE loss function is simple to implement and can be inserted into any CNN. The proposed method's localization performance significantly improves on standard benchmarks, as demonstrated in experiments. Furthermore, the method's broad applicability is demonstrated by obtaining the third place out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our model and code are available at https://github.com/Liumouliu/deepIBL.",1
"For ego-motion estimation, the feature representation of the scenes is crucial. Previous methods indicate that both the low-level and semantic feature-based methods can achieve promising results. Therefore, the incorporation of hierarchical feature representation may benefit from both methods. From this perspective, we propose a novel direct feature odometry framework, named DFO, for depth estimation and hierarchical feature representation learning from monocular videos. By exploiting the metric distance, our framework is able to learn the hierarchical feature representation without supervision. The pose is obtained with a coarse-to-fine approach from high-level to low-level features in enlarged feature maps. The pixel-level attention mask can be self-learned to provide the prior information. In contrast to the previous methods, our proposed method calculates the camera motion with a direct method rather than regressing the ego-motion from the pose network. With this approach, the consistency of the scale factor of translation can be constrained. Additionally, the proposed method is thus compatible with the traditional SLAM pipeline. Experiments on the KITTI dataset demonstrate the effectiveness of our method.",0
"The feature representation of scenes is essential for estimating ego-motion. Previous studies have shown that low-level and semantic feature-based methods can produce satisfactory results. Combining these two methods through hierarchical feature representation may be beneficial. Therefore, we present a new direct feature odometry approach called DFO, which uses monocular videos for depth estimation and learning hierarchical feature representation. Our framework utilizes metric distance to learn hierarchical features without supervision. The pose is determined using a coarse-to-fine approach from high-level to low-level features in enlarged feature maps. The pixel-level attention mask is self-learned to provide prior information. Unlike previous methods, our approach uses a direct method to calculate camera motion rather than regressing ego-motion from the pose network. This approach constrains the consistency of the scale factor of translation and is also compatible with traditional SLAM pipeline. Our experiments on the KITTI dataset demonstrate the effectiveness of our proposed method.",1
"Representation learning is a fundamental but challenging problem, especially when the distribution of data is unknown. We propose a new representation learning method, termed Structure Transfer Machine (STM), which enables feature learning process to converge at the representation expectation in a probabilistic way. We theoretically show that such an expected value of the representation (mean) is achievable if the manifold structure can be transferred from the data space to the feature space. The resulting structure regularization term, named manifold loss, is incorporated into the loss function of the typical deep learning pipeline. The STM architecture is constructed to enforce the learned deep representation to satisfy the intrinsic manifold structure from the data, which results in robust features that suit various application scenarios, such as digit recognition, image classification and object tracking. Compared to state-of-the-art CNN architectures, we achieve the better results on several commonly used benchmarks\footnote{The source code is available. https://github.com/stmstmstm/stm }.",0
"When dealing with unknown data distributions, representation learning can be a complex task. To address this issue, we introduce a novel approach called Structure Transfer Machine (STM) that uses a probabilistic method to enable feature learning to converge at the representation expectation. We prove that achieving the expected value of the representation is possible if the manifold structure can be transferred from the data space to the feature space. This results in a regularization term known as manifold loss, which is incorporated into the loss function of the deep learning pipeline. The STM architecture enforces the learned deep representation to comply with the intrinsic manifold structure in the data, leading to robust features suitable for various applications like digit recognition, image classification, and object tracking. We demonstrate the superior performance of STM compared to state-of-the-art CNN architectures on several commonly used benchmarks. The source code for STM is available at https://github.com/stmstmstm/stm.",1
"We propose a novel and unsupervised representation learning model, i.e., Robust Block-Diagonal Adaptive Locality-constrained Latent Representation (rBDLR). rBDLR is able to recover multi-subspace structures and extract the adaptive locality-preserving salient features jointly. Leveraging on the Frobenius-norm based latent low-rank representation model, rBDLR jointly learns the coding coefficients and salient features, and improves the results by enhancing the robustness to outliers and errors in given data, preserving local information of salient features adaptively and ensuring the block-diagonal structures of the coefficients. To improve the robustness, we perform the latent representation and adaptive weighting in a recovered clean data space. To force the coefficients to be block-diagonal, we perform auto-weighting by minimizing the reconstruction error based on salient features, constrained using a block-diagonal regularizer. This ensures that a strict block-diagonal weight matrix can be obtained and salient features will possess the adaptive locality preserving ability. By minimizing the difference between the coefficient and weights matrices, we can obtain a block-diagonal coefficients matrix and it can also propagate and exchange useful information between salient features and coefficients. Extensive results demonstrate the superiority of rBDLR over other state-of-the-art methods.",0
"Our proposal introduces a new method for representation learning called Robust Block-Diagonal Adaptive Locality-constrained Latent Representation (rBDLR), which operates in an unsupervised manner. rBDLR is capable of recovering multi-subspace structures and simultaneously extracting adaptive locality-preserving salient features. By utilizing a Frobenius-norm-based latent low-rank representation model, rBDLR can learn coding coefficients and salient features jointly, enhancing robustness against outliers and errors in the data, and adaptively preserving local information of salient features while ensuring block-diagonal coefficients. To achieve this, we perform latent representation and adaptive weighting in a recovered clean data space, and use auto-weighting to minimize reconstruction error based on salient features, with a block-diagonal regularizer constraint. This yields a strict block-diagonal weight matrix and enables salient features to possess adaptive locality-preserving ability. By minimizing the difference between coefficient and weight matrices, rBDLR can propagate and exchange information between salient features and coefficients, leading to a block-diagonal coefficients matrix. Our extensive results demonstrate the superiority of rBDLR over other state-of-the-art methods.",1
"Generative models for 3D geometric data arise in many important applications in 3D computer vision and graphics. In this paper, we focus on 3D deformable shapes that share a common topological structure, such as human faces and bodies. Morphable Models and their variants, despite their linear formulation, have been widely used for shape representation, while most of the recently proposed nonlinear approaches resort to intermediate representations, such as 3D voxel grids or 2D views. In this work, we introduce a novel graph convolutional operator, acting directly on the 3D mesh, that explicitly models the inductive bias of the fixed underlying graph. This is achieved by enforcing consistent local orderings of the vertices of the graph, through the spiral operator, thus breaking the permutation invariance property that is adopted by all the prior work on Graph Neural Networks. Our operator comes by construction with desirable properties (anisotropic, topology-aware, lightweight, easy-to-optimise), and by using it as a building block for traditional deep generative architectures, we demonstrate state-of-the-art results on a variety of 3D shape datasets compared to the linear Morphable Model and other graph convolutional operators.",0
"The use of generative models in 3D computer vision and graphics is widespread, particularly for 3D geometric data. This study focuses on 3D deformable shapes that have a shared topological structure, such as human faces and bodies. Morphable Models are frequently used for shape representation, despite their linear formulation, while more recent nonlinear approaches utilize intermediate representations such as 3D voxel grids or 2D views. The authors introduce a novel graph convolutional operator that acts directly on the 3D mesh and explicitly models the inductive bias of the fixed underlying graph. This is achieved by enforcing consistent local orderings of the vertices of the graph, breaking the permutation invariance property adopted by prior work on Graph Neural Networks. The operator has desirable properties, including anisotropy, topology awareness, lightweightness, and easy optimization. By using it as a building block for traditional deep generative architectures, the authors demonstrate state-of-the-art results on a variety of 3D shape datasets compared to the linear Morphable Model and other graph convolutional operators.",1
"Training deep models for lane detection is challenging due to the very subtle and sparse supervisory signals inherent in lane annotations. Without learning from much richer context, these models often fail in challenging scenarios, e.g., severe occlusion, ambiguous lanes, and poor lighting conditions. In this paper, we present a novel knowledge distillation approach, i.e., Self Attention Distillation (SAD), which allows a model to learn from itself and gains substantial improvement without any additional supervision or labels. Specifically, we observe that attention maps extracted from a model trained to a reasonable level would encode rich contextual information. The valuable contextual information can be used as a form of 'free' supervision for further representation learning through performing topdown and layer-wise attention distillation within the network itself. SAD can be easily incorporated in any feedforward convolutional neural networks (CNN) and does not increase the inference time. We validate SAD on three popular lane detection benchmarks (TuSimple, CULane and BDD100K) using lightweight models such as ENet, ResNet-18 and ResNet-34. The lightest model, ENet-SAD, performs comparatively or even surpasses existing algorithms. Notably, ENet-SAD has 20 x fewer parameters and runs 10 x faster compared to the state-of-the-art SCNN, while still achieving compelling performance in all benchmarks. Our code is available at https://github.com/cardwing/Codes-for-Lane-Detection.",0
"Lane detection using deep models is difficult due to the limited and subtle supervisory signals present in lane annotations, leading to models failing in challenging scenarios such as poor lighting conditions, ambiguous lanes, and severe occlusion. In this paper, we propose a novel knowledge distillation method called Self Attention Distillation (SAD), which allows a model to learn from itself and gain substantial improvement without additional supervision or labels. We discovered that attention maps extracted from a reasonably trained model contain valuable contextual information that can be used as a form of ""free"" supervision for further representation learning through top-down and layer-wise attention distillation within the network itself. SAD can be easily integrated into any feedforward convolutional neural network (CNN) without affecting inference time. We validate SAD on three lane detection benchmarks using lightweight models such as ENet, ResNet-18, and ResNet-34. Our lightest model, ENet-SAD, performs similarly or even surpasses existing algorithms while having significantly fewer parameters and faster running times than the state-of-the-art SCNN. Our code is available at https://github.com/cardwing/Codes-for-Lane-Detection.",1
"With higher-order neighborhood information of graph network, the accuracy of graph representation learning classification can be significantly improved. However, the current higher order graph convolutional network has a large number of parameters and high computational complexity. Therefore, we propose a Hybrid Lower order and Higher order Graph convolutional networks (HLHG) learning model, which uses weight sharing mechanism to reduce the number of network parameters. To reduce computational complexity, we propose a novel fusion pooling layer to combine the neighborhood information of high order and low order. Theoretically, we compare the model complexity of the proposed model with the other state-of-the-art model. Experimentally, we verify the proposed model on the large-scale text network datasets by supervised learning, and on the citation network datasets by semi-supervised learning. The experimental results show that the proposed model achieves highest classification accuracy with a small set of trainable weight parameters.",0
"The accuracy of graph representation learning classification can be greatly enhanced by incorporating higher-order neighborhood information of graph network. However, currently available higher order graph convolutional network tend to have a large number of parameters and high computational complexity. Thus, we have introduced a learning model called Hybrid Lower order and Higher order Graph convolutional networks (HLHG), that leverages a weight sharing mechanism to reduce the number of network parameters. Furthermore, we have proposed a fusion pooling layer to merge the neighborhood information of high order and low order, thereby minimizing computational complexity. We have compared the model complexity of the HLHG with other state-of-the-art models and have conducted experiments on large-scale text network datasets through supervised learning and citation network datasets by semi-supervised learning. Our experimental results demonstrate that the HLHG achieves the highest classification accuracy with a small set of trainable weight parameters.",1
"We introduce a novel method to combat label noise when training deep neural networks for classification. We propose a loss function that permits abstention during training thereby allowing the DNN to abstain on confusing samples while continuing to learn and improve classification performance on the non-abstained samples. We show how such a deep abstaining classifier (DAC) can be used for robust learning in the presence of different types of label noise. In the case of structured or systematic label noise -- where noisy training labels or confusing examples are correlated with underlying features of the data-- training with abstention enables representation learning for features that are associated with unreliable labels. In the case of unstructured (arbitrary) label noise, abstention during training enables the DAC to be used as an effective data cleaner by identifying samples that are likely to have label noise. We provide analytical results on the loss function behavior that enable dynamic adaption of abstention rates based on learning progress during training. We demonstrate the utility of the deep abstaining classifier for various image classification tasks under different types of label noise; in the case of arbitrary label noise, we show significant improvements over previously published results on multiple image benchmarks. Source code is available at https://github.com/thulas/dac-label-noise",0
"Our research introduces a novel approach to address label noise during the training of deep neural networks for classification. Our proposed loss function allows for abstention during training, enabling the DNN to abstain from confusing samples while still enhancing classification performance on non-abstained samples. We demonstrate that this deep abstaining classifier (DAC) can be used to achieve robust learning despite different types of label noise. When dealing with structured or systematic label noise, abstention during training facilitates representation learning for features associated with unreliable labels. Conversely, when dealing with unstructured (arbitrary) label noise, the DAC can be used as an effective data cleaner by identifying samples with likely label noise. We also provide analytical results on the loss function's behavior, which enables dynamic adaptation of abstention rates based on learning progress during training. Furthermore, we demonstrate the DAC's efficacy in various image classification tasks under different types of label noise, showcasing significant improvements over previously published results for arbitrary label noise. Our source code is available on https://github.com/thulas/dac-label-noise.",1
"Graph neural networks denote a group of neural network models introduced for the representation learning tasks on graph data specifically. Graph neural networks have been demonstrated to be effective for capturing network structure information, and the learned representations can achieve the state-of-the-art performance on node and graph classification tasks. Besides the different application scenarios, the architectures of graph neural network models also depend on the studied graph types a lot. Graph data studied in research can be generally categorized into two main types, i.e., small graphs vs. giant networks, which differ from each other a lot in the size, instance number and label annotation. Several different types of graph neural network models have been introduced for learning the representations from such different types of graphs already. In this paper, for these two different types of graph data, we will introduce the graph neural networks introduced in recent years. To be more specific, the graph neural networks introduced in this paper include IsoNN, SDBN, LF&ER, GCN, GAT, DifNN, GNL, GraphSage and seGEN. Among these graph neural network models, IsoNN, SDBN and LF&ER are initially proposed for small graphs and the remaining ones are initially proposed for giant networks instead. The readers are also suggested to refer to these papers for detailed information when reading this tutorial paper.",0
"Graph neural networks are a type of neural network models designed specifically for representation learning tasks on graph data. They have been proven to be effective in capturing network structure information, and the learned representations can achieve state-of-the-art performance in node and graph classification tasks. The architecture of graph neural network models depends heavily on the type of graph being studied, which can be categorized into small graphs and giant networks. Different types of graph neural network models have been introduced for learning representations from these different types of graphs. This paper introduces several graph neural networks, including IsoNN, SDBN, LF&ER, GCN, GAT, DifNN, GNL, GraphSage and seGEN, which are specifically designed for either small graphs or giant networks. Detailed information on these models can be found in the respective papers.",1
"Deep learning methods are successfully used in applications pertaining to ubiquitous computing, health, and well-being. Specifically, the area of human activity recognition (HAR) is primarily transformed by the convolutional and recurrent neural networks, thanks to their ability to learn semantic representations from raw input. However, to extract generalizable features, massive amounts of well-curated data are required, which is a notoriously challenging task; hindered by privacy issues, and annotation costs. Therefore, unsupervised representation learning is of prime importance to leverage the vast amount of unlabeled data produced by smart devices. In this work, we propose a novel self-supervised technique for feature learning from sensory data that does not require access to any form of semantic labels. We learn a multi-task temporal convolutional network to recognize transformations applied on an input signal. By exploiting these transformations, we demonstrate that simple auxiliary tasks of the binary classification result in a strong supervisory signal for extracting useful features for the downstream task. We extensively evaluate the proposed approach on several publicly available datasets for smartphone-based HAR in unsupervised, semi-supervised, and transfer learning settings. Our method achieves performance levels superior to or comparable with fully-supervised networks, and it performs significantly better than autoencoders. Notably, for the semi-supervised case, the self-supervised features substantially boost the detection rate by attaining a kappa score between 0.7-0.8 with only 10 labeled examples per class. We get similar impressive performance even if the features are transferred from a different data source. While this paper focuses on HAR as the application domain, the proposed technique is general and could be applied to a wide variety of problems in other areas.",0
"The use of deep learning techniques has proven to be effective in various fields such as ubiquitous computing, health, and well-being. Human activity recognition (HAR) has particularly benefited from convolutional and recurrent neural networks, which have the ability to learn meaningful representations from raw data. However, obtaining large amounts of quality data is a difficult task due to privacy concerns and annotation expenses. Therefore, unsupervised representation learning is crucial in utilizing the vast quantities of unlabelled data produced by smart devices. In this study, we introduce a new self-supervised approach for feature learning from sensory data that requires no semantic labels. By using a multi-task temporal convolutional network to recognize transformations applied to input signals, we show that simple auxiliary tasks of binary classification can provide a powerful supervisory signal for feature extraction. We extensively evaluate our method on publicly available datasets for smartphone-based HAR in various settings and achieve superior or comparable performance to fully-supervised networks and significantly better results than autoencoders. Notably, even with only 10 labeled examples per class, our self-supervised features improve detection rates with a kappa score of 0.7-0.8 in the semi-supervised case. This technique is not limited to HAR and can be applied to numerous other problems in different fields.",1
"Predicting bioactivity and physical properties of small molecules is a central challenge in drug discovery. Deep learning is becoming the method of choice but studies to date focus on mean accuracy as the main metric. However, to replace costly and mission-critical experiments by models, a high mean accuracy is not enough: Outliers can derail a discovery campaign, thus models need reliably predict when it will fail, even when the training data is biased; experiments are expensive, thus models need to be data-efficient and suggest informative training sets using active learning. We show that uncertainty quantification and active learning can be achieved by Bayesian semi-supervised graph convolutional neural networks. The Bayesian approach estimates uncertainty in a statistically principled way through sampling from the posterior distribution. Semi-supervised learning disentangles representation learning and regression, keeping uncertainty estimates accurate in the low data limit and allowing the model to start active learning from a small initial pool of training data. Our study highlights the promise of Bayesian deep learning for chemistry.",0
"Drug discovery faces a core challenge in predicting the bioactivity and physical properties of small molecules. While deep learning is emerging as the preferred method, studies have thus far focused on mean accuracy as the primary metric. However, high mean accuracy is insufficient to replace costly experiments with models because outliers can derail a discovery campaign. Consequently, models must predict failures reliably, even when the training data is biased, and be data-efficient to recommend informative training sets using active learning. This study demonstrates how Bayesian semi-supervised graph convolutional neural networks can achieve uncertainty quantification and active learning. The Bayesian approach uses sampling from the posterior distribution to estimate uncertainty in a statistically principled manner. Semi-supervised learning separates representation learning and regression, ensuring accurate uncertainty estimates in low-data situations and allowing the model to start active learning from a small initial pool of training data. This research underscores the potential of Bayesian deep learning for chemistry.",1
"Deep reinforcement learning has achieved great successes in recent years, but there are still open challenges, such as convergence to locally optimal policies and sample inefficiency. In this paper, we contribute a novel self-supervised auxiliary task, i.e., Terminal Prediction (TP), estimating temporal closeness to terminal states for episodic tasks. The intuition is to help representation learning by letting the agent predict how close it is to a terminal state, while learning its control policy. Although TP could be integrated with multiple algorithms, this paper focuses on Asynchronous Advantage Actor-Critic (A3C) and demonstrating the advantages of A3C-TP. Our extensive evaluation includes: a set of Atari games, the BipedalWalker domain, and a mini version of the recently proposed multi-agent Pommerman game. Our results on Atari games and the BipedalWalker domain suggest that A3C-TP outperforms standard A3C in most of the tested domains and in others it has similar performance. In Pommerman, our proposed method provides significant improvement both in learning efficiency and converging to better policies against different opponents.",0
"While deep reinforcement learning has made impressive strides in recent years, there remain unresolved challenges such as struggles with locally optimal policies and inefficiency in sample collection. To address these issues, our paper introduces a new self-supervised auxiliary task called Terminal Prediction (TP), which estimates the temporal proximity to terminal states for episodic tasks. By allowing the agent to predict its proximity to a terminal state while developing its control policy, TP enhances representation learning. Although TP can be integrated with various algorithms, our paper focuses on Asynchronous Advantage Actor-Critic (A3C) and highlights the benefits of A3C-TP. Our extensive evaluation encompasses several Atari games, the BipedalWalker domain, and a mini version of the recently proposed multi-agent Pommerman game. Our results indicate that A3C-TP yields superior performance to standard A3C in most of the tested domains, and in others, it performs comparably. In Pommerman, our proposed method significantly improves learning efficiency and policy convergence against different opponents.",1
"This work tackles the problem of semi-supervised learning of image classifiers. Our main insight is that the field of semi-supervised learning can benefit from the quickly advancing field of self-supervised visual representation learning. Unifying these two approaches, we propose the framework of self-supervised semi-supervised learning and use it to derive two novel semi-supervised image classification methods. We demonstrate the effectiveness of these methods in comparison to both carefully tuned baselines, and existing semi-supervised learning methods. We then show that our approach and existing semi-supervised methods can be jointly trained, yielding a new state-of-the-art result on semi-supervised ILSVRC-2012 with 10% of labels.",0
"The focus of this study is on the issue of developing image classifiers through semi-supervised learning. Our main contribution is recognizing the potential synergy between self-supervised visual representation learning and semi-supervised learning. By combining these two approaches, we introduce the concept of self-supervised semi-supervised learning, which leads to the development of two innovative semi-supervised image classification techniques. Our experimental results indicate that these methods outperform both existing semi-supervised learning techniques and carefully tuned baselines. Additionally, we demonstrate that our proposed approach can be used in conjunction with existing semi-supervised methods to achieve state-of-the-art results on semi-supervised ILSVRC-2012 with only 10% of labels.",1
"Clustering multi-view data has been a fundamental research topic in the computer vision community. It has been shown that a better accuracy can be achieved by integrating information of all the views than just using one view individually. However, the existing methods often struggle with the issues of dealing with the large-scale datasets and the poor performance in reconstructing samples. This paper proposes a novel multi-view clustering method by learning a shared generative latent representation that obeys a mixture of Gaussian distributions. The motivation is based on the fact that the multi-view data share a common latent embedding despite the diversity among the views. Specifically, benefited from the success of the deep generative learning, the proposed model not only can extract the nonlinear features from the views, but render a powerful ability in capturing the correlations among all the views. The extensive experimental results, on several datasets with different scales, demonstrate that the proposed method outperforms the state-of-the-art methods under a range of performance criteria.",0
"The computer vision community has long focused on clustering multi-view data, as it has been shown to yield higher accuracy than relying on a single view. However, current methods face challenges when dealing with large datasets and reconstructing samples. To address these issues, this paper introduces a new multi-view clustering approach that learns a shared generative latent representation based on a mixture of Gaussian distributions. The underlying premise is that, despite differences between views, multi-view data share a common latent embedding. Leveraging deep generative learning, the proposed model not only extracts nonlinear features from views but also captures correlations among all views. Experiments on various datasets demonstrate superior performance compared to state-of-the-art methods.",1
"The success of deep learning in medical imaging is mostly achieved at the cost of a large labeled data set. Semi-supervised learning (SSL) provides a promising solution by leveraging the structure of unlabeled data to improve learning from a small set of labeled data. Self-ensembling is a simple approach used in SSL to encourage consensus among ensemble predictions of unknown labels, improving generalization of the model by making it more insensitive to the latent space. Currently, such an ensemble is obtained by randomization such as dropout regularization and random data augmentation. In this work, we hypothesize -- from the generalization perspective -- that self-ensembling can be improved by exploiting the stochasticity of a disentangled latent space. To this end, we present a stacked SSL model that utilizes unsupervised disentangled representation learning as the stochastic embedding for self-ensembling. We evaluate the presented model for multi-label classification using chest X-ray images, demonstrating its improved performance over related SSL models as well as the interpretability of its disentangled representations.",0
"Deep learning has been successful in medical imaging, but it requires a large labeled data set. Semi-supervised learning (SSL) is a promising solution that utilizes the structure of unlabeled data to improve learning from a small set of labeled data. Self-ensembling is a simple approach used in SSL to improve generalization of the model and make it more insensitive to the latent space by encouraging consensus among ensemble predictions of unknown labels. Currently, an ensemble is obtained by randomization such as dropout regularization and random data augmentation. In this study, we propose that self-ensembling can be improved by exploiting the stochasticity of a disentangled latent space from a generalization perspective. We present a stacked SSL model that utilizes unsupervised disentangled representation learning as the stochastic embedding for self-ensembling. We evaluate the model for multi-label classification using chest X-ray images, demonstrating improved performance over related SSL models and the interpretability of disentangled representations.",1
"Learning representations that can disentangle explanatory attributes underlying the data improves interpretabilty as well as provides control on data generation. Various learning frameworks such as VAEs, GANs and auto-encoders have been used in the literature to learn such representations. Most often, the latent space is constrained to a partitioned representation or structured by a prior to impose disentangling. In this work, we advance the use of a latent representation based on a product space of Orthogonal Spheres PrOSe. The PrOSe model is motivated by the reasoning that latent-variables related to the physics of image-formation can under certain relaxed assumptions lead to spherical-spaces. Orthogonality between the spheres is motivated via physical independence models. Imposing the orthogonal-sphere constraint is much simpler than other complicated physical models, is fairly general and flexible, and extensible beyond the factors used to motivate its development. Under further relaxed assumptions of equal-sized latent blocks per factor, the constraint can be written down in closed form as an ortho-normality term in the loss function. We show that our approach improves the quality of disentanglement significantly. We find consistent improvement in disentanglement compared to several state-of-the-art approaches, across several benchmarks and metrics.",0
"Improving interpretability and control in data generation can be achieved by learning representations that disentangle explanatory attributes. Existing literature has employed various learning frameworks, including VAEs, GANs, and auto-encoders, to accomplish this. The latent space is often partitioned or structured to impose disentangling through priors. In this study, we propose a latent representation based on a product space of Orthogonal Spheres PrOSe, which is motivated by the assumption that latent variables related to image-formation physics can lead to spherical-spaces. Orthogonality between the spheres is enforced through physical independence models, which is simpler and more flexible than other physical models. With relaxed assumptions of equal-sized latent blocks per factor, the constraint can be expressed as an ortho-normality term in the loss function. Our approach significantly improves the quality of disentanglement and consistently outperforms several state-of-the-art methods across various benchmarks and metrics.",1
"Representation learning methods that transform encoded data (e.g., diagnosis and drug codes) into continuous vector spaces (i.e., vector embeddings) are critical for the application of deep learning in healthcare. Initial work in this area explored the use of variants of the word2vec algorithm to learn embeddings for medical concepts from electronic health records or medical claims datasets. We propose learning embeddings for medical concepts by using graph-based representation learning methods on SNOMED-CT, a widely popular knowledge graph in the healthcare domain with numerous operational and research applications. Current work presents an empirical analysis of various embedding methods, including the evaluation of their performance on multiple tasks of biomedical relevance (node classification, link prediction, and patient state prediction). Our results show that concept embeddings derived from the SNOMED-CT knowledge graph significantly outperform state-of-the-art embeddings, showing 5-6x improvement in ``concept similarity"" and 6-20\% improvement in patient diagnosis.",0
"In order to apply deep learning in healthcare, it is crucial to have representation learning techniques that can transform encoded data, such as diagnosis and drug codes, into continuous vector spaces known as vector embeddings. Early research in this field focused on word2vec algorithm variations to learn embeddings for medical concepts from electronic health records or medical claims datasets. However, we propose a different approach, using graph-based representation learning methods on SNOMED-CT, a widely-used knowledge graph in the healthcare industry with various operational and research applications. Our study involves an empirical analysis of different embedding methods and their performance on biomedical tasks, such as node classification, link prediction, and patient state prediction. Our findings indicate that concept embeddings derived from the SNOMED-CT knowledge graph are superior to state-of-the-art embeddings, with a 5-6x increase in ""concept similarity"" and a 6-20% improvement in patient diagnosis.",1
"Click-through rate (CTR) prediction is a critical task in online advertising systems. Most existing methods mainly model the feature-CTR relationship and suffer from the data sparsity issue. In this paper, we propose DeepMCP, which models other types of relationships in order to learn more informative and statistically reliable feature representations, and in consequence to improve the performance of CTR prediction. In particular, DeepMCP contains three parts: a matching subnet, a correlation subnet and a prediction subnet. These subnets model the user-ad, ad-ad and feature-CTR relationship respectively. When these subnets are jointly optimized under the supervision of the target labels, the learned feature representations have both good prediction powers and good representation abilities. Experiments on two large-scale datasets demonstrate that DeepMCP outperforms several state-of-the-art models for CTR prediction.",0
"Online advertising systems rely heavily on accurate click-through rate (CTR) prediction, but current methods struggle with sparse data due to their focus on modeling only the feature-CTR relationship. To address this, we propose DeepMCP, a model that considers multiple relationship types to generate more informative and statistically reliable feature representations and improve CTR prediction performance. DeepMCP comprises three subnets: a matching subnet for user-ad relationships, a correlation subnet for ad-ad relationships, and a prediction subnet for feature-CTR relationships. Jointly optimizing these subnets yields feature representations with strong predictive power and representation abilities. Our experiments on large-scale datasets show that DeepMCP outperforms several state-of-the-art CTR prediction models.",1
"The Information Bottleneck (IB) method (\cite{tishby2000information}) provides an insightful and principled approach for balancing compression and prediction for representation learning. The IB objective $I(X;Z)-\beta I(Y;Z)$ employs a Lagrange multiplier $\beta$ to tune this trade-off. However, in practice, not only is $\beta$ chosen empirically without theoretical guidance, there is also a lack of theoretical understanding between $\beta$, learnability, the intrinsic nature of the dataset and model capacity. In this paper, we show that if $\beta$ is improperly chosen, learning cannot happen -- the trivial representation $P(Z|X)=P(Z)$ becomes the global minimum of the IB objective. We show how this can be avoided, by identifying a sharp phase transition between the unlearnable and the learnable which arises as $\beta$ is varied. This phase transition defines the concept of IB-Learnability. We prove several sufficient conditions for IB-Learnability, which provides theoretical guidance for choosing a good $\beta$. We further show that IB-learnability is determined by the largest confident, typical, and imbalanced subset of the examples (the conspicuous subset), and discuss its relation with model capacity. We give practical algorithms to estimate the minimum $\beta$ for a given dataset. We also empirically demonstrate our theoretical conditions with analyses of synthetic datasets, MNIST, and CIFAR10.",0
"The Information Bottleneck (IB) method, introduced in \cite{tishby2000information}, presents a useful and systematic approach for representation learning that balances compression and prediction. By employing a Lagrange multiplier $\beta$, the IB objective $I(X;Z)-\beta I(Y;Z)$ achieves this balance. However, choosing an appropriate value for $\beta$ remains a challenge in practice, as there is no theoretical guidance available, and the relationship between $\beta$, learnability, dataset characteristics, and model capacity is not well understood. In this paper, we demonstrate that improper selection of $\beta$ results in learning failure, with the trivial representation $P(Z|X)=P(Z)$ becoming the global minimum of the IB objective. To overcome this issue, we identify a sharp phase transition between unlearnable and learnable states as $\beta$ is adjusted, which we term IB-Learnability. We prove several sufficient conditions for IB-Learnability that provide guidance for selecting a suitable $\beta$. Our analysis reveals that IB-Learnability is determined by the largest conspicuous subset of examples - the most confident, typical, and imbalanced subset - and its connection to model capacity. We present practical algorithms to estimate the minimum $\beta$ for a given dataset and demonstrate the effectiveness of our theoretical results through experiments on synthetic datasets, MNIST, and CIFAR10.",1
"Financial transactions can be considered edges in a heterogeneous graph between entities sending money and entities receiving money. For financial institutions, such a graph is likely large (with millions or billions of edges) while also sparsely connected. It becomes challenging to apply machine learning to such large and sparse graphs. Graph representation learning seeks to embed the nodes of a graph into a Euclidean vector space such that graph topological properties are preserved after the transformation. In this paper, we present a novel application of representation learning to bipartite graphs of credit card transactions in order to learn embeddings of account and merchant entities. Our framework is inspired by popular approaches in graph embeddings and is trained on two internal transaction datasets. This approach yields highly effective embeddings, as quantified by link prediction AUC and F1 score. Further, the resulting entity vectors retain intuitive semantic similarity that is explored through visualizations and other qualitative analyses. Finally, we show how these embeddings can be used as features in downstream machine learning business applications such as fraud detection.",0
"A heterogeneous graph can depict financial transactions as edges connecting entities that send and receive money. Financial institutions may have large graphs with millions or billions of edges that are sparsely connected, making it difficult to apply machine learning. Graph representation learning aims to transform the nodes of a graph into a Euclidean vector space while preserving its topological properties. Our paper proposes a new application of representation learning to bipartite graphs of credit card transactions, embedding account and merchant entities. We use popular approaches in graph embeddings and train our framework on two internal transaction datasets. Our approach yields highly effective embeddings, as measured by link prediction AUC and F1 score. The resulting entity vectors display intuitive semantic similarity that we explore through visualizations and qualitative analyses. We demonstrate how these embeddings can be utilized as features in downstream machine learning business applications such as fraud detection.",1
"Risk adjustment has become an increasingly important tool in healthcare. It has been extensively applied to payment adjustment for health plans to reflect the expected cost of providing coverage for members. Risk adjustment models are typically estimated using linear regression, which does not fully exploit the information in claims data. Moreover, the development of such linear regression models requires substantial domain expert knowledge and computational effort for data preprocessing. In this paper, we propose a novel approach for risk adjustment that uses semantic embeddings to represent patient medical histories. Embeddings efficiently represent medical concepts learned from diagnostic, procedure, and prescription codes in patients' medical histories. This approach substantially reduces the need for feature engineering. Our results show that models using embeddings had better performance than a commercial risk adjustment model on the task of prospective risk score prediction.",0
"Healthcare has seen an increase in the importance of risk adjustment, especially in adjusting payment for health plans to reflect the expected cost of coverage for members. Linear regression is commonly used to estimate risk adjustment models, but it does not fully utilize claims data and requires significant domain knowledge and computational effort. This research proposes a new approach that uses semantic embeddings to represent patient medical histories, which efficiently captures medical concepts from diagnostic, procedure, and prescription codes. This approach decreases the need for feature engineering and outperformed a commercial risk adjustment model in predicting prospective risk scores.",1
"In multi-task reinforcement learning there are two main challenges: at training time, the ability to learn different policies with a single model; at test time, inferring which of those policies applying without an external signal. In the case of continual reinforcement learning a third challenge arises: learning tasks sequentially without forgetting the previous ones. In this paper, we tackle these challenges by proposing DisCoRL, an approach combining state representation learning and policy distillation. We experiment on a sequence of three simulated 2D navigation tasks with a 3 wheel omni-directional robot. Moreover, we tested our approach's robustness by transferring the final policy into a real life setting. The policy can solve all tasks and automatically infer which one to run.",0
"The primary difficulties in multi-task reinforcement learning involve learning multiple policies with a single model during training, and determining which policy to apply without external guidance during testing. Continual reinforcement learning adds a third challenge: learning tasks sequentially without forgetting previous ones. This paper introduces DisCoRL, an approach that combines policy distillation and state representation learning to address these challenges. The approach is tested on three simulated 2D navigation tasks using a 3 wheel omni-directional robot. Additionally, the final policy is transferred to a real-life scenario to test its robustness, demonstrating its ability to automatically solve all tasks and select the appropriate one.",1
"Estimating average causal effect (ACE) is useful whenever we want to know the effect of an intervention on a given outcome. In the absence of a randomized experiment, many methods such as stratification and inverse propensity weighting have been proposed to estimate ACE. However, it is hard to know which method is optimal for a given dataset or which hyperparameters to use for a chosen method. To this end, we provide a framework to characterize the loss of a causal inference method against the true ACE, by framing causal inference as a representation learning problem. We show that many popular methods, including back-door methods can be considered as weighting or representation learning algorithms, and provide general error bounds for their causal estimates. In addition, we consider the case when unobserved variables can confound the causal estimate and extend proposed bounds using principles of robust statistics, considering confounding as contamination under the Huber contamination model. These bounds are also estimable; as an example, we provide empirical bounds for the Inverse Propensity Weighting (IPW) estimator and show how the bounds can be used to optimize the threshold of clipping extreme propensity scores. Our work provides a new way to reason about competing estimators, and opens up the potential of deriving new methods by minimizing the proposed error bounds.",0
"When we want to determine the impact of an intervention on a particular outcome, it is beneficial to estimate the average causal effect (ACE). In cases where a randomized experiment is not possible, several methods, including stratification and inverse propensity weighting, have been suggested to estimate ACE. However, it can be challenging to identify the most suitable method for a specific dataset or determine the appropriate hyperparameters for a chosen method. To address this issue, we propose a framework for characterizing the loss of a causal inference method against the true ACE by framing causal inference as a representation learning problem. We demonstrate that several popular methods, such as back-door methods, can be viewed as weighting or representation learning algorithms. Furthermore, we provide general error bounds for their causal estimates. We also consider situations where unobserved variables can confound the causal estimate and extend proposed bounds using principles of robust statistics, treating confounding as contamination under the Huber contamination model. These bounds are estimable, and we provide empirical bounds for the Inverse Propensity Weighting (IPW) estimator as an example. We illustrate how the bounds can be used to optimize the threshold of clipping extreme propensity scores. Our work provides a novel approach to comparing estimators and opens up the potential for developing new methods by minimizing the proposed error bounds.",1
"We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views -- e.g., presence of certain objects or occurrence of certain events.   Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.",0
"Our approach to self-supervised representation learning involves maximizing mutual information between features obtained from various perspectives of a common context. This could include generating multiple views of a local spatio-temporal context by observing it from different locations and through various modalities, or producing multiple views of an ImageNet image using data augmentation. To achieve this, we must capture information about high-level factors that span across multiple views, such as the presence of certain objects or events. By implementing our approach, we have developed a model that outperforms previous methods on the tasks we have considered, achieving 68.1% accuracy on ImageNet using standard linear evaluation. This represents an improvement of over 12% on prior results and 7% on concurrent results. Our model can also produce segmentation behavior when using mixture-based representations. Interested parties can access our code on GitHub at https://github.com/Philip-Bachman/amdim-public.",1
"Attention operators have been widely applied in various fields, including computer vision, natural language processing, and network embedding learning. Attention operators on graph data enables learnable weights when aggregating information from neighboring nodes. However, graph attention operators (GAOs) consume excessive computational resources, preventing their applications on large graphs. In addition, GAOs belong to the family of soft attention, instead of hard attention, which has been shown to yield better performance. In this work, we propose novel hard graph attention operator (hGAO) and channel-wise graph attention operator (cGAO). hGAO uses the hard attention mechanism by attending to only important nodes. Compared to GAO, hGAO improves performance and saves computational cost by only attending to important nodes. To further reduce the requirements on computational resources, we propose the cGAO that performs attention operations along channels. cGAO avoids the dependency on the adjacency matrix, leading to dramatic reductions in computational resource requirements. Experimental results demonstrate that our proposed deep models with the new operators achieve consistently better performance. Comparison results also indicates that hGAO achieves significantly better performance than GAO on both node and graph embedding tasks. Efficiency comparison shows that our cGAO leads to dramatic savings in computational resources, making them applicable to large graphs.",0
"Various fields have widely applied attention operators, such as computer vision, natural language processing, and network embedding learning. When used on graph data, attention operators allow for learnable weights while aggregating information from neighboring nodes. However, the soft attention mechanism used in graph attention operators (GAOs) leads to excessive computational resource consumption, limiting their applications on large graphs and hindering their performance compared to hard attention. This study introduces two novel graph attention operators: the hard graph attention operator (hGAO), which only attends to important nodes and improves performance while saving computational costs, and the channel-wise graph attention operator (cGAO), which avoids the dependency on the adjacency matrix and drastically reduces computational requirements. Experimental results show that the proposed deep models with these new operators consistently achieve better performance, with hGAO performing significantly better than GAO on both node and graph embedding tasks. Additionally, cGAO's efficiency makes it applicable to large graphs.",1
"In linear inverse problems, the goal is to recover a target signal from undersampled, incomplete or noisy linear measurements. Typically, the recovery relies on complex numerical optimization methods; recent approaches perform an unfolding of a numerical algorithm into a neural network form, resulting in a substantial reduction of the computational complexity. In this paper, we consider the recovery of a target signal with the aid of a correlated signal, the so-called side information (SI), and propose a deep unfolding model that incorporates SI. The proposed model is used to learn coupled representations of correlated signals from different modalities, enabling the recovery of multimodal data at a low computational cost. As such, our work introduces the first deep unfolding method with SI, which actually comes from a different modality. We apply our model to reconstruct near-infrared images from undersampled measurements given RGB images as SI. Experimental results demonstrate the superior performance of the proposed framework against single-modal deep learning methods that do not use SI, multimodal deep learning designs, and optimization algorithms.",0
"The aim of linear inverse problems is to retrieve a target signal from partial, noisy, or insufficient linear measurements. This is usually done using complicated numerical optimization methods. However, recent techniques have attempted to simplify this process by converting a numerical algorithm into a neural network format, resulting in a decrease in computational complexity. This study focuses on retrieving the target signal by incorporating a correlated signal, known as side information (SI), using a deep unfolding model. The proposed model enables the retrieval of multimodal data at a low computational cost by jointly learning representations of correlated signals from various modalities. The study also introduces the first deep unfolding method that utilizes SI from a different modality. We apply this model to reconstruct near-infrared images using undersampled measurements with RGB images as SI. Our experimental results demonstrate that the proposed framework outperforms single-modal deep learning methods that do not use SI, multimodal deep learning designs, and optimization algorithms.",1
"Continuous symmetries and their breaking play a prominent role in contemporary physics. Effective low-energy field theories around symmetry breaking states explain diverse phenomena such as superconductivity, magnetism, and the mass of nucleons. We show that such field theories can also be a useful tool in machine learning, in particular for loss functions with continuous symmetries that are spontaneously broken by random initializations. In this paper, we illuminate our earlier published work (Bamler & Mandt, 2018) on this topic more from the perspective of theoretical physics. We show that the analogies between superconductivity and symmetry breaking in temporal representation learning are rather deep, allowing us to formulate a gauge theory of `charged' embedding vectors in time series models. We show that making the loss function gauge invariant speeds up convergence in such models.",0
"Symmetries that are continuous and their breaking are crucial in modern physics. Low-energy field theories are effective in explaining various phenomena such as superconductivity, magnetism, and nucleon mass around symmetry breaking states. Our research demonstrates that these field theories can also be a valuable instrument in machine learning, particularly for loss functions with continuous symmetries that are spontaneously broken by random initializations. This paper delves deeper into our previously published work (Bamler & Mandt, 2018) and highlights the theoretical physics perspective. We have discovered that the parallels between superconductivity and symmetry breaking in temporal representation learning are significant. This has allowed us to devise a gauge theory of 'charged' embedding vectors in time series models. We have also shown that making the loss function gauge invariant hastens convergence in such models.",1
"This paper concerns dictionary learning, i.e., sparse coding, a fundamental representation learning problem. We show that a subgradient descent algorithm, with random initialization, can provably recover orthogonal dictionaries on a natural nonsmooth, nonconvex $\ell_1$ minimization formulation of the problem, under mild statistical assumptions on the data. This is in contrast to previous provable methods that require either expensive computation or delicate initialization schemes. Our analysis develops several tools for characterizing landscapes of nonsmooth functions, which might be of independent interest for provable training of deep networks with nonsmooth activations (e.g., ReLU), among numerous other applications. Preliminary experiments corroborate our analysis and show that our algorithm works well empirically in recovering orthogonal dictionaries.",0
"The focus of this article is on sparse coding, or dictionary learning, which is a crucial aspect of representation learning. We present evidence that a subgradient descent algorithm, with random initialization, can successfully reconstruct orthogonal dictionaries using a natural, nonsmooth, nonconvex $\ell_1$ minimization formulation of the problem. Mild statistical assumptions on the data are required for this to be possible. This approach stands in contrast to previous methods that either require expensive computations or intricate initialization schemes. Our analysis includes the development of tools for assessing the landscapes of nonsmooth functions, which could be beneficial for the provable training of deep networks with nonsmooth activations (e.g., ReLU), as well as other applications. Initial experiments support our analysis and demonstrate that our algorithm is effective in recovering orthogonal dictionaries.",1
"Self-supervised methods, wherein an agent learns representations solely by observing the results of its actions, become crucial in environments which do not provide a dense reward signal or have labels. In most cases, such methods are used for pretraining or auxiliary tasks for ""downstream"" tasks, such as control, exploration, or imitation learning. However, it is not clear which method's representations best capture meaningful features of the environment, and which are best suited for which types of environments. We present a small-scale study of self-supervised methods on two visual environments: Flappy Bird and Sonic The Hedgehog. In particular, we quantitatively evaluate the representations learned from these tasks in two contexts: a) the extent to which the representations capture true state information of the agent and b) how generalizable these representations are to novel situations, like new levels and textures. Lastly, we evaluate these self-supervised features by visualizing which parts of the environment they focus on. Our results show that the utility of the representations is highly dependent on the visuals and dynamics of the environment.",0
"In environments where there are no clear rewards or labels, self-supervised methods are important for an agent to learn representations by observing its own actions. These methods are commonly used for pretraining or auxiliary tasks for ""downstream"" tasks like control, exploration, or imitation learning. However, it is uncertain which methods are best suited for different types of environments and which ones capture meaningful features of the environment. To address this, we conducted a small-scale study on two visual environments (Flappy Bird and Sonic The Hedgehog). We assessed the representations learned from these tasks in two ways: how accurately they captured the agent's state information and how adaptable they were to new situations like different levels and textures. We also visualized which parts of the environment the self-supervised features focused on. Our findings revealed that the effectiveness of these representations largely depends on the visual and dynamic characteristics of the environment.",1
"This paper introduces a novel deep learning based method, named bridge neural network (BNN) to dig the potential relationship between two given data sources task by task. The proposed approach employs two convolutional neural networks that project the two data sources into a feature space to learn the desired common representation required by the specific task. The training objective with artificial negative samples is introduced with the ability of mini-batch training and it's asymptotically equivalent to maximizing the total correlation of the two data sources, which is verified by the theoretical analysis. The experiments on the tasks, including pair matching, canonical correlation analysis, transfer learning, and reconstruction demonstrate the state-of-the-art performance of BNN, which may provide new insights into the aspect of common representation learning.",0
"In this paper, a new technique called the bridge neural network (BNN) is presented, which uses deep learning to uncover potential correlations between two given data sources on a task-by-task basis. The method employs two convolutional neural networks to map the data sources onto a feature space, where a shared representation is learned for each specific task. To optimize the training process, artificial negative samples are used with mini-batch training, and the objective is asymptotically equivalent to maximizing the total correlation between the data sources. The efficacy of the BNN approach is demonstrated through experiments on various tasks, including pair matching, canonical correlation analysis, transfer learning, and reconstruction, which show superior performance compared to existing methods and offer new insights into common representation learning.",1
"We propose a new perspective on representation learning in reinforcement learning based on geometric properties of the space of value functions. We leverage this perspective to provide formal evidence regarding the usefulness of value functions as auxiliary tasks. Our formulation considers adapting the representation to minimize the (linear) approximation of the value function of all stationary policies for a given environment. We show that this optimization reduces to making accurate predictions regarding a special class of value functions which we call adversarial value functions (AVFs). We demonstrate that using value functions as auxiliary tasks corresponds to an expected-error relaxation of our formulation, with AVFs a natural candidate, and identify a close relationship with proto-value functions (Mahadevan, 2005). We highlight characteristics of AVFs and their usefulness as auxiliary tasks in a series of experiments on the four-room domain.",0
"A new perspective on representation learning in reinforcement learning is proposed based on the geometric properties of the space of value functions. The aim is to offer formal evidence of the usefulness of value functions as auxiliary tasks. The proposed formulation involves adapting the representation to minimize the linear approximation of the value function of all stationary policies in a given environment. This optimization reduces to accurate predictions of a special class of value functions, termed adversarial value functions (AVF). The use of value functions as auxiliary tasks is shown to correspond to an expected-error relaxation of the formulation, with AVFs being a natural candidate. The relationship with proto-value functions (Mahadevan, 2005) is identified, and the characteristics and usefulness of AVFs as auxiliary tasks are highlighted through a series of experiments on the four-room domain.",1
"With the fast development of various positioning techniques such as Global Position System (GPS), mobile devices and remote sensing, spatio-temporal data has become increasingly available nowadays. Mining valuable knowledge from spatio-temporal data is critically important to many real world applications including human mobility understanding, smart transportation, urban planning, public safety, health care and environmental management. As the number, volume and resolution of spatio-temporal datasets increase rapidly, traditional data mining methods, especially statistics based methods for dealing with such data are becoming overwhelmed. Recently, with the advances of deep learning techniques, deep leaning models such as convolutional neural network (CNN) and recurrent neural network (RNN) have enjoyed considerable success in various machine learning tasks due to their powerful hierarchical feature learning ability in both spatial and temporal domains, and have been widely applied in various spatio-temporal data mining (STDM) tasks such as predictive learning, representation learning, anomaly detection and classification. In this paper, we provide a comprehensive survey on recent progress in applying deep learning techniques for STDM. We first categorize the types of spatio-temporal data and briefly introduce the popular deep learning models that are used in STDM. Then a framework is introduced to show a general pipeline of the utilization of deep learning models for STDM. Next we classify existing literatures based on the types of ST data, the data mining tasks, and the deep learning models, followed by the applications of deep learning for STDM in different domains including transportation, climate science, human mobility, location based social network, crime analysis, and neuroscience. Finally, we conclude the limitations of current research and point out future research directions.",0
"Spatio-temporal data is now readily available due to the advancement of positioning techniques such as GPS, mobile devices, and remote sensing. It is crucial to extract valuable insights from this data for various real-world applications such as urban planning, health care, and environmental management. However, traditional data mining methods are struggling to handle the increasing number, volume, and resolution of spatio-temporal datasets. Recent progress in deep learning techniques, specifically convolutional neural networks (CNN) and recurrent neural networks (RNN), have shown promising results in spatio-temporal data mining tasks. This paper provides a comprehensive survey on the use of deep learning techniques in spatio-temporal data mining. It categorizes the types of spatio-temporal data and introduces popular deep learning models used in this field. Additionally, it presents a framework that outlines the general pipeline for utilizing deep learning models in spatio-temporal data mining. The paper also classifies existing literature based on the types of spatio-temporal data, data mining tasks, and deep learning models. Furthermore, it highlights the applications of deep learning in various domains such as transportation, climate science, human mobility, location-based social networks, crime analysis, and neuroscience. Lastly, the limitations of current research are discussed, and future research directions are suggested.",1
"Scaling end-to-end reinforcement learning to control real robots from vision presents a series of challenges, in particular in terms of sample efficiency. Against end-to-end learning, state representation learning can help learn a compact, efficient and relevant representation of states that speeds up policy learning, reducing the number of samples needed, and that is easier to interpret. We evaluate several state representation learning methods on goal based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning with better sample efficiency, and is robust to hyper-parameters change.",0
"There are obstacles in implementing end-to-end reinforcement learning for controlling real robots through vision, particularly in terms of sample efficiency. However, state representation learning can aid in producing a concise, effective, and significant representation of states that accelerates policy learning, reduces the number of samples required, and is simpler to understand. Our study scrutinizes various state representation learning techniques for goal-based robotics tasks and introduces a new unsupervised model that combines the benefits of several of these approaches by stacking representations. This approach captures all the relevant features, performs comparably or better than end-to-end learning with improved sample efficiency, and is not affected by changes in hyper-parameters.",1
"We consider the problem of imitation learning from expert demonstrations in partially observable Markov decision processes (POMDPs). Belief representations, which characterize the distribution over the latent states in a POMDP, have been modeled using recurrent neural networks and probabilistic latent variable models, and shown to be effective for reinforcement learning in POMDPs. In this work, we investigate the belief representation learning problem for generative adversarial imitation learning in POMDPs. Instead of training the belief module and the policy separately as suggested in prior work, we learn the belief module jointly with the policy, using a task-aware imitation loss to ensure that the representation is more aligned with the policy's objective. To improve robustness of representation, we introduce several informative belief regularization techniques, including multi-step prediction of dynamics and action-sequences. Evaluated on various partially observable continuous-control locomotion tasks, our belief-module imitation learning approach (BMIL) substantially outperforms several baselines, including the original GAIL algorithm and the task-agnostic belief learning algorithm. Extensive ablation analysis indicates the effectiveness of task-aware belief learning and belief regularization.",0
"The article discusses the issue of learning from expert demonstrations in partially observable Markov decision processes (POMDPs). Previous studies have used recurrent neural networks and probabilistic latent variable models to create belief representations, which describe the distribution of latent states in a POMDP. These models have been successful in reinforcement learning in POMDPs. The article focuses on generative adversarial imitation learning in POMDPs and proposes a joint learning approach where the belief module and policy are trained together. The belief representation is improved by implementing informative belief regularization techniques, including multi-step prediction of dynamics and action-sequences. The proposed approach, called belief-module imitation learning (BMIL), outperforms previous methods, including the original GAIL algorithm and the task-agnostic belief learning algorithm, in various partially observable continuous-control locomotion tasks. Extensive analysis confirms the effectiveness of task-aware belief learning and belief regularization.",1
"End-to-end reinforcement learning agents learn a state representation and a policy at the same time. Recurrent neural networks (RNNs) have been trained successfully as reinforcement learning agents in settings like dialogue that require structured prediction. In this paper, we investigate the representations learned by RNN-based agents when trained with both policy gradient and value-based methods. We show through extensive experiments and analysis that, when trained with policy gradient, recurrent neural networks often fail to learn a state representation that leads to an optimal policy in settings where the same action should be taken at different states. To explain this failure, we highlight the problem of state aliasing, which entails conflating two or more distinct states in the representation space. We demonstrate that state aliasing occurs when several states share the same optimal action and the agent is trained via policy gradient. We characterize this phenomenon through experiments on a simple maze setting and a more complex text-based game, and make recommendations for training RNNs with reinforcement learning.",0
"The agents in end-to-end reinforcement learning learn both a state representation and a policy simultaneously. Successful training of recurrent neural networks (RNNs) as reinforcement learning agents has been achieved in structured prediction scenarios such as dialogue. This study examines the learned representations of RNN-based agents trained with both value-based and policy gradient methods. Our findings suggest that RNNs trained with policy gradient often fail to learn optimal state representations in situations where the same action should be taken in different states. We attribute this failure to state aliasing, where multiple distinct states are conflated in the representation space. Our experiments on a simple maze and a more complex text-based game demonstrate the phenomenon of state aliasing. We recommend solutions for RNN training in reinforcement learning to address state aliasing.",1
"We study the problem of learning representations with controllable connectivity properties. This is beneficial in situations when the imposed structure can be leveraged upstream. In particular, we control the connectivity of an autoencoder's latent space via a novel type of loss, operating on information from persistent homology. Under mild conditions, this loss is differentiable and we present a theoretical analysis of the properties induced by the loss. We choose one-class learning as our upstream task and demonstrate that the imposed structure enables informed parameter selection for modeling the in-class distribution via kernel density estimators. Evaluated on computer vision data, these one-class models exhibit competitive performance and, in a low sample size regime, outperform other methods by a large margin. Notably, our results indicate that a single autoencoder, trained on auxiliary (unlabeled) data, yields a mapping into latent space that can be reused across datasets for one-class learning.",0
"Our focus is on acquiring knowledge about generating representations that possess controllable connectivity properties. This proves advantageous when the enforced structure can be utilized upstream. We accomplish this by regulating the connectivity of an autoencoder's latent space with an innovative loss function that operates on information derived from persistent homology. This loss is differentiable under mild conditions, and we offer a theoretical examination of the properties it induces. For our upstream task, we choose one-class learning and prove that the imposed structure facilitates informed parameter selection for modeling the in-class distribution via kernel density estimators. When tested on computer vision data, these one-class models display competitive performance and, in a low sample size setting, surpass other methods by a significant margin. Notably, our findings suggest that a single autoencoder trained on unlabeled auxiliary data produces a mapping into latent space that can be reused across datasets for one-class learning.",1
"Protein modeling is an increasingly popular area of machine learning research. Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We benchmark a range of approaches to semi-supervised protein representation learning, which span recent work as well as canonical sequence learning techniques. We find that self-supervised pretraining is helpful for almost all models on all tasks, more than doubling performance in some cases. Despite this increase, in several cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests a huge opportunity for innovative architecture design and improved modeling paradigms that better capture the signal in biological sequences. TAPE will help the machine learning community focus effort on scientifically relevant problems. Toward this end, all data and code used to run these experiments are available at https://github.com/songlab-cal/tape.",0
"The domain of machine learning research is increasingly interested in protein modeling. Due to the high costs associated with acquiring supervised protein labels, semi-supervised learning has become a vital paradigm in this area. However, the current literature lacks consistency in terms of datasets and standardized evaluation techniques. To advance this field, we present Tasks Assessing Protein Embeddings (TAPE), which consists of five relevant semi-supervised learning tasks across various protein biology domains. We have separated the tasks into specific training, validation, and test splits to ensure biologically relevant generalization that transfers to practical situations. We have assessed several approaches to semi-supervised protein representation learning, including recent and canonical sequence learning techniques. Self-supervised pretraining has proven helpful for most models across all tasks, with performance more than doubling in some cases. Despite this improvement, features learned by self-supervised pretraining still fall short of those extracted by state-of-the-art non-neural techniques. This performance gap indicates an opportunity for innovative architecture design and improved modeling paradigms that better capture the biological sequence signal. TAPE aids the machine learning community in focusing on scientific problems. All data and code used in these experiments are available at https://github.com/songlab-cal/tape.",1
"Encouraged by the success of deep learning in a variety of domains, we investigate the suitability and effectiveness of Recurrent Neural Networks (RNNs) in a domain where deep learning has not yet been used; namely detecting confusion from eye-tracking data. Through experiments with a dataset of user interactions with ValueChart (an interactive visualization tool), we found that RNNs learn a feature representation from the raw data that allows for a more powerful classifier than previous methods that use engineered features. This is evidenced by the stronger performance of the RNN (0.74/0.71 sensitivity/specificity), as compared to a Random Forest classifier (0.51/0.70 sensitivity/specificity), when both are trained on an un-augmented dataset. However, using engineered features allows for simple data augmentation methods to be used. These same methods are not as effective at augmentation for the feature representation learned from the raw data, likely due to an inability to match the temporal dynamics of the data.",0
"Our investigation explores the effectiveness of Recurrent Neural Networks (RNNs) in detecting confusion from eye-tracking data, an area where deep learning hasn't been utilized before. Building on the success of deep learning in other domains, we conducted experiments using a dataset of user interactions with ValueChart, an interactive visualization tool. Our findings indicate that RNNs can learn feature representation from raw data more effectively than previous methods that rely on engineered features. The RNN outperformed a Random Forest classifier in terms of sensitivity and specificity (0.74/0.71 vs. 0.51/0.70) when trained on an un-augmented dataset. However, using engineered features allowed for simple data augmentation methods, which were less effective with the feature representation learned from raw data due to difficulties in replicating the temporal dynamics of the information.",1
"We present a technique to improve the transferability of deep representations learned on small labeled datasets by introducing self-supervised tasks as auxiliary loss functions. While recent approaches for self-supervised learning have shown the benefits of training on large unlabeled datasets, we find improvements in generalization even on small datasets and when combined with strong supervision. Learning representations with self-supervised losses reduces the relative error rate of a state-of-the-art meta-learner by 5-25% on several few-shot learning benchmarks, as well as off-the-shelf deep networks on standard classification tasks when training from scratch. We find the benefits of self-supervision increase with the difficulty of the task. Our approach utilizes the images within the dataset to construct self-supervised losses and hence is an effective way of learning transferable representations without relying on any external training data.",0
"Our proposed technique enhances the transferability of deep representations acquired from limited labeled datasets by incorporating self-supervised tasks as additional loss functions. Despite recent self-supervised learning methods emphasizing the benefits of training on vast unlabeled datasets, we observe that our approach improves generalization even on small datasets, particularly when combined with strong supervision. By incorporating self-supervised losses into the learning process, we reduce the relative error rate of a top-tier meta-learner by 5-25% across various few-shot learning benchmarks. Additionally, our technique improves off-the-shelf deep networks' performance on standard classification tasks when trained from scratch. Interestingly, we notice that the benefits of self-supervision increase with the task's complexity. Our approach relies solely on the images within the dataset to generate self-supervised losses, making it a potent method for acquiring transferable representations without depending on external training data.",1
"Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.",0
"The acquisition of hidden node representations in graphs is a crucial and common task with a range of applications including node classification, link prediction, and graph visualization. Previous methods for graph representation learning have primarily concentrated on static graphs, yet many graphs encountered in the real world are dynamic and change over time. This study introduces the Dynamic Self-Attention Network (DySAT), an original neural structure that deals with dynamic graphs and acquires node representations that reflect both structural attributes and temporal evolutionary patterns. Specifically, DySAT computes node representations by concurrently utilizing self-attention layers in two dimensions: structural neighborhood and temporal dynamics. To evaluate its effectiveness, we carry out link prediction experiments on two types of graphs: communication networks and bipartite rating networks. Our findings demonstrate that DySAT surpasses several graph embedding baselines that are currently considered state-of-the-art.",1
"Graphs are a natural abstraction for many problems where nodes represent entities and edges represent a relationship across entities. An important area of research that has emerged over the last decade is the use of graphs as a vehicle for non-linear dimensionality reduction in a manner akin to previous efforts based on manifold learning with uses for downstream database processing, machine learning and visualization. In this systematic yet comprehensive experimental survey, we benchmark several popular network representation learning methods operating on two key tasks: link prediction and node classification. We examine the performance of 12 unsupervised embedding methods on 15 datasets. To the best of our knowledge, the scale of our study -- both in terms of the number of methods and number of datasets -- is the largest to date.   Our results reveal several key insights about work-to-date in this space. First, we find that certain baseline methods (task-specific heuristics, as well as classic manifold methods) that have often been dismissed or are not considered by previous efforts can compete on certain types of datasets if they are tuned appropriately. Second, we find that recent methods based on matrix factorization offer a small but relatively consistent advantage over alternative methods (e.g., random-walk based methods) from a qualitative standpoint. Specifically, we find that MNMF, a community preserving embedding method, is the most competitive method for the link prediction task. While NetMF is the most competitive baseline for node classification. Third, no single method completely outperforms other embedding methods on both node classification and link prediction tasks. We also present several drill-down analysis that reveals settings under which certain algorithms perform well (e.g., the role of neighborhood context on performance) -- guiding the end-user.",0
"Graphs serve as a natural abstraction for many problems where nodes represent entities and edges signify relationships between entities. In the past decade, there has been a surge in research on using graphs for non-linear dimensionality reduction, which is similar to manifold learning. This research has practical applications in downstream database processing, machine learning, and visualization. In this comprehensive survey, we tested the performance of 12 unsupervised embedding methods on 15 different datasets for two primary tasks: link prediction and node classification. Our study is the largest to date in terms of the number of methods and datasets analyzed. Our results reveal that certain baseline methods and classic manifold methods, which have previously been overlooked, can be competitive on certain datasets if they are appropriately fine-tuned. Recent methods based on matrix factorization offer a slight advantage over other methods, such as random-walk based methods. We also identified MNMF as the most effective method for link prediction and NetMF as the most competitive baseline for node classification. However, no single method outperforms all others on both tasks. Additionally, we present a detailed analysis of certain settings that reveal when specific algorithms perform well, which can guide end-users.",1
"Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for visual recognition problems. Nevertheless, the convolutional filters in these networks are local operations while ignoring the large-range dependency. Such drawback becomes even worse particularly for video recognition, since video is an information-intensive media with complex temporal variations. In this paper, we present a novel framework to boost the spatio-temporal representation learning by Local and Global Diffusion (LGD). Specifically, we construct a novel neural network architecture that learns the local and global representations in parallel. The architecture is composed of LGD blocks, where each block updates local and global features by modeling the diffusions between these two representations. Diffusions effectively interact two aspects of information, i.e., localized and holistic, for more powerful way of representation learning. Furthermore, a kernelized classifier is introduced to combine the representations from two aspects for video recognition. Our LGD networks achieve clear improvements on the large-scale Kinetics-400 and Kinetics-600 video classification datasets against the best competitors by 3.5% and 0.7%. We further examine the generalization of both the global and local representations produced by our pre-trained LGD networks on four different benchmarks for video action recognition and spatio-temporal action detection tasks. Superior performances over several state-of-the-art techniques on these benchmarks are reported. Code is available at: https://github.com/ZhaofanQiu/local-and-global-diffusion-networks.",0
"Visual recognition problems can be effectively solved using Convolutional Neural Networks (CNN). However, the local operations of the convolutional filters in these networks do not account for large-range dependency, which is particularly problematic for video recognition due to its complex temporal variations. To address this issue, we propose a novel approach called Local and Global Diffusion (LGD) that enhances spatio-temporal representation learning. Our neural network architecture consists of LGD blocks that simultaneously update local and global features by modeling diffusions between the two representations. This approach allows for more powerful representation learning by effectively combining localized and holistic information. Additionally, a kernelized classifier is introduced for video recognition. Our LGD networks outperform state-of-the-art techniques on large-scale datasets and generalization is demonstrated on four different benchmarks for video action recognition and spatio-temporal action detection tasks. Code can be accessed at https://github.com/ZhaofanQiu/local-and-global-diffusion-networks.",1
"We focus on the problem of teaching a robot to solve tasks presented sequentially, i.e., in a continual learning scenario. The robot should be able to solve all tasks it has encountered, without forgetting past tasks. We provide preliminary work on applying Reinforcement Learning to such setting, on 2D navigation tasks for a 3 wheel omni-directional robot. Our approach takes advantage of state representation learning and policy distillation. Policies are trained using learned features as input, rather than raw observations, allowing better sample efficiency. Policy distillation is used to combine multiple policies into a single one that solves all encountered tasks.",0
"Our area of focus is centered on the challenge of instructing a robot to sequentially solve tasks without forgetting previously learned ones. To achieve this, we have initiated preliminary work that applies Reinforcement Learning to 2D navigation tasks for a 3 wheel omni-directional robot. We have incorporated state representation learning and policy distillation into our approach, which enables us to train policies using learned features as opposed to raw observations, resulting in improved sample efficiency. Additionally, we have implemented policy distillation to combine multiple policies into a single solution that can successfully solve all encountered tasks.",1
"The automatic and efficient discovery of skills, without supervision, for long-living autonomous agents, remains a challenge of Artificial Intelligence. Intrinsically Motivated Goal Exploration Processes give learning agents a human-inspired mechanism to sequentially select goals to achieve. This approach gives a new perspective on the lifelong learning problem, with promising results on both simulated and real-world experiments. Until recently, those algorithms were restricted to domains with experimenter-knowledge, since the Goal Space used by the agents was built on engineered feature extractors. The recent advances of deep representation learning, enables new ways of designing those feature extractors, using directly the agent experience. Recent work has shown the potential of those methods on simple yet challenging simulated domains. In this paper, we present recent results showing the applicability of those principles on a real-world robotic setup, where a 6-joint robotic arm learns to manipulate a ball inside an arena, by choosing goals in a space learned from its past experience.",0
"Discovering skills automatically and efficiently for long-living autonomous agents is a challenge in the field of Artificial Intelligence. To address this, Intrinsically Motivated Goal Exploration Processes have been developed, which allow learning agents to select goals sequentially and achieve them. This approach offers a new perspective on the lifelong learning problem, and has yielded promising results in both simulated and real-world experiments. However, until recently, these algorithms were limited to domains where the experimenter had prior knowledge, as the Goal Space used by the agents relied on engineered feature extractors. With the advent of deep representation learning, there are new ways to design these feature extractors using the agent's experience directly. Recent research has demonstrated the potential of these methods in challenging simulated domains. In this paper, we present recent results that demonstrate the applicability of these principles in a real-world robotic setup. Specifically, a 6-joint robotic arm has learned to manipulate a ball inside an arena by selecting goals in a space learned from its past experience.",1
"To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.",0
"In order for Reinforcement Learning (RL) to achieve success in real-world tasks, it must utilize the world's compositional, relational, and hierarchical structure and apply it to the given task. With recent advancements in representation learning for language, it is now possible to create models that obtain knowledge from text corpora and incorporate it into decision-making problems. Therefore, we propose examining the close integration of natural language understanding into RL, specifically. Our review of the field includes research on following instructions, text-based games, and learning from textual domain knowledge. Additionally, we encourage the creation of new environments and further exploration of the potential applications of recent Natural Language Processing (NLP) techniques for such tasks.",1
"Network representation learning, as an approach to learn low dimensional representations of vertices, has attracted considerable research attention recently. It has been proven extremely useful in many machine learning tasks over large graph. Most existing methods focus on learning the structural representations of vertices in a static network, but cannot guarantee an accurate and efficient embedding in a dynamic network scenario. To address this issue, we present an efficient incremental skip-gram algorithm with negative sampling for dynamic network embedding, and provide a set of theoretical analyses to characterize the performance guarantee. Specifically, we first partition a dynamic network into the updated, including addition/deletion of links and vertices, and the retained networks over time. Then we factorize the objective function of network embedding into the added, vanished and retained parts of the network. Next we provide a new stochastic gradient-based method, guided by the partitions of the network, to update the nodes and the parameter vectors. The proposed algorithm is proven to yield an objective function value with a bounded difference to that of the original objective function. Experimental results show that our proposal can significantly reduce the training time while preserving the comparable performance. We also demonstrate the correctness of the theoretical analysis and the practical usefulness of the dynamic network embedding. We perform extensive experiments on multiple real-world large network datasets over multi-label classification and link prediction tasks to evaluate the effectiveness and efficiency of the proposed framework, and up to 22 times speedup has been achieved.",0
"Recently, there has been considerable interest in network representation learning, which involves learning low dimensional representations of vertices. This approach has proven to be useful in many machine learning tasks involving large graphs. However, most existing methods focus on learning the structural representations of vertices in static networks, and are not efficient or accurate in dynamic network scenarios. To address this issue, we propose an incremental skip-gram algorithm with negative sampling for dynamic network embedding, and provide theoretical analyses to characterize its performance. Our approach involves partitioning a dynamic network into updated and retained networks over time, and factoring the objective function of network embedding into the added, vanished, and retained parts of the network. We introduce a new stochastic gradient-based method, guided by the network partitions, to update the nodes and parameter vectors. Our algorithm has been proven to yield an objective function value with a bounded difference to that of the original objective function, and experimental results demonstrate its efficiency and effectiveness in large network datasets. We achieve up to 22 times speedup, and demonstrate the correctness of our theoretical analysis and practical usefulness of dynamic network embedding.",1
"We present a new method to learn video representations from unlabeled data. Given large-scale unlabeled video data, the objective is to benefit from such data by learning a generic and transferable representation space that can be directly used for a new task such as zero/few-shot learning. We formulate our unsupervised representation learning as a multi-modal, multi-task learning problem, where the representations are also shared across different modalities via distillation. Further, we also introduce the concept of finding a better loss function to train such multi-task multi-modal representation space using an evolutionary algorithm; our method automatically searches over different combinations of loss functions capturing multiple (self-supervised) tasks and modalities. Our formulation allows for the distillation of audio, optical flow and temporal information into a single, RGB-based convolutional neural network. We also compare the effects of using additional unlabeled video data and evaluate our representation learning on standard public video datasets.",0
"A novel approach is being presented for acquiring knowledge from unlabelled video data. The aim is to develop a general and versatile representation space that can be utilized for new tasks like zero/few-shot learning. The unsupervised representation learning is viewed as a multi-task and multi-modal problem, where the representations are shared across various modalities through distillation. Additionally, the concept of discovering an improved loss function to train such a multi-task and multi-modal representation space is introduced using an evolutionary algorithm. Our approach enables the integration of audio, optical flow and temporal information into a single RGB-based convolutional neural network. We also examine the impact of incorporating additional unlabelled video data and assess our representation learning on commonly used public video datasets.",1
"Recent successes in visual recognition can be primarily attributed to feature representation, learning algorithms, and the ever-increasing size of labeled training data. Extensive research has been devoted to the first two, but much less attention has been paid to the third. Due to the high cost of manual labeling, the size of recent efforts such as ImageNet is still relatively small in respect to daily applications. In this work, we mainly focus on how to automatically generate identifying image data for a given visual concept on a vast scale. With the generated image data, we can train a robust recognition model for the given concept. We evaluate the proposed webly supervised approach on the benchmark Pascal VOC 2007 dataset and the results demonstrates the superiority of our proposed approach in image data collection.",0
"The primary factors responsible for recent advancements in visual recognition are feature representation, learning algorithms, and the increasing size of labeled training data. While considerable research has been conducted on the first two, less attention has been paid to the third. The limited scale of recent efforts like ImageNet can be attributed to the high cost of manual labeling, making it insufficient for practical applications. This study primarily focuses on generating large-scale image data for a given visual concept to train a robust recognition model. The proposed webly supervised approach is evaluated on the Pascal VOC 2007 dataset, and the results highlight the effectiveness of our approach in image data collection.",1
"Capsule Networks attempt to represent patterns in images in a way that preserves hierarchical spatial relationships. Additionally, research has demonstrated that these techniques may be robust against adversarial perturbations. We present an improvement to training capsule networks with added robustness via non-parametric kernel methods. The representations learned through the capsule network are used to construct covariance kernels for Gaussian processes (GPs). We demonstrate that this approach achieves comparable prediction performance to Capsule Networks while improving robustness to adversarial perturbations and providing a meaningful measure of uncertainty that may aid in the detection of adversarial inputs.",0
"The aim of Capsule Networks is to maintain hierarchical spatial relationships when representing patterns in images. Moreover, studies have shown that these methods have the potential to be resistant to adversarial perturbations. Our study introduces a new technique for enhancing the resilience of Capsule Networks during training through non-parametric kernel methods. By utilizing the representations acquired from the capsule network, we construct covariance kernels for Gaussian processes (GPs). Our findings demonstrate that this method achieves comparable predictive performance to Capsule Networks while also enhancing resistance to adversarial perturbations and offering a meaningful uncertainty measurement that could help identify adversarial inputs.",1
"The Arcade Learning Environment (ALE) is a popular platform for evaluating reinforcement learning agents. Much of the appeal comes from the fact that Atari games demonstrate aspects of competency we expect from an intelligent agent and are not biased toward any particular solution approach. The challenge of the ALE includes (1) the representation learning problem of extracting pertinent information from raw pixels, and (2) the behavioural learning problem of leveraging complex, delayed associations between actions and rewards. Often, the research questions we are interested in pertain more to the latter, but the representation learning problem adds significant computational expense. We introduce MinAtar, short for miniature Atari, a new set of environments that capture the general mechanics of specific Atari games while simplifying the representational complexity to focus more on the behavioural challenges. MinAtar consists of analogues of five Atari games: Seaquest, Breakout, Asterix, Freeway and Space Invaders. Each MinAtar environment provides the agent with a 10x10xn binary state representation. Each game plays out on a 10x10 grid with n channels corresponding to game-specific objects, such as ball, paddle and brick in the game Breakout. To investigate the behavioural challenges posed by MinAtar, we evaluated a smaller version of the DQN architecture as well as online actor-critic with eligibility traces. With the representation learning problem simplified, we can perform experiments with significantly less computational expense. In our experiments, we use the saved compute time to perform step-size parameter sweeps and more runs than is typical for the ALE. Experiments like this improve reproducibility, and allow us to draw more confident conclusions. We hope that MinAtar can allow researchers to thoroughly investigate behavioural challenges similar to those inherent in the ALE.",0
"The Arcade Learning Environment (ALE) is a well-known platform for testing reinforcement learning agents. Its popularity stems from the fact that Atari games showcase the abilities we expect from an intelligent agent and are not biased towards any specific solution approach. However, ALE presents two challenges: (1) the representation learning problem, which involves extracting relevant information from raw pixels, and (2) the behavioural learning problem, which involves leveraging complex, delayed associations between actions and rewards. While the latter is often the focus of research, the former adds significant computational cost. To address this, we introduce MinAtar, a new set of environments that capture the mechanics of specific Atari games while simplifying the representation complexity to focus more on behavioural challenges. MinAtar includes analogues of five Atari games and provides each agent with a 10x10xn binary state representation. By simplifying the representation learning problem, we can perform experiments with significantly less computational cost and conduct step-size parameter sweeps and more runs. Our hope is that MinAtar will enable researchers to thoroughly investigate behavioural challenges similar to those in ALE and improve reproducibility while allowing for more confident conclusions.",1
"Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce the concept of a DeepMDP, a parameterized latent space model that is trained via the minimization of two tractable losses: prediction of rewards and prediction of the distribution over next latent states. We show that the optimization of these objectives guarantees (1) the quality of the latent space as a representation of the state space and (2) the quality of the DeepMDP as a model of the environment. We connect these results to prior work in the bisimulation literature, and explore the use of a variety of metrics. Our theoretical findings are substantiated by the experimental result that a trained DeepMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements over model-free RL.",0
"A lot of RL tasks give the agent complex observations that can be simplified into basic continuous states. To make this process more formal, we came up with the idea of a DeepMDP, which is a parameterized model that learns through minimizing two losses: predicting rewards and predicting the distribution of the next latent states. By optimizing these objectives, we can guarantee that the latent space is a good representation of the state space and that the DeepMDP is a solid model for the environment. We connect this idea to previous work in the bisimulation literature and use various metrics to test our theory. We proved our theory by showing that a trained DeepMDP can find the underlying structure of high-dimensional observations in a synthetic environment. Finally, we showed that learning a DeepMDP as an auxiliary task in the Atari 2600 domain can greatly improve performance over model-free RL.",1
"We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also \emph{flexibly fair}, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder---which does not require the sensitive attributes for inference---enables the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.",0
"The problem we address is how to achieve group and subgroup fairness in learning representations for multiple sensitive attributes. Drawing from the disentangled representation learning literature, we suggest an algorithm for creating compressed representations of datasets. These representations are useful for both prediction and reconstruction, and can be altered at test time to achieve subgroup demographic parity for various sensitive attributes and their combinations. Our findings indicate that the encoder produced through this process can be adjusted to accommodate diverse fair classification tasks, even with new target labels and subgroup definitions, without relying on sensitive attributes for inference.",1
"Self-supervised learning aims to learn representations from the data itself without explicit manual supervision. Existing efforts ignore a crucial aspect of self-supervised learning - the ability to scale to large amount of data because self-supervision requires no manual labels. In this work, we revisit this principle and scale two popular self-supervised approaches to 100 million images. We show that by scaling on various axes (including data size and problem 'hardness'), one can largely match or even exceed the performance of supervised pre-training on a variety of tasks such as object detection, surface normal estimation (3D) and visual navigation using reinforcement learning. Scaling these methods also provides many interesting insights into the limitations of current self-supervised techniques and evaluations. We conclude that current self-supervised methods are not 'hard' enough to take full advantage of large scale data and do not seem to learn effective high level semantic representations. We also introduce an extensive benchmark across 9 different datasets and tasks. We believe that such a benchmark along with comparable evaluation settings is necessary to make meaningful progress. Code is at: https://github.com/facebookresearch/fair_self_supervision_benchmark.",0
"The objective of self-supervised learning is to acquire data representations without explicit manual guidance. However, previous attempts neglect a significant aspect of self-supervised learning, which is the capability to adapt to a vast amount of data without the need for manual labels. This research revisits this principle and scales two prevalent self-supervised methods to 100 million images. The study demonstrates that by expanding on various factors, such as the size of the data and the difficulty of the problem, one can achieve comparable or superior results compared to supervised pre-training on tasks such as object detection, surface normal estimation, and visual navigation using reinforcement learning. Scaling these methods also provides valuable insights into the limitations of current self-supervised techniques and assessments. The study concludes that current self-supervised methods are not challenging enough to leverage large-scale data and do not seem to acquire effective high-level semantic representations. Additionally, the research introduces a comprehensive benchmark across nine different datasets and tasks, which could be useful in making significant progress. The code for this study is available at https://github.com/facebookresearch/fair_self_supervision_benchmark.",1
"Graph data widely exist in many high-impact applications. Inspired by the success of deep learning in grid-structured data, graph neural network models have been proposed to learn powerful node-level or graph-level representation. However, most of the existing graph neural networks suffer from the following limitations: (1) there is limited analysis regarding the graph convolution properties, such as seed-oriented, degree-aware and order-free; (2) the node's degree-specific graph structure is not explicitly expressed in graph convolution for distinguishing structure-aware node neighborhoods; (3) the theoretical explanation regarding the graph-level pooling schemes is unclear.   To address these problems, we propose a generic degree-specific graph neural network named DEMO-Net motivated by Weisfeiler-Lehman graph isomorphism test that recursively identifies 1-hop neighborhood structures. In order to explicitly capture the graph topology integrated with node attributes, we argue that graph convolution should have three properties: seed-oriented, degree-aware, order-free. To this end, we propose multi-task graph convolution where each task represents node representation learning for nodes with a specific degree value, thus leading to preserving the degree-specific graph structure. In particular, we design two multi-task learning methods: degree-specific weight and hashing functions for graph convolution. In addition, we propose a novel graph-level pooling/readout scheme for learning graph representation provably lying in a degree-specific Hilbert kernel space. The experimental results on several node and graph classification benchmark data sets demonstrate the effectiveness and efficiency of our proposed DEMO-Net over state-of-the-art graph neural network models.",0
"Graph data is prevalent in many high-impact applications, and graph neural network models have been proposed to learn powerful node-level or graph-level representation inspired by the success of deep learning in grid-structured data. However, existing graph neural networks suffer from limitations, such as limited analysis regarding the graph convolution properties, the node's degree-specific graph structure not being explicitly expressed in graph convolution, and unclear theoretical explanations regarding graph-level pooling schemes. To address these issues, we propose DEMO-Net, a generic degree-specific graph neural network that captures the graph topology integrated with node attributes. We argue that graph convolution should have three properties: seed-oriented, degree-aware, and order-free, and propose multi-task graph convolution to preserve the degree-specific graph structure. Additionally, we design two multi-task learning methods and a novel graph-level pooling/readout scheme to learn graph representation in a degree-specific Hilbert kernel space. Experimental results on multiple benchmark data sets demonstrate the effectiveness and efficiency of DEMO-Net over existing graph neural network models.",1
"Learning rich and compact representations is an open topic in many fields such as object recognition or image retrieval. Deep neural networks have made a major breakthrough during the last few years for these tasks but their representations are not necessary as rich as needed nor as compact as expected. To build richer representations, high order statistics have been exploited and have shown excellent performances, but they produce higher dimensional features. While this drawback has been partially addressed with factorization schemes, the original compactness of first order models has never been retrieved, or at the cost of a strong performance decrease. Our method, by jointly integrating codebook strategy to factorization scheme, is able to produce compact representations while keeping the second order performances with few additional parameters. This formulation leads to state-of-the-art results on three image retrieval datasets.",0
"Many fields such as object recognition and image retrieval are currently exploring ways to learn representations that are both rich and compact. Although deep neural networks have shown promise for these tasks, their representations are often not rich enough or compact enough. To address this issue, high order statistics have been utilized, but this can result in higher dimensional features. While factorization schemes have helped to mitigate this problem, they have not been able to fully restore the original compactness of first order models without sacrificing performance. Our approach combines a codebook strategy with a factorization scheme to produce compact representations that maintain second order performance with minimal additional parameters. This method has resulted in state-of-the-art performance on three image retrieval datasets.",1
"This paper investigates the resilience and robustness of Deep Reinforcement Learning (DRL) policies to adversarial perturbations in the state space. We first present an approach for the disentanglement of vulnerabilities caused by representation learning of DRL agents from those that stem from the sensitivity of the DRL policies to distributional shifts in state transitions. Building on this approach, we propose two RL-based techniques for quantitative benchmarking of adversarial resilience and robustness in DRL policies against perturbations of state transitions. We demonstrate the feasibility of our proposals through experimental evaluation of resilience and robustness in DQN, A2C, and PPO2 policies trained in the Cartpole environment.",0
"The objective of this study is to analyze the ability of Deep Reinforcement Learning (DRL) policies to withstand adversarial perturbations in the state space. The research initially presents a method that separates vulnerabilities caused by DRL agents' representation learning from those that arise from the sensitivity of DRL policies to distributional shifts in state transitions. Based on this approach, the study suggests two RL-based techniques for quantitatively assessing the adversarial resilience and robustness of DRL policies against perturbations of state transitions. The efficacy of these proposals is demonstrated through experimental evaluation of resilience and robustness in DQN, A2C, and PPO2 policies trained in the Cartpole environment.",1
"We introduce a novel approach to graph-level representation learning, which is to embed an entire graph into a vector space where the embeddings of two graphs preserve their graph-graph proximity. Our approach, UGRAPHEMB, is a general framework that provides a novel means to performing graph-level embedding in a completely unsupervised and inductive manner. The learned neural network can be considered as a function that receives any graph as input, either seen or unseen in the training set, and transforms it into an embedding. A novel graph-level embedding generation mechanism called Multi-Scale Node Attention (MSNA), is proposed. Experiments on five real graph datasets show that UGRAPHEMB achieves competitive accuracy in the tasks of graph classification, similarity ranking, and graph visualization.",0
"Our approach, named UGRAPHEMB, presents a new method for learning graph-level representations. Our method involves embedding entire graphs into a vector space, where the embeddings of two graphs maintain their proximity. This approach is completely unsupervised and inductive, making it a versatile framework for graph-level embedding. The neural network learned through this method can transform any graph, whether seen or unseen in the training set, into an embedding. We have also proposed a novel mechanism called Multi-Scale Node Attention (MSNA) for generating graph-level embeddings. Our experiments on five real graph datasets have shown that UGRAPHEMB is competitive in accuracy for graph classification, similarity ranking, and graph visualization tasks.",1
"We propose $\textit{weighted inner product similarity}$ (WIPS) for neural network-based graph embedding. In addition to the parameters of neural networks, we optimize the weights of the inner product by allowing positive and negative values. Despite its simplicity, WIPS can approximate arbitrary general similarities including positive definite, conditionally positive definite, and indefinite kernels. WIPS is free from similarity model selection, since it can learn any similarity models such as cosine similarity, negative Poincar\'e distance and negative Wasserstein distance. Our experiments show that the proposed method can learn high-quality distributed representations of nodes from real datasets, leading to an accurate approximation of similarities as well as high performance in inductive tasks.",0
"Our proposal for neural network-based graph embedding is the $\textit{weighted inner product similarity}$ (WIPS). This method optimizes the weights of the inner product, allowing for both positive and negative values, in addition to optimizing the neural network's parameters. Although WIPS is simple, it can approximate various general similarities, including positive definite, conditionally positive definite, and indefinite kernels. Furthermore, WIPS eliminates the need for similarity model selection, as it can learn any similarity models, such as cosine similarity, negative Poincar\'e distance, and negative Wasserstein distance. Our experiments indicate that the proposed approach can generate high-quality distributed representations of nodes from actual datasets, resulting in an accurate approximation of similarities and high performance in inductive tasks.",1
"Due to the ability of deep neural nets to learn rich representations, recent advances in unsupervised domain adaptation have focused on learning domain-invariant features that achieve a small error on the source domain. The hope is that the learnt representation, together with the hypothesis learnt from the source domain, can generalize to the target domain. In this paper, we first construct a simple counterexample showing that, contrary to common belief, the above conditions are not sufficient to guarantee successful domain adaptation. In particular, the counterexample exhibits \emph{conditional shift}: the class-conditional distributions of input features change between source and target domains. To give a sufficient condition for domain adaptation, we propose a natural and interpretable generalization upper bound that explicitly takes into account the aforementioned shift. Moreover, we shed new light on the problem by proving an information-theoretic lower bound on the joint error of \emph{any} domain adaptation method that attempts to learn invariant representations. Our result characterizes a fundamental tradeoff between learning invariant representations and achieving small joint error on both domains when the marginal label distributions differ from source to target. Finally, we conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of domain adaptation and representation learning algorithms.",0
"Recent advances in unsupervised domain adaptation have focused on developing domain-invariant features that achieve a small error on the source domain, thanks to the ability of deep neural nets to learn rich representations. The hope is that these features, combined with the learned hypothesis from the source domain, can generalize to the target domain. However, this paper shows that these conditions are not sufficient to ensure successful domain adaptation. A counterexample is presented that demonstrates ""conditional shift,"" where the class-conditional distributions of input features change between the source and target domains. To address this issue, a natural and interpretable generalization upper bound is proposed that explicitly accounts for the shift. Furthermore, the paper establishes an information-theoretic lower bound on the joint error of any domain adaptation method that attempts to learn invariant representations. This result highlights the fundamental tradeoff between learning invariant representations and achieving small joint error on both domains when the marginal label distributions differ between source and target. The theoretical findings are corroborated by experiments on real-world datasets, and the insights gained can guide the future design of domain adaptation and representation learning algorithms.",1
"The representations learned by deep neural networks are difficult to interpret in part due to their large parameter space and the complexities introduced by their multi-layer structure. We introduce a method for computing persistent homology over the graphical activation structure of neural networks, which provides access to the task-relevant substructures activated throughout the network for a given input. This topological perspective provides unique insights into the distributed representations encoded by neural networks in terms of the shape of their activation structures. We demonstrate the value of this approach by showing an alternative explanation for the existence of adversarial examples. By studying the topology of network activations across multiple architectures and datasets, we find that adversarial perturbations do not add activations that target the semantic structure of the adversarial class as previously hypothesized. Rather, adversarial examples are explainable as alterations to the dominant activation structures induced by the original image, suggesting the class representations learned by deep networks are problematically sparse on the input space.",0
"The intricate nature of deep neural networks and their extensive parameter space make it challenging to comprehend the representations acquired. Our novel technique involves utilizing persistent homology to examine the graphical activation structure of neural networks, providing access to the relevant substructures activated in response to a given input. This topological approach offers distinctive insights into the distributed representations encoded by neural networks regarding the shape of their activation structures. We illustrate the usefulness of this method by presenting an alternative explanation for the presence of adversarial examples. After investigating the topology of network activations across multiple datasets and architectures, we discovered that adversarial perturbations do not generate activations that target the semantic structure of the adversarial class, as previously hypothesized. Instead, adversarial examples can be accounted for as modifications to the dominant activation structures induced by the original image, implying that the class representations learned by deep networks are sparsely distributed across the input space.",1
"We investigate the effects of the unsupervised pre-training method under the perspective of information theory. If the input distribution displays multiple views of the supervision, then unsupervised pre-training allows to learn hierarchical representation which communicates these views across layers, while disentangling the supervision. Disentanglement of supervision leads learned features to be independent conditionally to the label. In case of binary features, we show that conditional independence allows to extract label's information with a linear model and therefore helps to solve under-fitting. We suppose that representations displaying multiple views help to solve over-fitting because each view provides information that helps to reduce model's variance. We propose a practical method to measure both disentanglement of supervision and quantity of views within a binary representation. We show that unsupervised pre-training helps to conserve views from input distribution, whereas representations learned using supervised models disregard most of them.",0
"From an information theory perspective, we are examining how the unsupervised pre-training method affects learning. When there are multiple views of supervision in the input distribution, unsupervised pre-training enables the acquisition of a hierarchical representation that conveys these views across layers while separating the supervision. This separation of supervision results in independent learned features that are conditional on the label. When dealing with binary features, we demonstrate that this conditional independence can aid in overcoming under-fitting by allowing for the extraction of label information through a linear model. We hypothesize that representations with multiple views can also alleviate over-fitting by providing information that reduces the model's variance. To measure the disentanglement of supervision and the quantity of views in a binary representation, we propose a practical method. We demonstrate that unsupervised pre-training preserves input distribution views, whereas supervised models disregard most of them.",1
"We propose the factorized action variational autoencoder (FAVAE), a state-of-the-art generative model for learning disentangled and interpretable representations from sequential data via the information bottleneck without supervision. The purpose of disentangled representation learning is to obtain interpretable and transferable representations from data. We focused on the disentangled representation of sequential data since there is a wide range of potential applications if disentanglement representation is extended to sequential data such as video, speech, and stock market. Sequential data are characterized by dynamic and static factors: dynamic factors are time dependent, and static factors are independent of time. Previous models disentangle static and dynamic factors by explicitly modeling the priors of latent variables to distinguish between these factors. However, these models cannot disentangle representations between dynamic factors, such as disentangling ""picking up"" and ""throwing"" in robotic tasks. FAVAE can disentangle multiple dynamic factors. Since it does not require modeling priors, it can disentangle ""between"" dynamic factors. We conducted experiments to show that FAVAE can extract disentangled dynamic factors.",0
"Our proposed generative model, the Factorized Action Variational Autoencoder (FAVAE), is a cutting-edge method for acquiring disentangled and interpretable representations from sequential data. This is achieved through the information bottleneck without the need for supervision. The goal of disentangled representation learning is to obtain representations that are understandable and transferable from data. We focused on disentangling sequential data since it has significant potential applications, such as video, speech, and stock market analysis. Sequential data consist of dynamic and static factors, with the former being time-dependent and the latter independent of time. Traditional models separate static and dynamic factors by modeling the priors of latent variables to differentiate between the two. However, these methods cannot disentangle dynamic factors like ""picking up"" and ""throwing"" in robotic tasks. In contrast, FAVAE can disentangle multiple dynamic factors and does not require modeling priors, allowing it to disentangle ""between"" dynamic factors. Our experiments demonstrate that FAVAE can extract disentangled dynamic factors effectively.",1
"Multi-Person Tracking (MPT) is often addressed within the detection-to-association paradigm. In such approaches, human detections are first extracted in every frame and person trajectories are then recovered by a procedure of data association (usually offline). However, their performances usually degenerate in presence of detection errors, mutual interactions and occlusions. In this paper, we present a deep learning based MPT approach that learns instance-aware representations of tracked persons and robustly online infers states of the tracked persons. Specifically, we design a multi-branch neural network (MBN), which predicts the classification confidences and locations of all targets by taking a batch of candidate regions as input. In our MBN architecture, each branch (instance-subnet) corresponds to an individual to be tracked and new branches can be dynamically created for handling newly appearing persons. Then based on the output of MBN, we construct a joint association matrix that represents meaningful states of tracked persons (e.g., being tracked or disappearing from the scene) and solve it by using the efficient Hungarian algorithm. Moreover, we allow the instance-subnets to be updated during tracking by online mining hard examples, accounting to person appearance variations over time. We comprehensively evaluate our framework on a popular MPT benchmark, demonstrating its excellent performance in comparison with recent online MPT methods.",0
"The typical method for Multi-Person Tracking (MPT) involves extracting human detections in each frame and then recovering the trajectories of individuals through offline data association. However, this approach often struggles with detection errors, occlusions, and interactions between people. To address these issues, we propose a deep learning-based MPT approach that uses instance-aware representations of tracked persons to robustly infer their states online. Our approach involves a multi-branch neural network (MBN) that predicts classification confidences and locations of targets using a batch of candidate regions as input. Each branch in the MBN corresponds to an individual, and new branches can be created dynamically to handle newly appearing people. We construct a joint association matrix based on the MBN output, representing meaningful states of tracked persons, and solve it using the efficient Hungarian algorithm. To account for changes in appearance over time, we allow the instance-subnets to be updated during tracking by online mining hard examples. We evaluate our framework on a popular MPT benchmark and demonstrate its superior performance compared to recent online MPT methods.",1
"In this paper, the concept of representation learning based on deep neural networks is applied as an alternative to the use of handcrafted features in a method for automatic visual inspection of corroded thermoelectric metallic pipes. A texture convolutional neural network (TCNN) replaces handcrafted features based on Local Phase Quantization (LPQ) and Haralick descriptors (HD) with the advantage of learning an appropriate textural representation and the decision boundaries into a single optimization process. Experimental results have shown that it is possible to reach the accuracy of 99.20% in the task of identifying different levels of corrosion in the internal surface of thermoelectric pipe walls, while using a compact network that requires much less effort in tuning parameters when compared to the handcrafted approach since the TCNN architecture is compact regarding the number of layers and connections. The observed results open up the possibility of using deep neural networks in real-time applications such as the automatic inspection of thermoelectric metal pipes.",0
"In this study, deep neural networks are utilized as an alternative to manual feature creation in an automated visual inspection method for corroded thermoelectric metallic pipes. A texture convolutional neural network (TCNN) is employed to replace Local Phase Quantization (LPQ) and Haralick descriptors (HD) with the advantage of simultaneously learning an appropriate textural representation and decision boundaries. The experimental results demonstrate that the TCNN approach achieves an accuracy of 99.20% in identifying different levels of corrosion in the internal surface of thermoelectric pipe walls with a compact network that requires less parameter tuning than the manual approach. These findings suggest the potential for real-time applications, such as automatic inspection of thermoelectric metal pipes utilizing deep neural networks.",1
"A significant issue in training deep neural networks to solve supervised learning tasks is the need for large numbers of labelled datapoints. The goal of semi-supervised learning is to leverage ubiquitous unlabelled data, together with small quantities of labelled data, to achieve high task performance. Though substantial recent progress has been made in developing semi-supervised algorithms that are effective for comparatively small datasets, many of these techniques do not scale readily to the large (unlaballed) datasets characteristic of real-world applications. In this paper we introduce a novel approach to scalable semi-supervised learning, called Local Label Propagation (LLP). Extending ideas from recent work on unsupervised embedding learning, LLP first embeds datapoints, labelled and otherwise, in a common latent space using a deep neural network. It then propagates pseudolabels from known to unknown datapoints in a manner that depends on the local geometry of the embedding, taking into account both inter-point distance and local data density as a weighting on propagation likelihood. The parameters of the deep embedding are then trained to simultaneously maximize pseudolabel categorization performance as well as a metric of the clustering of datapoints within each psuedo-label group, iteratively alternating stages of network training and label propagation. We illustrate the utility of the LLP method on the ImageNet dataset, achieving results that outperform previous state-of-the-art scalable semi-supervised learning algorithms by large margins, consistently across a wide variety of training regimes. We also show that the feature representation learned with LLP transfers well to scene recognition in the Places 205 dataset.",0
"The main problem encountered in training deep neural networks for supervised learning tasks is the requirement of a significant number of labelled data points. Semi-supervised learning aims to use the vast amount of unlabelled data along with a small amount of labelled data to achieve optimal task performance. While there has been considerable progress in developing effective semi-supervised algorithms for small datasets, they often fail to scale up to large, real-world unlabelled datasets. This paper presents a new approach to scalable semi-supervised learning called Local Label Propagation (LLP). LLP uses a deep neural network to embed both labelled and unlabelled data points in a common latent space, and then propagates pseudolabels from known to unknown points based on the local geometry of the embedding, including inter-point distance and local data density. The deep embedding parameters are trained to optimize pseudolabel categorization performance and the clustering of datapoints within each psuedo-label group. The LLP method is evaluated on the ImageNet dataset, and outperforms previous state-of-the-art scalable semi-supervised learning algorithms across various training regimes. The feature representation learned with LLP also transfers well to scene recognition in the Places 205 dataset.",1
"The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for solving downstream tasks. However, despite the proliferation of such methods, there is currently no study of their robustness to adversarial attacks. We provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial perturbations that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.",0
"The objective of learning network representation is to acquire compact node embeddings that portray the graph structure and can be utilized to solve subsequent tasks. Despite the widespread usage of this technique, there has been no investigation into its resilience against adversarial attacks. In this study, we are the first to conduct an analysis of the susceptibility of random walk-based methods to such attacks. We have successfully created efficient adversarial perturbations that contaminate the network structure. These perturbations have an adverse impact on both the quality of the embeddings and the downstream tasks. Furthermore, we have demonstrated that our attacks are transferable, as they can be applied to various models and are effective even under restrictive conditions.",1
"Image representations are commonly learned from class labels, which are a simplistic approximation of human image understanding. In this paper we demonstrate that transferable representations of images can be learned without manual annotations by modeling human visual attention. The basis of our analyses is a unique gaze tracking dataset of sonographers performing routine clinical fetal anomaly screenings. Models of sonographer visual attention are learned by training a convolutional neural network (CNN) to predict gaze on ultrasound video frames through visual saliency prediction or gaze-point regression. We evaluate the transferability of the learned representations to the task of ultrasound standard plane detection in two contexts. Firstly, we perform transfer learning by fine-tuning the CNN with a limited number of labeled standard plane images. We find that fine-tuning the saliency predictor is superior to training from random initialization, with an average F1-score improvement of 9.6% overall and 15.3% for the cardiac planes. Secondly, we train a simple softmax regression on the feature activations of each CNN layer in order to evaluate the representations independently of transfer learning hyper-parameters. We find that the attention models derive strong representations, approaching the precision of a fully-supervised baseline model for all but the last layer.",0
"Usually, image representations are learned based on class labels, which are an oversimplified approximation of human image comprehension. However, in this study, we illustrate that transferable image representations can be acquired without any manual annotations by modeling human visual attention. Our analysis relies on a unique gaze tracking dataset of sonographers performing routine clinical fetal anomaly screenings. We train a convolutional neural network (CNN) to forecast gaze on ultrasound video frames through visual saliency prediction or gaze-point regression to learn models of sonographer visual attention. We assess the transferability of the learned representations to the task of ultrasound standard plane detection in two different scenarios. Firstly, by adjusting the CNN using a limited number of labeled standard plane images, we find that fine-tuning the saliency predictor is more effective than training from random initialization, with an average F1-score improvement of 9.6% overall and 15.3% for the cardiac planes. Secondly, we train a simple softmax regression on the feature activations of each CNN layer to evaluate the representations independently of transfer learning hyper-parameters. We determine that the attention models generate robust representations that approach the precision of a fully-supervised baseline model for all but the last layer.",1
"Exploration is an extremely challenging problem in reinforcement learning, especially in high dimensional state and action spaces and when only sparse rewards are available. Effective representations can indicate which components of the state are task relevant and thus reduce the dimensionality of the space to explore. In this work, we take a representation learning viewpoint on exploration, utilizing prior experience to learn effective latent representations, which can subsequently indicate which regions to explore. Prior experience on separate but related tasks help learn representations of the state which are effective at predicting instantaneous rewards. These learned representations can then be used with an entropy-based exploration method to effectively perform exploration in high dimensional spaces by effectively lowering the dimensionality of the search space. We show the benefits of this representation for meta-exploration in a simulated object pushing environment.",0
"Reinforcement learning faces an arduous challenge in exploration, particularly when dealing with high-dimensional state and action spaces and sparse rewards. Effective representations can aid in identifying task-relevant state components, thereby reducing the exploration space's dimensionality. In this study, we approach exploration from a representation learning perspective by utilizing previous experiences to develop effective latent representations that can determine which areas to explore. Using prior experience from related but distinct tasks enables us to learn effective state representations that can accurately predict instantaneous rewards. We can then incorporate these learned representations into an entropy-based exploration approach, which effectively reduces the search space's dimensionality in high-dimensional environments. Our research demonstrates the usefulness of this representation for meta-exploration in a simulated object pushing environment.",1
"A new variational autoencoder (VAE) model is proposed that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed Wyner VAE model is based on two information theoretic problems---distributed simulation and channel synthesis---in which Wyner's common information arises as the fundamental limit of the succinctness of the common representation. The Wyner VAE decomposes a pair of correlated data variables into their common representation (e.g., a shared concept) and local representations that capture the remaining randomness (e.g., texture and style) in respective data variables by imposing the mutual information between the data variables and the common representation as a regularization term. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with and without style control using synthetic data and real images. Experimental results show that learning a succinct common representation achieves better generative performance and that the proposed model outperforms existing VAE variants and the variational information bottleneck method.",0
"A novel model for variational autoencoders (VAE) has been introduced, which can efficiently learn a concise shared representation of two related data variables for generating tasks that are both joint and conditional. The Wyner VAE model is based on two information-theoretic problems, namely distributed simulation and channel synthesis, wherein Wyner's common information forms the fundamental limit of the shared representation's conciseness. The model decomposes the correlated data variables into common and local representations, with the latter capturing the remaining randomness like texture and style. The mutual information between the data variables and the common representation is used as a regularization term. Synthetic and real images were used to demonstrate the model's effectiveness in joint and conditional generation with or without style control. The experiments showed that the proposed approach's generative performance is superior to existing VAE variants and the variational information bottleneck technique, indicating that learning a concise shared representation is effective.",1
"Robust road detection is a key challenge in safe autonomous driving. Recently, with the rapid development of 3D sensors, more and more researchers are trying to fuse information across different sensors to improve the performance of road detection. Although many successful works have been achieved in this field, methods for data fusion under deep learning framework is still an open problem. In this paper, we propose a Siamese deep neural network based on FCN-8s to detect road region. Our method uses data collected from a monocular color camera and a Velodyne-64 LiDAR sensor. We project the LiDAR point clouds onto the image plane to generate LiDAR images and feed them into one of the branches of the network. The RGB images are fed into another branch of our proposed network. The feature maps that these two branches extract in multiple scales are fused before each pooling layer, via padding additional fusion layers. Extensive experimental results on public dataset KITTI ROAD demonstrate the effectiveness of our proposed approach.",0
"The challenge of ensuring safe autonomous driving lies in the accurate detection of roads. To improve the detection of roads, researchers are exploring ways to merge data from multiple sensors, taking advantage of the rapid development of 3D sensors. While some successful methods have been developed, there is still a gap in the deep learning framework when it comes to data fusion. In our study, we present a novel approach to detecting road regions using a Siamese deep neural network based on FCN-8s. Our method leverages data from a monocular color camera and a Velodyne-64 LiDAR sensor, projecting LiDAR point clouds onto the image plane to generate LiDAR images. These images are fed into one branch of the network, while RGB images are fed into another. Before each pooling layer, feature maps from these branches are fused using additional fusion layers. Our experimental results on the KITTI ROAD dataset demonstrate the effectiveness of our approach.",1
"Auto-encoders have emerged as a successful framework for unsupervised learning. However, conventional auto-encoders are incapable of utilizing explicit relations in structured data. To take advantage of relations in graph-structured data, several graph auto-encoders have recently been proposed, but they neglect to reconstruct either the graph structure or node attributes. In this paper, we present the graph attention auto-encoder (GATE), a neural network architecture for unsupervised representation learning on graph-structured data. Our architecture is able to reconstruct graph-structured inputs, including both node attributes and the graph structure, through stacked encoder/decoder layers equipped with self-attention mechanisms. In the encoder, by considering node attributes as initial node representations, each layer generates new representations of nodes by attending over their neighbors' representations. In the decoder, we attempt to reverse the encoding process to reconstruct node attributes. Moreover, node representations are regularized to reconstruct the graph structure. Our proposed architecture does not need to know the graph structure upfront, and thus it can be applied to inductive learning. Our experiments demonstrate competitive performance on several node classification benchmark datasets for transductive and inductive tasks, even exceeding the performance of supervised learning baselines in most cases.",0
"Unsupervised learning has been successful with the use of auto-encoders. However, conventional auto-encoders cannot make use of explicit relations in structured data. To address this, graph auto-encoders have been proposed, but they do not reconstruct the graph structure or node attributes. This paper introduces the graph attention auto-encoder (GATE), which is a neural network architecture for unsupervised representation learning on graph-structured data. Our architecture is capable of reconstructing graph-structured inputs, including both node attributes and the graph structure, through stacked encoder/decoder layers equipped with self-attention mechanisms. The encoder generates new representations of nodes by attending over their neighbors' representations while the decoder attempts to reverse the encoding process to reconstruct node attributes. Node representations are also regulated to reconstruct the graph structure. Our proposed architecture can be applied to inductive learning without prior knowledge of the graph structure. Our experiments show competitive performance on several node classification benchmark datasets for transductive and inductive tasks, even outperforming supervised learning baselines in most cases.",1
"We investigate the high-dimensional data clustering problem by proposing a novel and unsupervised representation learning model called Robust Flexible Auto-weighted Local-coordinate Concept Factorization (RFA-LCF). RFA-LCF integrates the robust flexible CF, robust sparse local-coordinate coding and the adaptive reconstruction weighting learning into a unified model. The adaptive weighting is driven by including the joint manifold preserving constraints on the recovered clean data, basis concepts and new representation. Specifically, our RFA-LCF uses a L2,1-norm based flexible residue to encode the mismatch between clean data and its reconstruction, and also applies the robust adaptive sparse local-coordinate coding to represent the data using a few nearby basis concepts, which can make the factorization more accurate and robust to noise. The robust flexible factorization is also performed in the recovered clean data space for enhancing representations. RFA-LCF also considers preserving the local manifold structures of clean data space, basis concept space and the new coordinate space jointly in an adaptive manner way. Extensive comparisons show that RFA-LCF can deliver enhanced clustering results.",0
"Our research delves into the issue of high-dimensional data clustering and presents a solution in the form of a new and unsupervised representation learning model. This model is called Robust Flexible Auto-weighted Local-coordinate Concept Factorization (RFA-LCF) and combines robust flexible CF, robust sparse local-coordinate coding, and adaptive reconstruction weighting learning into one unified model. The adaptive weighting process is achieved by taking into account joint manifold preserving constraints on the recovered clean data, basis concepts, and new representation. Our RFA-LCF utilizes a L2,1-norm based flexible residue to encode the mismatch between clean data and its reconstruction and applies robust adaptive sparse local-coordinate coding to represent the data accurately and robustly to noise. Additionally, the robust flexible factorization is performed in the recovered clean data space, which enhances the representation. Furthermore, RFA-LCF considers preserving the local manifold structures of clean data space, basis concept space, and the new coordinate space jointly in an adaptive manner. Our extensive comparisons show that RFA-LCF provides better clustering results.",1
"Learning effective embedding has been proved to be useful in many real-world problems, such as recommender systems, search ranking and online advertisement. However, one of the challenges is data sparsity in learning large-scale item embedding, as users' historical behavior data are usually lacking or insufficient in an individual domain. In fact, user's behaviors from different domains regarding the same items are usually relevant. Therefore, we can learn complete user behaviors to alleviate the sparsity using complementary information from correlated domains. It is intuitive to model users' behaviors using graph, and graph neural networks (GNNs) have recently shown the great power for representation learning, which can be used to learn item embedding. However, it is challenging to transfer the information across domains and learn cross-domain representation using the existing GNNs. To address these challenges, in this paper, we propose a novel model - Deep Multi-Graph Embedding (DMGE) to learn cross-domain representation. Specifically, we first construct a multi-graph based on users' behaviors from different domains, and then propose a multi-graph neural network to learn cross-domain representation in an unsupervised manner. Particularly, we present a multiple-gradient descent optimizer for efficiently training the model. We evaluate our approach on various large-scale real-world datasets, and the experimental results show that DMGE outperforms other state-of-art embedding methods in various tasks.",0
"Many real-world problems, including recommender systems, search ranking, and online advertisement, benefit from effective embedding. However, a challenge arises when learning large-scale item embedding due to data sparsity, as users' historical behavior data is often insufficient in a single domain. Additionally, user behavior related to the same items may vary across different domains. To address this, we propose a novel model, Deep Multi-Graph Embedding (DMGE), that utilizes complementary information from correlated domains. DMGE constructs a multi-graph based on users' behaviors from different domains and uses a multi-graph neural network to learn cross-domain representation in an unsupervised manner. We present a multiple-gradient descent optimizer for efficient training and evaluate DMGE on various large-scale real-world datasets, demonstrating its superior performance compared to other state-of-the-art embedding methods.",1
"Unsupervised exploration and representation learning become increasingly important when learning in diverse and sparse environments. The information-theoretic principle of empowerment formalizes an unsupervised exploration objective through an agent trying to maximize its influence on the future states of its environment. Previous approaches carry certain limitations in that they either do not employ closed-loop feedback or do not have an internal state. As a consequence, a privileged final state is taken as an influence measure, rather than the full trajectory. We provide a model-free method which takes into account the whole trajectory while still offering the benefits of option-based approaches. We successfully apply our approach to settings with large action spaces, where discovery of meaningful action sequences is particularly difficult.",0
"Exploration and representation learning without supervision become increasingly crucial in learning diverse and sparse environments. The principle of empowerment, which is based on information theory, formalizes the objective of unsupervised exploration by having an agent maximize its influence on the future states of its environment. However, previous approaches have limitations, as they either lack closed-loop feedback or do not have an internal state, resulting in a privileged final state being taken as the influence measure instead of the full trajectory. To address this, we propose a model-free method that considers the entire trajectory while still offering the advantages of option-based approaches. Our approach is successfully applied to settings with large action spaces, where meaningful action sequences are particularly challenging to discover.",1
"Online signature verification (OSV) is one of the most challenging tasks in writer identification and digital forensics. Owing to the large intra-individual variability, there is a critical requirement to accurately learn the intra-personal variations of the signature to achieve higher classification accuracy. To achieve this, in this paper, we propose an OSV framework based on deep convolutional Siamese network (DCSN). DCSN automatically extracts robust feature descriptions based on metric-based loss function which decreases intra-writer variability (Genuine-Genuine) and increases inter-individual variability (Genuine-Forgery) and directs the DCSN for effective discriminative representation learning for online signatures and extend it for one shot learning framework. Comprehensive experimentation conducted on three widely accepted benchmark datasets MCYT-100 (DB1), MCYT-330 (DB2) and SVC-2004-Task2 demonstrate the capability of our framework to distinguish the genuine and forgery samples. Experimental results confirm the efficiency of deep convolutional Siamese network based OSV by achieving a lower error rate as compared to many recent and state-of-the art OSV techniques.",0
"The identification of online signatures presents a significant challenge in digital forensics and writer identification due to the considerable variation within individuals. Accurate recognition of intra-personal variations is crucial to achieve higher classification accuracy. This study proposes an OSV framework that utilizes deep convolutional Siamese network (DCSN) to automatically extract robust feature descriptions. The metric-based loss function of DCSN minimizes intra-writer variability and maximizes inter-individual variability, facilitating discriminative representation learning for online signatures. The proposed framework is tested on three benchmark datasets, MCYT-100 (DB1), MCYT-330 (DB2), and SVC-2004-Task2, demonstrating its ability to distinguish genuine and forgery samples. The experimental results confirm the effectiveness of the DCSN-based OSV approach, achieving lower error rates than recent and state-of-the-art OSV techniques.",1
"Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode. We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations). This approach is based primarily on the strategic routing of data through the two latent variables, and thus is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data). We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.",0
"Although deep neural networks provide highly informative representations, they are often opaque in terms of what information they encode. Our proposed probabilistic modelling approach addresses this issue by learning two separate deep representations for the data: an invariant representation that encodes the class information and an equivariant representation that encodes the symmetry transformation defining the data point within the class manifold. This approach is achieved by strategically routing data through the two latent variables, making it conceptually transparent, easy to implement, and applicable to any data consisting of discrete classes of continuous distributions (e.g., objects in images, topics in language, individuals in behavioral data). Our approach shows qualitatively compelling representation learning and competitive quantitative performance in both supervised and semi-supervised settings, without the need for extensive fine-tuning, compared to similar modelling approaches in the literature.",1
"Graph neural networks, which generalize deep neural network models to graph structured data, have attracted increasing attention in recent years. They usually learn node representations by transforming, propagating and aggregating node features and have been proven to improve the performance of many graph related tasks such as node classification and link prediction. To apply graph neural networks for the graph classification task, approaches to generate the \textit{graph representation} from node representations are demanded. A common way is to globally combine the node representations. However, rich structural information is overlooked. Thus a hierarchical pooling procedure is desired to preserve the graph structure during the graph representation learning. There are some recent works on hierarchically learning graph representation analogous to the pooling step in conventional convolutional neural (CNN) networks. However, the local structural information is still largely neglected during the pooling process. In this paper, we introduce a pooling operator $\pooling$ based on graph Fourier transform, which can utilize the node features and local structures during the pooling process. We then design pooling layers based on the pooling operator, which are further combined with traditional GCN convolutional layers to form a graph neural network framework $\m$ for graph classification. Theoretical analysis is provided to understand $\pooling$ from both local and global perspectives. Experimental results of the graph classification task on $6$ commonly used benchmarks demonstrate the effectiveness of the proposed framework.",0
"In recent years, there has been a growing interest in graph neural networks, which extend deep neural network models to graph structured data. These networks typically learn node representations by transforming, propagating and aggregating node features, leading to improved performance on various graph related tasks such as node classification and link prediction. However, to use graph neural networks for graph classification, there is a need for methods to generate a graph representation from node representations. Although a common approach is to globally combine node representations, this overlooks important structural information. Therefore, a hierarchical pooling procedure is desired to preserve the graph structure during graph representation learning. Previous works have attempted to learn graph representation hierarchically, but local structural information is still overlooked during the pooling process. To address this, we propose a pooling operator based on graph Fourier transform that can leverage node features and local structures during pooling. We also design pooling layers based on this operator, which are combined with traditional GCN convolutional layers to form a graph neural network framework for graph classification. Theoretical analysis is provided to understand the proposed pooling operator from both local and global perspectives. Experimental results on six commonly used benchmarks demonstrate the effectiveness of our framework.",1
"Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning.",0
"Many machine learning problems require the estimation and optimization of Mutual Information (MI). However, bounding MI in high dimensions is a difficult task. To overcome this challenge, recent studies have used variational bounds that are parameterized by neural networks, but the relationship between these bounds and their tradeoffs has not been fully understood. This research aims to unify these developments into a single framework. The study reveals that existing variational lower bounds perform poorly when the MI is large, with either high bias or high variance. To address this issue, the paper introduces a continuum of lower bounds that can trade off bias and variance flexibly. The new bounds are effective for estimation and representation learning, as demonstrated by empirical characterization of the bounds and their gradients on high-dimensional, controlled problems.",1
"The recent use of `Big Code' with state-of-the-art deep learning methods offers promising avenues to ease program source code writing and correction. As a first step towards automatic code repair, we implemented a graph neural network model that predicts token types for Javascript programs. The predictions achieve an accuracy above $90\%$, which improves on previous similar work.",0
"By utilizing `Big Code' in conjunction with advanced deep learning techniques, there are exciting prospects to facilitate the process of writing and fixing program source code. Initially, we created a graph neural network model to forecast token types for Javascript programs as a preliminary measure towards automatic code repair. Our predictions have surpassed previous comparable efforts with an accuracy rate exceeding $90\%$.",1
"Performing machine learning on structured data is complicated by the fact that such data does not have vectorial form. Therefore, multiple approaches have emerged to construct vectorial representations of structured data, from kernel and distance approaches to recurrent, recursive, and convolutional neural networks. Recent years have seen heightened attention in this demanding field of research and several new approaches have emerged, such as metric learning on structured data, graph convolutional neural networks, and recurrent decoder networks for structured data. In this contribution, we provide an high-level overview of the state-of-the-art in representation learning and embeddings for structured data across a wide range of machine learning fields.",0
"The complexity of applying machine learning to structured data arises from the lack of vectorial form of such data. To address this issue, various methods have been developed to create vectorial representations of structured data, including kernel and distance approaches, as well as recurrent, recursive, and convolutional neural networks. Over the years, there has been a growing interest in this area of research, resulting in the emergence of new approaches like metric learning on structured data, graph convolutional neural networks, and recurrent decoder networks for structured data. This article presents an overview of the latest advancements in representation learning and embeddings for structured data across various machine learning domains.",1
"Large-scale graph data in real-world applications is often not static but dynamic, i. e., new nodes and edges appear over time. Current graph convolution approaches are promising, especially, when all the graph's nodes and edges are available during training. When unseen nodes and edges are inserted after training, it is not yet evaluated whether up-training or re-training from scratch is preferable. We construct an experimental setup, in which we insert previously unseen nodes and edges after training and conduct a limited amount of inference epochs. In this setup, we compare adapting pretrained graph neural networks against retraining from scratch. Our results show that pretrained models yield high accuracy scores on the unseen nodes and that pretraining is preferable over retraining from scratch. Our experiments represent a first step to evaluate and develop truly online variants of graph neural networks.",0
"In real-world applications, large-scale graph data is often dynamic, meaning that new edges and nodes are added over time. Graph convolution approaches have shown promise, particularly when all nodes and edges are available during training. However, it is unclear whether up-training or re-training from scratch is better when unseen elements are added after training. To investigate this, we conducted an experiment where we added previously unseen nodes and edges and performed a limited amount of inference epochs. In this experiment, we compared pretrained graph neural networks to retraining from scratch. Our findings indicate that pretrained models achieve high accuracy scores on unseen nodes and that pretraining is preferable to retraining from scratch. Our study represents an initial step towards developing online variants of graph neural networks.",1
"Autoencoders are a deep learning model for representation learning. When trained to minimize the distance between the data and its reconstruction, linear autoencoders (LAEs) learn the subspace spanned by the top principal directions but cannot learn the principal directions themselves. In this paper, we prove that $L_2$-regularized LAEs are symmetric at all critical points and learn the principal directions as the left singular vectors of the decoder. We smoothly parameterize the critical manifold and relate the minima to the MAP estimate of probabilistic PCA. We illustrate these results empirically and consider implications for PCA algorithms, computational neuroscience, and the algebraic topology of learning.",0
"Autoencoders are a type of deep learning model used for representation learning. When trained to minimize the difference between the original data and its reconstructed version, linear autoencoders (LAEs) can identify the subspace defined by the top principal directions but are unable to identify the principal directions themselves. This study demonstrates that LAEs with $L_2$ regularization are symmetric at all critical points and can identify the principal directions as the left singular vectors of the decoder. The study also introduces a smooth parameterization of the critical manifold and explores the relationship between the minima and the MAP estimate of probabilistic PCA. Empirical evidence is provided to support the findings, and the implications for PCA algorithms, computational neuroscience, and the algebraic topology of learning are discussed.",1
"We introduce a novel unsupervised domain adaptation approach for object detection. We aim to alleviate the imperfect translation problem of pixel-level adaptations, and the source-biased discriminativity problem of feature-level adaptations simultaneously. Our approach is composed of two stages, i.e., Domain Diversification (DD) and Multi-domain-invariant Representation Learning (MRL). At the DD stage, we diversify the distribution of the labeled data by generating various distinctive shifted domains from the source domain. At the MRL stage, we apply adversarial learning with a multi-domain discriminator to encourage feature to be indistinguishable among the domains. DD addresses the source-biased discriminativity, while MRL mitigates the imperfect image translation. We construct a structured domain adaptation framework for our learning paradigm and introduce a practical way of DD for implementation. Our method outperforms the state-of-the-art methods by a large margin of 3%~11% in terms of mean average precision (mAP) on various datasets.",0
"Our innovative technique for object detection involves unsupervised domain adaptation. Our goal is to tackle both the imperfect translation problem of pixel-level adaptations and the source-biased discriminativity problem of feature-level adaptations simultaneously. We split our approach into two stages: Domain Diversification (DD) and Multi-domain-invariant Representation Learning (MRL). During the DD stage, we generate several distinct shifted domains from the source domain to diversify the labeled data distribution. During the MRL stage, we use adversarial learning with a multi-domain discriminator to encourage feature indistinguishability among the domains. DD addresses source-biased discriminativity, while MRL mitigates the imperfect image translation. We present a structured domain adaptation framework and propose a practical DD implementation strategy. Our method outperforms state-of-the-art methods, achieving a substantial mean average precision (mAP) improvement of 3% to 11% across various datasets.",1
"The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efficient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a commonly accepted definition and validation procedure is missing. We provide a causal perspective on representation learning which covers disentanglement and domain shift robustness as special cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of deep latent variable models. We show how this metric can be estimated from labeled observational data and further provide an efficient estimation algorithm that scales linearly in the dataset size.",0
"It is crucial for neural networks to efficiently and reliably learn disentangled representations that can separate underlying factors in complex and unstructured data. Although numerous methods have been suggested to achieve this objective, there is no widely accepted definition or validation process. Our approach to representation learning is based on causality and encompasses disentanglement and domain shift resilience. Using our causal framework, we have introduced a novel metric to quantitatively assess deep latent variable models. We demonstrate how this metric can be computed from labeled observational data and offer a fast estimation algorithm that scales proportionally with the dataset size.",1
Sensors are an integral part of modern Internet of Things (IoT) applications. There is a critical need for the analysis of heterogeneous multivariate temporal data obtained from the individual sensors of these systems. In this paper we particularly focus on the problem of the scarce amount of training data available per sensor. We propose a novel federated multi-task hierarchical attention model (FATHOM) that jointly trains classification/regression models from multiple sensors. The attention mechanism of the proposed model seeks to extract feature representations from the input and learn a shared representation focused on time dimensions across multiple sensors. The underlying temporal and non-linear relationships are modeled using a combination of attention mechanism and long-short term memory (LSTM) networks. We find that our proposed method outperforms a wide range of competitive baselines in both classification and regression settings on activity recognition and environment monitoring datasets. We further provide visualization of feature representations learned by our model at the input sensor level and central time level.,0
Modern Internet of Things (IoT) applications heavily rely on sensors. Analyzing heterogeneous multivariate temporal data from individual sensors is crucial. This paper specifically addresses the challenge of limited training data per sensor. A novel federated multi-task hierarchical attention model (FATHOM) is proposed to jointly train classification/regression models from multiple sensors. The attention mechanism of the proposed model extracts feature representations from the input and learns a shared representation focused on time dimensions across multiple sensors. The model uses a combination of attention mechanism and long-short term memory (LSTM) networks to model underlying temporal and non-linear relationships. The proposed method outperforms competitive baselines in both classification and regression settings on activity recognition and environment monitoring datasets. Feature representations learned by the model are visualized at the input sensor level and central time level.,1
"Multiple clustering aims at exploring alternative clusterings to organize the data into meaningful groups from different perspectives. Existing multiple clustering algorithms are designed for single-view data. We assume that the individuality and commonality of multi-view data can be leveraged to generate high-quality and diverse clusterings. To this end, we propose a novel multi-view multiple clustering (MVMC) algorithm. MVMC first adapts multi-view self-representation learning to explore the individuality encoding matrices and the shared commonality matrix of multi-view data. It additionally reduces the redundancy (i.e., enhancing the individuality) among the matrices using the Hilbert-Schmidt Independence Criterion (HSIC), and collects shared information by forcing the shared matrix to be smooth across all views. It then uses matrix factorization on the individual matrices, along with the shared matrix, to generate diverse clusterings of high-quality. We further extend multiple co-clustering on multi-view data and propose a solution called multi-view multiple co-clustering (MVMCC). Our empirical study shows that MVMC (MVMCC) can exploit multi-view data to generate multiple high-quality and diverse clusterings (co-clusterings), with superior performance to the state-of-the-art methods.",0
"The objective of multiple clustering is to create meaningful data groupings from various viewpoints. However, current multiple clustering algorithms only cater to single-view data. We believe that multi-view data's individuality and commonality can be utilized to generate superior and diverse clusterings. Therefore, we propose a new algorithm called multi-view multiple clustering (MVMC). MVMC employs multi-view self-representation learning to identify individuality encoding matrices and a shared commonality matrix of multi-view data. It leverages the Hilbert-Schmidt Independence Criterion (HSIC) to reduce redundancy and uses matrix factorization to produce high-quality and varied clusterings. In addition, we introduce multi-view multiple co-clustering (MVMCC) to extend multiple co-clustering on multi-view data. Our study shows that MVMC and MVMCC can produce multiple high-quality and varied clusterings and co-clusterings, respectively, surpassing current methods.",1
"We address the problem of graph classification based only on structural information. Inspired by natural language processing techniques (NLP), our model sequentially embeds information to estimate class membership probabilities. Besides, we experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We experimentally show that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis and give some insights on whether the node prediction helps the model better classify graphs.",0
"Our focus is on graph classification using solely structural information. Drawing inspiration from natural language processing (NLP) methods, we adopt a sequential information embedding approach to estimate membership probabilities. Additionally, we explore NLP-inspired variational regularization techniques that enable the model to predict the next node in the sequence as it reads it. Our experimental findings demonstrate that our model outperforms existing methods in classifying standard molecular datasets. Lastly, we conduct a qualitative analysis to evaluate the impact of node prediction on the model's graph classification performance.",1
"We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.",0
"The focus of our research is on learning how to represent graph data. While convolutional neural networks are well-suited for images, they face significant challenges when it comes to graph data. Since images can be seen as a type of graph with nodes arranged on a 2D lattice, embedding tasks for graphs can be compared to pixel-wise prediction tasks like segmentation for images. While encoder-decoder architectures like U-Nets have proven successful for many image prediction tasks, no similar methods exist for graph data. The issue lies in the unnatural nature of pooling and up-sampling operations for graphs. To tackle this problem, we introduce new graph pooling (gPool) and unpooling (gUnpool) operations. The gPool layer selects nodes to form a smaller graph based on their scalar projection values on a trainable projection vector, while the gUnpool layer restores the original graph structure using the position information of the selected nodes. With these new layers, we developed the graph U-Nets encoder-decoder model for graphs. Our results show that our approach outperforms previous models in both node classification and graph classification tasks.",1
"Graph neural network (GNN), as a powerful representation learning model on graph data, attracts much attention across various disciplines. However, recent studies show that GNN is vulnerable to adversarial attacks. How to make GNN more robust? What are the key vulnerabilities in GNN? How to address the vulnerabilities and defense GNN against the adversarial attacks? In this paper, we propose DefNet, an effective adversarial defense framework for GNNs. In particular, we first investigate the latent vulnerabilities in every layer of GNNs and propose corresponding strategies including dual-stage aggregation and bottleneck perceptron. Then, to cope with the scarcity of training data, we propose an adversarial contrastive learning method to train the GNN in a conditional GAN manner by leveraging the high-level graph representation. Extensive experiments on three public datasets demonstrate the effectiveness of DefNet in improving the robustness of popular GNN variants, such as Graph Convolutional Network and GraphSAGE, under various types of adversarial attacks.",0
"The Graph Neural Network (GNN) is a representation learning model that is gaining popularity in multiple fields due to its ability to analyze graph data. However, recent research has revealed that GNN is susceptible to adversarial attacks. This raises questions about how to strengthen GNN and protect it from such attacks. This paper proposes an effective defense framework for GNNs called DefNet. To achieve this, we first identify and address the vulnerabilities in the different layers of GNNs using dual-stage aggregation and bottleneck perceptron strategies. We also introduce an adversarial contrastive learning method to train GNN in a conditional GAN manner, leveraging high-level graph representations to address the limited training data. Extensive experiments on three public datasets demonstrate the effectiveness of DefNet in enhancing the robustness of popular GNN variants, such as Graph Convolutional Network and GraphSAGE, against various types of adversarial attacks.",1
"Hyperbolic space is a geometry that is known to be well-suited for representation learning of data with an underlying hierarchical structure. In this paper, we present a novel hyperbolic distribution called \textit{pseudo-hyperbolic Gaussian}, a Gaussian-like distribution on hyperbolic space whose density can be evaluated analytically and differentiated with respect to the parameters. Our distribution enables the gradient-based learning of the probabilistic models on hyperbolic space that could never have been considered before. Also, we can sample from this hyperbolic probability distribution without resorting to auxiliary means like rejection sampling. As applications of our distribution, we develop a hyperbolic-analog of variational autoencoder and a method of probabilistic word embedding on hyperbolic space. We demonstrate the efficacy of our distribution on various datasets including MNIST, Atari 2600 Breakout, and WordNet.",0
"The geometry of hyperbolic space is suitable for representation learning of data that has an underlying hierarchical structure. This research paper introduces a new hyperbolic distribution, called the ""pseudo-hyperbolic Gaussian,"" which is similar to the Gaussian distribution but is designed for use in hyperbolic space. The density of our distribution can be evaluated analytically and differentiated with respect to the parameters, making it possible to learn probabilistic models on hyperbolic space using gradient-based methods. Additionally, our distribution allows sampling without the need for auxiliary methods such as rejection sampling. We demonstrate the effectiveness of our distribution on various datasets, including MNIST, Atari 2600 Breakout, and WordNet, by developing a hyperbolic-analog of variational autoencoder and a method of probabilistic word embedding on hyperbolic space.",1
"Learning an effective representation for high-dimensional data is a challenging problem in reinforcement learning (RL). Deep reinforcement learning (DRL) such as Deep Q networks (DQN) achieves remarkable success in computer games by learning deeply encoded representation from convolution networks. In this paper, we propose a simple yet very effective method for representation learning with DRL algorithms. Our key insight is that features learned by DRL algorithms are highly correlated, which interferes with learning. By adding a regularized loss that penalizes correlation in latent features (with only slight computation), we decorrelate features represented by deep neural networks incrementally. On 49 Atari games, with the same regularization factor, our decorrelation algorithms perform $70\%$ in terms of human-normalized scores, which is $40\%$ better than DQN. In particular, ours performs better than DQN on 39 games with 4 close ties and lost only slightly on $6$ games. Empirical results also show that the decorrelation method applies to Quantile Regression DQN (QR-DQN) and significantly boosts performance. Further experiments on the losing games show that our decorelation algorithms can win over DQN and QR-DQN with a fined tuned regularization factor.",0
"Reinforcement learning (RL) faces a daunting challenge in acquiring an effective representation for high-dimensional data. To address this issue, deep reinforcement learning (DRL) has been developed, with Deep Q networks (DQN) achieving remarkable success in computer games by encoding representations in convolution networks. This paper proposes a straightforward yet highly effective method for representation learning using DRL algorithms. The authors found that DRL algorithm features are highly correlated, which can interfere with learning. To address this, the authors added a regularized loss that penalizes correlation in latent features, thereby incrementally decorrelating features represented by deep neural networks with only slight computation. Their decorrelation algorithms achieved a 70% human-normalized score on 49 Atari games, outperforming DQN by 40%, with 39 games showing better performance, four close ties, and only a slight loss in six games. Empirical results revealed that the decorrelation method also applied to Quantile Regression DQN (QR-DQN) and significantly boosted performance. Further experiments showed that the decorrelation algorithms could win over DQN and QR-DQN with a finely tuned regularization factor on the losing games.",1
"We propose an end-to-end deep learning learning model for graph classification and representation learning that is invariant to permutation of the nodes of the input graphs. We address the challenge of learning a fixed size graph representation for graphs of varying dimensions through a differentiable node attention pooling mechanism. In addition to a theoretical proof of its invariance to permutation, we provide empirical evidence demonstrating the statistically significant gain in accuracy when faced with an isomorphic graph classification task given only a small number of training examples. We analyse the effect of four different matrices to facilitate the local message passing mechanism by which graph convolutions are performed vs. a matrix parametrised by a learned parameter pair able to transition smoothly between the former. Finally, we show that our model achieves competitive classification performance with existing techniques on a set of molecule datasets.",0
"Our proposed deep learning model is designed for graph classification and representation learning, and it is capable of maintaining permutation invariance for input graph nodes. To address the challenge of creating a fixed size graph representation for varying graph dimensions, we have developed a differentiable node attention pooling mechanism. Our theoretical proof confirms the model's permutation invariance, and we have also provided empirical evidence that demonstrates a significant improvement in accuracy when faced with an isomorphic graph classification task with limited training examples. We have conducted an analysis of four different matrices that facilitate the local message passing mechanism for graph convolutions, as well as a matrix parametrized by a learned parameter pair that can smoothly transition between them. Ultimately, our model's performance is competitive with existing techniques when tested on a range of molecule datasets.",1
"In this paper, we reformulate the forest representation learning approach as an additive model which boosts the augmented feature instead of the prediction. We substantially improve the upper bound of generalization gap from $\mathcal{O}(\sqrt\frac{\ln m}{m})$ to $\mathcal{O}(\frac{\ln m}{m})$, while $\lambda$ - the margin ratio between the margin standard deviation and the margin mean is small enough. This tighter upper bound inspires us to optimize the margin distribution ratio $\lambda$. Therefore, we design the margin distribution reweighting approach (mdDF) to achieve small ratio $\lambda$ by boosting the augmented feature. Experiments and visualizations confirm the effectiveness of the approach in terms of performance and representation learning ability. This study offers a novel understanding of the cascaded deep forest from the margin-theory perspective and further uses the mdDF approach to guide the layer-by-layer forest representation learning.",0
"The forest representation learning approach has been reformulated in this paper as an additive model that enhances the augmented feature instead of the prediction. By ensuring that the margin ratio $\lambda$ between the margin standard deviation and the margin mean is small, we have been able to improve the upper bound of the generalization gap from $\mathcal{O}(\sqrt\frac{\ln m}{m})$ to $\mathcal{O}(\frac{\ln m}{m}). This tighter upper bound has motivated us to optimize the margin distribution ratio $\lambda$ and we have developed the margin distribution reweighting approach (mdDF) to achieve a small ratio $\lambda$ through augmented feature boosting. Through experiments and visualizations, we have demonstrated that this approach is effective in enhancing performance and representation learning ability. Our study offers insights into the cascaded deep forest from the margin-theory perspective and utilizes the mdDF approach to guide layer-by-layer forest representation learning.",1
"Vehicle Re-identification is attracting more and more attention in recent years. One of the most challenging problems is to learn an efficient representation for a vehicle from its multi-viewpoint images. Existing methods tend to derive features of dimensions ranging from thousands to tens of thousands. In this work we proposed a deep learning based framework that can lead to an efficient representation of vehicles. While the dimension of the learned features can be as low as 256, experiments on different datasets show that the Top-1 and Top-5 retrieval accuracies exceed multiple state-of-the-art methods. The key to our framework is two-fold. Firstly, variational feature learning is employed to generate variational features which are more discriminating. Secondly, long short-term memory (LSTM) is used to learn the relationship among different viewpoints of a vehicle. The LSTM also plays as an encoder to downsize the features.",0
"Over the years, there has been an increasing focus on Vehicle Re-identification. The primary challenge has been finding an effective way to represent a vehicle based on its multi-viewpoint images. Many existing methods generate features with dimensions ranging from thousands to tens of thousands. However, we have proposed a deep learning-based framework that can produce a more efficient representation of vehicles. Our framework generates features with dimensions as low as 256, yet it outperforms multiple state-of-the-art methods in terms of Top-1 and Top-5 retrieval accuracies across various datasets. Our approach has two key aspects: variational feature learning, which creates more discriminating features, and the use of long short-term memory (LSTM) to learn the relationship among different vehicle viewpoints while also downsizing the features.",1
"Image completion is the problem of generating whole images from fragments only. It encompasses inpainting (generating a patch given its surrounding), reverse inpainting/extrapolation (generating the periphery given the central patch) as well as colorization (generating one or several channels given other ones). In this paper, we employ a deep network to perform image completion, with adversarial training as well as perceptual and completion losses, and call it the ``missing data encoder'' (MDE). We consider several configurations based on how the seed fragments are chosen. We show that training MDE for ``random extrapolation and colorization'' (MDE-REC), i.e. using random channel-independent fragments, allows a better capture of the image semantics and geometry. MDE training makes use of a novel ``hide-and-seek'' adversarial loss, where the discriminator seeks the original non-masked regions, while the generator tries to hide them. We validate our models both qualitatively and quantitatively on several datasets, showing their interest for image completion, unsupervised representation learning as well as face occlusion handling.",0
"The problem of image completion involves creating complete images using only fragments. This includes tasks such as inpainting, reverse inpainting/extrapolation, and colorization. Our paper presents a solution to this problem using a deep network called the ""missing data encoder"" (MDE), which utilizes adversarial training, as well as perceptual and completion losses. We explore different configurations based on the selection of seed fragments and demonstrate that training MDE for ""random extrapolation and colorization"" (MDE-REC), which uses random, channel-independent fragments, results in a better capture of image semantics and geometry. Our MDE training employs a unique ""hide-and-seek"" adversarial loss, where the discriminator seeks the original non-masked regions while the generator tries to hide them. We validate our models qualitatively and quantitatively on various datasets and show its usefulness for image completion, unsupervised representation learning, and face occlusion handling.",1
"While deep representation learning has become increasingly capable of separating task-relevant representations from other confounding factors in the data, two significant challenges remain. First, there is often an unknown and potentially infinite number of confounding factors coinciding in the data. Second, not all of these factors are readily observable. In this paper, we present a deep conditional generative model that learns to disentangle a task-relevant representation from an unknown number of confounding factors that may grow infinitely. This is achieved by marrying the representational power of deep generative models with Bayesian non-parametric factor models, where a supervised deterministic encoder learns task-related representation and a probabilistic encoder with an Indian Buffet Process (IBP) learns the unknown number of unobservable confounding factors. We tested the presented model in two datasets: a handwritten digit dataset (MNIST) augmented with colored digits and a clinical ECG dataset with significant inter-subject variations and augmented with signal artifacts. These diverse data sets highlighted the ability of the presented model to grow with the complexity of the data and identify the absence or presence of unobserved confounding factors.",0
"Despite the increasing capabilities of deep representation learning in distinguishing task-relevant representations from other confounding factors in data, two significant challenges remain. Firstly, there may be an unknown and potentially infinite number of confounding factors that coincide within the data. Secondly, not all of these factors may be easily observable. This research paper proposes a deep conditional generative model that can disentangle a task-relevant representation from an unlimited number of confounding factors that may arise. This is achieved by combining the representational power of deep generative models with Bayesian non-parametric factor models. The model includes a deterministic encoder that learns the task-related representation and a probabilistic encoder with an Indian Buffet Process (IBP) that learns the unknown number of unobservable confounding factors. The model was tested using two datasets, one containing handwritten digits (MNIST) augmented with colored digits and another containing clinical ECG data with significant inter-subject variations and signal artifacts. The results demonstrate the model's ability to adapt to the complexity of the data and identify the presence or absence of unobserved confounding factors.",1
"Graphs are complex objects that do not lend themselves easily to typical learning tasks. Recently, a range of approaches based on graph kernels or graph neural networks have been developed for graph classification and for representation learning on graphs in general. As the developed methodologies become more sophisticated, it is important to understand which components of the increasingly complex methods are necessary or most effective.   As a first step, we develop a simple yet meaningful graph representation, and explore its effectiveness in graph classification. We test our baseline representation for the graph classification task on a range of graph datasets. Interestingly, this simple representation achieves similar performance as the state-of-the-art graph kernels and graph neural networks for non-attributed graph classification. Its performance on classifying attributed graphs is slightly weaker as it does not incorporate attributes. However, given its simplicity and efficiency, we believe that it still serves as an effective baseline for attributed graph classification. Our graph representation is efficient (linear-time) to compute. We also provide a simple connection with the graph neural networks.   Note that these observations are only for the task of graph classification while existing methods are often designed for a broader scope including node embedding and link prediction. The results are also likely biased due to the limited amount of benchmark datasets available. Nevertheless, the good performance of our simple baseline calls for the development of new, more comprehensive benchmark datasets so as to better evaluate and analyze different graph learning methods. Furthermore, given the computational efficiency of our graph summary, we believe that it is a good candidate as a baseline method for future graph classification (or even other graph learning) studies.",0
"Graphs are intricate objects that are not ideal for typical learning tasks. Recently, various graph kernels and graph neural networks have been developed for graph classification and representation learning on graphs. As these methodologies become more advanced, it is crucial to determine which components are necessary or most effective. To begin, we create a straightforward yet meaningful graph representation and test its efficiency in graph classification on several datasets. Remarkably, this simple representation performs similarly to the state-of-the-art graph kernels and neural networks for non-attributed graph classification. However, its performance weakens slightly in classifying attributed graphs as it does not incorporate attributes. Despite this, we believe it serves as an effective baseline for attributed graph classification due to its efficiency and simplicity. Our graph representation is quick to compute and has a simple connection to graph neural networks. It's important to note that our observations are only for graph classification and not for broader tasks like node embedding and link prediction. Additionally, our results may be biased due to the limited benchmark datasets available. Nonetheless, the good performance of our simple baseline calls for more comprehensive benchmark datasets to better evaluate and analyze different graph learning methods. Moreover, given the computational efficiency of our graph summary, we believe that it is a suitable candidate as a baseline method for future graph classification and other graph learning studies.",1
"The learning of domain-invariant representations in the context of domain adaptation with neural networks is considered. We propose a new regularization method that minimizes the discrepancy between domain-specific latent feature representations directly in the hidden activation space. Although some standard distribution matching approaches exist that can be interpreted as the matching of weighted sums of moments, e.g. Maximum Mean Discrepancy (MMD), an explicit order-wise matching of higher order moments has not been considered before. We propose to match the higher order central moments of probability distributions by means of order-wise moment differences. Our model does not require computationally expensive distance and kernel matrix computations. We utilize the equivalent representation of probability distributions by moment sequences to define a new distance function, called Central Moment Discrepancy (CMD). We prove that CMD is a metric on the set of probability distributions on a compact interval. We further prove that convergence of probability distributions on compact intervals w.r.t. the new metric implies convergence in distribution of the respective random variables. We test our approach on two different benchmark data sets for object recognition (Office) and sentiment analysis of product reviews (Amazon reviews). CMD achieves a new state-of-the-art performance on most domain adaptation tasks of Office and outperforms networks trained with MMD, Variational Fair Autoencoders and Domain Adversarial Neural Networks on Amazon reviews. In addition, a post-hoc parameter sensitivity analysis shows that the new approach is stable w.r.t. parameter changes in a certain interval. The source code of the experiments is publicly available.",0
"This article focuses on the acquisition of domain-invariant representations through neural networks in the context of domain adaptation. The authors introduce a novel regularization method that aims to minimize the discrepancy between domain-specific latent feature representations directly in the hidden activation space. While existing distribution matching approaches such as Maximum Mean Discrepancy (MMD) interpret the matching of weighted sums of moments, the authors propose an order-wise matching of higher order moments. They achieve this by matching the higher order central moments of probability distributions through order-wise moment differences. This method, called Central Moment Discrepancy (CMD), does not require computationally expensive distance and kernel matrix computations. The authors utilize the equivalent representation of probability distributions by moment sequences to define CMD as a metric on the set of probability distributions on a compact interval. They further prove that convergence of probability distributions on compact intervals w.r.t. the new metric implies convergence in distribution of the respective random variables. The authors conduct experiments on two benchmark data sets for object recognition and sentiment analysis of product reviews, achieving a new state-of-the-art performance on most domain adaptation tasks of Office and outperforming networks trained with MMD, Variational Fair Autoencoders, and Domain Adversarial Neural Networks on Amazon reviews. Lastly, a post-hoc parameter sensitivity analysis shows the stability of the new approach w.r.t. parameter changes in a certain interval. The source code of the experiments is publicly available.",1
"Deep generative architectures provide a way to model not only images but also complex, 3-dimensional objects, such as point clouds. In this work, we present a novel method to obtain meaningful representations of 3D shapes that can be used for challenging tasks including 3D points generation, reconstruction, compression, and clustering. Contrary to existing methods for 3D point cloud generation that train separate decoupled models for representation learning and generation, our approach is the first end-to-end solution that allows to simultaneously learn a latent space of representation and generate 3D shape out of it. Moreover, our model is capable of learning meaningful compact binary descriptors with adversarial training conducted on a latent space. To achieve this goal, we extend a deep Adversarial Autoencoder model (AAE) to accept 3D input and create 3D output. Thanks to our end-to-end training regime, the resulting method called 3D Adversarial Autoencoder (3dAAE) obtains either binary or continuous latent space that covers a much wider portion of training data distribution. Finally, our quantitative evaluation shows that 3dAAE provides state-of-the-art results for 3D points clustering and 3D object retrieval.",0
"The utilization of deep generative architectures has enabled the modeling of complex, three-dimensional objects like point clouds, in addition to images. Our research introduces a new technique for acquiring meaningful representations of three-dimensional shapes, which can be applied to challenging tasks such as the generation, compression, reconstruction, and clustering of 3D points. Unlike existing approaches that employ separate models for representation learning and generation, our method is the first end-to-end solution capable of simultaneously learning a latent space of representation and generating 3D shapes. Additionally, our model can learn compact binary descriptors with adversarial training on a latent space. We have extended the deep Adversarial Autoencoder model (AAE) to accept and generate 3D input and output, resulting in a method called 3D Adversarial Autoencoder (3dAAE). Our end-to-end training regime allows for a broader coverage of the training data distribution with either binary or continuous latent space. Lastly, our quantitative evaluation demonstrates that 3dAAE achieves state-of-the-art results for 3D points clustering and 3D object retrieval.",1
"Competitive diving is a well recognized aquatic sport in which a person dives from a platform or a springboard into the water. Based on the acrobatics performed during the dive, diving is classified into a finite set of action classes which are standardized by FINA. In this work, we propose an attention guided LSTM-based neural network architecture for the task of diving classification. The network takes the frames of a diving video as input and determines its class. We evaluate the performance of the proposed model on a recently introduced competitive diving dataset, Diving48. It contains over 18000 video clips which covers 48 classes of diving. The proposed model outperforms the classification accuracy of the state-of-the-art models in both 2D and 3D frameworks by 11.54% and 4.24%, respectively. We show that the network is able to localize the diver in the video frames during the dive without being trained with such a supervision.",0
"Competitive diving is an aquatic sport that involves diving from a platform or springboard into the water and is divided into a set of action classes standardized by FINA. This study proposes an attention-guided LSTM-based neural network architecture for classifying dives. The network takes diving video frames as input and evaluates its performance on the Diving48 dataset, which includes over 18,000 video clips covering 48 diving classes. Results show that the proposed model outperforms state-of-the-art models in both 2D and 3D frameworks by 11.54% and 4.24% respectively. The network is also able to locate the diver in the video frames during the dive without the need for specific training.",1
"In this paper we seek methods to effectively detect urban micro-events. Urban micro-events are events which occur in cities, have limited geographical coverage and typically affect only a small group of citizens. Because of their scale these are difficult to identify in most data sources. However, by using citizen sensing to gather data, detecting them becomes feasible. The data gathered by citizen sensing is often multimodal and, as a consequence, the information required to detect urban micro-events is distributed over multiple modalities. This makes it essential to have a classifier capable of combining them. In this paper we explore several methods of creating such a classifier, including early, late, hybrid fusion and representation learning using multimodal graphs. We evaluate performance on a real world dataset obtained from a live citizen reporting system. We show that a multimodal approach yields higher performance than unimodal alternatives. Furthermore, we demonstrate that our hybrid combination of early and late fusion with multimodal embeddings performs best in classification of urban micro-events.",0
"The aim of this paper is to discover effective techniques for identifying urban micro-events, which are localized events that impact a small group of individuals in cities. Due to their limited scope, detecting these events is challenging using most data sources. However, citizen sensing can provide valuable data to aid in their detection. This data is often varied, with information spread across multiple modalities, making it necessary to have a classifier that can integrate them. This paper examines several methods for creating such a classifier, including early, late, hybrid fusion, and representation learning using multimodal graphs. The performance of these methods is evaluated using a real-world dataset obtained from a live citizen reporting system. The study shows that a multimodal approach outperforms unimodal alternatives, and that the hybrid combination of early and late fusion with multimodal embeddings is the most effective method for classifying urban micro-events.",1
"Data for face analysis often exhibit highly-skewed class distribution, i.e., most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary deep learning methods typically follow classic strategies such as class re-sampling or cost-sensitive training. In this paper, we conduct extensive and systematic experiments to validate the effectiveness of these classic schemes for representation learning on class-imbalanced data. We further demonstrate that more discriminative deep representation can be learned by enforcing a deep network to maintain inter-cluster margins both within and between classes. This tight constraint effectively reduces the class imbalance inherent in the local data neighborhood, thus carving much more balanced class boundaries locally. We show that it is easy to deploy angular margins between the cluster distributions on a hypersphere manifold. Such learned Cluster-based Large Margin Local Embedding (CLMLE), when combined with a simple k-nearest cluster algorithm, shows significant improvements in accuracy over existing methods on both face recognition and face attribute prediction tasks that exhibit imbalanced class distribution.",0
"The distribution of data in face analysis often displays a significant imbalance, where certain classes are overrepresented and others have only a small number of instances. To address this issue, deep learning methods typically use classic strategies like class re-sampling or cost-sensitive training. This paper presents a series of systematic experiments that validate these methods' effectiveness for representation learning on class-imbalanced data. The study also demonstrates that enforcing inter-cluster margins within and between classes can lead to more discriminative deep representation. This constraint reduces the class imbalance inherent in local data neighborhoods, creating more balanced class boundaries. Angular margins between cluster distributions on a hypersphere manifold can accomplish this. The study shows that the Cluster-based Large Margin Local Embedding (CLMLE) learned in this way, when combined with a simple k-nearest cluster algorithm, significantly improves accuracy for face recognition and face attribute prediction tasks with imbalanced class distribution.",1
"As an efficient and scalable graph neural network, GraphSAGE has enabled an inductive capability for inferring unseen nodes or graphs by aggregating subsampled local neighborhoods and by learning in a mini-batch gradient descent fashion. The neighborhood sampling used in GraphSAGE is effective in order to improve computing and memory efficiency when inferring a batch of target nodes with diverse degrees in parallel. Despite this advantage, the default uniform sampling suffers from high variance in training and inference, leading to sub-optimum accuracy. We propose a new data-driven sampling approach to reason about the real-valued importance of a neighborhood by a non-linear regressor, and to use the value as a criterion for subsampling neighborhoods. The regressor is learned using a value-based reinforcement learning. The implied importance for each combination of vertex and neighborhood is inductively extracted from the negative classification loss output of GraphSAGE. As a result, in an inductive node classification benchmark using three datasets, our method enhanced the baseline using the uniform sampling, outperforming recent variants of a graph neural network in accuracy.",0
"GraphSAGE is a highly effective and scalable graph neural network that allows for inductive inference of unseen nodes or graphs through the aggregation of subsampled local neighborhoods and mini-batch gradient descent learning. While neighborhood sampling in GraphSAGE is useful for improving computing and memory efficiency when inferring diverse target nodes in parallel, the default uniform sampling technique has high variance in training and inference, thus reducing accuracy. To address this issue, we propose a data-driven sampling approach that uses a non-linear regressor to determine the real-valued importance of a neighborhood and subsamples based on this value. The regressor is trained using value-based reinforcement learning, with importance determined by the negative classification loss output of GraphSAGE for each vertex and neighborhood combination. Our method outperformed recent variants of graph neural networks in accuracy on three datasets in an inductive node classification benchmark using the uniform sampling baseline.",1
In this paper we present a self-supervised method for representation learning utilizing two different modalities. Based on the observation that cross-modal information has a high semantic meaning we propose a method to effectively exploit this signal. For our approach we utilize video data since it is available on a large scale and provides easily accessible modalities given by RGB and optical flow. We demonstrate state-of-the-art performance on highly contested action recognition datasets in the context of self-supervised learning. We show that our feature representation also transfers to other tasks and conduct extensive ablation studies to validate our core contributions. Code and model can be found at https://github.com/nawidsayed/Cross-and-Learn.,0
"In this article, we introduce a technique for learning representations through self-supervision that employs two different modalities. Our approach is based on the idea that cross-modal information is highly semantically meaningful, and we propose a method for effectively utilizing this information. We use video data as our primary source of information since it is widely available and provides easily accessible modalities such as RGB and optical flow. We provide evidence of our state-of-the-art performance in action recognition datasets, within the context of self-supervised learning. Moreover, we demonstrate that our feature representation can be transferred to other tasks, and we conduct extensive experiments to validate our key contributions. The code and model are accessible at https://github.com/nawidsayed/Cross-and-Learn.",1
"Graph Convolutional Networks (GCNs) have been widely studied for graph data representation and learning tasks. Existing GCNs generally use a fixed single graph which may lead to weak suboptimal for data representation/learning and are also hard to deal with multiple graphs. To address these issues, we propose a novel Graph Optimized Convolutional Network (GOCN) for graph data representation and learning. Our GOCN is motivated based on our re-interpretation of graph convolution from a regularization/optimization framework. The core idea of GOCN is to formulate graph optimization and graph convolutional representation into a unified framework and thus conducts both of them cooperatively to boost their respective performance in GCN learning scheme. Moreover, based on the proposed unified graph optimization-convolution framework, we propose a novel Multiple Graph Optimized Convolutional Network (M-GOCN) to naturally address the data with multiple graphs. Experimental results demonstrate the effectiveness and benefit of the proposed GOCN and M-GOCN.",0
"Extensive research has been conducted on Graph Convolutional Networks (GCNs) for graph data representation and learning tasks. However, these networks often utilize a singular, unchanging graph, which can lead to suboptimal results for data representation and learning, and can be challenging when dealing with multiple graphs. To overcome these issues, we have introduced a new Graph Optimized Convolutional Network (GOCN) for graph data representation and learning. Our GOCN approach is inspired by our interpretation of graph convolution from a regularization/optimization perspective. The primary concept of GOCN is to bring together graph optimization and graph convolutional representation in a single framework, allowing them to work together to enhance their performance in the GCN learning scheme. Additionally, we have proposed a Multiple Graph Optimized Convolutional Network (M-GOCN) that uses the unified graph optimization-convolution framework to naturally handle data with multiple graphs. Our experimental results prove the effectiveness and advantages of the proposed GOCN and M-GOCN.",1
"We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.",0
"PyTorch Geometric is a library developed on PyTorch for deep learning on input data that is irregularly structured, such as graphs, point clouds, and manifolds. The library includes various methods from the domains of relational learning and 3D data processing, in addition to general graph data structures and processing methods. PyTorch Geometric achieves high data throughput by using sparse GPU acceleration, dedicated CUDA kernels, and efficient mini-batch handling for input examples of different sizes. This article presents a detailed overview of the library and provides a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.",1
"Facial action unit (AU) recognition is a crucial task for facial expressions analysis and has attracted extensive attention in the field of artificial intelligence and computer vision. Existing works have either focused on designing or learning complex regional feature representations, or delved into various types of AU relationship modeling. Albeit with varying degrees of progress, it is still arduous for existing methods to handle complex situations. In this paper, we investigate how to integrate the semantic relationship propagation between AUs in a deep neural network framework to enhance the feature representation of facial regions, and propose an AU semantic relationship embedded representation learning (SRERL) framework. Specifically, by analyzing the symbiosis and mutual exclusion of AUs in various facial expressions, we organize the facial AUs in the form of structured knowledge-graph and integrate a Gated Graph Neural Network (GGNN) in a multi-scale CNN framework to propagate node information through the graph for generating enhanced AU representation. As the learned feature involves both the appearance characteristics and the AU relationship reasoning, the proposed model is more robust and can cope with more challenging cases, e.g., illumination change and partial occlusion. Extensive experiments on the two public benchmarks demonstrate that our method outperforms the previous work and achieves state of the art performance.",0
"The recognition of Facial Action Units (AUs) is a crucial task in the analysis of facial expressions. This task has gained extensive attention in the field of artificial intelligence and computer vision. Previous works have either focused on designing or learning complex regional feature representations, or on various types of AU relationship modeling. However, these methods struggle to handle complex situations. In this study, we investigate how to integrate semantic relationship propagation between AUs in a deep neural network framework to enhance the feature representation of facial regions. We propose an AU Semantic Relationship Embedded Representation Learning (SRERL) framework. By analyzing the symbiosis and mutual exclusion of AUs in various facial expressions, we organize the facial AUs in the form of a structured knowledge-graph. We integrate a Gated Graph Neural Network (GGNN) in a multi-scale CNN framework to propagate node information through the graph for generating enhanced AU representation. The proposed model is more robust and can cope with more challenging cases, such as illumination change and partial occlusion. Extensive experiments on two public benchmarks show that our method outperforms previous work and achieves state-of-the-art performance.",1
"Deep neural decision forest (NDF) achieved remarkable performance on various vision tasks via combining decision tree and deep representation learning. In this work, we first trace the decision-making process of this model and visualize saliency maps to understand which portion of the input influence it more for both classification and regression problems. We then apply NDF on a multi-task coordinate regression problem and demonstrate the distribution of routing probabilities, which is vital for interpreting NDF yet not shown for regression problems. The pre-trained model and code for visualization will be available at https://github.com/Nicholasli1995/VisualizingNDF",0
"By integrating decision tree and deep representation learning, the deep neural decision forest (NDF) has delivered remarkable results in several vision tasks. This study aims to explore the decision-making mechanism of NDF and generate saliency maps to identify the input components that impact its performance in classification and regression problems. Additionally, we utilize NDF in a multi-task coordinate regression problem to present the routing probabilities' distribution, which is crucial in interpreting NDF, especially in regression problems. The pre-trained model and visualization code will be accessible at https://github.com/Nicholasli1995/VisualizingNDF.",1
"ProductNet is a collection of high-quality product datasets for better product understanding. Motivated by ImageNet, ProductNet aims at supporting product representation learning by curating product datasets of high quality with properly chosen taxonomy. In this paper, the two goals of building high-quality product datasets and learning product representation support each other in an iterative fashion: the product embedding is obtained via a multi-modal deep neural network (master model) designed to leverage product image and catalog information; and in return, the embedding is utilized via active learning (local model) to vastly accelerate the annotation process. For the labeled data, the proposed master model yields high categorization accuracy (94.7% top-1 accuracy for 1240 classes), which can be used as search indices, partition keys, and input features for machine learning models. The product embedding, as well as the fined-tuned master model for a specific business task, can also be used for various transfer learning tasks.",0
"ProductNet is a compilation of superior product datasets that aim to enhance product comprehension, inspired by ImageNet. The main objective of ProductNet is to assist in product representation learning by selectively curating high-quality product datasets with an appropriate taxonomy. The paper highlights the two primary goals of creating top-notch product datasets and learning product representation, which work in tandem and iteratively. The multi-modal deep neural network (master model) is instrumental in acquiring product embedding by leveraging catalog information and product images. In turn, the embedding is used to speed up the annotation process via active learning (local model). The proposed master model delivers high categorization accuracy (94.7% top-1 accuracy for 1240 classes) for labeled data, which can be utilized as input features, partition keys, and search indices for machine learning models. The product embedding and fine-tuned master model can also be used for various transfer learning tasks, specifically for a particular business objective.",1
Learning disentangled representation from any unlabelled data is a non-trivial problem. In this paper we propose Information Maximising Autoencoder (InfoAE) where the encoder learns powerful disentangled representation through maximizing the mutual information between the representation and given information in an unsupervised fashion. We have evaluated our model on MNIST dataset and achieved 98.9 ($\pm .1$) $\%$ test accuracy while using complete unsupervised training.,0
"It is difficult to learn a disentangled representation from unlabelled data. This research introduces the Information Maximising Autoencoder (InfoAE), which trains the encoder to create a strong disentangled representation by maximizing the mutual information between the representation and the given information in an unsupervised manner. The model was tested using the MNIST dataset and achieved a test accuracy of 98.9 ($\pm .1$) $\%$ using entirely unsupervised training.",1
"We introduce a self-supervised representation learning method based on the task of temporal alignment between videos. The method trains a network using temporal cycle consistency (TCC), a differentiable cycle-consistency loss that can be used to find correspondences across time in multiple videos. The resulting per-frame embeddings can be used to align videos by simply matching frames using the nearest-neighbors in the learned embedding space.   To evaluate the power of the embeddings, we densely label the Pouring and Penn Action video datasets for action phases. We show that (i) the learned embeddings enable few-shot classification of these action phases, significantly reducing the supervised training requirements; and (ii) TCC is complementary to other methods of self-supervised learning in videos, such as Shuffle and Learn and Time-Contrastive Networks. The embeddings are also used for a number of applications based on alignment (dense temporal correspondence) between video pairs, including transfer of metadata of synchronized modalities between videos (sounds, temporal semantic labels), synchronized playback of multiple videos, and anomaly detection. Project webpage: https://sites.google.com/view/temporal-cycle-consistency .",0
"Our method involves self-supervised representation learning, which is centered around the task of aligning videos in time. The network is trained using a cycle-consistency loss, known as temporal cycle consistency (TCC), which is differentiable and allows for identifying correspondences across multiple videos. By generating per-frame embeddings, we can align videos by matching frames using nearest neighbors in the learned embedding space. We evaluated the effectiveness of the embeddings by labeling the Pouring and Penn Action video datasets for action phases. Our findings show that our method enables few-shot classification of action phases, significantly reducing the need for supervised training. Additionally, TCC complements other self-supervised learning methods, such as Shuffle and Learn and Time-Contrastive Networks. The embeddings also have various practical applications, such as transferring metadata and synchronized playback of multiple videos. Our project webpage can be found at https://sites.google.com/view/temporal-cycle-consistency.",1
"The Variational Autoencoder (VAE) is a powerful architecture capable of representation learning and generative modeling. When it comes to learning interpretable (disentangled) representations, VAE and its variants show unparalleled performance. However, the reasons for this are unclear, since a very particular alignment of the latent embedding is needed but the design of the VAE does not encourage it in any explicit way. We address this matter and offer the following explanation: the diagonal approximation in the encoder together with the inherent stochasticity force local orthogonality of the decoder. The local behavior of promoting both reconstruction and orthogonality matches closely how the PCA embedding is chosen. Alongside providing an intuitive understanding, we justify the statement with full theoretical analysis as well as with experiments.",0
"The Variational Autoencoder (VAE) is a potent structure that can perform representation learning and generative modeling. VAE and its variants display unparalleled performance when learning interpretable (disentangled) representations. However, it is unclear why this is the case since the VAE design does not explicitly encourage the particular alignment of the latent embedding required. We address this issue by providing an explanation. The diagonal approximation in the encoder, along with inherent stochasticity, enforces local orthogonality of the decoder. This local behavior of promoting both reconstruction and orthogonality closely matches how the PCA embedding is chosen. In addition to providing an intuitive understanding, we support this explanation with full theoretical analysis and experiments.",1
"We propose a dynamic neighborhood aggregation (DNA) procedure guided by (multi-head) attention for representation learning on graphs. In contrast to current graph neural networks which follow a simple neighborhood aggregation scheme, our DNA procedure allows for a selective and node-adaptive aggregation of neighboring embeddings of potentially differing locality. In order to avoid overfitting, we propose to control the channel-wise connections between input and output by making use of grouped linear projections. In a number of transductive node-classification experiments, we demonstrate the effectiveness of our approach.",0
"Our proposed method for representation learning on graphs is the dynamic neighborhood aggregation (DNA) procedure, which uses (multi-head) attention to guide the aggregation process. Unlike existing graph neural networks that use a basic neighborhood aggregation scheme, our DNA procedure enables selective and node-specific aggregation of neighboring embeddings with varying levels of locality. To prevent overfitting, we suggest using grouped linear projections to regulate the connections between the input and output channels. Through various transductive node-classification experiments, we show the efficacy of our approach.",1
"In recent years, there have been numerous developments towards solving multimodal tasks, aiming to learn a stronger representation than through a single modality. Certain aspects of the data can be particularly useful in this case - for example, correlations in the space or time domain across modalities - but should be wisely exploited in order to benefit from their full predictive potential. We propose two deep learning architectures with multimodal cross-connections that allow for dataflow between several feature extractors (XFlow). Our models derive more interpretable features and achieve better performances than models which do not exchange representations, usefully exploiting correlations between audio and visual data, which have a different dimensionality and are nontrivially exchangeable. Our work improves on existing multimodal deep learning algorithms in two essential ways: (1) it presents a novel method for performing cross-modality (before features are learned from individual modalities) and (2) extends the previously proposed cross-connections which only transfer information between streams that process compatible data. Illustrating some of the representations learned by the connections, we analyse their contribution to the increase in discrimination ability and reveal their compatibility with a lip-reading network intermediate representation. We provide the research community with Digits, a new dataset consisting of three data types extracted from videos of people saying the digits 0-9. Results show that both cross-modal architectures outperform their baselines (by up to 11.5%) when evaluated on the AVletters, CUAVE and Digits datasets, achieving state-of-the-art results.",0
"Recent developments have focused on solving multimodal tasks to learn a stronger representation than a single modality can provide. Correlations in the space or time domain across modalities can be particularly useful, but they should be wisely exploited to benefit from their full predictive potential. We propose two deep learning architectures with multimodal cross-connections, allowing dataflow between several feature extractors. Our models derive more interpretable features and achieve better performance by exploiting correlations between audio and visual data. Our work improves on existing multimodal deep learning algorithms by presenting a novel method for performing cross-modality and extending previously proposed cross-connections. We provide the research community with a new dataset called Digits, consisting of three data types extracted from videos of people saying the digits 0-9. Our results show that both cross-modal architectures outperform their baselines by up to 11.5%, achieving state-of-the-art results on the AVletters, CUAVE, and Digits datasets. We analyze the representations learned by the connections and their contribution to the increase in discrimination ability, revealing their compatibility with a lip-reading network intermediate representation.",1
"Spatio-temporal graphs such as traffic networks or gene regulatory systems present challenges for the existing deep learning methods due to the complexity of structural changes over time. To address these issues, we introduce Spatio-Temporal Deep Graph Infomax (STDGI)---a fully unsupervised node representation learning approach based on mutual information maximization that exploits both the temporal and spatial dynamics of the graph. Our model tackles the challenging task of node-level regression by training embeddings to maximize the mutual information between patches of the graph, at any given time step, and between features of the central nodes of patches, in the future. We demonstrate through experiments and qualitative studies that the learned representations can successfully encode relevant information about the input graph and improve the predictive performance of spatio-temporal auto-regressive forecasting models.",0
"The complexity of structural changes over time in spatio-temporal graphs such as traffic networks or gene regulatory systems presents a challenge for existing deep learning methods. In order to overcome this challenge, we have developed Spatio-Temporal Deep Graph Infomax (STDGI), which is an unsupervised node representation learning approach that maximizes mutual information. Our model takes advantage of both the spatial and temporal dynamics of the graph. By training embeddings to maximize the mutual information between patches of the graph at any given time step, and between features of the central nodes of patches in the future, our model tackles the challenging task of node-level regression. Through experiments and qualitative studies, we have demonstrated that the learned representations successfully encode relevant information about the input graph and improve the predictive performance of spatio-temporal auto-regressive forecasting models.",1
"For a product of interest, we propose a search method to surface a set of reference products. The reference products can be used as candidates to support downstream modeling tasks and business applications. The search method consists of product representation learning and fingerprint-type vector searching. The product catalog information is transformed into a high-quality embedding of low dimensions via a novel attention auto-encoder neural network, and the embedding is further coupled with a binary encoding vector for fast retrieval. We conduct extensive experiments to evaluate the proposed method, and compare it with peer services to demonstrate its advantage in terms of search return rate and precision.",0
"We suggest a search technique for identifying a collection of reference products related to a specific item. These reference products can serve as potential options to facilitate downstream modeling assignments and business uses. Our search method involves product representation learning and fingerprint-type vector searching. The product catalog data is transformed into a low-dimensional, high-quality embedding using a unique attention auto-encoder neural network. Moreover, the embedding is combined with a binary encoding vector for efficient retrieval. We conducted thorough experiments to assess the effectiveness of our method and compared it with similar services to showcase its superior search return rate and precision.",1
"Compressed sensing techniques enable efficient acquisition and recovery of sparse, high-dimensional data signals via low-dimensional projections. In this work, we propose Uncertainty Autoencoders, a learning framework for unsupervised representation learning inspired by compressed sensing. We treat the low-dimensional projections as noisy latent representations of an autoencoder and directly learn both the acquisition (i.e., encoding) and amortized recovery (i.e., decoding) procedures. Our learning objective optimizes for a tractable variational lower bound to the mutual information between the datapoints and the latent representations. We show how our framework provides a unified treatment to several lines of research in dimensionality reduction, compressed sensing, and generative modeling. Empirically, we demonstrate a 32% improvement on average over competing approaches for the task of statistical compressed sensing of high-dimensional datasets.",0
"The use of compressed sensing techniques allows for efficient acquisition and recovery of high-dimensional data signals that are sparse, using low-dimensional projections. We introduce a learning framework called Uncertainty Autoencoders, which is inspired by compressed sensing and achieves unsupervised representation learning. We regard the low-dimensional projections as noisy latent representations of an autoencoder and learn both the acquisition (encoding) and amortized recovery (decoding) procedures directly. Our learning objective optimizes a tractable variational lower bound to the mutual information between the data points and the latent representations. Our framework provides a unified approach to dimensionality reduction, compressed sensing, and generative modeling research. We demonstrate a 32% improvement on average over other methods for statistical compressed sensing of high-dimensional datasets through empirical evidence.",1
"We propose a new deep architecture for person re-identification (re-id). While re-id has seen much recent progress, spatial localization and view-invariant representation learning for robust cross-view matching remain key, unsolved problems. We address these questions by means of a new attention-driven Siamese learning architecture, called the Consistent Attentive Siamese Network. Our key innovations compared to existing, competing methods include (a) a flexible framework design that produces attention with only identity labels as supervision, (b) explicit mechanisms to enforce attention consistency among images of the same person, and (c) a new Siamese framework that integrates attention and attention consistency, producing principled supervisory signals as well as the first mechanism that can explain the reasoning behind the Siamese framework's predictions. We conduct extensive evaluations on the CUHK03-NP, DukeMTMC-ReID, and Market-1501 datasets and report competitive performance.",0
"Our proposal introduces a novel deep architecture for person re-identification (re-id) that aims to address the unresolved issues of spatial localization and view-invariant representation learning for accurate cross-view matching. We present the Consistent Attentive Siamese Network as an attention-based Siamese learning architecture that overcomes these challenges. Our approach offers several advancements compared to existing methods, including a flexible framework design that generates attention using only identity labels, explicit mechanisms for enforcing attention consistency among images of the same individual, and a new Siamese framework that integrates attention and attention consistency to provide principled supervisory signals. Additionally, our method offers the first mechanism capable of explaining the reasoning behind the Siamese framework's predictions. We evaluate our approach on the CUHK03-NP, DukeMTMC-ReID, and Market-1501 datasets and achieve competitive performance.",1
"Gait, the walking pattern of individuals, is one of the most important biometrics modalities. Most of the existing gait recognition methods take silhouettes or articulated body models as the gait features. These methods suffer from degraded recognition performance when handling confounding variables, such as clothing, carrying and view angle. To remedy this issue, we propose a novel AutoEncoder framework to explicitly disentangle pose and appearance features from RGB imagery and the LSTM-based integration of pose features over time produces the gait feature. In addition, we collect a Frontal-View Gait (FVG) dataset to focus on gait recognition from frontal-view walking, which is a challenging problem since it contains minimal gait cues compared to other views. FVG also includes other important variations, e.g., walking speed, carrying, and clothing. With extensive experiments on CASIA-B, USF and FVG datasets, our method demonstrates superior performance to the state of the arts quantitatively, the ability of feature disentanglement qualitatively, and promising computational efficiency.",0
"One of the most important biometric modalities is gait, which refers to an individual's walking pattern. Most gait recognition methods currently in use employ silhouettes or articulated body models as gait features. However, these methods are not always effective in recognizing gait due to various confounding variables, such as clothing, carrying, and view angle. To address these issues, we propose a new AutoEncoder framework that can disentangle pose and appearance features from RGB imagery. We also integrate pose features over time using an LSTM-based approach to produce the gait feature. To test our approach, we have collected a Frontal-View Gait (FVG) dataset that focuses on gait recognition from frontal-view walking, which is particularly challenging since it contains minimal gait cues compared to other views. The FVG dataset also includes other important variations, such as walking speed, carrying, and clothing. Through extensive experiments on CASIA-B, USF, and FVG datasets, our method has demonstrated superior performance compared to existing methods, both quantitatively and qualitatively. Our feature disentanglement approach has also shown promising computational efficiency.",1
"Binaural audio provides a listener with 3D sound sensation, allowing a rich perceptual experience of the scene. However, binaural recordings are scarcely available and require nontrivial expertise and equipment to obtain. We propose to convert common monaural audio into binaural audio by leveraging video. The key idea is that visual frames reveal significant spatial cues that, while explicitly lacking in the accompanying single-channel audio, are strongly linked to it. Our multi-modal approach recovers this link from unlabeled video. We devise a deep convolutional neural network that learns to decode the monaural (single-channel) soundtrack into its binaural counterpart by injecting visual information about object and scene configurations. We call the resulting output 2.5D visual sound---the visual stream helps ""lift"" the flat single channel audio into spatialized sound. In addition to sound generation, we show the self-supervised representation learned by our network benefits audio-visual source separation. Our video results: http://vision.cs.utexas.edu/projects/2.5D_visual_sound/",0
"The experience of 3D sound provided by binaural audio is highly immersive, but the scarcity of available recordings and the need for specialized equipment and expertise make it difficult to obtain. We propose a solution wherein common monaural audio is converted into binaural audio using video. By analyzing visual frames, which contain valuable spatial cues that are absent in the accompanying single-channel audio, we are able to establish a strong link between the two. Our approach employs a deep convolutional neural network that decodes the monaural soundtrack into its binaural equivalent by incorporating visual information about the scene and objects. This process results in 2.5D visual sound, which elevates the flat single channel audio into spatialized sound. Additionally, our network's self-supervised representation improves audio-visual source separation. Our video demonstration can be viewed at http://vision.cs.utexas.edu/projects/2.5D_visual_sound/.",1
"Conditional GANs are at the forefront of natural image synthesis. The main drawback of such models is the necessity for labeled data. In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, and take a step towards bridging the gap between conditional and unconditional GANs. In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training. We test empirically both the quality of the learned image representations, and the quality of the synthesized images. Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts. Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 23.4 on unconditional ImageNet generation.",0
"Leading the way in natural image synthesis are Conditional GANs, though their main drawback is the requirement for labeled data. Our work aims to address this issue by incorporating two popular unsupervised learning techniques, adversarial training and self-supervision, to bridge the gap between conditional and unconditional GANs. This involves enabling the networks to collaborate on representation learning while playing an adversarial GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations without forgetting them during training. We evaluate the quality of both image representations and synthesized images, finding that our self-supervised GAN performs similarly to state-of-the-art conditional GANs. We also demonstrate how our fully unsupervised approach can be scaled to achieve an FID of 23.4 in generating unconditional ImageNet images.",1
"High-resolution representation learning plays an essential role in many vision problems, e.g., pose estimation and semantic segmentation. The high-resolution network (HRNet)~\cite{SunXLW19}, recently developed for human pose estimation, maintains high-resolution representations through the whole process by connecting high-to-low resolution convolutions in \emph{parallel} and produces strong high-resolution representations by repeatedly conducting fusions across parallel convolutions.   In this paper, we conduct a further study on high-resolution representations by introducing a simple yet effective modification and apply it to a wide range of vision tasks. We augment the high-resolution representation by aggregating the (upsampled) representations from all the parallel convolutions rather than only the representation from the high-resolution convolution as done in~\cite{SunXLW19}. This simple modification leads to stronger representations, evidenced by superior results. We show top results in semantic segmentation on Cityscapes, LIP, and PASCAL Context, and facial landmark detection on AFLW, COFW, $300$W, and WFLW. In addition, we build a multi-level representation from the high-resolution representation and apply it to the Faster R-CNN object detection framework and the extended frameworks. The proposed approach achieves superior results to existing single-model networks on COCO object detection. The code and models have been publicly available at \url{https://github.com/HRNet}.",0
"Many vision problems, such as pose estimation and semantic segmentation, require high-resolution representation learning. The development of the high-resolution network (HRNet)~\cite{SunXLW19} has made it possible to maintain high-resolution representations throughout the process by connecting high-to-low resolution convolutions in parallel. This approach creates strong high-resolution representations by conducting multiple fusions across parallel convolutions. In this study, we introduce a simple and effective modification to the high-resolution representation by aggregating the representations from all parallel convolutions, rather than just the high-resolution convolution as done in~\cite{SunXLW19}. This modification leads to stronger representations and superior results in semantic segmentation on Cityscapes, LIP, and PASCAL Context, as well as facial landmark detection on AFLW, COFW, $300$W, and WFLW. Additionally, we build a multi-level representation from the high-resolution representation and apply it to the Faster R-CNN object detection framework and its extensions, achieving superior results to existing single-model networks on COCO object detection. The code and models are publicly available at \url{https://github.com/HRNet}.",1
"We integrate two powerful ideas, geometry and deep visual representation learning, into recurrent network architectures for mobile visual scene understanding. The proposed networks learn to ""lift"" and integrate 2D visual features over time into latent 3D feature maps of the scene. They are equipped with differentiable geometric operations, such as projection, unprojection, egomotion estimation and stabilization, in order to compute a geometrically-consistent mapping between the world scene and their 3D latent feature state. We train the proposed architectures to predict novel camera views given short frame sequences as input. Their predictions strongly generalize to scenes with a novel number of objects, appearances and configurations; they greatly outperform previous works that do not consider egomotion stabilization or a space-aware latent feature state. We train the proposed architectures to detect and segment objects in 3D using the latent 3D feature map as input--as opposed to per frame features. The resulting object detections persist over time: they continue to exist even when an object gets occluded or leaves the field of view. Our experiments suggest the proposed space-aware latent feature memory and egomotion-stabilized convolutions are essential architectural choices for spatial common sense to emerge in artificial embodied visual agents.",0
"Our approach combines two powerful concepts, namely geometry and deep visual representation learning, to create recurrent network architectures that can understand visual scenes on mobile devices. Our networks learn to elevate and integrate 2D visual features over time to create 3D latent feature maps of the scene. They also employ differentiable geometric operations, such as projection, unprojection, egomotion estimation, and stabilization, to ensure a consistent mapping between the world scene and the 3D latent feature state. These architectures are trained to predict new camera views from short frame sequences, and their predictions generalize well to scenes with varying objects, appearances, and configurations. Compared to previous works that don't incorporate egomotion stabilization or a space-aware latent feature state, our approach performs significantly better. Additionally, we train our architectures to detect and segment objects in 3D using the latent 3D feature map instead of per frame features. This results in persistent object detections even when an object is occluded or moves out of the field of view. Our experiments show that our approach's space-aware latent feature memory and egomotion-stabilized convolutions are crucial architectural choices that enable artificial embodied visual agents to develop spatial common sense.",1
"Spherical data is found in many applications. By modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, graph convolutions are computationally more efficient than spherical convolutions. As equivariance is desired to exploit rotational symmetries, we discuss how to approach rotation equivariance using the graph neural network introduced in Defferrard et al. (2016). Experiments show good performance on rotation-invariant learning problems. Code and examples are available at https://github.com/SwissDataScienceCenter/DeepSphere",0
"Spherical data is utilized in various applications and can be represented as a graph to handle non-uniform and changing samplings efficiently. Additionally, graph convolutions are more computationally effective than spherical convolutions. To achieve rotation equivariance and take advantage of rotational symmetries, we describe the use of the graph neural network presented in Defferrard et al. (2016). Our experiments demonstrate effective performance for learning problems that are rotation-invariant. Code and examples are accessible at https://github.com/SwissDataScienceCenter/DeepSphere.",1
"We address the problem of video representation learning without human-annotated labels. While previous efforts address the problem by designing novel self-supervised tasks using video data, the learned features are merely on a frame-by-frame basis, which are not applicable to many video analytic tasks where spatio-temporal features are prevailing. In this paper we propose a novel self-supervised approach to learn spatio-temporal features for video representation. Inspired by the success of two-stream approaches in video classification, we propose to learn visual features by regressing both motion and appearance statistics along spatial and temporal dimensions, given only the input video data. Specifically, we extract statistical concepts (fast-motion region and the corresponding dominant direction, spatio-temporal color diversity, dominant color, etc.) from simple patterns in both spatial and temporal domains. Unlike prior puzzles that are even hard for humans to solve, the proposed approach is consistent with human inherent visual habits and therefore easy to answer. We conduct extensive experiments with C3D to validate the effectiveness of our proposed approach. The experiments show that our approach can significantly improve the performance of C3D when applied to video classification tasks. Code is available at https://github.com/laura-wang/video_repres_mas.",0
"Our focus is on video representation learning without human-annotated labels. Although previous attempts have utilized self-supervised tasks to learn features from video data, these features are only frame-by-frame and not suitable for many video analytic tasks where spatio-temporal features are crucial. In this paper, we propose a new self-supervised approach to learn spatio-temporal features for video representation. Inspired by the success of two-stream approaches in video classification, we aim to learn visual features by regressing both motion and appearance statistics along spatial and temporal dimensions, without any human-annotated labels. We extract statistical concepts such as fast-motion regions, dominant directions, spatio-temporal color diversity, and dominant colors from simple patterns in both spatial and temporal domains. Our approach is designed to be consistent with human visual habits and therefore easy to answer. We conduct extensive experiments with C3D to validate the effectiveness of our approach, which significantly improves the performance of C3D in video classification tasks. The code is available at https://github.com/laura-wang/video_repres_mas.",1
"Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization.",0
"In machine learning, input data is often represented using vector embeddings or discrete codes. When the input data exhibits compositional structure, such as objects made up of parts or procedures created from subroutines, it is important to determine whether this structure is reflected in the learned representations. While linguistics and related fields have extensively studied the assessment of compositionality in languages, there is a lack of general-purpose tools in machine learning for measuring compositional structure in vector-valued representation spaces. To address this gap, we present a procedure for evaluating compositionality by measuring how well a true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. Our procedure allows for formal and empirical characterizations of compositional structure in various settings, including the exploration of the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization.",1
"Temporal segmentation of long videos is an important problem, that has largely been tackled through supervised learning, often requiring large amounts of annotated training data. In this paper, we tackle the problem of self-supervised temporal segmentation of long videos that alleviate the need for any supervision. We introduce a self-supervised, predictive learning framework that draws inspiration from cognitive psychology to segment long, visually complex videos into individual, stable segments that share the same semantics. We also introduce a new adaptive learning paradigm that helps reduce the effect of catastrophic forgetting in recurrent neural networks. Extensive experiments on three publicly available datasets - Breakfast Actions, 50 Salads, and INRIA Instructional Videos datasets show the efficacy of the proposed approach. We show that the proposed approach is able to outperform weakly-supervised and other unsupervised learning approaches by up to 24% and have competitive performance compared to fully supervised approaches. We also show that the proposed approach is able to learn highly discriminative features that help improve action recognition when used in a representation learning paradigm.",0
"The issue of temporal segmentation for lengthy videos has been typically addressed through supervised learning, which necessitates significant amounts of annotated training data. However, this paper aims to solve the problem of self-supervised temporal segmentation for extended videos without the need for supervision. By adopting a self-supervised, predictive learning model inspired by cognitive psychology, we can segment intricate videos into stable, individual segments with identical semantics. Additionally, we introduce a novel adaptive learning paradigm to mitigate catastrophic forgetting in recurrent neural networks. Through extensive experiments on three publicly accessible datasets - Breakfast Actions, 50 Salads, and INRIA Instructional Videos - the proposed approach has proved to be effective, with a 24% improvement over weakly-supervised and other unsupervised learning techniques, as well as competitive performance compared to entirely supervised methods. Furthermore, we have demonstrated that the proposed approach can learn exceptionally discriminative features that enhance action recognition when implemented in a representation learning paradigm.",1
"Automatic eye gaze estimation has interested researchers for a while now. In this paper, we propose an unsupervised learning based method for estimating the eye gaze region. To train the proposed network ""Ize-Net"" in self-supervised manner, we collect a large `in the wild' dataset containing 1,54,251 images from the web. For the images in the database, we divide the gaze into three regions based on an automatic technique based on pupil-centers localization and then use a feature-based technique to determine the gaze region. The performance is evaluated on the Tablet Gaze and CAVE datasets by fine-tuning results of Ize-Net for the task of eye gaze estimation. The feature representation learned is also used to train traditional machine learning algorithms for eye gaze estimation. The results demonstrate that the proposed method learns a rich data representation, which can be efficiently fine-tuned for any eye gaze estimation dataset.",0
"Researchers have been exploring automatic eye gaze estimation for some time now. This study presents a self-supervised learning approach to estimate the eye gaze region using an unsupervised method. To accomplish this, a vast dataset with 154,251 images from the internet was collected and used to train the ""Ize-Net"" network. By employing pupil-centers localization and a feature-based technique, the gaze was divided into three regions. The performance of Ize-Net was evaluated on the Tablet Gaze and CAVE datasets, and the results were used to fine-tune the network for eye gaze estimation. Additionally, the feature representation learned was used to train traditional machine learning algorithms. The findings indicate that the proposed method can learn a robust data representation that can be efficiently fine-tuned for any eye gaze estimation dataset.",1
"Recently, data-driven deep saliency models have achieved high performance and have outperformed classical saliency models, as demonstrated by results on datasets such as the MIT300 and SALICON. Yet, there remains a large gap between the performance of these models and the inter-human baseline. Some outstanding questions include what have these models learned, how and where they fail, and how they can be improved. This article attempts to answer these questions by analyzing the representations learned by individual neurons located at the intermediate layers of deep saliency models. To this end, we follow the steps of existing deep saliency models, that is borrowing a pre-trained model of object recognition to encode the visual features and learning a decoder to infer the saliency. We consider two cases when the encoder is used as a fixed feature extractor and when it is fine-tuned, and compare the inner representations of the network. To study how the learned representations depend on the task, we fine-tune the same network using the same image set but for two different tasks: saliency prediction versus scene classification. Our analyses reveal that: 1) some visual regions (e.g. head, text, symbol, vehicle) are already encoded within various layers of the network pre-trained for object recognition, 2) using modern datasets, we find that fine-tuning pre-trained models for saliency prediction makes them favor some categories (e.g. head) over some others (e.g. text), 3) although deep models of saliency outperform classical models on natural images, the converse is true for synthetic stimuli (e.g. pop-out search arrays), an evidence of significant difference between human and data-driven saliency models, and 4) we confirm that, after-fine tuning, the change in inner-representations is mostly due to the task and not the domain shift in the data.",0
"In recent times, deep saliency models that rely on data have shown impressive performance and surpassed classical saliency models, as evidenced by their results on datasets like MIT300 and SALICON. However, there is still a significant discrepancy between the performance of these models and the inter-human baseline. There are several outstanding queries, including what these models have learned, how and where they fail, and how they can be enhanced. This article aims to address these questions by examining the representations that individual neurons located at intermediate layers of deep saliency models have learned. To achieve this, we follow the same steps as existing deep saliency models by using a pre-trained object recognition model to encode the visual features and learn a decoder to infer the saliency. We consider two scenarios, one where the encoder is used as a fixed feature extractor and another where it is fine-tuned, and compare the internal representations of the network. To investigate how the learned representations are dependent on the task, we fine-tune the same network with the same image set but for two different tasks: scene classification and saliency prediction. Our analysis indicates that certain visual regions such as head, text, symbol, and vehicle are already encoded within various layers of the network that is pre-trained for object recognition. Additionally, fine-tuning pre-trained models for saliency prediction makes them favor some categories (e.g. head) over others (e.g. text) when using modern datasets. Although deep saliency models outperform classical models on natural images, the opposite is true for synthetic stimuli (e.g. pop-out search arrays), which demonstrates a significant difference between human and data-driven saliency models. Finally, our results confirm that after fine-tuning, the change in inner-representations is primarily due to the task and not the domain shift in the data.",1
"The goal of knowledge representation learning is to embed entities and relations into a low-dimensional, continuous vector space. How to push a model to its limit and obtain better results is of great significance in knowledge graph's applications. We propose a simple and elegant method, Trans-DLR, whose main idea is dynamic learning rate control during training. Our method achieves remarkable improvement, compared with recent GAN-based method. Moreover, we introduce a new negative sampling trick which corrupts not only entities, but also relations, in different probabilities. We also develop an efficient way, which fully utilizes multiprocessing and parallel computing, to speed up evaluation of the model in link prediction tasks. Experiments show that our method is effective.",0
"The objective of knowledge representation learning is to embed entities and relations into a low-dimensional, continuous vector space. It is crucial to determine how to maximize a model's potential and produce superior outcomes in knowledge graph applications. Our proposed Trans-DLR method employs a straightforward and refined approach that involves dynamic learning rate control during training. Our method surpasses recent GAN-based techniques in terms of performance enhancement. Additionally, we have devised a novel negative sampling technique that corrupts both entities and relations at varying probabilities. We have also created an efficient method that utilizes multiprocessing and parallel computing to accelerate the model's assessment in link prediction tasks. Our experiments demonstrate the efficacy of our approach.",1
"We propose Deep Multiset Canonical Correlation Analysis (dMCCA) as an extension to representation learning using CCA when the underlying signal is observed across multiple (more than two) modalities. We use deep learning framework to learn non-linear transformations from different modalities to a shared subspace such that the representations maximize the ratio of between- and within-modality covariance of the observations. Unlike linear discriminant analysis, we do not need class information to learn these representations, and we show that this model can be trained for complex data using mini-batches. Using synthetic data experiments, we show that dMCCA can effectively recover the common signal across the different modalities corrupted by multiplicative and additive noise. We also analyze the sensitivity of our model to recover the correlated components with respect to mini-batch size and dimension of the embeddings. Performance evaluation on noisy handwritten datasets shows that our model outperforms other CCA-based approaches and is comparable to deep neural network models trained end-to-end on this dataset.",0
"As a means of advancing representation learning using CCA for signals observed across multiple modalities, we propose the extension of Deep Multiset Canonical Correlation Analysis (dMCCA). Employing a deep learning framework, our method learns nonlinear transformations from diverse modalities to a shared subspace, optimizing representations to maximize the ratio of between- and within-modality covariance of the observations. Unlike linear discriminant analysis, our approach does not require class information for representation learning and can be trained for complex data using mini-batches. Synthetic data experiments demonstrate dMCCA's ability to effectively recover the common signal across different modalities while accounting for multiplicative and additive noise. We also analyze the model's sensitivity to recover correlated components with respect to mini-batch size and embedding dimension. Furthermore, our assessment on noisy handwritten datasets reveals that dMCCA outperforms other CCA-based methods and achieves comparable results to deep neural network models trained end-to-end on the same dataset.",1
"The problem of distributed representation learning is one in which multiple sources of information $X_1,\ldots,X_K$ are processed separately so as to learn as much information as possible about some ground truth $Y$. We investigate this problem from information-theoretic grounds, through a generalization of Tishby's centralized Information Bottleneck (IB) method to the distributed setting. Specifically, $K$ encoders, $K \geq 2$, compress their observations $X_1,\ldots,X_K$ separately in a manner such that, collectively, the produced representations preserve as much information as possible about $Y$. We study both discrete memoryless (DM) and memoryless vector Gaussian data models. For the discrete model, we establish a single-letter characterization of the optimal tradeoff between complexity (or rate) and relevance (or information) for a class of memoryless sources (the observations $X_1,\ldots,X_K$ being conditionally independent given $Y$). For the vector Gaussian model, we provide an explicit characterization of the optimal complexity-relevance tradeoff. Furthermore, we develop a variational bound on the complexity-relevance tradeoff which generalizes the evidence lower bound (ELBO) to the distributed setting. We also provide two algorithms that allow to compute this bound: i) a Blahut-Arimoto type iterative algorithm which enables to compute optimal complexity-relevance encoding mappings by iterating over a set of self-consistent equations, and ii) a variational inference type algorithm in which the encoding mappings are parametrized by neural networks and the bound approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on synthetic and real datasets are provided to support the efficiency of the approaches and algorithms developed in this paper.",0
"Learning distributed representations involves processing multiple sources of information, denoted as $X_1,\ldots,X_K$, separately to gain insight into some underlying truth $Y$. In this study, we examine this problem from an information-theoretic perspective by extending Tishby's Information Bottleneck (IB) approach to the distributed setting. Specifically, we use $K$ encoders, where $K \geq 2$, to compress their respective observations in a way that collectively preserves as much information as possible about $Y$. We consider both discrete memoryless (DM) and memoryless vector Gaussian data models and provide a single-letter characterization of the optimal tradeoff between complexity and relevance for a class of memoryless sources. For the vector Gaussian model, we present an explicit characterization of the optimal complexity-relevance tradeoff and a variational bound that extends the evidence lower bound (ELBO) to the distributed setting. We also propose two algorithms to compute this bound: a Blahut-Arimoto iterative algorithm and a variational inference type algorithm. We evaluate the efficiency of our approaches and algorithms using synthetic and real datasets.",1
"This paper generalizes the Maurer--Pontil framework of finite-dimensional lossy coding schemes to the setting where a high-dimensional random vector is mapped to an element of a compact set of latent representations in a lower-dimensional Euclidean space, and the reconstruction map belongs to a given class of nonlinear maps. Under this setup, which encompasses a broad class of unsupervised representation learning problems, we establish a connection to approximate generative modeling under structural constraints using the tools from the theory of optimal transportation. Next, we consider problem of learning a coding scheme on the basis of a finite collection of training samples and present generalization bounds that hold with high probability. We then illustrate the general theory in the setting where the reconstruction maps are implemented by deep neural nets.",0
"The aim of this paper is to extend the Maurer-Pontil framework for lossy coding schemes in a finite-dimensional setting to the scenario in which a high-dimensional random vector is transformed into a latent representation within a compact set in a lower-dimensional Euclidean space. Additionally, the reconstruction map must belong to a specific class of nonlinear maps. This generalization covers a wide range of unsupervised representation learning problems and is linked to approximate generative modeling with structural limitations, using optimal transportation theory. The paper also addresses the problem of developing a coding scheme based on a limited set of training samples and presents generalization bounds that are highly probable. Finally, the theory is applied to the use of deep neural nets for implementing reconstruction maps.",1
"Mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. However, such approaches are fundamentally limited since a tight lower bound of mutual information requires sample size exponential in the mutual information. This limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. In these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. This leads to incomplete representations that are not optimal for downstream tasks. In this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. To mitigate these problems we introduce the Wasserstein dependency measure, which learns more complete representations by using the Wasserstein distance instead of the KL divergence in the mutual information estimator. We show that a practical approximation to this theoretically motivated solution, constructed using Lipschitz constraint techniques from the GAN literature, achieves substantially improved results on tasks where incomplete representations are a major challenge.",0
"Unsupervised representation learning using mutual information maximization has shown impressive performance in various applications such as object recognition, speech recognition, and reinforcement learning. However, these approaches face inherent limitations due to the requirement of a large sample size for a tight lower bound of mutual information. Hence, they are not suitable for tasks with high mutual information, such as video understanding or reinforcement learning, as they tend to overfit and capture only a few relevant factors of variation, leading to incomplete representations. In this study, we demonstrate the inadequacy of mutual information-based representation learning in several designed and real-world tasks. To address these issues, we propose the Wasserstein dependency measure, which utilizes the Wasserstein distance instead of the KL divergence in the mutual information estimator to learn more complete representations. We show that our proposed approach, which approximates this theoretically motivated solution using Lipschitz constraint techniques from the GAN literature, achieves substantially improved results in tasks where incomplete representations are a significant problem.",1
"Deep learning methods for person identification based on electroencephalographic (EEG) brain activity encounters the problem of exploiting the temporally correlated structures or recording session specific variability within EEG. Furthermore, recent methods have mostly trained and evaluated based on single session EEG data. We address this problem from an invariant representation learning perspective. We propose an adversarial inference approach to extend such deep learning models to learn session-invariant person-discriminative representations that can provide robustness in terms of longitudinal usability. Using adversarial learning within a deep convolutional network, we empirically assess and show improvements with our approach based on longitudinally collected EEG data for person identification from half-second EEG epochs.",0
"The utilization of electroencephalographic (EEG) brain activity for person identification using deep learning techniques poses a challenge due to the need to exploit temporally correlated structures and variability that is specific to recording sessions. Current methods primarily rely on single session EEG data for training and evaluation. To address this issue, we adopt an invariant representation learning approach. Our proposed solution involves an adversarial inference method to extend deep learning models and create session-invariant person-discriminative representations that offer robustness for longitudinal usability. Through the use of adversarial learning within a deep convolutional network, we demonstrate the effectiveness of our approach using longitudinally collected EEG data for person identification from half-second EEG epochs.",1
"Recent advances in deep generative models have lead to remarkable progress in synthesizing high quality images. Following their successful application in image processing and representation learning, an important next step is to consider videos. Learning generative models of video is a much harder task, requiring a model to capture the temporal dynamics of a scene, in addition to the visual presentation of objects. While recent attempts at formulating generative models of video have had some success, current progress is hampered by (1) the lack of qualitative metrics that consider visual quality, temporal coherence, and diversity of samples, and (2) the wide gap between purely synthetic video data sets and challenging real-world data sets in terms of complexity. To this extent we propose Fr\'{e}chet Video Distance (FVD), a new metric for generative models of video, and StarCraft 2 Videos (SCV), a benchmark of game play from custom starcraft 2 scenarios that challenge the current capabilities of generative models of video. We contribute a large-scale human study, which confirms that FVD correlates well with qualitative human judgment of generated videos, and provide initial benchmark results on SCV.",0
"Significant progress has been made in producing high-quality images through the use of deep generative models. However, the next challenge is to extend these models to video generation. This presents a more difficult task as a model must not only capture the visual presentation of objects but also the temporal dynamics of the scene. Although some attempts have been made to develop generative models of video, there are two main challenges. Firstly, a lack of qualitative metrics that consider the visual quality, temporal coherence, and diversity of samples. Secondly, the significant difference in complexity between purely synthetic video data sets and real-world data sets. To overcome these challenges, the authors propose a new metric for generative models of video called Frchet Video Distance (FVD) and a benchmark of gameplay from custom StarCraft 2 scenarios called StarCraft 2 Videos (SCV). The authors conducted a large-scale human study to confirm that FVD aligns with qualitative human judgment of generated videos and provide initial benchmark results on SCV.",1
"Predicting human perceptual similarity is a challenging subject of ongoing research. The visual process underlying this aspect of human vision is thought to employ multiple different levels of visual analysis (shapes, objects, texture, layout, color, etc). In this paper, we postulate that the perception of image similarity is not an explicitly learned capability, but rather one that is a byproduct of learning others. This claim is supported by leveraging representations learned from a diverse set of visual tasks and using them jointly to predict perceptual similarity. This is done via simple feature concatenation, without any further learning. Nevertheless, experiments performed on the challenging Totally-Looks-Like (TLL) benchmark significantly surpass recent baselines, closing much of the reported gap towards prediction of human perceptual similarity. We provide an analysis of these results and discuss them in a broader context of emergent visual capabilities and their implications on the course of machine-vision research.",0
"The field of predicting human perceptual similarity is continuously being researched and is considered challenging. It is believed that this aspect of human vision involves various levels of visual analysis, such as shapes, objects, texture, layout, and color. The authors of this paper propose that the ability to perceive image similarity is not a skill that is explicitly taught but is acquired through learning from others. They support this claim by utilizing representations acquired from different visual tasks and combining them to predict perceptual similarity. This is achieved by simply concatenating features without any additional learning. Despite this, the experiments conducted on the TLL benchmark show significant improvement compared to recent baselines, narrowing the gap in predicting human perceptual similarity. The implications of these results are analyzed and discussed in the context of machine-vision research.",1
"Current deep domain adaptation methods used in computer vision have mainly focused on learning discriminative and domain-invariant features across different domains. In this paper, we present a novel approach that bridges the domain gap by projecting the source and target domains into a common association space through an unsupervised ``cross-grafted representation stacking'' (CGRS) mechanism. Specifically, we construct variational auto-encoders (VAE) for the two domains, and form bidirectional associations by cross-grafting the VAEs' decoder stacks. Furthermore, generative adversarial networks (GAN) are employed for label alignment (LA), mapping the target domain data to the known label space of the source domain. The overall adaptation process hence consists of three phases: feature representation learning by VAEs, association generation, and association label alignment by GANs. Experimental results demonstrate that our CGRS-LA approach outperforms the state-of-the-art on a number of unsupervised domain adaptation benchmarks.",0
"The current methods for deep domain adaptation in computer vision have primarily concentrated on acquiring discriminative and domain-invariant features across various domains. This paper introduces a new technique that overcomes the domain gap by utilizing an unsupervised ""cross-grafted representation stacking"" (CGRS) mechanism to project the source and target domains into a common association space. To achieve this, we construct variational auto-encoders (VAE) for both domains and establish bi-directional associations by cross-grafting the VAEs' decoder stacks. Additionally, label alignment (LA) is accomplished by generative adversarial networks (GAN), which map the target domain data to the known label space of the source domain. The overall adaptation process comprises three stages: VAEs for feature representation learning, association generation, and GANs for association label alignment. Our CGRS-LA method surpasses the existing state-of-the-art on several unsupervised domain adaptation benchmarks, as demonstrated by experimental results.",1
"The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years. In spite of the advance, clustering with representation learning has been limited to flat-level categories, which often involves cohesive clustering with a focus on instance relations. To overcome the limitations of flat clustering, we introduce hierarchically-clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space. Compared with a few prior works, HCRL firstly attempts to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components. In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features. We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines.",0
"Recently, the joint optimization of representation learning and clustering in the embedding space has made significant progress. However, current clustering methods with representation learning have been limited to flat-level categories, which focus on cohesive clustering based on the relationships between instances. To overcome this limitation, we propose hierarchically-clustered representation learning (HCRL), which optimizes representation learning and hierarchical clustering in the embedding space simultaneously. HCRL goes beyond previous works by generating deep embeddings from every component of the hierarchy, not just leaf components. Apart from obtaining hierarchically clustered embeddings, HCRL can reconstruct data at various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features. We evaluated HCRL using image and text domains and found that it outperformed the baselines with better likelihoods and accuracies.",1
"The success of deep neural networks often relies on a large amount of labeled examples, which can be difficult to obtain in many real scenarios. To address this challenge, unsupervised methods are strongly preferred for training neural networks without using any labeled data. In this paper, we present a novel paradigm of unsupervised representation learning by Auto-Encoding Transformation (AET) in contrast to the conventional Auto-Encoding Data (AED) approach. Given a randomly sampled transformation, AET seeks to predict it merely from the encoded features as accurately as possible at the output end. The idea is the following: as long as the unsupervised features successfully encode the essential information about the visual structures of original and transformed images, the transformation can be well predicted. We will show that this AET paradigm allows us to instantiate a large variety of transformations, from parameterized, to non-parameterized and GAN-induced ones. Our experiments show that AET greatly improves over existing unsupervised approaches, setting new state-of-the-art performances being greatly closer to the upper bounds by their fully supervised counterparts on CIFAR-10, ImageNet and Places datasets.",0
"In many real-life situations, obtaining a large amount of labeled examples necessary for the success of deep neural networks can be challenging. Hence, unsupervised methods are preferred for training neural networks without using labeled data. This paper proposes a new paradigm of unsupervised representation learning called Auto-Encoding Transformation (AET), which differs from the conventional Auto-Encoding Data (AED) approach. AET predicts a randomly sampled transformation from the encoded features as accurately as possible at the output end. The key idea is that if the unsupervised features encode the essential information about the visual structures of original and transformed images, the transformation can be predicted well. The paper shows that AET can be used for a variety of transformations, from parameterized to non-parameterized and GAN-induced ones. Experiments show that AET significantly improves over existing unsupervised approaches and sets new state-of-the-art performances on CIFAR-10, ImageNet, and Places datasets, approaching the upper bounds of their fully supervised counterparts.",1
"We exploit altered patterns in brain functional connectivity as features for automatic discriminative analysis of neuropsychiatric patients. Deep learning methods have been introduced to functional network classification only very recently for fMRI, and the proposed architectures essentially focused on a single type of connectivity measure. We propose a deep convolutional neural network (CNN) framework for classification of electroencephalogram (EEG)-derived brain connectome in schizophrenia (SZ). To capture complementary aspects of disrupted connectivity in SZ, we explore combination of various connectivity features consisting of time and frequency-domain metrics of effective connectivity based on vector autoregressive model and partial directed coherence, and complex network measures of network topology. We design a novel multi-domain connectome CNN (MDC-CNN) based on a parallel ensemble of 1D and 2D CNNs to integrate the features from various domains and dimensions using different fusion strategies. Hierarchical latent representations learned by the multiple convolutional layers from EEG connectivity reveal apparent group differences between SZ and healthy controls (HC). Results on a large resting-state EEG dataset show that the proposed CNNs significantly outperform traditional support vector machine classifiers. The MDC-CNN with combined connectivity features further improves performance over single-domain CNNs using individual features, achieving remarkable accuracy of $93.06\%$ with a decision-level fusion. The proposed MDC-CNN by integrating information from diverse brain connectivity descriptors is able to accurately discriminate SZ from HC. The new framework is potentially useful for developing diagnostic tools for SZ and other disorders.",0
"Our approach for automatic discriminative analysis of neuropsychiatric patients involves utilizing altered patterns in brain functional connectivity as features. While deep learning methods have recently been introduced to functional network classification for fMRI, these architectures have mainly focused on a single type of connectivity measure. For classification of electroencephalogram (EEG)-derived brain connectome in schizophrenia (SZ), we propose a deep convolutional neural network (CNN) framework. To capture different aspects of disrupted connectivity in SZ, we explore a combination of various connectivity features, including time and frequency-domain metrics of effective connectivity, complex network measures of network topology based on vector autoregressive model, and partial directed coherence. Our novel multi-domain connectome CNN (MDC-CNN) uses a parallel ensemble of 1D and 2D CNNs to integrate the features from different domains and dimensions using different fusion strategies. The results on a large resting-state EEG dataset show that the proposed CNNs outperform traditional support vector machine classifiers. By integrating information from diverse brain connectivity descriptors, our MDC-CNN accurately discriminates SZ from healthy controls (HC), achieving remarkable accuracy of $93.06\%$ with decision-level fusion. Our proposed framework has the potential to develop diagnostic tools for SZ and other disorders.",1
"Knowledge Transfer (KT) techniques tackle the problem of transferring the knowledge from a large and complex neural network into a smaller and faster one. However, existing KT methods are tailored towards classification tasks and they cannot be used efficiently for other representation learning tasks. In this paper a novel knowledge transfer technique, that is capable of training a student model that maintains the same amount of mutual information between the learned representation and a set of (possible unknown) labels as the teacher model, is proposed. Apart from outperforming existing KT techniques, the proposed method allows for overcoming several limitations of existing methods providing new insight into KT as well as novel KT applications, ranging from knowledge transfer from handcrafted feature extractors to {cross-modal} KT from the textual modality into the representation extracted from the visual modality of the data.",0
"The issue of transferring knowledge from a complex neural network to a smaller and quicker one is addressed by Knowledge Transfer (KT) techniques. However, the current KT methods are designed specifically for classification tasks and are not effective for other kinds of representation learning tasks. This paper presents a new KT technique that can train a student model to maintain the same amount of mutual information as the teacher model between the learned representation and a set of (potentially unknown) labels. This method not only outperforms existing KT techniques but also overcomes several limitations of those methods, providing new insights into KT and new applications, such as transferring knowledge from handcrafted feature extractors and {cross-modal} KT between the textual and visual modalities of data.",1
"Unlike conventional frame-based sensors, event-based visual sensors output information through spikes at a high temporal resolution. By only encoding changes in pixel intensity, they showcase a low-power consuming, low-latency approach to visual information sensing. To use this information for higher sensory tasks like object recognition and tracking, an essential simplification step is the extraction and learning of features. An ideal feature descriptor must be robust to changes involving (i) local transformations and (ii) re-appearances of a local event pattern. To that end, we propose a novel spatiotemporal feature representation learning algorithm based on slow feature analysis (SFA). Using SFA, smoothly changing linear projections are learnt which are robust to local visual transformations. In order to determine if the features can learn to be invariant to various visual transformations, feature point tracking tasks are used for evaluation. Extensive experiments across two datasets demonstrate the adaptability of the spatiotemporal feature learner to translation, scaling and rotational transformations of the feature points. More importantly, we find that the obtained feature representations are able to exploit the high temporal resolution of such event-based cameras in generating better feature tracks.",0
"Event-based visual sensors differ from traditional frame-based sensors in that they use spikes to output information with high temporal resolution, encoding only changes in pixel intensity to offer a low-power, low-latency approach to visual sensing. However, to utilize this information for more complex tasks like object recognition and tracking, it is necessary to simplify the information through feature extraction and learning. A feature descriptor that is robust to local transformations and re-appearances of a local event pattern is ideal. To address this, we propose a spatiotemporal feature representation learning algorithm based on slow feature analysis (SFA), which learns smoothly changing linear projections that can withstand local visual transformations. Feature point tracking tasks are used to evaluate the adaptability of the features to various visual transformations. Our extensive experiments across two datasets demonstrate the effectiveness of the spatiotemporal feature learner in generating better feature tracks by exploiting the high temporal resolution of event-based cameras.",1
"We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.",0
"THOR, or The House Of inteRactions, is a visual AI research framework that can be accessed at http://ai2thor.allenai.org. It features indoor scenes that are nearly photo-realistic in appearance, allowing AI agents to navigate and interact with objects to complete tasks. AI2-THOR is versatile and can be applied to a variety of domains, such as deep reinforcement learning, imitation learning, planning, and more. Its purpose is to aid in the creation of visually intelligent models and advance research in this field.",1
"Multiple kernel learning (MKL) algorithms combine different base kernels to obtain a more efficient representation in the feature space. Focusing on discriminative tasks, MKL has been used successfully for feature selection and finding the significant modalities of the data. In such applications, each base kernel represents one dimension of the data or is derived from one specific descriptor. Therefore, MKL finds an optimal weighting scheme for the given kernels to increase the classification accuracy. Nevertheless, the majority of the works in this area focus on only binary classification problems or aim for linear separation of the classes in the kernel space, which are not realistic assumptions for many real-world problems. In this paper, we propose a novel multi-class MKL framework which improves the state-of-the-art by enhancing the local separation of the classes in the feature space. Besides, by using a sparsity term, our large-margin multiple kernel algorithm (LMMK) performs discriminative feature selection by aiming to employ a small subset of the base kernels. Based on our empirical evaluations on different real-world datasets, LMMK provides a competitive classification accuracy compared with the state-of-the-art algorithms in MKL. Additionally, it learns a sparse set of non-zero kernel weights which leads to a more interpretable feature selection and representation learning.",0
"Algorithms for multiple kernel learning (MKL) are used to combine various base kernels, resulting in a more efficient representation in the feature space. MKL is widely used in discriminative tasks for successful feature selection and identifying significant modalities of the data. Each base kernel represents a dimension of the data or is derived from a specific descriptor. MKL finds an optimal weighting scheme for the given kernels to increase classification accuracy. However, the majority of research in this area focuses on binary classification problems or linear separation of classes in the kernel space, which is not realistic for real-world problems. This paper proposes a novel multi-class MKL framework that enhances local separation of classes in the feature space. The large-margin multiple kernel algorithm (LMMK) performs discriminative feature selection with a sparsity term, aiming to employ a small subset of base kernels. Through empirical evaluations on various real-world datasets, LMMK provides competitive classification accuracy compared to state-of-the-art algorithms in MKL. It also learns a sparse set of non-zero kernel weights, resulting in more interpretable feature selection and representation learning.",1
"Offline Signature Verification (OSV) is a challenging pattern recognition task, especially in presence of skilled forgeries that are not available during training. This study aims to tackle its challenges and meet the substantial need for generalization for OSV by examining different loss functions for Convolutional Neural Network (CNN). We adopt our new approach to OSV by asking two questions: 1. which classification loss provides more generalization for feature learning in OSV? , and 2. How integration of different losses into a unified multi-loss function lead to an improved learning framework? These questions are studied based on analysis of three loss functions, including cross entropy, Cauchy-Schwarz divergence, and hinge loss. According to complementary features of these losses, we combine them into a dynamic multi-loss function and propose a novel ensemble framework for simultaneous use of them in CNN. Our proposed Multi-Loss Snapshot Ensemble (MLSE) consists of several sequential trials. In each trial, a dominant loss function is selected from the multi-loss set, and the remaining losses act as a regularizer. Different trials learn diverse representations for each input based on signature identification task. This multi-representation set is then employed for the verification task. An ensemble of SVMs is trained on these representations, and their decisions are finally combined according to the selection of most generalizable SVM for each user. We conducted two sets of experiments based on two different protocols of OSV, i.e., writer-dependent and writer-independent on three signature datasets: GPDS-Synthetic, MCYT, and UT-SIG. Based on the writer-dependent OSV protocol, we achieved substantial improvements over the best EERs in the literature. The results of the second set of experiments also confirmed the robustness to the arrival of new users enrolled in the OSV system.",0
"The task of Offline Signature Verification (OSV) is difficult to accomplish due to the existence of skilled forgeries that are not available during training. This study aims to address this challenge by examining different loss functions for Convolutional Neural Network (CNN) to meet the significant need for generalization for OSV. The study poses two questions: 1. which classification loss provides greater generalization for feature learning in OSV? and 2. How can the integration of different losses into a unified multi-loss function lead to an improved learning framework? To answer these questions, the study analyzes three loss functions - cross entropy, Cauchy-Schwarz divergence, and hinge loss - and combines them into a dynamic multi-loss function using a novel ensemble framework called Multi-Loss Snapshot Ensemble (MLSE). MLSE consists of several sequential trials, each using a dominant loss function from the multi-loss set and the remaining losses as regularizers. The resulting multi-representation set is then employed for the verification task, and an ensemble of SVMs is trained on these representations. The study conducted two sets of experiments based on two different protocols of OSV - writer-dependent and writer-independent - on three signature datasets: GPDS-Synthetic, MCYT, and UT-SIG. Based on the writer-dependent OSV protocol, the study achieved significant improvements over the best EERs in the literature. The results of the second set of experiments also confirmed the robustness of the approach to the arrival of new users in the OSV system.",1
"Dynamic scenes that contain both object motion and egomotion are a challenge for monocular visual odometry (VO). Another issue with monocular VO is the scale ambiguity, i.e. these methods cannot estimate scene depth and camera motion in real scale. Here, we propose a learning based approach to predict camera motion parameters directly from optic flow, by marginalizing depthmap variations and outliers. This is achieved by learning a sparse overcomplete basis set of egomotion in an autoencoder network, which is able to eliminate irrelevant components of optic flow for the task of camera parameter or motionfield estimation. The model is trained using a sparsity regularizer and a supervised egomotion loss, and achieves the state-of-the-art performances on trajectory prediction and camera rotation prediction tasks on KITTI and Virtual KITTI datasets, respectively. The sparse latent space egomotion representation learned by the model is robust and requires only 5% of the hidden layer neurons to maintain the best trajectory prediction accuracy on KITTI dataset. Additionally, in presence of depth information, the proposed method demonstrates faithful object velocity prediction for wide range of object sizes and speeds by global compensation of predicted egomotion and a divisive normalization procedure.",0
"Monocular visual odometry (VO) faces difficulties in processing dynamic scenes with both object and egomotion, and scale ambiguity due to the inability to estimate scene depth and camera motion in real scale. To address these issues, we suggest a learning-based approach that predicts camera motion parameters directly from optic flow by eliminating irrelevant components of optic flow through learning a sparse overcomplete basis set of egomotion in an autoencoder network. The model is trained using a sparsity regularizer and a supervised egomotion loss, achieving state-of-the-art performances on trajectory prediction and camera rotation prediction tasks on KITTI and Virtual KITTI datasets. The sparse latent space egomotion representation learned by the model is robust and requires only 5% of the hidden layer neurons to maintain the best trajectory prediction accuracy on KITTI dataset. Furthermore, in the presence of depth information, the proposed method accurately predicts object velocity for a wide range of object sizes and speeds through global compensation of predicted egomotion and a divisive normalization procedure.",1
"3D multi object generative models allow us to synthesize a large range of novel 3D multi object scenes and also identify objects, shapes, layouts and their positions. But multi object scenes are difficult to create because of the dataset being multimodal in nature. The conventional 3D generative adversarial models are not efficient in generating multi object scenes, they usually tend to generate either one object or generate fuzzy results of multiple objects. Auto-encoder models have much scope in feature extraction and representation learning using the unsupervised paradigm in probabilistic spaces. We try to make use of this property in our proposed model. In this paper we propose a novel architecture using 3DConvNets trained with the progressive training paradigm that has been able to generate realistic high resolution 3D scenes of rooms, bedrooms, offices etc. with various pieces of furniture and objects. We make use of the adversarial auto-encoder along with the WGAN-GP loss parameter in our discriminator loss function. Finally this new approach to multi object scene generation has also been able to generate more number of objects per scene.",0
"Using 3D multi object generative models enables the production of diverse 3D multi object scenarios and allows for the identification of object properties such as shape, layout, and positioning. However, creating multi object scenes is challenging due to the multimodal nature of the dataset. Traditional 3D generative adversarial models have limitations in generating multi object scenes, often producing either only one object or unclear results for multiple objects. Auto-encoder models have potential for feature extraction and representation learning with the unsupervised paradigm in probabilistic spaces. To address this, we present a new architecture using 3DConvNets trained with the progressive training paradigm, capable of generating realistic high resolution 3D scenes of rooms, bedrooms, offices, and more with various furniture and objects. Our approach uses the adversarial auto-encoder with the WGAN-GP loss parameter in the discriminator loss function. This approach has resulted in generating a greater number of objects per scene and demonstrates a new method for multi object scene generation.",1
"There is a long history of using meta learning as representation learning, specifically for determining the relevance of inputs. In this paper, we examine an instance of meta-learning in which feature relevance is learned by adapting step size parameters of stochastic gradient descent---building on a variety of prior work in stochastic approximation, machine learning, and artificial neural networks. In particular, we focus on stochastic meta-descent introduced in the Incremental Delta-Bar-Delta (IDBD) algorithm for setting individual step sizes for each feature of a linear function approximator. Using IDBD, a feature with large or small step sizes will have a large or small impact on generalization from training examples. As a main contribution of this work, we extend IDBD to temporal-difference (TD) learning---a form of learning which is effective in sequential, non i.i.d. problems. We derive a variety of IDBD generalizations for TD learning, demonstrating that they are able to distinguish which features are relevant and which are not. We demonstrate that TD IDBD is effective at learning feature relevance in both an idealized gridworld and a real-world robotic prediction task.",0
"Meta learning has been used for representation learning for a long time, particularly for determining input relevance. This paper examines a specific example of meta-learning, where feature relevance is learned by adjusting the step size parameters of stochastic gradient descent. This is an extension of prior work in stochastic approximation, machine learning, and artificial neural networks. The focus is on the stochastic meta-descent introduced in the IDBD algorithm, which sets individual step sizes for each feature of a linear function approximator. IDBD can determine the impact of a feature with large or small step sizes on generalization from training examples. The main contribution of this work is the extension of IDBD to TD learning, which is effective in sequential, non i.i.d. problems. We derive various IDBD generalizations for TD learning to distinguish relevant and irrelevant features. Our experiments show that TD IDBD is effective at learning feature relevance in both an idealized gridworld and a real-world robotic prediction task.",1
We develop hierarchically quantized efficient embedding representations for similarity-based search and show that this representation provides not only the state of the art performance on the search accuracy but also provides several orders of speed up during inference. The idea is to hierarchically quantize the representation so that the quantization granularity is greatly increased while maintaining the accuracy and keeping the computational complexity low. We also show that the problem of finding the optimal sparse compound hash code respecting the hierarchical structure can be optimized in polynomial time via minimum cost flow in an equivalent flow network. This allows us to train the method end-to-end in a mini-batch stochastic gradient descent setting. Our experiments on Cifar100 and ImageNet datasets show the state of the art search accuracy while providing several orders of magnitude search speedup respectively over exhaustive linear search over the dataset.,0
"We have developed a method for similarity-based search that involves creating efficient embedding representations with hierarchical quantization. This approach not only achieves the highest level of search accuracy, but also speeds up the inference process significantly. By increasing the granularity of the quantization while maintaining accuracy and keeping computational complexity low, we are able to optimize the optimal sparse compound hash code in polynomial time using minimum cost flow in an equivalent flow network. This allows us to train the method end-to-end in a mini-batch stochastic gradient descent setting. Our experiments on Cifar100 and ImageNet datasets demonstrate that our method outperforms exhaustive linear search over the dataset, providing several orders of magnitude search speedup while maintaining state-of-the-art search accuracy.",1
"This paper introduces a novel measure-theoretic theory for machine learning that does not require statistical assumptions. Based on this theory, a new regularization method in deep learning is derived and shown to outperform previous methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposed theory provides a theoretical basis for a family of practically successful regularization methods in deep learning. We discuss several consequences of our results on one-shot learning, representation learning, deep learning, and curriculum learning. Unlike statistical learning theory, the proposed learning theory analyzes each problem instance individually via measure theory, rather than a set of problem instances via statistics. As a result, it provides different types of results and insights when compared to statistical learning theory.",0
"In this paper, a new measure-theoretic theory for machine learning is presented, which eliminates the need for statistical assumptions. The theory is used to derive a novel regularization method for deep learning, which outperforms previous methods in CIFAR-10, CIFAR-100, and SVHN. Additionally, the proposed theory establishes a theoretical foundation for a range of effective regularization methods in deep learning. The implications of these results on one-shot learning, representation learning, deep learning, and curriculum learning are also discussed. Unlike statistical learning theory, the proposed approach examines each problem instance individually through measure theory, rather than analyzing a set of problem instances via statistics. As a result, it yields distinct types of insights and findings compared to statistical learning theory.",1
"Large amounts of labeled data are typically required to train deep learning models. For many real-world problems, however, acquiring additional data can be expensive or even impossible. We present semi-supervised deep kernel learning (SSDKL), a semi-supervised regression model based on minimizing predictive variance in the posterior regularization framework. SSDKL combines the hierarchical representation learning of neural networks with the probabilistic modeling capabilities of Gaussian processes. By leveraging unlabeled data, we show improvements on a diverse set of real-world regression tasks over supervised deep kernel learning and semi-supervised methods such as VAT and mean teacher adapted for regression.",0
"Usually, deep learning models require a vast amount of labeled data for training. However, in certain cases, obtaining additional data may be unfeasible or pricey. We introduce semi-supervised deep kernel learning (SSDKL), a semi-supervised regression model that decreases predictive variance in the posterior regularization structure. SSDKL merges the neural network's hierarchical representation learning with Gaussian processes' probabilistic modeling capabilities. By utilizing unlabeled data, we demonstrate advancements in various real-world regression tasks compared to supervised deep kernel learning and semi-supervised techniques like VAT and mean teacher adapted for regression.",1
"In this paper, we present a novel strategy to design disentangled 3D face shape representation. Specifically, a given 3D face shape is decomposed into identity part and expression part, which are both encoded and decoded in a nonlinear way. To solve this problem, we propose an attribute decomposition framework for 3D face mesh. To better represent face shapes which are usually nonlinear deformed between each other, the face shapes are represented by a vertex based deformation representation rather than Euclidean coordinates. The experimental results demonstrate that our method has better performance than existing methods on decomposing the identity and expression parts. Moreover, more natural expression transfer results can be achieved with our method than existing methods.",0
"This article introduces a new approach for creating disentangled 3D face shape representation. The method involves breaking down a 3D face shape into two parts: identity and expression. These two parts are both encoded and decoded using non-linear techniques. To address this issue, a framework for attribute decomposition for 3D face mesh is proposed. To represent face shapes that are often non-linearly deformed, a vertex-based deformation representation is used instead of Euclidean coordinates. The experimental results reveal that our method outperforms existing approaches in decomposing identity and expression parts. Furthermore, our method produces more natural expression transfer outcomes than other methods.",1
"Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training. Particularly, DenseNet, which connects each layer to every other layer in a feed-forward fashion, has shown impressive performances in natural image classification tasks. We propose HyperDenseNet, a 3D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path, but also between those across different paths. This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network. Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation. We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on 6-month infant data and the latter on adult images. HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks. We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning. Our code is publicly available at https://www.github.com/josedolz/HyperDenseNet.",0
"In computer vision, there has been a recent surge of interest in dense connections due to their ability to facilitate gradient flow and deep supervision during training. Among these, DenseNet has exhibited impressive performance in natural image classification tasks by connecting each layer to all other layers in a feed-forward fashion. This paper introduces HyperDenseNet, a 3D fully convolutional neural network that extends dense connectivity to multi-modal segmentation problems. In this network, each imaging modality has its own path and dense connections occur not only within paths but also between them. This approach contrasts with existing multi-modal CNNs that rely on a single joint layer for fusion. Thus, HyperDenseNet is able to learn more complex combinations between modalities, increasing the learning representation. The network was tested on two competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, and outperformed many state-of-the-art segmentation networks. The importance of hyper-dense connections in multi-modal representation learning was confirmed through a comprehensive experimental analysis of features re-use. The code for HyperDenseNet is publicly available at https://www.github.com/josedolz/HyperDenseNet.",1
"An increasing number of datasets contain multiple views, such as video, sound and automatic captions. A basic challenge in representation learning is how to leverage multiple views to learn better representations. This is further complicated by the existence of a latent alignment between views, such as between speech and its transcription, and by the multitude of choices for the learning objective. We explore an advanced, correlation-based representation learning method on a 4-way parallel, multimodal dataset, and assess the quality of the learned representations on retrieval-based tasks. We show that the proposed approach produces rich representations that capture most of the information shared across views. Our best models for speech and textual modalities achieve retrieval rates from 70.7% to 96.9% on open-domain, user-generated instructional videos. This shows it is possible to learn reliable representations across disparate, unaligned and noisy modalities, and encourages using the proposed approach on larger datasets.",0
"The number of datasets that contain various views such as video, sound, and captions is on the rise. In the field of representation learning, a significant challenge is how to use these multiple views to improve the quality of the representations. However, this is made more complex by the latent alignment between views, such as the connection between speech and transcription, and the numerous choices for the learning objective. Our study examines an advanced representation learning method that is based on correlation, using a 4-way parallel, multimodal dataset. We evaluate the quality of the learned representations by measuring how well they perform on retrieval-based tasks. Our results show that the proposed approach generates intricate representations that capture most of the shared information across views. Our top-performing models for speech and text modalities achieve retrieval rates ranging from 70.7% to 96.9% on open-domain, user-generated instructional videos. These findings demonstrate that it is feasible to obtain dependable representations across various, unaligned, and noisy modalities, and encourage the use of our approach on larger datasets.",1
"Exploiting multi-scale representations is critical to improve edge detection for objects at different scales. To extract edges at dramatically different scales, we propose a Bi-Directional Cascade Network (BDCN) structure, where an individual layer is supervised by labeled edges at its specific scale, rather than directly applying the same supervision to all CNN outputs. Furthermore, to enrich multi-scale representations learned by BDCN, we introduce a Scale Enhancement Module (SEM) which utilizes dilated convolution to generate multi-scale features, instead of using deeper CNNs or explicitly fusing multi-scale edge maps. These new approaches encourage the learning of multi-scale representations in different layers and detect edges that are well delineated by their scales. Learning scale dedicated layers also results in compact network with a fraction of parameters. We evaluate our method on three datasets, i.e., BSDS500, NYUDv2, and Multicue, and achieve ODS Fmeasure of 0.828, 1.3% higher than current state-of-the art on BSDS500. The code has been available at https://github.com/pkuCactus/BDCN.",0
"To enhance the detection of edges for objects of varying scales, it is crucial to utilize multi-scale representations. Our proposed Bi-Directional Cascade Network (BDCN) structure addresses this by supervising individual layers with labeled edges at their specific scales, as opposed to applying the same supervision to all CNN outputs. Additionally, we introduce a Scale Enhancement Module (SEM) which generates multi-scale features using dilated convolution, rather than relying on deeper CNNs or explicitly fusing multi-scale edge maps. These novel approaches promote the learning of multi-scale representations in different layers, resulting in a compact network with fewer parameters. We evaluated our method on three datasets (BSDS500, NYUDv2, and Multicue) and achieved an ODS Fmeasure of 0.828, 1.3% higher than the current state-of-the-art on BSDS500. The code is available at https://github.com/pkuCactus/BDCN.",1
"Training recurrent neural networks (RNNs) with backpropagation through time (BPTT) has known drawbacks such as being difficult to capture longterm dependencies in sequences. Successful alternatives to BPTT have not yet been discovered. Recently, BP with synthetic gradients by a decoupled neural interface module has been proposed to replace BPTT for training RNNs. On the other hand, it has been shown that the representations learned with synthetic and real gradients are different though they are functionally identical. In this project, we explore ways of combining synthetic and real gradients with application to neural language modeling tasks. Empirically, we demonstrate the effectiveness of alternating training with synthetic and real gradients after periodic warm restarts on language modeling tasks.",0
"The process of training recurrent neural networks (RNNs) using backpropagation through time (BPTT) has limitations, particularly with regards to the difficulty of capturing long-term dependencies in sequences. Despite this, there has been no successful alternative to BPTT. Recently, a new method called BP with synthetic gradients has been introduced as a replacement for BPTT in RNN training. However, it has been discovered that the representations learned with synthetic gradients differ from those learned with real gradients despite their functional similarity. In this project, we aim to explore the possibility of combining synthetic and real gradients for neural language modeling tasks. Our empirical results show that alternating training with synthetic and real gradients, following periodic warm restarts, improves the effectiveness of language modeling tasks.",1
"A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks. Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.",0
"The primary objective of unsupervised learning is to identify useful data representations for future tasks without relying on supervised labels during training. This is typically achieved by minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the expectation that the representations will be beneficial for future tasks. However, we propose a different approach in this study, whereby we directly target future desired tasks by meta-learning an unsupervised learning rule that will generate helpful representations. Our focus is on semi-supervised classification performance, and we have meta-learned an unsupervised weight update rule that creates representations that are useful for this task. To ensure that the unsupervised update rule is biologically motivated and neuron-local, we have imposed constraints that enable it to generalize across various neural network architectures, datasets, and data modalities. Our findings demonstrate that the meta-learned unsupervised update rule generates valuable features and sometimes surpasses existing unsupervised learning methods. We have also demonstrated that the meta-learned unsupervised update rule can generalize to train networks with different widths, depths, and nonlinearities. Furthermore, it can train on data with randomly permuted input dimensions and even extend from image datasets to text tasks.",1
"We provide a new approach to training neural models to exhibit transparency in a well-defined, functional manner. Our approach naturally operates over structured data and tailors the predictor, functionally, towards a chosen family of (local) witnesses. The estimation problem is setup as a co-operative game between an unrestricted predictor such as a neural network, and a set of witnesses chosen from the desired transparent family. The goal of the witnesses is to highlight, locally, how well the predictor conforms to the chosen family of functions, while the predictor is trained to minimize the highlighted discrepancy. We emphasize that the predictor remains globally powerful as it is only encouraged to agree locally with locally adapted witnesses. We analyze the effect of the proposed approach, provide example formulations in the context of deep graph and sequence models, and empirically illustrate the idea in chemical property prediction, temporal modeling, and molecule representation learning.",0
"Our novel method aims to train neural models to demonstrate transparency in a clear and functional manner. Our approach is designed to work with structured data and customizes the predictor to a selected group of local witnesses. We frame the estimation problem as a collaborative game between an unrestricted predictor, such as a neural network, and a set of witnesses chosen from the transparent family to highlight how well the predictor adheres to the chosen function family. The predictor is trained to minimize the highlighted differences, while remaining globally powerful by only agreeing locally with locally adapted witnesses. We analyze the impact of this approach, provide examples of its application in deep graph and sequence models, and demonstrate its effectiveness in chemical property prediction, temporal modeling, and molecule representation learning.",1
"Short-term road traffic prediction (STTP) is one of the most important modules in Intelligent Transportation Systems (ITS). However, network-level STTP still remains challenging due to the difficulties both in modeling the diverse traffic patterns and tacking high-dimensional time series with low latency. Therefore, a framework combining with a deep clustering (DeepCluster) module is developed for STTP at largescale networks in this paper. The DeepCluster module is proposed to supervise the representation learning in a visualized way from the large unlabeled dataset. More specifically, to fully exploit the traffic periodicity, the raw series is first split into a number of sub-series for triplets generation. The convolutional neural networks (CNNs) with triplet loss are utilized to extract the features of shape by transferring the series into visual images. The shape-based representations are then used for road segments clustering. Thereafter, motivated by the fact that the road segments in a group have similar patterns, a model sharing strategy is further proposed to build recurrent NNs (RNNs)-based predictions through a group-based model (GM), instead of individual-based model (IM) in which one model are built for one road exclusively. Our framework can not only significantly reduce the number of models and cost, but also increase the number of training data and the diversity of samples. In the end, we evaluate the proposed framework over the network of Liuli Bridge in Beijing. Experimental results show that the DeepCluster can effectively cluster the road segments and GM can achieve comparable performance against the IM with less number of models.",0
"In Intelligent Transportation Systems (ITS), Short-term road traffic prediction (STTP) is a crucial component. However, network-level STTP is still challenging due to the difficulties in modeling diverse traffic patterns and handling high-dimensional time series with low latency. To address this, a framework combining a deep clustering (DeepCluster) module has been developed for STTP at large-scale networks. The DeepCluster module supervises representation learning in a visualized manner from a large unlabeled dataset. To fully exploit traffic periodicity, the raw series is split into sub-series for triplets generation. The convolutional neural networks (CNNs) with triplet loss extract shape-based features by transforming the series into visual images. These representations are then used for clustering road segments. To build recurrent NNs (RNNs)-based predictions, a model-sharing strategy is proposed using a group-based model (GM) motivated by the fact that road segments in a group have similar patterns. This approach reduces the number of models and cost while increasing the number of training data and sample diversity. The framework is evaluated on the Liuli Bridge network in Beijing, and results show that DeepCluster effectively clusters road segments, and the GM achieves comparable performance against the individual-based model (IM) with fewer models.",1
"Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically ""similar"" data points and ""negative samples,"" the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term contrastive learning for such algorithms and presents a theoretical framework for analyzing them by introducing latent classes and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.",0
"In recent studies, unlabeled data has been utilized to acquire feature representations that have broad applicability in classification tasks. These techniques resemble the well-known word2vec embedding algorithm, where the availability of similar data points and negative samples is used to increase the inner product of representations of similar pairs. The term ""contrastive learning"" is used in this paper to describe these algorithms, which are analyzed using a theoretical framework that introduces latent classes and assumes that semantically similar points are sampled from the same class. This framework enables us to provide proven guarantees regarding the performance of the learned representations in average classification tasks that involve a subset of the same latent classes. Our generalization bounds also demonstrate that the learned representations can reduce sample complexity in downstream tasks that require labeled data. To support our theory, we conduct controlled experiments in both the text and image domains.",1
"This is an official pytorch implementation of Deep High-Resolution Representation Learning for Human Pose Estimation. In this work, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. The code and models have been publicly available at \url{https://github.com/leoxiaobin/deep-high-resolution-net.pytorch}.",0
"This is the official PyTorch implementation of Deep High-Resolution Representation Learning for Human Pose Estimation. Our focus in this study is the human pose estimation issue and our primary concern is to learn high-resolution representations that are dependable. Current methods use a high-to-low resolution network to recover high-resolution representations from low-resolution representations. However, our proposed network maintains high-resolution representations throughout the entire process. We start with a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the multi-resolution subnetworks in parallel. We perform repeated multi-scale fusions to ensure that each high-to-low resolution representation receives information from other parallel representations repeatedly, resulting in rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We have demonstrated the effectiveness of our network through superior pose estimation results on two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. The code and models are publicly available on \url{https://github.com/leoxiaobin/deep-high-resolution-net.pytorch}.",1
"Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training. To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training. Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of ""Information Plasticity"". Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.",0
"Deep artificial neural networks, like humans and animals, experience critical periods that can hinder the development of a skill when subjected to a temporary stimulus deficit. The degree of impairment is dependent on the timing and duration of the deficit window, as well as the size of the neural network. Deficits that don't affect low-level statistics can be overcome with further training and do not have a lasting effect on performance. To better comprehend this occurrence, we utilize the Fisher Information of the weights to gauge the effective connectivity between layers of a network during training. Contrary to expectations, information peaks early on in training before declining, hindering the redistribution of information resources in a phenomenon known as ""Information Plasticity"" loss. Our study indicates that the first few epochs are crucial in creating robust connections that are optimal in relation to the input data distribution. Once these connections have been established, they don't appear to change during additional training. Our findings suggest that the initial learning transient, which is often overlooked in comparison to asymptotic behavior, plays a significant role in determining the training process's outcome. Additionally, our research, combined with recent theoretical results in the literature, suggests that forgetting (a decrease in information in the weights) is necessary to achieve invariance and disentanglement in representation learning. Finally, critical periods aren't limited to biological systems but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constraints arising from learning dynamics and information processing.",1
"In this paper, we present a hypergraph neural networks (HGNN) framework for data representation learning, which can encode high-order data correlation in a hypergraph structure. Confronting the challenges of learning representation for complex data in real practice, we propose to incorporate such data structure in a hypergraph, which is more flexible on data modeling, especially when dealing with complex data. In this method, a hyperedge convolution operation is designed to handle the data correlation during representation learning. In this way, traditional hypergraph learning procedure can be conducted using hyperedge convolution operations efficiently. HGNN is able to learn the hidden layer representation considering the high-order data structure, which is a general framework considering the complex data correlations. We have conducted experiments on citation network classification and visual object recognition tasks and compared HGNN with graph convolutional networks and other traditional methods. Experimental results demonstrate that the proposed HGNN method outperforms recent state-of-the-art methods. We can also reveal from the results that the proposed HGNN is superior when dealing with multi-modal data compared with existing methods.",0
"This paper introduces a framework for data representation learning called hypergraph neural networks (HGNN). The proposed method addresses the challenges of learning complex data representation by incorporating a hypergraph structure that can encode high-order data correlation. This structure is more flexible and efficient in modeling complex data. The HGNN approach employs a hyperedge convolution operation to handle data correlation during representation learning. This method enables traditional hypergraph learning procedures to be conducted effectively. HGNN is a general framework that considers complex data correlations and can learn hidden layer representations while considering high-order data structures. Experimental results on citation network classification and visual object recognition tasks demonstrate that HGNN outperforms recent state-of-the-art methods, particularly in dealing with multi-modal data.",1
"Neural networks designed for the task of classification have become a commodity in recent years. Many works target the development of more effective networks, which results in a complexification of their architectures with more layers, multiple sub-networks, or even the combination of multiple classifiers, but this often comes at the expense of producing uninterpretable black boxes. In this paper, we redesign a simple capsule network to enable it to synthesize class-representative samples, called prototypes, by replacing the last layer with a novel Hit-or-Miss layer. This layer contains activated vectors, called capsules, that we train to hit or miss a fixed target capsule by tailoring a specific centripetal loss function. This possibility allows to develop a data augmentation step combining information from the data space and the feature space, resulting in a hybrid data augmentation process. We show that our network, named HitNet, is able to reach better performances than those reproduced with the initial CapsNet on several datasets, while allowing to visualize the nature of the features extracted as deformations of the prototypes, which provides a direct insight into the feature representation learned by the network .",0
"Over the years, classification neural networks have become commonplace. However, many researchers strive to improve them by making their architectures more complex with additional layers, sub-networks, and even multiple classifiers. Unfortunately, this often results in producing uninterpretable black boxes. This paper presents a solution by redesigning a simple capsule network with a Hit-or-Miss layer to synthesize class-representative samples, or prototypes. We train activated vectors, called capsules, to hit or miss a fixed target capsule using a specific centripetal loss function. This allows for a hybrid data augmentation process by combining information from the data space and feature space. The resulting network, HitNet, outperforms the initial CapsNet and enables visualization of the features extracted as deformations of the prototypes. This approach provides insight into the feature representation learned by the network.",1
"Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",0
"Graph Neural Networks (GNNs) are a successful method for learning graph representations. They use a neighborhood aggregation approach where a node's representation vector is computed by aggregating and transforming the vectors of its neighboring nodes. Several GNN variations have been proposed and have achieved excellent results in both node and graph classification tasks. However, despite their success, there is limited understanding of GNNs' representational characteristics and limitations. Therefore, we have introduced a theoretical framework that can analyze the expressive power of GNNs for capturing different graph structures. Our study reveals that popular GNN variants like Graph Convolutional Networks and GraphSAGE cannot learn to distinguish some simple graph structures. We have also developed a simple architecture that is the most expressive among GNNs and can compete with the Weisfeiler-Lehman graph isomorphism test. We have validated our theoretical findings on various graph classification benchmarks and demonstrated that our model achieves state-of-the-art performance.",1
"In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.",0
"The aim of this study is to develop representations through unsupervised learning by maximizing mutual information between the input and the output of a deep neural network encoder. The research demonstrates that the structure is crucial, and incorporating knowledge about the input's locality into the objective can significantly enhance the representation's applicability for downstream tasks. Additionally, the team ensures that the representation's characteristics are controlled by matching them to a prior distribution adversarially. This method, which they refer to as Deep InfoMax (DIM), surpasses several popular unsupervised learning approaches and competes with fully-supervised learning in various classification tasks. DIM creates new possibilities for unsupervised learning of representations and is a critical milestone towards developing flexible formulations of representation-learning objectives to meet specific end-goals.",1
"Unsupervised learning of compact and relevant state representations has been proved very useful at solving complex reinforcement learning tasks. In this paper, we propose a recurrent capsule network that learns such representations by trying to predict the future observations in an agent's trajectory.",0
"The acquisition of concise and pertinent state representations through unsupervised learning has been demonstrated to be highly effective in resolving intricate reinforcement learning challenges. Our study introduces a recurrent capsule network, which acquires such representations by anticipating the agent's future observations in their path.",1
"Learning powerful discriminative features for remote sensing image scene classification is a challenging computer vision problem. In the past, most classification approaches were based on handcrafted features. However, most recent approaches to remote sensing scene classification are based on Convolutional Neural Networks (CNNs). The de facto practice when learning these CNN models is only to use original RGB patches as input with training performed on large amounts of labeled data (ImageNet). In this paper, we show class activation map (CAM) encoded CNN models, codenamed DDRL-AM, trained using original RGB patches and attention map based class information provide complementary information to the standard RGB deep models. To the best of our knowledge, we are the first to investigate attention information encoded CNNs. Additionally, to enhance the discriminability, we further employ a recently developed object function called ""center loss,"" which has proved to be very useful in face recognition. Finally, our framework provides attention guidance to the model in an end-to-end fashion. Extensive experiments on two benchmark datasets show that our approach matches or exceeds the performance of other methods.",0
"Remote sensing image scene classification presents a difficult challenge for computer vision due to the need for powerful discriminative features. Historically, handcrafted features were the basis for most classification approaches, but in recent years, Convolutional Neural Networks (CNNs) have become the preferred method. Typically, only original RGB patches are used as input when training CNN models with large amounts of labeled data (ImageNet). However, in this study, we introduce DDRL-AM, a CNN model that incorporates attention map based class information and class activation map (CAM) encoding to provide complementary information to standard RGB deep models. Our investigation of attention information encoded CNNs is the first of its kind. Additionally, we utilize a recently developed object function called ""center loss"" to enhance the discriminability of our model, which has proven successful in face recognition. Our framework incorporates attention guidance to the model in an end-to-end manner. Through extensive experiments on two benchmark datasets, we demonstrate that our approach matches or exceeds the performance of other methods.",1
"Fine-grained visual categorization is to recognize hundreds of subcategories belonging to the same basic-level category, which is a highly challenging task due to the quite subtle and local visual distinctions among similar subcategories. Most existing methods generally learn part detectors to discover discriminative regions for better categorization performance. However, not all parts are beneficial and indispensable for visual categorization, and the setting of part detector number heavily relies on prior knowledge as well as experimental validation. As is known to all, when we describe the object of an image via textual descriptions, we mainly focus on the pivotal characteristics, and rarely pay attention to common characteristics as well as the background areas. This is an involuntary transfer from human visual attention to textual attention, which leads to the fact that textual attention tells us how many and which parts are discriminative and significant to categorization. So textual attention could help us to discover visual attention in image. Inspired by this, we propose a fine-grained visual-textual representation learning (VTRL) approach, and its main contributions are: (1) Fine-grained visual-textual pattern mining devotes to discovering discriminative visual-textual pairwise information for boosting categorization performance through jointly modeling vision and text with generative adversarial networks (GANs), which automatically and adaptively discovers discriminative parts. (2) Visual-textual representation learning jointly combines visual and textual information, which preserves the intra-modality and inter-modality information to generate complementary fine-grained representation, as well as further improves categorization performance.",0
"Recognizing hundreds of subcategories within a basic-level category through fine-grained visual categorization is a difficult task due to the subtle and localized visual differences between similar subcategories. Most existing methods use part detectors to identify distinctive regions for better categorization performance. However, not all parts are necessary for visual categorization, and determining the number of part detectors relies heavily on prior knowledge and experimentation. When describing an object in an image through textual descriptions, we tend to focus on the crucial characteristics while ignoring common features and background areas. This unintentional shift from visual to textual attention can help us identify discriminative and significant parts for categorization. To leverage this, we propose a fine-grained visual-textual representation learning (VTRL) approach, which uses generative adversarial networks (GANs) to jointly model vision and text and automatically discover discriminative parts. This approach combines visual and textual information to generate a complementary fine-grained representation that further enhances categorization performance.",1
"Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.",0
"The field of graph representation learning has been transformed by graph neural networks (GNNs), which can effectively learn node embeddings and achieve state-of-the-art results in tasks like node classification and link prediction. However, existing GNN methods are flat and don't learn hierarchical representations of graphs, which is a major issue for graph classification, where the aim is to predict the label for an entire graph. To address this limitation, we present DiffPool, a differentiable graph pooling module that generates hierarchical representations of graphs and can be integrated with various GNN architectures in an end-to-end manner. DiffPool uses a differentiable soft cluster assignment to map nodes to clusters at each layer of a deep GNN, which serves as the coarsened input for the next GNN layer. Our experiments show that combining existing GNN methods with DiffPool enhances accuracy by 5-10% on average for graph classification benchmarks compared to all other pooling approaches, achieving a new state-of-the-art on four of five benchmark data sets.",1
"Building agents to interact with the web would allow for significant improvements in knowledge understanding and representation learning. However, web navigation tasks are difficult for current deep reinforcement learning (RL) models due to the large discrete action space and the varying number of actions between the states. In this work, we introduce DOM-Q-NET, a novel architecture for RL-based web navigation to address both of these problems. It parametrizes Q functions with separate networks for different action categories: clicking a DOM element and typing a string input. Our model utilizes a graph neural network to represent the tree-structured HTML of a standard web page. We demonstrate the capabilities of our model on the MiniWoB environment where we can match or outperform existing work without the use of expert demonstrations. Furthermore, we show 2x improvements in sample efficiency when training in the multi-task setting, allowing our model to transfer learned behaviours across tasks.",0
"Enabling agents to interact with the internet could result in significant enhancements in knowledge comprehension and representation learning. However, current deep reinforcement learning (RL) models face challenges in web navigation tasks due to the vast discrete action space and varying action numbers between states. In this study, we present DOM-Q-NET, a novel RL-based web navigation architecture that addresses these issues. Our model parametrizes Q functions with distinct networks for clicking DOM elements and typing string inputs. It employs a graph neural network to represent the tree-structured HTML of a standard web page. We illustrate our model's capabilities on the MiniWoB environment, where we can match or exceed existing work without expert demonstrations. Moreover, we demonstrate 2x advancements in sample efficiency while training in the multi-task context, allowing our model to transfer learned behaviors between tasks.",1
"Deep generative models like variational autoencoders approximate the intrinsic geometry of high dimensional data manifolds by learning low-dimensional latent-space variables and an embedding function. The geometric properties of these latent spaces has been studied under the lens of Riemannian geometry; via analysis of the non-linearity of the generator function. In new developments, deep generative models have been used for learning semantically meaningful `disentangled' representations; that capture task relevant attributes while being invariant to other attributes. In this work, we explore the geometry of popular generative models for disentangled representation learning. We use several metrics to compare the properties of latent spaces of disentangled representation models in terms of class separability and curvature of the latent-space. The results we obtain establish that the class distinguishable features in the disentangled latent space exhibits higher curvature as opposed to a variational autoencoder. We evaluate and compare the geometry of three such models with variational autoencoder on two different datasets. Further, our results show that distances and interpolation in the latent space are significantly improved with Riemannian metrics derived from the curvature of the space. We expect these results will have implications on understanding how deep-networks can be made more robust, generalizable, as well as interpretable.",0
"Variational autoencoders are deep generative models that approximate the intrinsic geometry of high-dimensional data manifolds by learning low-dimensional latent space variables and an embedding function. The geometric properties of these latent spaces have been studied using Riemannian geometry by analyzing the non-linearity of the generator function. Recent developments have focused on using deep generative models to learn ""disentangled"" representations that capture task-relevant attributes while being invariant to other attributes. In this study, we investigate the geometry of popular generative models for disentangled representation learning by comparing the properties of their latent spaces in terms of class separability and curvature. Our findings reveal that the disentangled latent space exhibits higher curvature in class-distinguishable features compared to a variational autoencoder. We evaluate and compare the geometry of three models with a variational autoencoder on two different datasets. Furthermore, our results demonstrate that Riemannian metrics derived from the curvature of the space significantly improve distances and interpolation in the latent space. We anticipate that these results will contribute to a better understanding of how deep networks can be made more robust, generalizable, and interpretable.",1
"A Restricted Boltzmann Machine (RBM) is an unsupervised machine-learning bipartite graphical model that jointly learns a probability distribution over data and extracts their relevant statistical features. As such, RBM were recently proposed for characterizing the patterns of coevolution between amino acids in protein sequences and for designing new sequences. Here, we study how the nature of the features learned by RBM changes with its defining parameters, such as the dimensionality of the representations (size of the hidden layer) and the sparsity of the features. We show that for adequate values of these parameters, RBM operate in a so-called compositional phase in which visible configurations sampled from the RBM are obtained by recombining these features. We then compare the performance of RBM with other standard representation learning algorithms, including Principal or Independent Component Analysis, autoencoders (AE), variational auto-encoders (VAE), and their sparse variants. We show that RBM, due to the stochastic mapping between data configurations and representations, better capture the underlying interactions in the system and are significantly more robust with respect to sample size than deterministic methods such as PCA or ICA. In addition, this stochastic mapping is not prescribed a priori as in VAE, but learned from data, which allows RBM to show good performance even with shallow architectures. All numerical results are illustrated on synthetic lattice-protein data, that share similar statistical features with real protein sequences, and for which ground-truth interactions are known.",0
"The Restricted Boltzmann Machine (RBM) is a bipartite graphical model used in unsupervised machine learning to learn a probability distribution over data and extract relevant statistical features. Recently, RBMs have been used to study coevolution patterns between amino acids in protein sequences and design new sequences. This study investigates how RBM's features change with its parameters, such as the size of the hidden layer and feature sparsity. RBMs operate in a compositional phase where visible configurations are obtained by recombining features. This study compares RBMs to other standard representation learning algorithms and shows that due to their stochastic mapping between data and representations, RBMs better capture underlying interactions and are more robust than deterministic methods. RBMs also show good performance with shallow architectures since the stochastic mapping is learned from data and not prescribed a priori. Numerical results are illustrated on synthetic lattice-protein data with known ground-truth interactions.",1
"Network representation learning (NRL) has been widely used to help analyze large-scale networks through mapping original networks into a low-dimensional vector space. However, existing NRL methods ignore the impact of properties of relations on the object relevance in heterogeneous information networks (HINs). To tackle this issue, this paper proposes a new NRL framework, called Event2vec, for HINs to consider both quantities and properties of relations during the representation learning process. Specifically, an event (i.e., a complete semantic unit) is used to represent the relation among multiple objects, and both event-driven first-order and second-order proximities are defined to measure the object relevance according to the quantities and properties of relations. We theoretically prove how event-driven proximities can be preserved in the embedding space by Event2vec, which utilizes event embeddings to facilitate learning the object embeddings. Experimental studies demonstrate the advantages of Event2vec over state-of-the-art algorithms on four real-world datasets and three network analysis tasks (including network reconstruction, link prediction, and node classification).",0
"The use of Network Representation Learning (NRL) to analyze large-scale networks by mapping them into a low-dimensional vector space has become widespread. However, the current NRL methods do not take into account the impact of relation properties on the relevance of objects in heterogeneous information networks (HINs). To address this issue, the authors propose a new NRL framework, Event2vec, for HINs. This framework considers both the quantities and properties of relations in the representation learning process. An event, which represents the relation among multiple objects, is used to measure object relevance. Both event-driven first-order and second-order proximities are defined to capture the quantities and properties of relations. The authors prove the preservation of event-driven proximities in the embedding space by Event2vec, which employs event embeddings to learn object embeddings. The paper presents experimental studies that demonstrate the superiority of Event2vec over state-of-the-art algorithms on four real-world datasets and three network analysis tasks, including network reconstruction, link prediction, and node classification.",1
"Multimodal representation learning is gaining more and more interest within the deep learning community. While bilinear models provide an interesting framework to find subtle combination of modalities, their number of parameters grows quadratically with the input dimensions, making their practical implementation within classical deep learning pipelines challenging. In this paper, we introduce BLOCK, a new multimodal fusion based on the block-superdiagonal tensor decomposition. It leverages the notion of block-term ranks, which generalizes both concepts of rank and mode ranks for tensors, already used for multimodal fusion. It allows to define new ways for optimizing the tradeoff between the expressiveness and complexity of the fusion model, and is able to represent very fine interactions between modalities while maintaining powerful mono-modal representations. We demonstrate the practical interest of our fusion model by using BLOCK for two challenging tasks: Visual Question Answering (VQA) and Visual Relationship Detection (VRD), where we design end-to-end learnable architectures for representing relevant interactions between modalities. Through extensive experiments, we show that BLOCK compares favorably with respect to state-of-the-art multimodal fusion models for both VQA and VRD tasks. Our code is available at https://github.com/Cadene/block.bootstrap.pytorch.",0
"The deep learning community is increasingly interested in multimodal representation learning. Although bilinear models offer an intriguing framework to identify subtle modalities combinations, their practical implementation in classical deep learning pipelines is challenging due to the quadratic growth of parameters with input dimensions. In this study, we present BLOCK, a novel multimodal fusion approach that uses the block-superdiagonal tensor decomposition and the concept of block-term ranks. This approach enables us to optimize the tradeoff between the complexity and expressiveness of the fusion model, resulting in fine multimodal interactions and strong mono-modal representations. We demonstrate the practical potential of BLOCK by developing end-to-end learnable architectures for Visual Question Answering (VQA) and Visual Relationship Detection (VRD) tasks. Our experiments show that BLOCK outperforms state-of-the-art multimodal fusion models for VQA and VRD tasks. Our code is accessible at https://github.com/Cadene/block.bootstrap.pytorch.",1
"We provide a theoretical analysis of the representation learning problem aimed at learning the latent variables (design matrix) $\Theta$ of observations $Y$ with the knowledge of the coefficient matrix $X$. The design matrix is learned under the assumption that the latent variables $\Theta$ are smooth with respect to a (known) topological structure $\mathcal{G}$. To learn such latent variables, we study a graph Laplacian regularized estimator, which is the penalized least squares estimator with penalty term proportional to a Laplacian quadratic form. This type of estimators has recently received considerable attention due to its capability in incorporating underlying topological graph structure of variables into the learning process. While the estimation problem can be solved efficiently by state-of-the-art optimization techniques, its statistical consistency properties have been largely overlooked. In this work, we develop a non-asymptotic bound of estimation error under the classical statistical setting, where sample size is larger than the ambient dimension of the latent variables. This bound illustrates theoretically the impact of the alignment between the data and the graph structure as well as the graph spectrum on the estimation accuracy. It also provides theoretical evidence of the advantage, in terms of convergence rate, of the graph Laplacian regularized estimator over classical ones (that ignore the graph structure) in case of a smoothness prior. Finally, we provide empirical results of the estimation error to corroborate the theoretical analysis.",0
"Our focus is on the representation learning problem, which involves learning the design matrix $\Theta$ of observations $Y$ with knowledge of the coefficient matrix $X$. We assume that the latent variables $\Theta$ are smooth in relation to a known topological structure $\mathcal{G}$, and we aim to learn these variables through a graph Laplacian regularized estimator. This type of estimator has gained attention for its ability to incorporate the underlying graph structure of variables into the learning process. While this problem can be efficiently solved using optimization techniques, its statistical consistency properties have been largely overlooked. In this work, we develop a non-asymptotic bound that quantifies the estimation error under the classical statistical setting, where sample size exceeds the ambient dimension of the latent variables. This bound provides insight into the impact of data alignment and graph spectrum on estimation accuracy and supports the use of graph Laplacian regularization over classical methods. We also present empirical results to support our theoretical analysis.",1
"Network representation learning in low dimensional vector space has attracted considerable attention in both academic and industrial domains. Most real-world networks are dynamic with addition/deletion of nodes and edges. The existing graph embedding methods are designed for static networks and they cannot capture evolving patterns in a large dynamic network. In this paper, we propose a dynamic embedding method, dynnode2vec, based on the well-known graph embedding method node2vec. Node2vec is a random walk based embedding method for static networks. Applying static network embedding in dynamic settings has two crucial problems: 1) Generating random walks for every time step is time consuming 2) Embedding vector spaces in each timestamp are different. In order to tackle these challenges, dynnode2vec uses evolving random walks and initializes the current graph embedding with previous embedding vectors. We demonstrate the advantages of the proposed dynamic network embedding by conducting empirical evaluations on several large dynamic network datasets.",0
"The concept of representing networks in a low dimensional vector space has gained significant interest from academic and industrial sectors. However, most real-world networks are subject to constant changes with the addition or removal of nodes and edges. The current graph embedding methods are designed to cater to static networks and cannot capture evolving patterns in a large dynamic network. In this study, we introduce dynnode2vec, a dynamic embedding method that builds on the popular graph embedding method node2vec. Node2vec is a random walk-based embedding method for static networks, and applying it to dynamic settings presents two significant challenges: 1) Generating random walks for every time step is time-consuming, and 2) Embedding vector spaces in each timestamp are different. To overcome these issues, dynnode2vec utilizes evolving random walks and initializes the current graph embedding with previous embedding vectors. We conducted empirical evaluations on several large dynamic network datasets to demonstrate the advantages of the proposed dynamic network embedding.",1
"Nonlinear ICA is a fundamental problem for unsupervised representation learning, emphasizing the capacity to recover the underlying latent variables generating the data (i.e., identifiability). Recently, the very first identifiability proofs for nonlinear ICA have been proposed, leveraging the temporal structure of the independent components. Here, we propose a general framework for nonlinear ICA, which, as a special case, can make use of temporal structure. It is based on augmenting the data by an auxiliary variable, such as the time index, the history of the time series, or any other available information. We propose to learn nonlinear ICA by discriminating between true augmented data, or data in which the auxiliary variable has been randomized. This enables the framework to be implemented algorithmically through logistic regression, possibly in a neural network. We provide a comprehensive proof of the identifiability of the model as well as the consistency of our estimation method. The approach not only provides a general theoretical framework combining and generalizing previously proposed nonlinear ICA models and algorithms, but also brings practical advantages.",0
"Unsupervised representation learning relies on nonlinear ICA as a crucial problem, with the ability to identify the underlying latent variables that generate data being the key focus. Recently, identifiability proofs for nonlinear ICA have been developed, utilizing the independent components' temporal structure. We propose a general framework for nonlinear ICA that can take advantage of temporal structure as a special case. The framework involves augmenting the data with an auxiliary variable such as a time index or history of the time series. The proposed method learns nonlinear ICA by distinguishing between true augmented data and randomized data. This allows for algorithmic implementation through logistic regression or a neural network. We provide proof of the model's identifiability and the consistency of the estimation method. Our approach is not only a comprehensive theoretical framework but also offers practical advantages. It combines and generalizes previously proposed nonlinear ICA models and algorithms.",1
"In this work we explore a new approach for robots to teach themselves about the world simply by observing it. In particular we investigate the effectiveness of learning task-agnostic representations for continuous control tasks. We extend Time-Contrastive Networks (TCN) that learn from visual observations by embedding multiple frames jointly in the embedding space as opposed to a single frame. We show that by doing so, we are now able to encode both position and velocity attributes significantly more accurately. We test the usefulness of this self-supervised approach in a reinforcement learning setting. We show that the representations learned by agents observing themselves take random actions, or other agents perform tasks successfully, can enable the learning of continuous control policies using algorithms like Proximal Policy Optimization (PPO) using only the learned embeddings as input. We also demonstrate significant improvements on the real-world Pouring dataset with a relative error reduction of 39.4% for motion attributes and 11.1% for static attributes compared to the single-frame baseline. Video results are available at https://sites.google.com/view/actionablerepresentations .",0
"The aim of this study is to introduce a novel method for robots to gain knowledge about their surroundings through observation. The focus is on examining how learning task-agnostic representations for continuous control tasks can be effective. The study builds on Time-Contrastive Networks (TCN) which are capable of learning from visual observations by embedding multiple frames together in the embedding space rather than a single frame. By doing this, the researchers were able to encode position and velocity attributes more accurately. The efficacy of this self-supervised technique was tested in a reinforcement learning environment and it was found that the learned representations could be used to train continuous control policies using algorithms such as Proximal Policy Optimization (PPO). The results showed a significant improvement in the real-world Pouring dataset, with a relative error reduction of 39.4% for motion attributes and 11.1% for static attributes compared to the single-frame baseline. Video demonstrations of the findings can be found at https://sites.google.com/view/actionablerepresentations.",1
"Representation learning is a central challenge across a range of machine learning areas. In reinforcement learning, effective and functional representations have the potential to tremendously accelerate learning progress and solve more challenging problems. Most prior work on representation learning has focused on generative approaches, learning representations that capture all underlying factors of variation in the observation space in a more disentangled or well-ordered manner. In this paper, we instead aim to learn functionally salient representations: representations that are not necessarily complete in terms of capturing all factors of variation in the observation space, but rather aim to capture those factors of variation that are important for decision making -- that are ""actionable."" These representations are aware of the dynamics of the environment, and capture only the elements of the observation that are necessary for decision making rather than all factors of variation, without explicit reconstruction of the observation. We show how these representations can be useful to improve exploration for sparse reward problems, to enable long horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. We evaluate our method on a number of simulated environments, and compare it to prior methods for representation learning, exploration, and hierarchical reinforcement learning.",0
"Across various machine learning domains, representation learning poses a significant challenge. In reinforcement learning, acquiring effective and functional representations can immensely expedite the learning process and offer solutions to more intricate problems. While the majority of previous work on representation learning has concentrated on generative approaches, where the focus is on creating representations that capture all the underlying factors of variation in the observation domain in a more disentangled or well-ordered manner, our paper's objective is to learn functionally salient representations. These representations may not necessarily be complete in capturing all factors of variation in the observation domain. Instead, they aim to capture the factors that are crucial for decision-making, or ""actionable."" These representations are cognizant of the environment's dynamics and capture only the observation elements necessary for decision-making, rather than all factors of variation without the explicit reconstruction of the observation. We demonstrate how these representations can be beneficial in enhancing exploration for sparse reward problems, enabling long-horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. We evaluate our method on multiple simulated environments and compare it to previous techniques for representation learning, exploration, and hierarchical reinforcement learning.",1
"Modeling the underlying person structure for person re-identification (re-ID) is difficult due to diverse deformable poses, changeable camera views and imperfect person detectors. How to exploit underlying person structure information without extra annotations to improve the performance of person re-ID remains largely unexplored. To address this problem, we propose a novel Relative Local Distance (RLD) method that integrates a relative local distance constraint into convolutional neural networks (CNNs) in an end-to-end way. It is the first time that the relative local constraint is proposed to guide the global feature representation learning. Specially, a relative local distance matrix is computed by using feature maps and then regarded as a regularizer to guide CNNs to learn a structure-aware feature representation. With the discovered underlying person structure, the RLD method builds a bridge between the global and local feature representation and thus improves the capacity of feature representation for person re-ID. Furthermore, RLD also significantly accelerates deep network training compared with conventional methods. The experimental results show the effectiveness of RLD on the CUHK03, Market-1501, and DukeMTMC-reID datasets. Code is available at \url{https://github.com/Wanggcong/RLD_codes}.",0
"Person re-identification (re-ID) is a challenging task due to the variability in poses, camera perspectives, and the accuracy of person detectors. Despite the lack of additional annotations, there is a need to leverage the underlying person structure to enhance the performance of person re-ID. In response to this challenge, we propose a new method called Relative Local Distance (RLD) that integrates a relative local distance constraint into convolutional neural networks (CNNs) in an end-to-end manner. Our approach is the first to utilize the relative local constraint to guide global feature representation learning. Specifically, we compute a relative local distance matrix using feature maps, which serves as a regularizer to guide CNNs in learning structure-aware feature representation. By discovering the underlying person structure, RLD establishes a connection between global and local feature representation, thus enhancing the feature representation capacity for person re-ID. Additionally, our method significantly speeds up deep network training compared to conventional approaches. Our experimental results demonstrate the effectiveness of RLD on three datasets: CUHK03, Market-1501, and DukeMTMC-reID. The code is available at \url{https://github.com/Wanggcong/RLD_codes}.",1
"One of the key challenges of performing label prediction over a data stream concerns with the emergence of instances belonging to unobserved class labels over time. Previously, this problem has been addressed by detecting such instances and using them for appropriate classifier adaptation. The fundamental aspect of a novel-class detection strategy relies on the ability of comparison among observed instances to discriminate them into known and unknown classes. Therefore, studies in the past have proposed various metrics suitable for comparison over the observed feature space. Unfortunately, these similarity measures fail to reliably identify distinct regions in observed feature spaces useful for class discrimination and novel-class detection, especially in streams containing high-dimensional data instances such as images and texts. In this paper, we address this key challenge by proposing a semi-supervised multi-task learning framework called \sysname{} which aims to intrinsically search for a latent space suitable for detecting labels of instances from both known and unknown classes. We empirically measure the performance of \sysname{} over multiple real-world image and text datasets and demonstrate its superiority by comparing its performance with existing semi-supervised methods.",0
"A major obstacle in predicting labels for data streams is the appearance of instances belonging to unobserved class labels over time. In the past, this dilemma has been resolved by identifying such instances and adapting suitable classifiers. Detecting unknown classes entails comparing observed instances to distinguish between known and unknown classes. Hence, previous research has proposed several metrics for comparison in the observed feature space. However, similarity measures are inadequate in identifying unique areas in observed feature spaces that aid in class discrimination and novel-class detection, particularly in streams with high-dimensional data instances such as images and texts. This study proposes a semi-supervised multi-task learning framework called \sysname{}, which seeks to intrinsically search for a latent space that is appropriate for detecting labels of instances from both known and unknown classes. The performance of \sysname{} is measured empirically across multiple real-world image and text datasets, and its superiority is demonstrated by comparing it with existing semi-supervised methods.",1
"Unsupervised visual representation learning remains a largely unsolved problem in computer vision research. Among a big body of recently proposed approaches for unsupervised learning of visual representations, a class of self-supervised techniques achieves superior performance on many challenging benchmarks. A large number of the pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of convolutional neural networks (CNN), has not received equal attention. Therefore, we revisit numerous previously proposed self-supervised models, conduct a thorough large scale study and, as a result, uncover multiple crucial insights. We challenge a number of common practices in selfsupervised visual representation learning and observe that standard recipes for CNN design do not always translate to self-supervised representation learning. As part of our study, we drastically boost the performance of previously proposed techniques and outperform previously published state-of-the-art results by a large margin.",0
"The problem of unsupervised visual representation learning is still not fully solved in computer vision research. Although many approaches have been proposed for unsupervised learning of visual representations, a specific type of self-supervised techniques has shown excellent performance on challenging benchmarks. While many pretext tasks for self-supervised learning have been studied, the choice of convolutional neural networks (CNN) has not received adequate attention. Thus, we conduct a comprehensive large-scale study on previously proposed self-supervised models and uncover several essential insights. Our study challenges some common practices in self-supervised visual representation learning, and we observe that standard CNN design recipes do not always work for self-supervised representation learning. Our research significantly enhances the performance of previously proposed techniques and outperforms previously published state-of-the-art results by a substantial margin.",1
"The remarkable success of machine learning, especially deep learning, has produced a variety of cloud-based services for mobile users. Such services require an end user to send data to the service provider, which presents a serious challenge to end-user privacy. To address this concern, prior works either add noise to the data or send features extracted from the raw data. They struggle to balance between the utility and privacy because added noise reduces utility and raw data can be reconstructed from extracted features. This work represents a methodical departure from prior works: we balance between a measure of privacy and another of utility by leveraging adversarial learning to find a sweeter tradeoff. We design an encoder that optimizes against the reconstruction error (a measure of privacy), adversarially by a Decoder, and the inference accuracy (a measure of utility) by a Classifier. The result is RAN, a novel deep model with a new training algorithm that automatically extracts features for classification that are both private and useful. It turns out that adversarially forcing the extracted features to only conveys the intended information required by classification leads to an implicit regularization leading to better classification accuracy than the original model which completely ignores privacy. Thus, we achieve better privacy with better utility, a surprising possibility in machine learning! We conducted extensive experiments on five popular datasets over four training schemes, and demonstrate the superiority of RAN compared with existing alternatives.",0
"The success of machine learning, particularly deep learning, has led to the development of cloud-based services for mobile users. However, these services require users to send data to the provider, which poses a challenge to privacy. Previous attempts to address this issue involved adding noise to the data or sending extracted features. However, these methods struggle to balance privacy and utility. This study takes a different approach by using adversarial learning to find a better balance between privacy and utility. The researchers created RAN, a new deep model with a training algorithm that optimizes against reconstruction error and inference accuracy. Using adversarial learning to extract features for classification leads to better accuracy and privacy. The researchers conducted experiments on five datasets and found that RAN outperformed existing alternatives.",1
"While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",0
"Although supervised learning has made significant strides in numerous applications, unsupervised learning has not gained as much popularity and remains a challenging aspect of artificial intelligence. Our research introduces a universal unsupervised learning technique called Contrastive Predictive Coding to derive valuable representations from high-dimensional data. Our model employs powerful autoregressive models to predict the future in latent space, and we utilize a probabilistic contrastive loss to ensure that the latent space captures information that is highly useful in predicting future samples. We also make the model more accessible by employing negative sampling. While previous studies have concentrated on evaluating representations for a specific modality, our approach demonstrates its ability to learn valuable representations that achieve impressive results in four diverse domains: speech, images, text, and reinforcement learning in 3D environments.",1
"An important part of many machine learning workflows on graphs is vertex representation learning, i.e., learning a low-dimensional vector representation for each vertex in the graph. Recently, several powerful techniques for unsupervised representation learning have been demonstrated to give the state-of-the-art performance in downstream tasks such as vertex classification and edge prediction. These techniques rely on random walks performed on the graph in order to capture its structural properties. These structural properties are then encoded in the vector representation space.   However, most contemporary representation learning methods only apply to static graphs while real-world graphs are often dynamic and change over time. Static representation learning methods are not able to update the vector representations when the graph changes; therefore, they must re-generate the vector representations on an updated static snapshot of the graph regardless of the extent of the change in the graph. In this work, we propose computationally efficient algorithms for vertex representation learning that extend random walk based methods to dynamic graphs. The computation complexity of our algorithms depends upon the extent and rate of changes (the number of edges changed per update) and on the density of the graph. We empirically evaluate our algorithms on real world datasets for downstream machine learning tasks of multi-class and multi-label vertex classification. The results show that our algorithms can achieve competitive results to the state-of-the-art methods while being computationally efficient.",0
"Vertex representation learning is a crucial component of machine learning workflows on graphs. It involves learning a low-dimensional vector representation for each vertex in the graph. Several unsupervised representation learning techniques have been developed that rely on random walks to capture the structural properties of the graph and encode them in the vector representation space. However, most existing methods are designed for static graphs and cannot update the vector representations when the graph changes. This means that they must generate new vector representations every time the graph changes, regardless of the extent of the change. To address this limitation, we propose computationally efficient algorithms for vertex representation learning that can handle dynamic graphs. The complexity of our algorithms depends on the extent and rate of changes and the density of the graph. We evaluate our algorithms on real-world data sets for downstream machine learning tasks and show that they can achieve competitive results with state-of-the-art methods while being computationally efficient.",1
"Despite significant advances in clustering methods in recent years, the outcome of clustering of a natural image dataset is still unsatisfactory due to two important drawbacks. Firstly, clustering of images needs a good feature representation of an image and secondly, we need a robust method which can discriminate these features for making them belonging to different clusters such that intra-class variance is less and inter-class variance is high. Often these two aspects are dealt with independently and thus the features are not sufficient enough to partition the data meaningfully. In this paper, we propose a method where we discover these features required for the separation of the images using deep autoencoder. Our method learns the image representation features automatically for the purpose of clustering and also select a coherent image and an incoherent image simultaneously for a given image so that the feature representation learning can learn better discriminative features for grouping the similar images in a cluster and at the same time separating the dissimilar images across clusters. Experiment results show that our method produces significantly better result than the state-of-the-art methods and we also show that our method is more generalized across different dataset without using any pre-trained model like other existing methods.",0
"Although clustering methods have made significant progress in recent years, clustering natural image datasets remains unsatisfactory due to two major drawbacks. Firstly, good feature representation is required for image clustering, and secondly, a robust method is needed to distinguish these features in order to classify them into different clusters with low intra-class variance and high inter-class variance. These two aspects are often dealt with separately, which means the features are not adequate enough to partition the data meaningfully. This study proposes a method that utilizes deep autoencoder to discover the required features for image separation. Our method automatically learns image representation features for clustering and simultaneously selects a coherent and an incoherent image for a given image to improve the discriminative features that group similar images in a cluster while separating dissimilar images across clusters. Our experimental results demonstrate that our approach outperforms existing state-of-the-art methods and is more generalized across different datasets without using pre-trained models.",1
"Syndrome differentiation in Traditional Chinese Medicine (TCM) is the process of understanding and reasoning body condition, which is the essential step and premise of effective treatments. However, due to its complexity and lack of standardization, it is challenging to achieve. In this study, we consider each patient's record as a one-dimensional image and symptoms as pixels, in which missing and negative values are represented by zero pixels. The objective is to find relevant symptoms first and then map them to proper syndromes, that is similar to the object detection problem in computer vision. Inspired from it, we employ multi-instance multi-task learning combined with the convolutional neural network (MIMT-CNN) for syndrome differentiation, which takes region proposals as input and output image labels directly. The neural network consists of region proposals generation, convolutional layer, fully connected layer, and max pooling (multi-instance pooling) layer followed by the sigmoid function in each syndrome prediction task for image representation learning and final results generation. On the diabetes dataset, it performs better than all other baseline methods. Moreover, it shows stability and reliability to generate results, even on the dataset with small sample size, a large number of missing values and noises.",0
"Syndrome differentiation is a crucial step in TCM for effective treatments, but its complexity and lack of standardization make it challenging. To address this issue, we treat each patient's record as a one-dimensional image and symptoms as pixels, where missing and negative values are zero pixels. Our objective is to identify relevant symptoms and map them to proper syndromes, similar to object detection in computer vision. Using MIMT-CNN, we employ multi-instance multi-task learning to differentiate syndromes based on region proposals. Our neural network includes region proposals generation, convolutional layer, fully connected layer, max pooling layer, and sigmoid function. Our method outperforms all other baseline methods on the diabetes dataset and is robust to small sample sizes, missing values, and noise.",1
"In this work, we study value function approximation in reinforcement learning (RL) problems with high dimensional state or action spaces via a generalized version of representation policy iteration (RPI). We consider the limitations of proto-value functions (PVFs) at accurately approximating the value function in low dimensions and we highlight the importance of features learning for an improved low-dimensional value function approximation. Then, we adopt different representation learning algorithm on graphs to learn the basis functions that best represent the value function. We empirically show that node2vec, an algorithm for scalable feature learning in networks, and the Variational Graph Auto-Encoder constantly outperform the commonly used smooth proto-value functions in low-dimensional feature space.",0
"The focus of our research is on value function approximation in reinforcement learning (RL) problems where the state or action spaces are of high dimensions. We use a generalized version of representation policy iteration (RPI) to achieve this. We recognize the limitations of proto-value functions (PVFs) in accurately approximating the value function in low dimensions and highlight the significance of feature learning for an improved low-dimensional value function approximation. To this end, we apply various representation learning algorithms on graphs to learn the basis functions that can best represent the value function. Our empirical findings demonstrate that node2vec, a scalable feature learning algorithm in networks, and the Variational Graph Auto-Encoder consistently outperform the commonly used smooth proto-value functions in low-dimensional feature space.",1
"Representation learning becomes especially important for complex systems with multimodal data sources such as cameras or sensors. Recent advances in reinforcement learning and optimal control make it possible to design control algorithms on these latent representations, but the field still lacks a large-scale standard dataset for unified comparison. In this work, we present a large-scale dataset and evaluation framework for representation learning for the complex task of landing an airplane. We implement and compare several approaches to representation learning on this dataset in terms of the quality of simple supervised learning tasks and disentanglement scores. The resulting representations can be used for further tasks such as anomaly detection, optimal control, model-based reinforcement learning, and other applications.",0
"When dealing with complex systems that have multiple sources of data like cameras or sensors, representation learning is crucial. While advances in reinforcement learning and optimal control have made it possible to create control algorithms using these learned representations, the absence of a large-scale standard dataset for comparison hinders the field's progress. This study aims to fill this gap by presenting a large-scale dataset and evaluation framework for representation learning in the challenging task of landing an airplane. Various approaches to representation learning are compared based on their performance on simple supervised learning tasks and disentanglement scores. The resulting representations can be applied to other tasks like anomaly detection, optimal control, and model-based reinforcement learning.",1
"In general, recommendation can be viewed as a matching problem, i.e., match proper items for proper users. However, due to the huge semantic gap between users and items, it's almost impossible to directly match users and items in their initial representation spaces. To solve this problem, many methods have been studied, which can be generally categorized into two types, i.e., representation learning-based CF methods and matching function learning-based CF methods. Representation learning-based CF methods try to map users and items into a common representation space. In this case, the higher similarity between a user and an item in that space implies they match better. Matching function learning-based CF methods try to directly learn the complex matching function that maps user-item pairs to matching scores. Although both methods are well developed, they suffer from two fundamental flaws, i.e., the limited expressiveness of dot product and the weakness in capturing low-rank relations respectively. To this end, we propose a general framework named DeepCF, short for Deep Collaborative Filtering, to combine the strengths of the two types of methods and overcome such flaws. Extensive experiments on four publicly available datasets demonstrate the effectiveness of the proposed DeepCF framework.",0
"Recommendation is typically seen as a matching problem where proper items are matched with proper users. However, the large semantic gap between users and items makes it nearly impossible to match them directly in their initial representation spaces. To address this issue, two main types of methods have been studied: representation learning-based CF methods and matching function learning-based CF methods. Representation learning-based CF methods aim to map users and items into a common representation space, where higher similarity between a user and an item implies a better match. In contrast, matching function learning-based CF methods attempt to learn the complex matching function that maps user-item pairs to matching scores. Despite their development, both methods suffer from the limited expressiveness of dot product and the weakness in capturing low-rank relations. Therefore, we introduce a general framework called DeepCF (short for Deep Collaborative Filtering) that combines the strengths of both methods and overcomes these flaws. Through extensive experiments on four publicly available datasets, we demonstrate the effectiveness of the proposed DeepCF framework.",1
"We propose a one-class neural network (OC-NN) model to detect anomalies in complex data sets. OC-NN combines the ability of deep networks to extract a progressively rich representation of data with the one-class objective of creating a tight envelope around normal data. The OC-NN approach breaks new ground for the following crucial reason: data representation in the hidden layer is driven by the OC-NN objective and is thus customized for anomaly detection. This is a departure from other approaches which use a hybrid approach of learning deep features using an autoencoder and then feeding the features into a separate anomaly detection method like one-class SVM (OC-SVM). The hybrid OC-SVM approach is sub-optimal because it is unable to influence representational learning in the hidden layers. A comprehensive set of experiments demonstrate that on complex data sets (like CIFAR and GTSRB), OC-NN performs on par with state-of-the-art methods and outperformed conventional shallow methods in some scenarios.",0
"To detect anomalies in complex data sets, we propose using a one-class neural network (OC-NN) model. OC-NN utilizes deep networks to progressively extract rich data representations and the one-class objective to create a tight envelope around normal data. Unlike other approaches that use a hybrid method, which learns deep features using an autoencoder and then applies a separate anomaly detection method like one-class SVM (OC-SVM), OC-NN breaks new ground by customizing data representation in the hidden layer for anomaly detection. The hybrid OC-SVM approach is sub-optimal because it does not impact representational learning in the hidden layers. We conducted a comprehensive set of experiments to demonstrate that OC-NN performs comparably to state-of-the-art methods and outperforms conventional shallow methods in some scenarios on complex data sets such as CIFAR and GTSRB.",1
"Due to the phenomenon of ""posterior collapse,"" current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires augmenting the objective so it does not only maximize the likelihood of the data. In this paper, we propose an alternative that utilizes the most powerful generative models as decoders, whilst optimising the variational lower bound all while ensuring that the latent variables preserve and encode useful information. Our proposed $\delta$-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate the efficacy of our approach at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet $32\times 32$.",0
"Current latent variable generative models face a challenging design choice due to ""posterior collapse,"" which can result in a weakened decoder capacity or the need to augment the objective beyond maximizing data likelihood. Our paper offers an alternative approach that utilizes powerful generative models as decoders, optimizes the variational lower bound, and ensures that latent variables encode useful information. We achieve this through our proposed $\delta$-VAEs, which constrain the minimum distance between the prior and the posterior's variational family. This approach resembles the classic representation learning technique of slow feature analysis for sequential latent variable models. Our results demonstrate the effectiveness of our approach in modeling text on LM1B and images, improving sample quality, and achieving state-of-the-art log-likelihood on CIFAR-10 and ImageNet $32\times 32$.",1
"The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs.",0
"The variational autoencoder (VAE) is renowned for its ability to estimate density and learn representation. Traditionally, the variational principle advises using an expressive inference model to ensure that the variational approximation is precise. Nevertheless, it is often disregarded that an excessively expressive inference model can be harmful to the performance of both the amortized posterior approximator and the generative density estimator on the test set. This paper proposes techniques for amortized inference regularization (AIR) that can regulate the smoothness of the inference model, leveraging the fact that VAEs rely on amortized inference. By implementing AIR, we can enhance VAE generalization on both inference and generative performance. Our research contradicts the notion that amortized inference is a mere device for approximating maximum likelihood training and demonstrates that amortization family regularization offers a new approach to comprehend and enhance generalization in VAEs.",1
"Transfer learning can address the learning tasks of unlabeled data in the target domain by leveraging plenty of labeled data from a different but related source domain. A core issue in transfer learning is to learn a shared feature space in where the distributions of the data from two domains are matched. This learning process can be named as transfer representation learning (TRL). The feature transformation methods are crucial to ensure the success of TRL. The most commonly used feature transformation method in TRL is kernel-based nonlinear mapping to the high-dimensional space followed by linear dimensionality reduction. But the kernel functions are lack of interpretability and are difficult to be selected. To this end, the TSK fuzzy system (TSK-FS) is combined with transfer learning and a more intuitive and interpretable modeling method, called transfer representation learning with TSK-FS (TRL-TSK-FS) is proposed in this paper. Specifically, TRL-TSK-FS realizes TRL from two aspects. On one hand, the data in the source and target domains are transformed into the fuzzy feature space in which the distribution distance of the data between two domains is min-imized. On the other hand, discriminant information and geo-metric properties of the data are preserved by linear discriminant analysis and principal component analysis. In addition, another advantage arises with the proposed method, that is, the nonlinear transformation is realized by constructing fuzzy mapping with the antecedent part of the TSK-FS instead of kernel functions which are difficult to be selected. Extensive experiments are conducted on the text and image datasets. The results obviously show the superiority of the proposed method.",0
"Transfer learning can utilize labeled data from a related source domain to address learning tasks of unlabeled data in the target domain. A crucial aspect of transfer learning is to establish a shared feature space that aligns with the distributions of the data from both domains. This process is known as transfer representation learning (TRL) and requires reliable feature transformation methods for success. While kernel-based nonlinear mapping followed by linear dimensionality reduction is commonly used, kernel functions lack interpretability and are challenging to select. This paper proposes a more intuitive and interpretable modeling method, called transfer representation learning with TSK-FS (TRL-TSK-FS), by combining TSK fuzzy systems (TSK-FS) with transfer learning. TRL-TSK-FS achieves TRL through fuzzy feature space transformation that minimizes distribution distance between domains and preserves discriminant information and geometric properties of the data through linear discriminant analysis and principal component analysis. The proposed method also provides the advantage of constructing fuzzy mapping with the antecedent part of the TSK-FS instead of kernel functions. The effectiveness of the proposed method is demonstrated through extensive experiments on text and image datasets.",1
"Multiview representation learning is very popular for latent factor analysis. It naturally arises in many data analysis, machine learning, and information retrieval applications to model dependent structures among multiple data sources. For computational convenience, existing approaches usually formulate the multiview representation learning as convex optimization problems, where global optima can be obtained by certain algorithms in polynomial time. However, many pieces of evidence have corroborated that heuristic nonconvex approaches also have good empirical computational performance and convergence to the global optima, although there is a lack of theoretical justification. Such a gap between theory and practice motivates us to study a nonconvex formulation for multiview representation learning, which can be efficiently solved by a simple stochastic gradient descent (SGD) algorithm. We first illustrate the geometry of the nonconvex formulation; Then, we establish asymptotic global rates of convergence to the global optima by diffusion approximations. Numerical experiments are provided to support our theory.",0
"Multiview representation learning is a popular method for analyzing latent factors. It is commonly used in data analysis, machine learning, and information retrieval applications to model relationships among multiple data sources. To simplify the computation process, previous approaches have typically formulated multiview representation learning as convex optimization problems, which can be solved using algorithms that find global optima in polynomial time. However, empirical evidence shows that nonconvex approaches, while lacking theoretical justification, can still perform well and converge to global optima. This gap between theory and practice has inspired us to develop a nonconvex formulation for multiview representation learning that can be solved using a simple stochastic gradient descent (SGD) algorithm. We begin by examining the geometry of the nonconvex formulation and then establish asymptotic global rates of convergence to the global optima using diffusion approximations. We also provide numerical experiments to support our theory.",1
"In this work we present a method to improve the pruning step of the current state-of-the-art methodology to compress neural networks. The novelty of the proposed pruning technique is in its differentiability, which allows pruning to be performed during the backpropagation phase of the network training. This enables an end-to-end learning and strongly reduces the training time. The technique is based on a family of differentiable pruning functions and a new regularizer specifically designed to enforce pruning. The experimental results show that the joint optimization of both the thresholds and the network weights permits to reach a higher compression rate, reducing the number of weights of the pruned network by a further 14% to 33% compared to the current state-of-the-art. Furthermore, we believe that this is the first study where the generalization capabilities in transfer learning tasks of the features extracted by a pruned network are analyzed. To achieve this goal, we show that the representations learned using the proposed pruning methodology maintain the same effectiveness and generality of those learned by the corresponding non-compressed network on a set of different recognition tasks.",0
"Our work introduces an innovative approach to enhance the pruning phase of the most advanced technique for compressing neural networks. The distinctiveness of our proposed method lies in its differentiability, enabling pruning to take place during the network training's backpropagation phase. This leads to an end-to-end learning process and significantly reduces the training duration. The method uses a group of differentiable pruning functions and a novel regularizer that enforces pruning. The outcomes of our experiments demonstrate that optimizing both the network weights and thresholds results in a higher compression rate, reducing the number of weights of the pruned network by a further 14% to 33% compared with the current state-of-the-art. Additionally, our study is the first to analyze the generalization capabilities of the features extracted by a pruned network in transfer learning tasks. We show that the representations acquired through our pruning technique maintain the same effectiveness and generality as those learned by the non-compressed network on various recognition tasks.",1
"Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation. Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.",0
"The Variational Autoencoder (VAE) is a deep generative model that is both simple and effective. It has been successful in many empirical applications and has led to the development of numerous advanced variants and theoretical investigations. However, recent studies have shown that when VAE is equipped with expressive generative distributions (also known as decoders), it may learn uninformative latent representations that result in a phenomenon called KL Varnishing. This causes VAE to collapse into an unconditional generative model. To address this issue, we propose a novel regularization called mutual posterior-divergence regularization. This regularization controls the geometry of the latent space, allowing for meaningful representation learning while achieving comparable or superior density estimation capability. Our experiments on three image benchmark datasets demonstrate that our model performs well on both density estimation and representation learning when equipped with powerful decoders.",1
"Hashing has been recognized as an efficient representation learning method to effectively handle big data due to its low computational complexity and memory cost. Most of the existing hashing methods focus on learning the low-dimensional vectorized binary features based on the high-dimensional raw vectorized features. However, studies on how to obtain preferable binary codes from the original 2D image features for retrieval is very limited. This paper proposes a bilinear supervised discrete hashing (BSDH) method based on 2D image features which utilizes bilinear projections to binarize the image matrix features such that the intrinsic characteristics in the 2D image space are preserved in the learned binary codes. Meanwhile, the bilinear projection approximation and vectorization binary codes regression are seamlessly integrated together to formulate the final robust learning framework. Furthermore, a discrete optimization strategy is developed to alternatively update each variable for obtaining the high-quality binary codes. In addition, two 2D image features, traditional SURF-based FVLAD feature and CNN-based AlexConv5 feature are designed for further improving the performance of the proposed BSDH method. Results of extensive experiments conducted on four benchmark datasets show that the proposed BSDH method almost outperforms all competing hashing methods with different input features by different evaluation protocols.",0
"Due to its low computational complexity and memory cost, hashing is an efficient representation learning method that has been acknowledged for its ability to handle big data. Although most existing hashing methods focus on learning low-dimensional vectorized binary features from high-dimensional raw vectorized features, there is limited research on obtaining optimal binary codes from original 2D image features for retrieval. This study presents a bilinear supervised discrete hashing (BSDH) method that utilizes bilinear projections to binarize image matrix features, preserving intrinsic characteristics in the 2D image space in the learned binary codes. The bilinear projection approximation and vectorization binary codes regression are seamlessly integrated into the final robust learning framework. Moreover, a discrete optimization strategy is developed to update each variable alternately to obtain high-quality binary codes. Additionally, two 2D image features, traditional SURF-based FVLAD feature and CNN-based AlexConv5 feature, are designed to further enhance the performance of the proposed BSDH method. Extensive experiments conducted on four benchmark datasets reveal that the proposed BSDH method outperforms all competing hashing methods, with different input features evaluated by various protocols.",1
"High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time. To address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space. This model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty. We evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.",0
"In many fields, high-dimensional time series data is prevalent. However, human cognition is not well-suited to working in such spaces, so there is a need for simplified, easy-to-understand representations. Unfortunately, most representation learning algorithms for time series data are complex and difficult to interpret. This is mainly due to non-linear mappings from data features to important aspects of the representation and non-smoothness over time. To address this issue, we propose a new representation learning framework that combines ideas from interpretable discrete dimensionality reduction and deep generative modeling. With this framework, we can learn discrete representations of time series that lead to smooth, interpretable embeddings with excellent clustering performance. We have developed a new approach to overcome the non-differentiability problem in discrete representation learning, and we present a gradient-based version of the traditional self-organizing map algorithm that outperforms the original. Additionally, we integrate a Markov model into the representation space to allow for a probabilistic interpretation of our approach. This model improves clustering performance even further and provides insights into the temporal transition structure, as well as a natural representation of uncertainty. We evaluate our model on several data sets and find that our learned representations are superior to competitor methods and facilitate downstream tasks on real-world data.",1
"Learning informative representations of data is one of the primary goals of deep learning, but there is still little understanding as to what representations a neural network actually learns. To better understand this, subspace match was recently proposed as a method for assessing the similarity of the representations learned by neural networks. It has been shown that two networks with the same architecture trained from different initializations learn representations that at hidden layers show low similarity when assessed with subspace match, even when the output layers show high similarity and the networks largely exhibit similar performance on classification tasks. In this note, we present a simple example motivated by standard results in commutative algebra to illustrate how this can happen, and show that although the subspace match at a hidden layer may be 0, the representations learned may be isomorphic as vector spaces. This leads us to conclude that a subspace match comparison of learned representations may well be uninformative, and it points to the need for better methods of understanding learned representations.",0
"The primary objective of deep learning is to acquire informative data representations, yet there is limited comprehension regarding the representations neural networks actually learn. To address this issue, a method called subspace match was recently introduced to evaluate the similarity of neural network representations. Studies have revealed that two networks with the same structure and diverse initializations display low similarity in their learned representations at hidden layers, even though their output layers exhibit high similarity, and both networks perform comparably in classification tasks. In this article, we present a simple example motivated by standard commutative algebra results to demonstrate how this can occur. We show that despite a subspace match of 0 in a hidden layer, the learned representations may be isomorphic as vector spaces. As a result, we conclude that comparing learned representations using subspace match may not be useful, and there is a need for better approaches to comprehend learned representations.",1
"We present a representation learning algorithm that learns a low-dimensional latent dynamical system from high-dimensional \textit{sequential} raw data, e.g., video. The framework builds upon recent advances in amortized inference methods that use both an inference network and a refinement procedure to output samples from a variational distribution given an observation sequence, and takes advantage of the duality between control and inference to approximately solve the intractable inference problem using the path integral control approach. The learned dynamical model can be used to predict and plan the future states; we also present the efficient planning method that exploits the learned low-dimensional latent dynamics. Numerical experiments show that the proposed path-integral control based variational inference method leads to tighter lower bounds in statistical model learning of sequential data. The supplementary video: https://youtu.be/xCp35crUoLQ",0
"Our approach involves an algorithm for representation learning that enables the discovery of a low-dimensional latent dynamical system from high-dimensional sequential raw data, such as video. This technique leverages recent developments in amortized inference methods, which employ an inference network and refinement procedure to generate samples from a variational distribution based on an observation sequence. By applying the path integral control approach, which involves exploiting the relationship between control and inference, we can solve the intractable inference problem with a reasonable degree of accuracy. The resulting dynamical model can be used to predict and plan future states, and we have developed an efficient planning method that takes advantage of the low-dimensional latent dynamics. Our numerical experiments indicate that the proposed path-integral control based variational inference method produces tighter lower bounds when learning statistical models for sequential data. For additional information, please refer to our supplementary video at https://youtu.be/xCp35crUoLQ.",1
"Convolution is an efficient technique to obtain abstract feature representations using hierarchical layers in deep networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere ($\mathbb{S}^2$) or a unit ball ($\mathbb{B}^3$)---entails unique challenges. In this work, we propose a novel `\emph{volumetric convolution}' operation that can effectively convolve arbitrary functions in $\mathbb{B}^3$. We develop a theoretical framework for \emph{volumetric convolution} based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer for deep networks. Furthermore, our formulation leads to derivation of a novel formula to measure the symmetry of a function in $\mathbb{B}^3$ around an arbitrary axis, that is useful in 3D shape analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on a possible use-case i.e., 3D object recognition task.",0
"Using hierarchical layers in deep networks, convolution is an effective technique to obtain abstract feature representations. However, convolving in topological spaces other than Euclidean geometries, such as a sphere ($\mathbb{S}^2$) or a unit ball ($\mathbb{B}^3$), presents unique challenges. We have introduced a new operation, called `\emph{volumetric convolution}', which can convolve arbitrary functions in $\mathbb{B}^3$ efficiently. Our theoretical framework is based on Zernike polynomials, and we have implemented it as a differentiable and easily pluggable layer for deep networks. Additionally, our formulation has led to the derivation of a novel formula to measure the symmetry of a function in $\mathbb{B}^3$ around an arbitrary axis, which is useful in 3D shape analysis tasks. We have demonstrated the effectiveness of our proposed volumetric convolution operation on a 3D object recognition task.",1
"When doing representation learning on data that lives on a known non-trivial manifold embedded in high dimensional space, it is natural to desire the encoder to be homeomorphic when restricted to the manifold, so that it is bijective and continuous with a continuous inverse. Using topological arguments, we show that when the manifold is non-trivial, the encoder must be globally discontinuous and propose a universal, albeit impractical, construction. In addition, we derive necessary constraints which need to be satisfied when designing manifold-specific practical encoders. These are used to analyse candidates for a homeomorphic encoder for the manifold of 3D rotations $SO(3)$.",0
"When performing representation learning on data residing on a known complex manifold within a high-dimensional space, it is natural to aim for the encoder to be homeomorphic within the constraints of the manifold. This would ensure that it is both continuous with a continuous inverse and bijective. However, using topological arguments, we demonstrate that when the manifold is complex, a globally discontinuous encoder is necessary, and we propose a universal construction that is not practical. To address this, we establish essential requirements for designing practical encoders that are specific to the manifold. We apply these constraints to evaluate potential options for a homeomorphic encoder for the manifold of 3D rotations, $SO(3)$.",1
"Recently, the topic of graph representation learning has received plenty of attention. Existing approaches usually focus on structural properties only and thus they are not sufficient for those spatial graphs where the nodes are associated with some spatial information. In this paper, we present the first deep learning approach called s2vec for learning spatial graph representations, which is based on denoising autoencoders framework (DAF). We evaluate the learned representations on real datasets and the results verified the effectiveness of s2vec when used for spatial clustering.",0
"In recent times, there has been a significant focus on graph representation learning. However, the existing methods primarily concentrate on structural properties and do not adequately cater to spatial graphs where nodes are linked to spatial information. This study introduces a novel deep learning approach named s2vec, which utilizes the denoising autoencoders framework (DAF) to learn spatial graph representations. The effectiveness of s2vec is verified through evaluations on real datasets, which demonstrate its potential for spatial clustering.",1
"The standard loss function used to train neural network classifiers, categorical cross-entropy (CCE), seeks to maximize accuracy on the training data; building useful representations is not a necessary byproduct of this objective. In this work, we propose clustering-oriented representation learning (COREL) as an alternative to CCE in the context of a generalized attractive-repulsive loss framework. COREL has the consequence of building latent representations that collectively exhibit the quality of natural clustering within the latent space of the final hidden layer, according to a predefined similarity function. Despite being simple to implement, COREL variants outperform or perform equivalently to CCE in a variety of scenarios, including image and news article classification using both feed-forward and convolutional neural networks. Analysis of the latent spaces created with different similarity functions facilitates insights on the different use cases COREL variants can satisfy, where the Cosine-COREL variant makes a consistently clusterable latent space, while Gaussian-COREL consistently obtains better classification accuracy than CCE.",0
"The usual loss function for training neural network classifiers, namely categorical cross-entropy (CCE), focuses on maximizing accuracy on the training data and does not necessarily lead to the development of useful representations. To address this issue, this study introduces clustering-oriented representation learning (COREL) as an alternative to CCE within a general attractive-repulsive loss framework. COREL is designed to create latent representations that exhibit natural clustering within the latent space of the final hidden layer, based on a predefined similarity function. Although COREL is easy to implement, its variants outperform or perform similarly to CCE in various scenarios involving feed-forward and convolutional neural networks for image and news article classification. The analysis of the latent spaces generated by different similarity functions provides insights into the different use cases that COREL variants can fulfill, with the Cosine-COREL variant consistently producing a clusterable latent space, while Gaussian-COREL consistently achieving better classification accuracy than CCE.",1
"We aim to obtain an interpretable, expressive, and disentangled scene representation that contains comprehensive structural and textural information for each object. Previous scene representations learned by neural networks are often uninterpretable, limited to a single object, or lacking 3D knowledge. In this work, we propose 3D scene de-rendering networks (3D-SDN) to address the above issues by integrating disentangled representations for semantics, geometry, and appearance into a deep generative model. Our scene encoder performs inverse graphics, translating a scene into a structured object-wise representation. Our decoder has two components: a differentiable shape renderer and a neural texture generator. The disentanglement of semantics, geometry, and appearance supports 3D-aware scene manipulation, e.g., rotating and moving objects freely while keeping the consistent shape and texture, and changing the object appearance without affecting its shape. Experiments demonstrate that our editing scheme based on 3D-SDN is superior to its 2D counterpart.",0
"Our objective is to acquire a scene representation that is both understandable and illustrative, while also being disentangled, containing extensive structural and textural information for every object. Traditional neural network-based scene representations have proven to be incomprehensible, restricted to a single object, or without knowledge of 3D. For this reason, we suggest using 3D scene de-rendering networks (3D-SDN) to tackle these concerns by integrating disentangled representations of semantics, geometry, and appearance into a deep generative model. Our scene encoder accomplishes inverse graphics by transforming a scene into a structured object-wise representation. Our decoder comprises two components: a differentiable shape renderer and a neural texture generator. The disentanglement of semantics, geometry, and appearance enables 3D-aware scene manipulation, such as the ability to rotate and move objects freely while maintaining consistent shape and texture, as well as alter an object's appearance without changing its shape. Our experiments demonstrate that our editing method based on 3D-SDN surpasses the 2D equivalent.",1
"In a Massive Open Online Course (MOOC), predictive models of student behavior can support multiple aspects of learning, including instructor feedback and timely intervention. Ongoing courses, when the student outcomes are yet unknown, must rely on models trained from the historical data of previously offered courses. It is possible to transfer models, but they often have poor prediction performance. One reason is features that inadequately represent predictive attributes common to both courses. We present an automated transductive transfer learning approach that addresses this issue. It relies on problem-agnostic, temporal organization of the MOOC clickstream data, where, for each student, for multiple courses, a set of specific MOOC event types is expressed for each time unit. It consists of two alternative transfer methods based on representation learning with auto-encoders: a passive approach using transductive principal component analysis and an active approach that uses a correlation alignment loss term. With these methods, we investigate the transferability of dropout prediction across similar and dissimilar MOOCs and compare with known methods. Results show improved model transferability and suggest that the methods are capable of automatically learning a feature representation that expresses common predictive characteristics of MOOCs.",0
"In a MOOC, predictive models can aid in various aspects of learning, such as providing feedback and intervention. However, in ongoing courses, where student outcomes are uncertain, models must be trained using historical data from past courses. However, transferring these models can be challenging due to inadequate representation of predictive attributes in both courses. To address this issue, we propose an automated transductive transfer learning approach that organizes MOOC clickstream data by student and time unit. Our approach uses two transfer methods based on representation learning: a passive method using transductive principal component analysis and an active method with a correlation alignment loss term. Our study investigates the transferability of dropout prediction across similar and dissimilar MOOCs, and our results demonstrate improved model transferability. These methods can automatically learn a feature representation that captures common predictive characteristics of MOOCs.",1
"Among the representation learning, the low-rank representation (LRR) is one of the hot research topics in many fields, especially in image processing and pattern recognition. Although LRR can capture the global structure, the ability of local structure preservation is limited because LRR lacks dictionary learning. In this paper, we propose a novel multi-focus image fusion method based on dictionary learning and LRR to get a better performance in both global and local structure. Firstly, the source images are divided into several patches by sliding window technique. Then, the patches are classified according to the Histogram of Oriented Gradient (HOG) features. And the sub-dictionaries of each class are learned by K-singular value decomposition (K-SVD) algorithm. Secondly, a global dictionary is constructed by combining these sub-dictionaries. Then, we use the global dictionary in LRR to obtain the LRR coefficients vector for each patch. Finally, the l_1-norm and choose-max fuse strategy for each coefficients vector is adopted to reconstruct fused image from the fused LRR coefficients and the global dictionary. Experimental results demonstrate that the proposed method can obtain state-of-the-art performance in both qualitative and quantitative evaluations compared with serval classical methods and novel methods.The Code of our fusion method is available at https://github.com/hli1221/imagefusion_dllrr",0
"LRR is a popular research topic in image processing and pattern recognition due to its ability to capture global structures. However, it falls short in preserving local structures without dictionary learning. This paper proposes a novel multi-focus image fusion method that uses both dictionary learning and LRR to improve overall performance. The method involves dividing source images into patches, classifying them based on HOG features, and learning sub-dictionaries for each class using K-SVD. A global dictionary is then constructed from these sub-dictionaries, and LRR coefficients vector is obtained for each patch. The fused image is reconstructed using the l_1-norm and choose-max fuse strategy. Experimental results show that this method outperforms classical and novel methods in both qualitative and quantitative evaluations. The code for this method is available at https://github.com/hli1221/imagefusion_dllrr.",1
"We introduce adversarial neural networks for representation learning as a novel approach to transfer learning in brain-computer interfaces (BCIs). The proposed approach aims to learn subject-invariant representations by simultaneously training a conditional variational autoencoder (cVAE) and an adversarial network. We use shallow convolutional architectures to realize the cVAE, and the learned encoder is transferred to extract subject-invariant features from unseen BCI users' data for decoding. We demonstrate a proof-of-concept of our approach based on analyses of electroencephalographic (EEG) data recorded during a motor imagery BCI experiment.",0
"A new technique for transfer learning in brain-computer interfaces (BCIs) is presented using adversarial neural networks for representation learning. The approach is designed to acquire subject-invariant representations by training a conditional variational autoencoder (cVAE) and an adversarial network simultaneously. Shallow convolutional architectures are employed to implement the cVAE, and the encoded information is utilized to extract subject-invariant features from new BCI users' data for decoding. Using electroencephalographic (EEG) data obtained from a motor imagery BCI experiment, we present a proof-of-concept of the proposed approach.",1
"This paper focuses on designing data-driven models to learn a discriminant representation space for face recognition using RGB-D data. Unlike hand-crafted representations, learned models can extract and organize the discriminant information from the data, and can automatically adapt to build new compute vision applications faster. We proposed an effective way to train Convolutional Neural Networks to learn face patch discriminant features. The proposed solution was tested and validated on state-of-the-art RGB-D datasets and showed competitive and promising results relatively to standard hand-crafted feature extractors.",0
"The main objective of this research is to create data-based models that can acquire a distinctive representation space for facial recognition through RGB-D data. Learning models can gather and categorize discriminant details from the data more efficiently than manually designed representations. Additionally, these models can quickly adapt to new computer vision applications. Our suggested method employs Convolutional Neural Networks to teach facial patch discriminant traits. We evaluated our approach on the latest RGB-D datasets and discovered that it produced competitive and optimistic outcomes in comparison to standard manual feature extractors.",1
"Facial expression recognition has been an active area in computer vision with application areas including animation, social robots, personalized banking, etc. In this study, we explore the problem of image classification for detecting facial expressions based on features extracted from pre-trained convolutional neural networks trained on ImageNet database. Features are extracted and transferred to a Linear Support Vector Machine for classification. All experiments are performed on two publicly available datasets such as JAFFE and CK+ database. The results show that representations learned from pre-trained networks for a task such as object recognition can be transferred, and used for facial expression recognition. Furthermore, for a small dataset, using features from earlier layers of the VGG19 network provides better classification accuracy. Accuracies of 92.26% and 92.86% were achieved for the CK+ and JAFFE datasets respectively.",0
"The recognition of facial expressions is a field of interest in computer vision that has many applications, including animation, personalized banking, and social robots. In this study, we aim to classify images and detect facial expressions by utilizing pre-trained convolutional neural networks from the ImageNet database. The extracted features are then transferred to a Linear Support Vector Machine for classification, and the experiments are carried out on two publicly available datasets, namely JAFFE and CK+. The results demonstrate that pre-trained networks' learned representations for object recognition can be transferred and used for facial expression recognition. Additionally, using features from earlier layers of the VGG19 network results in better classification accuracy for smaller datasets. The achieved accuracies for the CK+ and JAFFE datasets are 92.26% and 92.86%, respectively.",1
"In this paper we introduce evidence transfer for clustering, a deep learning method that can incrementally manipulate the latent representations of an autoencoder, according to external categorical evidence, in order to improve a clustering outcome. By evidence transfer we define the process by which the categorical outcome of an external, auxiliary task is exploited to improve a primary task, in this case representation learning for clustering. Our proposed method makes no assumptions regarding the categorical evidence presented, nor the structure of the latent space. We compare our method, against the baseline solution by performing k-means clustering before and after its deployment. Experiments with three different kinds of evidence show that our method effectively manipulates the latent representations when introduced with real corresponding evidence, while remaining robust when presented with low quality evidence.",0
"This paper presents a technique called evidence transfer for clustering, which is a deep learning approach that can adjust the latent representations of an autoencoder based on external categorical evidence. This method aims to enhance the clustering outcome by utilizing the outcome of an auxiliary task to improve the primary task of representation learning for clustering. Our approach does not make any assumptions regarding the structure of the latent space or the categorical evidence provided. We compared our method to the baseline solution by conducting k-means clustering before and after its implementation. We conducted experiments using three different types of evidence, and the results demonstrate that our method effectively adjusts the latent representations when presented with relevant evidence and remains robust even when the quality of the evidence is low.",1
"Person re-identification (PReID) has received increasing attention due to it is an important part in intelligent surveillance. Recently, many state-of-the-art methods on PReID are part-based deep models. Most of them focus on learning the part feature representation of person body in horizontal direction. However, the feature representation of body in vertical direction is usually ignored. Besides, the spatial information between these part features and the different feature channels is not considered. In this study, we introduce a multi-branches deep model for PReID. Specifically, the model consists of five branches. Among the five branches, two of them learn the local feature with spatial information from horizontal or vertical orientations, respectively. The other one aims to learn interdependencies knowledge between different feature channels generated by the last convolution layer. The remains of two other branches are identification and triplet sub-networks, in which the discriminative global feature and a corresponding measurement can be learned simultaneously. All the five branches can improve the representation learning. We conduct extensive comparative experiments on three PReID benchmarks including CUHK03, Market-1501 and DukeMTMC-reID. The proposed deep framework outperforms many state-of-the-art in most cases.",0
"Person re-identification (PReID) is an essential aspect of intelligent surveillance, and its significance has grown in recent times. State-of-the-art PReID methods are predominantly part-based deep models that emphasize the learning of part feature representation of the person's body in the horizontal direction. Unfortunately, the feature representation of the body in the vertical direction is often disregarded, and the spatial relationship between these part features and different feature channels is not considered. This study introduces a multi-branch deep model for PReID that comprises five branches. Two of the branches learn local features with spatial information from the horizontal or vertical orientation, while another branch aims to learn interdependencies knowledge among the different feature channels generated by the last convolution layer. The remaining two branches are identification and triplet sub-networks, where the discriminative global feature and a corresponding measurement can be learned simultaneously. All five branches improve the representation learning. Extensive comparative experiments conducted on three PReID benchmarks, including CUHK03, Market-1501, and DukeMTMC-reID, demonstrate that the proposed deep framework outperforms many state-of-the-art methods in most cases.",1
"Recently, graph neural networks have been adopted in a wide variety of applications ranging from relational representations to modeling irregular data domains such as point clouds and social graphs. However, the space of graph neural network architectures remains highly fragmented impeding the development of optimized implementations similar to what is available for convolutional neural networks. In this work, we present BiGraphNet, a graph neural network architecture that generalizes many popular graph neural network models and enables new efficient operations similar to those supported by ConvNets. By explicitly separating the input and output nodes, BiGraphNet: (i) generalizes the graph convolution to support new efficient operations such as coarsened graph convolutions (similar to strided convolution in convnets), multiple input graphs convolution and graph expansions (unpooling) which can be used to implement various graph architectures such as graph autoencoders, and graph residual nets; and (ii) accelerates and scales the computations and memory requirements in hierarchical networks by performing computations only at specified output nodes.",0
"Graph neural networks have gained popularity in diverse applications, including modeling relational representations and irregular data domains like social graphs and point clouds. However, the lack of a unified framework for graph neural network architectures presents a challenge to optimizing implementations, unlike convolutional neural networks. To address this issue, we introduce BiGraphNet, a graph neural network architecture that generalizes many popular models and supports efficient operations similar to ConvNets. BiGraphNet separates input and output nodes, facilitating the generalization of graph convolution to include coarsened graph convolutions, multiple input graph convolutions, and graph expansions (unpooling). These features enable the implementation of various graph architectures like graph autoencoders and graph residual nets. Additionally, BiGraphNet accelerates hierarchical networks' computations and reduces memory requirements by computing only at specific output nodes.",1
"Learning useful representations with little or no supervision is a key challenge in artificial intelligence. We provide an in-depth review of recent advances in representation learning with a focus on autoencoder-based models. To organize these results we make use of meta-priors believed useful for downstream tasks, such as disentanglement and hierarchical organization of features. In particular, we uncover three main mechanisms to enforce such properties, namely (i) regularizing the (approximate or aggregate) posterior distribution, (ii) factorizing the encoding and decoding distribution, or (iii) introducing a structured prior distribution. While there are some promising results, implicit or explicit supervision remains a key enabler and all current methods use strong inductive biases and modeling assumptions. Finally, we provide an analysis of autoencoder-based representation learning through the lens of rate-distortion theory and identify a clear tradeoff between the amount of prior knowledge available about the downstream tasks, and how useful the representation is for this task.",0
"The challenge of artificial intelligence lies in acquiring valuable representations with minimal or no supervision. Our review delves into the recent advancements in representation learning, with emphasis on autoencoder-based models. To categorize these findings, we utilize meta-priors that are believed to be advantageous for downstream tasks, such as disentanglement and hierarchical feature organization. We highlight three primary mechanisms to enforce these properties, namely (i) regulating the (approximate or aggregate) posterior distribution, (ii) decomposing the encoding and decoding distributions, or (iii) introducing a structured prior distribution. While some techniques demonstrate promise, explicit or implicit supervision remains a crucial facilitator, and current methods rely heavily on inductive biases and modeling assumptions. Lastly, we examine autoencoder-based representation learning through the perspective of rate-distortion theory and identify a distinct tradeoff between the amount of prior knowledge available for downstream tasks and the usefulness of the representation for these tasks.",1
"Machine hearing or listening represents an emerging area. Conventional approaches rely on the design of handcrafted features specialized to a specific audio task and that can hardly generalized to other audio fields. For example, Mel-Frequency Cepstral Coefficients (MFCCs) and its variants were successfully applied to computational auditory scene recognition while Chroma vectors are good at music chord recognition. Unfortunately, these predefined features may be of variable discrimination power while extended to other tasks or even within the same task due to different nature of clips. Motivated by this need of a principled framework across domain applications for machine listening, we propose a generic and data-driven representation learning approach. For this sake, a novel and efficient supervised dictionary learning method is presented. The method learns dissimilar dictionaries, one per each class, in order to extract heterogeneous information for classification. In other words, we are seeking to minimize the intra-class homogeneity and maximize class separability. This is made possible by promoting pairwise orthogonality between class specific dictionaries and controlling the sparsity structure of the audio clip's decomposition over these dictionaries. The resulting optimization problem is non-convex and solved using a proximal gradient descent method. Experiments are performed on both computational auditory scene (East Anglia and Rouen) and synthetic music chord recognition datasets. Obtained results show that our method is capable to reach state-of-the-art hand-crafted features for both applications.",0
"The field of machine listening is a new and developing area. Traditional methods rely on creating features that are designed specifically for certain audio tasks, but these features cannot be easily applied to other audio fields. For instance, Mel-Frequency Cepstral Coefficients (MFCCs) and Chroma vectors are effective for computational auditory scene recognition and music chord recognition, respectively. However, these pre-defined features may not always be useful for different tasks or even within the same task due to variations in audio clips. To address this challenge and create a more unified approach to machine listening, we propose a data-driven representation learning method. Our approach involves using a supervised dictionary learning method to extract heterogeneous information for classification. We accomplish this by creating dissimilar dictionaries for each class and promoting orthogonality between them, while controlling the sparsity structure of the audio clip's decomposition over these dictionaries. We solve this non-convex optimization problem using a proximal gradient descent method. We conduct experiments on both computational auditory scene and synthetic music chord recognition datasets and show that our method achieves state-of-the-art performance compared to hand-crafted features.",1
"Deep reinforcement-learning methods have achieved remarkable performance on challenging control tasks. Observations of the resulting behavior give the impression that the agent has constructed a generalized representation that supports insightful action decisions. We re-examine what is meant by generalization in RL, and propose several definitions based on an agent's performance in on-policy, off-policy, and unreachable states. We propose a set of practical methods for evaluating agents with these definitions of generalization. We demonstrate these techniques on a common benchmark task for deep RL, and we show that the learned networks make poor decisions for states that differ only slightly from on-policy states, even though those states are not selected adversarially. Taken together, these results call into question the extent to which deep Q-networks learn generalized representations, and suggest that more experimentation and analysis is necessary before claims of representation learning can be supported.",0
"Impressive results have been achieved by deep reinforcement-learning methods on difficult control tasks, leading to the belief that agents have developed a broad representation that enables them to make informed decisions. However, we revisit the definition of generalization in RL and put forth various explanations based on an agent's performance in on-policy, off-policy, and unreachable states. We also suggest practical methods for assessing agents using these definitions of generalization. Using a common benchmark task for deep RL, we demonstrate that the learned networks make suboptimal decisions for states that are only slightly different from on-policy states, even when those states are not chosen adversarially. These findings challenge the extent to which deep Q-networks acquire generalized representations and indicate that further experimentation and analysis are necessary before assertions of representation learning can be validated.",1
"We consider the problem of building a state representation model in a continual fashion. As the environment changes, the aim is to efficiently compress the sensory state's information without losing past knowledge. The learned features are then fed to a Reinforcement Learning algorithm to learn a policy. We propose to use Variational Auto-Encoders for state representation, and Generative Replay, i.e. the use of generated samples, to maintain past knowledge. We also provide a general and statistically sound method for automatic environment change detection. Our method provides efficient state representation as well as forward transfer, and avoids catastrophic forgetting. The resulting model is capable of incrementally learning information without using past data and with a bounded system size.",0
"Our focus is on developing a continual state representation model that efficiently compresses sensory state information, even as the environment changes, while retaining past knowledge. We suggest utilizing Variational Auto-Encoders for state representation and Generative Replay, which involves using generated samples, to hold onto past knowledge. Additionally, we introduce a reliable approach for automatically detecting changes in the environment. Our method enables efficient state representation, forward transfer, and avoids catastrophic forgetting. As a result, the model can learn new information incrementally with a restricted system size, without the need for past data.",1
"Facial landmark localization plays a critical role in face recognition and analysis. In this paper, we propose a novel cascaded backbone-branches fully convolutional neural network~(BB-FCN) for rapidly and accurately localizing facial landmarks in unconstrained and cluttered settings. Our proposed BB-FCN generates facial landmark response maps directly from raw images without any preprocessing. BB-FCN follows a coarse-to-fine cascaded pipeline, which consists of a backbone network for roughly detecting the locations of all facial landmarks and one branch network for each type of detected landmark for further refining their locations. Furthermore, to facilitate the facial landmark localization under unconstrained settings, we propose a large-scale benchmark named SYSU16K, which contains 16000 faces with large variations in pose, expression, illumination and resolution. Extensive experimental evaluations demonstrate that our proposed BB-FCN can significantly outperform the state-of-the-art under both constrained (i.e., within detected facial regions only) and unconstrained settings. We further confirm that high-quality facial landmarks localized with our proposed network can also improve the precision and recall of face detection.",0
"The localization of facial landmarks is crucial for analyzing and recognizing faces. In our study, we introduce a new method for accurately and swiftly localizing facial landmarks in complex and unstructured environments. Our technique, called the cascaded backbone-branches fully convolutional neural network (BB-FCN), generates facial landmark response maps directly from raw images without any pre-processing. The BB-FCN utilizes a multi-step process, beginning with a backbone network for initially detecting facial landmarks, followed by branch networks for refining their locations. To improve facial landmark localization in unstructured settings, we present a comprehensive benchmark, SYSU16K, containing 16,000 faces with a wide range of pose, expression, illumination, and resolution variations. Our extensive experiments reveal that our proposed BB-FCN significantly outperforms the current state-of-the-art approaches under both constrained and unconstrained settings. Additionally, we demonstrate that precise facial landmark localization can enhance the accuracy of face detection.",1
"We present a novel method that can learn a graph representation from multivariate data. In our representation, each node represents a cluster of data points and each edge represents the subset-superset relationship between clusters, which can be mutually overlapped. The key to our method is to use formal concept analysis (FCA), which can extract hierarchical relationships between clusters based on the algebraic closedness property. We empirically show that our method can effectively extract hierarchical structures of clusters compared to the baseline method.",0
"Our novel approach can acquire a graph representation from multivariate data. In our representation, each node denotes a cluster of data points, and each edge signifies the subset-superset relationship between clusters, which may overlap. Our technique relies on formal concept analysis (FCA) to extract hierarchical relationships between clusters using the algebraic closedness property. Our empirical evidence demonstrates that our method outperforms the baseline method in effectively extracting hierarchical structures of clusters.",1
"An unsupervised human action modeling framework can provide useful pose-sequence representation, which can be utilized in a variety of pose analysis applications. In this work we propose a novel temporal pose-sequence modeling framework, which can embed the dynamics of 3D human-skeleton joints to a continuous latent space in an efficient manner. In contrast to end-to-end framework explored by previous works, we disentangle the task of individual pose representation learning from the task of learning actions as a trajectory in pose embedding space. In order to realize a continuous pose embedding manifold with improved reconstructions, we propose an unsupervised, manifold learning procedure named Encoder GAN, (or EnGAN). Further, we use the pose embeddings generated by EnGAN to model human actions using a bidirectional RNN auto-encoder architecture, PoseRNN. We introduce first-order gradient loss to explicitly enforce temporal regularity in the predicted motion sequence. A hierarchical feature fusion technique is also investigated for simultaneous modeling of local skeleton joints along with global pose variations. We demonstrate state-of-the-art transfer-ability of the learned representation against other supervisedly and unsupervisedly learned motion embeddings for the task of fine-grained action recognition on SBU interaction dataset. Further, we show the qualitative strengths of the proposed framework by visualizing skeleton pose reconstructions and interpolations in pose-embedding space, and low dimensional principal component projections of the reconstructed pose trajectories.",0
"The utilization of an unsupervised human action modeling framework can yield a useful pose-sequence representation for various pose analysis applications. This paper presents a new temporal pose-sequence modeling framework that efficiently embeds the dynamics of 3D human-skeleton joints into a continuous latent space. Unlike previous works, this framework disentangles the learning of individual pose representation from the learning of actions as a trajectory in pose embedding space. To achieve a continuous pose embedding manifold with improved reconstructions, an unsupervised manifold learning process called Encoder GAN (EnGAN) is proposed. The pose embeddings generated by EnGAN are then used to model human actions using a bidirectional RNN auto-encoder architecture called PoseRNN. A first-order gradient loss is introduced to enforce temporal regularity in the predicted motion sequence, and a hierarchical feature fusion technique is investigated for simultaneous modeling of local skeleton joints and global pose variations. The paper also demonstrates the state-of-the-art transfer-ability of the learned representation for the task of fine-grained action recognition on SBU interaction dataset. The proposed framework's qualitative strengths are shown by visualizing skeleton pose reconstructions and interpolations in pose-embedding space, and low dimensional principal component projections of the reconstructed pose trajectories.",1
"How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.",0
"In what way can intelligent agents effectively tackle various tasks with limited data? One approach to solving this problem is through disentangled representation learning, which suggests that an agent can benefit by separating the underlying structure of the world into distinct parts of its representation. However, there is no widely accepted definition of disentangling, as it is challenging to formalize the notion of world structure beyond simple datasets with known generative processes. To address this issue, we propose that a systematic method of characterizing disentangled representations can be achieved by focusing on the transformation properties of the world. By examining transformations that only affect certain properties of the underlying world state while leaving all others unchanged, we can identify exploitable structure in any kind of data. This approach is similar to the study of symmetry transformations in physics, which has revolutionized our understanding of the world's structure. Using the formalism of group and representation theory, we establish the first formal definition of disentangled representations, which aligns with current intuitions about disentangling while resolving previous points of contention. Although our work concentrates on formally defining disentangling rather than solving the learning problem, we believe that shifting the perspective to studying data transformations can encourage the advancement of better representation learning algorithms.",1
"Basic binary relations such as equality and inequality are fundamental to relational data structures. Neural networks should learn such relations and generalise to new unseen data. We show in this study, however, that this generalisation fails with standard feed-forward networks on binary vectors. Even when trained with maximal training data, standard networks do not reliably detect equality.We introduce differential rectifier (DR) units that we add to the network in different configurations. The DR units create an inductive bias in the networks, so that they do learn to generalise, even from small numbers of examples and we have not found any negative effect of their inclusion in the network. Given the fundamental nature of these relations, we hypothesize that feed-forward neural network learning benefits from inductive bias in other relations as well. Consequently, the further development of suitable inductive biases will be beneficial to many tasks in relational learning with neural networks.",0
"Relational data structures rely on basic binary relations like equality and inequality. Neural networks should be able to learn and apply these relations to new data, but our study has shown that standard feed-forward networks struggle to do so with binary vectors, even when given ample training data. To address this issue, we introduce differential rectifier (DR) units that create an inductive bias in the network, allowing it to generalize even from minimal examples. Our findings suggest that inductive biases could benefit other relational learning tasks, and further development in this area could improve the performance of neural networks in these tasks.",1
"Timely prediction of clinically critical events in Intensive Care Unit (ICU) is important for improving care and survival rate. Most of the existing approaches are based on the application of various classification methods on explicitly extracted statistical features from vital signals. In this work, we propose to eliminate the high cost of engineering hand-crafted features from multivariate time-series of physiologic signals by learning their representation with a sequence-to-sequence auto-encoder. We then propose to hash the learned representations to enable signal similarity assessment for the prediction of critical events. We apply this methodological framework to predict Acute Hypotensive Episodes (AHE) on a large and diverse dataset of vital signal recordings. Experiments demonstrate the ability of the presented framework in accurately predicting an upcoming AHE.",0
"Accurately predicting critical events in the Intensive Care Unit (ICU) is crucial for improving patient care and increasing survival rates. Currently, many methods rely on classification techniques applied to statistical features extracted from vital signals. However, we suggest a new approach that eliminates the high cost of manually engineering features by using a sequence-to-sequence auto-encoder to learn the representation of multivariate time-series physiologic signals. This representation is then hashed to enable signal similarity assessment, allowing for the prediction of critical events. Our experiments on a large and diverse dataset of vital signal recordings demonstrate the framework's ability to predict Acute Hypotensive Episodes (AHE) accurately.",1
"This paper presents a novel method for rare event detection from an image pair with class-imbalanced datasets. A straightforward approach for event detection tasks is to train a detection network from a large-scale dataset in an end-to-end manner. However, in many applications such as building change detection on satellite images, few positive samples are available for the training. Moreover, scene image pairs contain many trivial events, such as in illumination changes or background motions. These many trivial events and the class imbalance problem lead to false alarms for rare event detection. In order to overcome these difficulties, we propose a novel method to learn disentangled representations from only low-cost negative samples. The proposed method disentangles different aspects in a pair of observations: variant and invariant factors that represent trivial events and image contents, respectively. The effectiveness of the proposed approach is verified by the quantitative evaluations on four change detection datasets, and the qualitative analysis shows that the proposed method can acquire the representations that disentangle rare events from trivial ones.",0
"In this paper, a new approach is introduced for detecting rare events in imbalanced datasets using image pairs. Although training a detection network using a large dataset is a common method for event detection, it is not practical in situations where there are limited positive samples, such as in building change detection using satellite images. Additionally, many trivial events, such as changes in illumination or background motion, can lead to false alarms in rare event detection due to the imbalance in classes. To address these challenges, a novel method is proposed that utilizes low-cost negative samples to learn disentangled representations. This method separates variant and invariant factors, which represent trivial events and image contents, respectively. The effectiveness of this approach is demonstrated through quantitative evaluations on four change detection datasets. Moreover, the proposed method is capable of differentiating rare events from trivial ones, as shown through qualitative analysis.",1
"For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised ""practice"" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.",0
"To successfully achieve various user-defined objectives during testing, an independent agent must possess versatile and all-purpose skill sets that can handle raw sensory inputs like images. To achieve this level of generalization, our paper suggests a technique that combines unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the agent doesn't know the specific goals that may arise during testing, it undergoes a self-supervised ""practice"" phase where it creates and attempts to achieve imaginary goals. We develop a visual representation that serves three distinct purposes - generating goals for self-supervised practice, transforming raw sensory inputs, and computing a reward signal for goal attainment. Additionally, we propose a retroactive goal labeling approach to enhance the efficiency of our method. Our off-policy algorithm is capable of effectively learning policies that can operate on raw image observations and goals for a real-world robotic system, and it surpasses previous methods by a significant margin.",1
"We propose Deep Hierarchical Machine (DHM), a model inspired from the divide-and-conquer strategy while emphasizing representation learning ability and flexibility. A stochastic routing framework as used by recent deep neural decision/regression forests is incorporated, but we remove the need to evaluate unnecessary computation paths by utilizing a different topology and introducing a probabilistic pruning technique. We also show a specified version of DHM (DSHM) for efficiency, which inherits the sparse feature extraction process as in traditional decision tree with pixel-difference feature. To achieve sparse feature extraction, we propose to utilize sparse convolution operation in DSHM and show one possibility of introducing sparse convolution kernels by using local binary convolution layer. DHM can be applied to both classification and regression problems, and we validate it on standard image classification and face alignment tasks to show its advantages over past architectures.",0
"Our proposed model, the Deep Hierarchical Machine (DHM), draws inspiration from the divide-and-conquer strategy, but places greater emphasis on representation learning ability and flexibility. We incorporate a stochastic routing framework, similar to recent deep neural decision/regression forests, but enhance efficiency by using a different topology and introducing a probabilistic pruning technique that eliminates unnecessary computation paths. Additionally, we introduce a specialized version of DHM (DSHM) that utilizes sparse feature extraction, similar to traditional decision trees with pixel-difference feature. To achieve this, we propose using sparse convolution operation in DSHM and demonstrate the possibility of introducing sparse convolution kernels through a local binary convolution layer. DHM is suitable for both classification and regression problems, and we demonstrate its superiority over past architectures by validating it on standard image classification and face alignment tasks.",1
"In this work, we investigate unsupervised representation learning on medical time series, which bears the promise of leveraging copious amounts of existing unlabeled data in order to eventually assist clinical decision making. By evaluating on the prediction of clinically relevant outcomes, we show that in a practical setting, unsupervised representation learning can offer clear performance benefits over end-to-end supervised architectures. We experiment with using sequence-to-sequence (Seq2Seq) models in two different ways, as an autoencoder and as a forecaster, and show that the best performance is achieved by a forecasting Seq2Seq model with an integrated attention mechanism, proposed here for the first time in the setting of unsupervised learning for medical time series.",0
"The aim of this study is to explore the potential of unsupervised representation learning for medical time series. This method could make use of the vast amount of unlabeled data available to assist in clinical decision making. Our analysis focuses on predicting clinically significant outcomes, revealing that unsupervised representation learning outperforms end-to-end supervised architectures in practical applications. We experiment with sequence-to-sequence (Seq2Seq) models, using an autoencoder and a forecaster. Our findings demonstrate that the most effective approach is a forecasting Seq2Seq model with an integrated attention mechanism, which is introduced for the first time in the context of unsupervised learning for medical time series.",1
"User representations are routinely used in recommendation systems by platform developers, targeted advertisements by marketers, and by public policy researchers to gauge public opinion across demographic groups. Computer scientists consider the problem of inferring user representations more abstractly; how does one extract a stable user representation - effective for many downstream tasks - from a medium as noisy and complicated as social media?   The quality of a user representation is ultimately task-dependent (e.g. does it improve classifier performance, make more accurate recommendations in a recommendation system) but there are proxies that are less sensitive to the specific task. Is the representation predictive of latent properties such as a person's demographic features, socioeconomic class, or mental health state? Is it predictive of the user's future behavior?   In this thesis, we begin by showing how user representations can be learned from multiple types of user behavior on social media. We apply several extensions of generalized canonical correlation analysis to learn these representations and evaluate them at three tasks: predicting future hashtag mentions, friending behavior, and demographic features. We then show how user features can be employed as distant supervision to improve topic model fit. Finally, we show how user features can be integrated into and improve existing classifiers in the multitask learning framework. We treat user representations - ground truth gender and mental health features - as auxiliary tasks to improve mental health state prediction. We also use distributed user representations learned in the first chapter to improve tweet-level stance classifiers, showing that distant user information can inform classification tasks at the granularity of a single message.",0
"Platform developers, marketers, and public policy researchers commonly use user representations in recommendation systems, targeted advertisements, and to understand public opinion among different groups. However, computer scientists face the challenge of obtaining a stable user representation from the noisy and complex social media environment. A user representation's quality depends on the task, but certain proxies can be used to assess its effectiveness, such as its predictability of demographic features, socioeconomic status, mental health, and future behavior. This thesis demonstrates how user representations can be learned from different types of social media behavior using generalized canonical correlation analysis, and evaluates their performance in predicting future hashtag mentions, friending behavior, and demographic features. The thesis also explores how user features can be used to improve topic modeling and classification tasks, specifically in predicting mental health state and stance classification at the tweet level using distributed user representations.",1
"Learning visual features from unlabeled image data is an important yet challenging task, which is often achieved by training a model on some annotation-free information. We consider spatial contexts, for which we solve so-called jigsaw puzzles, i.e., each image is cut into grids and then disordered, and the goal is to recover the correct configuration. Existing approaches formulated it as a classification task by defining a fixed mapping from a small subset of configurations to a class set, but these approaches ignore the underlying relationship between different configurations and also limit their application to more complex scenarios. This paper presents a novel approach which applies to jigsaw puzzles with an arbitrary grid size and dimensionality. We provide a fundamental and generalized principle, that weaker cues are easier to be learned in an unsupervised manner and also transfer better. In the context of puzzle recognition, we use an iterative manner which, instead of solving the puzzle all at once, adjusts the order of the patches in each step until convergence. In each step, we combine both unary and binary features on each patch into a cost function judging the correctness of the current configuration. Our approach, by taking similarity between puzzles into consideration, enjoys a more reasonable way of learning visual knowledge. We verify the effectiveness of our approach in two aspects. First, it is able to solve arbitrarily complex puzzles, including high-dimensional puzzles, that prior methods are difficult to handle. Second, it serves as a reliable way of network initialization, which leads to better transfer performance in a few visual recognition tasks including image classification, object detection, and semantic segmentation.",0
"The task of learning visual features from unlabeled image data is a challenging yet significant undertaking. Typically, this is accomplished by training a model on annotation-free information. To tackle this, we focus on spatial contexts and utilize jigsaw puzzles where images are cut into grids and disordered, and the goal is to reconstruct the correct configuration. Existing approaches treat this as a classification task, with a fixed mapping from a subset of configurations to a class set. However, this ignores the relationship between different configurations and limits application to complex scenarios. In this paper, we present a novel approach that is applicable to jigsaw puzzles of any grid size and dimensionality. Our approach utilizes a fundamental principle that weaker cues are easier to learn unsupervised and transfer better. We employ an iterative manner that adjusts patch order until convergence and combines unary and binary features on each patch into a cost function. Our approach considers the similarity between puzzles, leading to a more reasonable way of learning visual knowledge. Our approach is effective in solving complex puzzles and serves as a reliable way of network initialization, leading to better transfer performance in visual recognition tasks such as image classification, object detection, and semantic segmentation.",1
"Graph convolutional network (GCN) is an emerging neural network approach. It learns new representation of a node by aggregating feature vectors of all neighbors in the aggregation process without considering whether the neighbors or features are useful or not. Recent methods have improved solutions by sampling a fixed size set of neighbors, or assigning different weights to different neighbors in the aggregation process, but features within a feature vector are still treated equally in the aggregation process. In this paper, we introduce a new convolution operation on regular size feature maps constructed from features of a fixed node bandwidth via sampling to get the first-level node representation, which is then passed to a standard GCN to learn the second-level node representation. Experiments show that our method outperforms competing methods in semi-supervised node classification tasks. Furthermore, our method opens new doors for exploring new GCN architectures, particularly deeper GCN models.",0
"An emerging neural network approach known as Graph Convolutional Network (GCN) has been developed. It learns new representations of a node by aggregating feature vectors of all neighbors in the aggregation process, regardless of their usefulness. Although recent methods have improved solutions by sampling a fixed size set of neighbors, or assigning different weights to different neighbors in the aggregation process, features within a feature vector are still treated equally. This paper introduces a new convolution operation on regular size feature maps, constructed from features of a fixed node bandwidth via sampling. This enables the creation of the first-level node representation, which is then passed to a standard GCN to learn the second-level node representation. Experiments show that this method outperforms other methods in semi-supervised node classification tasks. Furthermore, it opens up new possibilities for exploring new GCN architectures, particularly deeper GCN models.",1
"Convolutional Neural Networks (CNNs) can learn effective features, though have been shown to suffer from a performance drop when the distribution of the data changes from training to test data. In this paper we analyze the internal representations of CNNs and observe that the representations of unseen data in each class, spread more (with higher variance) in the embedding space of the CNN compared to representations of the training data. More importantly, this difference is more extreme if the unseen data comes from a shifted distribution. Based on this observation, we objectively evaluate the degree of representation's variance in each class via eigenvalue decomposition on the within-class covariance of the internal representations of CNNs and observe the same behaviour. This can be problematic as larger variances might lead to mis-classification if the sample crosses the decision boundary of its class. We apply nearest neighbor classification on the representations and empirically show that the embeddings with the high variance actually have significantly worse KNN classification performances, although this could not be foreseen from their end-to-end classification results. To tackle this problem, we propose Deep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that significantly reduces the within-class covariance of a DNN's representation, improving performance on unseen test data from a shifted distribution. We empirically evaluate DWCCA on two datasets for Acoustic Scene Classification (DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA significantly improve the network's internal representation, it also increases the end-to-end classification accuracy, especially when the test set exhibits a distribution shift. By adding DWCCA to a VGG network, we achieve around 6 percentage points improvement in the case of a distribution mismatch.",0
"The performance of Convolutional Neural Networks (CNNs) can decrease when the distribution of data changes from training to test data, even though they can effectively learn features. Our study examines the internal representations of CNNs and discovers that the representations of unseen data spread more in the embedding space of the CNN, with a higher variance compared to representations of training data. This variance is even greater if the unseen data comes from a shifted distribution. We objectively evaluate the degree of representation's variance in each class using eigenvalue decomposition on the within-class covariance of the internal representations of CNNs and observe the same behavior. This difference in variance can be problematic as larger variances may lead to misclassification. We apply nearest neighbor classification and find that embeddings with high variance have significantly worse KNN classification performance, although this cannot be predicted from their end-to-end classification results. To address this issue, we propose Deep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that reduces the within-class covariance of a DNN's representation, improving performance on unseen test data from a shifted distribution. We evaluate DWCCA on two datasets for Acoustic Scene Classification (DCASE2016 and DCASE2017) and demonstrate that DWCCA not only significantly enhances the network's internal representation, but also increases the end-to-end classification accuracy, particularly when the test set shows a distribution shift. By incorporating DWCCA into a VGG network, we achieve approximately 6 percentage points improvement in the case of a distribution mismatch.",1
"The recent success in deep learning has lead to various effective representation learning methods for videos. However, the current approaches for video representation require large amount of human labeled datasets for effective learning. We present an unsupervised representation learning framework to encode scene dynamics in videos captured from multiple viewpoints. The proposed framework has two main components: Representation Learning Network (RL-NET), which learns a representation with the help of Blending Network (BL-NET), and Video Rendering Network (VR-NET), which is used for video synthesis. The framework takes as input video clips from different viewpoints and time, learns an internal representation and uses this representation to render a video clip from an arbitrary given viewpoint and time. The ability of the proposed network to render video frames from arbitrary viewpoints and time enable it to learn a meaningful and robust representation of the scene dynamics. We demonstrate the effectiveness of the proposed method in rendering view-aware as well as time-aware video clips on two different real-world datasets including UCF-101 and NTU-RGB+D. To further validate the effectiveness of the learned representation, we use it for the task of view-invariant activity classification where we observe a significant improvement (~26%) in the performance on NTU-RGB+D dataset compared to the existing state-of-the art methods.",0
"Various effective representation learning methods for videos have been developed as a result of the recent success in deep learning. However, these approaches require large amounts of human labeled datasets for learning. To address this issue, we introduce an unsupervised representation learning framework that encodes scene dynamics in videos captured from multiple viewpoints. The framework consists of two main components: a Representation Learning Network (RL-NET) that learns a representation with the help of a Blending Network (BL-NET), and a Video Rendering Network (VR-NET) that synthesizes videos. By taking video clips from different viewpoints and times as input, the framework learns an internal representation that can be used to render a video clip from an arbitrary viewpoint and time. This ability enables the network to learn a robust and meaningful representation of scene dynamics. Our proposed method is effective in rendering view-aware and time-aware video clips, as demonstrated on real-world datasets such as UCF-101 and NTU-RGB+D. Furthermore, we use the learned representation for the task of view-invariant activity classification and demonstrate a significant improvement (~26%) in performance on the NTU-RGB+D dataset compared to existing state-of-the-art methods.",1
"It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations. We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match.",0
"The success of deep neural networks is widely attributed to the learning of good representations. However, there is a lack of theory and systematic approach for quantitatively characterizing the representations that deep neural networks learn. This work aims to move towards a better understanding of representations by studying the similarity between the representations learned by two networks with identical architecture but different initializations. Using the neuron activation subspace match model, we develop a rigorous theory that characterizes the structure of neuron activation subspace matches. Our theory introduces the concepts of maximum match and simple match, which describe the overall and finest similarity between sets of neurons in two networks, respectively. We also propose efficient algorithms to find the maximum match and simple matches. Our experiments show that the representations learned by the same convolutional layers of networks trained from different initializations are surprisingly not as similar as expected, at least in terms of subspace match.",1
"Increasing numbers of software vulnerabilities are discovered every year whether they are reported publicly or discovered internally in proprietary code. These vulnerabilities can pose serious risk of exploit and result in system compromise, information leaks, or denial of service. We leveraged the wealth of C and C++ open-source code available to develop a large-scale function-level vulnerability detection system using machine learning. To supplement existing labeled vulnerability datasets, we compiled a vast dataset of millions of open-source functions and labeled it with carefully-selected findings from three different static analyzers that indicate potential exploits. The labeled dataset is available at: https://osf.io/d45bw/. Using these datasets, we developed a fast and scalable vulnerability detection tool based on deep feature representation learning that directly interprets lexed source code. We evaluated our tool on code from both real software packages and the NIST SATE IV benchmark dataset. Our results demonstrate that deep feature representation learning on source code is a promising approach for automated software vulnerability detection.",0
"Every year, there is a growing number of software vulnerabilities being discovered, whether they are publicly reported or found within proprietary code. These vulnerabilities pose a serious risk of system compromise, information leaks, or denial of service. Our team utilized the abundance of open-source C and C++ code to create a machine learning-based, large-scale function-level vulnerability detection system. To enhance current labeled vulnerability datasets, we compiled a massive dataset of open-source functions, labeled with carefully-selected findings from three different static analyzers indicating potential exploits, which is available at: https://osf.io/d45bw/. Using these datasets, we developed a fast and scalable vulnerability detection tool based on deep feature representation learning that interprets lexed source code directly. We tested our tool on code from real software packages and the NIST SATE IV benchmark dataset and found that deep feature representation learning on source code is a promising method for automated software vulnerability detection.",1
"Heart rate (HR) is an important physiological signal that reflects the physical and emotional activities of humans. Traditional HR measurements are mainly based on contact monitors, which are inconvenient and may cause discomfort for the subjects. Recently, methods have been proposed for remote HR estimation from face videos. However, most of the existing methods focus on well-controlled scenarios, their generalization ability into less-constrained scenarios are not known. At the same time, lacking large-scale databases has limited the use of deep representation learning methods in remote HR estimation. In this paper, we introduce a large-scale multi-modal HR database (named as VIPL-HR), which contains 2,378 visible light videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Our VIPL-HR database also contains various variations such as head movements, illumination variations, and acquisition device changes. We also learn a deep HR estimator (named as RhythmNet) with the proposed spatial-temporal representation, which achieves promising results on both the public-domain and our VIPL-HR HR estimation databases. We would like to put the VIPL-HR database into the public domain.",0
"The human heart rate (HR) is a significant physiological signal that indicates both physical and emotional activities. Traditional HR measurements rely on contact monitors, which are inconvenient and may cause discomfort for individuals. Although remote HR estimation methods from face videos have been proposed, most of them are designed for well-controlled scenarios and may not be applicable in less-constrained environments. Additionally, deep representation learning methods have been limited by the lack of large-scale databases. To address these issues, we introduce the VIPL-HR database, which includes 2,378 visible light videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Our database also incorporates various variations such as head movements, illumination changes, and acquisition device differences. Moreover, we develop a deep HR estimator called RhythmNet that utilizes our proposed spatial-temporal representation and achieves promising results on both the public-domain and our VIPL-HR databases. We are making the VIPL-HR database available to the public.",1
"Among many unsolved puzzles in theories of Deep Neural Networks (DNNs), there are three most fundamental challenges that highly demand solutions, namely, expressibility, optimisability, and generalisability. Although there have been significant progresses in seeking answers using various theories, e.g. information bottleneck theory, sparse representation, statistical inference, Riemannian geometry, etc., so far there is no single theory that is able to provide solutions to all these challenges. In this work, we propose to engage the theory of differential topology to address the three problems. By modelling the dataset of interest as a smooth manifold, DNNs can be considered as compositions of smooth maps between smooth manifolds. Specifically, our work offers a differential topological view of loss landscape of DNNs, interplay between width and depth in expressibility, and regularisations for generalisability. Finally, in the setting of deep representation learning, we further apply the quotient topology to investigate the architecture of DNNs, which enables to capture nuisance factors in data with respect to a specific learning task.",0
"There are three key challenges that remain unsolved in Deep Neural Networks (DNNs): expressibility, optimisability, and generalisability. Although various theories, such as information bottleneck theory, sparse representation, statistical inference, and Riemannian geometry, have made significant progress towards addressing these challenges, no single theory has provided a complete solution. To tackle these issues, we propose using the theory of differential topology, which models the dataset as a smooth manifold and DNNs as compositions of smooth maps between these manifolds. Our approach offers a differential topological perspective on the loss landscape of DNNs, the relationship between width and depth in expressibility, and regularisations for generalisability. Moreover, we apply the quotient topology to explore the architecture of DNNs in deep representation learning, which helps capture nuisance factors in data relevant to a specific learning task.",1
"We present a generic and flexible module that encodes region proposals by both their intrinsic features and the extrinsic correlations to the others. The proposed non-local region of interest (NL-RoI) can be seamlessly adapted into different generalized R-CNN architectures to better address various perception tasks. Observe that existing techniques from R-CNN treat RoIs independently and perform the prediction solely based on image features within each region proposal. However, the pairwise relationships between proposals could further provide useful information for detection and segmentation. NL-RoI is thus formulated to enrich each RoI representation with the information from all other RoIs, and yield a simple, low-cost, yet effective module for region-based convolutional networks. Our experimental results show that NL-RoI can improve the performance of Faster/Mask R-CNN for object detection and instance segmentation.",0
"We introduce a module that offers a versatile and adaptable method for encoding region proposals, utilizing both intrinsic features and extrinsic correlations with other regions. Our non-local region of interest (NL-RoI) can be integrated into various generalized R-CNN architectures, making it an ideal solution for a variety of perception tasks. Current R-CNN techniques treat RoIs as separate entities, relying solely on image features within each region proposal to make predictions. However, the relationships between proposals can provide valuable information for detection and segmentation. We designed NL-RoI to enhance the representation of each RoI by incorporating information from all other RoIs, resulting in a simple, cost-effective, yet effective module for region-based convolutional networks. Our experiments demonstrate that NL-RoI can improve the performance of Faster/Mask R-CNN in object detection and instance segmentation.",1
"Self-supervised tasks such as colorization, inpainting and zigsaw puzzle have been utilized for visual representation learning for still images, when the number of labeled images is limited or absent at all. Recently, this worthwhile stream of study extends to video domain where the cost of human labeling is even more expensive. However, the most of existing methods are still based on 2D CNN architectures that can not directly capture spatio-temporal information for video applications. In this paper, we introduce a new self-supervised task called as \textit{Space-Time Cubic Puzzles} to train 3D CNNs using large scale video dataset. This task requires a network to arrange permuted 3D spatio-temporal crops. By completing \textit{Space-Time Cubic Puzzles}, the network learns both spatial appearance and temporal relation of video frames, which is our final goal. In experiments, we demonstrate that our learned 3D representation is well transferred to action recognition tasks, and outperforms state-of-the-art 2D CNN-based competitors on UCF101 and HMDB51 datasets.",0
"Self-supervised tasks such as colorization, inpainting, and jigsaw puzzles have been utilized to learn visual representations for still images when labeled images are limited or unavailable. This approach has recently been extended to the video domain, where labeling costs are even higher. However, most current methods are based on 2D CNN architectures, which cannot directly capture spatio-temporal information for video applications. In this study, we introduce a new self-supervised task called ""Space-Time Cubic Puzzles"" that trains 3D CNNs using large-scale video datasets. This task requires the network to arrange permuted 3D spatio-temporal crops. By completing Space-Time Cubic Puzzles, the network learns both the spatial appearance and temporal relation of video frames, which is our ultimate goal. Our experiments demonstrate that our learned 3D representation transfers well to action recognition tasks and outperforms state-of-the-art 2D CNN-based approaches on UCF101 and HMDB51 datasets.",1
"Adversarial approach has been widely used for data generation in the last few years. However, this approach has not been extensively utilized for classifier training. In this paper, we propose an adversarial framework for classifier training that can also handle imbalanced data. Indeed, a network is trained via an adversarial approach to give weights to samples of the majority class such that the obtained classification problem becomes more challenging for the discriminator and thus boosts its classification capability. In addition to the general imbalanced classification problems, the proposed method can also be used for problems such as graph representation learning in which it is desired to discriminate similar nodes from dissimilar nodes. Experimental results on imbalanced data classification and on the tasks like graph link prediction show the superiority of the proposed method compared to the state-of-the-art methods.",0
"Over the past few years, the adversarial approach has been commonly utilized for data generation, but its use in training classifiers has been limited. This paper introduces an adversarial framework for classifier training that addresses imbalanced data. Through this framework, a network is trained adversarially to assign weights to samples of the majority class, resulting in a more challenging classification problem for the discriminator and ultimately improving its classification ability. This method is not only applicable to general imbalanced classification problems, but also to tasks like graph representation learning, where discrimination between similar and dissimilar nodes is desired. Our experiments on imbalanced data classification and graph link prediction demonstrate the effectiveness of our proposed method, which outperforms state-of-the-art methods.",1
"Joint embeddings between medical imaging modalities and associated radiology reports have the potential to offer significant benefits to the clinical community, ranging from cross-domain retrieval to conditional generation of reports to the broader goals of multimodal representation learning. In this work, we establish baseline joint embedding results measured via both local and global retrieval methods on the soon to be released MIMIC-CXR dataset consisting of both chest X-ray images and the associated radiology reports. We examine both supervised and unsupervised methods on this task and show that for document retrieval tasks with the learned representations, only a limited amount of supervision is needed to yield results comparable to those of fully-supervised methods.",0
"The integration of medical imaging modalities and corresponding radiology reports through joint embeddings can bring substantial advantages to the clinical community, including cross-domain retrieval, conditional report generation, and multimodal representation learning. Our study focuses on establishing baseline joint embedding outcomes using local and global retrieval techniques for the forthcoming MIMIC-CXR dataset, which comprises chest X-ray images and their corresponding radiology reports. We investigate supervised and unsupervised methods for this purpose and demonstrate that minimal supervision is sufficient to achieve comparable results to fully-supervised methods for document retrieval tasks utilizing the learned representations.",1
"Representation learning of pedestrian trajectories transforms variable-length timestamp-coordinate tuples of a trajectory into a fixed-length vector representation that summarizes spatiotemporal characteristics. It is a crucial technique to connect feature-based data mining with trajectory data. Trajectory representation is a challenging problem, because both environmental constraints (e.g., wall partitions) and temporal user dynamics should be meticulously considered and accounted for. Furthermore, traditional sequence-to-sequence autoencoders using maximum log-likelihood often require dataset covering all the possible spatiotemporal characteristics to perform well. This is infeasible or impractical in reality. We propose TREP, a practical pedestrian trajectory representation learning algorithm which captures the environmental constraints and the pedestrian dynamics without the need of any training dataset. By formulating a sequence-to-sequence autoencoder with a spatial-aware objective function under the paradigm of actor-critic reinforcement learning, TREP intelligently encodes spatiotemporal characteristics of trajectories with the capability of handling diverse trajectory patterns. Extensive experiments on both synthetic and real datasets validate the high fidelity of TREP to represent trajectories.",0
"Pedestrian trajectory representation learning is a technique that transforms variable-length timestamp-coordinate tuples of a trajectory into a fixed-length vector representation to summarize spatiotemporal characteristics. This technique is crucial for connecting feature-based data mining with trajectory data. However, it is challenging to represent trajectories due to environmental constraints and temporal user dynamics. Traditional sequence-to-sequence autoencoders using maximum log-likelihood require a dataset covering all possible spatiotemporal characteristics, which is impractical in reality. To address this issue, we propose TREP, a practical pedestrian trajectory representation learning algorithm that captures environmental constraints and pedestrian dynamics without a training dataset. TREP formulates a sequence-to-sequence autoencoder with a spatial-aware objective function using actor-critic reinforcement learning. This approach intelligently encodes spatiotemporal characteristics of trajectories, making it capable of handling diverse trajectory patterns. Extensive experiments on both synthetic and real datasets validate the high fidelity of TREP in representing trajectories.",1
"Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections. Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.",0
"Learning representations of graph vertices is a crucial task, and Graph Convolutional Networks (GCNs) are a valuable tool for achieving this. However, when dealing with large-scale graphs, GCNs face a significant challenge in terms of scalability. The issue lies in the substantial computational and memory cost incurred due to uncontrolled neighborhood expansion across layers. In this paper, we present a solution to this problem by introducing an adaptive layer-wise sampling method that accelerates GCN training. Our approach involves building the network layer by layer in a top-down manner and sampling the lower layer based on the top one. This ensures that the sampled neighborhoods are shared by different parent nodes, and fixed-size sampling prevents over-expansion. Additionally, our proposed sampler is adaptive and can effectively reduce variance, improving the training process. Furthermore, we introduce an economical approach to promote message passing over distant nodes by applying skip connections. Our method achieves faster convergence speed while maintaining high classification accuracy, as verified by intensive experiments on several benchmarks.",1
"Building agents that can explore their environments intelligently is a challenging open problem. In this paper, we make a step towards understanding how a hierarchical design of the agent's policy can affect its exploration capabilities. First, we design EscapeRoom environments, where the agent must figure out how to navigate to the exit by accomplishing a number of intermediate tasks (\emph{subgoals}), such as finding keys or opening doors. Our environments are procedurally generated and vary in complexity, which can be controlled by the number of subgoals and relationships between them. Next, we propose to measure the complexity of each environment by constructing dependency graphs between the goals and analytically computing \emph{hitting times} of a random walk in the graph. We empirically evaluate Proximal Policy Optimization (PPO) with sparse and shaped rewards, a variation of policy sketches, and a hierarchical version of PPO (called HiPPO) akin to h-DQN. We show that analytically estimated \emph{hitting time} in goal dependency graphs is an informative metric of the environment complexity. We conjecture that the result should hold for environments other than navigation. Finally, we show that solving environments beyond certain level of complexity requires hierarchical approaches.",0
"Developing intelligent agents that can effectively navigate their environment is a difficult problem that remains unsolved. This article aims to explore the impact of a hierarchical policy design on an agent's ability to explore their surroundings. To do so, we have created EscapeRoom environments that require the agent to complete various tasks, or subgoals, to reach the final goal of reaching the exit. These environments are procedurally generated and can be adjusted in complexity based on the number of subgoals and their relationships. We propose using dependency graphs between goals and calculating the hitting times of a random walk in the graph to assess environment complexity. We evaluate various reinforcement learning techniques, including sparse and shaped rewards, policy sketches, and a hierarchical version of Proximal Policy Optimization (PPO) called HiPPO. Our results demonstrate that the hitting time is an informative metric for assessing environment complexity, and that hierarchical approaches are necessary for solving environments beyond a certain level of complexity. We believe that this finding is relevant beyond navigation-based environments.",1
"Sufficient physical activity and restful sleep play a major role in the prevention and cure of many chronic conditions. Being able to proactively screen and monitor such chronic conditions would be a big step forward for overall health. The rapid increase in the popularity of wearable devices provides a significant new source, making it possible to track the user's lifestyle real-time. In this paper, we propose a novel unsupervised representation learning technique called activity2vec that learns and ""summarizes"" the discrete-valued activity time-series. It learns the representations with three components: (i) the co-occurrence and magnitude of the activity levels in a time-segment, (ii) neighboring context of the time-segment, and (iii) promoting subject-invariance with adversarial training. We evaluate our method on four disorder prediction tasks using linear classifiers. Empirical evaluation demonstrates that our proposed method scales and performs better than many strong baselines. The adversarial regime helps improve the generalizability of our representations by promoting subject invariant features. We also show that using the representations at the level of a day works the best since human activity is structured in terms of daily routines",0
"The prevention and treatment of various chronic conditions heavily rely on adequate physical activity and restful sleep, and the ability to proactively screen and monitor such conditions would be a significant advancement for overall health. The surge in popularity of wearable devices offers a promising new source, allowing for real-time tracking of a user's lifestyle. This paper introduces a novel unsupervised representation learning approach called activity2vec, which summarizes discrete-valued activity time-series. The method involves learning representations that consider the co-occurrence and magnitude of activity levels within a time-segment, the neighboring context of the time-segment, and promoting subject-invariance through adversarial training. The effectiveness of the proposed method is evaluated on four disorder prediction tasks using linear classifiers, and it outperforms several strong baselines. The adversarial regime enhances the generalizability of the representations by promoting subject invariant features. Moreover, using the representations at the daily level is found to work best due to the structured nature of human activity in terms of daily routines.",1
"We introduce GSimCNN (Graph Similarity Computation via Convolutional Neural Networks) for predicting the similarity score between two graphs. As the core operation of graph similarity search, pairwise graph similarity computation is a challenging problem due to the NP-hard nature of computing many graph distance/similarity metrics. We demonstrate our model using the Graph Edit Distance (GED) as the example metric. Experiments on three real graph datasets demonstrate that our model achieves the state-of-the-art performance on graph similarity search.",0
"Our latest innovation, GSimCNN (Graph Similarity Computation via Convolutional Neural Networks), is designed to anticipate the similarity rating of two graphs. The calculation of pairwise graph similarity is a complex task, as the NP-hard nature of computing several graph distance/similarity metrics presents a challenge. We have selected Graph Edit Distance (GED) as an illustration metric to demonstrate our model. Through experiments conducted on three genuine graph datasets, we have established that our model achieves the best results for graph similarity search.",1
"Graph classification has recently received a lot of attention from various fields of machine learning e.g. kernel methods, sequential modeling or graph embedding. All these approaches offer promising results with different respective strengths and weaknesses. However, most of them rely on complex mathematics and require heavy computational power to achieve their best performance. We propose a simple and fast algorithm based on the spectral decomposition of graph Laplacian to perform graph classification and get a first reference score for a dataset. We show that this method obtains competitive results compared to state-of-the-art algorithms.",0
"Various fields of machine learning, including kernel methods, sequential modeling, and graph embedding, have recently shown a lot of interest in graph classification. Although these approaches offer promising results with varying strengths and weaknesses, they often rely on complex mathematical concepts and require significant computational power to achieve optimal performance. This paper proposes a simple and fast algorithm that relies on the spectral decomposition of graph Laplacian to perform graph classification and generate an initial reference score for a given dataset. Our results demonstrate that this method achieves comparable outcomes to state-of-the-art algorithms.",1
"In this work we explore the generalization characteristics of unsupervised representation learning by leveraging disentangled VAE's to learn a useful latent space on a set of relational reasoning problems derived from Raven Progressive Matrices. We show that the latent representations, learned by unsupervised training using the right objective function, significantly outperform the same architectures trained with purely supervised learning, especially when it comes to generalization.",0
"This study investigates the capacity of unsupervised representation learning to generalize by utilizing disentangled VAE's to acquire a valuable latent space on a collection of relational reasoning problems based on Raven Progressive Matrices. Our findings demonstrate that the latent representations obtained through unsupervised training with the appropriate objective function achieve considerably superior performance as compared to those trained solely with supervised learning, especially in terms of generalization.",1
"Extracting actionable insight from Electronic Health Records (EHRs) poses several challenges for traditional machine learning approaches. Patients are often missing data relative to each other; the data comes in a variety of modalities, such as multivariate time series, free text, and categorical demographic information; important relationships among patients can be difficult to detect; and many others. In this work, we propose a novel approach to address these first three challenges using a representation learning scheme based on message passing. We show that our proposed approach is competitive with or outperforms the state of the art for predicting in-hospital mortality (binary classification), the length of hospital visits (regression) and the discharge destination (multiclass classification).",0
"Traditional machine learning approaches face several challenges when extracting actionable insight from Electronic Health Records (EHRs). These challenges include missing data among patients, varying data modalities such as multivariate time series, free text, and categorical demographic information, and difficulty in detecting important relationships among patients. Our work proposes a unique approach to tackle these challenges using a message passing-based representation learning scheme. Our proposed approach demonstrates competitiveness with and sometimes outperforms the state of the art in predicting binary classification of in-hospital mortality, regression in length of hospital visits, and multiclass classification of discharge destination.",1
"We introduce a new scene graph generation method called image-level attentional context modeling (ILAC). Our model includes an attentional graph network that effectively propagates contextual information across the graph using image-level features. Whereas previous works use an object-centric context, we build an image-level context agent to encode the scene properties. The proposed method comprises a single-stream network that iteratively refines the scene graph with a nested graph neural network. We demonstrate that our approach achieves competitive performance with the state-of-the-art for scene graph generation on the Visual Genome dataset, while requiring fewer parameters than other methods. We also show that ILAC can improve regular object detectors by incorporating relational image-level information.",0
"We present a novel technique for generating scene graphs, referred to as image-level attentional context modeling (ILAC). Our method entails incorporating contextual information into a graph network through image-level features via an attentional mechanism. Unlike previous approaches that utilize object-centric context, we introduce an image-level context agent to encode scene properties. Our proposed approach employs a single-stream network that iteratively enhances the scene graph with a nested graph neural network. Our results demonstrate that ILAC performs comparably to the current state-of-the-art for scene graph generation on the Visual Genome dataset, with fewer parameters than other methods. Furthermore, we demonstrate that ILAC can enhance traditional object detectors by integrating relational image-level information.",1
"While neural networks for learning representation of multi-view data have been previously proposed as one of the state-of-the-art multi-view dimension reduction techniques, how to make the representation discriminative with only a small amount of labeled data is not well-studied. We introduce a semi-supervised neural network model, named Multi-view Discriminative Neural Network (MDNN), for multi-view problems. MDNN finds nonlinear view-specific mappings by projecting samples to a common feature space using multiple coupled deep networks. It is capable of leveraging both labeled and unlabeled data to project multi-view data so that samples from different classes are separated and those from the same class are clustered together. It also uses the inter-view correlation between views to exploit the available information in both the labeled and unlabeled data. Extensive experiments conducted on four datasets demonstrate the effectiveness of the proposed algorithm for multi-view semi-supervised learning.",0
"Previous proposals for using neural networks to learn representations of multi-view data have been among the leading multi-view dimension reduction techniques. However, the issue of making the resulting representation discriminative with a small amount of labeled data has not been thoroughly studied. Our solution is the introduction of a semi-supervised neural network model called the Multi-view Discriminative Neural Network (MDNN) for multi-view problems. MDNN employs multiple coupled deep networks to discover nonlinear view-specific mappings by projecting samples to a common feature space. It can utilize both labeled and unlabeled data to project multi-view data in a way that separates samples from different classes and clusters those from the same class. Additionally, it leverages inter-view correlation between views to exploit available information from both the labeled and unlabeled data. We conducted extensive experiments on four datasets, which demonstrate the effectiveness of the proposed algorithm for multi-view semi-supervised learning.",1
"We tackle the blackbox issue of deep neural networks in the settings of reinforcement learning (RL) where neural agents learn towards maximizing reward gains in an uncontrollable way. Such learning approach is risky when the interacting environment includes an expanse of state space because it is then almost impossible to foresee all unwanted outcomes and penalize them with negative rewards beforehand. Unlike reverse analysis of learned neural features from previous works, our proposed method \nj{tackles the blackbox issue by encouraging} an RL policy network to learn interpretable latent features through an implementation of a disentangled representation learning method. Toward this end, our method allows an RL agent to understand self-efficacy by distinguishing its influences from uncontrollable environmental factors, which closely resembles the way humans understand their scenes. Our experimental results show that the learned latent factors not only are interpretable, but also enable modeling the distribution of entire visited state space with a specific action condition. We have experimented that this characteristic of the proposed structure can lead to ex post facto governance for desired behaviors of RL agents.",0
"In the realm of reinforcement learning (RL), where neural agents strive to maximize reward gains in an unpredictable manner, we address the issue of blackbox neural networks. This learning approach poses a risk when the environment being interacted with is vast, making it nearly impossible to anticipate all unfavorable outcomes and penalize them with negative rewards beforehand. Our proposed method differs from previous works that analyze learned neural features in reverse; we tackle the blackbox issue by promoting the learning of interpretable latent features by an RL policy network through a disentangled representation learning technique. This approach allows the RL agent to comprehend self-efficacy by distinguishing its impact from uncontrollable environmental factors, which mirrors human understanding of their surroundings. Our experimental findings demonstrate that the learned latent factors are not only interpretable but also enable the modeling of the distribution of the entire visited state space with a specific action condition. We have established that this characteristic of the proposed structure can provide ex post facto governance for desired RL agent behaviors.",1
"We tackle the problem of audiovisual scene analysis for weakly-labeled data. To this end, we build upon our previous audiovisual representation learning framework to perform object classification in noisy acoustic environments and integrate audio source enhancement capability. This is made possible by a novel use of non-negative matrix factorization for the audio modality. Our approach is founded on the multiple instance learning paradigm. Its effectiveness is established through experiments over a challenging dataset of music instrument performance videos. We also show encouraging visual object localization results.",0
"Our focus is on addressing the issue of analyzing audiovisual scenes when dealing with data that has weak labeling. In order to achieve this, we enhance our existing framework for audiovisual representation learning. This allows us to classify objects in noisy acoustic environments and improve the quality of audio sources. By using non-negative matrix factorization for the audio aspect, we can accomplish this. Our method is based on the concept of multiple instance learning and we prove its effectiveness through experiments on a difficult dataset of music instrument performance videos. We also demonstrate promising outcomes in visual object localization.",1
"The key to success in machine learning (ML) is the use of effective data representations. Traditionally, data representations were hand-crafted. Recently it has been demonstrated that, given sufficient data, deep neural networks can learn effective implicit representations from simple input representations. However, for most scientific problems, the use of deep learning is not appropriate as the amount of available data is limited, and/or the output models must be explainable. Nevertheless, many scientific problems do have significant amounts of data available on related tasks, which makes them amenable to multi-task learning, i.e. learning many related problems simultaneously. Here we propose a novel and general representation learning approach for multi-task learning that works successfully with small amounts of data. The fundamental new idea is to transform an input intrinsic data representation (i.e., handcrafted features), to an extrinsic representation based on what a pre-trained set of models predict about the examples. This transformation has the dual advantages of producing significantly more accurate predictions, and providing explainable models. To demonstrate the utility of this transformative learning approach, we have applied it to three real-world scientific problems: drug-design (quantitative structure activity relationship learning), predicting human gene expression (across different tissue types and drug treatments), and meta-learning for machine learning (predicting which machine learning methods work best for a given problem). In all three problems, transformative machine learning significantly outperforms the best intrinsic representation.",0
"Effective data representations are crucial to achieving success in machine learning (ML). Traditionally, these representations were created manually. However, recent advances have shown that deep neural networks can learn implicit representations from simple input representations with enough data. Nevertheless, deep learning is not always suitable for scientific problems due to a lack of available data or the need for explainable models. Multi-task learning is a promising solution, as many scientific problems have significant amounts of data available on related tasks. This paper proposes a novel representation learning approach for multi-task learning that works well with small amounts of data. The approach transforms intrinsic data representations into extrinsic ones based on pre-trained models' predictions, providing more accurate predictions and explainable models. The approach was applied to three real-world scientific problems, including drug design, predicting human gene expression, and meta-learning for machine learning. The results demonstrate the effectiveness of transformative machine learning over the best intrinsic representation.",1
"We examine two fundamental tasks associated with graph representation learning: link prediction and node classification. We present a new autoencoder architecture capable of learning a joint representation of local graph structure and available node features for the simultaneous multi-task learning of unsupervised link prediction and semi-supervised node classification. Our simple, yet effective and versatile model is efficiently trained end-to-end in a single stage, whereas previous related deep graph embedding methods require multiple training steps that are difficult to optimize. We provide an empirical evaluation of our model on five benchmark relational, graph-structured datasets and demonstrate significant improvement over three strong baselines for graph representation learning. Reference code and data are available at https://github.com/vuptran/graph-representation-learning",0
"In this paper, we explore two essential activities related to graph representation learning: node classification and link prediction. To tackle these challenges, we introduce a novel autoencoder architecture that can simultaneously learn the local graph structure and available node features. Our model can perform both unsupervised link prediction and semi-supervised node classification. Unlike previous deep graph embedding techniques, our model is straightforward but still effective and adaptable. Moreover, we can train our model end-to-end in a single stage, which is more efficient than the multi-stage training required by other methods. We assess the performance of our model on five benchmark relational, graph-structured datasets, and compare it with three robust baselines. Our model shows significant improvement compared to the baselines. To facilitate further research, we make our code and data available at https://github.com/vuptran/graph-representation-learning.",1
"A recent method employs 3D voxels to represent 3D shapes, but this limits the approach to low resolutions due to the computational cost caused by the cubic complexity of 3D voxels. Hence the method suffers from a lack of detailed geometry. To resolve this issue, we propose Y^2Seq2Seq, a view-based model, to learn cross-modal representations by joint reconstruction and prediction of view and word sequences. Specifically, the network architecture of Y^2Seq2Seq bridges the semantic meaning embedded in the two modalities by two coupled `Y' like sequence-to-sequence (Seq2Seq) structures. In addition, our novel hierarchical constraints further increase the discriminability of the cross-modal representations by employing more detailed discriminative information. Experimental results on cross-modal retrieval and 3D shape captioning show that Y^2Seq2Seq outperforms the state-of-the-art methods.",0
"A new technique has been developed that uses 3D voxels to represent 3D shapes. However, this method has limitations when it comes to high resolutions due to the computational cost associated with the cubic complexity of 3D voxels. As a result, the approach lacks detailed geometry. To address this problem, we propose Y^2Seq2Seq, a view-based model that learns cross-modal representations by jointly reconstructing and predicting view and word sequences. The Y^2Seq2Seq network architecture connects the semantic meaning of the two modalities through two coupled `Y' like sequence-to-sequence (Seq2Seq) structures. Furthermore, our new hierarchical constraints increase the discriminability of the cross-modal representations by incorporating more detailed discriminative information. Our experiments on cross-modal retrieval and 3D shape captioning demonstrate that Y^2Seq2Seq is superior to the existing state-of-the-art methods.",1
"In this paper we present a novel unsupervised representation learning approach for 3D shapes, which is an important research challenge as it avoids the manual effort required for collecting supervised data. Our method trains an RNN-based neural network architecture to solve multiple view inter-prediction tasks for each shape. Given several nearby views of a shape, we define view inter-prediction as the task of predicting the center view between the input views, and reconstructing the input views in a low-level feature space. The key idea of our approach is to implement the shape representation as a shape-specific global memory that is shared between all local view inter-predictions for each shape. Intuitively, this memory enables the system to aggregate information that is useful to better solve the view inter-prediction tasks for each shape, and to leverage the memory as a view-independent shape representation. Our approach obtains the best results using a combination of L_2 and adversarial losses for the view inter-prediction task. We show that VIP-GAN outperforms state-of-the-art methods in unsupervised 3D feature learning on three large scale 3D shape benchmarks.",0
"The paper introduces a new method for unsupervised representation learning of 3D shapes, a challenge that eliminates the need for manual collection of supervised data. The proposed approach involves training an RNN-based neural network to perform multiple view inter-prediction tasks for each shape, where the center view is predicted and the input views are reconstructed in a low-level feature space. To achieve this, a shape-specific global memory is utilized, which aggregates useful information to solve the view inter-prediction task and serves as a view-independent shape representation. The approach employs a combination of L_2 and adversarial losses to achieve optimal results, outperforming existing methods in unsupervised 3D feature learning on three large scale 3D shape benchmarks.",1
"Since its introduction, unsupervised representation learning has attracted a lot of attention from the research community, as it is demonstrated to be highly effective and easy-to-apply in tasks such as dimension reduction, clustering, visualization, information retrieval, and semi-supervised learning. In this work, we propose a novel unsupervised representation learning framework called neighbor-encoder, in which domain knowledge can be easily incorporated into the learning process without modifying the general encoder-decoder architecture of the classic autoencoder.In contrast to autoencoder, which reconstructs the input data itself, neighbor-encoder reconstructs the input data's neighbors. As the proposed representation learning problem is essentially a neighbor reconstruction problem, domain knowledge can be easily incorporated in the form of an appropriate definition of similarity between objects. Based on that observation, our framework can leverage any off-the-shelf similarity search algorithms or side information to find the neighbor of an input object. Applications of other algorithms (e.g., association rule mining) in our framework are also possible, given that the appropriate definition of neighbor can vary in different contexts. We have demonstrated the effectiveness of our framework in many diverse domains, including images, text, and time series, and for various data mining tasks including classification, clustering, and visualization. Experimental results show that neighbor-encoder not only outperforms autoencoder in most of the scenarios we consider, but also achieves the state-of-the-art performance on text document clustering.",0
"Unsupervised representation learning has gained significant attention from researchers due to its effectiveness and simplicity in tasks such as clustering, dimension reduction, visualization, information retrieval, and semi-supervised learning. This study proposes a new unsupervised representation learning framework, called neighbor-encoder, which allows easy integration of domain knowledge into the learning process without modifying the general encoder-decoder architecture of the classic autoencoder. Unlike autoencoder, which reconstructs the input data, neighbor-encoder reconstructs the input data's neighbors, making it easier to incorporate domain knowledge in the form of appropriate object similarity definitions. The framework can utilize any off-the-shelf similarity search algorithms or side information to find the neighbors of an input object. The proposed framework has been demonstrated to be effective in various domains, including images, text, and time series, and for different data mining tasks, including clustering, classification, and visualization. Experimental results show that in most scenarios, the proposed approach outperforms autoencoder and achieves state-of-the-art performance in text document clustering.",1
"Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution. There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control. In this paper, we bridge these two objectives and introduce the concept of cross-domain disentanglement. We aim to separate the internal representation into three parts. The shared part contains information for both domains. The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain. We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. Our model offers multiple advantages. We can output diverse samples covering multiple modes of the distributions of both domains, perform domain-specific image transfer and interpolation, and cross-domain retrieval without the need of labeled data, only paired images. We compare our model to the state-of-the-art in multi-modal image translation and achieve better results for translation on challenging datasets as well as for cross-domain retrieval on realistic datasets.",0
"Recent advancements in deep image translation techniques have exhibited exceptional outcomes by generating top-notch images that encompass various modes of data distribution. Moreover, there has been growing interest in disentangling the internal representations acquired through deep learning methods to enhance their performance and achieve precise control. This paper aims to unite these two objectives by introducing the concept of cross-domain disentanglement. The objective is to segregate the internal representation into three components, where the shared component contains information for both domains, and the exclusive components contain only the specific factors of variation for each domain. We accomplish this through bidirectional image translation using Generative Adversarial Networks and cross-domain autoencoders, a new network component. Our model offers several benefits such as diverse sample generation, domain-specific image transfer and interpolation, and cross-domain retrieval without requiring labeled data but only paired images. We compare our model to the state-of-the-art in multi-modal image translation and achieve better results for translation on challenging datasets, as well as for cross-domain retrieval on realistic datasets.",1
"Intrinsically motivated goal exploration processes enable agents to autonomously sample goals to explore efficiently complex environments with high-dimensional continuous actions. They have been applied successfully to real world robots to discover repertoires of policies producing a wide diversity of effects. Often these algorithms relied on engineered goal spaces but it was recently shown that one can use deep representation learning algorithms to learn an adequate goal space in simple environments. However, in the case of more complex environments containing multiple objects or distractors, an efficient exploration requires that the structure of the goal space reflects the one of the environment. In this paper we show that using a disentangled goal space leads to better exploration performances than an entangled goal space. We further show that when the representation is disentangled, one can leverage it by sampling goals that maximize learning progress in a modular manner. Finally, we show that the measure of learning progress, used to drive curiosity-driven exploration, can be used simultaneously to discover abstract independently controllable features of the environment.",0
"Agents can explore complex environments with high-dimensional continuous actions efficiently through intrinsically motivated goal exploration processes. These processes have been successfully applied to real world robots, resulting in a diverse range of policies. Although previous algorithms relied on engineered goal spaces, recent studies have shown that deep representation learning algorithms can learn an adequate goal space in simple environments. However, in more complex environments, efficient exploration requires a goal space that reflects the environment's structure. This study demonstrates that using a disentangled goal space leads to better exploration performances than an entangled goal space. Additionally, a disentangled representation allows for modular sampling of goals that maximize learning progress. The measure of learning progress used for curiosity-driven exploration can also be used to discover abstract and independently controllable features of the environment simultaneously.",1
"Recent advances in representation learning on graphs, mainly leveraging graph convolutional networks, have brought a substantial improvement on many graph-based benchmark tasks. While novel approaches to learning node embeddings are highly suitable for node classification and link prediction, their application to graph classification (predicting a single label for the entire graph) remains mostly rudimentary, typically using a single global pooling step to aggregate node features or a hand-designed, fixed heuristic for hierarchical coarsening of the graph structure. An important step towards ameliorating this is differentiable graph coarsening---the ability to reduce the size of the graph in an adaptive, data-dependent manner within a graph neural network pipeline, analogous to image downsampling within CNNs. However, the previous prominent approach to pooling has quadratic memory requirements during training and is therefore not scalable to large graphs. Here we combine several recent advances in graph neural network design to demonstrate that competitive hierarchical graph classification results are possible without sacrificing sparsity. Our results are verified on several established graph classification benchmarks, and highlight an important direction for future research in graph-based neural networks.",0
"Significant progress has been made in representation learning on graphs, particularly through the use of graph convolutional networks. These new methods have greatly enhanced the performance of graph-based benchmark tasks, especially for node classification and link prediction. However, they are less developed for graph classification, which involves predicting a single label for the entire graph. Current approaches often rely on a single global pooling step or a predetermined heuristic for coarsening the graph structure. To improve this, differentiable graph coarsening is needed to reduce graph size in an adaptive, data-dependent manner. However, previous pooling methods have memory limitations that make them unsuitable for large graphs. Our study combines recent advances in graph neural network design to show that high-quality hierarchical graph classification can be achieved while maintaining sparsity. Our findings were validated across multiple graph classification benchmarks and underscore the need for continued research in graph-based neural networks.",1
"The success and generalisation of deep learning algorithms heavily depend on learning good feature representations. In medical imaging this entails representing anatomical information, as well as properties related to the specific imaging setting. Anatomical information is required to perform further analysis, whereas imaging information is key to disentangle scanner variability and potential artefacts. The ability to factorise these would allow for training algorithms only on the relevant information according to the task. To date, such factorisation has not been attempted. In this paper, we propose a methodology of latent space factorisation relying on the cycle-consistency principle. As an example application, we consider cardiac MR segmentation, where we separate information related to the myocardium from other features related to imaging and surrounding substructures. We demonstrate the proposed method's utility in a semi-supervised setting: we use very few labelled images together with many unlabelled images to train a myocardium segmentation neural network. Specifically, we achieve comparable performance to fully supervised networks using a fraction of labelled images in experiments on ACDC and a dataset from Edinburgh Imaging Facility QMRI. Code will be made available at https://github.com/agis85/spatial_factorisation.",0
"The effectiveness and applicability of deep learning algorithms rely heavily on acquiring high-quality feature representations. In the field of medical imaging, this involves capturing anatomical details and properties specific to the imaging scenario. While anatomical information is essential for further analysis, imaging information is crucial for distinguishing scanner variations and potential distortions. If we can separate these two types of information, it would enable us to train algorithms specifically for the task at hand. However, no such separation has been attempted so far. This paper proposes a latent space factorization approach based on the cycle-consistency principle. We demonstrate its effectiveness through an example application of cardiac MR segmentation, where we segregate myocardium-related information from other imaging and surrounding substructure features. Using very few labeled images and many unlabeled images, we show that the proposed method can achieve comparable performance to fully supervised networks in a semi-supervised setting. Our experiments on ACDC and a dataset from Edinburgh Imaging Facility QMRI validate the efficacy of the proposed approach. The code for this methodology will be available at https://github.com/agis85/spatial_factorisation.",1
"Fake news are nowadays an issue of pressing concern, given their recent rise as a potential threat to high-quality journalism and well-informed public discourse. The Fake News Challenge (FNC-1) was organized in 2017 to encourage the development of machine learning-based classification systems for stance detection (i.e., for identifying whether a particular news article agrees, disagrees, discusses, or is unrelated to a particular news headline), thus helping in the detection and analysis of possible instances of fake news. This article presents a new approach to tackle this stance detection problem, based on the combination of string similarity features with a deep neural architecture that leverages ideas previously advanced in the context of learning efficient text representations, document classification, and natural language inference. Specifically, we use bi-directional Recurrent Neural Networks, together with max-pooling over the temporal/sequential dimension and neural attention, for representing (i) the headline, (ii) the first two sentences of the news article, and (iii) the entire news article. These representations are then combined/compared, complemented with similarity features inspired on other FNC-1 approaches, and passed to a final layer that predicts the stance of the article towards the headline. We also explore the use of external sources of information, specifically large datasets of sentence pairs originally proposed for training and evaluating natural language inference methods, in order to pre-train specific components of the neural network architecture (e.g., the RNNs used for encoding sentences). The obtained results attest to the effectiveness of the proposed ideas and show that our model, particularly when considering pre-training and the combination of neural representations together with similarity features, slightly outperforms the previous state-of-the-art.",0
"The rise of fake news has become a major concern for high-quality journalism and informed public discourse. In response, the Fake News Challenge (FNC-1) was created in 2017 to encourage the development of machine learning-based systems for detecting the stance of news articles towards a particular headline. This article presents a new approach to this problem by combining string similarity features with a deep neural architecture. Bi-directional Recurrent Neural Networks, max-pooling, and neural attention are used to represent the headline, the first two sentences of the article, and the entire article. These representations are then compared and passed to a final layer that predicts the article's stance towards the headline. External sources of information, such as large datasets for natural language inference, are also explored for pre-training specific components of the neural network architecture. The results show that this approach outperforms the previous state-of-the-art, particularly when considering pre-training and the combination of neural representations with similarity features.",1
"A feature learning task involves training models that are capable of inferring good representations (transformations of the original space) from input data alone. When working with limited or unlabelled data, and also when multiple visual domains are considered, methods that rely on large annotated datasets, such as Convolutional Neural Networks (CNNs), cannot be employed. In this paper we investigate different auto-encoder (AE) architectures, which require no labels, and explore training strategies to learn representations from images. The models are evaluated considering both the reconstruction error of the images and the feature spaces in terms of their discriminative power. We study the role of dense and convolutional layers on the results, as well as the depth and capacity of the networks, since those are shown to affect both the dimensionality reduction and the capability of generalising for different visual domains. Classification results with AE features were as discriminative as pre-trained CNN features. Our findings can be used as guidelines for the design of unsupervised representation learning methods within and across domains.",0
"The task of feature learning involves training models to generate effective representations of input data without any labels. When dealing with limited or unlabelled data, and multiple visual domains, traditional methods like Convolutional Neural Networks (CNNs) that rely on large annotated datasets are not suitable. This study examines various auto-encoder (AE) architectures, which require no labels, and explores training techniques to learn representations from images. The models are evaluated based on the reconstruction error of the images and the feature spaces in terms of their discriminative power. The study investigates the impact of dense and convolutional layers, network depth, and capacity on the dimensionality reduction and generalization for different visual domains. The study shows that AE features perform as well as pre-trained CNN features in classification tasks. The results provide useful guidance for designing unsupervised learning methods for representation learning within and across domains.",1
"The explosive growth of digital images in video surveillance and social media has led to the significant need for efficient search of persons of interest in law enforcement and forensic applications. Despite tremendous progress in primary biometric traits (e.g., face and fingerprint) based person identification, a single biometric trait alone cannot meet the desired recognition accuracy in forensic scenarios. Tattoos, as one of the important soft biometric traits, have been found to be valuable for assisting in person identification. However, tattoo search in a large collection of unconstrained images remains a difficult problem, and existing tattoo search methods mainly focus on matching cropped tattoos, which is different from real application scenarios. To close the gap, we propose an efficient tattoo search approach that is able to learn tattoo detection and compact representation jointly in a single convolutional neural network (CNN) via multi-task learning. While the features in the backbone network are shared by both tattoo detection and compact representation learning, individual latent layers of each sub-network optimize the shared features toward the detection and feature learning tasks, respectively. We resolve the small batch size issue inside the joint tattoo detection and compact representation learning network via random image stitch and preceding feature buffering. We evaluate the proposed tattoo search system using multiple public-domain tattoo benchmarks, and a gallery set with about 300K distracter tattoo images compiled from these datasets and images from the Internet. In addition, we also introduce a tattoo sketch dataset containing 300 tattoos for sketch-based tattoo search. Experimental results show that the proposed approach has superior performance in tattoo detection and tattoo search at scale compared to several state-of-the-art tattoo retrieval algorithms.",0
"The significant increase in digital images in video surveillance and social media has created a need for efficient search methods to locate individuals of interest in law enforcement and forensic applications. While primary biometric traits such as face and fingerprint identification have made great progress, they cannot achieve the desired recognition accuracy in forensic scenarios alone. Soft biometric traits, such as tattoos, have proven valuable in aiding person identification. However, searching for tattoos in a large collection of unconstrained images remains challenging, and current methods focus on matching cropped tattoos, which is not representative of real-life scenarios. To address this issue, we propose a tattoo search method that utilizes a single convolutional neural network (CNN) for both tattoo detection and representation learning. In this approach, shared features in the backbone network are optimized for both tasks, while individual latent layers optimize the features for detection and feature learning tasks, respectively. We address the small batch size issue using random image stitching and preceding feature buffering. We evaluate our approach using multiple public-domain tattoo benchmarks and a gallery set of about 300K distracter images, and introduce a tattoo sketch dataset for sketch-based tattoo search. Our experimental results show that our approach outperforms current state-of-the-art tattoo retrieval algorithms in tattoo detection and search at scale.",1
"State-of-the-art speaker diarization systems utilize knowledge from external data, in the form of a pre-trained distance metric, to effectively determine relative speaker identities to unseen data. However, much of recent focus has been on choosing the appropriate feature extractor, ranging from pre-trained $i-$vectors to representations learned via different sequence modeling architectures (e.g. 1D-CNNs, LSTMs, attention models), while adopting off-the-shelf metric learning solutions. In this paper, we argue that, regardless of the feature extractor, it is crucial to carefully design a metric learning pipeline, namely the loss function, the sampling strategy and the discrimnative margin parameter, for building robust diarization systems. Furthermore, we propose to adopt a fine-grained validation process to obtain a comprehensive evaluation of the generalization power of metric learning pipelines. To this end, we measure diarization performance across different language speakers, and variations in the number of speakers in a recording. Using empirical studies, we provide interesting insights into the effectiveness of different design choices and make recommendations.",0
"Modern speaker diarization systems rely on external data in the form of pre-trained distance metrics to accurately identify speakers in new data. While recent research has focused on selecting appropriate feature extractors including pre-trained i-vectors and representations learned through different sequence modeling architectures, little attention has been given to designing robust metric learning pipelines. This paper argues that a carefully designed pipeline including a loss function, sampling strategy, and discriminative margin parameter is essential for effective diarization, regardless of the feature extractor used. Additionally, the paper proposes a fine-grained validation process to comprehensively evaluate the generalization power of metric learning pipelines across different language speakers and recording scenarios. Empirical studies provide valuable insights into the effectiveness of different design choices and recommendations are made.",1
"Class imbalance is a pervasive issue among classification models including deep learning, whose capacity to extract task-specific features is affected in imbalanced settings. However, the challenges of handling imbalance among a large number of classes, commonly addressed by deep learning, have not received a significant amount of attention in previous studies. In this paper, we propose an extension of the deep over-sampling framework, to exploit automatically-generated abstract-labels, i.e., a type of side-information used in weak-label learning, to enhance deep representation learning against class imbalance. We attempt to exploit the labels to guide the deep representation of instances towards different subspaces, to induce a soft-separation of inherent subtasks of the classification problem. Our empirical study shows that the proposed framework achieves a substantial improvement on image classification benchmarks with imbalanced among large and small numbers of classes.",0
"The issue of class imbalance is prevalent in classification models, including deep learning, which struggles to extract task-specific features in such settings. However, the problem of managing imbalance in a large number of classes, which is commonly addressed by deep learning, has not been given sufficient attention in prior research. This study proposes an expansion of the deep over-sampling framework that utilizes automatically-generated abstract-labels, a form of side-information employed in weak-label learning, to enhance the deep representation learning against class imbalance. The labels are used to direct the deep representation of instances towards distinct subspaces, creating a soft-separation of inherent subtasks of the classification problem. Our empirical findings demonstrate a significant improvement in the proposed framework's image classification benchmarks, particularly in imbalanced settings involving a large and small number of classes.",1
"Depth estimation from a single image represents a very exciting challenge in computer vision. While other image-based depth sensing techniques leverage on the geometry between different viewpoints (e.g., stereo or structure from motion), the lack of these cues within a single image renders ill-posed the monocular depth estimation task. For inference, state-of-the-art encoder-decoder architectures for monocular depth estimation rely on effective feature representations learned at training time. For unsupervised training of these models, geometry has been effectively exploited by suitable images warping losses computed from views acquired by a stereo rig or a moving camera. In this paper, we make a further step forward showing that learning semantic information from images enables to improve effectively monocular depth estimation as well. In particular, by leveraging on semantically labeled images together with unsupervised signals gained by geometry through an image warping loss, we propose a deep learning approach aimed at joint semantic segmentation and depth estimation. Our overall learning framework is semi-supervised, as we deploy groundtruth data only in the semantic domain. At training time, our network learns a common feature representation for both tasks and a novel cross-task loss function is proposed. The experimental findings show how, jointly tackling depth prediction and semantic segmentation, allows to improve depth estimation accuracy. In particular, on the KITTI dataset our network outperforms state-of-the-art methods for monocular depth estimation.",0
"The task of estimating depth from a single image is a highly challenging task in computer vision. Unlike other methods that rely on geometrical information from multiple viewpoints, such as stereo or structure from motion, the absence of these cues in a single image makes the monocular depth estimation problem ambiguous. Current encoder-decoder architectures for monocular depth estimation rely on effective feature representations learned during training. Unsupervised training of these models has been successful by exploiting geometry with image warping losses computed from views captured by a stereo rig or a moving camera. In this study, we propose a new deep learning approach to jointly perform semantic segmentation and depth estimation by leveraging semantically labeled images and unsupervised signals from geometry via an image warping loss. Our learning framework is semi-supervised, using groundtruth data only in the semantic domain. We train a network to learn a common feature representation for both tasks, and propose a novel cross-task loss function. Our experiments show that jointly addressing depth prediction and semantic segmentation yields improved depth estimation accuracy. Our network outperforms state-of-the-art methods for monocular depth estimation on the KITTI dataset.",1
"Edge detection is among the most fundamental vision problems for its role in perceptual grouping and its wide applications. Recent advances in representation learning have led to considerable improvements in this area. Many state of the art edge detection models are learned with fully convolutional networks (FCNs). However, FCN-based edge learning tends to be vulnerable to misaligned labels due to the delicate structure of edges. While such problem was considered in evaluation benchmarks, similar issue has not been explicitly addressed in general edge learning. In this paper, we show that label misalignment can cause considerably degraded edge learning quality, and address this issue by proposing a simultaneous edge alignment and learning framework. To this end, we formulate a probabilistic model where edge alignment is treated as latent variable optimization, and is learned end-to-end during network training. Experiments show several applications of this work, including improved edge detection with state of the art performance, and automatic refinement of noisy annotations.",0
"The problem of edge detection is crucial in computer vision due to its significant role in perceptual grouping and numerous practical applications. Recent developments in representation learning have resulted in significant advancements in this field. Although many advanced edge detection models use fully convolutional networks (FCNs), they are susceptible to misaligned labels that arise due to the intricate nature of edges. While this problem has been acknowledged in evaluation benchmarks, it has not been explicitly addressed in general edge learning. Our study reveals that misaligned labels can drastically impair edge learning quality and proposes a solution by introducing a simultaneous edge alignment and learning framework. We have devised a probabilistic model that treats edge alignment as latent variable optimization and can be learned end-to-end during network training. Our experiments demonstrate that our approach can enhance edge detection and automatically refine noisy annotations.",1
"Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper introduces two categories for multi-view representation learning: multi-view representation alignment and multi-view representation fusion. Consequently, we first review the representative methods and theories of multi-view representation learning based on the perspective of alignment, such as correlation-based alignment. Representative examples are canonical correlation analysis (CCA) and its several extensions. Then from the perspective of representation fusion we investigate the advancement of multi-view representation learning that ranges from generative methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to neural network-based methods including multi-modal autoencoders, multi-view convolutional neural networks, and multi-modal recurrent neural networks. Further, we also investigate several important applications of multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical foundation and state-of-the-art developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications.",0
"In the field of machine learning and data mining, multi-view representation learning has emerged as a rapidly growing area. This article presents two categories for multi-view representation learning, namely multi-view representation alignment and multi-view representation fusion. The former includes methods like correlation-based alignment, which is exemplified by canonical correlation analysis (CCA) and its extensions. The latter includes generative methods like multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, as well as neural network-based methods like multi-modal autoencoders, multi-view convolutional neural networks, and multi-modal recurrent neural networks. Additionally, various applications of multi-view representation learning are explored. The aim of this survey is to provide an insightful overview of the theoretical foundation and state-of-the-art developments in multi-view representation learning, and to assist researchers in selecting the most suitable tools for specific applications.",1
"This paper aims to develop a new and robust approach to feature representation. Motivated by the success of Auto-Encoders, we first theoretical summarize the general properties of all algorithms that are based on traditional Auto-Encoders: 1) The reconstruction error of the input can not be lower than a lower bound, which can be viewed as a guiding principle for reconstructing the input. Additionally, when the input is corrupted with noises, the reconstruction error of the corrupted input also can not be lower than a lower bound. 2) The reconstruction of a hidden representation achieving its ideal situation is the necessary condition for the reconstruction of the input to reach the ideal state. 3) Minimizing the Frobenius norm of the Jacobian matrix of the hidden representation has a deficiency and may result in a much worse local optimum value. We believe that minimizing the reconstruction error of the hidden representation is more robust than minimizing the Frobenius norm of the Jacobian matrix of the hidden representation. Based on the above analysis, we propose a new model termed Double Denoising Auto-Encoders (DDAEs), which uses corruption and reconstruction on both the input and the hidden representation. We demonstrate that the proposed model is highly flexible and extensible and has a potentially better capability to learn invariant and robust feature representations. We also show that our model is more robust than Denoising Auto-Encoders (DAEs) for dealing with noises or inessential features. Furthermore, we detail how to train DDAEs with two different pre-training methods by optimizing the objective function in a combined and separate manner, respectively. Comparative experiments illustrate that the proposed model is significantly better for representation learning than the state-of-the-art models.",0
"The objective of this study is to create a sturdy and innovative technique for feature representation. Drawing inspiration from the achievements of Auto-Encoders, we initially summarize the fundamental characteristics of algorithms based on traditional Auto-Encoders. Firstly, the reconstruction error of the input cannot be lower than a certain threshold, which serves as a guiding principle for input reconstruction. Additionally, when noises corrupt the input, the reconstruction error of the corrupted input cannot be lower than a lower bound. Secondly, the reconstruction of a hidden representation reaching its ideal state is necessary for the input reconstruction to achieve the ideal state. Thirdly, minimizing the Frobenius norm of the Jacobian matrix of the hidden representation can lead to a worse local optimum value. In contrast, we propose that minimizing the reconstruction error of the hidden representation is more reliable. We introduce a new model called Double Denoising Auto-Encoders (DDAEs) based on the above analysis. This model uses corruption and reconstruction on both the input and the hidden representation, making it highly flexible and extensible and potentially better at learning invariant and robust feature representations. We also demonstrate that our model is more robust than Denoising Auto-Encoders (DAEs) for managing noises or inessential features. Furthermore, we explain how to train DDAEs with two different pre-training methods by optimizing the objective function in a combined and separate manner. Comparative experiments prove that the proposed model surpasses the state-of-the-art models for representation learning.",1
"In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.",0
This paper argues that representation learning is crucial in preventing unfair prediction results. The authors propose using adversarial representation learning to ensure third parties with unknown intentions act fairly when using learned representations. The authors link group fairness to various adversarial objectives and demonstrate through theoretical guarantees and experiments that the choice of objective is critical for fair prediction. The paper also presents the first comprehensive experimental demonstration of fair transfer learning and shows that learned representations allow for fair predictions on new tasks while retaining utility. This highlights the importance of fair representation learning.,1
"We introduce a new unsupervised representation learning and visualization using deep convolutional networks and self organizing maps called Deep Neural Maps (DNM). DNM jointly learns an embedding of the input data and a mapping from the embedding space to a two-dimensional lattice. We compare visualizations of DNM with those of t-SNE and LLE on the MNIST and COIL-20 data sets. Our experiments show that the DNM can learn efficient representations of the input data, which reflects characteristics of each class. This is shown via back-projecting the neurons of the map on the data space.",0
"Deep Neural Maps (DNM) is a novel technique that utilizes deep convolutional networks and self-organizing maps for unsupervised representation learning and visualization. It involves learning an embedding of the input data and a mapping from the embedding space to a two-dimensional lattice. To assess its efficacy, we compared DNM with t-SNE and LLE on the MNIST and COIL-20 datasets. Our experiments demonstrate that DNM is capable of learning efficient representations of input data that reflect class characteristics. This was demonstrated by back-projecting the neurons of the map onto the data space.",1
"Network representation learning (NRL) methods aim to map each vertex into a low dimensional space by preserving the local and global structure of a given network, and in recent years they have received a significant attention thanks to their success in several challenging problems. Although various approaches have been proposed to compute node embeddings, many successful methods benefit from random walks in order to transform a given network into a collection of sequences of nodes and then they target to learn the representation of nodes by predicting the context of each vertex within the sequence. In this paper, we introduce a general framework to enhance the embeddings of nodes acquired by means of the random walk-based approaches. Similar to the notion of topical word embeddings in NLP, the proposed method assigns each vertex to a topic with the favor of various statistical models and community detection methods, and then generates the enhanced community representations. We evaluate our method on two downstream tasks: node classification and link prediction. The experimental results demonstrate that the incorporation of vertex and topic embeddings outperform widely-known baseline NRL methods.",0
"The aim of Network representation learning (NRL) methods is to create a mapping of each vertex of a given network into a low dimensional space while maintaining the local and global structure. These methods have gained significant attention in recent years due to their success in tackling challenging problems. Although various approaches exist for computing node embeddings, many successful methods use random walks to transform the network into a sequence of nodes. These methods then predict the context of each vertex within the sequence to learn the node representation. In this paper, we introduce a general framework that enhances node embeddings acquired through random walk-based methods. Our proposed method assigns each vertex to a topic using statistical models and community detection methods, similar to the idea of topical word embeddings in NLP. It then generates enhanced community representations. We evaluate our method on two downstream tasks: node classification and link prediction. The results show that incorporating vertex and topic embeddings outperforms widely-known baseline NRL methods.",1
"Representation learning is typically applied to only one mode of a data matrix, either its rows or columns. Yet in many applications, there is an underlying geometry to both the rows and the columns. We propose utilizing this coupled structure to perform co-manifold learning: uncovering the underlying geometry of both the rows and the columns of a given matrix, where we focus on a missing data setting. Our unsupervised approach consists of three components. We first solve a family of optimization problems to estimate a complete matrix at multiple scales of smoothness. We then use this collection of smooth matrix estimates to compute pairwise distances on the rows and columns based on a new multi-scale metric that implicitly introduces a coupling between the rows and the columns. Finally, we construct row and column representations from these multi-scale metrics. We demonstrate that our approach outperforms competing methods in both data visualization and clustering.",0
"Typically, representation learning is only applied to one aspect of a data matrix, either its rows or columns. However, in various applications, both the rows and columns possess an underlying geometry. We suggest making use of this linked structure to execute co-manifold learning, which involves discovering the underlying geometry of both the rows and columns of a given matrix. Our focus is on a missing data environment. Our unsupervised strategy comprises three elements. Firstly, we solve a set of optimization problems to estimate a complete matrix at multiple levels of smoothness. We then use this set of smooth matrix approximations to calculate pairwise distances on the rows and columns using a new multi-scale metric that implicitly introduces a coupling between the two. Lastly, we create row and column representations based on these multi-scale metrics. We prove that our methodology outperforms competing methods in both data visualization and clustering.",1
"One-class support vector machine (OC-SVM) for a long time has been one of the most effective anomaly detection methods and extensively adopted in both research as well as industrial applications. The biggest issue for OC-SVM is yet the capability to operate with large and high-dimensional datasets due to optimization complexity. Those problems might be mitigated via dimensionality reduction techniques such as manifold learning or autoencoder. However, previous work often treats representation learning and anomaly prediction separately. In this paper, we propose autoencoder based one-class support vector machine (AE-1SVM) that brings OC-SVM, with the aid of random Fourier features to approximate the radial basis kernel, into deep learning context by combining it with a representation learning architecture and jointly exploit stochastic gradient descent to obtain end-to-end training. Interestingly, this also opens up the possible use of gradient-based attribution methods to explain the decision making for anomaly detection, which has ever been challenging as a result of the implicit mappings between the input space and the kernel space. To the best of our knowledge, this is the first work to study the interpretability of deep learning in anomaly detection. We evaluate our method on a wide range of unsupervised anomaly detection tasks in which our end-to-end training architecture achieves a performance significantly better than the previous work using separate training.",0
"For a considerable duration, the one-class support vector machine (OC-SVM) has been an exceptionally effective anomaly detection approach, widely utilized in both research and practical applications. However, the primary challenge for OC-SVM lies in its ability to operate with large and high-dimensional datasets due to optimization complexity. To address this, manifold learning or autoencoder can be employed for dimensionality reduction. Nevertheless, previous studies have treated representation learning and anomaly prediction as separate entities. This paper introduces the autoencoder based one-class support vector machine (AE-1SVM), which incorporates OC-SVM with a representation learning architecture and random Fourier features to approximate the radial basis kernel. This approach combines deep learning and stochastic gradient descent for end-to-end training, which facilitates gradient-based attribution methods for explaining the decision-making process in anomaly detection. Notably, this is the first study to investigate the interpretability of deep learning in anomaly detection. We evaluate our method on various unsupervised anomaly detection tasks and demonstrate that our end-to-end training architecture outperforms previous separate training approaches.",1
"The smallest eigenvectors of the graph Laplacian are well-known to provide a succinct representation of the geometry of a weighted graph. In reinforcement learning (RL), where the weighted graph may be interpreted as the state transition process induced by a behavior policy acting on the environment, approximating the eigenvectors of the Laplacian provides a promising approach to state representation learning. However, existing methods for performing this approximation are ill-suited in general RL settings for two main reasons: First, they are computationally expensive, often requiring operations on large matrices. Second, these methods lack adequate justification beyond simple, tabular, finite-state settings. In this paper, we present a fully general and scalable method for approximating the eigenvectors of the Laplacian in a model-free RL context. We systematically evaluate our approach and empirically show that it generalizes beyond the tabular, finite-state setting. Even in tabular, finite-state settings, its ability to approximate the eigenvectors outperforms previous proposals. Finally, we show the potential benefits of using a Laplacian representation learned using our method in goal-achieving RL tasks, providing evidence that our technique can be used to significantly improve the performance of an RL agent.",0
"The geometry of a weighted graph can be succinctly represented by its smallest eigenvectors. In reinforcement learning, the graph's weighted edges represent the state transition process resulting from a behavior policy. Therefore, approximating the eigenvectors of the graph Laplacian can be a useful method for state representation learning. However, current approximation methods are not suitable for general RL settings due to their computational expense and lack of justification beyond simple, finite-state settings. To address these limitations, we propose a scalable method for approximating the eigenvectors of the Laplacian in a model-free RL context. Our method outperforms previous proposals even in finite-state settings and has the potential to significantly improve the performance of an RL agent in goal-achieving tasks.",1
"State representation learning aims at learning compact representations from raw observations in robotics and control applications. Approaches used for this objective are auto-encoders, learning forward models, inverse dynamics or learning using generic priors on the state characteristics. However, the diversity in applications and methods makes the field lack standard evaluation datasets, metrics and tasks. This paper provides a set of environments, data generators, robotic control tasks, metrics and tools to facilitate iterative state representation learning and evaluation in reinforcement learning settings.",0
"The goal of state representation learning is to obtain condensed representations from unprocessed observations in control and robotics contexts. Various methods, such as auto-encoders, forward model learning, inverse dynamics, and generic state characteristic learning, are employed to achieve this aim. Nevertheless, due to the range of applications and approaches, the field suffers from a dearth of standard evaluation datasets, metrics, and tasks. To support iterative state representation learning and assessment in reinforcement learning scenarios, this article presents a collection of environments, data generators, robotic control tasks, metrics, and tools.",1
"Semantic image parsing, which refers to the process of decomposing images into semantic regions and constructing the structure representation of the input, has recently aroused widespread interest in the field of computer vision. The recent application of deep representation learning has driven this field into a new stage of development. In this paper, we summarize three aspects of the progress of research on semantic image parsing, i.e., category-level semantic segmentation, instance-level semantic segmentation, and beyond segmentation. Specifically, we first review the general frameworks for each task and introduce the relevant variants. The advantages and limitations of each method are also discussed. Moreover, we present a comprehensive comparison of different benchmark datasets and evaluation metrics. Finally, we explore the future trends and challenges of semantic image parsing.",0
"The computer vision field has shown great interest in semantic image parsing, which involves breaking images down into semantic regions and constructing a representation of the input's structure. The use of deep representation learning has propelled this area of research into a new stage of development. This paper presents a summary of three aspects of the progress of semantic image parsing research, namely category-level semantic segmentation, instance-level semantic segmentation, and beyond segmentation. Our review covers the general frameworks for each task, including relevant variants, as well as the advantages and limitations of each method. We also provide a comprehensive comparison of various benchmark datasets and evaluation metrics. Lastly, we explore the future trends and challenges of semantic image parsing.",1
"Intrinsically motivated goal exploration algorithms enable machines to discover repertoires of policies that produce a diversity of effects in complex environments. These exploration algorithms have been shown to allow real world robots to acquire skills such as tool use in high-dimensional continuous state and action spaces. However, they have so far assumed that self-generated goals are sampled in a specifically engineered feature space, limiting their autonomy. In this work, we propose to use deep representation learning algorithms to learn an adequate goal space. This is a developmental 2-stage approach: first, in a perceptual learning stage, deep learning algorithms use passive raw sensor observations of world changes to learn a corresponding latent space; then goal exploration happens in a second stage by sampling goals in this latent space. We present experiments where a simulated robot arm interacts with an object, and we show that exploration algorithms using such learned representations can match the performance obtained using engineered representations.",0
"Algorithms that explore goals based on intrinsic motivation enable machines to discover various policies that produce different outcomes in complex environments. These algorithms have proven to be effective in helping real-world robots acquire skills such as tool use in continuous state and action spaces. However, their autonomy is limited as they assume self-generated goals are sampled in an engineered feature space. To overcome this limitation, we propose a 2-stage approach that uses deep representation learning algorithms to learn an appropriate goal space. In the first stage, deep learning algorithms use passive sensor observations to learn a latent space in a perceptual learning stage. The second stage involves goal exploration by sampling goals in this latent space. Our experiments show that exploration algorithms using such learned representations can match the performance obtained using engineered representations, as demonstrated by a simulated robot arm interacting with an object.",1
"This work studies the problem of learning appropriate low dimensional image representations. We propose a generic algorithmic framework, which leverages two classic representation learning paradigms, i.e., sparse representation and the trace quotient criterion. The former is a well-known powerful tool to identify underlying self-explanatory factors of data, while the latter is known for disentangling underlying low dimensional discriminative factors in data. Our developed solutions disentangle sparse representations of images by employing the trace quotient criterion. We construct a unified cost function, coined as the SPARse LOW dimensional representation (SparLow) function, for jointly learning both a sparsifying dictionary and a dimensionality reduction transformation. The SparLow function is widely applicable for developing various algorithms in three classic machine learning scenarios, namely, unsupervised, supervised, and semi-supervised learning. In order to develop efficient joint learning algorithms for maximizing the SparLow function, we deploy a framework of sparse coding with appropriate convex priors to ensure the sparse representations to be locally differentiable. Moreover, we develop an efficient geometric conjugate gradient algorithm to maximize the SparLow function on its underlying Riemannian manifold. Performance of the proposed SparLow algorithmic framework is investigated on several image processing tasks, such as 3D data visualization, face/digit recognition, and object/scene categorization.",0
"The objective of this study is to explore the issue of acquiring appropriate low dimensional image representations. To achieve this, we present a generic algorithmic framework that combines two established representation learning paradigms: sparse representation and the trace quotient criterion. The former is a well-known technique for identifying the fundamental explanatory factors of data, while the latter is renowned for separating the underlying low dimensional discriminative factors in data. Our approach extracts sparse representations of images by utilizing the trace quotient criterion. We introduce a unified cost function called SparLow, which facilitates the joint learning of both a sparsifying dictionary and a dimensionality reduction transformation. The SparLow function can be used to develop algorithms for unsupervised, supervised, and semi-supervised learning. To maximize the SparLow function, we have developed a sparse coding framework with appropriate convex priors to ensure that the sparse representations are locally differentiable. Additionally, we have designed an efficient geometric conjugate gradient algorithm for maximizing the SparLow function on its underlying Riemannian manifold. We evaluate the performance of the proposed SparLow algorithmic framework on various image processing tasks, such as face/digit recognition, object/scene categorization, and 3D data visualization.",1
"Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN's output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat surprisingly, we find that DNNs with randomly-initialized weights produce explanations that are both visually and quantitatively similar to those produced by DNNs with learned weights. Our conjecture is that this phenomenon occurs because these explanations are dominated by the lower level features of a DNN, and that a DNN's architecture provides a strong prior which significantly affects the representations learned at these lower layers. NOTE: This work is now subsumed by our recent manuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we expand on findings and address concerns raised in Sundararajan et. al. (2018).",0
"The central challenge in machine learning is to explain the output of a complicated model like a deep neural network (DNN). Various local explanation methods have been proposed to address this issue, which identify the dimensions of a single input that are most responsible for a DNN's output. The objective of this study is to evaluate the sensitivity of local explanations to DNN parameter values. Surprisingly, we discovered that DNNs with randomly-initialized weights produce similar explanations, both visually and quantitatively, to those produced by DNNs with learned weights. Our supposition is that this occurs because these explanations are dominated by the lower level features of a DNN, and its architecture provides a strong prior that significantly influences the representations learned at these lower layers. Please note that this study is now incorporated into our recent manuscript, ""Sanity Checks for Saliency Maps"" (to be published in NIPS 2018), in which we expand on our findings and address concerns raised by Sundararajan et al. (2018).",1
"Deep neural networks, including recurrent networks, have been successfully applied to human activity recognition. Unfortunately, the final representation learned by recurrent networks might encode some noise (irrelevant signal components, unimportant sensor modalities, etc.). Besides, it is difficult to interpret the recurrent networks to gain insight into the models' behavior. To address these issues, we propose two attention models for human activity recognition: temporal attention and sensor attention. These two mechanisms adaptively focus on important signals and sensor modalities. To further improve the understandability and mean F1 score, we add continuity constraints, considering that continuous sensor signals are more robust than discrete ones. We evaluate the approaches on three datasets and obtain state-of-the-art results. Furthermore, qualitative analysis shows that the attention learned by the models agree well with human intuition.",0
"Human activity recognition has seen success with the application of deep neural networks, including recurrent networks. However, the final representation learned by these networks may contain noise, such as irrelevant signal components and unimportant sensor modalities. Additionally, it is challenging to interpret the behavior of these networks. To address these challenges, we propose two attention models: temporal attention and sensor attention. These models focus on important signals and sensor modalities in an adaptive manner. To enhance understandability and increase the mean F1 score, we include continuity constraints, as continuous sensor signals are more robust than discrete ones. We evaluate our approaches on three datasets and achieve state-of-the-art results. Our qualitative analysis demonstrates that the attention learned by our models aligns with human intuition.",1
"In this work, we propose an ensemble of classifiers to distinguish between various degrees of abnormalities of the heart using Phonocardiogram (PCG) signals acquired using digital stethoscopes in a clinical setting, for the INTERSPEECH 2018 Computational Paralinguistics (ComParE) Heart Beats SubChallenge. Our primary classification framework constitutes a convolutional neural network with 1D-CNN time-convolution (tConv) layers, which uses features transferred from a model trained on the 2016 Physionet Heart Sound Database. We also employ a Representation Learning (RL) approach to generate features in an unsupervised manner using Deep Recurrent Autoencoders and use Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) classifiers. Finally, we utilize an SVM classifier on a high-dimensional segment-level feature extracted using various functionals on short-term acoustic features, i.e., Low-Level Descriptors (LLD). An ensemble of the three different approaches provides a relative improvement of 11.13% compared to our best single sub-system in terms of the Unweighted Average Recall (UAR) performance metric on the evaluation dataset.",0
"Our study proposes an ensemble of classifiers that can differentiate between different levels of heart abnormalities by analyzing Phonocardiogram (PCG) signals obtained from digital stethoscopes used in a clinical setting. Our primary classification framework involves a convolutional neural network with 1D-CNN time-convolution (tConv) layers, which utilizes transferred features from a model trained on the 2016 Physionet Heart Sound Database. Additionally, we employ a Representation Learning (RL) method to generate unsupervised features using Deep Recurrent Autoencoders and use Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) classifiers. For high-dimensional segment-level feature extraction, we utilize various functionals on short-term acoustic features, i.e., Low-Level Descriptors (LLD), and use an SVM classifier. An ensemble of these three different approaches yields an 11.13% improvement in the Unweighted Average Recall (UAR) performance metric on the evaluation dataset as compared to our best single sub-system.",1
"Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages still remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, we develop effective and novel training strategies for end-to-end learning the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR and several other cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-the-arts. Benchmarking our model on one of the most popular unconstrained face recognition datasets IJB-C additionally verifies the promising generalizability of AIM in recognizing faces in the wild.",0
"Although there have been notable advancements in face recognition technologies, accurately identifying faces across different age groups remains a significant obstacle. The appearance of a human face undergoes substantial changes over time, leading to significant intra-class variations. Rather than relying on current techniques for age-invariant face recognition, which involve extracting age-invariant features or synthesizing a face that matches the target age before feature extraction, we believe it is more effective to perform both tasks simultaneously. Our solution is a deep Age-Invariant Model (AIM) for recognizing faces in the wild, which has three unique features. Firstly, AIM employs a novel unified deep architecture that simultaneously performs cross-age face synthesis and recognition, enhancing each other. Secondly, AIM accomplishes continuous face rejuvenation/aging with exceptional photorealistic and identity-preserving qualities, eliminating the need for paired data and the true age of testing samples. Thirdly, we develop innovative and effective training strategies to learn the entire deep architecture end-to-end, generating powerful age-invariant face representations explicitly disentangled from the age variation. Additionally, we introduce a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to support ongoing research and advance the field of age-invariant face recognition. Our experiments on both CAFR and other cross-age datasets (MORPH, CACD, and FG-NET) demonstrate the superiority of AIM over existing techniques. Furthermore, benchmarking our model on the popular unconstrained face recognition dataset IJB-C confirms the promising generalizability of AIM for recognizing faces in the wild.",1
"This paper presents an automatic network adaptation method that finds a ConvNet structure well-suited to a given target task, e.g., image classification, for efficiency as well as accuracy in transfer learning. We call the concept target-aware transfer learning. Given only small-scale labeled data, and starting from an ImageNet pre-trained network, we exploit a scheme of removing its potential redundancy for the target task through iterative operations of filter-wise pruning and network optimization. The basic motivation is that compact networks are on one hand more efficient and should also be more tolerant, being less complex, against the risk of overfitting which would hinder the generalization of learned representations in the context of transfer learning. Further, unlike existing methods involving network simplification, we also let the scheme identify redundant portions across the entire network, which automatically results in a network structure adapted to the task at hand. We achieve this with a few novel ideas: (i) cumulative sum of activation statistics for each layer, and (ii) a priority evaluation of pruning across multiple layers. Experimental results by the method on five datasets (Flower102, CUB200-2011, Dog120, MIT67, and Stanford40) show favorable accuracies over the related state-of-the-art techniques while enhancing the computational and storage efficiency of the transferred model.",0
"The paper introduces a method for automatic network adaptation that aims to find a suitable ConvNet structure for a given target task. This approach, referred to as target-aware transfer learning, focuses on both efficiency and accuracy in transfer learning. The method involves removing potential redundancy from an ImageNet pre-trained network through iterative filter-wise pruning and network optimization, with the aim of creating a more compact and less complex network that is less prone to overfitting. Unlike other methods that simplify networks, this approach identifies redundant portions across the entire network, resulting in a network structure adapted to the specific task. The method incorporates two novel ideas: a cumulative sum of activation statistics for each layer and a priority evaluation of pruning across multiple layers. Experimental results on five datasets show improved accuracies compared to state-of-the-art techniques, while also enhancing the computational and storage efficiency of the transferred model.",1
"Abnormal event detection is one of the important objectives in research and practical applications of video surveillance. However, there are still three challenging problems for most anomaly detection systems in practical setting: limited labeled data, ambiguous definition of ""abnormal"" and expensive feature engineering steps. This paper introduces a unified detection framework to handle these challenges using energy-based models, which are powerful tools for unsupervised representation learning. Our proposed models are firstly trained on unlabeled raw pixels of image frames from an input video rather than hand-crafted visual features; and then identify the locations of abnormal objects based on the errors between the input video and its reconstruction produced by the models. To handle video stream, we develop an online version of our framework, wherein the model parameters are updated incrementally with the image frames arriving on the fly. Our experiments show that our detectors, using Restricted Boltzmann Machines (RBMs) and Deep Boltzmann Machines (DBMs) as core modules, achieve superior anomaly detection performance to unsupervised baselines and obtain accuracy comparable with the state-of-the-art approaches when evaluating at the pixel-level. More importantly, we discover that our system trained with DBMs is able to simultaneously perform scene clustering and scene reconstruction. This capacity not only distinguishes our method from other existing detectors but also offers a unique tool to investigate and understand how the model works.",0
"The detection of abnormal events is a crucial aim in video surveillance research and practical applications. However, anomaly detection systems face three challenging problems in real-life situations: the limited availability of labeled data, the vague definition of what constitutes ""abnormal,"" and the costly process of feature engineering. To address these challenges, this study proposes a unified detection framework that utilizes energy-based models, which are effective tools for unsupervised representation learning. Our models are initially trained on unlabeled raw pixel data from video frames, rather than relying on hand-crafted visual features, and subsequently locate anomalous objects based on the discrepancies between the input video and its reconstruction generated by the models. To accommodate video streams, we introduce an online version of our framework, in which the model parameters are continuously updated with arriving image frames. Our experimental results indicate that our detectors, which incorporate Restricted Boltzmann Machines (RBMs) and Deep Boltzmann Machines (DBMs) as core modules, achieve superior anomaly detection performance compared to unsupervised baselines and achieve accuracy equivalent to state-of-the-art approaches when assessed at the pixel-level. Furthermore, we observe that our system, trained with DBMs, can simultaneously perform scene clustering and scene reconstruction, which distinguishes our method from other existing detectors and offers a unique tool for investigating and comprehending how the model functions.",1
"Multi-view frame reconstruction is an important problem particularly when multiple frames are missing and past and future frames within the camera are far apart from the missing ones. Realistic coherent frames can still be reconstructed using corresponding frames from other overlapping cameras. We propose an adversarial approach to learn the spatio-temporal representation of the missing frame using conditional Generative Adversarial Network (cGAN). The conditional input to each cGAN is the preceding or following frames within the camera or the corresponding frames in other overlapping cameras, all of which are merged together using a weighted average. Representations learned from frames within the camera are given more weight compared to the ones learned from other cameras when they are close to the missing frames and vice versa. Experiments on two challenging datasets demonstrate that our framework produces comparable results with the state-of-the-art reconstruction method in a single camera and achieves promising performance in multi-camera scenario.",0
"When multiple frames are missing and there's a significant time gap between past and future frames within the camera, reconstructing a multi-view frame becomes crucial. However, even with missing frames, realistic and coherent frames can be reconstructed through the use of corresponding frames from overlapping cameras. Our proposed solution is an adversarial approach that employs a conditional Generative Adversarial Network (cGAN) to learn the spatio-temporal representation of the missing frame. The cGAN takes in a conditional input, which can be the preceding or following frames within the camera, or corresponding frames from other overlapping cameras. These inputs are merged together using a weighted average. Frames from within the camera are given more weight when they're closer to the missing frames, and vice versa for frames from other cameras. In our experiments with two challenging datasets, our framework produced comparable results with the state-of-the-art reconstruction method in a single camera, and promising performance in a multi-camera scenario.",1
"In information theory, Fisher information and Shannon information (entropy) are respectively used to quantify the uncertainty associated with the distribution modeling and the uncertainty in specifying the outcome of given variables. These two quantities are complementary and are jointly applied to information behavior analysis in most cases. The uncertainty property in information asserts a fundamental trade-off between Fisher information and Shannon information, which enlightens us the relationship between the encoder and the decoder in variational auto-encoders (VAEs). In this paper, we investigate VAEs in the Fisher-Shannon plane and demonstrate that the representation learning and the log-likelihood estimation are intrinsically related to these two information quantities. Through extensive qualitative and quantitative experiments, we provide with a better comprehension of VAEs in tasks such as high-resolution reconstruction, and representation learning in the perspective of Fisher information and Shannon information. We further propose a variant of VAEs, termed as Fisher auto-encoder (FAE), for practical needs to balance Fisher information and Shannon information. Our experimental results have demonstrated its promise in improving the reconstruction accuracy and avoiding the non-informative latent code as occurred in previous works.",0
"In the realm of information theory, Fisher information and Shannon information are utilized to measure the uncertainty associated with distribution modeling and the uncertainty in determining variable outcomes, respectively. These two measures are complementary and commonly employed together in the analysis of information behavior. The concept of uncertainty in information highlights a fundamental trade-off between Fisher information and Shannon information, which sheds light on the relationship between encoder and decoder in variational auto-encoders (VAEs). This study explores VAEs in the Fisher-Shannon plane and shows that representation learning and log-likelihood estimation are intrinsically linked to these information measures. Through extensive qualitative and quantitative experiments, this research provides a deeper understanding of VAEs in tasks such as high-resolution reconstruction and representation learning from the perspective of Fisher information and Shannon information. Furthermore, a new version of VAEs, referred to as Fisher auto-encoder (FAE), is presented to balance Fisher information and Shannon information for practical purposes. Experimental results demonstrate its potential in enhancing reconstruction accuracy and avoiding non-informative latent codes as observed in previous works.",1
"While enormous progress has been made to Variational Autoencoder (VAE) in recent years, similar to other deep networks, VAE with deep networks suffers from the problem of degeneration, which seriously weakens the correlation between the input and the corresponding latent codes, deviating from the goal of the representation learning. To investigate how degeneration affects VAE from a theoretical perspective, we illustrate the information transmission in VAE and analyze the intermediate layers of the encoders/decoders. Specifically, we propose a Fisher Information measure for the layer-wise analysis. With such measure, we demonstrate that information loss is ineluctable in feed-forward networks and causes the degeneration in VAE. We show that skip connections in VAE enable the preservation of information without changing the model architecture. We call this class of VAE equipped with skip connections as SCVAE and perform a range of experiments to show its advantages in information preservation and degeneration mitigation.",0
"Despite significant advancements in Variational Autoencoder (VAE) technology in recent years, the issue of degeneration continues to plague VAEs with deep networks, leading to weakened correlations between input and corresponding latent codes and undermining the goal of representation learning. To explore the theoretical implications of degeneration on VAEs, we investigate information transmission within these networks and analyze intermediate layers of encoders and decoders using a Fisher Information measure for layer-wise analysis. Our findings demonstrate that information loss is inevitable in feed-forward networks and contributes to VAE degeneration. However, we also demonstrate that the inclusion of skip connections in VAEs allows for information preservation without altering the model architecture, leading to the creation of a new class of VAE, which we call SCVAE. Through a range of experiments, we show that SCVAEs offer significant advantages in information preservation and degeneration mitigation.",1
"This paper investigates conditional generative adversarial networks (cGANs) to overcome a fundamental limitation of using geotagged media for geographic discovery, namely its sparse and uneven spatial distribution. We train a cGAN to generate ground-level views of a location given overhead imagery. We show the ""fake"" ground-level images are natural looking and are structurally similar to the real images. More significantly, we show the generated images are representative of the locations and that the representations learned by the cGANs are informative. In particular, we show that dense feature maps generated using our framework are more effective for land-cover classification than approaches which spatially interpolate features extracted from sparse ground-level images. To our knowledge, ours is the first work to use cGANs to generate ground-level views given overhead imagery and to explore the benefits of the learned representations.",0
"The aim of this study is to investigate the potential of conditional generative adversarial networks (cGANs) in addressing the issue of sparse and uneven spatial distribution in geotagged media for geographic discovery. The study focuses on training a cGAN model to produce ground-level views of a location using overhead imagery. The results show that the generated images are natural-looking and structurally similar to real images. Furthermore, the study confirms that the generated images are representative of the locations and the representations learned by the cGANs are informative. Dense feature maps generated using this model are found to be more effective for land-cover classification compared to approaches that interpolate features extracted from sparse ground-level images. This study is the first of its kind to use cGANs for generating ground-level views using overhead imagery and exploring the benefits of the learned representations.",1
"Talent search and recommendation systems at LinkedIn strive to match the potential candidates to the hiring needs of a recruiter or a hiring manager expressed in terms of a search query or a job posting. Recent work in this domain has mainly focused on linear models, which do not take complex relationships between features into account, as well as ensemble tree models, which introduce non-linearity but are still insufficient for exploring all the potential feature interactions, and strictly separate feature generation from modeling. In this paper, we present the results of our application of deep and representation learning models on LinkedIn Recruiter. Our key contributions include: (i) Learning semantic representations of sparse entities within the talent search domain, such as recruiter ids, candidate ids, and skill entity ids, for which we utilize neural network models that take advantage of LinkedIn Economic Graph, and (ii) Deep models for learning recruiter engagement and candidate response in talent search applications. We also explore learning to rank approaches applied to deep models, and show the benefits for the talent search use case. Finally, we present offline and online evaluation results for LinkedIn talent search and recommendation systems, and discuss potential challenges along the path to a fully deep model architecture. The challenges and approaches discussed generalize to any multi-faceted search engine.",0
"The aim of LinkedIn's talent search and recommendation systems is to match job candidates with the requirements of recruiters and hiring managers. Current research in this area has concentrated on linear models that fail to consider complex feature relationships, and ensemble tree models that introduce non-linearity but do not effectively explore all potential feature interactions. This paper presents the results of our use of deep and representation learning models on LinkedIn Recruiter. Our contributions include learning semantic representations of sparse entities within the talent search domain, and deep models for learning recruiter engagement and candidate response. We also explore learning to rank approaches for deep models and demonstrate their benefits for talent search applications. Finally, we present offline and online evaluation results and discuss the challenges and potential solutions to achieving a fully deep model architecture, which can be applied to any multi-faceted search engine.",1
"Scattering networks are a class of designed Convolutional Neural Networks (CNNs) with fixed weights. We argue they can serve as generic representations for modelling images. In particular, by working in scattering space, we achieve competitive results both for supervised and unsupervised learning tasks, while making progress towards constructing more interpretable CNNs. For supervised learning, we demonstrate that the early layers of CNNs do not necessarily need to be learned, and can be replaced with a scattering network instead. Indeed, using hybrid architectures, we achieve the best results with predefined representations to-date, while being competitive with end-to-end learned CNNs. Specifically, even applying a shallow cascade of small-windowed scattering coefficients followed by 1$\times$1-convolutions results in AlexNet accuracy on the ILSVRC2012 classification task. Moreover, by combining scattering networks with deep residual networks, we achieve a single-crop top-5 error of 11.4% on ILSVRC2012. Also, we show they can yield excellent performance in the small sample regime on CIFAR-10 and STL-10 datasets, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. For unsupervised learning, scattering coefficients can be a competitive representation that permits image recovery. We use this fact to train hybrid GANs to generate images. Finally, we empirically analyze several properties related to stability and reconstruction of images from scattering coefficients.",0
"Designed Convolutional Neural Networks (CNNs) with fixed weights, known as scattering networks, are capable of serving as generic representations for modeling images. By operating in scattering space, we achieve competitive results in both supervised and unsupervised learning tasks while advancing the development of more interpretable CNNs. Our findings demonstrate that early CNN layers do not necessarily require learning and can be substituted with a scattering network instead, resulting in the best outcomes to-date using fixed representations and remaining competitive with end-to-end learned CNNs. We achieve high accuracy on the ILSVRC2012 classification task by applying a shallow cascade of small-windowed scattering coefficients followed by 1x1-convolutions. Additionally, combining scattering networks with deep residual networks results in a single-crop top-5 error of 11.4% on ILSVRC2012. The ability of scattering networks to incorporate geometrical priors allows them to exceed their end-to-end counterparts in the small sample regime on CIFAR-10 and STL-10 datasets. Furthermore, scattering coefficients are an effective representation for image recovery in unsupervised learning, as evidenced by their use in training hybrid GANs to generate images. Finally, we conduct an empirical analysis of several properties related to stability and reconstruction of images from scattering coefficients.",1
"How to economically cluster large-scale multi-view images is a long-standing problem in computer vision. To tackle this challenge, we introduce a novel approach named Highly-economized Scalable Image Clustering (HSIC) that radically surpasses conventional image clustering methods via binary compression. We intuitively unify the binary representation learning and efficient binary cluster structure learning into a joint framework. In particular, common binary representations are learned by exploiting both sharable and individual information across multiple views to capture their underlying correlations. Meanwhile, cluster assignment with robust binary centroids is also performed via effective discrete optimization under L21-norm constraint. By this means, heavy continuous-valued Euclidean distance computations can be successfully reduced by efficient binary XOR operations during the clustering procedure. To our best knowledge, HSIC is the first binary clustering work specifically designed for scalable multi-view image clustering. Extensive experimental results on four large-scale image datasets show that HSIC consistently outperforms the state-of-the-art approaches, whilst significantly reducing computational time and memory footprint.",0
"The problem of economically clustering large-scale multi-view images has been a challenge in computer vision for a long time. Our solution to this challenge is a new approach called Highly-economized Scalable Image Clustering (HSIC), which surpasses traditional image clustering methods by using binary compression. We combine binary representation learning and efficient binary cluster structure learning into a joint framework, learning common binary representations by using both sharable and individual information across multiple views to capture their correlations. In addition, we perform cluster assignment with robust binary centroids via effective discrete optimization under L21-norm constraint. This reduces heavy continuous-valued Euclidean distance computations by using efficient binary XOR operations during the clustering procedure. HSIC is the first binary clustering method designed for scalable multi-view image clustering, and our extensive experimental results on four large-scale image datasets show that it consistently outperforms state-of-the-art approaches while significantly reducing computational time and memory footprint.",1
"Building behavior profiles of Android applications (apps) with holistic, rich and multi-view information (e.g., incorporating several semantic views of an app such as API sequences, system calls, etc.) would help catering downstream analytics tasks such as app categorization, recommendation and malware analysis significantly better. Towards this goal, we design a semi-supervised Representation Learning (RL) framework named apk2vec to automatically generate a compact representation (aka profile/embedding) for a given app. More specifically, apk2vec has the three following unique characteristics which make it an excellent choice for largescale app profiling: (1) it encompasses information from multiple semantic views such as API sequences, permissions, etc., (2) being a semi-supervised embedding technique, it can make use of labels associated with apps (e.g., malware family or app category labels) to build high quality app profiles, and (3) it combines RL and feature hashing which allows it to efficiently build profiles of apps that stream over time (i.e., online learning). The resulting semi-supervised multi-view hash embeddings of apps could then be used for a wide variety of downstream tasks such as the ones mentioned above. Our extensive evaluations with more than 42,000 apps demonstrate that apk2vec's app profiles could significantly outperform state-of-the-art techniques in four app analytics tasks namely, malware detection, familial clustering, app clone detection and app recommendation.",0
"To improve downstream analytics tasks such as app categorization, recommendation, and malware analysis, it would be helpful to construct behavior profiles of Android apps using comprehensive and diverse information, such as semantic views like API sequences and system calls. To achieve this objective, we have developed a semi-supervised Representation Learning (RL) framework named apk2vec. This framework can automatically generate a condensed representation (also known as a profile or embedding) for each app. apk2vec boasts three unique characteristics that make it an excellent option for large-scale app profiling: (1) it encompasses multiple semantic views, such as API sequences and permissions, (2) it is a semi-supervised embedding technique that can use labels associated with apps to create high-quality profiles, and (3) it combines RL and feature hashing to create profiles for apps that flow over time (i.e., online learning). These semi-supervised multi-view hash embeddings of apps can be used for various downstream tasks. Our extensive evaluations of over 42,000 apps demonstrate that apk2vec's app profiles can significantly outperform state-of-the-art methods in four app analytics tasks, including malware detection, familial clustering, app clone detection, and app recommendation.",1
"The ability to anticipate the future is essential when making real time critical decisions, provides valuable information to understand dynamic natural scenes, and can help unsupervised video representation learning. State-of-art video prediction is based on LSTM recursive networks and/or generative adversarial network learning. These are complex architectures that need to learn large numbers of parameters, are potentially hard to train, slow to run, and may produce blurry predictions. In this paper, we introduce DYAN, a novel network with very few parameters and easy to train, which produces accurate, high quality frame predictions, significantly faster than previous approaches. DYAN owes its good qualities to its encoder and decoder, which are designed following concepts from systems identification theory and exploit the dynamics-based invariants of the data. Extensive experiments using several standard video datasets show that DYAN is superior generating frames and that it generalizes well across domains.",0
"Anticipating the future is crucial for making real-time decisions, understanding natural scenes, and facilitating unsupervised video representation learning. The current state-of-the-art video prediction relies on complex LSTM recursive networks and/or generative adversarial network learning. These models are challenging to train, slow to run, and may produce blurry predictions due to their large number of parameters. This paper presents a new network called DYAN, which uses a minimal number of parameters and is simple to train. DYAN's encoder and decoder utilize systems identification theory and dynamics-based invariants to generate accurate frame predictions at a faster rate than previous methods. Extensive experiments on standard video datasets demonstrate that DYAN outperforms other models in generating frames and is adaptable across domains.",1
"Deep learning algorithms excel at extracting patterns from raw data, and with large datasets, they have been very successful in computer vision and natural language applications. However, in other domains, large datasets on which to learn representations from may not exist. In this work, we develop a novel multimodal CNN-MLP neural network architecture that utilizes both domain-specific feature engineering as well as learned representations from raw data. We illustrate the effectiveness of such network designs in the chemical sciences, for predicting biodegradability. DeepBioD, a multimodal CNN-MLP network is more accurate than either standalone network designs, and achieves an error classification rate of 0.125 that is 27% lower than the current state-of-the-art. Thus, our work indicates that combining traditional feature engineering with representation learning can be effective, particularly in situations where labeled data is limited.",0
"The ability of deep learning algorithms to detect patterns in raw data has made them highly successful in computer vision and natural language applications when using large datasets. However, in other areas where there may be a lack of large datasets from which to learn, this success may not be replicated. This study introduces a new approach that combines domain-specific feature engineering and learned representations from raw data using a novel multimodal CNN-MLP neural network architecture. The effectiveness of this combination is demonstrated in predicting biodegradability in the chemical sciences. The multimodal CNN-MLP network, DeepBioD, achieves a lower error classification rate of 0.125 that is 27% better than the current state-of-the-art, surpassing the performance of standalone network designs. Therefore, this study suggests that combining traditional feature engineering with representation learning can be highly effective, especially when labeled data is limited.",1
"Deep neural networks (DNN) excel at extracting patterns. Through representation learning and automated feature engineering on large datasets, such models have been highly successful in computer vision and natural language applications. Designing optimal network architectures from a principled or rational approach however has been less than successful, with the best successful approaches utilizing an additional machine learning algorithm to tune the network hyperparameters. However, in many technical fields, there exist established domain knowledge and understanding about the subject matter. In this work, we develop a novel furcated neural network architecture that utilizes domain knowledge as high-level design principles of the network. We demonstrate proof-of-concept by developing IL-Net, a furcated network for predicting the properties of ionic liquids, which is a class of complex multi-chemicals entities. Compared to existing state-of-the-art approaches, we show that furcated networks can improve model accuracy by approximately 20-35%, without using additional labeled data. Lastly, we distill two key design principles for furcated networks that can be adapted to other domains.",0
"Extracting patterns is a strength of deep neural networks (DNN). By utilizing representation learning and automated feature engineering on large datasets, DNN models have achieved great success in computer vision and natural language applications. However, designing optimal network architectures from a principled or rational approach has been a challenge. The best successful approaches have involved using an additional machine learning algorithm to tune the network hyperparameters. Nevertheless, many technical fields possess established domain knowledge and understanding about the subject matter. This work develops a novel furcated neural network architecture that utilizes domain knowledge as high-level design principles of the network. IL-Net, a furcated network for predicting the properties of ionic liquids, which is a class of complex multi-chemical entities, is presented as proof-of-concept. Compared to existing state-of-the-art approaches, the results show that furcated networks can improve model accuracy by approximately 20-35%, without requiring additional labeled data. Lastly, two key design principles for furcated networks are distilled, which can be adapted to other domains.",1
"The large pose discrepancy between two face images is one of the fundamental challenges in automatic face recognition. Conventional approaches to pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes a Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator enables DR-GAN to learn a representation that is both generative and discriminative, which can be used for face image synthesis and pose-invariant face recognition. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified identity representation along with an arbitrary number of synthetic face images. Extensive quantitative and qualitative evaluation on a number of controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art in both learning representations and rotating large-pose face images.",0
"One of the major difficulties in automatic face recognition is the significant difference in poses between two face images. Traditional methods for pose-invariant face recognition either conduct frontalization on a non-frontal face image or learn a pose-invariant representation from it. However, we believe that it is more advantageous to perform these tasks simultaneously to gain benefits from each other. In this study, we propose a Disentangled Representation learning-Generative Adversarial Network (DR-GAN) that has three unique features. The generator's encoder-decoder structure allows DR-GAN to learn a representation that can be used for both face image synthesis and pose-invariant face recognition. This representation is explicitly disentangled from other face variations such as pose, thanks to the pose code provided to the decoder and pose estimation in the discriminator. Furthermore, DR-GAN can accept one or multiple images as input and produce a single unified identity representation, as well as an arbitrary number of synthetic face images. DR-GAN's superiority over the state-of-the-art in learning representations and rotating large-pose face images is demonstrated through extensive quantitative and qualitative evaluations on various controlled and in-the-wild databases.",1
"Reliable facial expression recognition plays a critical role in human-machine interactions. However, most of the facial expression analysis methodologies proposed to date pay little or no attention to the protection of a user's privacy. In this paper, we propose a Privacy-Preserving Representation-Learning Variational Generative Adversarial Network (PPRL-VGAN) to learn an image representation that is explicitly disentangled from the identity information. At the same time, this representation is discriminative from the standpoint of facial expression recognition and generative as it allows expression-equivalent face image synthesis. We evaluate the proposed model on two public datasets under various threat scenarios. Quantitative and qualitative results demonstrate that our approach strikes a balance between the preservation of privacy and data utility. We further demonstrate that our model can be effectively applied to other tasks such as expression morphing and image completion.",0
"Facial expression recognition is crucial for human-machine interactions, but existing methodologies often overlook the importance of protecting user privacy. To address this issue, we introduce a Privacy-Preserving Representation-Learning Variational Generative Adversarial Network (PPRL-VGAN) that can learn an image representation that separates identity information from the facial expression. Our approach is both discriminative and generative, allowing for expression-equivalent face image synthesis. We evaluate our model on two public datasets and demonstrate that it strikes a balance between privacy preservation and data utility. Additionally, our model can be applied to tasks such as expression morphing and image completion.",1
"Network embedding algorithms are able to learn latent feature representations of nodes, transforming networks into lower dimensional vector representations. Typical key applications, which have effectively been addressed using network embeddings, include link prediction, multilabel classification and community detection. In this paper, we propose BiasedWalk, a scalable, unsupervised feature learning algorithm that is based on biased random walks to sample context information about each node in the network. Our random-walk based sampling can behave as Breath-First-Search (BFS) and Depth-First-Search (DFS) samplings with the goal to capture homophily and role equivalence between the nodes in the network. We have performed a detailed experimental evaluation comparing the performance of the proposed algorithm against various baseline methods, on several datasets and learning tasks. The experiment results show that the proposed method outperforms the baseline ones in most of the tasks and datasets.",0
"The ability of network embedding algorithms to acquire latent feature representations of nodes in a network and transform them into lower dimensional vector representations has proven to be effective in addressing various key applications such as community detection, multilabel classification, and link prediction. This paper introduces BiasedWalk, an unsupervised feature learning algorithm that uses biased random walks to gather context information about each node in the network, allowing it to behave like BFS and DFS samplings to capture homophily and role equivalence among the nodes. We conducted a comprehensive experimental evaluation of the proposed algorithm against various baseline methods on different datasets and learning tasks, and the results demonstrate that our approach outperforms the baseline ones in most cases.",1
"The recent success in human action recognition with deep learning methods mostly adopt the supervised learning paradigm, which requires significant amount of manually labeled data to achieve good performance. However, label collection is an expensive and time-consuming process. In this work, we propose an unsupervised learning framework, which exploits unlabeled data to learn video representations. Different from previous works in video representation learning, our unsupervised learning task is to predict 3D motion in multiple target views using video representation from a source view. By learning to extrapolate cross-view motions, the representation can capture view-invariant motion dynamics which is discriminative for the action. In addition, we propose a view-adversarial training method to enhance learning of view-invariant features. We demonstrate the effectiveness of the learned representations for action recognition on multiple datasets.",0
"Most deep learning methods for human action recognition rely on supervised learning, which necessitates a considerable amount of labeled data to achieve satisfactory performance. However, collecting labels is a time-consuming and costly task. This paper introduces an unsupervised learning framework that utilizes unlabeled data to develop video representations. Our unsupervised learning task differs from previous video representation learning approaches in that it aims to predict 3D motion in multiple target views based on video representations from a source view. By learning to extrapolate cross-view motions, the representation can capture view-invariant motion dynamics that are discriminating for the action. Additionally, this paper proposes a view-adversarial training method to improve learning of view-invariant features. The effectiveness of the learned representations for action recognition is demonstrated on multiple datasets.",1
"Multi-omic data provides multiple views of the same patients. Integrative analysis of multi-omic data is crucial to elucidate the molecular underpinning of disease etiology. However, multi-omic data has the ""big p, small N"" problem (the number of features is large, but the number of samples is small), it is challenging to train a complicated machine learning model from the multi-omic data alone and make it generalize well. Here we propose a framework termed Multi-view Factorization AutoEncoder with network constraints to integrate multi-omic data with domain knowledge (biological interactions networks). Our framework employs deep representation learning to learn feature embeddings and patient embeddings simultaneously, enabling us to integrate feature interaction network and patient view similarity network constraints into the training objective. The whole framework is end-to-end differentiable. We applied our approach to the TCGA Pan-cancer dataset and achieved satisfactory results to predict disease progression-free interval (PFI) and patient overall survival (OS) events. Code will be made publicly available.",0
"Multi-omic data offers various perspectives on patients. To comprehend the molecular basis of disease etiology, integrating multi-omic data is crucial. Nevertheless, multi-omic data has a ""big p, small N"" problem, making it challenging to train a complex machine learning model and ensure its generalization with limited samples. We propose a framework, Multi-view Factorization AutoEncoder with network constraints, which integrates multi-omic data with biological interaction networks through deep representation learning. Our framework simultaneously learns feature and patient embeddings and incorporates feature interaction network and patient view similarity network constraints into the training objective. The entire framework is end-to-end differentiable. We applied our approach to the TCGA Pan-cancer dataset and obtained satisfactory results in predicting disease progression-free interval and patient overall survival events. The code will be publicly available.",1
"The success of graph embeddings or node representation learning in a variety of downstream tasks, such as node classification, link prediction, and recommendation systems, has led to their popularity in recent years. Representation learning algorithms aim to preserve local and global network structure by identifying node neighborhood notions. However, many existing algorithms generate embeddings that fail to properly preserve the network structure, or lead to unstable representations due to random processes (e.g., random walks to generate context) and, thus, cannot generate to multi-graph problems. In this paper, we propose RECS, a novel, stable graph embedding algorithmic framework. RECS learns graph representations using connection subgraphs by employing the analogy of graphs with electrical circuits. It preserves both local and global connectivity patterns, and addresses the issue of high-degree nodes. Further, it exploits the strength of weak ties and meta-data that have been neglected by baselines. The experiments show that RECS outperforms state-of-the-art algorithms by up to 36.85% on multi-label classification problem. Further, in contrast to baselines, RECS, being deterministic, is completely stable.",0
"Graph embeddings or node representation learning have become popular due to their success in various downstream tasks, such as node classification, link prediction, and recommendation systems. These algorithms aim to identify node neighborhood notions to preserve local and global network structure. However, most existing algorithms fail to properly preserve network structure, resulting in unstable representations due to random processes, which cannot handle multi-graph problems. To address these issues, this paper proposes RECS, a novel, stable graph embedding algorithmic framework that learns graph representations using connection subgraphs. RECS employs the analogy of graphs with electrical circuits to preserve both local and global connectivity patterns and address the issue of high-degree nodes. It also exploits the strength of weak ties and meta-data that have been neglected by baselines. Experimental results show that RECS outperforms state-of-the-art algorithms by up to 36.85% on multi-label classification problem and is completely stable.",1
"We present compositional nearest neighbors (CompNN), a simple approach to visually interpreting distributed representations learned by a convolutional neural network (CNN) for pixel-level tasks (e.g., image synthesis and segmentation). It does so by reconstructing both a CNN's input and output image by copy-pasting corresponding patches from the training set with similar feature embeddings. To do so efficiently, it makes of a patch-match-based algorithm that exploits the fact that the patch representations learned by a CNN for pixel level tasks vary smoothly. Finally, we show that CompNN can be used to establish semantic correspondences between two images and control properties of the output image by modifying the images contained in the training set. We present qualitative and quantitative experiments for semantic segmentation and image-to-image translation that demonstrate that CompNN is a good tool for interpreting the embeddings learned by pixel-level CNNs.",0
"The technique we introduce is called compositional nearest neighbors (CompNN), which provides a straightforward method of comprehending distributed representations acquired by a convolutional neural network (CNN) for tasks involving individual pixels, such as image synthesis and segmentation. Our method accomplishes this by reassembling the input and output images of a CNN through the selection of comparable training set patches with matching feature embeddings. This process is facilitated by a patch-based algorithm that leverages the smooth variations in patch representations learned by a CNN for pixel-level tasks. We also demonstrate that CompNN can establish semantic correspondences between two images and control the output image's properties by modifying the images present in the training set. Our qualitative and quantitative experiments showcase the efficacy of CompNN for semantic segmentation and image-to-image translation, underscoring its value in interpreting the embeddings produced by pixel-level CNNs.",1
"We investigate the potential of a restricted Boltzmann Machine (RBM) for discriminative representation learning. By imposing the class information preservation constraints on the hidden layer of the RBM, we propose a Signed Laplacian Restricted Boltzmann Machine (SLRBM) for supervised discriminative representation learning. The model utilizes the label information and preserves the global data locality of data points simultaneously. Experimental results on the benchmark data set show the effectiveness of our method.",0
"Our study explores the capability of a restricted Boltzmann Machine (RBM) in learning discriminative representations. We introduce the Signed Laplacian Restricted Boltzmann Machine (SLRBM), which enforces constraints on the hidden layer to preserve class information. Our proposed model utilizes label information and maintains the global data locality of data points concurrently. Through experiments on a benchmark dataset, we demonstrate the efficacy of our approach.",1
"Sum-Product Networks (SPNs) are recently introduced deep tractable probabilistic models by which several kinds of inference queries can be answered exactly and in a tractable time. Up to now, they have been largely used as black box density estimators, assessed only by comparing their likelihood scores only. In this paper we explore and exploit the inner representations learned by SPNs. We do this with a threefold aim: first we want to get a better understanding of the inner workings of SPNs; secondly, we seek additional ways to evaluate one SPN model and compare it against other probabilistic models, providing diagnostic tools to practitioners; lastly, we want to empirically evaluate how good and meaningful the extracted representations are, as in a classic Representation Learning framework. In order to do so we revise their interpretation as deep neural networks and we propose to exploit several visualization techniques on their node activations and network outputs under different types of inference queries. To investigate these models as feature extractors, we plug some SPNs, learned in a greedy unsupervised fashion on image datasets, in supervised classification learning tasks. We extract several embedding types from node activations by filtering nodes by their type, by their associated feature abstraction level and by their scope. In a thorough empirical comparison we prove them to be competitive against those generated from popular feature extractors as Restricted Boltzmann Machines. Finally, we investigate embeddings generated from random probabilistic marginal queries as means to compare other tractable probabilistic models on a common ground, extending our experiments to Mixtures of Trees.",0
"SPNs, or Sum-Product Networks, are a type of deep tractable probabilistic model that can provide exact answers to various inference queries within a reasonable amount of time. Currently, they are primarily used as black box density estimators and evaluated solely based on their likelihood scores. However, in this paper, we aim to delve deeper into the inner workings of SPNs and explore the representations they learn. Our objectives are threefold: firstly, to gain a better understanding of how SPNs work; secondly, to provide practitioners with additional methods to evaluate and compare SPN models against other probabilistic models; and lastly, to assess the quality and significance of the extracted representations using a classic Representation Learning framework. To achieve this, we reinterpret SPNs as deep neural networks and utilize visualization techniques to analyze their node activations and network outputs under different inference queries. We also test their capability as feature extractors by integrating SPNs, learned through unsupervised learning on image datasets, into supervised classification learning tasks. We extract various types of embeddings from the node activations by filtering nodes based on their type, associated feature abstraction level, and scope. Through extensive empirical comparisons, we demonstrate that these embeddings are as competitive as those generated by popular feature extractors like Restricted Boltzmann Machines. Finally, we extend our experiments to Mixtures of Trees and use embeddings generated from random probabilistic marginal queries to compare other tractable probabilistic models on a common ground.",1
"Network embedding methodologies, which learn a distributed vector representation for each vertex in a network, have attracted considerable interest in recent years. Existing works have demonstrated that vertex representation learned through an embedding method provides superior performance in many real-world applications, such as node classification, link prediction, and community detection. However, most of the existing methods for network embedding only utilize topological information of a vertex, ignoring a rich set of nodal attributes (such as, user profiles of an online social network, or textual contents of a citation network), which is abundant in all real-life networks. A joint network embedding that takes into account both attributional and relational information entails a complete network information and could further enrich the learned vector representations. In this work, we present Neural-Brane, a novel Neural Bayesian Personalized Ranking based Attributed Network Embedding. For a given network, Neural-Brane extracts latent feature representation of its vertices using a designed neural network model that unifies network topological information and nodal attributes; Besides, it utilizes Bayesian personalized ranking objective, which exploits the proximity ordering between a similar node-pair and a dissimilar node-pair. We evaluate the quality of vertex embedding produced by Neural-Brane by solving the node classification and clustering tasks on four real-world datasets. Experimental results demonstrate the superiority of our proposed method over the state-of-the-art existing methods.",0
"Recently, there has been a growing interest in network embedding methodologies that aim to learn a distributed vector representation for each vertex in a network. Previous research has shown that using an embedding method to learn vertex representations can lead to better performance in various real-world applications, including node classification, link prediction, and community detection. However, most existing network embedding methods only consider the topological information of a vertex, neglecting the rich set of nodal attributes present in real-life networks (such as user profiles in an online social network or textual contents in a citation network). A joint network embedding method that takes into account both attributional and relational information can provide a more complete network information and improve the learned vector representations. In this study, we propose a novel Neural Bayesian Personalized Ranking based Attributed Network Embedding method called Neural-Brane. This method extracts latent feature representations of network vertices using a neural network model that combines both topological and nodal attribute information, and utilizes a Bayesian personalized ranking objective that considers the proximity ordering between similar and dissimilar node-pairs. We evaluate the quality of the vertex embedding produced by Neural-Brane by solving node classification and clustering tasks on four real-world datasets. Our experimental results demonstrate that our proposed method outperforms existing state-of-the-art methods.",1
"Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.",0
"To behave intelligently in the real world, one must be able to acquire new knowledge from ongoing experiences and maintain past knowledge. Our proposal is a unique algorithm called Variational Autoencoder with Shared Embeddings (VASE), which learns unsupervised representations from visual data that is piece-wise stationary. VASE uses the Minimum Description Length principle to automatically detect changes in the data distribution and allocate additional representational capacity to new knowledge while safeguarding previously learned representations from catastrophic forgetting. Our approach promotes disentangled representations, which allows VASE to handle uncertain inputs, enhance its own representations through imagination-based exploration, and most importantly, exhibit meaningful sharing of latents between different datasets. Compared to entangled representation baselines, our approach goes beyond surface-level statistics and performs meaningful cross-domain inference.",1
"Object categories inherently form a hierarchy with different levels of concept abstraction, especially for fine-grained categories. For example, birds (Aves) can be categorized according to a four-level hierarchy of order, family, genus, and species. This hierarchy encodes rich correlations among various categories across different levels, which can effectively regularize the semantic space and thus make prediction less ambiguous. However, previous studies of fine-grained image recognition primarily focus on categories of one certain level and usually overlook this correlation information. In this work, we investigate simultaneously predicting categories of different levels in the hierarchy and integrating this structured correlation information into the deep neural network by developing a novel Hierarchical Semantic Embedding (HSE) framework. Specifically, the HSE framework sequentially predicts the category score vector of each level in the hierarchy, from highest to lowest. At each level, it incorporates the predicted score vector of the higher level as prior knowledge to learn finer-grained feature representation. During training, the predicted score vector of the higher level is also employed to regularize label prediction by using it as soft targets of corresponding sub-categories. To evaluate the proposed framework, we organize the 200 bird species of the Caltech-UCSD birds dataset with the four-level category hierarchy and construct a large-scale butterfly dataset that also covers four level categories. Extensive experiments on these two and the newly-released VegFru datasets demonstrate the superiority of our HSE framework over the baseline methods and existing competitors.",0
"Categories of objects naturally form a hierarchy that includes various levels of concept abstraction, particularly for fine-grained categories. For instance, birds (Aves) can be classified based on a four-level hierarchy of order, family, genus, and species, which reveals connections among different categories across levels and promotes regularity in the semantic space, thereby reducing ambiguity in predictions. However, research on fine-grained image recognition has mostly concentrated on categories of a single level, failing to acknowledge this correlation information. This study explores the simultaneous prediction of categories at different levels in the hierarchy and integrates this structured correlation information into a deep neural network using a novel Hierarchical Semantic Embedding (HSE) framework. The HSE framework predicts the category score vector of each level in the hierarchy sequentially, from highest to lowest, and incorporates the predicted score vector of the higher level as prior knowledge to learn finer-grained feature representation at each level. During training, the predicted score vector of the higher level is also utilized to regulate label prediction by using it as soft targets of corresponding sub-categories. To test the effectiveness of the proposed framework, a four-level category hierarchy is established for the 200 bird species of the Caltech-UCSD birds dataset, and a large-scale butterfly dataset covering four level categories is constructed. The VegFru dataset is also utilized in the evaluation. The results of extensive experiments on these datasets demonstrate the superiority of our HSE framework over the baseline methods and existing competitors.",1
"While machine learning approaches to visual emotion recognition offer great promise, current methods consider training and testing models on small scale datasets covering limited visual emotion concepts. Our analysis identifies an important but long overlooked issue of existing visual emotion benchmarks in the form of dataset biases. We design a series of tests to show and measure how such dataset biases obstruct learning a generalizable emotion recognition model. Based on our analysis, we propose a webly supervised approach by leveraging a large quantity of stock image data. Our approach uses a simple yet effective curriculum guided training strategy for learning discriminative emotion features. We discover that the models learned using our large scale stock image dataset exhibit significantly better generalization ability than the existing datasets without the manual collection of even a single label. Moreover, visual representation learned using our approach holds a lot of promise across a variety of tasks on different image and video datasets.",0
"Although machine learning techniques for recognizing emotions through visual cues show potential, current methods rely on training and testing models on small datasets that cover only a limited range of emotional concepts. Our analysis reveals a previously overlooked problem with existing visual emotion benchmarks: dataset biases. To demonstrate how these biases impede the development of a generalizable emotion recognition model, we conduct a series of tests. Our solution is a webly supervised approach that utilizes a vast quantity of stock image data. By employing a simple yet effective curriculum-guided training strategy for identifying discriminative emotion features, our approach produces models that exhibit superior generalization abilities compared to existing datasets, even without manual labeling. Furthermore, our visually-representative approach demonstrates promise across a range of tasks involving different image and video datasets.",1
"The increasing availability of electrocardiogram (ECG) data has motivated the use of data-driven models for automating various clinical tasks based on ECG data. The development of subject-specific models are limited by the cost and difficulty of obtaining sufficient training data for each individual. The alternative of population model, however, faces challenges caused by the significant inter-subject variations within the ECG data. We address this challenge by investigating for the first time the problem of learning representations for clinically-informative variables while disentangling other factors of variations within the ECG data. In this work, we present a conditional variational autoencoder (VAE) to extract the subject-specific adjustment to the ECG data, conditioned on task-specific representations learned from a deterministic encoder. To encourage the representation for inter-subject variations to be independent from the task-specific representation, maximum mean discrepancy is used to match all the moments between the distributions learned by the VAE conditioning on the code from the deterministic encoder. The learning of the task-specific representation is regularized by a weak supervision in the form of contrastive regularization. We apply the proposed method to a novel yet important clinical task of classifying the origin of ventricular tachycardia (VT) into pre-defined segments, demonstrating the efficacy of the proposed method against the standard VAE.",0
"The use of data-driven models for clinical tasks based on electrocardiogram (ECG) data has been motivated by its increasing availability. However, developing subject-specific models is constrained by the cost and difficulty of attaining sufficient training data for every individual. On the other hand, population models face challenges posed by the considerable inter-subject variations within the ECG data. To overcome this, we address the problem of learning representations for clinically-informative variables while disentangling other factors of variations within the ECG data. Our approach involves presenting a conditional variational autoencoder (VAE) that extracts the subject-specific adjustment to the ECG data, conditioned on task-specific representations learned from a deterministic encoder. To ensure the representation for inter-subject variations is independent of the task-specific representation, we use maximum mean discrepancy to match all the moments between the distributions learned by the VAE conditioning on the code from the deterministic encoder. The learning of the task-specific representation is regularized by weak supervision in the form of contrastive regularization. We apply this method to a novel and significant clinical task of classifying the origin of ventricular tachycardia (VT) into pre-defined segments, demonstrating the effectiveness of our proposed method compared to the standard VAE.",1
"Instance-level human parsing towards real-world human analysis scenarios is still under-explored due to the absence of sufficient data resources and technical difficulty in parsing multiple instances in a single pass. Several related works all follow the ""parsing-by-detection"" pipeline that heavily relies on separately trained detection models to localize instances and then performs human parsing for each instance sequentially. Nonetheless, two discrepant optimization targets of detection and parsing lead to suboptimal representation learning and error accumulation for final results. In this work, we make the first attempt to explore a detection-free Part Grouping Network (PGN) for efficiently parsing multiple people in an image in a single pass. Our PGN reformulates instance-level human parsing as two twinned sub-tasks that can be jointly learned and mutually refined via a unified network: 1) semantic part segmentation for assigning each pixel as a human part (e.g., face, arms); 2) instance-aware edge detection to group semantic parts into distinct person instances. Thus the shared intermediate representation would be endowed with capabilities in both characterizing fine-grained parts and inferring instance belongings of each part. Finally, a simple instance partition process is employed to get final results during inference. We conducted experiments on PASCAL-Person-Part dataset and our PGN outperforms all state-of-the-art methods. Furthermore, we show its superiority on a newly collected multi-person parsing dataset (CIHP) including 38,280 diverse images, which is the largest dataset so far and can facilitate more advanced human analysis. The CIHP benchmark and our source code are available at http://sysu-hcp.net/lip/.",0
"Due to insufficient data resources and technical difficulties in parsing multiple instances in a single pass, there has been limited exploration of instance-level human parsing in real-world human analysis scenarios. Most related works rely on separate detection models to localize instances and then sequentially perform human parsing, resulting in suboptimal representation learning and error accumulation for final results due to the discrepant optimization targets of detection and parsing. In this study, we propose a detection-free Part Grouping Network (PGN) that can efficiently parse multiple people in a single pass. Our PGN reformulates instance-level human parsing into two sub-tasks that can be jointly learned and refined via a unified network: semantic part segmentation and instance-aware edge detection. The shared intermediate representation enables fine-grained part characterization and instance inference. During inference, a simple instance partition process is used to obtain final results. We evaluated our PGN on the PASCAL-Person-Part dataset and a newly collected CIHP dataset, which is the largest multi-person parsing dataset to date. Our PGN outperforms all state-of-the-art methods and can facilitate more advanced human analysis. The CIHP benchmark and our source code are available at http://sysu-hcp.net/lip/.",1
"Many loss functions in representation learning are invariant under a continuous symmetry transformation. For example, the loss function of word embeddings (Mikolov et al., 2013) remains unchanged if we simultaneously rotate all word and context embedding vectors. We show that representation learning models for time series possess an approximate continuous symmetry that leads to slow convergence of gradient descent. We propose a new optimization algorithm that speeds up convergence using ideas from gauge theory in physics. Our algorithm leads to orders of magnitude faster convergence and to more interpretable representations, as we show for dynamic extensions of matrix factorization and word embedding models. We further present an example application of our proposed algorithm that translates modern words into their historic equivalents.",0
"Continuous symmetry transformations are prevalent in various loss functions used for representation learning. A case in point is the loss function for word embeddings, where rotating all word and context embedding vectors simultaneously does not affect the function's outcome. In our study, we discovered that time series representation learning models exhibit an approximate continuous symmetry that slows down gradient descent. To address this issue, we developed an optimization algorithm inspired by gauge theory in physics that significantly accelerates convergence and yields more interpretable representations. We demonstrate the effectiveness of our approach on dynamic extensions of matrix factorization and word embedding models, showcasing orders of magnitude faster convergence and improved interpretability. Additionally, we showcase an application of our algorithm that translates modern words to their historical counterparts.",1
"Denoising autoencoders (DAEs) have proven useful for unsupervised representation learning, but a thorough theoretical understanding is still lacking of how the input noise influences learning. Here we develop theory for how noise influences learning in DAEs. By focusing on linear DAEs, we are able to derive analytic expressions that exactly describe their learning dynamics. We verify our theoretical predictions with simulations as well as experiments on MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise allows DAEs to ignore low variance directions in the inputs while learning to reconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs to standard regularised autoencoders, we show that noise has a similar regularisation effect to weight decay, but with faster training dynamics. We also show that our theoretical predictions approximate learning dynamics on real-world data and qualitatively match observed dynamics in nonlinear DAEs.",0
"Although denoising autoencoders (DAEs) are useful for unsupervised representation learning, the impact of input noise on learning lacks a comprehensive theoretical understanding. In this study, we develop a theory that explains how noise influences learning in DAEs. By focusing on linear DAEs, we derive analytic expressions that precisely describe their learning dynamics and validate our predictions through simulations and experiments on MNIST and CIFAR-10. Our theory demonstrates that, when correctly tuned, noise enables DAEs to disregard low variance directions in the inputs while learning to reconstruct them. Additionally, we compare the learning dynamics of DAEs to standard regularised autoencoders and find that noise has a similar regularisation effect as weight decay but with faster training dynamics. We also show that our theoretical predictions closely approximate learning dynamics on real-world data and match observed dynamics in nonlinear DAEs.",1
"We examine two fundamental tasks associated with graph representation learning: link prediction and semi-supervised node classification. We present a novel autoencoder architecture capable of learning a joint representation of both local graph structure and available node features for the multi-task learning of link prediction and node classification. Our autoencoder architecture is efficiently trained end-to-end in a single learning stage to simultaneously perform link prediction and node classification, whereas previous related methods require multiple training steps that are difficult to optimize. We provide a comprehensive empirical evaluation of our models on nine benchmark graph-structured datasets and demonstrate significant improvement over related methods for graph representation learning. Reference code and data are available at https://github.com/vuptran/graph-representation-learning",0
"In this study, we focus on two key tasks in graph representation learning: predicting links and classifying nodes in a semi-supervised manner. To achieve multi-task learning for these tasks, we propose a new autoencoder architecture that can jointly learn the local graph structure and node features. Unlike previous methods that require multiple training steps, our approach can efficiently train end-to-end in a single stage. We conducted extensive experiments on nine benchmark datasets, and our results show significant improvements over related methods. The reference code and data can be found at https://github.com/vuptran/graph-representation-learning.",1
"Deeper convolutional neural networks provide more capacity to approximate complex mapping functions. However, increasing network depth imposes difficulties on training and increases model complexity. This paper presents a new nonlinear computational layer of considerably high capacity to the deep convolutional neural network architectures. This layer performs a set of comprehensive convolution operations that mimics the overall function of the human visual system (HVS) via focusing on learning structural information in its input. The core of its computations is evaluating the components of the structural similarity metric (SSIM) in a setting that allows the kernels to learn to match structural information. The proposed SSIMLayer is inherently nonlinear and hence, it does not require subsequent nonlinear transformations. Experiments conducted on CIFAR-10 benchmark demonstrates that the SSIMLayer provides better convergence than the traditional convolutional layer, bypasses the need for nonlinear transformations and shows more robustness against noise perturbations and adversarial attacks.",0
"The capacity of convolutional neural networks to approximate complex mapping functions increases with their depth. However, training becomes difficult and model complexity increases with deeper networks. This study introduces a new nonlinear computational layer with high capacity for deep convolutional neural network architectures. The layer performs comprehensive convolution operations that imitate the human visual system by focusing on learning structural information in its input. Its computations evaluate the components of the structural similarity metric to enable kernels to learn how to match structural information. The proposed SSIMLayer is nonlinear, eliminating the need for subsequent nonlinear transformations. Experimentation on CIFAR-10 benchmark shows that the SSIMLayer provides better convergence than traditional convolutional layers, is more resistant to noise perturbations and adversarial attacks, and does not require nonlinear transformations.",1
"Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level semantic features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24).",0
"Although convolutional neural networks (CNNs) have led to steady progress in video analysis, the improvement in this field has not been as significant as in 2D static image classification due to three main challenges: spatial feature representation, temporal information representation, and model/computation complexity. Researchers have recently found promising results using 3D CNNs inflated from 2D networks and pretrained on ImageNet for spatial and temporal representation learning, but these models are more expensive and prone to overfitting. To balance speed and accuracy, we explored critical network design choices and found that replacing many 3D convolutions with low-cost 2D convolutions resulted in the best performance, especially when replacing 3D convolutions at the bottom of the network. Our system also includes several cost-effective designs, such as separable spatial/temporal convolution and feature gating, which produce competitive results on various action classification and detection benchmarks.",1
"We propose a new clustering method based on optimal transportation. We solve optimal transportation with variational principles, and investigate the use of power diagrams as transportation plans for aggregating arbitrary domains into a fixed number of clusters. We iteratively drive centroids through target domains while maintaining the minimum clustering energy by adjusting the power diagrams. Thus, we simultaneously pursue clustering and the Wasserstein distances between the centroids and the target domains, resulting in a measure-preserving mapping. We demonstrate the use of our method in domain adaptation, remeshing, and representation learning on synthetic and real data.",0
"A novel approach to clustering is suggested, which utilizes optimal transportation. The optimal transportation problem is solved using variational principles, and power diagrams are explored as a means of transportation planning to group diverse domains into a predetermined number of clusters. The centroids are moved through the target domains in an iterative manner while minimizing the clustering energy by modifying the power diagrams. Consequently, the clustering and the Wasserstein distances between the centroids and the target domains are simultaneously pursued, leading to a mapping that preserves the measure. Our method is applied to synthetic and real data to illustrate its efficacy in domain adaptation, remeshing, and representation learning.",1
"In this report we propose a classification technique for skin lesion images as a part of our submission for ISIC 2018 Challenge in Skin Lesion Analysis Towards Melanoma Detection. Our data was extracted from the ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection grand challenge datasets. The features are extracted through a Convolutional Neural Network, in our case ResNet50 and then using these features we train a DeepForest, having cascading layers, to classify our skin lesion images. We know that Convolutional Neural Networks are a state-of-the-art technique in representation learning for images, with the convolutional filters learning to detect features from images through backpropagation. These features are then usually fed to a classifier like a softmax layer or other such classifiers for classification tasks. In our case we do not use the traditional backpropagation method and train a softmax layer for classification. Instead, we use Deep Forest, a novel decision tree ensemble approach with performance highly competitive to deep neural networks in a broad range of tasks. Thus we use a ResNet50 to extract the features from skin lesion images and then use the Deep Forest to classify these images. This method has been used because Deep Forest has been found to be hugely efficient in areas where there are only small-scale training data available. Also as the Deep Forest network decides its complexity by itself, it also caters to the problem of dataset imbalance we faced in this problem.",0
"Our report proposes a method for classifying skin lesion images in the ISIC 2018 Challenge for Skin Lesion Analysis Towards Melanoma Detection. We obtained our data from the ISIC 2018 grand challenge datasets and used a Convolutional Neural Network (CNN), specifically ResNet50, to extract features from the images. These features were then used to train a Deep Forest, a novel decision tree ensemble approach, for image classification. Unlike traditional backpropagation methods, we did not use a softmax layer for classification. Instead, we opted for the highly competitive Deep Forest approach, which proved to be efficient in handling small-scale training data and dataset imbalance.",1
"Recently, image-to-image translation has been made much progress owing to the success of conditional Generative Adversarial Networks (cGANs). And some unpaired methods based on cycle consistency loss such as DualGAN, CycleGAN and DiscoGAN are really popular. However, it's still very challenging for translation tasks with the requirement of high-level visual information conversion, such as photo-to-caricature translation that requires satire, exaggeration, lifelikeness and artistry. We present an approach for learning to translate faces in the wild from the source photo domain to the target caricature domain with different styles, which can also be used for other high-level image-to-image translation tasks. In order to capture global structure with local statistics while translation, we design a dual pathway model with one coarse discriminator and one fine discriminator. For generator, we provide one extra perceptual loss in association with adversarial loss and cycle consistency loss to achieve representation learning for two different domains. Also the style can be learned by the auxiliary noise input. Experiments on photo-to-caricature translation of faces in the wild show considerable performance gain of our proposed method over state-of-the-art translation methods as well as its potential real applications.",0
"Conditional Generative Adversarial Networks (cGANs) have greatly advanced image-to-image translation, and unpaired methods like DualGAN, CycleGAN, and DiscoGAN are popular. However, translating high-level visual information, such as photo-to-caricature conversion, remains a challenge due to the need for satire, exaggeration, lifelikeness, and artistry. Our approach translates faces from a source photo domain to a target caricature domain with different styles, which can be used for other high-level image-to-image translation tasks. To capture global structure with local statistics during translation, we use a dual pathway model with one coarse and one fine discriminator. We also provide an extra perceptual loss in association with adversarial and cycle consistency losses to achieve representation learning for two different domains. The style can be learned through an auxiliary noise input. Our experiments show significant performance gains over state-of-the-art translation methods, demonstrating the potential for real-world applications.",1
"In recent years, deep generative models have been shown to 'imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans -- a plausible sequence of observations that transition a dynamical system from its current configuration to a desired goal state, which can later be used as a reference trajectory for control. We focus on systems with high-dimensional observations, such as images, and propose an approach that naturally combines representation learning and planning. Our framework learns a generative model of sequential observations, where the generative process is induced by a transition in a low-dimensional planning model, and an additional noise. By maximizing the mutual information between the generated observations and the transition in the planning model, we obtain a low-dimensional representation that best explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. Finally, to generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on imagining plausible visual plans of rope manipulation.",0
"Recently, deep generative models have proven capable of creating convincing high-dimensional observations, including images, audio, and video, by learning from raw data. This study examines how to generate goal-directed visual plans that can serve as a reference trajectory for control. Our focus is on systems with high-dimensional observations, such as images. We propose a framework that combines representation learning and planning. Our framework learns a generative model of sequential observations induced by a transition in a low-dimensional planning model and additional noise. We maximize the mutual information between the generated observations and the transition in the planning model to obtain a low-dimensional representation that explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several models based on discrete or continuous states. To generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on creating plausible visual plans of rope manipulation.",1
"Visual question answering (VQA) models respond to open-ended natural language questions about images. While VQA is an increasingly popular area of research, it is unclear to what extent current VQA architectures learn key semantic distinctions between visually-similar images. To investigate this question, we explore a reformulation of the VQA task that challenges models to identify counterexamples: images that result in a different answer to the original question. We introduce two methods for evaluating existing VQA models against a supervised counterexample prediction task, VQA-CX. While our models surpass existing benchmarks on VQA-CX, we find that the multimodal representations learned by an existing state-of-the-art VQA model do not meaningfully contribute to performance on this task. These results call into question the assumption that successful performance on the VQA benchmark is indicative of general visual-semantic reasoning abilities.",0
"The task of visual question answering (VQA) involves answering natural language questions about images in an open-ended manner. Despite its growing popularity, it remains uncertain if current VQA architectures can accurately distinguish between visually-similar images on a semantic level. To address this, we modify the VQA task to include counterexamples, or images that produce a different answer to the original question, and evaluate existing models using a supervised counterexample prediction task called VQA-CX. Our study introduces two methods for this evaluation and demonstrates that our models outperform existing benchmarks on VQA-CX. However, we discover that the multimodal representations learned by a state-of-the-art VQA model do not significantly contribute to performance on this task. As a result, we question the assumption that high performance on the VQA benchmark indicates proficiency in general visual-semantic reasoning.",1
"Knowledge graphs have emerged as an important model for studying complex multi-relational data. This has given rise to the construction of numerous large scale but incomplete knowledge graphs encoding information extracted from various resources. An effective and scalable approach to jointly learn over multiple graphs and eventually construct a unified graph is a crucial next step for the success of knowledge-based inference for many downstream applications. To this end, we propose LinkNBed, a deep relational learning framework that learns entity and relationship representations across multiple graphs. We identify entity linkage across graphs as a vital component to achieve our goal. We design a novel objective that leverage entity linkage and build an efficient multi-task training procedure. Experiments on link prediction and entity linkage demonstrate substantial improvements over the state-of-the-art relational learning approaches.",0
"The study of complex multi-relational data has been greatly aided by the emergence of knowledge graphs. Despite the construction of numerous large-scale graphs, information extracted from various resources has led to their incompleteness. To successfully utilize knowledge-based inference for many downstream applications, an effective and scalable approach to jointly learn over multiple graphs and construct a unified graph is necessary. Our proposed framework, LinkNBed, is a deep relational learning model that learns entity and relationship representations across multiple graphs. We recognize the importance of entity linkage across graphs and have designed a novel objective that incorporates this idea into an efficient multi-task training procedure. In experiments involving link prediction and entity linkage, we have demonstrated substantial improvements over existing relational learning approaches.",1
"We demonstrate an approach to face attribute detection that retains or improves attribute detection accuracy across gender and race subgroups by learning demographic information prior to learning the attribute detection task. The system, which we call InclusiveFaceNet, detects face attributes by transferring race and gender representations learned from a held-out dataset of public race and gender identities. Leveraging learned demographic representations while withholding demographic inference from the downstream face attribute detection task preserves potential users' demographic privacy while resulting in some of the best reported numbers to date on attribute detection in the Faces of the World and CelebA datasets.",0
"Our approach to detecting face attributes, named InclusiveFaceNet, utilizes demographic information to enhance accuracy across gender and race subgroups. By first learning demographic representations from a separate dataset of public race and gender identities, we are able to transfer this knowledge to the face attribute detection task. This method ensures that users' demographic privacy is maintained while achieving impressive results on the Faces of the World and CelebA datasets.",1
"Neural network models and deep models are one of the leading and state of the art models in machine learning. Most successful deep neural models are the ones with many layers which highly increases their number of parameters. Training such models requires a large number of training samples which is not always available. One of the fundamental issues in neural networks is overfitting which is the issue tackled in this thesis. Such problem often occurs when the training of large models is performed using few training samples. Many approaches have been proposed to prevent the network from overfitting and improve its generalization performance such as data augmentation, early stopping, parameters sharing, unsupervised learning, dropout, batch normalization, etc.   In this thesis, we tackle the neural network overfitting issue from a representation learning perspective by considering the situation where few training samples are available which is the case of many real world applications. We propose three contributions. The first one presented in chapter 2 is dedicated to dealing with structured output problems to perform multivariate regression when the output variable y contains structural dependencies between its components. The second contribution described in chapter 3 deals with the classification task where we propose to exploit prior knowledge about the internal representation of the hidden layers in neural networks. Our last contribution presented in chapter 4 showed the interest of transfer learning in applications where only few samples are available. In this contribution, we provide an automatic system based on such learning scheme with an application to medical domain. In this application, the task consists in localizing the third lumbar vertebra in a 3D CT scan. This work has been done in collaboration with the clinic Rouen Henri Becquerel Center who provided us with data.",0
"Machine learning has made significant advancements with the emergence of neural network models and deep models, which are considered to be state-of-the-art. To increase the number of parameters, the most successful deep neural models contain multiple layers; however, their training requires a substantial amount of data, which may not always be available. Overfitting is a critical issue with neural networks, particularly when large models are trained with a limited amount of data. Various approaches have been suggested to address this problem, including data augmentation, early stopping, parameters sharing, unsupervised learning, dropout, and batch normalization. This thesis focuses on the overfitting problem from a representation learning viewpoint, with a specific emphasis on applications with limited training data. Three contributions are presented: the first deals with structured output problems in multivariate regression, the second exploits prior knowledge about the internal representation of hidden layers in classification, and the third examines the usefulness of transfer learning in applications with few samples. The third contribution includes an automatic system designed for medical domain applications, such as the localization of the third lumbar vertebra in a 3D CT scan, which was completed in collaboration with the clinic Rouen Henri Becquerel Center who provided the data.",1
"Face recognition technology has demonstrated tremendous progress over the past few years, primarily due to advances in representation learning. As we witness the widespread adoption of these systems, it is imperative to consider the security of face representations. In this paper, we explore the practicality of using a fully homomorphic encryption based framework to secure a database of face templates. This framework is designed to preserve the privacy of users and prevent information leakage from the templates, while maintaining their utility through template matching directly in the encrypted domain. Additionally, we also explore a batching and dimensionality reduction scheme to trade-off face matching accuracy and computational complexity. Experiments on benchmark face datasets (LFW, IJB-A, IJB-B, CASIA) indicate that secure face matching can be practically feasible (16 KB template size and 0.01 sec per match pair for 512-dimensional features from SphereFace) while exhibiting minimal loss in matching performance.",0
"Due to advances in representation learning, face recognition technology has made significant progress in recent years. As these systems become more widely adopted, it is crucial to consider the security of face representations. This study examines the practicality of safeguarding a database of face templates using a fully homomorphic encryption based framework. The aim is to maintain the utility of the templates while protecting users' privacy and preventing information leakage. Furthermore, the paper explores a batching and dimensionality reduction scheme to balance face matching accuracy and computational complexity. The experiments conducted on benchmark face datasets (LFW, IJB-A, IJB-B, CASIA) demonstrate that secure face matching is feasible with a 16 KB template size and 0.01 sec per match pair for 512-dimensional features from SphereFace, with minimal loss in matching performance.",1
"In this paper, we propose a novel ranking framework for collaborative filtering with the overall aim of learning user preferences over items by minimizing a pairwise ranking loss. We show the minimization problem involves dependent random variables and provide a theoretical analysis by proving the consistency of the empirical risk minimization in the worst case where all users choose a minimal number of positive and negative items. We further derive a Neural-Network model that jointly learns a new representation of users and items in an embedded space as well as the preference relation of users over the pairs of items. The learning objective is based on three scenarios of ranking losses that control the ability of the model to maintain the ordering over the items induced from the users' preferences, as well as, the capacity of the dot-product defined in the learned embedded space to produce the ordering. The proposed model is by nature suitable for implicit feedback and involves the estimation of only very few parameters. Through extensive experiments on several real-world benchmarks on implicit data, we show the interest of learning the preference and the embedding simultaneously when compared to learning those separately. We also demonstrate that our approach is very competitive with the best state-of-the-art collaborative filtering techniques proposed for implicit feedback.",0
"This paper presents a new approach for collaborative filtering that aims to learn user preferences for items by reducing pairwise ranking loss. The minimization problem involves dependent random variables, and we provide a theoretical analysis by proving the empirical risk minimization consistency in the worst-case scenario. Additionally, we introduce a Neural-Network model that simultaneously learns user and item representations in an embedded space and the preference relation of users over item pairs. We use three ranking loss scenarios to control the model's ability to maintain the ordering of users' preferences and the dot-product's capacity in the learned embedded space to produce the ordering. Our model is well-suited for implicit feedback and requires estimation of few parameters. We conduct extensive experiments on implicit data from real-world benchmarks and demonstrate that learning preferences and embeddings simultaneously is more effective than learning them separately. Furthermore, we show that our approach is highly competitive with the best state-of-the-art collaborative filtering techniques designed for implicit feedback.",1
"Computer-aided diagnosis (CAD) techniques for lung field segmentation from chest radiographs (CXR) have been proposed for adult cohorts, but rarely for pediatric subjects. Statistical shape models (SSMs), the workhorse of most state-of-the-art CXR-based lung field segmentation methods, do not efficiently accommodate shape variation of the lung field during the pediatric developmental stages. The main contributions of our work are: (1) a generic lung field segmentation framework from CXR accommodating large shape variation for adult and pediatric cohorts; (2) a deep representation learning detection mechanism, \emph{ensemble space learning}, for robust object localization; and (3) \emph{marginal shape deep learning} for the shape deformation parameter estimation. Unlike the iterative approach of conventional SSMs, the proposed shape learning mechanism transforms the parameter space into marginal subspaces that are solvable efficiently using the recursive representation learning mechanism. Furthermore, our method is the first to include the challenging retro-cardiac region in the CXR-based lung segmentation for accurate lung capacity estimation. The framework is evaluated on 668 CXRs of patients between 3 month to 89 year of age. We obtain a mean Dice similarity coefficient of $0.96\pm0.03$ (including the retro-cardiac region). For a given accuracy, the proposed approach is also found to be faster than conventional SSM-based iterative segmentation methods. The computational simplicity of the proposed generic framework could be similarly applied to the fast segmentation of other deformable objects.",0
"While computer-aided diagnosis (CAD) techniques for lung field segmentation from chest radiographs (CXR) have been developed for adult cohorts, they are rarely utilized for pediatric subjects. This is because statistical shape models (SSMs), which are commonly used in CXR-based lung field segmentation, do not effectively account for the shape variation of the lung field during the pediatric developmental stage. Our study introduces several contributions to address these limitations: (1) a CXR-based lung field segmentation framework that can accommodate a wide range of shape variations in both adult and pediatric cohorts, (2) a detection mechanism called ""ensemble space learning"" that utilizes deep representation learning to improve object localization, and (3) ""marginal shape deep learning"" for accurate estimation of shape deformation parameters. Our proposed approach differs from conventional SSMs by transforming the parameter space into marginal subspaces that can be solved efficiently using recursive representation learning. Additionally, our method includes the retro-cardiac region in the CXR-based lung segmentation, which has not been achieved in previous studies. We evaluated our framework using 668 CXRs of patients ranging from 3 months to 89 years of age and obtained a mean Dice similarity coefficient of $0.96\pm0.03$, including the retro-cardiac region. Our proposed approach is not only accurate but also faster than conventional SSM-based iterative segmentation methods. Furthermore, the computational simplicity of our approach can be applied to the fast segmentation of other deformable objects.",1
"Audio-visual representation learning is an important task from the perspective of designing machines with the ability to understand complex events. To this end, we propose a novel multimodal framework that instantiates multiple instance learning. We show that the learnt representations are useful for classifying events and localizing their characteristic audio-visual elements. The system is trained using only video-level event labels without any timing information. An important feature of our method is its capacity to learn from unsynchronized audio-visual events. We achieve state-of-the-art results on a large-scale dataset of weakly-labeled audio event videos. Visualizations of localized visual regions and audio segments substantiate our system's efficacy, especially when dealing with noisy situations where modality-specific cues appear asynchronously.",0
"Designing machines with the ability to comprehend complex events requires the significant task of audio-visual representation learning. In order to address this, we propose a novel multimodal approach utilizing multiple instance learning, where the learned representations are effective in classifying events and identifying their characteristic audio-visual components. Our system is exclusively trained with video-level event labels and lacks timing information, but it has the unique ability to learn from unsynchronized audio-visual events. Our method achieves state-of-the-art results on a large-scale dataset of weakly-labeled audio event videos. Visualizations of localized visual regions and audio segments provide evidence of our system's effectiveness, particularly in noisy situations where modality-specific cues occur asynchronously.",1
"Many methods have been proposed to solve the domain adaptation problem recently. However, the success of them implicitly funds on the assumption that the information of domains are fully transferrable. If the assumption is not satisfied, the effect of negative transfer may degrade domain adaptation. In this paper, a better learning network has been proposed by considering three tasks - domain adaptation, disentangled representation, and style transfer simultaneously. Firstly, the learned features are disentangled into common parts and specific parts. The common parts represent the transferrable features, which are suitable for domain adaptation with less negative transfer. Conversely, the specific parts characterize the unique style of each individual domain. Based on this, the new concept of feature exchange across domains, which can not only enhance the transferability of common features but also be useful for image style transfer, is introduced. These designs allow us to introduce five types of training objectives to realize the three challenging tasks at the same time. The experimental results show that our architecture can be adaptive well to full transfer learning and partial transfer learning upon a well-learned disentangled representation. Besides, the trained network also demonstrates high potential to generate style-transferred images.",0
"Recently, there have been numerous proposals for solving the domain adaptation problem. However, their success depends on the assumption that domain information can be fully transferred. If this assumption is not met, negative transfer may hinder domain adaptation. In this paper, we propose a better learning network that addresses three tasks: domain adaptation, disentangled representation, and style transfer concurrently. Firstly, our network disentangles learned features into common and specific parts, where common parts represent transferrable features suitable for domain adaptation with less negative transfer, and specific parts capture the unique style of each domain. We introduce a new concept of feature exchange across domains, which enhances the transferability of common features and facilitates image style transfer. Our design includes five training objectives for the three challenging tasks. Experimental results demonstrate that our architecture can adapt well to full and partial transfer learning with a well-learned disentangled representation. Furthermore, our trained network shows high potential for generating style-transferred images.",1
"The visual attributes of cells, such as the nuclear morphology and chromatin openness, are critical for histopathology image analysis. By learning cell-level visual representation, we can obtain a rich mix of features that are highly reusable for various tasks, such as cell-level classification, nuclei segmentation, and cell counting. In this paper, we propose a unified generative adversarial networks architecture with a new formulation of loss to perform robust cell-level visual representation learning in an unsupervised setting. Our model is not only label-free and easily trained but also capable of cell-level unsupervised classification with interpretable visualization, which achieves promising results in the unsupervised classification of bone marrow cellular components. Based on the proposed cell-level visual representation learning, we further develop a pipeline that exploits the varieties of cellular elements to perform histopathology image classification, the advantages of which are demonstrated on bone marrow datasets.",0
"The analysis of histopathology images relies heavily on the visual characteristics of cells, including nuclear morphology and chromatin openness. By acquiring a comprehensive understanding of cell-level visual representation, a range of tasks, such as cell-level classification, nuclei segmentation, and cell counting, can be accomplished using a diverse set of features. This study introduces a novel, unified generative adversarial networks architecture that incorporates a new loss formulation to enable robust cell-level visual representation learning in an unsupervised setting. Our model is not only easy to train and label-free but also capable of unsupervised classification at the cell level with an interpretable visualization technique, which yields promising outcomes in the unsupervised classification of bone marrow cellular components. Furthermore, we leverage our proposed cell-level visual representation learning to create a pipeline that takes advantage of the various cellular elements to achieve histopathology image classification, which is demonstrated on bone marrow datasets.",1
"Recently deep neural networks have been widely and successfully applied in computer vision tasks and attracted growing interests in medical imaging. One barrier for the application of deep neural networks to medical imaging is the need of large amounts of prior training pairs, which is not always feasible in clinical practice. In this work we propose a personalized representation learning framework where no prior training pairs are needed, but only the patient's own prior images. The representation is expressed using a deep neural network with the patient's prior images as network input. We then applied this novel image representation to inverse problems in medical imaging in which the original inverse problem was formulated as a constraint optimization problem and solved using the alternating direction method of multipliers (ADMM) algorithm. Anatomically guided brain positron emission tomography (PET) image reconstruction and image denoising were employed as examples to demonstrate the effectiveness of the proposed framework. Quantification results based on simulation and real datasets show that the proposed personalized representation framework outperform other widely adopted methods.",0
"In recent times, deep neural networks have achieved considerable success when applied to computer vision tasks, and have started to gain popularity in medical imaging as well. However, the use of deep neural networks for medical imaging faces a challenge due to the requirement of a large number of training pairs, which may not be practical in a clinical setting. To address this issue, we propose a framework for personalized representation learning that uses a patient's prior images as input to a deep neural network. This framework does not require any prior training pairs. We apply this novel image representation to inverse problems in medical imaging, where we formulate the original inverse problem as a constraint optimization problem and solve it using the ADMM algorithm. We demonstrate the effectiveness of the proposed framework through examples of anatomically guided brain PET image reconstruction and image denoising. Simulation and real dataset-based quantification results indicate that the proposed personalized representation framework outperforms other widely adopted methods.",1
"Humans can naturally understand an image in depth with the aid of rich knowledge accumulated from daily lives or professions. For example, to achieve fine-grained image recognition (e.g., categorizing hundreds of subordinate categories of birds) usually requires a comprehensive visual concept organization including category labels and part-level attributes. In this work, we investigate how to unify rich professional knowledge with deep neural network architectures and propose a Knowledge-Embedded Representation Learning (KERL) framework for handling the problem of fine-grained image recognition. Specifically, we organize the rich visual concepts in the form of knowledge graph and employ a Gated Graph Neural Network to propagate node message through the graph for generating the knowledge representation. By introducing a novel gated mechanism, our KERL framework incorporates this knowledge representation into the discriminative image feature learning, i.e., implicitly associating the specific attributes with the feature maps. Compared with existing methods of fine-grained image classification, our KERL framework has several appealing properties: i) The embedded high-level knowledge enhances the feature representation, thus facilitating distinguishing the subtle differences among subordinate categories. ii) Our framework can learn feature maps with a meaningful configuration that the highlighted regions finely accord with the nodes (specific attributes) of the knowledge graph. Extensive experiments on the widely used Caltech-UCSD bird dataset demonstrate the superiority of our KERL framework over existing state-of-the-art methods.",0
"With the help of their accumulated knowledge from daily experiences or professions, humans can naturally comprehend an image in depth. For instance, to precisely categorize hundreds of subordinate categories of birds, a comprehensive visual concept organization, including category labels and part-level attributes, is required. In this study, we explore the integration of rich professional knowledge with deep neural network architectures, proposing a Knowledge-Embedded Representation Learning (KERL) framework to address the issue of fine-grained image recognition. We represent the rich visual concepts in the form of a knowledge graph and utilize a Gated Graph Neural Network to transmit node messages through the graph to create the knowledge representation. Our KERL framework incorporates this representation into the discriminative image feature learning by introducing a novel gated mechanism, implicitly associating specific attributes with feature maps. Our KERL framework has several benefits over existing methods of fine-grained image classification, including enhanced feature representation and meaningful configuration of feature maps that align with the nodes of the knowledge graph. We demonstrate the superiority of our KERL framework over existing state-of-the-art methods through extensive experiments on the widely used Caltech-UCSD bird dataset.",1
"Deep latent variable models, trained using variational autoencoders or generative adversarial networks, are now a key technique for representation learning of continuous structures. However, applying similar methods to discrete structures, such as text sequences or discretized images, has proven to be more challenging. In this work, we propose a flexible method for training deep latent variable models of discrete structures. Our approach is based on the recently-proposed Wasserstein autoencoder (WAE) which formalizes the adversarial autoencoder (AAE) as an optimal transport problem. We first extend this framework to model discrete sequences, and then further explore different learned priors targeting a controllable representation. This adversarially regularized autoencoder (ARAE) allows us to generate natural textual outputs as well as perform manipulations in the latent space to induce change in the output space. Finally we show that the latent representation can be trained to perform unaligned textual style transfer, giving improvements both in automatic/human evaluation compared to existing methods.",0
"Variational autoencoders and generative adversarial networks are commonly used in representation learning of continuous structures, but it has been more difficult to apply these methods to discrete structures such as text sequences or discretized images. In this study, we propose a flexible approach for training deep latent variable models of discrete structures using the recently-introduced Wasserstein autoencoder (WAE), which formalizes the adversarial autoencoder (AAE) as an optimal transport problem. Our method, called adversarially regularized autoencoder (ARAE), is able to generate natural textual outputs and manipulate the latent space to induce changes in the output space. We also explore different learned priors to target a controllable representation. Additionally, we demonstrate that the latent representation can be trained for unaligned textual style transfer, resulting in improvements in both automatic and human evaluation compared to existing methods.",1
"Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.",0
"By utilizing the fundamental principles of Statistics and Information Theory, we demonstrate that a deep neural network's resistance to irrelevant factors is the same as the minimization of information in the acquired representation. Furthermore, we reveal that the utilization of layered stacking and noise injection throughout training naturally directs the network to acquire invariant representations. Additionally, we dissect the cross-entropy loss utilized during training and expose an inherent overfitting term. We suggest controlling the loss by limiting this term in two corresponding methods, one of which pertains to a PAC-Bayes perspective through a Kullbach-Leibler term, while the other utilizes information in the weights as a measure of complexity for a learned model to produce a new Information Bottleneck for the weights. Ultimately, we prove that the information in the weights determines the maximum and minimum bounds for invariance and independence of the elements within the acquired representation learned by the network, which are implicitly optimized throughout training. This theory enables precise prediction and quantification of the abrupt transitions between underfitting and overfitting of random labels when utilizing our regularized loss, as demonstrated in our experiments. The theory also sheds light on the relationship between the geometry of the loss function, the network's resistance to irrelevant factors, and the generalization error.",1
"Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of ""neighboring"" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.",0
"Recent techniques in deep learning for graph representation learning use a method called neighborhood aggregation. We have examined some crucial aspects of these models and have developed a plan to overcome their limitations. Specifically, the range of nearby nodes that a node's representation draws from is closely linked to the structure of the graph, much like the spread of a random walk. To address this and adapt to local neighborhood properties and tasks, we present the jumping knowledge (JK) network architecture, which enables nodes to leverage various neighborhood ranges and better represent the structure. Through experiments on social, bioinformatics, and citation networks, we show that our model achieves state-of-the-art performance. Additionally, when combined with models such as Graph Convolutional Networks, GraphSAGE, and Graph Attention Networks, the JK framework consistently improves their performance.",1
"This paper describes InfoCatVAE, an extension of the variational autoencoder that enables unsupervised disentangled representation learning. InfoCatVAE uses multimodal distributions for the prior and the inference network and then maximizes the evidence lower bound objective (ELBO). We connect the new ELBO derived for our model with a natural soft clustering objective which explains the robustness of our approach. We then adapt the InfoGANs method to our setting in order to maximize the mutual information between the categorical code and the generated inputs and obtain an improved model.",0
"The article presents InfoCatVAE, a modified version of the variational autoencoder that facilitates unsupervised learning of disentangled representations. To achieve this, InfoCatVAE utilizes multimodal distributions for the prior and inference network and maximizes the evidence lower bound objective. The authors establish a connection between the novel ELBO of their model and a soft clustering objective, which accounts for the approach's resilience. Additionally, they implement the InfoGANs technique to enhance the model by maximizing the mutual information between the categorical code and the generated inputs.",1
"It has been shown that for automated PAP-smear image classification, nucleus features can be very informative. Therefore, the primary step for automated screening can be cell-nuclei detection followed by segmentation of nuclei in the resulting single cell PAP-smear images. We propose a patch based approach using CNN for segmentation of nuclei in single cell images. We then pose the question of ion of segmentation for classification using representation learning with CNN, and whether low-level CNN features may be useful for classification. We suggest a CNN-based feature level analysis and a transfer learning based approach for classification using both segmented as well full single cell images. We also propose a decision-tree based approach for classification. Experimental results demonstrate the effectiveness of the proposed algorithms individually (with low-level CNN features), and simultaneously proving the sufficiency of cell-nuclei detection (rather than accurate segmentation) for classification. Thus, we propose a system for analysis of multi-cell PAP-smear images consisting of a simple nuclei detection algorithm followed by classification using transfer learning.",0
"Research has revealed that nucleus features are highly informative in automated PAP-smear image classification. As a result, a potential initial step for automated screening is detecting cell-nuclei and segmenting them in single cell PAP-smear images. In this study, we suggest a patch-based strategy that employs CNN for the segmentation of nuclei in single cell images. Our research also investigates whether low-level CNN features are valuable for classification and proposes a CNN-based feature level analysis, transfer learning-based classification, and decision-tree based classification using both segmented and full single cell images. Our experimental results show the effectiveness of our proposed algorithms, demonstrating that cell-nuclei detection is sufficient for classification rather than accurate segmentation. Therefore, we propose a system for analyzing multi-cell PAP-smear images that involves a straightforward nuclei detection algorithm followed by classification using transfer learning.",1
"A cognitive model of human learning provides information about skills a learner must acquire to perform accurately in a task domain. Cognitive models of learning are not only of scientific interest, but are also valuable in adaptive online tutoring systems. A more accurate model yields more effective tutoring through better instructional decisions. Prior methods of automated cognitive model discovery have typically focused on well-structured domains, relied on student performance data or involved substantial human knowledge engineering. In this paper, we propose Cognitive Representation Learner (CogRL), a novel framework to learn accurate cognitive models in ill-structured domains with no data and little to no human knowledge engineering. Our contribution is two-fold: firstly, we show that representations learnt using CogRL can be used for accurate automatic cognitive model discovery without using any student performance data in several ill-structured domains: Rumble Blocks, Chinese Character, and Article Selection. This is especially effective and useful in domains where an accurate human-authored cognitive model is unavailable or authoring a cognitive model is difficult. Secondly, for domains where a cognitive model is available, we show that representations learned through CogRL can be used to get accurate estimates of skill difficulty and learning rate parameters without using any student performance data. These estimates are shown to highly correlate with estimates using student performance data on an Article Selection dataset.",0
"Human learning can be understood through cognitive models, which identify the necessary skills for accurate task performance. These models are valuable for adaptive online tutoring systems, as they inform instructional decisions. However, automated cognitive model discovery methods have typically been limited to well-structured domains and require student performance data or human knowledge engineering. This paper introduces CogRL, a framework for learning accurate cognitive models in ill-structured domains without data or significant human knowledge engineering. CogRL representations are effective for automatic cognitive model discovery in domains like Rumble Blocks, Chinese Character, and Article Selection, where human-authored models are unavailable or difficult to create. Additionally, CogRL representations can estimate skill difficulty and learning rate parameters without student performance data, with results highly correlated to those obtained with performance data.",1
"We present a self-supervised approach using spatio-temporal signals between video frames for action recognition. A two-stream architecture is leveraged to tangle spatial and temporal representation learning. Our task is formulated as both a sequence verification and spatio-temporal alignment tasks. The former task requires motion temporal structure understanding while the latter couples the learned motion with the spatial representation. The self-supervised pre-trained weights effectiveness is validated on the action recognition task. Quantitative evaluation shows the self-supervised approach competence on three datasets: HMDB51, UCF101, and Honda driving dataset (HDD). Further investigations to boost performance and generalize validity are still required.",0
"Our proposed method for action recognition utilizes spatio-temporal signals between video frames, employing a self-supervised approach. To achieve this, we utilize a two-stream architecture to learn spatial and temporal representations. Our approach involves two tasks: sequence verification and spatio-temporal alignment. The former focuses on understanding motion temporal structure, while the latter combines the learned motion with spatial representation. We validate the effectiveness of our self-supervised pre-trained weights on the action recognition task and demonstrate its competence on three datasets - HMDB51, UCF101, and Honda driving dataset (HDD). However, there is a need for further investigations to enhance performance and generalize validity.",1
"Purpose: This paper focuses on an automated analysis of surgical motion profiles for objective skill assessment and task recognition in robot-assisted surgery. Existing techniques heavily rely on conventional statistic measures or shallow modelings based on hand-engineered features and gesture segmentation. Such developments require significant expert knowledge, are prone to errors, and are less efficient in online adaptive training systems. Methods: In this work, we present an efficient analytic framework with a parallel deep learning architecture, SATR-DL, to assess trainee expertise and recognize surgical training activity. Through an end-to-end learning technique, abstract information of spatial representations and temporal dynamics is jointly obtained directly from raw motion sequences. Results: By leveraging a shared high-level representation learning, the resulting model is successful in the recognition of trainee skills and surgical tasks, suturing, needle-passing, and knot-tying. Meanwhile, we explore the use of ensemble in classification at the trial level, where the SATR-DL outperforms state-of-the-art performance by achieving accuracies of 0.960 and 1.000 in skill assessment and task recognition, respectively. Conclusion: This study highlights the potential of SATR-DL to provide improvements for an efficient data-driven assessment in intelligent robotic surgery.",0
"The main focus of this paper is to discuss the automated analysis of surgical motion profiles for objective skill assessment and task recognition in robot-assisted surgery. Current techniques rely heavily on traditional statistical measures or shallow modelings that are based on hand-engineered features and gesture segmentation. These methods require significant expert knowledge, are prone to errors, and are less efficient in online adaptive training systems. To address these issues, this work presents an efficient analytic framework called SATR-DL, which uses a parallel deep learning architecture to assess trainee expertise and recognize surgical training activity. The model leverages a shared high-level representation learning to successfully recognize trainee skills and surgical tasks, including suturing, needle-passing, and knot-tying. The use of ensemble in classification at the trial level is also explored, where SATR-DL outperforms state-of-the-art performance with accuracies of 0.960 and 1.000 in skill assessment and task recognition, respectively. Overall, this study highlights the potential of SATR-DL to improve data-driven assessment in intelligent robotic surgery.",1
"Learning expressive low-dimensional representations of ultrahigh-dimensional data, e.g., data with thousands/millions of features, has been a major way to enable learning methods to address the curse of dimensionality. However, existing unsupervised representation learning methods mainly focus on preserving the data regularity information and learning the representations independently of subsequent outlier detection methods, which can result in suboptimal and unstable performance of detecting irregularities (i.e., outliers).   This paper introduces a ranking model-based framework, called RAMODO, to address this issue. RAMODO unifies representation learning and outlier detection to learn low-dimensional representations that are tailored for a state-of-the-art outlier detection approach - the random distance-based approach. This customized learning yields more optimal and stable representations for the targeted outlier detectors. Additionally, RAMODO can leverage little labeled data as prior knowledge to learn more expressive and application-relevant representations. We instantiate RAMODO to an efficient method called REPEN to demonstrate the performance of RAMODO.   Extensive empirical results on eight real-world ultrahigh dimensional data sets show that REPEN (i) enables a random distance-based detector to obtain significantly better AUC performance and two orders of magnitude speedup; (ii) performs substantially better and more stably than four state-of-the-art representation learning methods; and (iii) leverages less than 1% labeled data to achieve up to 32% AUC improvement.",0
"To overcome the curse of dimensionality, learning low-dimensional representations of ultrahigh-dimensional data has been crucial. However, current unsupervised representation learning approaches mainly prioritize preserving regularity information and independently learning representations, leading to suboptimal and unreliable outlier detection. This paper presents a framework named RAMODO that utilizes a ranking model to integrate representation learning and outlier detection. This approach generates tailored low-dimensional representations for a top-performing outlier detection technique, resulting in more optimal and stable outcomes. Additionally, RAMODO can leverage a small amount of labeled data to create more expressive representations relevant to the application. The paper applies RAMODO to an efficient algorithm called REPEN, demonstrating significant improvements in performance and speed over existing methods on eight real-world ultrahigh-dimensional datasets. REPEN also achieved up to 32% AUC improvement with less than 1% labeled data.",1
"Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.",0
"The study of representation learning and generative modeling is well-suited for examining three-dimensional geometric data. Specifically, this paper focuses on the representation of geometric data in the form of point clouds. The authors present a deep AutoEncoder (AE) network that exhibits exceptional reconstruction quality and generalization ability. The learned representations outperform existing methods in 3D recognition tasks and allow for shape editing through simple algebraic manipulations, including semantic part editing, shape analogies, shape interpolation, and shape completion. The authors conduct a comprehensive analysis of various generative models, including GANs operating on raw point clouds, improved GANs trained in the fixed latent space of their AEs, and Gaussian Mixture Models (GMMs). To assess generative models quantitatively, they introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, their evaluation of generality, fidelity, and diversity shows that GMMs trained in the latent space of their AEs provide the best overall results.",1
"The goal of this paper is to compare surface-based and volumetric 3D object shape representations, as well as viewer-centered and object-centered reference frames for single-view 3D shape prediction. We propose a new algorithm for predicting depth maps from multiple viewpoints, with a single depth or RGB image as input. By modifying the network and the way models are evaluated, we can directly compare the merits of voxels vs. surfaces and viewer-centered vs. object-centered for familiar vs. unfamiliar objects, as predicted from RGB or depth images. Among our findings, we show that surface-based methods outperform voxel representations for objects from novel classes and produce higher resolution outputs. We also find that using viewer-centered coordinates is advantageous for novel objects, while object-centered representations are better for more familiar objects. Interestingly, the coordinate frame significantly affects the shape representation learned, with object-centered placing more importance on implicitly recognizing the object category and viewer-centered producing shape representations with less dependence on category recognition.",0
"The aim of this article is to compare surface-based and volumetric 3D object shape representations, as well as viewer-centered and object-centered reference frames for single-view 3D shape prediction. Our proposed algorithm predicts depth maps from multiple viewpoints using a single depth or RGB image as input. We have made modifications to the network and evaluation process, allowing us to directly compare the advantages of voxels versus surfaces and viewer-centered versus object-centered when predicting familiar and unfamiliar objects from RGB or depth images. Our research reveals that surface-based methods are superior to voxel representations for objects from unfamiliar classes, producing higher resolution outputs. We have also discovered that viewer-centered coordinates are more effective for unfamiliar objects, while object-centered representations are better suited for familiar objects. Interestingly, the coordinate frame has a significant impact on the learned shape representation, with object-centered placing more emphasis on implicit recognition of the object category and viewer-centered producing shape representations that are less reliant on category recognition.",1
"Embedding representation learning via neural networks is at the core foundation of modern similarity based search. While much effort has been put in developing algorithms for learning binary hamming code representations for search efficiency, this still requires a linear scan of the entire dataset per each query and trades off the search accuracy through binarization. To this end, we consider the problem of directly learning a quantizable embedding representation and the sparse binary hash code end-to-end which can be used to construct an efficient hash table not only providing significant search reduction in the number of data but also achieving the state of the art search accuracy outperforming previous state of the art deep metric learning methods. We also show that finding the optimal sparse binary hash code in a mini-batch can be computed exactly in polynomial time by solving a minimum cost flow problem. Our results on Cifar-100 and on ImageNet datasets show the state of the art search accuracy in precision@k and NMI metrics while providing up to 98X and 478X search speedup respectively over exhaustive linear search. The source code is available at https://github.com/maestrojeong/Deep-Hash-Table-ICML18",0
"At the root of modern similarity-based search lies the incorporation of representation learning via neural networks. Despite efforts to develop algorithms for learning binary hamming code representations in order to enhance search efficiency, this still requires a linear scan of the entire dataset per each query and results in a trade-off between search accuracy and binarization. Therefore, we present a solution which involves directly learning a quantizable embedding representation and a sparse binary hash code end-to-end. This approach enables the creation of an efficient hash table, reducing the number of data to be searched and achieving superior search accuracy when compared to previous state of the art deep metric learning methods. Our research also demonstrates that determining the optimal sparse binary hash code in a mini-batch can be done exactly in polynomial time by resolving a minimum cost flow problem. Our findings, as demonstrated on Cifar-100 and ImageNet datasets, indicate state of the art search accuracy in precision@k and NMI metrics, while providing search speedup of up to 98X and 478X respectively over exhaustive linear search. Our source code can be accessed at https://github.com/maestrojeong/Deep-Hash-Table-ICML18.",1
"Compared to earlier multistage frameworks using CNN features, recent end-to-end deep approaches for fine-grained recognition essentially enhance the mid-level learning capability of CNNs. Previous approaches achieve this by introducing an auxiliary network to infuse localization information into the main classification network, or a sophisticated feature encoding method to capture higher order feature statistics. We show that mid-level representation learning can be enhanced within the CNN framework, by learning a bank of convolutional filters that capture class-specific discriminative patches without extra part or bounding box annotations. Such a filter bank is well structured, properly initialized and discriminatively learned through a novel asymmetric multi-stream architecture with convolutional filter supervision and a non-random layer initialization. Experimental results show that our approach achieves state-of-the-art on three publicly available fine-grained recognition datasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and visualizations are provided to understand our approach.",0
"Recent end-to-end deep approaches for fine-grained recognition have significantly improved the mid-level learning capability of CNNs compared to earlier multistage frameworks that used CNN features. In the past, auxiliary networks or sophisticated feature encoding methods were used to infuse localization information into the main classification network to achieve this. However, we demonstrate that mid-level representation learning can be improved within the CNN framework by learning a bank of convolutional filters that capture class-specific discriminative patches, without the need for extra part or bounding box annotations. Our approach involves a well-structured, properly initialized filter bank, which is discriminatively learned through a novel asymmetric multi-stream architecture with convolutional filter supervision and non-random layer initialization. Our approach achieves state-of-the-art results on three publicly available fine-grained recognition datasets (CUB-200-2011, Stanford Cars, and FGVC-Aircraft), and we provide ablation studies and visualizations to aid understanding.",1
"In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.",0
"This study adopts a representation learning approach to hierarchical reinforcement learning, where the challenge of learning lower layers in a hierarchy is converted to learning trajectory-level generative models. The authors demonstrate that continuous latent representations of trajectories can be acquired, enabling effective resolution of temporally extended and multi-stage problems. Their proposed SeCTAR model, inspired by variational autoencoders, learns latent representations of trajectories and incorporates a latent-conditioned policy and model that are mutually consistent. The policy generates a trajectory that matches the model's prediction when given the same latent. The model predicts the outcome of closed loop policy behavior, providing an embedded prediction mechanism. The authors propose a novel algorithm for hierarchical RL using this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. Several simulated tasks demonstrate that the model outperforms standard RL methods and prior methods for hierarchical reasoning, model-based planning, and exploration, particularly for reasoning over long horizons with sparse rewards.",1
"Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA's vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts - even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).",0
"Deep reinforcement learning (RL) faces the challenge of domain adaptation, particularly in scenarios where data is difficult to obtain. To address this, agents can learn a source policy in an environment where data is readily available, with the hope of generalizing well to the target domain. We introduce DARLA (DisentAngled Representation Learning Agent), a novel multi-stage RL agent that prioritizes learning to see before learning to act. DARLA's vision is based on acquiring a disentangled representation of the observed environment. Once this is achieved, DARLA can acquire source policies that are robust to many domain shifts, even without access to the target domain. DARLA outperforms conventional baselines in zero-shot domain adaptation scenarios across various RL environments (such as Jaco arm and DeepMind Lab) and base RL algorithms (including DQN, A3C, and EC).",1
"Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, ""Would this patient have lower blood sugar had she received a different medication?"". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.",0
"The value of observational studies has increased due to the abundance of data available in various fields like healthcare, education, employment, and ecology. Our focus is on providing answers to hypothetical questions like, ""Would the blood sugar of this patient have been reduced if she had been given a different medication?"". We have introduced a novel algorithmic framework for counterfactual inference that combines concepts from domain adaptation and representation learning. Along with a theoretical explanation, we have conducted a comparison with earlier techniques of causal inference from observational data. Our deep learning algorithm has shown significantly superior performance than the existing state-of-the-art.",1
"Conditional density estimation is a general framework for solving various problems in machine learning. Among existing methods, non-parametric and/or kernel-based methods are often difficult to use on large datasets, while methods based on neural networks usually make restrictive parametric assumptions on the probability densities. Here, we propose a novel method for estimating the conditional density based on score matching. In contrast to existing methods, we employ scalable neural networks, but do not make explicit parametric assumptions on densities. The key challenge in applying score matching to neural networks is computation of the first- and second-order derivatives of a model for the log-density. We tackle this challenge by developing a new neural-kernelized approach, which can be applied on large datasets with stochastic gradient descent, while the reproducing kernels allow for easy computation of the derivatives needed in score matching. We show that the neural-kernelized function approximator has universal approximation capability and that our method is consistent in conditional density estimation. We numerically demonstrate that our method is useful in high-dimensional conditional density estimation, and compares favourably with existing methods. Finally, we prove that the proposed method has interesting connections to two probabilistically principled frameworks of representation learning: Nonlinear sufficient dimension reduction and nonlinear independent component analysis.",0
"Conditional density estimation is a widely used framework in machine learning with non-parametric and kernel-based methods being difficult to use on large datasets and neural network-based methods relying on restrictive parametric assumptions. To address these limitations, we propose a novel method for estimating conditional density using score matching with scalable neural networks without explicit parametric assumptions. However, computing the first- and second-order derivatives of the model for the log-density poses a challenge. To overcome this issue, we introduce a new neural-kernelized approach that uses reproducing kernels to facilitate the computation of derivatives. Our method is consistent in conditional density estimation and has universal approximation capability. Numerical experiments demonstrate its effectiveness in high-dimensional conditional density estimation and its superiority over existing methods. We also show that our method has connections to nonlinear sufficient dimension reduction and nonlinear independent component analysis, two probabilistically principled frameworks of representation learning.",1
"Graph embedding is a central problem in social network analysis and many other applications, aiming to learn the vector representation for each node. While most existing approaches need to specify the neighborhood and the dependence form to the neighborhood, which may significantly degrades the flexibility of representation, we propose a novel graph node embedding method (namely GESF) via the set function technique. Our method can 1) learn an arbitrary form of representation function from neighborhood, 2) automatically decide the significance of neighbors at different distances, and 3) be applied to heterogeneous graph embedding, which may contain multiple types of nodes. Theoretical guarantee for the representation capability of our method has been proved for general homogeneous and heterogeneous graphs and evaluation results on benchmark data sets show that the proposed GESF outperforms the state-of-the-art approaches on producing node vectors for classification tasks.",0
"The task of graph embedding is crucial in various applications, including social network analysis. Its goal is to obtain a vector representation for each node. However, most existing methods require the neighborhood and its dependency form to be specified, limiting the flexibility of representation. Our proposed approach, GESF, uses a set function technique to overcome this limitation. GESF can 1) learn any form of representation function from the neighborhood, 2) determine the importance of neighbors at different distances automatically, and 3) be applied to heterogeneous graphs with multiple node types. Our method has a theoretical guarantee for representation capability in both homogeneous and heterogeneous graphs. Evaluation results on benchmark datasets demonstrate that GESF outperforms state-of-the-art approaches in producing node vectors for classification tasks.",1
"Representation learning is at the heart of what makes deep learning effective. In this work, we introduce a new framework for representation learning that we call ""Holographic Neural Architectures"" (HNAs). In the same way that an observer can experience the 3D structure of a holographed object by looking at its hologram from several angles, HNAs derive Holographic Representations from the training set. These representations can then be explored by moving along a continuous bounded single dimension. We show that HNAs can be used to make generative networks, state-of-the-art regression models and that they are inherently highly resistant to noise. Finally, we argue that because of their denoising abilities and their capacity to generalize well from very few examples, models based upon HNAs are particularly well suited for biological applications where training examples are rare or noisy.",0
"Deep learning is made effective through representation learning, which is the core concept of this work. We introduce a new framework for representation learning, known as ""Holographic Neural Architectures"" (HNAs). HNAs derive Holographic Representations from the training set, which can be explored by moving along a continuous bounded single dimension. This framework can be used to create generative networks, state-of-the-art regression models, and is highly resistant to noise. We contend that HNAs are well-suited for biological applications due to their denoising abilities and capacity to generalize from very few examples, making them ideal for training with rare or noisy data. Additionally, HNAs derive their name from the way they allow observers to experience the 3D structure of a holographed object by looking at its hologram from several angles.",1
"Current deep learning based text classification methods are limited by their ability to achieve fast learning and generalization when the data is scarce. We address this problem by integrating a meta-learning procedure that uses the knowledge learned across many tasks as an inductive bias towards better natural language understanding. Based on the Model-Agnostic Meta-Learning framework (MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification. The essential difference between MAML and ATAML is in the separation of task-agnostic representation learning and task-specific attentive adaptation. The proposed ATAML is designed to encourage task-agnostic representation learning by way of task-agnostic parameterization and facilitate task-specific adaptation via attention mechanisms. We provide evidence to show that the attention mechanism in ATAML has a synergistic effect on learning performance. In comparisons with models trained from random initialization, pretrained models and meta trained MAML, our proposed ATAML method generalizes better on single-label and multi-label classification tasks in miniRCV1 and miniReuters-21578 datasets.",0
"Deep learning text classification methods currently face limitations in achieving fast learning and generalization when working with scarce data. Our solution to this problem involves incorporating a meta-learning process that leverages acquired knowledge from numerous tasks to improve natural language comprehension. We introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification, utilizing the Model-Agnostic Meta-Learning (MAML) framework. Unlike MAML, ATAML separates task-agnostic representation learning and task-specific attentive adaptation, promoting representation learning through task-agnostic parameterization and facilitating adaptation via attention mechanisms. Our evidence demonstrates that the attention mechanism employed by ATAML has a synergistic effect on learning performance. Compared to models trained through random initialization, pretraining, and meta-trained MAML, ATAML exhibits superior generalization abilities on miniRCV1 and miniReuters-21578 datasets for single-label and multi-label classification tasks.",1
"While deep learning methods are increasingly being applied to tasks such as computer-aided diagnosis, these models are difficult to interpret, do not incorporate prior domain knowledge, and are often considered as a ""black-box."" The lack of model interpretability hinders them from being fully understood by target users such as radiologists. In this paper, we present a novel interpretable deep hierarchical semantic convolutional neural network (HSCNN) to predict whether a given pulmonary nodule observed on a computed tomography (CT) scan is malignant. Our network provides two levels of output: 1) low-level radiologist semantic features, and 2) a high-level malignancy prediction score. The low-level semantic outputs quantify the diagnostic features used by radiologists and serve to explain how the model interprets the images in an expert-driven manner. The information from these low-level tasks, along with the representations learned by the convolutional layers, are then combined and used to infer the high-level task of predicting nodule malignancy. This unified architecture is trained by optimizing a global loss function including both low- and high-level tasks, thereby learning all the parameters within a joint framework. Our experimental results using the Lung Image Database Consortium (LIDC) show that the proposed method not only produces interpretable lung cancer predictions but also achieves significantly better results compared to common 3D CNN approaches.",0
"As the use of deep learning methods becomes more common for tasks like computer-aided diagnosis, they are often seen as a ""black-box"" and challenging to interpret, lacking prior domain knowledge. This lack of interpretability is a hindrance to target users such as radiologists. In this study, we introduce a new interpretable deep hierarchical semantic convolutional neural network (HSCNN) for predicting the malignancy of a pulmonary nodule observed on a computed tomography (CT) scan. Our network has two levels of output: 1) low-level semantic features, and 2) high-level malignancy prediction score. The low-level semantic outputs explain how the model interprets the images in an expert-driven way, and the information from these outputs and the representations learned by the convolutional layers are combined to infer the high-level task of predicting nodule malignancy. The network is trained using a joint framework that optimizes a global loss function for both low- and high-level tasks. Using the Lung Image Database Consortium (LIDC), our proposed method not only produces interpretable lung cancer predictions, but it also outperforms common 3D CNN approaches.",1
"Multi-layered representation is believed to be the key ingredient of deep neural networks especially in cognitive tasks like computer vision. While non-differentiable models such as gradient boosting decision trees (GBDTs) are the dominant methods for modeling discrete or tabular data, they are hard to incorporate with such representation learning ability. In this work, we propose the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring the ability to learn hierarchical representations by stacking several layers of regression GBDTs as its building block. The model can be jointly trained by a variant of target propagation across layers, without the need to derive back-propagation nor differentiability. Experiments and visualizations confirmed the effectiveness of the model in terms of performance and representation learning ability.",0
"The key to successful deep neural networks in cognitive tasks, such as computer vision, is believed to be multi-layered representation. However, gradient boosting decision trees (GBDTs), which are commonly used for modeling tabular or discrete data, are not easily integrated with this type of representation learning ability because they are non-differentiable. To address this issue, we propose using a multi-layered GBDT forest (mGBDTs) that utilizes stacked regression GBDTs to learn hierarchical representations. This model can be trained jointly using a variant of target propagation across layers, eliminating the need for back-propagation or differentiability. Results from experiments and visualizations confirm the effectiveness of the model in terms of performance and representation learning ability.",1
"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ""posterior collapse"" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",0
"The challenge of acquiring valuable representations in machine learning without supervision is still a major obstacle. This article introduces a powerful generative model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), which learns discrete representations in a simple manner. Unlike VAEs, our model has two significant differences: the encoder network generates discrete codes instead of continuous ones, and the prior is learned rather than fixed. To acquire a discrete latent representation, we incorporate vector quantisation (VQ) concepts. The VQ method enables the model to avoid ""posterior collapse,"" which is typically observed in the VAE framework, where latents are overlooked when paired with a robust autoregressive decoder. By combining these representations with an autoregressive prior, the model can produce high-quality images, videos, and speech, as well as performing high-quality speaker conversion and unsupervised learning of phonemes, further demonstrating the effectiveness of the learned representations.",1
"Geospatial analysis lacks methods like the word vector representations and pre-trained networks that significantly boost performance across a wide range of natural language and computer vision tasks. To fill this gap, we introduce Tile2Vec, an unsupervised representation learning algorithm that extends the distributional hypothesis from natural language -- words appearing in similar contexts tend to have similar meanings -- to spatially distributed data. We demonstrate empirically that Tile2Vec learns semantically meaningful representations on three datasets. Our learned representations significantly improve performance in downstream classification tasks and, similar to word vectors, visual analogies can be obtained via simple arithmetic in the latent space.",0
"The absence of techniques such as word vector representations and pre-trained networks hinders the efficacy of geospatial analysis in various natural language and computer vision tasks. To address this limitation, we propose Tile2Vec, an unsupervised algorithm for representation learning that extends the distributional hypothesis of natural language to spatially distributed data. Our experiments on three datasets demonstrate that Tile2Vec's learned representations have semantic significance. These representations contribute substantially to the accuracy of classification tasks, and like word vectors, they enable us to obtain visual analogies through basic operations in the latent space.",1
"Multimodal sensory data resembles the form of information perceived by humans for learning, and are easy to obtain in large quantities. Compared to unimodal data, synchronization of concepts between modalities in such data provides supervision for disentangling the underlying explanatory factors of each modality. Previous work leveraging multimodal data has mainly focused on retaining only the modality-invariant factors while discarding the rest. In this paper, we present a partitioned variational autoencoder (PVAE) and several training objectives to learn disentangled representations, which encode not only the shared factors, but also modality-dependent ones, into separate latent variables. Specifically, PVAE integrates a variational inference framework and a multimodal generative model that partitions the explanatory factors and conditions only on the relevant subset of them for generation. We evaluate our model on two parallel speech/image datasets, and demonstrate its ability to learn disentangled representations by qualitatively exploring within-modality and cross-modality conditional generation with semantics and styles specified by examples. For quantitative analysis, we evaluate the classification accuracy of automatically discovered semantic units. Our PVAE can achieve over 99% accuracy on both modalities.",0
"The data obtained from multiple senses is similar to how humans learn and can be easily collected in large quantities. When compared to data from a single sense, synchronized concepts between the senses provide guidance in uncovering the factors that explain each sense. Prior research has only focused on retaining factors that are invariant across senses while discarding others. In our study, we introduce a partitioned variational autoencoder (PVAE) and several training objectives to learn disentangled representations by encoding shared and modality-dependent factors separately. PVAE uses a variational inference framework and a multimodal generative model to partition the explanatory factors and generate only the relevant ones. We assess our model on two speech/image datasets and demonstrate its ability to learn disentangled representations through qualitative exploration and quantitative analysis of classification accuracy. Our PVAE achieves over 99% accuracy on both modalities.",1
"Few-shot learning that trains image classifiers over few labeled examples per category is a challenging task. In this paper, we propose to exploit an additional big dataset with different categories to improve the accuracy of few-shot learning over our target dataset. Our approach is based on the observation that images can be decomposed into objects, which may appear in images from both the additional dataset and our target dataset. We use the object-level relation learned from the additional dataset to infer the similarity of images in our target dataset with unseen categories. Nearest neighbor search is applied to do image classification, which is a non-parametric model and thus does not need fine-tuning. We evaluate our algorithm on two popular datasets, namely Omniglot and MiniImagenet. We obtain 8.5\% and 2.7\% absolute improvements for 5-way 1-shot and 5-way 5-shot experiments on MiniImagenet, respectively. Source code will be published upon acceptance.",0
"Training image classifiers with few labeled examples per category, known as few-shot learning, is a difficult task. In this paper, we suggest utilizing a large dataset with different categories to enhance the accuracy of few-shot learning on our target dataset. Our method is based on the idea that images can be broken down into objects that may appear in both the additional dataset and our target dataset. We use the object-level relationship learned from the additional dataset to determine the similarity of images in our target dataset with unseen categories. We use nearest neighbor search for image classification, which is a non-parametric model and does not require fine-tuning. We evaluate our algorithm on two well-known datasets, Omniglot and MiniImagenet, and achieve 8.5\% and 2.7\% absolute improvements for 5-way 1-shot and 5-way 5-shot experiments on MiniImagenet, respectively. We plan to publish the source code upon acceptance.",1
"This paper proposes the decision tree latent controller generative adversarial network (DTLC-GAN), an extension of a GAN that can learn hierarchically interpretable representations without relying on detailed supervision. To impose a hierarchical inclusion structure on latent variables, we incorporate a new architecture called the DTLC into the generator input. The DTLC has a multiple-layer tree structure in which the ON or OFF of the child node codes is controlled by the parent node codes. By using this architecture hierarchically, we can obtain the latent space in which the lower layer codes are selectively used depending on the higher layer ones. To make the latent codes capture salient semantic features of images in a hierarchically disentangled manner in the DTLC, we also propose a hierarchical conditional mutual information regularization and optimize it with a newly defined curriculum learning method that we propose as well. This makes it possible to discover hierarchically interpretable representations in a layer-by-layer manner on the basis of information gain by only using a single DTLC-GAN model. We evaluated the DTLC-GAN on various datasets, i.e., MNIST, CIFAR-10, Tiny ImageNet, 3D Faces, and CelebA, and confirmed that the DTLC-GAN can learn hierarchically interpretable representations with either unsupervised or weakly supervised settings. Furthermore, we applied the DTLC-GAN to image-retrieval tasks and showed its effectiveness in representation learning.",0
"The DTLC-GAN is a GAN extension that can acquire interpretable representations in a hierarchical manner without the need for detailed supervision. To establish a hierarchical inclusion structure in latent variables, we introduce the DTLC architecture into the generator input. The DTLC features a multi-layer tree configuration where the parent node codes control the ON or OFF status of the child node codes. By utilizing this hierarchical architecture, we can obtain a latent space where lower layer codes are selectively applied based on higher layer ones. We propose a hierarchical conditional mutual information regularization to ensure that the latent codes capture significant semantic features of images in a hierarchically disentangled fashion. To optimize this, we introduce a new curriculum learning method. Using a single DTLC-GAN model, our approach permits the discovery of hierarchically interpretable representations in a layer-by-layer fashion based on information gain. The DTLC-GAN was assessed on various datasets, including MNIST, CIFAR-10, Tiny ImageNet, 3D Faces, and CelebA, and was shown to be capable of learning hierarchical interpretable representations with unsupervised or weakly supervised settings. Additionally, we tested the DTLC-GAN on image-retrieval tasks and demonstrated its effectiveness in representation learning.",1
"Kinship verification has a number of applications such as organizing large collections of images and recognizing resemblances among humans. In this research, first, a human study is conducted to understand the capabilities of human mind and to identify the discriminatory areas of a face that facilitate kinship-cues. Utilizing the information obtained from the human study, a hierarchical Kinship Verification via Representation Learning (KVRL) framework is utilized to learn the representation of different face regions in an unsupervised manner. We propose a novel approach for feature representation termed as filtered contractive deep belief networks (fcDBN). The proposed feature representation encodes relational information present in images using filters and contractive regularization penalty. A compact representation of facial images of kin is extracted as an output from the learned model and a multi-layer neural network is utilized to verify the kin accurately. A new WVU Kinship Database is created which consists of multiple images per subject to facilitate kinship verification. The results show that the proposed deep learning framework (KVRL-fcDBN) yields stateof-the-art kinship verification accuracy on the WVU Kinship database and on four existing benchmark datasets. Further, kinship information is used as a soft biometric modality to boost the performance of face verification via product of likelihood ratio and support vector machine based approaches. Using the proposed KVRL-fcDBN framework, an improvement of over 20% is observed in the performance of face verification.",0
"The process of verifying kinship has various practical applications, including the organization of extensive image collections and the identification of similarities between individuals. To explore the effectiveness of human cognition in detecting kinship cues, a human study is conducted. This study aims to identify the facial features that contribute to kinship recognition. Based on the insights gained from the human study, a Kinship Verification via Representation Learning (KVRL) framework is developed. This framework utilizes a filtered contractive deep belief network (fcDBN) to represent different facial regions in an unsupervised manner. The fcDBN encodes relational information present in the images using filters and contractive regularization penalty. The model extracts a compact representation of the facial images of kin, which is then used in a multi-layer neural network to verify kinship accurately. A WVU Kinship Database is created to facilitate kinship verification, which contains multiple images per subject. The proposed KVRL-fcDBN framework achieves state-of-the-art kinship verification accuracy on the WVU Kinship database and four other benchmark datasets. Additionally, kinship information is used as a soft biometric modality to enhance face verification performance via product of likelihood ratio and support vector machine based approaches. The proposed framework improves the performance of face verification by over 20%.",1
"Object detection and semantic segmentation are two main themes in object retrieval from high-resolution remote sensing images, which have recently achieved remarkable performance by surfing the wave of deep learning and, more notably, convolutional neural networks (CNNs). In this paper, we are interested in a novel, more challenging problem of vehicle instance segmentation, which entails identifying, at a pixel-level, where the vehicles appear as well as associating each pixel with a physical instance of a vehicle. In contrast, vehicle detection and semantic segmentation each only concern one of the two. We propose to tackle this problem with a semantic boundary-aware multi-task learning network. More specifically, we utilize the philosophy of residual learning (ResNet) to construct a fully convolutional network that is capable of harnessing multi-level contextual feature representations learned from different residual blocks. We theoretically analyze and discuss why residual networks can produce better probability maps for pixel-wise segmentation tasks. Then, based on this network architecture, we propose a unified multi-task learning network that can simultaneously learn two complementary tasks, namely, segmenting vehicle regions and detecting semantic boundaries. The latter subproblem is helpful for differentiating closely spaced vehicles, which are usually not correctly separated into instances. Currently, datasets with pixel-wise annotation for vehicle extraction are ISPRS dataset and IEEE GRSS DFC2015 dataset over Zeebrugge, which specializes in semantic segmentation. Therefore, we built a new, more challenging dataset for vehicle instance segmentation, called the Busy Parking Lot UAV Video dataset, and we make our dataset available at http://www.sipeo.bgu.tum.de/download so that it can be used to benchmark future vehicle instance segmentation algorithms.",0
"Recent advancements in deep learning and convolutional neural networks (CNNs) have led to significant improvements in object retrieval from high-resolution remote sensing images in the areas of object detection and semantic segmentation. However, we focus on a more complex problem of vehicle instance segmentation, which involves identifying vehicles at a pixel-level and associating each pixel with a physical instance of a vehicle. Our proposed solution is a semantic boundary-aware multi-task learning network that utilizes the philosophy of residual learning to create a fully convolutional network capable of harnessing multi-level contextual feature representations from different residual blocks. We also propose a unified multi-task learning network that can simultaneously learn two complementary tasks of segmenting vehicle regions and detecting semantic boundaries. We have built a new dataset, the Busy Parking Lot UAV Video dataset, for vehicle instance segmentation, which is available for download at http://www.sipeo.bgu.tum.de/download to benchmark future algorithms.",1
"In this paper, we introduce an alternative approach, namely GEN (Genetic Evolution Network) Model, to the deep learning models. Instead of building one single deep model, GEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. Significantly different from the wellknown representation learning models with extremely deep structures, the unit models covered in GEN are of a much shallower architecture. In the training process, from each generation, a subset of unit models will be selected based on their performance to evolve and generate the child models in the next generation. GEN has significant advantages compared with existing deep representation learning models in terms of both learning effectiveness, efficiency and interpretability of the learning process and learned results. Extensive experiments have been done on diverse benchmark datasets, and the experimental results have demonstrated the outstanding performance of GEN compared with the state-of-the-art baseline methods in both effectiveness of efficiency.",0
"The GEN (Genetic Evolution Network) Model is proposed as an alternative to deep learning models in this paper. Unlike traditional deep models, GEN employs a genetic-evolutionary learning strategy to develop multiple unit models in each generation. These unit models are relatively shallow in design, unlike the highly complex structures of representation learning models. During the training process, a subset of the unit models is selected from each generation based on their performance, and they are used to generate child models in the next generation. The GEN model offers significant advantages over existing deep representation learning models in terms of learning effectiveness, efficiency, and interpretability. Extensive experiments on various benchmark datasets have demonstrated that GEN outperforms state-of-the-art baseline methods in both effectiveness and efficiency.",1
"Many problems at the intersection of combinatorics and computer science require solving for a permutation that optimally matches, ranks, or sorts some data. These problems usually have a task-specific, often non-differentiable objective function that data-driven algorithms can use as a learning signal. In this paper, we propose the Sinkhorn Policy Gradient (SPG) algorithm for learning policies on permutation matrices. The actor-critic neural network architecture we introduce for SPG uniquely decouples representation learning of the state space from the highly-structured action space of permutations with a temperature-controlled Sinkhorn layer. The Sinkhorn layer produces continuous relaxations of permutation matrices so that the actor-critic architecture can be trained end-to-end. Our empirical results show that agents trained with SPG can perform competitively on sorting, the Euclidean TSP, and matching tasks. We also observe that SPG is significantly more data efficient at the matching task than the baseline methods, which indicates that SPG is conducive to learning representations that are useful for reasoning about permutations.",0
"The intersection of combinatorics and computer science often involves solving problems that require finding optimal permutations for sorting, ranking, or matching data. These problems typically involve a task-specific objective function that cannot be differentiated, but can be used as a learning signal for data-driven algorithms. This paper presents the Sinkhorn Policy Gradient (SPG) algorithm for training policies on permutation matrices. The unique actor-critic neural network architecture used in SPG separates the representation learning of the state space from the structured action space of permutations with a temperature-controlled Sinkhorn layer. This layer produces continuous relaxations of permutation matrices, allowing for end-to-end training of the actor-critic architecture. Empirical results demonstrate that agents trained with SPG perform well on sorting, the Euclidean TSP, and matching tasks. Additionally, SPG is more data efficient for matching tasks compared to baseline methods, suggesting that it can effectively learn useful representations for reasoning about permutations.",1
"We introduce the task of directly modeling a visually intelligent agent. Computer vision typically focuses on solving various subtasks related to visual intelligence. We depart from this standard approach to computer vision; instead we directly model a visually intelligent agent. Our model takes visual information as input and directly predicts the actions of the agent. Toward this end we introduce DECADE, a large-scale dataset of ego-centric videos from a dog's perspective as well as her corresponding movements. Using this data we model how the dog acts and how the dog plans her movements. We show under a variety of metrics that given just visual input we can successfully model this intelligent agent in many situations. Moreover, the representation learned by our model encodes distinct information compared to representations trained on image classification, and our learned representation can generalize to other domains. In particular, we show strong results on the task of walkable surface estimation by using this dog modeling task as representation learning.",0
"Our objective is to directly model a visually intelligent agent, which differs from the traditional computer vision approach of solving subtasks related to visual intelligence. Our model takes visual information and predicts the actions of the agent. To achieve this, we present DECADE, a vast dataset of ego-centric videos from a dog's perspective, and corresponding movements. We utilize this data to understand how the dog behaves and plans its movements. Our model successfully models this intelligent agent in various circumstances, as demonstrated by several metrics. Additionally, our model's learned representation contains unique information and can generalize to other domains. We demonstrate this by achieving excellent results in estimating walkable surfaces through representation learning from the dog modeling task.",1
"Person re-identification aims to match a person's identity across multiple camera streams. Deep neural networks have been successfully applied to the challenging person re-identification task. One remarkable bottleneck is that the existing deep models are data hungry and require large amounts of labeled training data. Acquiring manual annotations for pedestrian identity matchings in large-scale surveillance camera installations is a highly cumbersome task. Here, we propose the first semi-supervised approach that performs pseudo-labeling by considering complex relationships between unlabeled and labeled training samples in the feature space. Our approach first approximates the actual data manifold by learning a generative model via adversarial training. Given the trained model, data augmentation can be performed by generating new synthetic data samples which are unlabeled. An open research problem is how to effectively use this additional data for improved feature learning. To this end, this work proposes a novel Feature Affinity based Pseudo-Labeling (FAPL) approach with two possible label encodings under a unified setting. Our approach measures the affinity of unlabeled samples with the underlying clusters of labeled data samples using the intermediate feature representations from deep networks. FAPL trains with the joint supervision of cross-entropy loss together with a center regularization term, which not only ensures discriminative feature representation learning but also simultaneously predicts pseudo-labels for unlabeled data. Our extensive experiments on two standard large-scale datasets, Market-1501 and DukeMTMC-reID, demonstrate significant performance boosts over closely related competitors and outperforms state-of-the-art person re-identification techniques in most cases.",0
"The goal of person re-identification is to match a person's identity across various camera streams, and deep neural networks have proven to be effective in this task. However, a major issue is that existing deep models require a substantial amount of labeled training data, which can be difficult to obtain for large-scale surveillance camera installations. In this study, we propose a semi-supervised approach that utilizes pseudo-labeling to consider the complex relationships between labeled and unlabeled training samples in the feature space. Our approach first learns a generative model to approximate the data manifold through adversarial training, which allows for data augmentation by generating new synthetic data samples that are unlabeled. We then propose a Feature Affinity based Pseudo-Labeling (FAPL) approach that measures the affinity of unlabeled samples with labeled data samples' underlying clusters using deep network intermediate feature representations. FAPL trains using joint supervision of cross-entropy loss and a center regularization term, which simultaneously predicts pseudo-labels for unlabeled data and ensures discriminative feature representation learning. Our extensive experiments on two standard large-scale datasets show that our approach significantly outperforms related competitors and state-of-the-art person re-identification techniques in most cases.",1
"While generic object detection has achieved large improvements with rich feature hierarchies from deep nets, detecting small objects with poor visual cues remains challenging. Motion cues from multiple frames may be more informative for detecting such hard-to-distinguish objects in each frame. However, how to encode discriminative motion patterns, such as deformations and pose changes that characterize objects, has remained an open question. To learn them and thereby realize small object detection, we present a neural model called the Recurrent Correlational Network, where detection and tracking are jointly performed over a multi-frame representation learned through a single, trainable, and end-to-end network. A convolutional long short-term memory network is utilized for learning informative appearance change for detection, while learned representation is shared in tracking for enhancing its performance. In experiments with datasets containing images of scenes with small flying objects, such as birds and unmanned aerial vehicles, the proposed method yielded consistent improvements in detection performance over deep single-frame detectors and existing motion-based detectors. Furthermore, our network performs as well as state-of-the-art generic object trackers when it was evaluated as a tracker on the bird dataset.",0
"Despite the advances made in generic object detection through deep nets with rich feature hierarchies, detecting small objects with poor visual cues remains a difficult task. Detecting such objects in each frame may be more achievable by incorporating motion cues from multiple frames. However, how to encode the discriminative motion patterns that are unique to each object, such as deformations and pose changes, is still a challenge. In order to address this, we have developed a neural model, the Recurrent Correlational Network, which utilizes a single, trainable, and end-to-end network to perform joint detection and tracking over a multi-frame representation. A convolutional long short-term memory network is used to learn informative appearance changes for detection while the learned representation is shared in tracking to enhance its performance. Our method consistently outperforms deep single-frame detectors and existing motion-based detectors in detecting small flying objects, such as birds and unmanned aerial vehicles, and performs equally well as state-of-the-art generic object trackers when evaluated as a tracker on the bird dataset.",1
"Structured representations, such as Bags of Words, VLAD and Fisher Vectors, have proven highly effective to tackle complex visual recognition tasks. As such, they have recently been incorporated into deep architectures. However, while effective, the resulting deep structured representation learning strategies typically aggregate local features from the entire image, ignoring the fact that, in complex recognition tasks, some regions provide much more discriminative information than others.   In this paper, we introduce an attentional structured representation learning framework that incorporates an image-specific attention mechanism within the aggregation process. Our framework learns to predict jointly the image class label and an attention map in an end-to-end fashion and without any other supervision than the target label. As evidenced by our experiments, this consistently outperforms attention-less structured representation learning and yields state-of-the-art results on standard scene recognition and fine-grained categorization benchmarks.",0
"Structured representations like Bags of Words, VLAD, and Fisher Vectors have been successful in handling complex visual recognition tasks and have been integrated into deep architectures. However, the resulting deep structured representation learning strategies typically aggregate local features from the entire image, disregarding the fact that some regions provide more discriminative information than others in complex recognition tasks. This paper proposes an attentional structured representation learning framework that incorporates an image-specific attention mechanism in the aggregation process. The framework jointly predicts the image class label and an attention map in an end-to-end fashion without additional supervision. The experiments demonstrate that this approach consistently outperforms attention-less structured representation learning and achieves state-of-the-art results on standard scene recognition and fine-grained categorization benchmarks.",1
"Scale-space representation has been popular in computer vision community due to its theoretical foundation. The motivation for generating a scale-space representation of a given data set originates from the basic observation that real-world objects are composed of different structures at different scales. Hence, it's reasonable to consider learning features with image pyramids generated by smoothing and down-sampling operations. In this paper we propose Laplacian pyramid auto-encoders, a straightforward modification of the deep convolutional auto-encoder architecture, for unsupervised representation learning. The method uses multiple encoding-decoding sub-networks within a Laplacian pyramid framework to reconstruct the original image and the low pass filtered images. The last layer of each encoding sub-network also connects to an encoding layer of the sub-network in the next level, which aims to reverse the process of Laplacian pyramid generation. Experimental results showed that Laplacian pyramid benefited the classification and reconstruction performance of deep auto-encoder approaches, and batch normalization is critical to get deep auto-encoders approaches to begin learning.",0
"The theoretical foundation of scale-space representation has made it popular in the computer vision community. This representation is motivated by the fact that real-world objects consist of various structures at different scales. Therefore, it is reasonable to use image pyramids generated through smoothing and down-sampling operations to learn features. Our paper introduces Laplacian pyramid auto-encoders, a modification of the deep convolutional auto-encoder architecture for unsupervised representation learning. This method uses multiple encoding-decoding sub-networks within a Laplacian pyramid framework to reconstruct the original and low pass filtered images. The last layer of each encoding sub-network also connects to an encoding layer of the sub-network in the next level, which reverses the Laplacian pyramid generation process. We found that the Laplacian pyramid improves the classification and reconstruction performance of deep auto-encoder approaches, and that batch normalization is crucial for deep auto-encoders to start learning.",1
"Identifying potential abuses of human rights through imagery is a novel and challenging task in the field of computer vision, that will enable to expose human rights violations over large-scale data that may otherwise be impossible. While standard databases for object and scene categorisation contain hundreds of different classes, the largest available dataset of human rights violations contains only 4 classes. Here, we introduce the `Human Rights Archive Database' (HRA), a verified-by-experts repository of 3050 human rights violations photographs, labelled with human rights semantic categories, comprising a list of the types of human rights abuses encountered at present. With the HRA dataset and a two-phase transfer learning scheme, we fine-tuned the state-of-the-art deep convolutional neural networks (CNNs) to provide human rights violations classification CNNs (HRA-CNNs). We also present extensive experiments refined to evaluate how well object-centric and scene-centric CNN features can be combined for the task of recognising human rights abuses. With this, we show that HRA database poses a challenge at a higher level for the well studied representation learning methods, and provide a benchmark in the task of human rights violations recognition in visual context. We expect this dataset can help to open up new horizons on creating systems able of recognising rich information about human rights violations. Our dataset, codes and trained models are available online at https://github.com/GKalliatakis/Human-Rights-Archive-CNNs.",0
"In the field of computer vision, identifying potential abuses of human rights through imagery is a difficult and innovative task that can reveal human rights violations present in large-scale data that would otherwise go unnoticed. Standard databases for object and scene categorisation are abundant with hundreds of different classes, but the available dataset for human rights violations only contains four classes. To combat this issue, we have introduced the `Human Rights Archive Database' (HRA), which is a repository of 3050 human rights violations photographs that have been labelled with human rights semantic categories. This dataset presents a list of the types of human rights abuses encountered at present and has been verified by experts. Through a two-phase transfer learning scheme, we fine-tuned deep convolutional neural networks (CNNs) to create human rights violations classification CNNs (HRA-CNNs). Our experiments evaluated how object-centric and scene-centric CNN features could be combined for the recognition of human rights abuses, proving that the HRA database poses a challenge at a higher level for representation learning methods. This dataset and our trained models are available online at https://github.com/GKalliatakis/Human-Rights-Archive-CNNs, and we hope that they will aid in the creation of systems capable of recognising rich information about human rights violations.",1
"As digital medical imaging becomes more prevalent and archives increase in size, representation learning exposes an interesting opportunity for enhanced medical decision support systems. On the other hand, medical imaging data is often scarce and short on annotations. In this paper, we present an assessment of unsupervised feature learning approaches for images in the biomedical literature, which can be applied to automatic biomedical concept detection. Six unsupervised representation learning methods were built, including traditional bags of visual words, autoencoders, and generative adversarial networks. Each model was trained, and their respective feature space evaluated using images from the ImageCLEF 2017 concept detection task. We conclude that it is possible to obtain more powerful representations with modern deep learning approaches, in contrast with previously popular computer vision methods. Although generative adversarial networks can provide good results, they are harder to succeed in highly varied data sets. The possibility of semi-supervised learning, as well as their use in medical information retrieval problems, are the next steps to be strongly considered.",0
"Improved medical decision support systems can be achieved through representation learning as digital medical imaging becomes more widespread and archives grow in size. However, medical imaging data is typically scarce and lacks annotations. This study evaluates six unsupervised feature learning methods for biomedical images, including bags of visual words, autoencoders, and generative adversarial networks, for automatic biomedical concept detection. The feature space of each model was assessed using images from the ImageCLEF 2017 concept detection task. The study concludes that modern deep learning approaches can provide more robust representations compared to previously popular computer vision methods, although generative adversarial networks may struggle with highly diverse datasets. Future research should explore semi-supervised learning and the use of these methods in medical information retrieval problems.",1
"Graph representations have increasingly grown in popularity during the last years. Existing representation learning approaches explicitly encode network structure. Despite their good performance in downstream processes (e.g., node classification, link prediction), there is still room for improvement in different aspects, like efficacy, visualization, and interpretability. In this paper, we propose, t-PINE, a method that addresses these limitations. Contrary to baseline methods, which generally learn explicit graph representations by solely using an adjacency matrix, t-PINE avails a multi-view information graph, the adjacency matrix represents the first view, and a nearest neighbor adjacency, computed over the node features, is the second view, in order to learn explicit and implicit node representations, using the Canonical Polyadic (a.k.a. CP) decomposition. We argue that the implicit and the explicit mapping from a higher-dimensional to a lower-dimensional vector space is the key to learn more useful, highly predictable, and gracefully interpretable representations. Having good interpretable representations provides a good guidance to understand how each view contributes to the representation learning process. In addition, it helps us to exclude unrelated dimensions. Extensive experiments show that t-PINE drastically outperforms baseline methods by up to 158.6% with respect to Micro-F1, in several multi-label classification problems, while it has high visualization and interpretability utility.",0
"In recent years, graph representations have gained popularity, with existing approaches focusing on encoding network structure to improve downstream processes, such as node classification and link prediction. Despite their success, there is still room for improvement in areas such as efficacy, visualization, and interpretability. This paper introduces t-PINE, a method that addresses these limitations by utilizing a multi-view information graph that includes both an adjacency matrix and a nearest neighbor adjacency computed over node features. By using Canonical Polyadic decomposition, t-PINE is able to learn both explicit and implicit node representations, leading to more useful and predictable results. Improved interpretability also allows for a better understanding of how each view contributes to the representation learning process, leading to better performance in multi-label classification problems. In experiments, t-PINE outperforms baseline methods by up to 158.6% in Micro-F1, while also offering high visualization and interpretability.",1
"Machine Learning (ML) is increasingly being used for computer aided diagnosis of brain related disorders based on structural magnetic resonance imaging (MRI) data. Most of such work employs biologically and medically meaningful hand-crafted features calculated from different regions of the brain. The construction of such highly specialized features requires a considerable amount of time, manual oversight and careful quality control to ensure the absence of errors in the computational process. Recent advances in Deep Representation Learning have shown great promise in extracting highly non-linear and information-rich features from data. In this paper, we present a novel large-scale deep unsupervised approach to learn generic feature representations of structural brain MRI scans, which requires no specialized domain knowledge or manual intervention. Our method produces low-dimensional representations of brain structure, which can be used to reconstruct brain images with very low error and exhibit performance comparable to FreeSurfer features on various classification tasks.",0
"The use of Machine Learning (ML) is becoming more common in computer aided diagnosis of brain-related disorders that rely on structural magnetic resonance imaging (MRI) data. Typically, this work uses manually crafted features that are biologically and medically relevant and are calculated from different brain regions. Creating these specialized features is time-consuming and requires careful quality control to avoid computational errors. However, Deep Representation Learning has recently shown promise in extracting highly informative features from data. This study introduces a new approach that uses deep unsupervised learning to generate generic feature representations of structural brain MRI scans without requiring specialized knowledge or manual intervention. The resulting low-dimensional representations can be used to create brain images with minimal error and perform comparably to FreeSurfer features on multiple classification tasks.",1
"While representation learning aims to derive interpretable features for describing visual data, representation disentanglement further results in such features so that particular image attributes can be identified and manipulated. However, one cannot easily address this task without observing ground truth annotation for the training data. To address this problem, we propose a novel deep learning model of Cross-Domain Representation Disentangler (CDRD). By observing fully annotated source-domain data and unlabeled target-domain data of interest, our model bridges the information across data domains and transfers the attribute information accordingly. Thus, cross-domain joint feature disentanglement and adaptation can be jointly performed. In the experiments, we provide qualitative results to verify our disentanglement capability. Moreover, we further confirm that our model can be applied for solving classification tasks of unsupervised domain adaptation, and performs favorably against state-of-the-art image disentanglement and translation methods.",0
"Representation learning aims to create features that can be used to describe visual data, but representation disentanglement goes one step further by creating features that allow specific image attributes to be identified and manipulated. However, this task is difficult without ground truth annotation for training data. To solve this problem, we propose the Cross-Domain Representation Disentangler (CDRD), a novel deep learning model that uses fully annotated source-domain data and unlabeled target-domain data to transfer attribute information and perform cross-domain joint feature disentanglement and adaptation. Our experiments show that the CDRD has strong disentanglement capabilities and can be used for unsupervised domain adaptation classification tasks, outperforming state-of-the-art image disentanglement and translation methods.",1
"Person re-identification (re-ID) requires rapid, flexible yet discriminant representations to quickly generalize to unseen observations on-the-fly and recognize the same identity across disjoint camera views. Recent effective methods are developed in a pair-wise similarity learning system to detect a fixed set of features from distinct regions which are mapped to their vector embeddings for the distance measuring. However, the most relevant and crucial parts of each image are detected independently without referring to the dependency conditioned on one and another. Also, these region based methods rely on spatial manipulation to position the local features in comparable similarity measuring. To combat these limitations, in this paper we introduce the Deep Co-attention based Comparators (DCCs) that fuse the co-dependent representations of the paired images so as to focus on the relevant parts of both images and produce their \textit{relative representations}. Given a pair of pedestrian images to be compared, the proposed model mimics the foveation of human eyes to detect distinct regions concurrent on both images, namely co-dependent features, and alternatively attend to relevant regions to fuse them into the similarity learning. Our comparator is capable of producing dynamic representations relative to a particular sample every time, and thus well-suited to the case of re-identifying pedestrians on-the-fly. We perform extensive experiments to provide the insights and demonstrate the effectiveness of the proposed DCCs in person re-ID. Moreover, our approach has achieved the state-of-the-art performance on three benchmark data sets: DukeMTMC-reID \cite{DukeMTMC}, CUHK03 \cite{FPNN}, and Market-1501 \cite{Market1501}.",0
"The task of person re-identification (re-ID) necessitates the use of representations that are capable of rapid, flexible and discriminative learning, in order to recognize the same individual across different camera views and generalize to unseen observations. Current methods rely on a pairwise similarity learning system, which identifies a fixed set of features from distinct regions and maps them to their vector embeddings for distance calculation. However, these methods have limitations in detecting the most relevant parts of each image independently, without considering the inter-dependency between them. Furthermore, they rely on spatial manipulation for feature positioning during similarity calculation. To address these limitations, this paper introduces Deep Co-attention based Comparators (DCCs), which fuse co-dependent representations of paired images to focus on relevant parts of both images and produce their relative representations. Our comparator mimics the foveation of human eyes to detect distinct regions concurrent on both images, and alternately attends to relevant regions to fuse them into the similarity learning. This approach allows for dynamic representations relative to a particular sample every time, making it well-suited to the task of re-identifying pedestrians on-the-fly. Experimental results demonstrate the effectiveness of DCCs in person re-ID and show that our approach outperforms state-of-the-art methods on three benchmark datasets: DukeMTMC-reID, CUHK03, and Market-1501.",1
"Video representation learning is a vital problem for classification task. Recently, a promising unsupervised paradigm termed self-supervised learning has emerged, which explores inherent supervisory signals implied in massive data for feature learning via solving auxiliary tasks. However, existing methods in this regard suffer from two limitations when extended to video classification. First, they focus only on a single task, whereas ignoring complementarity among different task-specific features and thus resulting in suboptimal video representation. Second, high computational and memory cost hinders their application in real-world scenarios. In this paper, we propose a graph-based distillation framework to address these problems: (1) We propose logits graph and representation graph to transfer knowledge from multiple self-supervised tasks, where the former distills classifier-level knowledge by solving a multi-distribution joint matching problem, and the latter distills internal feature knowledge from pairwise ensembled representations with tackling the challenge of heterogeneity among different features; (2) The proposal that adopts a teacher-student framework can reduce the redundancy of knowledge learnt from teachers dramatically, leading to a lighter student model that solves classification task more efficiently. Experimental results on 3 video datasets validate that our proposal not only helps learn better video representation but also compress model for faster inference.",0
"The problem of video representation learning is crucial for classification tasks. A promising approach, called self-supervised learning, has emerged to explore supervisory signals inherent in massive data for feature learning by solving auxiliary tasks. However, current methods for self-supervised learning have two limitations when applied to video classification. Firstly, they focus on a single task and ignore the complementarity of different task-specific features, leading to suboptimal video representation. Secondly, their high computational and memory costs make them unsuitable for real-world scenarios. This paper proposes a graph-based distillation framework to address these issues. The framework involves a logits graph and a representation graph to transfer knowledge from multiple self-supervised tasks. The former distills classifier-level knowledge by solving a multi-distribution joint matching problem, while the latter distills internal feature knowledge from pairwise ensembled representations, addressing the heterogeneity among different features. The proposed teacher-student framework reduces the redundancy of knowledge learnt from teachers, resulting in a lighter student model that solves the classification task more efficiently. The experimental results on three video datasets validate that our approach helps learn better video representation and compresses the model for faster inference.",1
"In this study, we propose the integration of competitive learning into convolutional neural networks (CNNs) to improve the representation learning and efficiency of fine-tuning. Conventional CNNs use back propagation learning, and it enables powerful representation learning by a discrimination task. However, it requires huge amount of labeled data, and acquisition of labeled data is much harder than that of unlabeled data. Thus, efficient use of unlabeled data is getting crucial for DNNs. To address the problem, we introduce unsupervised competitive learning into the convolutional layer, and utilize unlabeled data for effective representation learning. The results of validation experiments using a toy model demonstrated that strong representation learning effectively extracted bases of images into convolutional filters using unlabeled data, and accelerated the speed of the fine-tuning of subsequent supervised back propagation learning. The leverage was more apparent when the number of filters was sufficiently large, and, in such a case, the error rate steeply decreased in the initial phase of fine-tuning. Thus, the proposed method enlarged the number of filters in CNNs, and enabled a more detailed and generalized representation. It could provide a possibility of not only deep but broad neural networks.",0
"The aim of our study is to enhance the efficiency and representation learning of convolutional neural networks (CNNs) by integrating competitive learning. The conventional CNNs use back propagation learning for representation learning through a discrimination task. However, this method requires an extensive amount of labeled data, which is difficult to acquire compared to unlabeled data. Hence, we propose utilizing unsupervised competitive learning in the convolutional layer to leverage unlabeled data for effective representation learning. Our validation experiments on a toy model showed that this approach effectively extracted image bases into convolutional filters using unlabeled data and accelerated the speed of fine-tuning through subsequent supervised back propagation learning. The benefits were more noticeable when the number of filters was sufficiently large, resulting in a significant reduction in error rates during the initial phase of fine-tuning. Our proposed method also expanded the number of filters in CNNs, allowing for more detailed and generalized representations. This approach has the potential to enable not only deep but also broad neural networks.",1
"Learning the disentangled representation of interpretable generative factors of data is one of the foundations to allow artificial intelligence to think like people. In this paper, we propose the analogical training strategy for the unsupervised disentangled representation learning in generative models. The analogy is one of the typical cognitive processes, and our proposed strategy is based on the observation that sample pairs in which one is different from the other in one specific generative factor show the same analogical relation. Thus, the generator is trained to generate sample pairs from which a designed classifier can identify the underlying analogical relation. In addition, we propose a disentanglement metric called the subspace score, which is inspired by subspace learning methods and does not require supervised information. Experiments show that our proposed training strategy allows the generative models to find the disentangled factors, and that our methods can give competitive performances as compared with the state-of-the-art methods.",0
"To enable artificial intelligence to reason like humans, it is essential to learn the disentangled representation of interpretable generative factors of data. The present study introduces an analogical training strategy for unsupervised disentangled representation learning in generative models. Analogical reasoning is a typical cognitive process, and our suggested strategy is based on the observation that sample pairs that differ in a specific generative factor share the same analogical relation. This approach trains the generator to produce sample pairs that a designed classifier can use to identify the underlying analogical relation. Furthermore, we propose a disentanglement metric named the subspace score, inspired by subspace learning methods and does not require supervised information. Our experiments demonstrate that our proposed training strategy enables generative models to identify the disentangled factors and give competitive performances when compared to state-of-the-art methods.",1
"Methods for learning feature representations for Offline Handwritten Signature Verification have been successfully proposed in recent literature, using Deep Convolutional Neural Networks to learn representations from signature pixels. Such methods reported large performance improvements compared to handcrafted feature extractors. However, they also introduced an important constraint: the inputs to the neural networks must have a fixed size, while signatures vary significantly in size between different users. In this paper we propose addressing this issue by learning a fixed-sized representation from variable-sized signatures by modifying the network architecture, using Spatial Pyramid Pooling. We also investigate the impact of the resolution of the images used for training, and the impact of adapting (fine-tuning) the representations to new operating conditions (different acquisition protocols, such as writing instruments and scan resolution). On the GPDS dataset, we achieve results comparable with the state-of-the-art, while removing the constraint of having a maximum size for the signatures to be processed. We also show that using higher resolutions (300 or 600dpi) can improve performance when skilled forgeries from a subset of users are available for feature learning, but lower resolutions (around 100dpi) can be used if only genuine signatures are used. Lastly, we show that fine-tuning can improve performance when the operating conditions change.",0
"Recent literature has proposed successful methods for learning feature representations for Offline Handwritten Signature Verification. These methods utilize Deep Convolutional Neural Networks to learn representations from signature pixels and have shown significant performance improvements compared to handcrafted feature extractors. However, these methods have a drawback: the inputs to the neural networks must be of a fixed size, which is a constraint since signatures vary in size between different users. To address this issue, this paper proposes modifying the network architecture by using Spatial Pyramid Pooling to learn a fixed-sized representation from variable-sized signatures. Additionally, the impact of resolution and adapting representations to new operating conditions is investigated. Results on the GPDS dataset show that this approach achieves comparable results to the state-of-the-art while removing the constraint of a maximum signature size. Using higher resolutions (300 or 600dpi) can improve performance, but lower resolutions (around 100dpi) can be used if only genuine signatures are available. Furthermore, fine-tuning can improve performance when the operating conditions change.",1
"From the frame/clip-level feature learning to the video-level representation building, deep learning methods in action recognition have developed rapidly in recent years. However, current methods suffer from the confusion caused by partial observation training, or without end-to-end learning, or restricted to single temporal scale modeling and so on. In this paper, we build upon two-stream ConvNets and propose Deep networks with Temporal Pyramid Pooling (DTPP), an end-to-end video-level representation learning approach, to address these problems. Specifically, at first, RGB images and optical flow stacks are sparsely sampled across the whole video. Then a temporal pyramid pooling layer is used to aggregate the frame-level features which consist of spatial and temporal cues. Lastly, the trained model has compact video-level representation with multiple temporal scales, which is both global and sequence-aware. Experimental results show that DTPP achieves the state-of-the-art performance on two challenging video action datasets: UCF101 and HMDB51, either by ImageNet pre-training or Kinetics pre-training.",0
"In recent years, deep learning methods have made significant progress in action recognition, from frame/clip-level feature learning to video-level representation building. However, current approaches have limitations such as partial observation training, lack of end-to-end learning, or restricted single temporal scale modeling. To address these issues, this paper proposes Deep networks with Temporal Pyramid Pooling (DTPP), an end-to-end video-level representation learning method that builds on two-stream ConvNets. DTPP sparsely samples RGB images and optical flow stacks across the entire video and uses a temporal pyramid pooling layer to aggregate frame-level features that include spatial and temporal cues. The trained model generates a compact video-level representation with multiple temporal scales that is both global and sequence-aware. Experimental results demonstrate that DTPP achieves state-of-the-art performance on challenging video action datasets, UCF101 and HMDB51, with either ImageNet pre-training or Kinetics pre-training.",1
"Voxelwise classification approaches are popular and effective methods for tissue quantification in brain magnetic resonance imaging (MRI) scans. However, generalization of these approaches is hampered by large differences between sets of MRI scans such as differences in field strength, vendor or acquisition protocols. Due to this acquisition related variation, classifiers trained on data from a specific scanner fail or under-perform when applied to data that was acquired differently. In order to address this lack of generalization, we propose a Siamese neural network (MRAI-net) to learn a representation that minimizes the between-scanner variation, while maintaining the contrast between brain tissues necessary for brain tissue quantification. The proposed MRAI-net was evaluated on both simulated and real MRI data. After learning the MR acquisition invariant representation, any supervised classification model that uses feature vectors can be applied. In this paper, we provide a proof of principle, which shows that a linear classifier applied on the MRAI representation is able to outperform supervised convolutional neural network classifiers for tissue classification when little target training data is available.",0
"Classifying brain tissues in MRI scans using voxelwise classification approaches is effective, but their generalization is hindered by differences in field strength, vendor, or acquisition protocols between sets of scans. This acquisition-related variation causes classifiers trained on specific scanners to fail when applied to differently acquired data. To overcome this issue, a Siamese neural network called MRAI-net is proposed to learn a representation that reduces between-scanner variation while maintaining necessary tissue contrast. The efficacy of the MRAI-net is demonstrated on simulated and real MRI data, and a linear classifier applied on the MRAI representation outperforms supervised convolutional neural network classifiers for tissue classification with limited target training data.",1
"Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep information bottleneck model, identify its shortcomings and propose a model that circumvents them. To this end, we apply a copula transformation which, by restoring the invariance properties of the information bottleneck method, leads to disentanglement of the features in the latent space. Building on that, we show how this transformation translates to sparsity of the latent space in the new model. We evaluate our method on artificial and real data.",0
"Representation learning can be effectively performed with deep latent variable models. The deep information bottleneck model is utilized in this study, but we identify its limitations and present a new model that overcomes them. By applying a copula transformation, we restore the information bottleneck method's invariance properties, resulting in the separation of features in the latent space. Furthermore, we demonstrate how this transformation results in sparsity of the latent space in the new model. The efficacy of our approach is evaluated using both artificial and real data.",1
"This thesis investigates unsupervised time series representation learning for sequence prediction problems, i.e. generating nice-looking input samples given a previous history, for high dimensional input sequences by decoupling the static input representation from the recurrent sequence representation. We introduce three models based on Generative Stochastic Networks (GSN) for unsupervised sequence learning and prediction. Experimental results for these three models are presented on pixels of sequential handwritten digit (MNIST) data, videos of low-resolution bouncing balls, and motion capture data. The main contribution of this thesis is to provide evidence that GSNs are a viable framework to learn useful representations of complex sequential input data, and to suggest a new framework for deep generative models to learn complex sequences by decoupling static input representations from dynamic time dependency representations.",0
"The aim of this study is to explore the possibility of unsupervised time series representation learning for sequence prediction problems. This involves creating attractive input samples based on previous history for high dimensional input sequences by separating the static input representation from the recurrent sequence representation. To achieve this, three models that rely on Generative Stochastic Networks (GSN) for unsupervised sequence learning and prediction were developed. The efficacy of these models was tested using sequential handwritten digit (MNIST) data pixels, low-resolution bouncing ball videos, and motion capture data. The primary objective of this thesis is to demonstrate that GSNs can be used to construct useful representations of complex sequential input data, and to propose a novel framework for deep generative models that can learn complex sequences by separating static input representations from dynamic time dependency representations.",1
"Data of different modalities generally convey complimentary but heterogeneous information, and a more discriminative representation is often preferred by combining multiple data modalities like the RGB and infrared features. However in reality, obtaining both data channels is challenging due to many limitations. For example, the RGB surveillance cameras are often restricted from private spaces, which is in conflict with the need of abnormal activity detection for personal security. As a result, using partial data channels to build a full representation of multi-modalities is clearly desired. In this paper, we propose a novel Partial-modal Generative Adversarial Networks (PM-GANs) that learns a full-modal representation using data from only partial modalities. The full representation is achieved by a generated representation in place of the missing data channel. Extensive experiments are conducted to verify the performance of our proposed method on action recognition, compared with four state-of-the-art methods. Meanwhile, a new Infrared-Visible Dataset for action recognition is introduced, and will be the first publicly available action dataset that contains paired infrared and visible spectrum.",0
"Different types of data often provide complementary yet distinct information, and combining multiple data modalities such as RGB and infrared features can result in a more effective and precise representation. However, obtaining both types of data is often difficult due to various limitations. For instance, surveillance cameras that capture RGB data are often restricted from accessing private areas, which conflicts with the need for personal security and abnormal activity detection. Therefore, it is necessary to use partial data channels to construct a full representation of multi-modalities. This paper presents a new approach called Partial-modal Generative Adversarial Networks (PM-GANs) that can generate a full-modal representation using incomplete data from partial modalities. The missing data channel is replaced with a generated representation to achieve a complete representation. We conducted extensive experiments to assess the efficacy of the proposed method in action recognition and compared it with four state-of-the-art methods. Additionally, we introduced a new Infrared-Visible Dataset for action recognition, which is the first publicly available dataset that includes paired infrared and visible spectrum.",1
"Reliably modeling normality and differentiating abnormal appearances from normal cases is a very appealing approach for detecting pathologies in medical images. A plethora of such unsupervised anomaly detection approaches has been made in the medical domain, based on statistical methods, content-based retrieval, clustering and recently also deep learning. Previous approaches towards deep unsupervised anomaly detection model patches of normal anatomy with variants of Autoencoders or GANs, and detect anomalies either as outliers in the learned feature space or from large reconstruction errors. In contrast to these patch-based approaches, we show that deep spatial autoencoding models can be efficiently used to capture normal anatomical variability of entire 2D brain MR images. A variety of experiments on real MR data containing MS lesions corroborates our hypothesis that we can detect and even delineate anomalies in brain MR images by simply comparing input images to their reconstruction. Results show that constraints on the latent space and adversarial training can further improve the segmentation performance over standard deep representation learning.",0
"Detecting pathologies in medical images by reliably modeling normality and distinguishing abnormal appearances from normal cases is an attractive approach. The medical field has numerous unsupervised anomaly detection approaches based on statistical methods, content-based retrieval, clustering, and deep learning. Previous methods for deep unsupervised anomaly detection involved modeling patches of normal anatomy using Autoencoders or GANs and detecting anomalies as outliers in the learned feature space or from large reconstruction errors. Our research demonstrates that deep spatial autoencoding models are efficient in capturing normal anatomical variability of entire 2D brain MR images, contrary to the patch-based approaches. We have conducted multiple experiments on real MR data with MS lesions, and our hypothesis is corroborated that we can identify and even outline anomalies in brain MR images by comparing input images to their reconstruction. Our results indicate that incorporating constraints on the latent space and adversarial training can improve the segmentation performance beyond standard deep representation learning.",1
"This paper presents a novel unsupervised segmentation method for 3D medical images. Convolutional neural networks (CNNs) have brought significant advances in image segmentation. However, most of the recent methods rely on supervised learning, which requires large amounts of manually annotated data. Thus, it is challenging for these methods to cope with the growing amount of medical images. This paper proposes a unified approach to unsupervised deep representation learning and clustering for segmentation. Our proposed method consists of two phases. In the first phase, we learn deep feature representations of training patches from a target image using joint unsupervised learning (JULE) that alternately clusters representations generated by a CNN and updates the CNN parameters using cluster labels as supervisory signals. We extend JULE to 3D medical images by utilizing 3D convolutions throughout the CNN architecture. In the second phase, we apply k-means to the deep representations from the trained CNN and then project cluster labels to the target image in order to obtain the fully segmented image. We evaluated our methods on three images of lung cancer specimens scanned with micro-computed tomography (micro-CT). The automatic segmentation of pathological regions in micro-CT could further contribute to the pathological examination process. Hence, we aim to automatically divide each image into the regions of invasive carcinoma, noninvasive carcinoma, and normal tissue. Our experiments show the potential abilities of unsupervised deep representation learning for medical image segmentation.",0
"This article presents a new method for segmenting 3D medical images without supervision. While convolutional neural networks have advanced image segmentation, most current methods rely on supervised learning, which requires significant amounts of annotated data, making it difficult to handle the increasing number of medical images. The proposed approach combines unsupervised deep representation learning and clustering for segmentation in two phases. In the first phase, training patches from a target image are used to generate deep feature representations with joint unsupervised learning. The second phase involves applying k-means to the deep representations and projecting cluster labels to the target image to obtain a fully segmented image. The method was tested on three lung cancer specimens scanned with micro-CT and was able to automatically divide each image into invasive carcinoma, noninvasive carcinoma, and normal tissue regions, demonstrating the potential of unsupervised deep representation learning for medical image segmentation.",1
"This paper presents a novel method for unsupervised segmentation of pathology images. Staging of lung cancer is a major factor of prognosis. Measuring the maximum dimensions of the invasive component in a pathology images is an essential task. Therefore, image segmentation methods for visualizing the extent of invasive and noninvasive components on pathology images could support pathological examination. However, it is challenging for most of the recent segmentation methods that rely on supervised learning to cope with unlabeled pathology images. In this paper, we propose a unified approach to unsupervised representation learning and clustering for pathology image segmentation. Our method consists of two phases. In the first phase, we learn feature representations of training patches from a target image using the spherical k-means. The purpose of this phase is to obtain cluster centroids which could be used as filters for feature extraction. In the second phase, we apply conventional k-means to the representations extracted by the centroids and then project cluster labels to the target images. We evaluated our methods on pathology images of lung cancer specimen. Our experiments showed that the proposed method outperforms traditional k-means segmentation and the multithreshold Otsu method both quantitatively and qualitatively with an improved normalized mutual information (NMI) score of 0.626 compared to 0.168 and 0.167, respectively. Furthermore, we found that the centroids can be applied to the segmentation of other slices from the same sample.",0
"A new approach to unsupervised segmentation of pathology images is introduced in this article. The staging of lung cancer is a critical prognostic factor, and measuring the maximum dimensions of the invasive component in pathology images is essential. However, most segmentation methods that rely on supervised learning struggle to handle unlabeled pathology images. To address this challenge, the proposed method employs a unified approach to unsupervised representation learning and clustering for pathology image segmentation. The method consists of two phases: first, training patches are used to learn feature representations via spherical k-means, and second, conventional k-means is applied to the extracted representations to project cluster labels to target images. The method outperforms traditional k-means segmentation and the multithreshold Otsu method, as demonstrated by improved normalized mutual information (NMI) scores. Additionally, the proposed method's centroids can be used for segmenting other slices from the same sample.",1
"In this paper, we introduce a method for adapting the step-sizes of temporal difference (TD) learning. The performance of TD methods often depends on well chosen step-sizes, yet few algorithms have been developed for setting the step-size automatically for TD learning. An important limitation of current methods is that they adapt a single step-size shared by all the weights of the learning system. A vector step-size enables greater optimization by specifying parameters on a per-feature basis. Furthermore, adapting parameters at different rates has the added benefit of being a simple form of representation learning. We generalize Incremental Delta Bar Delta (IDBD)---a vectorized adaptive step-size method for supervised learning---to TD learning, which we name TIDBD. We demonstrate that TIDBD is able to find appropriate step-sizes in both stationary and non-stationary prediction tasks, outperforming ordinary TD methods and TD methods with scalar step-size adaptation; we demonstrate that it can differentiate between features which are relevant and irrelevant for a given task, performing representation learning; and we show on a real-world robot prediction task that TIDBD is able to outperform ordinary TD methods and TD methods augmented with AlphaBound and RMSprop.",0
"This paper introduces a method that adapts the step-sizes of temporal difference (TD) learning. TD methods' performance is often reliant on well-selected step-sizes, but few algorithms exist for automating step-size selection for TD learning. Current methods' primary limitation is that they adjust a single step-size that applies to all the learning system's weights. A vector step-size offers more optimization opportunities by specifying parameters on a per-feature basis. Additionally, adjusting parameters at different rates is an uncomplicated form of representation learning. This research generalizes Incremental Delta Bar Delta (IDBD) to TD learning, naming it TIDBD. Results show that TIDBD can identify appropriate step-sizes in both stationary and non-stationary prediction tasks, outperforming ordinary TD methods and TD methods with scalar step-size adaptation. TIDBD can differentiate between relevant and irrelevant features for a task, performing representation learning. Furthermore, TIDBD outperforms ordinary TD methods and TD methods augmented with AlphaBound and RMSprop on a real-world robot prediction task.",1
"Being able to predict what may happen in the future requires an in-depth understanding of the physical and causal rules that govern the world. A model that is able to do so has a number of appealing applications, from robotic planning to representation learning. However, learning to predict raw future observations, such as frames in a video, is exceedingly challenging -- the ambiguous nature of the problem can cause a naively designed model to average together possible futures into a single, blurry prediction. Recently, this has been addressed by two distinct approaches: (a) latent variational variable models that explicitly model underlying stochasticity and (b) adversarially-trained models that aim to produce naturalistic images. However, a standard latent variable model can struggle to produce realistic results, and a standard adversarially-trained model underutilizes latent variables and fails to produce diverse predictions. We show that these distinct methods are in fact complementary. Combining the two produces predictions that look more realistic to human raters and better cover the range of possible futures. Our method outperforms prior and concurrent work in these aspects.",0
"An in-depth understanding of the physical and causal rules that govern the world is necessary for predicting future events. A model with this ability has numerous practical applications, such as robotic planning and representation learning. However, predicting raw future observations, such as video frames, can be challenging due to the vague nature of the problem. This can result in a model that creates a single, unclear prediction by averaging all possible outcomes. Two approaches have recently been developed to address this issue: (a) latent variational variable models that take into account underlying stochasticity and (b) adversarially-trained models that aim to produce natural-looking images. However, these methods have their limitations. Latent variable models may struggle to produce realistic results, while adversarially-trained models may not use latent variables to their full potential and fail to create diverse predictions. By combining these two methods, we have developed an approach that produces more realistic and diverse predictions, as observed by human raters. Our method outperforms previous and concurrent work in these aspects.",1
"A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.",0
"Learning abstract representations that are effective in specifying goals, planning, and generalization is a significant challenge in complex visuomotor control. To address this, universal planning networks (UPN) have been introduced. These networks embed differentiable planning in a goal-directed policy by unrolling a forward model in a latent space and inferring the optimal action plan through gradient descent trajectory optimization. The process of plan-by-gradient-descent and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. The representations learned through UPN are effective for goal-directed visual imitation via gradient-based trajectory optimization and can also provide a metric for specifying goals using images. The learned representations can be utilized to specify distance-based rewards for model-free reinforcement learning, resulting in more effective learning when solving new tasks described via image-based goals. Successful transfer of visuomotor planning strategies across robots with differing morphologies and actuation capabilities has been achieved.",1
"In this paper, we explore neural network models that learn to associate segments of spoken audio captions with the semantically relevant portions of natural images that they refer to. We demonstrate that these audio-visual associative localizations emerge from network-internal representations learned as a by-product of training to perform an image-audio retrieval task. Our models operate directly on the image pixels and speech waveform, and do not rely on any conventional supervision in the form of labels, segmentations, or alignments between the modalities during training. We perform analysis using the Places 205 and ADE20k datasets demonstrating that our models implicitly learn semantically-coupled object and word detectors.",0
"The purpose of this paper is to investigate neural network models that can learn to match segments of spoken audio captions with the relevant portions of natural images. Our study shows that these audio-visual associations are formed through the network's internal representations, which are acquired while training for an image-audio retrieval task. Our models do not require traditional supervision, such as labels, segmentations, or modal alignment during training, and operate directly on the image pixels and speech waveform. We demonstrate the effectiveness of our models using the Places 205 and ADE20k datasets and reveal that they can implicitly learn object and word detectors that are semantically linked.",1
"Learning compact representation is vital and challenging for large scale multimedia data. Cross-view/cross-modal hashing for effective binary representation learning has received significant attention with exponentially growing availability of multimedia content. Most existing cross-view hashing algorithms emphasize the similarities in individual views, which are then connected via cross-view similarities. In this work, we focus on the exploitation of the discriminative information from different views, and propose an end-to-end method to learn semantic-preserving and discriminative binary representation, dubbed Discriminative Cross-View Hashing (DCVH), in light of learning multitasking binary representation for various tasks including cross-view retrieval, image-to-image retrieval, and image annotation/tagging. The proposed DCVH has the following key components. First, it uses convolutional neural network (CNN) based nonlinear hashing functions and multilabel classification for both images and texts simultaneously. Such hashing functions achieve effective continuous relaxation during training without explicit quantization loss by using Direct Binary Embedding (DBE) layers. Second, we propose an effective view alignment via Hamming distance minimization, which is efficiently accomplished by bit-wise XOR operation. Extensive experiments on two image-text benchmark datasets demonstrate that DCVH outperforms state-of-the-art cross-view hashing algorithms as well as single-view image hashing algorithms. In addition, DCVH can provide competitive performance for image annotation/tagging.",0
"It is crucial and difficult to learn compact representations for large-scale multimedia data. With the exponentially increasing availability of multimedia content, cross-view/cross-modal hashing has garnered significant attention as an effective binary representation learning technique. Most existing cross-view hashing algorithms focus on the similarities within individual views and connect them via cross-view similarities. However, our work focuses on exploiting the discriminative information from different views to develop an end-to-end method for learning semantic-preserving and discriminative binary representation, named Discriminative Cross-View Hashing (DCVH). DCVH accomplishes multitasking binary representation for various tasks such as cross-view retrieval, image-to-image retrieval, and image annotation/tagging. It comprises two key components: convolutional neural network-based nonlinear hashing functions and multilabel classification for images and texts simultaneously, and an effective view alignment via Hamming distance minimization achieved by bit-wise XOR operation. During training, the hashing functions achieve effective continuous relaxation without explicit quantization loss using Direct Binary Embedding (DBE) layers. Extensive experiments on two image-text benchmark datasets demonstrate that DCVH outperforms state-of-the-art cross-view hashing algorithms and single-view image hashing algorithms. Furthermore, DCVH can provide competitive performance for image annotation/tagging.",1
"Deep metric learning has been demonstrated to be highly effective in learning semantic representation and encoding information that can be used to measure data similarity, by relying on the embedding learned from metric learning. At the same time, variational autoencoder (VAE) has widely been used to approximate inference and proved to have a good performance for directed probabilistic models. However, for traditional VAE, the data label or feature information are intractable. Similarly, traditional representation learning approaches fail to represent many salient aspects of the data. In this project, we propose a novel integrated framework to learn latent embedding in VAE by incorporating deep metric learning. The features are learned by optimizing a triplet loss on the mean vectors of VAE in conjunction with standard evidence lower bound (ELBO) of VAE. This approach, which we call Triplet based Variational Autoencoder (TVAE), allows us to capture more fine-grained information in the latent embedding. Our model is tested on MNIST data set and achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma & Welling, 2013) achieves triplet accuracy of 75.08%.",0
"The effectiveness of deep metric learning in creating semantic representation and measuring data similarity through embedding has been demonstrated. Variational autoencoder (VAE) is commonly used for approximating inference in directed probabilistic models with good performance. However, traditional VAE struggles to handle data label or feature information, and standard representation learning approaches often fail to represent crucial aspects of data. This project presents a new integrated framework called Triplet based Variational Autoencoder (TVAE) that combines deep metric learning and VAE to learn latent embedding. TVAE optimizes a triplet loss on VAE's mean vectors with the standard evidence lower bound (ELBO) of VAE to capture more detailed information in the latent embedding. When tested on MNIST data, TVAE achieves a high triplet accuracy of 95.60%, while the traditional VAE (Kingma & Welling, 2013) reaches only 75.08%.",1
"Visual Domain Adaptation is a problem of immense importance in computer vision. Previous approaches showcase the inability of even deep neural networks to learn informative representations across domain shift. This problem is more severe for tasks where acquiring hand labeled data is extremely hard and tedious. In this work, we focus on adapting the representations learned by segmentation networks across synthetic and real domains. Contrary to previous approaches that use a simple adversarial objective or superpixel information to aid the process, we propose an approach based on Generative Adversarial Networks (GANs) that brings the embeddings closer in the learned feature space. To showcase the generality and scalability of our approach, we show that we can achieve state of the art results on two challenging scenarios of synthetic to real domain adaptation. Additional exploratory experiments show that our approach: (1) generalizes to unseen domains and (2) results in improved alignment of source and target distributions.",0
"The field of computer vision regards Visual Domain Adaptation as a problem of utmost significance. Even deep neural networks have failed to produce informative representations across domain shift, as previous approaches have demonstrated. This issue is more pronounced for tasks that require laborious acquisition of hand-labeled data. This study focuses on adapting the representations learned by segmentation networks across synthetic and real domains. Unlike previous methods that rely on a simple adversarial objective or superpixel information to aid the process, this approach utilizes Generative Adversarial Networks (GANs) to bring the embeddings closer in the learned feature space. The results of this study demonstrate the generality and scalability of the proposed approach, which achieves state-of-the-art outcomes in two challenging scenarios of synthetic to real domain adaptation. Furthermore, additional exploratory experiments reveal that this approach (1) generalizes to unseen domains and (2) leads to improved alignment of source and target distributions.",1
"Person re-identification is a challenging task mainly due to factors such as background clutter, pose, illumination and camera point of view variations. These elements hinder the process of extracting robust and discriminative representations, hence preventing different identities from being successfully distinguished. To improve the representation learning, usually, local features from human body parts are extracted. However, the common practice for such a process has been based on bounding box part detection. In this paper, we propose to adopt human semantic parsing which, due to its pixel-level accuracy and capability of modeling arbitrary contours, is naturally a better alternative. Our proposed SPReID integrates human semantic parsing in person re-identification and not only considerably outperforms its counter baseline, but achieves state-of-the-art performance. We also show that by employing a \textit{simple} yet effective training strategy, standard popular deep convolutional architectures such as Inception-V3 and ResNet-152, with no modification, while operating solely on full image, can dramatically outperform current state-of-the-art. Our proposed methods improve state-of-the-art person re-identification on: Market-1501 by ~17% in mAP and ~6% in rank-1, CUHK03 by ~4% in rank-1 and DukeMTMC-reID by ~24% in mAP and ~10% in rank-1.",0
"Identifying a person again after losing sight of them is difficult due to various factors like background clutter, pose, illumination, and camera perspective changes. These factors make it hard to extract reliable and distinct features, which makes it challenging to differentiate between different individuals. To address this issue, local features from human body parts are typically used, but this method commonly relies on detecting parts using a bounding box. In contrast, we propose using human semantic parsing, which provides more accurate pixel-level information and can model arbitrary contours. Our approach, called SPReID, integrates human semantic parsing with person re-identification and outperforms existing methods. We also demonstrate that even popular deep convolutional architectures like Inception-V3 and ResNet-152, without modification and on full images, can achieve remarkable results by using our simple yet effective training strategy. Our approach improves the state-of-the-art on Market-1501 by approximately 17% in mAP and 6% in rank-1, CUHK03 by about 4% in rank-1, and DukeMTMC-reID by roughly 24% in mAP and 10% in rank-1.",1
"Popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action---indeed, many are common across multiple actions---pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To this end, we learn a (nonlinear) hyperplane that separates this unknown, yet discriminative, feature from the rest. Applying multiple instance learning in a large-margin setup, we use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they serve as robust representations for pooling of the features. We formulate a joint objective and an efficient solver that learns these hyperplanes per video and the corresponding action classifiers over the hyperplanes. Our pooling scheme is end-to-end trainable within a deep framework. We report results from experiments on three benchmark datasets spanning a variety of challenges and demonstrate state-of-the-art performance across these tasks.",0
"Many popular deep models for recognizing actions in videos generate predictions for short clips, which are then combined to assign an action label to the entire video segment. However, not all frames may accurately depict the action, and pooling schemes that give equal importance to all frames may not be ideal. To address this issue, we propose a discriminative pooling method that identifies a feature among the deep features generated from short clips that characterizes the action. We achieve this by learning a nonlinear hyperplane that separates the discriminative feature from the rest, using multiple instance learning in a large-margin setup. The parameters of this hyperplane serve as a descriptor for the full video segment and provide robust representations for pooling of the features. We have formulated a joint objective and an efficient solver that can learn these hyperplanes per video and the corresponding action classifiers over the hyperplanes. Our method can be trained end-to-end within a deep framework. We have conducted experiments on three benchmark datasets and demonstrated state-of-the-art performance across a variety of challenges.",1
"It is of high interest for a company to identify customers expected to bring the largest profit in the upcoming period. Knowing as much as possible about each customer is crucial for such predictions. However, their demographic data, preferences, and other information that might be useful for building loyalty programs is often missing. Additionally, modeling relations among different customers as a network can be beneficial for predictions at an individual level, as similar customers tend to have similar purchasing patterns. We address this problem by proposing a robust framework for structured regression on deficient data in evolving networks with a supervised representation learning based on neural features embedding. The new method is compared to several unstructured and structured alternatives for predicting customer behavior (e.g. purchasing frequency and customer ticket) on user networks generated from customer databases of two companies from different industries. The obtained results show $4\%$ to $130\%$ improvement in accuracy over alternatives when all customer information is known. Additionally, the robustness of our method is demonstrated when up to $80\%$ of demographic information was missing where it was up to several folds more accurate as compared to alternatives that are either ignoring cases with missing values or learn their feature representation in an unsupervised manner.",0
"Identifying customers who are likely to generate the highest profit in the future is crucial for any company. However, obtaining comprehensive information about each customer is not always possible, making it challenging to predict their behavior accurately. In particular, demographic data and preferences, which are essential for building loyalty programs, are often missing. To address this issue, we propose a robust framework that uses structured regression on evolving networks with a supervised representation learning based on neural features embedding. By modeling relationships among customers as a network, our method can predict individual behavior more accurately, as similar customers tend to have similar purchasing patterns. We compared our approach to several unstructured and structured alternatives using data from two companies in different industries. Our results demonstrate that our method outperforms alternatives, with an improvement in accuracy of between 4% and 130% when all customer information is available. Furthermore, our method remains robust even when up to 80% of demographic information is missing, outperforming alternatives that either ignore cases with missing values or learn feature representations in an unsupervised manner.",1
"Disentangling factors of variation has become a very challenging problem on representation learning. Existing algorithms suffer from many limitations, such as unpredictable disentangling factors, poor quality of generated images from encodings, lack of identity information, etc. In this paper, we propose a supervised learning model called DNA-GAN which tries to disentangle different factors or attributes of images. The latent representations of images are DNA-like, in which each individual piece (of the encoding) represents an independent factor of the variation. By annihilating the recessive piece and swapping a certain piece of one latent representation with that of the other one, we obtain two different representations which could be decoded into two kinds of images with the existence of the corresponding attribute being changed. In order to obtain realistic images and also disentangled representations, we further introduce the discriminator for adversarial training. Experiments on Multi-PIE and CelebA datasets finally demonstrate that our proposed method is effective for factors disentangling and even overcome certain limitations of the existing methods.",0
"Representation learning faces a challenging problem in disentangling factors of variation, as current algorithms are limited by unpredictable disentangling factors, poor image quality from encodings, and lack of identity information. This paper introduces a supervised learning model, DNA-GAN, which aims to disentangle various attributes of images. The latent representations are DNA-like, with each piece representing an independent variation factor. By removing the recessive piece and swapping a certain piece between two representations, two different images with changed attributes can be decoded. To achieve realistic images and disentangled representations, the model uses a discriminator for adversarial training. Results from experiments on Multi-PIE and CelebA datasets demonstrate the effectiveness of DNA-GAN in disentangling factors and surpassing limitations of existing methods.",1
"We present a parameterized synthetic dataset called Moving Symbols to support the objective study of video prediction networks. Using several instantiations of the dataset in which variation is explicitly controlled, we highlight issues in an existing state-of-the-art approach and propose the use of a performance metric with greater semantic meaning to improve experimental interpretability. Our dataset provides canonical test cases that will help the community better understand, and eventually improve, the representations learned by such networks in the future. Code is available at https://github.com/rszeto/moving-symbols .",0
"Our article introduces the Moving Symbols dataset, a synthetic dataset with adjustable parameters that facilitates the investigation of video prediction networks. By creating diverse versions of the dataset with controlled variations, we identify flaws in a current cutting-edge method and recommend adopting a performance metric that offers more meaningful semantics to enhance experimental comprehension. Our dataset offers fundamental scenarios for testing, which will aid the research community in comprehending and enhancing the learned representations of such networks in the future. The code is accessible at https://github.com/rszeto/moving-symbols.",1
"Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .",0
"In recent years, ConvNets have revolutionized computer vision by learning high level semantic image features. However, this typically requires costly and impractical manual labeling of massive amounts of data. Therefore, unsupervised semantic feature learning is essential to effectively utilize the vast amount of visual data available. Our approach trains ConvNets to recognize the 2D rotation of input images, which provides a powerful supervisory signal for semantic feature learning. We extensively evaluate our method in unsupervised feature learning benchmarks and achieve state-of-the-art performance, dramatically improving upon prior approaches and closing the gap with supervised learning. Our unsupervised pre-trained AlexNet model achieves an mAP of 54.4% on the PASCAL VOC 2007 detection task, only 2.4 points lower than the supervised case. Our unsupervised learned features also perform exceptionally well on ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification tasks. The code and models of our paper will be published on https://github.com/gidariss/FeatureLearningRotNet.",1
"The meteoric rise of deep learning models in computer vision research, having achieved human-level accuracy in image recognition tasks is firm evidence of the impact of representation learning of deep neural networks. In the chemistry domain, recent advances have also led to the development of similar CNN models, such as Chemception, that is trained to predict chemical properties using images of molecular drawings. In this work, we investigate the effects of systematically removing and adding localized domain-specific information to the image channels of the training data. By augmenting images with only 3 additional basic information, and without introducing any architectural changes, we demonstrate that an augmented Chemception (AugChemception) outperforms the original model in the prediction of toxicity, activity, and solvation free energy. Then, by altering the information content in the images, and examining the resulting model's performance, we also identify two distinct learning patterns in predicting toxicity/activity as compared to solvation free energy. These patterns suggest that Chemception is learning about its tasks in the manner that is consistent with established knowledge. Thus, our work demonstrates that advanced chemical knowledge is not a pre-requisite for deep learning models to accurately predict complex chemical properties.",0
"The success of deep learning models in computer vision research, which has resulted in image recognition accuracy comparable to that of humans, is concrete proof of the impact of deep neural network representation learning. In the field of chemistry, similar CNN models have been developed, such as Chemception, which predicts chemical properties using molecular drawing images. In this study, we explore the effects of selectively removing and adding localized domain-specific information to the image channels of the training data. By augmenting the images with only three basic information without introducing any architectural changes, we demonstrate that the augmented model, AugChemception, outperforms the original model in predicting toxicity, activity, and solvation free energy. Furthermore, we identify two distinct learning patterns in predicting toxicity/activity compared to solvation free energy by altering the information content in the images and analyzing the resulting model's performance. These patterns suggest that Chemception is learning in a way that is consistent with established knowledge. Our findings demonstrate that advanced chemical knowledge is not necessary for deep learning models to accurately predict complex chemical properties.",1
"Despite a lack of theoretical understanding, deep neural networks have achieved unparalleled performance in a wide range of applications. On the other hand, shallow representation learning with component analysis is associated with rich intuition and theory, but smaller capacity often limits its usefulness. To bridge this gap, we introduce Deep Component Analysis (DeepCA), an expressive multilayer model formulation that enforces hierarchical structure through constraints on latent variables in each layer. For inference, we propose a differentiable optimization algorithm implemented using recurrent Alternating Direction Neural Networks (ADNNs) that enable parameter learning using standard backpropagation. By interpreting feed-forward networks as single-iteration approximations of inference in our model, we provide both a novel theoretical perspective for understanding them and a practical technique for constraining predictions with prior knowledge. Experimentally, we demonstrate performance improvements on a variety of tasks, including single-image depth prediction with sparse output constraints.",0
"Although deep neural networks lack a theoretical understanding, they have been incredibly successful in various applications. Conversely, shallow representation learning with component analysis has a rich understanding of theory and intuition, but its limited capacity restricts its usefulness. To bridge this gap, we introduce Deep Component Analysis (DeepCA), a multilayer model that enforces hierarchical structure through constraints on latent variables in each layer. We propose a differentiable optimization algorithm using Recurrent Alternating Direction Neural Networks (ADNNs) for parameter learning with standard backpropagation. By interpreting feed-forward networks as approximations of inference in our model, we provide both a theoretical perspective and a practical technique for constraining predictions with prior knowledge. Our experimental results demonstrate performance improvements in various tasks, such as single-image depth prediction with sparse output constraints.",1
"How can we effectively encode evolving information over dynamic graphs into low-dimensional representations? In this paper, we propose DyRep, an inductive deep representation learning framework that learns a set of functions to efficiently produce low-dimensional node embeddings that evolves over time. The learned embeddings drive the dynamics of two key processes namely, communication and association between nodes in dynamic graphs. These processes exhibit complex nonlinear dynamics that evolve at different time scales and subsequently contribute to the update of node embeddings. We employ a time-scale dependent multivariate point process model to capture these dynamics. We devise an efficient unsupervised learning procedure and demonstrate that our approach significantly outperforms representative baselines on two real-world datasets for the problem of dynamic link prediction and event time prediction.",0
"The paper discusses the challenge of encoding evolving information on dynamic graphs into low-dimensional representations. The proposed solution is DyRep, an inductive deep learning framework that learns functions to produce low-dimensional node embeddings that change over time. These embeddings drive the communication and association between nodes in dynamic graphs, which have complex nonlinear dynamics that evolve at varying time scales. A time-scale dependent multivariate point process model is used to capture these dynamics. An efficient unsupervised learning approach is employed, and the results on two real-world datasets show that DyRep outperforms existing baselines for dynamic link prediction and event time prediction.",1
"Rectified linear units, or ReLUs, have become the preferred activation function for artificial neural networks. In this paper we consider two basic learning problems assuming that the underlying data follow a generative model based on a ReLU-network -- a neural network with ReLU activations. As a primarily theoretical study, we limit ourselves to a single-layer network. The first problem we study corresponds to dictionary-learning in the presence of nonlinearity (modeled by the ReLU functions). Given a set of observation vectors $\mathbf{y}^i \in \mathbb{R}^d, i =1, 2, \dots , n$, we aim to recover $d\times k$ matrix $A$ and the latent vectors $\{\mathbf{c}^i\} \subset \mathbb{R}^k$ under the model $\mathbf{y}^i = \mathrm{ReLU}(A\mathbf{c}^i +\mathbf{b})$, where $\mathbf{b}\in \mathbb{R}^d$ is a random bias. We show that it is possible to recover the column space of $A$ within an error of $O(d)$ (in Frobenius norm) under certain conditions on the probability distribution of $\mathbf{b}$.   The second problem we consider is that of robust recovery of the signal in the presence of outliers, i.e., large but sparse noise. In this setting we are interested in recovering the latent vector $\mathbf{c}$ from its noisy nonlinear sketches of the form $\mathbf{v} = \mathrm{ReLU}(A\mathbf{c}) + \mathbf{e}+\mathbf{w}$, where $\mathbf{e} \in \mathbb{R}^d$ denotes the outliers with sparsity $s$ and $\mathbf{w} \in \mathbb{R}^d$ denote the dense but small noise. This line of work has recently been studied (Soltanolkotabi, 2017) without the presence of outliers. For this problem, we show that a generalized LASSO algorithm is able to recover the signal $\mathbf{c} \in \mathbb{R}^k$ within an $\ell_2$ error of $O(\sqrt{\frac{(k+s)\log d}{d}})$ when $A$ is a random Gaussian matrix.",0
"Artificial neural networks have come to rely on Rectified Linear Units (ReLUs) as their preferred activation function. In this paper, we explore two basic learning problems using a generative model based on a ReLU-network, which is a neural network with ReLU activations. Since our study is primarily theoretical, we limit ourselves to a single-layer network. The first problem we investigate is dictionary-learning in the presence of nonlinearity, modeled by ReLU functions. Our aim is to recover a matrix $A$ of size $d\times k$ and the latent vectors $\{\mathbf{c}^i\}\subset \mathbb{R}^k$ from a set of observation vectors $\mathbf{y}^i\in \mathbb{R}^d, i=1,2,\dots,n$. We assume that $\mathbf{y}^i = \mathrm{ReLU}(A\mathbf{c}^i+\mathbf{b})$ where $\mathbf{b}\in \mathbb{R}^d$ is a random bias. We show that the column space of $A$ can be recovered within an error of $O(d)$ (in Frobenius norm) under certain conditions on the probability distribution of $\mathbf{b}$. In the second problem, we focus on robust recovery of the signal in the presence of outliers, which are large but sparse noise. We aim to recover the latent vector $\mathbf{c}$ from its noisy nonlinear sketches of the form $\mathbf{v} = \mathrm{ReLU}(A\mathbf{c})+\mathbf{e}+\mathbf{w}$, where $\mathbf{e}\in \mathbb{R}^d$ denotes the outliers with sparsity $s$ and $\mathbf{w}\in \mathbb{R}^d$ denotes the dense but small noise. This problem has previously been studied without the presence of outliers. We show that a generalized LASSO algorithm can recover the signal $\mathbf{c}\in \mathbb{R}^k$ within an $\ell_2$ error of $O(\sqrt{\frac{(k+s)\log d}{d}})$ when $A$ is a random Gaussian matrix.",1
"Domain adaptation aims at generalizing a high-performance learner on a target domain via utilizing the knowledge distilled from a source domain which has a different but related data distribution. One solution to domain adaptation is to learn domain invariant feature representations while the learned representations should also be discriminative in prediction. To learn such representations, domain adaptation frameworks usually include a domain invariant representation learning approach to measure and reduce the domain discrepancy, as well as a discriminator for classification. Inspired by Wasserstein GAN, in this paper we propose a novel approach to learn domain invariant feature representations, namely Wasserstein Distance Guided Representation Learning (WDGRL). WDGRL utilizes a neural network, denoted by the domain critic, to estimate empirical Wasserstein distance between the source and target samples and optimizes the feature extractor network to minimize the estimated Wasserstein distance in an adversarial manner. The theoretical advantages of Wasserstein distance for domain adaptation lie in its gradient property and promising generalization bound. Empirical studies on common sentiment and image classification adaptation datasets demonstrate that our proposed WDGRL outperforms the state-of-the-art domain invariant representation learning approaches.",0
"The goal of domain adaptation is to extend the performance of a strong learner to a target domain by utilizing knowledge gained from a source domain with a different but related data distribution. One method for achieving domain adaptation involves learning feature representations that are both invariant and discriminative. To accomplish this, domain adaptation frameworks often use a domain invariant representation learning approach to identify and reduce domain discrepancies, as well as a discriminator for classification. This paper introduces a new approach for learning domain invariant feature representations, called Wasserstein Distance Guided Representation Learning (WDGRL), which is inspired by the Wasserstein GAN. WDGRL employs a neural network, referred to as the domain critic, to compute the empirical Wasserstein distance between source and target samples and optimizes the feature extractor network to minimize this estimated distance in an adversarial way. Theoretical benefits of using Wasserstein distance for domain adaptation include its gradient properties and potential for generalization. Empirical studies using common sentiment and image classification adaptation datasets demonstrate that our proposed WDGRL approach outperforms existing domain invariant representation learning methods.",1
"Classification of sequence data is the topic of interest for dynamic Bayesian models and Recurrent Neural Networks (RNNs). While the former can explicitly model the temporal dependencies between class variables, the latter have a capability of learning representations. Several attempts have been made to improve performance by combining these two approaches or increasing the processing capability of the hidden units in RNNs. This often results in complex models with a large number of learning parameters. In this paper, a compact model is proposed which offers both representation learning and temporal inference of class variables by rolling Restricted Boltzmann Machines (RBMs) and class variables over time. We address the key issue of intractability in this variant of RBMs by optimising a conditional distribution, instead of a joint distribution. Experiments reported in the paper on melody modelling and optical character recognition show that the proposed model can outperform the state-of-the-art. Also, the experimental results on optical character recognition, part-of-speech tagging and text chunking demonstrate that our model is comparable to recurrent neural networks with complex memory gates while requiring far fewer parameters.",0
"Dynamic Bayesian models and Recurrent Neural Networks (RNNs) are both interested in the classification of sequence data. The former focuses on modeling temporal dependencies between class variables, while the latter is capable of learning representations. Combining these two approaches or enhancing the processing ability of RNNs' hidden units has resulted in complex models with numerous learning parameters. This paper proposes a compact model that uses Rolling Restricted Boltzmann Machines (RBMs) and class variables to offer both representation learning and temporal inference of class variables. To address the issue of intractability, we optimize a conditional distribution instead of a joint distribution. Our experiments on melody modeling and optical character recognition demonstrate that our model outperforms the state-of-the-art. Furthermore, our model requires significantly fewer parameters than recurrent neural networks with complex memory gates, as shown in our experiments on optical character recognition, part-of-speech tagging, and text chunking.",1
"Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research.   A recent view argues that Resnets perform iterative refinement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative refinement in Resnets by showing that residual connections naturally encourage features of residual blocks to move along the negative gradient of loss as we go from one block to the next. In addition, our empirical analysis suggests that Resnets are able to perform both representation learning and iterative refinement. In general, a Resnet block tends to concentrate representation learning behavior in the first few layers while higher layers perform iterative refinement of features. Finally we observe that sharing residual layers naively leads to representation explosion and counterintuitively, overfitting, and we show that simple existing strategies can help alleviating this problem.",0
"Although Residual Networks (Resnets) have become a widely-used architecture in deep learning, there is still much research being conducted to gain a comprehensive understanding of them. Recently, it has been suggested that Resnets perform iterative refinement of features, and we aim to further explore this concept. Through analytical and empirical studies, we have formalized the notion of iterative refinement in Resnets and shown that residual connections naturally encourage features of residual blocks to move along the negative gradient of loss as we move from one block to the next. Our empirical analysis also indicates that Resnets are capable of performing both representation learning and iterative refinement. Generally, a Resnet block concentrates representation learning behavior in the first few layers, while higher layers perform iterative refinement of features. Moreover, we observed that sharing residual layers can lead to representation explosion and counterintuitive overfitting, but we have demonstrated that simple existing strategies can help alleviate this problem.",1
"Recent studies show that large-scale sketch-based image retrieval (SBIR) can be efficiently tackled by cross-modal binary representation learning methods, where Hamming distance matching significantly speeds up the process of similarity search. Providing training and test data subjected to a fixed set of pre-defined categories, the cutting-edge SBIR and cross-modal hashing works obtain acceptable retrieval performance. However, most of the existing methods fail when the categories of query sketches have never been seen during training. In this paper, the above problem is briefed as a novel but realistic zero-shot SBIR hashing task. We elaborate the challenges of this special task and accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An end-to-end three-network architecture is built, two of which are treated as the binary encoders. The third network mitigates the sketch-image heterogeneity and enhances the semantic relations among data by utilizing the Kronecker fusion layer and graph convolution, respectively. As an important part of ZSIH, we formulate a generative hashing scheme in reconstructing semantic knowledge representations for zero-shot retrieval. To the best of our knowledge, ZSIH is the first zero-shot hashing work suitable for SBIR and cross-modal search. Comprehensive experiments are conducted on two extended datasets, i.e., Sketchy and TU-Berlin with a novel zero-shot train-test split. The proposed model remarkably outperforms related works.",0
"New research indicates that the use of cross-modal binary representation learning methods is an effective approach to managing large-scale sketch-based image retrieval (SBIR), where Hamming distance matching can significantly expedite the process of similarity search. While cutting-edge SBIR and cross-modal hashing techniques are able to achieve acceptable retrieval performance by providing training and test data based on a predetermined set of categories, the majority of current methods struggle to handle situations where query sketches feature categories that were not seen during training. This paper introduces a novel yet realistic zero-shot SBIR hashing task that addresses this issue. We outline the unique challenges of this task and propose a zero-shot sketch-image hashing (ZSIH) model that utilizes an end-to-end three-network architecture, with two binary encoders and a third network that leverages the Kronecker fusion layer and graph convolution to mitigate sketch-image heterogeneity and enhance semantic relations across data. Our generative hashing scheme reconstructs semantic knowledge representations for zero-shot retrieval, making ZSIH the first zero-shot hashing method suitable for SBIR and cross-modal search. We conduct comprehensive experiments on two extended datasets (Sketchy and TU-Berlin) using a novel zero-shot train-test split, and our proposed model outperforms related works.",1
"In this paper we address the problem of learning robust cross-domain representations for sketch-based image retrieval (SBIR). While most SBIR approaches focus on extracting low- and mid-level descriptors for direct feature matching, recent works have shown the benefit of learning coupled feature representations to describe data from two related sources. However, cross-domain representation learning methods are typically cast into non-convex minimization problems that are difficult to optimize, leading to unsatisfactory performance. Inspired by self-paced learning, a learning methodology designed to overcome convergence issues related to local optima by exploiting the samples in a meaningful order (i.e. easy to hard), we introduce the cross-paced partial curriculum learning (CPPCL) framework. Compared with existing self-paced learning methods which only consider a single modality and cannot deal with prior knowledge, CPPCL is specifically designed to assess the learning pace by jointly handling data from dual sources and modality-specific prior information provided in the form of partial curricula. Additionally, thanks to the learned dictionaries, we demonstrate that the proposed CPPCL embeds robust coupled representations for SBIR. Our approach is extensively evaluated on four publicly available datasets (i.e. CUFS, Flickr15K, QueenMary SBIR and TU-Berlin Extension datasets), showing superior performance over competing SBIR methods.",0
"The aim of this paper is to address the issue of developing resilient cross-domain representations for sketch-based image retrieval (SBIR). While most SBIR approaches concentrate on creating low- and mid-level descriptors for direct feature matching, recent studies have shown that learning related feature representations can be advantageous in describing data from two different sources. However, cross-domain representation learning methods are typically complex non-convex minimization problems that are challenging to optimize and often result in unsatisfactory performance. In response to this, we introduce the cross-paced partial curriculum learning (CPPCL) framework, which draws inspiration from self-paced learning. CPPCL is designed to evaluate the learning pace by jointly handling data from dual sources and modality-specific prior information provided in the form of partial curricula. Our approach involves learned dictionaries and demonstrates that CPPCL embeds robust coupled representations for SBIR. We evaluate our approach extensively on four publicly available datasets, including CUFS, Flickr15K, QueenMary SBIR, and TU-Berlin Extension datasets. Our results show that CPPCL outperforms other existing SBIR methods.",1
"Predictive models that generalize well under distributional shift are often desirable and sometimes crucial to building robust and reliable machine learning applications. We focus on distributional shift that arises in causal inference from observational data and in unsupervised domain adaptation. We pose both of these problems as prediction under a shift in design. Popular methods for overcoming distributional shift make unrealistic assumptions such as having a well-specified model or knowing the policy that gave rise to the observed data. Other methods are hindered by their need for a pre-specified metric for comparing observations, or by poor asymptotic properties. We devise a bound on the generalization error under design shift, incorporating both representation learning and sample re-weighting. Based on the bound, we propose an algorithmic framework that does not require any of the above assumptions and which is asymptotically consistent. We empirically study the new framework using two synthetic datasets, and demonstrate its effectiveness compared to previous methods.",0
"Developing robust and reliable machine learning applications often requires predictive models that can generalize well even when there is a shift in distribution. Our focus is on two scenarios: causal inference from observational data and unsupervised domain adaptation, both of which involve a shift in design. Existing methods for dealing with distributional shift have limitations, such as unrealistic assumptions or poor asymptotic properties. To address these challenges, we propose a new framework that incorporates representation learning and sample re-weighting to bound the generalization error under design shift. Our approach does not require any of the aforementioned assumptions and is asymptotically consistent. We evaluate our framework on two synthetic datasets and demonstrate its effectiveness compared to previous methods.",1
"Statistical methods protecting sensitive information or the identity of the data owner have become critical to ensure privacy of individuals as well as of organizations. This paper investigates anonymization methods based on representation learning and deep neural networks, and motivated by novel information theoretical bounds. We introduce a novel training objective for simultaneously training a predictor over target variables of interest (the regular labels) while preventing an intermediate representation to be predictive of the private labels. The architecture is based on three sub-networks: one going from input to representation, one from representation to predicted regular labels, and one from representation to predicted private labels. The training procedure aims at learning representations that preserve the relevant part of the information (about regular labels) while dismissing information about the private labels which correspond to the identity of a person. We demonstrate the success of this approach for two distinct classification versus anonymization tasks (handwritten digits and sentiment analysis).",0
"To protect the privacy of individuals and organizations, statistical methods are necessary. This paper explores methods that use representation learning and deep neural networks for anonymization, guided by novel information theoretical bounds. A new training objective is introduced that trains a predictor for target variables while preventing the intermediate representation from predicting private labels. The architecture comprises three sub-networks: input to representation, representation to predicted regular labels, and representation to predicted private labels. The training aims to preserve relevant information about regular labels while disregarding private label information that identifies a person. The success of this approach is demonstrated in two classification versus anonymization tasks: handwritten digits and sentiment analysis.",1
"Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning. However, autonomously learning effective sets of options is still a major challenge in the field. In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process. Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment. We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available. We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels. It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation. We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.",0
"Reinforcement learning offers agents the ability to break down a task into smaller subtasks through options, potentially increasing the speed of learning and planning. However, creating effective sets of options without human intervention remains a significant challenge in the field. This paper focuses on using representation learning techniques to aid in the option discovery process, specifically exploring eigenoptions, which are options derived from representations that encode information flow in the environment. The study extends existing algorithms for eigenoption discovery to stochastic transitions and situations where handcrafted features are not available. An algorithm is proposed that discovers eigenoptions while learning non-linear state representations from raw pixels, taking advantage of recent advances in deep reinforcement learning and the connection between proto-value functions and successor representations. The effectiveness of the approach is demonstrated using traditional tabular domains for intuitive purposes and Atari 2600 games to showcase its potential.",1
"Advances in unsupervised learning enable reconstruction and generation of samples from complex distributions, but this success is marred by the inscrutability of the representations learned. We propose an information-theoretic approach to characterizing disentanglement and dependence in representation learning using multivariate mutual information, also called total correlation. The principle of total Cor-relation Ex-planation (CorEx) has motivated successful unsupervised learning applications across a variety of domains, but under some restrictive assumptions. Here we relax those restrictions by introducing a flexible variational lower bound to CorEx. Surprisingly, we find that this lower bound is equivalent to the one in variational autoencoders (VAE) under certain conditions. This information-theoretic view of VAE deepens our understanding of hierarchical VAE and motivates a new algorithm, AnchorVAE, that makes latent codes more interpretable through information maximization and enables generation of richer and more realistic samples.",0
"Unsupervised learning advancements have made it possible to reconstruct and generate samples from complicated distributions. However, the success of this technique is overshadowed by the obscurity of the representations learned. We suggest an information-theoretic approach to assess disentanglement and dependence in representation learning using multivariate mutual information, which is also known as total correlation. The concept of total Cor-relation Ex-planation (CorEx) has been successful in unsupervised learning applications across various domains, but it has some restrictive assumptions. In this study, we introduce a flexible variational lower bound to CorEx to overcome these restrictions. Surprisingly, we found that this lower bound is equivalent to that of variational autoencoders (VAE) under certain conditions. This information-theoretic perspective on VAE enhances our comprehension of hierarchical VAE and inspires a new algorithm, AnchorVAE, which improves the interpretability of latent codes through information maximization and allows for the generation of richer and more realistic samples.",1
"A grand challenge in representation learning is to learn the different explanatory factors of variation behind the high dimen- sional data. Encoder models are often determined to optimize performance on training data when the real objective is to generalize well to unseen data. Although there is enough numerical evidence suggesting that noise injection (during training) at the representation level might improve the generalization ability of encoders, an information-theoretic understanding of this principle remains elusive. This paper presents a sample-dependent bound on the generalization gap of the cross-entropy loss that scales with the information complexity (IC) of the representations, meaning the mutual information between inputs and their representations. The IC is empirically investigated for standard multi-layer neural networks with SGD on MNIST and CIFAR-10 datasets; the behaviour of the gap and the IC appear to be in direct correlation, suggesting that SGD selects encoders to implicitly minimize the IC. We specialize the IC to study the role of Dropout on the generalization capacity of deep encoders which is shown to be directly related to the encoder capacity, being a measure of the distinguishability among samples from their representations. Our results support some recent regularization methods.",0
"Learning the various explanatory factors of variation behind high-dimensional data is a significant challenge in representation learning. Although encoder models are typically optimized for performance on training data, the ultimate goal is to generalize well to unseen data. While injecting noise at the representation level during training has been shown to enhance generalization ability, the underlying information-theoretic principles remain unclear. This study introduces a sample-dependent bound on the generalization gap of the cross-entropy loss, which scales with the information complexity (IC) of the representations. Empirical investigation of the IC for standard neural networks trained on MNIST and CIFAR-10 datasets reveals a direct correlation between the IC and the generalization gap, suggesting that SGD selects encoders that implicitly minimize the IC. The study also examines the role of Dropout in deep encoders and finds that it is directly related to encoder capacity, which measures the distinguishability among samples from their representations. These findings lend support to recent regularization methods.",1
"Recent work in unsupervised representation learning has focused on learning deep directed latent-variable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.",0
"Unsupervised representation learning has recently focused on deep directed latent-variable models. Maximizing the marginal likelihood or evidence to fit these models is usually impossible, so maximizing the evidence lower bound (ELBO) is a common approximation. However, we have shown both theoretically and empirically that maximum likelihood training, whether exact or approximate, does not always result in a good latent representation. We have derived lower and upper bounds on the mutual information between the input and the latent variable, and we have used these bounds to create a tradeoff between compression and reconstruction accuracy. Our framework has demonstrated that there are models with identical ELBO but different characteristics. We have also proposed a simple method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.",1
"Recent advances in weakly supervised classification allow us to train a classifier only from positive and unlabeled (PU) data. However, existing PU classification methods typically require an accurate estimate of the class-prior probability, which is a critical bottleneck particularly for high-dimensional data. This problem has been commonly addressed by applying principal component analysis in advance, but such unsupervised dimension reduction can collapse underlying class structure. In this paper, we propose a novel representation learning method from PU data based on the information-maximization principle. Our method does not require class-prior estimation and thus can be used as a preprocessing method for PU classification. Through experiments, we demonstrate that our method combined with deep neural networks highly improves the accuracy of PU class-prior estimation, leading to state-of-the-art PU classification performance.",0
"Advancements in weakly supervised classification now allow classifiers to be trained solely from positive and unlabeled data. However, current PU classification techniques often face difficulties in accurately estimating the class-prior probability, especially for high-dimensional data. Previous solutions have implemented principal component analysis, but this unsupervised dimension reduction may cause the underlying class structure to collapse. To combat this issue, we present a novel method for representation learning from PU data based on the information-maximization principle. Our approach does not require class-prior estimation and can be used as a preprocessing method for PU classification. Through experimentation, we show that our method, when combined with deep neural networks, significantly improves the accuracy of PU class-prior estimation, leading to state-of-the-art PU classification performance.",1
"We study the role of latent space dimensionality in Wasserstein auto-encoders (WAEs). Through experimentation on synthetic and real datasets, we argue that random encoders should be preferred over deterministic encoders. We highlight the potential of WAEs for representation learning with promising results on a benchmark disentanglement task.",0
"Our focus is on investigating the significance of latent space dimensionality in Wasserstein auto-encoders (WAEs). Our research involves conducting experiments on both real and synthetic datasets, with the conclusion that random encoders are more favorable than deterministic ones. Additionally, we emphasize the potential of WAEs for representation learning, as evidenced by encouraging outcomes on a disentanglement task benchmark.",1
"This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. We believe that high model interpretability may help people to break several bottlenecks of deep learning, e.g., learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.",0
"This article examines recent research on comprehending neural-network representations and training neural networks with interpretable/disentangled middle-layer representations. While deep neural networks have demonstrated excellent performance in various tasks, their interpretability remains a significant challenge. Currently, deep neural networks achieve high discrimination ability at the expense of low interpretability of their black-box representations. We believe that enhancing model interpretability can help overcome several deep learning obstacles, such as learning from minimal annotations, learning through human-computer communication at the semantic level, and semantically debugging network representations. The focus is on convolutional neural networks (CNNs), specifically, the visualization of CNN representations, methods for diagnosing pre-trained CNN representations, approaches for disentangling pre-trained CNN representations, training CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss future trends in explainable artificial intelligence.",1
"In this paper, we explore methods of complicating self-supervised tasks for representation learning. That is, we do severe damage to data and encourage a network to recover them. First, we complicate each of three powerful self-supervised task candidates: jigsaw puzzle, inpainting, and colorization. In addition, we introduce a novel complicated self-supervised task called ""Completing damaged jigsaw puzzles"" which is puzzles with one piece missing and the other pieces without color. We train a convolutional neural network not only to solve the puzzles, but also generate the missing content and colorize the puzzles. The recovery of the aforementioned damage pushes the network to obtain robust and general-purpose representations. We demonstrate that complicating the self-supervised tasks improves their original versions and that our final task learns more robust and transferable representations compared to the previous methods, as well as the simple combination of our candidate tasks. Our approach achieves state-of-the-art performance in transfer learning on PASCAL classification and semantic segmentation.",0
"This article delves into ways of making self-supervised tasks more intricate for representation learning. The approach involves inflicting significant damage to data and compelling a network to recover them. Initially, we complicate three powerful self-supervised task options: jigsaw puzzle, inpainting, and colorization. Furthermore, we introduce a novel complex self-supervised task entitled ""Completing damaged jigsaw puzzles,"" which involves puzzles with one missing piece and the remaining pieces without color. We train a convolutional neural network to solve the puzzles, generate the missing content, and colorize the puzzles. The recovery of the damage mentioned above coerces the network to obtain robust and versatile representations. We illustrate that complicating the self-supervised tasks enhances their original versions and that our final task learns more resilient and transferable representations than previous methods and the simple combination of our candidate tasks. Our method attains state-of-the-art performance in transfer learning on PASCAL classification and semantic segmentation.",1
"Basing on the analysis by revealing the equivalence of modern networks, we find that both ResNet and DenseNet are essentially derived from the same ""dense topology"", yet they only differ in the form of connection -- addition (dubbed ""inner link"") vs. concatenation (dubbed ""outer link""). However, both two forms of connections have the superiority and insufficiency. To combine their advantages and avoid certain limitations on representation learning, we present a highly efficient and modularized Mixed Link Network (MixNet) which is equipped with flexible inner link and outer link modules. Consequently, ResNet, DenseNet and Dual Path Network (DPN) can be regarded as a special case of MixNet, respectively. Furthermore, we demonstrate that MixNets can achieve superior efficiency in parameter over the state-of-the-art architectures on many competitive datasets like CIFAR-10/100, SVHN and ImageNet.",0
"Through analysis of modern networks, it was discovered that ResNet and DenseNet both originated from the same ""dense topology,"" with their only difference being in their type of connection: addition (referred to as ""inner link"") versus concatenation (referred to as ""outer link""). However, each type of connection has its advantages and limitations. To overcome these limitations and combine the strengths of both types of connections, a highly efficient and modularized Mixed Link Network (MixNet) was developed, equipped with flexible inner and outer link modules. This makes ResNet, DenseNet, and Dual Path Network (DPN) special cases of MixNet. MixNets have been proven to achieve superior parameter efficiency compared to other state-of-the-art architectures on various competitive datasets such as CIFAR-10/100, SVHN, and ImageNet.",1
"Recently the deep learning techniques have achieved success in multi-label classification due to its automatic representation learning ability and the end-to-end learning framework. Existing deep neural networks in multi-label classification can be divided into two kinds: binary relevance neural network (BRNN) and threshold dependent neural network (TDNN). However, the former needs to train a set of isolate binary networks which ignore dependencies between labels and have heavy computational load, while the latter needs an additional threshold function mechanism to transform the multi-class probabilities to multi-label outputs. In this paper, we propose a joint binary neural network (JBNN), to address these shortcomings. In JBNN, the representation of the text is fed to a set of logistic functions instead of a softmax function, and the multiple binary classifications are carried out synchronously in one neural network framework. Moreover, the relations between labels are captured via training on a joint binary cross entropy (JBCE) loss. To better meet multi-label emotion classification, we further proposed to incorporate the prior label relations into the JBCE loss. The experimental results on the benchmark dataset show that our model performs significantly better than the state-of-the-art multi-label emotion classification methods, in both classification performance and computational efficiency.",0
"The ability of deep learning techniques to learn representations automatically and their end-to-end learning framework has led to their success in multi-label classification. Deep neural networks used in this field can be classified into two types: binary relevance neural network (BRNN) and threshold dependent neural network (TDNN). However, BRNN requires the training of isolated binary networks that do not account for label dependencies and are computationally intensive, while TDNN needs a threshold function mechanism to convert multi-class probabilities into multi-label outputs. This paper proposes a joint binary neural network (JBNN) that addresses these limitations by utilizing logistic functions for text representation and conducting multiple binary classifications synchronously in one neural network framework. JBNN captures label relationships through training on a joint binary cross entropy (JBCE) loss. To improve multi-label emotion classification, the paper further suggests incorporating prior label relations into the JBCE loss. Experimental results on a benchmark dataset demonstrate that our model outperforms state-of-the-art multi-label emotion classification methods in both classification performance and computational efficiency.",1
"Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.",0
"Convolutional neural networks (CNNs) have been established on the basis of convolution as inner product, which is essential for end-to-end visual representation learning. While recent CNNs have shown stronger representation abilities owing to deeper architectures, the bigger parameter space and increased depth have posed challenges in network training. As a solution to these issues, we present SphereConv, a novel learning framework that offers angular representations on hyperspheres. SphereNet, a deep hyperspherical convolution network, is proposed, which differs from conventional inner product based convolutional networks by adopting SphereConv as its basic convolution operator. The network is supervised by generalized angular softmax loss, a natural loss formulation under SphereConv. We demonstrate that SphereNet effectively encodes discriminative representation and mitigates training difficulty, leading to easier optimization, faster convergence, and comparable (or even better) classification accuracy compared to convolutional counterparts. Additionally, we introduce the learnable SphereConv, an enhancement over prefixed SphereConv, and SphereNorm, which is hyperspherical learning as a normalization method. Our conclusions are supported by experimental results, and we also provide theoretical insights into the advantages of learning on hyperspheres.",1
"Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning. In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data. The representation learning process is formulated as an adversarial minimax game. We analyze the optimal equilibrium of such a game and find that it amounts to maximizing the uncertainty of inferring the detrimental factor given the representation while maximizing the certainty of making task-specific predictions. On three benchmark tasks, namely fair and bias-free classification, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved performance.",0
"The issue of machine learning involves acquiring valuable representations that preserve the necessary content for a given task while eliminating harmful variations. Our paper addresses the challenge of obtaining invariant representations that filter out a specific trait or factor of the data. We use an adversarial minimax game to facilitate the representation learning process. By examining the optimal equilibrium of the game, we discover that the most effective strategy involves maximizing the uncertainty of identifying the harmful factor based on the representation while simultaneously maximizing the accuracy of task-specific predictions. Our proposed framework produces an invariant representation on three benchmark tasks, including unbiased classification, language-independent generation, and lighting-independent image classification. The results demonstrate that our approach leads to improved performance due to better generalization.",1
"Time series data constitutes a distinct and growing problem in machine learning. As the corpus of time series data grows larger, deep models that simultaneously learn features and classify with these features can be intractable or suboptimal. In this paper, we present feature learning via long short term memory (LSTM) networks and prediction via gradient boosting trees (XGB). Focusing on the consequential setting of electronic health record data, we predict the occurrence of hypoxemia five minutes into the future based on past features. We make two observations: 1) long short term memory networks are effective at capturing long term dependencies based on a single feature and 2) gradient boosting trees are capable of tractably combining a large number of features including static features like height and weight. With these observations in mind, we generate features by performing ""supervised"" representation learning with LSTM networks. Augmenting the original XGB model with these features gives significantly better performance than either individual method.",0
"The problem of time series data is becoming increasingly significant in machine learning. As the amount of time series data increases, it can be challenging or suboptimal for deep models to learn features and classify using them at the same time. This article proposes a solution by introducing feature learning through long short term memory (LSTM) networks and prediction through gradient boosting trees (XGB). The study focuses on predicting hypoxemia in electronic health record data five minutes in advance using past features. The article makes two observations: 1) LSTM networks are useful for capturing long-term dependencies based on a single feature, and 2) XGB can efficiently combine a large number of features, including static features such as height and weight. Based on these observations, the article generates features by utilizing supervised representation learning with LSTM networks. The results show that augmenting the original XGB model with these features leads to significant performance improvement compared to using either method individually.",1
"Automatic emotion recognition has become a trending research topic in the past decade. While works based on facial expressions or speech abound, recognizing affect from body gestures remains a less explored topic. We present a new comprehensive survey hoping to boost research in the field. We first introduce emotional body gestures as a component of what is commonly known as ""body language"" and comment general aspects as gender differences and culture dependence. We then define a complete framework for automatic emotional body gesture recognition. We introduce person detection and comment static and dynamic body pose estimation methods both in RGB and 3D. We then comment the recent literature related to representation learning and emotion recognition from images of emotionally expressive gestures. We also discuss multi-modal approaches that combine speech or face with body gestures for improved emotion recognition. While pre-processing methodologies (e.g. human detection and pose estimation) are nowadays mature technologies fully developed for robust large scale analysis, we show that for emotion recognition the quantity of labelled data is scarce, there is no agreement on clearly defined output spaces and the representations are shallow and largely based on naive geometrical representations.",0
"Over the past decade, there has been a surge in interest in automatic emotion recognition. While existing research has mainly focused on facial expressions or speech, there has been less exploration into recognizing emotions through body gestures. To address this gap, we present a comprehensive survey aimed at advancing research in this field. The survey starts by introducing emotional body gestures as a key component of body language, highlighting general aspects such as gender differences and cultural dependence. Next, we establish a complete framework for automatic emotional body gesture recognition, which includes person detection and static and dynamic body pose estimation methods in both RGB and 3D. Additionally, we discuss the latest literature on representation learning and emotion recognition from images of emotionally expressive gestures, as well as multi-modal approaches that combine speech or face with body gestures to improve emotion recognition. Despite mature pre-processing methodologies (e.g., human detection and pose estimation) for robust large scale analysis, we show that there is a scarcity of labeled data, no consensus on clearly defined output spaces, and shallow representations largely based on naive geometrical representations in the context of emotion recognition.",1
"Deep Neural Networks (DNNs) often struggle with one-shot learning where we have only one or a few labeled training examples per category. In this paper, we argue that by using side information, we may compensate the missing information across classes. We introduce two statistical approaches for fusing side information into data representation learning to improve one-shot learning. First, we propose to enforce the statistical dependency between data representations and multiple types of side information. Second, we introduce an attention mechanism to efficiently treat examples belonging to the 'lots-of-examples' classes as quasi-samples (additional training samples) for 'one-example' classes. We empirically show that our learning architecture improves over traditional softmax regression networks as well as state-of-the-art attentional regression networks on one-shot recognition tasks.",0
"When we only have one or a few labeled training examples per category, Deep Neural Networks (DNNs) may have difficulty with one-shot learning. However, we propose that if we utilize side information, we can compensate for the missing information across classes. This paper presents two statistical approaches for integrating side information into data representation learning, which can enhance one-shot learning. Firstly, we suggest enforcing statistical dependency between data representations and various types of side information. Secondly, we introduce an attention mechanism that can treat examples from 'lots-of-examples' classes as quasi-samples (additional training samples) for 'one-example' classes effectively. Through empirical evidence, we demonstrate that our learning architecture surpasses both traditional softmax regression networks and state-of-the-art attentional regression networks in one-shot recognition tasks.",1
"Recent works have shown that exploiting multi-scale representations deeply learned via convolutional neural networks (CNN) is of tremendous importance for accurate contour detection. This paper presents a novel approach for predicting contours which advances the state of the art in two fundamental aspects, i.e. multi-scale feature generation and fusion. Different from previous works directly consider- ing multi-scale feature maps obtained from the inner layers of a primary CNN architecture, we introduce a hierarchical deep model which produces more rich and complementary representations. Furthermore, to refine and robustly fuse the representations learned at different scales, the novel Attention-Gated Conditional Random Fields (AG-CRFs) are proposed. The experiments ran on two publicly available datasets (BSDS500 and NYUDv2) demonstrate the effectiveness of the latent AG-CRF model and of the overall hierarchical framework.",0
"Recent research has highlighted the importance of utilizing multi-scale representations gained through the use of convolutional neural networks (CNN) for precise contour detection. This article introduces a new method for predicting contours that improves upon existing techniques in two key areas: multi-scale feature generation and fusion. Unlike previous approaches that solely consider multi-scale feature maps from the inner layers of the primary CNN architecture, our approach employs a hierarchical deep model that generates more comprehensive and complementary representations. Additionally, we propose Attention-Gated Conditional Random Fields (AG-CRFs) to refine and robustly combine the representations learned at different scales. Experiments conducted on two publicly available datasets (BSDS500 and NYUDv2) demonstrate the efficacy of the novel AG-CRF model and the overall hierarchical framework.",1
"Fabric image retrieval is beneficial to many applications including clothing searching, online shopping and cloth modeling. Learning pairwise image similarity is of great importance to an image retrieval task. With the resurgence of Convolutional Neural Networks (CNNs), recent works have achieved significant progresses via deep representation learning with metric embedding, which drives similar examples close to each other in a feature space, and dissimilar ones apart from each other. In this paper, we propose a novel embedding method termed focus ranking that can be easily unified into a CNN for jointly learning image representations and metrics in the context of fine-grained fabric image retrieval. Focus ranking aims to rank similar examples higher than all dissimilar ones by penalizing ranking disorders via the minimization of the overall cost attributed to similar samples being ranked below dissimilar ones. At the training stage, training samples are organized into focus ranking units for efficient optimization. We build a large-scale fabric image retrieval dataset (FIRD) with about 25,000 images of 4,300 fabrics, and test the proposed model on the FIRD dataset. Experimental results show the superiority of the proposed model over existing metric embedding models.",0
"The retrieval of fabric images has various applications, such as searching for clothing, online shopping, and cloth modeling. For effective image retrieval, it is crucial to learn the similarity between pairs of images. Recent studies have utilized Convolutional Neural Networks (CNNs) to achieve significant progress in deep representation learning with metric embedding. This method places similar images close to each other and dissimilar images far apart in a feature space. This study proposes a new embedding technique called focus ranking, which can be easily integrated into a CNN for learning image representations and metrics in the context of fine-grained fabric image retrieval. Focus ranking aims to improve ranking accuracy by penalizing ranking disorders and maximizing the ranking of similar samples over dissimilar ones. During the training stage, the samples are sorted into focus ranking units for efficient optimization. The study also introduces a large-scale fabric image retrieval dataset (FIRD) containing 25,000 images of 4,300 fabrics, and evaluates the proposed model's performance on the FIRD dataset. The experimental results demonstrate that the focus ranking method outperforms other metric embedding models.",1
"Physical activity and sleep play a major role in the prevention and management of many chronic conditions. It is not a trivial task to understand their impact on chronic conditions. Currently, data from electronic health records (EHRs), sleep lab studies, and activity/sleep logs are used. The rapid increase in the popularity of wearable health devices provides a significant new data source, making it possible to track the user's lifestyle real-time through web interfaces, both to consumer as well as their healthcare provider, potentially. However, at present there is a gap between lifestyle data (e.g., sleep, physical activity) and clinical outcomes normally captured in EHRs. This is a critical barrier for the use of this new source of signal for healthcare decision making. Applying deep learning to wearables data provides a new opportunity to overcome this barrier.   To address the problem of the unavailability of clinical data from a major fraction of subjects and unrepresentative subject populations, we propose a novel unsupervised (task-agnostic) time-series representation learning technique called act2vec. act2vec learns useful features by taking into account the co-occurrence of activity levels along with periodicity of human activity patterns. The learned representations are then exploited to boost the performance of disorder-specific supervised learning models. Furthermore, since many disorders are often related to each other, a phenomenon referred to as co-morbidity, we use a multi-task learning framework for exploiting the shared structure of disorder inducing life-style choices partially captured in the wearables data. Empirical evaluation using actigraphy data from 4,124 subjects shows that our proposed method performs and generalizes substantially better than the conventional time-series symbolic representational methods and task-specific deep learning models.",0
"The prevention and management of chronic conditions can be significantly aided by physical activity and sleep, but understanding their impact is not an easy task. Electronic health records, sleep lab studies, and activity/sleep logs are currently used to gather data, but wearable health devices are becoming increasingly popular and provide a new source of real-time lifestyle data. However, there is currently a gap between this data and clinical outcomes captured in EHRs. To overcome this barrier, deep learning can be applied to wearable data. To address the problem of unavailability and unrepresentative subject populations, a new unsupervised time-series representation learning technique called act2vec is proposed. This technique takes into account the co-occurrence of activity levels and periodicity of human activity patterns to learn useful features that can be used to boost the performance of disorder-specific supervised learning models. A multi-task learning framework is also used to exploit the shared structure of disorder-inducing lifestyle choices. Empirical evaluation using actigraphy data from 4,124 subjects shows that our proposed method performs substantially better than conventional time-series symbolic representational methods and task-specific deep learning models.",1
"Training deep neural networks is known to require a large number of training samples. However, in many applications only few training samples are available. In this work, we tackle the issue of training neural networks for classification task when few training samples are available. We attempt to solve this issue by proposing a new regularization term that constrains the hidden layers of a network to learn class-wise invariant representations. In our regularization framework, learning invariant representations is generalized to the class membership where samples with the same class should have the same representation. Numerical experiments over MNIST and its variants showed that our proposal helps improving the generalization of neural network particularly when trained with few samples. We provide the source code of our framework https://github.com/sbelharbi/learning-class-invariant-features .",0
"Deep neural networks require a large number of training samples, but in some cases, only a few are available. This study addresses the challenge of training neural networks for classification tasks with limited training samples. To address this issue, we propose a new regularization term that restricts the hidden layers of a network to learn invariant representations for each class. Our regularization framework aims to generalize the concept of learning invariant representations to class membership, where samples belonging to the same class should have the same representation. Our numerical experiments on MNIST and its variants demonstrate that our proposal improves the generalization of neural networks, especially when trained with limited samples. We provide the source code for our framework at https://github.com/sbelharbi/learning-class-invariant-features.",1
"Recently there has been a dramatic increase in the performance of recognition systems due to the introduction of deep architectures for representation learning and classification. However, the mathematical reasons for this success remain elusive. This tutorial will review recent work that aims to provide a mathematical justification for several properties of deep networks, such as global optimality, geometric stability, and invariance of the learned representations.",0
"The performance of recognition systems has significantly improved with the implementation of deep architectures for representation learning and classification. Nonetheless, the underlying mathematical reasons for this progress are yet to be fully comprehended. This tutorial will examine recent research efforts that seek to establish a mathematical basis for various features of deep networks, including global optimality, geometric stability, and the invariance of the learned representations.",1
"The unsupervised Pretraining method has been widely used in aiding human action recognition. However, existing methods focus on reconstructing the already present frames rather than generating frames which happen in future.In this paper, We propose an improved Variantial Autoencoder model to extract the features with a high connection to the coming scenarios, also known as Predictive Learning. Our framework lists as following: two steam 3D-convolution neural networks are used to extract both spatial and temporal information as latent variables. Then a resample method is introduced to create new normal distribution probabilistic latent variables and finally, the deconvolution neural network will use these latent variables generate next frames. Through this possess, we train the model to focus more on how to generate the future and thus it will extract the future high connected features. In the experiment stage, A large number of experiments on UT and UCF101 datasets reveal that future generation aids Prediction does improve the performance. Moreover, the Future Representation Learning Network reach a higher score than other methods when in half observation. This means that Future Representation Learning is better than the traditional Representation Learning and other state- of-the-art methods in solving the human action prediction problems to some extends.",0
"The Pretraining method without supervision has been extensively utilized to assist in human action recognition. However, the current approaches concentrate on reconstructing the frames that are already present instead of creating frames that will occur in the future. This article presents an enhanced Variantial Autoencoder model that extracts features that are highly connected to the forthcoming scenarios, which is known as Predictive Learning. Our framework consists of two 3D-convolution neural networks that extract both spatial and temporal information as latent variables. Then, a resampling method is introduced to generate new normal distribution probabilistic latent variables. Finally, the deconvolution neural network utilizes these latent variables to generate subsequent frames. This process trains the model to concentrate more on how to generate the future, resulting in the extraction of future high connected features. The results of numerous experiments on UT and UCF101 datasets indicate that future generation enhances prediction performance. Furthermore, the Future Representation Learning Network outperforms other methods when half of the observations are present. This implies that Future Representation Learning is superior to conventional Representation Learning and other state-of-the-art methods in addressing human action prediction issues to some degree.",1
"This paper introduces a probabilistic framework for k-shot image classification. The goal is to generalise from an initial large-scale classification task to a separate task comprising new classes and small numbers of examples. The new approach not only leverages the feature-based representation learned by a neural network from the initial task (representational transfer), but also information about the classes (concept transfer). The concept information is encapsulated in a probabilistic model for the final layer weights of the neural network which acts as a prior for probabilistic k-shot learning. We show that even a simple probabilistic model achieves state-of-the-art on a standard k-shot learning dataset by a large margin. Moreover, it is able to accurately model uncertainty, leading to well calibrated classifiers, and is easily extensible and flexible, unlike many recent approaches to k-shot learning.",0
"A probabilistic framework for k-shot image classification is presented in this paper. The framework aims to extend the large-scale classification task to a new task with smaller numbers of examples and different classes. It utilizes both the feature-based representation obtained from the neural network in the initial task (representational transfer) and the information about the classes (concept transfer). The concept information is included in a probabilistic model for the final layer weights of the neural network, which acts as a prior for probabilistic k-shot learning. The results show that even a simple probabilistic model outperforms the state-of-the-art on a standard k-shot learning dataset by a significant margin. Additionally, it can model uncertainty accurately, leading to well-calibrated classifiers. This approach is highly extensible and flexible compared to recent k-shot learning approaches.",1
"Audio events are quite often overlapping in nature, and more prone to noise than visual signals. There has been increasing evidence for the superior performance of representations learned using sparse dictionaries for applications like audio denoising and speech enhancement. This paper concentrates on modifying the traditional reconstructive dictionary learning algorithms, by incorporating a discriminative term into the objective function in order to learn class-specific adversarial dictionaries that are good at representing samples of their own class at the same time poor at representing samples belonging to any other class. We quantitatively demonstrate the effectiveness of our learned dictionaries as a stand-alone solution for both binary as well as multi-class audio classification problems.",0
"The nature of audio events often involves overlapping and noise, unlike visual signals. Sparse dictionaries have been proven to be more effective in applications such as speech enhancement and audio denoising. This study focuses on modifying traditional reconstructive dictionary learning algorithms by including a discriminative term in the objective function. This allows for the creation of class-specific adversarial dictionaries that represent their own class well but poorly represent other classes. Our learned dictionaries are demonstrated to be effective in both binary and multi-class audio classification problems as a standalone solution.",1
"Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Unsupervised cross-modal hashing is more flexible and applicable than supervised methods, since no intensive labeling work is involved. However, existing unsupervised methods learn hashing functions by preserving inter and intra correlations, while ignoring the underlying manifold structure across different modalities, which is extremely helpful to capture meaningful nearest neighbors of different modalities for cross-modal retrieval. To address the above problem, in this paper we propose an Unsupervised Generative Adversarial Cross-modal Hashing approach (UGACH), which makes full use of GAN's ability for unsupervised representation learning to exploit the underlying manifold structure of cross-modal data. The main contributions can be summarized as follows: (1) We propose a generative adversarial network to model cross-modal hashing in an unsupervised fashion. In the proposed UGACH, given a data of one modality, the generative model tries to fit the distribution over the manifold structure, and select informative data of another modality to challenge the discriminative model. The discriminative model learns to distinguish the generated data and the true positive data sampled from correlation graph to achieve better retrieval accuracy. These two models are trained in an adversarial way to improve each other and promote hashing function learning. (2) We propose a correlation graph based approach to capture the underlying manifold structure across different modalities, so that data of different modalities but within the same manifold can have smaller Hamming distance and promote retrieval accuracy. Extensive experiments compared with 6 state-of-the-art methods verify the effectiveness of our proposed approach.",0
"The goal of cross-modal hashing is to map multimedia data of different types into a single Hamming space for efficient retrieval. Unsupervised methods are more practical than supervised ones because they do not require extensive labeling work. However, existing unsupervised methods overlook the underlying manifold structure of the data, which is crucial for finding meaningful nearest neighbors across modalities. To address this issue, we present an Unsupervised Generative Adversarial Cross-modal Hashing (UGACH) approach that leverages the unsupervised learning capabilities of GANs to capture the manifold structure of cross-modal data. Our approach includes a generative model that fits the distribution over the manifold structure and a discriminative model that distinguishes between generated and true positive data. These models are trained in an adversarial way to improve hashing function learning. Additionally, we propose a correlation graph approach to capture the underlying manifold structure, which results in smaller Hamming distances and better retrieval accuracy. Our experiments show that our approach outperforms six state-of-the-art methods.",1
"Generative Adversarial Networks (GANs) represent a promising class of generative networks that combine neural networks with game theory. From generating realistic images and videos to assisting musical creation, GANs are transforming many fields of arts and sciences. However, their application to healthcare has not been fully realized, more specifically in generating electronic health records (EHR) data. In this paper, we propose a framework for exploring the value of GANs in the context of continuous laboratory time series data. We devise an unsupervised evaluation method that measures the predictive power of synthetic laboratory test time series. Further, we show that when it comes to predicting the impact of drug exposure on laboratory test data, incorporating representation learning of the training cohorts prior to training GAN models is beneficial.",0
"Generative Adversarial Networks (GANs) are a promising type of generative networks that merge neural networks with game theory. They have an impact on various fields, including the arts and sciences, by creating realistic images, videos, and music. Despite this, healthcare has not yet fully explored their potential, particularly in generating electronic health records (EHR) data. This study proposes a framework for examining the usefulness of GANs in continuous laboratory time series data. An unsupervised evaluation method measures the predictive power of synthetic laboratory test time series. Additionally, the research shows that incorporating representation learning of the training cohorts before training GAN models is beneficial for predicting the effect of drug exposure on laboratory test data.",1
"In this paper, we study the problem of using representation learning to assist information diffusion prediction on graphs. In particular, we aim at estimating the probability of an inactive node to be activated next in a cascade. Despite the success of recent deep learning methods for diffusion, we find that they often underexplore the cascade structure. We consider a cascade as not merely a sequence of nodes ordered by their activation time stamps; instead, it has a richer structure indicating the diffusion process over the data graph. As a result, we introduce a new data model, namely diffusion topologies, to fully describe the cascade structure. We find it challenging to model diffusion topologies, which are dynamic directed acyclic graphs (DAGs), with the existing neural networks. Therefore, we propose a novel topological recurrent neural network, namely Topo-LSTM, for modeling dynamic DAGs. We customize Topo-LSTM for the diffusion prediction task, and show it improves the state-of-the-art baselines, by 20.1%--56.6% (MAP) relatively, across multiple real-world data sets. Our code and data sets are available online at https://github.com/vwz/topolstm.",0
"The focus of this research is on using representation learning to aid in predicting information diffusion on graphs. Specifically, the goal is to estimate the likelihood of an inactive node becoming activated in a cascade. While deep learning methods have been successful in this task, they do not fully explore the complex structure of cascades. This study argues that cascades should be viewed as more than just a sequence of nodes and proposes a new data model called diffusion topologies to capture the full diffusion process. However, modelling dynamic directed acyclic graphs (DAGs) with existing neural networks is a challenge. Thus, researchers introduce a new topological recurrent neural network called Topo-LSTM to model dynamic DAGs. This model is customized for the diffusion prediction task and outperforms state-of-the-art baselines by 20.1% to 56.6% (MAP) across multiple real-world datasets. The code and datasets are available at https://github.com/vwz/topolstm.",1
"One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the corresponding value function can be approximated more easily by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.",0
"Reinforcement learning (RL) faces a significant obstacle in achieving generalization, which is often accomplished in deep RL methods by using a deep network to approximate the optimal value function with a low-dimensional representation. However, this approach may not work well in domains where the optimal value function cannot be easily reduced, leading to slow and unstable learning. To address this challenge, a new technique called Hybrid Reward Architecture (HRA) has been proposed. HRA takes a decomposed reward function as input and learns a separate value function for each component reward function. Since each component relies on only a subset of features, their corresponding value function can be more easily approximated with a low-dimensional representation, leading to more effective learning. HRA has been tested on a toy-problem and the Atari game Ms. Pac-Man, achieving above-human performance.",1
"Then detection and identification of extreme weather events in large-scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, many different types of spatially localized climate patterns are of interest including hurricanes, extra-tropical cyclones, weather fronts, and blocking events among others. Existing labeled data for these patterns can be incomplete in various ways, such as covering only certain years or geographic areas and having false negatives. This type of climate data therefore poses a number of interesting machine learning challenges. We present a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. We demonstrate that our approach is able to leverage temporal information and unlabeled data to improve the localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data. We present a dataset, ExtremeWeather, to encourage machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change. The dataset is available at extremeweatherdataset.github.io and the code is available at https://github.com/eracah/hur-detect.",0
"Identifying extreme weather events in large-scale climate simulations is crucial for managing risks, making informed governmental policy decisions, and enhancing our comprehension of the climate system. Previously, supervised convolutional neural networks (CNNs) have been utilized to classify well-known types of extreme weather events with high accuracy when abundant labeled data is available. However, there are various types of spatially localized climate patterns of interest, such as hurricanes, extra-tropical cyclones, weather fronts, and blocking events, for which labeled data may be incomplete and inaccurate. This presents machine learning challenges that require novel solutions. In this study, we propose a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. Our approach leverages temporal information and unlabeled data to improve the localization of extreme weather events. Additionally, we examine the representations learned by our model to enhance our comprehension of this valuable data. We also introduce ExtremeWeather, a dataset to encourage further machine learning research in this field and facilitate efforts to mitigate the impact of climate change. The dataset and code are available at extremeweatherdataset.github.io and https://github.com/eracah/hur-detect, respectively.",1
"In representation learning (RL), how to make the learned representations easy to interpret and less overfitted to training data are two important but challenging issues. To address these problems, we study a new type of regulariza- tion approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orthogonality among vectors and sparsity of each vector. We apply the proposed regularizer to two models: neural networks (NNs) and sparse coding (SC), and develop an efficient ADMM-based algorithm for regu- larized SC. Experiments on various datasets demonstrate that weight vectors learned under our regularizer are more interpretable and have better generalization performance.",0
"Interpreting and avoiding overfitting of learned representations is a challenging task in representation learning (RL). To tackle these issues, we explore a novel regularization technique that encourages weight vector supports in RL models to have minimal overlap. Our approach simultaneously promotes sparsity and near-orthogonality among vectors. We apply this regularization to two models - neural networks (NNs) and sparse coding (SC) - and develop an efficient ADMM-based algorithm for regularized SC. Our experiments on different datasets show that weight vectors learned using our regularizer are more interpretable and exhibit superior generalization performance.",1
"Graphs (networks) are ubiquitous and allow us to model entities (nodes) and the dependencies (edges) between them. Learning a useful feature representation from graph data lies at the heart and success of many machine learning tasks such as classification, anomaly detection, link prediction, among many others. Many existing techniques use random walks as a basis for learning features or estimating the parameters of a graph model for a downstream prediction task. Examples include recent node embedding methods such as DeepWalk, node2vec, as well as graph-based deep learning algorithms. However, the simple random walk used by these methods is fundamentally tied to the identity of the node. This has three main disadvantages. First, these approaches are inherently transductive and do not generalize to unseen nodes and other graphs. Second, they are not space-efficient as a feature vector is learned for each node which is impractical for large graphs. Third, most of these approaches lack support for attributed graphs.   To make these methods more generally applicable, we propose a framework for inductive network representation learning based on the notion of attributed random walk that is not tied to node identity and is instead based on learning a function $\Phi : \mathrm{\rm \bf x} \rightarrow w$ that maps a node attribute vector $\mathrm{\rm \bf x}$ to a type $w$. This framework serves as a basis for generalizing existing methods such as DeepWalk, node2vec, and many other previous methods that leverage traditional random walks.",0
"Graphs, which consist of nodes and edges, are widely used to represent entities and their dependencies. The ability to learn a useful feature representation from graph data is crucial for successful machine learning tasks like link prediction, anomaly detection, and classification. Random walks have been used as the basis for many existing techniques, including node embedding methods like DeepWalk and node2vec, as well as graph-based deep learning algorithms. However, these methods have three main drawbacks: they are transductive, not space-efficient, and lack support for attributed graphs. To address these issues, we propose a framework for inductive network representation learning based on attributed random walks. This approach is not tied to node identity and instead learns a function $\Phi : \mathrm{\rm \bf x} \rightarrow w$ that maps a node attribute vector $\mathrm{\rm \bf x}$ to a type $w$. This framework can be used to generalize existing methods and make them more widely applicable.",1
"The goal of graph representation learning is to embed each vertex in a graph into a low-dimensional vector space. Existing graph representation learning methods can be classified into two categories: generative models that learn the underlying connectivity distribution in the graph, and discriminative models that predict the probability of edge existence between a pair of vertices. In this paper, we propose GraphGAN, an innovative graph representation learning framework unifying above two classes of methods, in which the generative model and discriminative model play a game-theoretical minimax game. Specifically, for a given vertex, the generative model tries to fit its underlying true connectivity distribution over all other vertices and produces ""fake"" samples to fool the discriminative model, while the discriminative model tries to detect whether the sampled vertex is from ground truth or generated by the generative model. With the competition between these two models, both of them can alternately and iteratively boost their performance. Moreover, when considering the implementation of generative model, we propose a novel graph softmax to overcome the limitations of traditional softmax function, which can be proven satisfying desirable properties of normalization, graph structure awareness, and computational efficiency. Through extensive experiments on real-world datasets, we demonstrate that GraphGAN achieves substantial gains in a variety of applications, including link prediction, node classification, and recommendation, over state-of-the-art baselines.",0
"The objective of graph representation learning is to transform each vertex in a graph into a low-dimensional vector space. The available methods for graph representation learning can be divided into two groups: generative models, which learn the underlying connectivity distribution in the graph, and discriminative models, which forecast the probability of edge existence between a pair of vertices. This study introduces GraphGAN, a groundbreaking graph representation learning model that combines both classes of methods. GraphGAN utilizes a game-theoretical minimax game approach where the generative model and discriminative model compete against each other. The generative model attempts to match the underlying true connectivity distribution for a given vertex with other vertices and generates ""fake"" samples to deceive the discriminative model. On the other hand, the discriminative model endeavors to identify whether the sampled vertex is from the ground truth or generated by the generative model. Through the competition between these two models, they can improve their performance iteratively. Additionally, a novel graph softmax is proposed to overcome the limitations of traditional softmax functions when implementing the generative model. This novel graph softmax satisfies desirable properties of normalization, graph structure awareness, and computational efficiency. Extensive experiments on real-world datasets demonstrate that GraphGAN outperforms state-of-the-art baselines in various applications, including link prediction, node classification, and recommendation.",1
"Learning low-dimensional representations of networks has proved effective in a variety of tasks such as node classification, link prediction and network visualization. Existing methods can effectively encode different structural properties into the representations, such as neighborhood connectivity patterns, global structural role similarities and other high-order proximities. However, except for objectives to capture network structural properties, most of them suffer from lack of additional constraints for enhancing the robustness of representations. In this paper, we aim to exploit the strengths of generative adversarial networks in capturing latent features, and investigate its contribution in learning stable and robust graph representations. Specifically, we propose an Adversarial Network Embedding (ANE) framework, which leverages the adversarial learning principle to regularize the representation learning. It consists of two components, i.e., a structure preserving component and an adversarial learning component. The former component aims to capture network structural properties, while the latter contributes to learning robust representations by matching the posterior distribution of the latent representations to given priors. As shown by the empirical results, our method is competitive with or superior to state-of-the-art approaches on benchmark network embedding tasks.",0
"The acquisition of low-dimensional representations of networks has proven to be useful in various tasks, including node classification, link prediction, and network visualization. Current techniques are effective in encoding diverse structural characteristics into these representations, such as neighborhood connectivity patterns, global structural role similarities, and other high-order proximities. Nevertheless, most of these methods lack additional constraints to improve the robustness of the representations beyond capturing network structural properties. This paper proposes an Adversarial Network Embedding (ANE) framework that exploits the strengths of generative adversarial networks to capture latent features and investigate their contribution to learning stable and robust graph representations. The framework consists of two components: a structure-preserving component that captures network structural properties and an adversarial learning component that regularizes representation learning by matching the posterior distribution of the latent representations to given priors. Empirical results show that the proposed method is competitive with or surpasses state-of-the-art approaches in benchmark network embedding tasks.",1
"With the development of deep learning, supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks (CNNs). However, due to the limited amount of labeled data available, supervised learning is often difficult to carry out. Therefore, we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) to learn a representation using only unlabeled data. MARTA GANs consists of both a generative model $G$ and a discriminative model $D$. We treat $D$ as a feature extractor. To fit the complex properties of remote sensing data, we use a fusion layer to merge the mid-level and global features. $G$ can produce numerous images that are similar to the training data; therefore, $D$ can learn better representations of remotely sensed images using the training data provided by $G$. The classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-the-art methods.",0
"Convolutional networks (CNNs) have become a popular choice for classifying remotely sensed images using supervised learning. However, the scarcity of labeled data often makes this approach challenging. To address this issue, we introduced an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) that solely relies on unlabeled data to learn a representation. MARTA GANs comprises a generative model, G, and a discriminative model, D, which is treated as a feature extractor. We utilize a fusion layer to combine mid-level and global features to accommodate the intricate properties of remote sensing data. G generates multiple images that resemble the training data, enabling D to acquire better representations of remotely sensed images from the data provided by G. Our method outperforms other state-of-the-art methods in terms of classification outcomes on two widely used remote sensing image databases.",1
"A novel 3D shape classification scheme, based on collaborative representation learning, is investigated in this work. A data-driven feature-extraction procedure, taking the form of a simple projection operator, is in the core of our methodology. Provided a shape database, a graph encapsulating the structural relationships among all the available shapes, is first constructed and then employed in defining low-dimensional sparse projections. The recently introduced method of CRPs (collaborative representation based projections), which is based on L2-Graph, is the first variant that is included towards this end. A second algorithm, that particularizes the CRPs to shape descriptors that are inherently nonnegative, is also introduced as potential alternative. In both cases, the weights in the graph reflecting the database structure are calculated so as to approximate each shape as a sparse linear combination of the remaining dataset objects. By way of solving a generalized eigenanalysis problem, a linear matrix operator is designed that will act as the feature extractor. Two popular, inherently high dimensional descriptors, namely ShapeDNA and Global Point Signature (GPS), are employed in our experimentations with SHREC10, SHREC11 and SCHREC 15 datasets, where shape recognition is cast as a multi-class classification problem that is tackled by means of an SVM (support vector machine) acting within the reduced dimensional space of the crafted projections. The results are very promising and outperform state of the art methods, providing evidence about the highly discriminative nature of the introduced 3D shape representations.",0
"This study explores a novel approach to classify 3D shapes, utilizing collaborative representation learning. The methodology involves a data-driven feature extraction process using a simple projection operator. First, a graph is constructed to encapsulate the structural relationships within the shape database, which is then used to define low-dimensional sparse projections. Two algorithms are introduced, including CRPs and a nonnegative shape descriptor variant, both using weights in the graph to approximate each shape as a sparse linear combination of the remaining dataset objects. A linear matrix operator is designed by solving a generalized eigenanalysis problem to act as the feature extractor. ShapeDNA and Global Point Signature (GPS) descriptors are used in the experimentations with SHREC10, SHREC11, and SCHREC15 datasets, with shape recognition cast as a multi-class classification problem solved by an SVM within the reduced dimensional space of the projections. The results show promising performance, outperforming state-of-the-art methods, indicating the highly discriminative nature of the introduced 3D shape representations.",1
"Learning discriminative representations for unseen person images is critical for person Re-Identification (ReID). Most of current approaches learn deep representations in classification tasks, which essentially minimize the empirical classification risk on the training set. As shown in our experiments, such representations commonly focus on several body parts discriminative to the training set, rather than the entire human body. Inspired by the structural risk minimization principle in SVM, we revise the traditional deep representation learning procedure to minimize both the empirical classification risk and the representation learning risk. The representation learning risk is evaluated by the proposed part loss, which automatically generates several parts for an image, and computes the person classification loss on each part separately. Compared with traditional global classification loss, simultaneously considering multiple part loss enforces the deep network to focus on the entire human body and learn discriminative representations for different parts. Experimental results on three datasets, i.e., Market1501, CUHK03, VIPeR, show that our representation outperforms the existing deep representations.",0
"Developing discriminative representations for unseen person images is essential in the context of person Re-Identification (ReID). Currently, many methods focus on deep representation learning in the context of classification tasks, which aim to minimize the empirical classification risk on the training data. However, our experiments demonstrate that such representations often concentrate on a few body parts that are discriminative only in the context of the training set, rather than the entire human body. Drawing on the structural risk minimization principle in SVM, we have re-designed the traditional deep representation learning process to reduce both the empirical classification risk and the representation learning risk. The part loss we have proposed to measure representation learning risk generates several parts for an image and calculates the person classification loss for each part independently. By simultaneously considering multiple part losses, the deep network is compelled to concentrate on the entire human body and learn discriminative representations for different parts. Our representation has been evaluated on three datasets, Market1501, CUHK03, and VIPeR, and has been found to outperform existing deep representations.",1
"Increasingly many real world tasks involve data in multiple modalities or views. This has motivated the development of many effective algorithms for learning a common latent space to relate multiple domains. However, most existing cross-view learning algorithms assume access to paired data for training. Their applicability is thus limited as the paired data assumption is often violated in practice: many tasks have only a small subset of data available with pairing annotation, or even no paired data at all. In this paper we introduce Deep Matching Autoencoders (DMAE), which learn a common latent space and pairing from unpaired multi-modal data. Specifically we formulate this as a cross-domain representation learning and object matching problem. We simultaneously optimise parameters of representation learning auto-encoders and the pairing of unpaired multi-modal data. This framework elegantly spans the full regime from fully supervised, semi-supervised, and unsupervised (no paired data) multi-modal learning. We show promising results in image captioning, and on a new task that is uniquely enabled by our methodology: unsupervised classifier learning.",0
"The use of data in multiple modalities or views has become increasingly prevalent in real-world tasks. As a result, various algorithms have been developed to learn a common latent space for relating multiple domains effectively. However, most existing cross-view learning algorithms require paired data for training, which limits their applicability as this assumption is often not met in practice. In many cases, only a small subset of data is available with pairing annotation or no paired data is available at all. To address this limitation, we propose Deep Matching Autoencoders (DMAE), which can learn a common latent space and pairing from unpaired multi-modal data. We formulate this as a cross-domain representation learning and object matching problem and optimize the parameters of representation learning auto-encoders and the pairing of unpaired multi-modal data simultaneously. This framework covers the entire spectrum from fully supervised to unsupervised (no paired data) multi-modal learning. We demonstrate promising results in image captioning and introduce a new task that is uniquely enabled by our methodology: unsupervised classifier learning.",1
"For the past 5 years, the ILSVRC competition and the ImageNet dataset have attracted a lot of interest from the Computer Vision community, allowing for state-of-the-art accuracy to grow tremendously. This should be credited to the use of deep artificial neural network designs. As these became more complex, the storage, bandwidth, and compute requirements increased. This means that with a non-distributed approach, even when using the most high-density server available, the training process may take weeks, making it prohibitive. Furthermore, as datasets grow, the representation learning potential of deep networks grows as well by using more complex models. This synchronicity triggers a sharp increase in the computational requirements and motivates us to explore the scaling behaviour on petaflop scale supercomputers. In this paper we will describe the challenges and novel solutions needed in order to train ResNet-50 in this large scale environment. We demonstrate above 90\% scaling efficiency and a training time of 28 minutes using up to 104K x86 cores. This is supported by software tools from Intel's ecosystem. Moreover, we show that with regular 90 - 120 epoch train runs we can achieve a top-1 accuracy as high as 77\% for the unmodified ResNet-50 topology. We also introduce the novel Collapsed Ensemble (CE) technique that allows us to obtain a 77.5\% top-1 accuracy, similar to that of a ResNet-152, while training a unmodified ResNet-50 topology for the same fixed training budget. All ResNet-50 models as well as the scripts needed to replicate them will be posted shortly.",0
"Over the last 5 years, the Computer Vision community has shown great interest in the ILSVRC competition and the ImageNet dataset, resulting in a significant increase in the accuracy of state-of-the-art models. This progress can be attributed to the use of complex deep artificial neural network designs, which require greater storage, bandwidth, and compute resources. However, training these models on a non-distributed system can take weeks, even with the most high-density server available, making it impractical. Additionally, larger datasets lead to an increase in the computational requirements, prompting researchers to explore the scaling behavior on petaflop scale supercomputers. This paper describes the challenges faced and innovative solutions developed for training ResNet-50 in a large-scale environment. The team achieved above 90% scaling efficiency and a training time of 28 minutes using up to 104K x86 cores, aided by software tools from Intel's ecosystem. The experiments also revealed that regular 90-120 epoch train runs can achieve a top-1 accuracy as high as 77% for the unmodified ResNet-50 topology. Furthermore, the researchers introduced the Collapsed Ensemble technique, which allows for a 77.5% top-1 accuracy, similar to that of a ResNet-152, while training an unmodified ResNet-50 topology for the same fixed training budget. All ResNet-50 models and scripts will be made available shortly.",1
"Deep learning techniques have been successfully used in learning a common representation for multi-view data, wherein the different modalities are projected onto a common subspace. In a broader perspective, the techniques used to investigate common representation learning falls under the categories of canonical correlation-based approaches and autoencoder based approaches. In this paper, we investigate the performance of deep autoencoder based methods on multi-view data. We propose a novel step-based correlation multi-modal CNN (CorrMCNN) which reconstructs one view of the data given the other while increasing the interaction between the representations at each hidden layer or every intermediate step. Finally, we evaluate the performance of the proposed model on two benchmark datasets - MNIST and XRMB. Through extensive experiments, we find that the proposed model achieves better performance than the current state-of-the-art techniques on joint common representation learning and transfer learning tasks.",0
"The successful use of deep learning techniques in acquiring a common representation for multi-view data involves projecting different modalities onto a shared subspace. Two categories of approaches are commonly used in examining common representation learning: canonical correlation-based and autoencoder-based approaches. This study focuses on the performance of deep autoencoder-based methods for multi-view data. A new step-based correlation multi-modal CNN (CorrMCNN) is proposed, which enhances the interaction between representations at each hidden layer or intermediate step, and reconstructs one view of the data given the other. The proposed model is evaluated on two benchmark datasets, MNIST and XRMB, and is found to outperform current state-of-the-art techniques in joint common representation learning and transfer learning tasks.",1
"This paper shows that a simple baseline based on a Bag-of-Words (BoW) representation learns surprisingly good knowledge graph embeddings. By casting knowledge base completion and question answering as supervised classification problems, we observe that modeling co-occurences of entities and relations leads to state-of-the-art performance with a training time of a few minutes using the open sourced library fastText.",0
"The article demonstrates that an uncomplicated baseline utilizing a Bag-of-Words (BoW) depiction acquires unexpectedly valuable knowledge graph embeddings. We establish that including co-occurring entities and relations in supervised classification tasks for knowledge base completion and question answering produces state-of-the-art results. This is accomplished with fastText, an open-source library, in just a few minutes of training time.",1
"Person Re-identification (ReID) is to identify the same person across different cameras. It is a challenging task due to the large variations in person pose, occlusion, background clutter, etc How to extract powerful features is a fundamental problem in ReID and is still an open problem today. In this paper, we design a Multi-Scale Context-Aware Network (MSCAN) to learn powerful features over full body and body parts, which can well capture the local context knowledge by stacking multi-scale convolutions in each layer. Moreover, instead of using predefined rigid parts, we propose to learn and localize deformable pedestrian parts using Spatial Transformer Networks (STN) with novel spatial constraints. The learned body parts can release some difficulties, eg pose variations and background clutters, in part-based representation. Finally, we integrate the representation learning processes of full body and body parts into a unified framework for person ReID through multi-class person identification tasks. Extensive evaluations on current challenging large-scale person ReID datasets, including the image-based Market1501, CUHK03 and sequence-based MARS datasets, show that the proposed method achieves the state-of-the-art results.",0
"Re-identification of individuals across different cameras, known as Person ReID, is a daunting task due to various factors like pose changes, background disturbances, and occlusion. One of the primary challenges in ReID is the extraction of powerful features, which remains an unsolved problem. In this research, we have developed the Multi-Scale Context-Aware Network (MSCAN) to capture local context knowledge by integrating multi-scale convolutions in each layer and learn powerful features over full body and body parts. We have also proposed a Spatial Transformer Network (STN) to learn and localize deformable pedestrian parts using novel spatial constraints instead of predefined rigid parts. The learned body parts alleviate pose variations and background clutter challenges in part-based representations. Finally, we have combined the representation learning processes of full body and body parts into a unified framework for person ReID via multi-class person identification tasks. The proposed method has been extensively tested on several large-scale person ReID datasets, including Market1501, CUHK03, and MARS, and has achieved state-of-the-art results.",1
"Label distribution learning (LDL) is a general learning framework, which assigns to an instance a distribution over a set of labels rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning, e.g., to learn deep features in an end-to-end manner. This paper presents label distribution learning forests (LDLFs) - a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by a mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning. We define a distribution-based loss function for a forest, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on several LDL tasks and a computer vision application, showing significant improvements to the state-of-the-art LDL methods.",0
"The concept of Label Distribution Learning (LDL) involves assigning a distribution of labels to an instance, rather than a single or multiple labels. However, current LDL methods either have limitations in representation learning or make assumptions about the label distribution expression form. In this paper, we introduce Label Distribution Learning Forests (LDLFs), a new algorithm based on differentiable decision trees that overcomes these limitations. The use of decision trees allows for modeling any form of label distribution through leaf node predictions. Additionally, representation learning can be combined with the learning of differentiable decision trees. We propose a distribution-based loss function for the forest, enabling the learning of all trees together. We show that an update function for leaf node predictions can be derived by variational bounding, ensuring a decrease in the loss function. Our LDLFs outperform state-of-the-art LDL methods in various LDL tasks and a computer vision application.",1
"Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. Established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this application requires substantial implementation effort. Thus, there has been substantial duplication of effort and incompatible infrastructure developed across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon.   NiftyNet provides a modular deep-learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications. Components of the NiftyNet pipeline including data loading, data augmentation, network architectures, loss functions and evaluation metrics are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted intervention. NiftyNet is built on TensorFlow and supports TensorBoard visualization of 2D and 3D images and computational graphs by default.   We present 3 illustrative medical image analysis applications built using NiftyNet: (1) segmentation of multiple abdominal organs from computed tomography; (2) image regression to predict computed tomography attenuation maps from brain magnetic resonance images; and (3) generation of simulated ultrasound images for specified anatomical poses.   NiftyNet enables researchers to rapidly develop and distribute deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications.",0
"Deep learning is increasingly being used to address medical image analysis and computer-assisted intervention problems. However, established deep-learning platforms lack specific functionality for medical image analysis, resulting in duplication of effort and incompatible infrastructure across research groups. To address this issue, NiftyNet is introduced as an open-source platform for deep learning in medical imaging. NiftyNet provides a modular pipeline for various medical imaging applications, including segmentation, regression, image generation, and representation learning. The platform is built on TensorFlow and supports TensorBoard visualization. Three medical image analysis applications are presented as examples of NiftyNet's capabilities. The platform enables researchers to quickly develop and distribute deep learning solutions or extend the platform to new applications.",1
"This paper presents a general graph representation learning framework called DeepGL for learning deep node and edge representations from large (attributed) graphs. In particular, DeepGL begins by deriving a set of base features (e.g., graphlet features) and automatically learns a multi-layered hierarchical graph representation where each successive layer leverages the output from the previous layer to learn features of a higher-order. Contrary to previous work, DeepGL learns relational functions (each representing a feature) that generalize across-networks and therefore useful for graph-based transfer learning tasks. Moreover, DeepGL naturally supports attributed graphs, learns interpretable features, and is space-efficient (by learning sparse feature vectors). In addition, DeepGL is expressive, flexible with many interchangeable components, efficient with a time complexity of $\mathcal{O}(|E|)$, and scalable for large networks via an efficient parallel implementation. Compared with the state-of-the-art method, DeepGL is (1) effective for across-network transfer learning tasks and attributed graph representation learning, (2) space-efficient requiring up to 6x less memory, (3) fast with up to 182x speedup in runtime performance, and (4) accurate with an average improvement of 20% or more on many learning tasks.",0
"The article introduces DeepGL, a comprehensive framework for learning deep node and edge representations from large graphs with attributes. The approach uses base features, such as graphlet features, to automatically learn a multi-layered hierarchical graph representation where each layer builds on the previous one to learn higher-order features. Unlike prior work, DeepGL learns relational functions that generalize across networks, making it suitable for graph-based transfer learning tasks. The method is also capable of handling attributed graphs, producing interpretable features, and learning sparse feature vectors, which makes it space-efficient. DeepGL is expressive, flexible, and efficient, with a time complexity of $\mathcal{O}(|E|)$, and it can scale to large networks through parallel implementation. The results show that DeepGL is effective for across-network transfer learning tasks and attributed graph representation learning, space-efficient, fast, and accurate, outperforming the state-of-the-art method by up to 20% or more on many learning tasks.",1
"We have witnessed the discovery of many techniques for network representation learning in recent years, ranging from encoding the context in random walks to embedding the lower order connections, to finding latent space representations with auto-encoders. However, existing techniques are looking mostly into the local structures in a network, while higher-level properties such as global community structures are often neglected. We propose a novel network representations learning model framework called RUM (network Representation learning throUgh Multi-level structural information preservation). In RUM, we incorporate three essential aspects of a node that capture a network's characteristics in multiple levels: a node's affiliated local triads, its neighborhood relationships, and its global community affiliations. Therefore the framework explicitly and comprehensively preserves the structural information of a network, extending the encoding process both to the local end of the structural information spectrum and to the global end. The framework is also flexible enough to take various community discovery algorithms as its preprocessor. Empirical results show that the representations learned by RUM have demonstrated substantial performance advantages in real-life tasks.",0
"In recent years, numerous techniques for network representation learning have been discovered, including encoding context in random walks, embedding lower order connections, and using auto-encoders to find latent space representations. However, these techniques primarily focus on local structures in a network, disregarding higher-level properties such as global community structures. To address this gap, we introduce a novel network representations learning model framework called RUM (network Representation learning throUgh Multi-level structural information preservation). RUM incorporates three essential aspects of a node that capture a network's characteristics at multiple levels: a node's affiliated local triads, its neighborhood relationships, and its global community affiliations. This comprehensive approach preserves the structural information of a network, extending the encoding process from the local to the global end of the structural information spectrum. Additionally, RUM is flexible enough to integrate various community discovery algorithms as its preprocessor. Empirical results demonstrate that RUM's representations provide substantial performance advantages in real-life tasks.",1
"Sketch portrait generation benefits a wide range of applications such as digital entertainment and law enforcement. Although plenty of efforts have been dedicated to this task, several issues still remain unsolved for generating vivid and detail-preserving personal sketch portraits. For example, quite a few artifacts may exist in synthesizing hairpins and glasses, and textural details may be lost in the regions of hair or mustache. Moreover, the generalization ability of current systems is somewhat limited since they usually require elaborately collecting a dictionary of examples or carefully tuning features/components. In this paper, we present a novel representation learning framework that generates an end-to-end photo-sketch mapping through structure and texture decomposition. In the training stage, we first decompose the input face photo into different components according to their representational contents (i.e., structural and textural parts) by using a pre-trained Convolutional Neural Network (CNN). Then, we utilize a Branched Fully Convolutional Neural Network (BFCN) for learning structural and textural representations, respectively. In addition, we design a Sorted Matching Mean Square Error (SM-MSE) metric to measure texture patterns in the loss function. In the stage of sketch rendering, our approach automatically generates structural and textural representations for the input photo and produces the final result via a probabilistic fusion scheme. Extensive experiments on several challenging benchmarks suggest that our approach outperforms example-based synthesis algorithms in terms of both perceptual and objective metrics. In addition, the proposed method also has better generalization ability across dataset without additional training.",0
"Generating sketch portraits has a wide range of applications, including digital entertainment and law enforcement. Despite considerable efforts, there are still issues with creating realistic and detailed personal sketch portraits. Synthesizing hairpins and glasses can result in artifacts, and textural details may be lost in hair or mustache regions. Current systems have limited generalization ability and require the collection of a dictionary of examples or careful feature/component tuning. This paper presents a novel framework for representation learning that uses structure and texture decomposition to generate an end-to-end photo-sketch mapping. In the training stage, a pre-trained Convolutional Neural Network (CNN) decomposes the input face photo into components based on their representational contents. A Branched Fully Convolutional Neural Network (BFCN) learns structural and textural representations, and a Sorted Matching Mean Square Error (SM-MSE) metric measures texture patterns in the loss function. During sketch rendering, the approach generates structural and textural representations and produces the final result through a probabilistic fusion scheme. Extensive experiments show that this approach outperforms example-based synthesis algorithms in both perceptual and objective metrics and has better generalization ability.",1
"Face attribute estimation has many potential applications in video surveillance, face retrieval, and social media. While a number of methods have been proposed for face attribute estimation, most of them did not explicitly consider the attribute correlation and heterogeneity (e.g., ordinal vs. nominal and holistic vs. local) during feature representation learning. In this paper, we present a Deep Multi-Task Learning (DMTL) approach to jointly estimate multiple heterogeneous attributes from a single face image. In DMTL, we tackle attribute correlation and heterogeneity with convolutional neural networks (CNNs) consisting of shared feature learning for all the attributes, and category-specific feature learning for heterogeneous attributes. We also introduce an unconstrained face database (LFW+), an extension of public-domain LFW, with heterogeneous demographic attributes (age, gender, and race) obtained via crowdsourcing. Experimental results on benchmarks with multiple face attributes (MORPH II, LFW+, CelebA, LFWA, and FotW) show that the proposed approach has superior performance compared to state of the art. Finally, evaluations on a public-domain face database (LAP) with a single attribute show that the proposed approach has excellent generalization ability.",0
"The estimation of facial attributes has numerous potential applications such as video surveillance, social media, and face retrieval. However, current methods for attribute estimation often fail to account for attribute correlation and heterogeneity, including ordinal vs. nominal and holistic vs. local. This paper introduces a Deep Multi-Task Learning (DMTL) approach that jointly estimates multiple heterogeneous attributes from a single face image. The approach utilizes convolutional neural networks (CNNs) with shared feature learning for all attributes and category-specific feature learning for heterogeneous attributes. An unconstrained face database (LFW+) with demographic attributes obtained via crowdsourcing was also introduced. The proposed approach outperforms state-of-the-art methods on multiple face attribute benchmarks, and has excellent generalization ability on a single attribute public-domain database (LAP).",1
"Random walks are at the heart of many existing deep learning algorithms for graph data. However, such algorithms have many limitations that arise from the use of random walks, e.g., the features resulting from these methods are unable to transfer to new nodes and graphs as they are tied to node identity. In this work, we introduce the notion of attributed random walks which serves as a basis for generalizing existing methods such as DeepWalk, node2vec, and many others that leverage random walks. Our proposed framework enables these methods to be more widely applicable for both transductive and inductive learning as well as for use on graphs with attributes (if available). This is achieved by learning functions that generalize to new nodes and graphs. We show that our proposed framework is effective with an average AUC improvement of 16.1% while requiring on average 853 times less space than existing methods on a variety of graphs from several domains.",0
"Numerous deep learning algorithms for graph data rely on random walks as their foundation. However, these algorithms are fraught with limitations that stem from the use of random walks. For instance, the outcomes of these methods cannot be transferred to new nodes and graphs due to their dependence on node identity. In this study, we present the idea of attributed random walks, which forms the basis for broadening the scope of existing techniques like DeepWalk, node2vec, and others that employ random walks. Our proposed framework allows for wider applicability of these methods for transductive and inductive learning, as well as for use on graphs that have attributes (if available). This is made possible by learning functions that have the ability to generalize to new nodes and graphs. We demonstrate the effectiveness of our proposed framework by showing an average AUC increase of 16.1%, while using 853 times less space than existing methods on various graphs from multiple domains.",1
"We propose a new method for embedding graphs while preserving directed edge information. Learning such continuous-space vector representations (or embeddings) of nodes in a graph is an important first step for using network information (from social networks, user-item graphs, knowledge bases, etc.) in many machine learning tasks.   Unlike previous work, we (1) explicitly model an edge as a function of node embeddings, and we (2) propose a novel objective, the ""graph likelihood"", which contrasts information from sampled random walks with non-existent edges. Individually, both of these contributions improve the learned representations, especially when there are memory constraints on the total size of the embeddings. When combined, our contributions enable us to significantly improve the state-of-the-art by learning more concise representations that better preserve the graph structure.   We evaluate our method on a variety of link-prediction task including social networks, collaboration networks, and protein interactions, showing that our proposed method learn representations with error reductions of up to 76% and 55%, on directed and undirected graphs. In addition, we show that the representations learned by our method are quite space efficient, producing embeddings which have higher structure-preserving accuracy but are 10 times smaller.",0
"Our proposed approach aims to embed graphs while retaining the directed edge information. Generating continuous-space vector representations of nodes in a graph is crucial in utilizing network information for various machine learning tasks, such as social networks, user-item graphs, and knowledge bases. Our method stands out from previous work as we (1) model an edge explicitly as a function of node embeddings, and (2) introduce a new objective, the ""graph likelihood,"" which compares information from sampled random walks with non-existent edges. These contributions enhance the learned representations, especially when facing memory constraints on the total size of the embeddings. By combining them, we achieve a significant improvement in the state-of-the-art by acquiring more concise representations that better preserve the graph structure. Our method is evaluated on different link-prediction tasks, including social networks, collaboration networks, and protein interactions. We demonstrate that our proposed method produces embeddings with error reductions of up to 76% and 55% for directed and undirected graphs, respectively. Moreover, our method produces space-efficient embeddings that have higher structure-preserving accuracy but are 10 times smaller.",1
"The huge variance of human pose and the misalignment of detected human images significantly increase the difficulty of person Re-Identification (Re-ID). Moreover, efficient Re-ID systems are required to cope with the massive visual data being produced by video surveillance systems. Targeting to solve these problems, this work proposes a Global-Local-Alignment Descriptor (GLAD) and an efficient indexing and retrieval framework, respectively. GLAD explicitly leverages the local and global cues in human body to generate a discriminative and robust representation. It consists of part extraction and descriptor learning modules, where several part regions are first detected and then deep neural networks are designed for representation learning on both the local and global regions. A hierarchical indexing and retrieval framework is designed to eliminate the huge redundancy in the gallery set, and accelerate the online Re-ID procedure. Extensive experimental results show GLAD achieves competitive accuracy compared to the state-of-the-art methods. Our retrieval framework significantly accelerates the online Re-ID procedure without loss of accuracy. Therefore, this work has potential to work better on person Re-ID tasks in real scenarios.",0
"Person Re-Identification (Re-ID) is a challenging task due to the vast range of human poses and misaligned images. Additionally, video surveillance systems generate a massive amount of visual data, making it crucial to have efficient Re-ID systems. This study proposes a Global-Local-Alignment Descriptor (GLAD) and an efficient indexing and retrieval framework to address these issues. GLAD utilizes both local and global cues in the human body to create a robust and discriminative representation. It includes part extraction and descriptor learning modules, where deep neural networks are used for representation learning on both local and global regions. A hierarchical indexing and retrieval framework is also designed to eliminate redundancy and accelerate the online Re-ID process. Experimental results demonstrate that GLAD is competitive with state-of-the-art methods, and the retrieval framework significantly speeds up the online Re-ID process while maintaining accuracy. These findings suggest that this approach could be effective for person Re-ID tasks in real scenarios.",1
"Sparse representation learning has recently gained a great success in signal and image processing, thanks to recent advances in dictionary learning. To this end, the $\ell_0$-norm is often used to control the sparsity level. Nevertheless, optimization problems based on the $\ell_0$-norm are non-convex and NP-hard. For these reasons, relaxation techniques have been attracting much attention of researchers, by priorly targeting approximation solutions (e.g. $\ell_1$-norm, pursuit strategies). On the contrary, this paper considers the exact $\ell_0$-norm optimization problem and proves that it can be solved effectively, despite of its complexity. The proposed method reformulates the problem as a Mixed-Integer Quadratic Program (MIQP) and gets the global optimal solution by applying existing optimization software. Because the main difficulty of this approach is its computational time, two techniques are introduced that improve the computational speed. Finally, our method is applied to image denoising which shows its feasibility and relevance compared to the state-of-the-art.",0
"Recent advances in dictionary learning have led to great success in signal and image processing through sparse representation learning. The sparsity level is typically controlled using the $\ell_0$-norm, but optimization problems based on this norm are non-convex and NP-hard. Therefore, relaxation techniques that aim to approximate solutions, such as the $\ell_1$-norm and pursuit strategies, have gained attention. However, this paper proposes an exact solution to the $\ell_0$-norm optimization problem using a Mixed-Integer Quadratic Program (MIQP) and existing optimization software, despite its complexity. To improve computational time, two techniques are introduced. The feasibility and relevance of this method are demonstrated in image denoising compared to state-of-the-art approaches.",1
"Deep Neural Networks trained on large datasets can be easily transferred to new domains with far fewer labeled examples by a process called fine-tuning. This has the advantage that representations learned in the large source domain can be exploited on smaller target domains. However, networks designed to be optimal for the source task are often prohibitively large for the target task. In this work we address the compression of networks after domain transfer.   We focus on compression algorithms based on low-rank matrix decomposition. Existing methods base compression solely on learned network weights and ignore the statistics of network activations. We show that domain transfer leads to large shifts in network activations and that it is desirable to take this into account when compressing. We demonstrate that considering activation statistics when compressing weights leads to a rank-constrained regression problem with a closed-form solution. Because our method takes into account the target domain, it can more optimally remove the redundancy in the weights. Experiments show that our Domain Adaptive Low Rank (DALR) method significantly outperforms existing low-rank compression techniques. With our approach, the fc6 layer of VGG19 can be compressed more than 4x more than using truncated SVD alone -- with only a minor or no loss in accuracy. When applied to domain-transferred networks it allows for compression down to only 5-20% of the original number of parameters with only a minor drop in performance.",0
"Fine-tuning is a process that enables Deep Neural Networks trained on large datasets to be easily transferred to new domains with far fewer labeled examples. This is advantageous because it allows for the exploitation of representations learned in the large source domain on smaller target domains. However, networks designed for the source task can often be excessively large for the target task. This paper focuses on network compression after domain transfer and proposes compression algorithms based on low-rank matrix decomposition. Existing methods solely base compression on learned network weights and disregard the statistics of network activations. The study shows that domain transfer results in significant shifts in network activations, and it is necessary to consider this when compressing. The proposed method uses a rank-constrained regression problem with a closed-form solution that considers activation statistics when compressing weights. This approach can optimally remove redundancy in the weights because it takes into account the target domain. Experimental results show that the proposed Domain Adaptive Low Rank (DALR) method outperforms existing low-rank compression techniques. Specifically, the fc6 layer of VGG19 can be compressed more than 4x more than using truncated SVD alone, with only a minor or no loss in accuracy. When applied to domain-transferred networks, it allows for compression down to only 5-20% of the original number of parameters with only a minor drop in performance.",1
"This paper proposes a multi-level feature learning framework for human action recognition using a single body-worn inertial sensor. The framework consists of three phases, respectively designed to analyze signal-based (low-level), components (mid-level) and semantic (high-level) information. Low-level features capture the time and frequency domain property while mid-level representations learn the composition of the action. The Max-margin Latent Pattern Learning (MLPL) method is proposed to learn high-level semantic descriptions of latent action patterns as the output of our framework. The proposed method achieves the state-of-the-art performances, 88.7%, 98.8% and 72.6% (weighted F1 score) respectively, on Skoda, WISDM and OPP datasets.",0
"A multi-level feature learning framework is suggested in this paper to recognize human actions using only one body-worn inertial sensor. The framework comprises three phases that respectively analyze signal-based (low-level), components (mid-level), and semantic (high-level) information. The low-level features capture the time and frequency domain property, while the mid-level representations learn the action's composition. To learn high-level semantic descriptions of latent action patterns as the output of the framework, the proposed method is Max-margin Latent Pattern Learning (MLPL). On Skoda, WISDM, and OPP datasets, the proposed method attains state-of-the-art performances, specifically 88.7%, 98.8%, and 72.6% (weighted F1 score), respectively.",1
"We study the problem of acoustic feature learning in the setting where we have access to another (non-acoustic) modality for feature learning but not at test time. We use deep variational canonical correlation analysis (VCCA), a recently proposed deep generative method for multi-view representation learning. We also extend VCCA with improved latent variable priors and with adversarial learning. Compared to other techniques for multi-view feature learning, VCCA's advantages include an intuitive latent variable interpretation and a variational lower bound objective that can be trained end-to-end efficiently. We compare VCCA and its extensions with previous feature learning methods on the University of Wisconsin X-ray Microbeam Database, and show that VCCA-based feature learning improves over previous methods for speaker-independent phonetic recognition.",0
"The focus of our research is on the acquisition of acoustic features, with the added complication that there is access to a non-acoustic modality for feature learning, which is not available during testing. Our approach involves the utilization of deep variational canonical correlation analysis (VCCA), a novel deep generative method for multi-view representation learning. We have developed VCCA further by incorporating improved latent variable priors and adversarial learning. VCCA has distinct advantages over other techniques for multi-view feature learning, including an easily understood latent variable interpretation and a variational lower bound objective that can be trained efficiently in an end-to-end manner. Our study compares VCCA and its extensions against previous feature learning methods, using the University of Wisconsin X-ray Microbeam Database. We demonstrate that VCCA-based feature learning outperforms previous methods for speaker-independent phonetic recognition.",1
"The recently developed variational autoencoders (VAEs) have proved to be an effective confluence of the rich representational power of neural networks with Bayesian methods. However, most work on VAEs use a rather simple prior over the latent variables such as standard normal distribution, thereby restricting its applications to relatively simple phenomena. In this work, we propose hierarchical nonparametric variational autoencoders, which combines tree-structured Bayesian nonparametric priors with VAEs, to enable infinite flexibility of the latent representation space. Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference. The resulting model induces a hierarchical structure of latent semantic concepts underlying the data corpus, and infers accurate representations of data instances. We apply our model in video representation learning. Our method is able to discover highly interpretable activity hierarchies, and obtain improved clustering accuracy and generalization capacity based on the learned rich representations.",0
"The combination of neural networks and Bayesian methods in variational autoencoders (VAEs) has shown promise. However, the use of a simple prior over the latent variables, such as a standard normal distribution, has limited its usefulness in complex scenarios. To address this, we introduce hierarchical nonparametric variational autoencoders that use tree-structured Bayesian nonparametric priors to enable infinite flexibility in the latent representation space. Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference. This results in a hierarchical structure of latent semantic concepts that accurately represents data instances. We apply this model to video representation learning, discovering highly interpretable activity hierarchies and improving clustering accuracy and generalization capacity.",1
"We introduce a novel method for representation learning that uses an artificial supervision signal based on counting visual primitives. This supervision signal is obtained from an equivariance relation, which does not require any manual annotation. We relate transformations of images to transformations of the representations. More specifically, we look for the representation that satisfies such relation rather than the transformations that match a given representation. In this paper, we use two image transformations in the context of counting: scaling and tiling. The first transformation exploits the fact that the number of visual primitives should be invariant to scale. The second transformation allows us to equate the total number of visual primitives in each tile to that in the whole image. These two transformations are combined in one constraint and used to train a neural network with a contrastive loss. The proposed task produces representations that perform on par or exceed the state of the art in transfer learning benchmarks.",0
"We present an innovative approach to representation learning that employs an artificial supervision signal based on counting visual primitives, eliminating the need for manual annotation. Using an equivariance relation, we connect image transformations to transformations of the representations, seeking the representation that satisfies the relation rather than finding the transformations that match a given representation. Our study uses two image transformations - scaling and tiling - in the context of counting. By exploiting the invariance of the number of visual primitives to scale and equating the total number of visual primitives in each tile to that in the entire image, we combine these transformations into one constraint and employ a neural network with a contrastive loss to train it. This task generates representations that either match or exceed the current state of the art in the field of transfer learning benchmarks.",1
"In this paper we study the problem of image representation learning without human annotation. By following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.",0
"The focus of our research is on developing an approach for image representation learning that does not rely on human annotation. To achieve this, we utilize self-supervision and construct a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles without manual labeling. This CNN can then be repurposed to perform object classification and detection tasks. To ensure compatibility between these tasks, we introduce a context-free network (CFN), which is a siamese-ennead CNN that takes image tiles as input and limits the receptive field of its early processing units to one tile at a time. Our results demonstrate that the CFN has fewer parameters than AlexNet, but still exhibits similar semantic learning capabilities. By training the CFN on Jigsaw puzzles, we are able to learn both a feature mapping of object parts and their correct spatial arrangement. Our experiments show that the learned features capture semantically relevant content, and our proposed approach outperforms existing methods in various transfer learning benchmarks.",1
"The main contribution of this paper is a simple semi-supervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 and DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at https://github.com/layumi/Person-reID_GAN.",0
"This paper introduces a semi-supervised pipeline that relies solely on the original training set, eliminating the need for additional data collection. The pipeline's main challenge is twofold: how to generate more training data from the existing set and how to use this new data. To address this issue, the paper utilizes a generative adversarial network (GAN) to create unlabeled samples and proposes label smoothing regularization for outliers (LSRO) to assign a uniform label distribution to these samples. The proposed method is tested on the task of person re-identification (re-ID), using a deep convolutional generative adversarial network (DCGAN) for sample generation and a baseline convolutional neural network (CNN) for representation learning. Results show that the addition of GAN-generated data improves the CNN embeddings' discriminative ability, with rank-1 precision increasing by +4.37%, +1.6%, and +2.46% on the Market-1501, CUHK03, and DukeMTMC-reID datasets, respectively. The proposed method is also applied to fine-grained bird recognition, resulting in a +0.6% improvement over a strong baseline. The code for this work is publicly available at https://github.com/layumi/Person-reID_GAN.",1
"The success of deep learning in computer vision is rooted in the ability of deep networks to scale up model complexity as demanded by challenging visual tasks. As complexity is increased, so is the need for large amounts of labeled data to train the model. This is associated with a costly human annotation effort. To address this concern, with the long-term goal of leveraging the abundance of cheap unlabeled data, we explore methods of unsupervised ""pre-training."" In particular, we propose to use self-supervised automatic image colorization.   We show that traditional methods for unsupervised learning, such as layer-wise clustering or autoencoders, remain inferior to supervised pre-training. In search for an alternative, we develop a fully automatic image colorization method. Our method sets a new state-of-the-art in revitalizing old black-and-white photography, without requiring human effort or expertise. Additionally, it gives us a method for self-supervised representation learning. In order for the model to appropriately re-color a grayscale object, it must first be able to identify it. This ability, learned entirely self-supervised, can be used to improve other visual tasks, such as classification and semantic segmentation. As a future direction for self-supervision, we investigate if multiple proxy tasks can be combined to improve generalization. This turns out to be a challenging open problem. We hope that our contributions to this endeavor will provide a foundation for future efforts in making self-supervision compete with supervised pre-training.",0
"Deep learning has been successful in computer vision due to the capability of deep networks to increase model complexity as the visual tasks become more demanding. However, this also requires a substantial amount of labeled data to train the model, which can be costly and time-consuming. To tackle this issue, we aim to utilize unsupervised ""pre-training"" using self-supervised automatic image colorization to leverage the abundance of cheap unlabeled data. We have observed that traditional unsupervised learning methods are less effective than supervised pre-training. Therefore, we have developed a fully automatic image colorization method that sets a new state-of-the-art in revitalizing old black-and-white photography without requiring human expertise. This self-supervised representation learning method can be used to improve other visual tasks such as classification and semantic segmentation. As a future direction, we are exploring the possibility of combining multiple proxy tasks to improve generalization, which is a challenging open problem. Our contributions will provide a foundation for future efforts in making self-supervision more competitive with supervised pre-training.",1
"The success of any machine learning system depends critically on effective representations of data. In many cases, it is desirable that a representation scheme uncovers the parts-based, additive nature of the data. Of current representation learning schemes, restricted Boltzmann machines (RBMs) have proved to be highly effective in unsupervised settings. However, when it comes to parts-based discovery, RBMs do not usually produce satisfactory results. We enhance such capacity of RBMs by introducing nonnegativity into the model weights, resulting in a variant called nonnegative restricted Boltzmann machine (NRBM). The NRBM produces not only controllable decomposition of data into interpretable parts but also offers a way to estimate the intrinsic nonlinear dimensionality of data, and helps to stabilize linear predictive models. We demonstrate the capacity of our model on applications such as handwritten digit recognition, face recognition, document classification and patient readmission prognosis. The decomposition quality on images is comparable with or better than what produced by the nonnegative matrix factorization (NMF), and the thematic features uncovered from text are qualitatively interpretable in a similar manner to that of the latent Dirichlet allocation (LDA). The stability performance of feature selection on medical data is better than RBM and competitive with NMF. The learned features, when used for classification, are more discriminative than those discovered by both NMF and LDA and comparable with those by RBM.",0
"The effectiveness of a machine learning system relies heavily on the representation of the data. Ideally, a representation scheme should reveal the additive, parts-based nature of the data. Restricted Boltzmann machines (RBMs) have been successful in unsupervised settings, but they often fall short in discovering parts-based structures. To address this, we introduce nonnegativity into the model weights of the RBM, creating a variant called nonnegative restricted Boltzmann machine (NRBM). The NRBM not only produces interpretable parts-based decompositions of data, but also estimates the intrinsic nonlinear dimensionality of the data and improves the stability of linear predictive models. We demonstrate the effectiveness of our model in various applications, including handwritten digit recognition, face recognition, document classification, and patient readmission prognosis. Our model's decomposition quality in images is similar to or better than that of nonnegative matrix factorization (NMF), and its thematic features in text are interpretable in the same way as latent Dirichlet allocation (LDA). Our model's feature selection stability on medical data is better than RBM and comparable to NMF. Additionally, the features learned by our model for classification purposes are more discriminative than those discovered by both NMF and LDA, and comparable to those by RBM.",1
"The analysis of mixed data has been raising challenges in statistics and machine learning. One of two most prominent challenges is to develop new statistical techniques and methodologies to effectively handle mixed data by making the data less heterogeneous with minimum loss of information. The other challenge is that such methods must be able to apply in large-scale tasks when dealing with huge amount of mixed data. To tackle these challenges, we introduce parameter sharing and balancing extensions to our recent model, the mixed-variate restricted Boltzmann machine (MV.RBM) which can transform heterogeneous data into homogeneous representation. We also integrate structured sparsity and distance metric learning into RBM-based models. Our proposed methods are applied in various applications including latent patient profile modelling in medical data analysis and representation learning for image retrieval. The experimental results demonstrate the models perform better than baseline methods in medical data and outperform state-of-the-art rivals in image dataset.",0
"In statistics and machine learning, mixed data analysis presents two main challenges. The first challenge is to develop statistical techniques and methodologies that can handle mixed data effectively while minimizing the loss of information and reducing heterogeneous data. The second challenge is to apply these methods to large-scale tasks that involve huge amounts of mixed data. To address these challenges, we have introduced extensions to our recent model, the mixed-variate restricted Boltzmann machine (MV.RBM). These extensions include parameter sharing and balancing, as well as structured sparsity and distance metric learning. Our proposed methods have been successfully applied in various applications, such as latent patient profile modelling in medical data analysis and representation learning for image retrieval. Our experimental results have demonstrated that our models perform better than baseline methods in medical data, and outperform state-of-the-art rivals in image dataset.",1
"Learning visual representations with self-supervised learning has become popular in computer vision. The idea is to design auxiliary tasks where labels are free to obtain. Most of these tasks end up providing data to learn specific kinds of invariance useful for recognition. In this paper, we propose to exploit different self-supervised approaches to learn representations invariant to (i) inter-instance variations (two objects in the same class should have similar features) and (ii) intra-instance variations (viewpoint, pose, deformations, illumination, etc). Instead of combining two approaches with multi-task learning, we argue to organize and reason the data with multiple variations. Specifically, we propose to generate a graph with millions of objects mined from hundreds of thousands of videos. The objects are connected by two types of edges which correspond to two types of invariance: ""different instances but a similar viewpoint and category"" and ""different viewpoints of the same instance"". By applying simple transitivity on the graph with these edges, we can obtain pairs of images exhibiting richer visual invariance. We use this data to train a Triplet-Siamese network with VGG16 as the base architecture and apply the learned representations to different recognition tasks. For object detection, we achieve 63.2% mAP on PASCAL VOC 2007 using Fast R-CNN (compare to 67.3% with ImageNet pre-training). For the challenging COCO dataset, our method is surprisingly close (23.5%) to the ImageNet-supervised counterpart (24.4%) using the Faster R-CNN framework. We also show that our network can perform significantly better than the ImageNet network in the surface normal estimation task.",0
"Self-supervised learning has gained popularity in computer vision for learning visual representations. The approach involves designing auxiliary tasks that do not require labels. Such tasks provide data for learning specific kinds of invariance useful for recognition. This paper proposes to use different self-supervised techniques to learn representations that are invariant to inter-instance and intra-instance variations. Rather than combining two approaches with multi-task learning, the data is organized and reasoned with multiple variations. A graph is generated with millions of objects from hundreds of thousands of videos, connected by two types of edges that correspond to two types of invariance. The resulting pairs of images exhibit richer visual invariance, and are used to train a Triplet-Siamese network with VGG16 as the base architecture. The learned representations are applied to different recognition tasks, achieving impressive results for object detection and surface normal estimation. For example, the method achieves 63.2% mAP on PASCAL VOC 2007 using Fast R-CNN, compared to 67.3% with ImageNet pre-training. On the challenging COCO dataset, the method is surprisingly close (23.5%) to the ImageNet-supervised counterpart (24.4%) using the Faster R-CNN framework.",1
"With the rapid growth of online fashion market, demand for effective fashion recommendation systems has never been greater. In fashion recommendation, the ability to find items that goes well with a few other items based on style is more important than picking a single item based on the user's entire purchase history. Since the same user may have purchased dress suits in one month and casual denims in another, it is impossible to learn the latent style features of those items using only the user ratings. If we were able to represent the style features of fashion items in a reasonable way, we will be able to recommend new items that conform to some small subset of pre-purchased items that make up a coherent style set. We propose Style2Vec, a vector representation model for fashion items. Based on the intuition of distributional semantics used in word embeddings, Style2Vec learns the representation of a fashion item using other items in matching outfits as context. Two different convolutional neural networks are trained to maximize the probability of item co-occurrences. For evaluation, a fashion analogy test is conducted to show that the resulting representation connotes diverse fashion related semantics like shapes, colors, patterns and even latent styles. We also perform style classification using Style2Vec features and show that our method outperforms other baselines.",0
"The demand for effective fashion recommendation systems has increased with the rapid growth of the online fashion market. In fashion recommendation, finding items that complement a few other items based on style is more important than selecting a single item based on the user's complete purchase history. As users may buy dress suits in one month and casual denims in another, it is impossible to learn the latent style features of those items using only their ratings. However, if we can represent the style features of fashion items reasonably, we can recommend new items that conform to a small subset of previously purchased items that make up a coherent style set. We present Style2Vec, a vector representation model for fashion items that learns the representation of a fashion item using other items in matching outfits as context. The resulting representation conveys diverse fashion-related semantics, such as shapes, colors, patterns, and latent styles, as demonstrated through a fashion analogy test. Furthermore, our method outperforms other baselines in style classification using Style2Vec features.",1
"We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.",0
"Our team has created a completely automated system for image colorization using the latest advancements in deep network technology. By utilizing low-level and semantic representations, we are able to predict color histograms on a per-pixel basis. This allows us to generate a color image automatically or make additional adjustments before the final image is formed. Our method surpasses current techniques in both fully and partially automated colorization tasks. Additionally, we investigate the potential of colorization for self-supervised visual representation learning.",1
"Given a large unlabeled set of images, how to efficiently and effectively group them into clusters based on extracted visual representations remains a challenging problem. To address this problem, we propose a convolutional neural network (CNN) to jointly solve clustering and representation learning in an iterative manner. In the proposed method, given an input image set, we first randomly pick k samples and extract their features as initial cluster centroids using the proposed CNN with an initial model pre-trained from the ImageNet dataset. Mini-batch k-means is then performed to assign cluster labels to individual input samples for a mini-batch of images randomly sampled from the input image set until all images are processed. Subsequently, the proposed CNN simultaneously updates the parameters of the proposed CNN and the centroids of image clusters iteratively based on stochastic gradient descent. We also proposed a feature drift compensation scheme to mitigate the drift error caused by feature mismatch in representation learning. Experimental results demonstrate the proposed method outperforms start-of-the-art clustering schemes in terms of accuracy and storage complexity on large-scale image sets containing millions of images.",0
"Clustering a large collection of images into groups based on their extracted visual features is a difficult task. To tackle this challenge, we suggest a convolutional neural network (CNN) that can solve both representation learning and clustering in an iterative manner. Our method involves randomly selecting k samples from the input image set and utilizing the CNN to extract their features as initial cluster centroids. We then perform mini-batch k-means to assign cluster labels to individual input samples in a mini-batch of images randomly selected from the input set. This process is repeated until all images are processed. The proposed CNN updates its parameters and image cluster centroids iteratively using stochastic gradient descent. Furthermore, we introduce a feature drift compensation scheme to minimize feature mismatch in representation learning. Our experimental results demonstrate that our method outperforms state-of-the-art clustering algorithms in terms of accuracy and storage complexity, especially on large-scale image sets comprising millions of images.",1
